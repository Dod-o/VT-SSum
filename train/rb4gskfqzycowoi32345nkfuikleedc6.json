{
    "id": "rb4gskfqzycowoi32345nkfuikleedc6",
    "title": "Kernels and Gaussian Processes",
    "info": {
        "author": [
            "Mark Girolami, University of Glasgow"
        ],
        "published": "July 9, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_girolami_kgp/",
    "segmentation": [
        [
            "When I was asked to.",
            "To give some lectures to this course, I had to look into my crystal ball and try and figure out who was going to be here and at what level of expertise they had.",
            "Unfortunately, I didn't get much information from my crystal ball, so.",
            "What I'm going to be talking about over the next three, possibly 4 lectures.",
            "Would be starting off at a very gentle level where I'll introduce the whole notion of linear regression models from a very classical loss based approach.",
            "From there I'll move on and introduce you to the whole notion of linear models within a probabilistic framework.",
            "So within the likelihood framework and we'll be looking at maximum likelihood estimators of linear and generalized linear models.",
            "From there, we'll take a step forward and I'll introduce you to the whole notion of the Bayesian paradigm within the linear and generalized linear modeling framework.",
            "And then from there I will introduce you to the whole notion of Gaussian process priors over functions and how these priors can be used for regression.",
            "And more Interestingly, for classification problems.",
            "So be warned, I will be starting at a very basic and simple level.",
            "And by the time we get to the final lecture, I would imagine most off you will be getting a lot out of what I'm talking about, so I'll be introducing you to mean field approximations, variational approximations, alternative likelihood functions such as the multinomial probit and so forth for Gaussian process is OK, so don't think that because this lecture is going to be simple over estar, and although this is going to be simple, I hope that you'll get some insights, perhaps into things that.",
            "Maybe you already know and for those of you who don't know all of this work, then hopefully and it will give you a good foundation too.",
            "Develop your own methods or to study this field a little bit more now.",
            "I have I'm using some lecture slides from a class which I teach at Glasgow.",
            "And this is the web address.",
            "So you'll be able to download some of the lecture slides in PDF, and there are also sets of notes which give you a lot more detail about the material in the slides.",
            "And then there are also some laboratory exercises, predominantly laboratory exercises, and a whole load of a little MATLAB scripts which you can use to study the various methods which are being considered and will be using some of these in the lab.",
            "So if you just take a note of that web address and we will use some of these scripts in the laboratories stations.",
            "OK. Great so."
        ],
        [
            "So let's get started now.",
            "Why am I starting off with linear regression when I'm meant to be talking about Kernow methods and Gaussian process is, well, primarily because these are the.",
            "The whole basis upon which all of these other methods are built.",
            "So I think if we recover these and we have a good understanding of what these methods are, then we can.",
            "Better appreciate the most sophisticated or certainly more so."
        ],
        [
            "Sticky, thin sounding methods which will come later on.",
            "On OK.",
            "Linear regression again, you probably some of you have probably covered this, and if it feels familiar to you, so hopefully this will be a refresher.",
            "And for those of you who haven't, then hopefully you'll find this somewhat interesting.",
            "Now I'm going to try and cram two lectures into one, so let's see how we get on.",
            "So the whole idea of machine learning at the end of the day is that we want to learn or as a losers who are most statistically minded would say we want to infer a functional relationship between a set of attribute variables and some response or target variables.",
            "And for those of you who are listening to Watkins lectures or some of his discussions about the work he's doing in Microsoft.",
            "One of the things that we lose sleep about an awful lot of things like click through rates and well click through rate.",
            "What's that?",
            "Well, it's an response.",
            "It's a target variable and the attribute variables that they have for that particular problem are attributes such as the set of web pages that have been visited, the amount of time that has been spent in those web pages, and so forth.",
            "So this is a very general.",
            "Problem and is 1 which is quite universal, so learning a functional relationship."
        ],
        [
            "Between attribute values and target or response variables.",
            "And of course, the whole motivation is that if we have a good model.",
            "Of the relationship between the attributes between your predictors and our our target values, then we can use this model to predict target values in an unknown setting.",
            "So given a new set of attributes, what is the prediction that we can make about our particular target value?",
            "So given a new user who's been browsing on our website, what is our prediction for the click through rate of this individual?"
        ],
        [
            "OK, so the question of course is how do we learn this relationship given that we only have a finite set of observations?"
        ],
        [
            "I'm.",
            "And more importantly, once we have learned this relationship.",
            "So once we have inferred some functional relationship, how can we assess how good the model is as a predictor?",
            "How much trust can be placed in the model when we go into an unknown environment and we start making predictions?",
            "So we need to be able to objectively assess this."
        ],
        [
            "And this is basically what we're going to be talking about in this lecture.",
            "Now in the UK, or certainly in London an.",
            "We are very proud that the Olympic Games are coming to the UK in 2012.",
            "At this courts are quite pleased as well, but the English probably more so, but.",
            "So it struck me that the the the bookies the gambling shops will probably be taking lots of bets for the.",
            "The.",
            "The time that wins the gold medal in the 100 meters, or in this case, the distance in the long jump, right?",
            "So what is the distance that wins the gold medal in the long jump?",
            "And so I thought that what we would do is we would try and use some very basic machine learning to learn predictor of what the gold medal distance is going to be in the long jump.",
            "And once we've done that.",
            "We can make a prediction and then we can go and put some money and bet or input that distance will be OK.",
            "So what do we do?"
        ],
        [
            "Well, we've got some data.",
            "Which.",
            "Corresponds to the winning distance and the year that the games took place."
        ],
        [
            "Now of course there are lots of other attributes that we could have.",
            "The current form of all of the top athletes that potentially may be competing with what the weather may well be like on and so forth.",
            "Anne, and clearly, if we were really going to put any money down and making a bet as to what the actual distance would be, then we would want to use as much information as we possibly could.",
            "So we would gather as as many attributes as possible as many indicators of what the likely winning disk."
        ],
        [
            "This would be.",
            "But from a just from from our little perspective here from the boot counts for perspective, what we want to do is we'll see what sort of predictions we can make if, although we take into account is the amount of time that has elapsed from the very first modern Olympic Games.",
            "So in many ways we're discarding a lot of valuable information, but will use time or elapsed time as a surrogate."
        ],
        [
            "For all of that information that potentially could be available and also on clutters, it doesn't clutter the what I'm going to develop here.",
            "So as one of the speakers said last week, always look at your data.",
            "So let's just do that.",
            "Let's look at the data that we have."
        ],
        [
            "And we will plot the gold medal winning distance against the time.",
            "So the amount of time that elapsed from the start of the first modern games.",
            "I'm not going to use this laser pointer too much because you can see I've got a little shaky hand becausw.",
            "The Rio Ha was drinking last night was a bit stronger than anticipated, so.",
            "OK, so let's let's look at the data and see what information that is.",
            "So the first question is our attribute, which is the.",
            "The time elapsed from the first modern games and the winning distance is there a functional relationship between them.",
            "It is a lapse time.",
            "A predictor of distance.",
            "Someone show to please.",
            "Yes or no.",
            "Yes, you only have to look at the data and you can see there is a relationship.",
            "OK, so there's a number of things, so those of you who are really keen on the long long jump.",
            "Well, of course the games were interrupted during the two wars and you can see this little.",
            "This thing here was of course Bob Beamon in 1969 games held in.",
            "Mexico.",
            "OK, but anyway, there's certainly a functional relation."
        ],
        [
            "And they want to make this sort of."
        ],
        [
            "Qnective visually we can see there's something, so from a formal perspective, what we want to do is we want to find a class of functions which will."
        ],
        [
            "Op are attributes which in this case are integers.",
            "Onto the real line.",
            "Well, it's an actual fact on to the the positive half of the real line, so we're looking for some function.",
            "And given what we've observed, it would seem sort of reasonable that a linear relationship exists.",
            "So we could say that the class of functional that we have is going to be from the linear class, which is parameterized by an intercept value W nought, and some slope value W one.",
            "So here's our model.",
            "The sort of thing that we learned in second year high school linear model.",
            "So the WSR, the three parameters of our model within this linear funk."
        ],
        [
            "Class.",
            "Now what we have to do is we have to identify.",
            "The model parameters.",
            "And one way of doing this.",
            "Is by considering what's called a loss function, and I think someone may be talking in a bit more detail and a bit more depth about loss functions and classes of loss functions, but in essence, what we're going to do is we're just going to define a loss function which will enumerate the mismatch between what the model predicts and the data that we have actually observed, which is our target value T the distance."
        ],
        [
            "I'm.",
            "No, clearly I've loss should be averaged over all possible observations, so we would take an expectation, but here we're going to take.",
            "An empirical expectation.",
            "So we would define our loss for all of the available input output or attribute target example pairs XN&TN, and in this case we've got 25."
        ],
        [
            "Samples.",
            "So we could say that the empirical sample average based was is just going to be the loss between our target value and our model prediction given the attributes and well.",
            "We just take the sample average."
        ],
        [
            "So what sort of loss could we use?",
            "And again, I think.",
            "Various classes of losses will be discussed later on this week, but the most obvious one that we could consider."
        ],
        [
            "Would be a squared error loss.",
            "No, this is quite a sensible choice, and as we'll see in my subsequent lectures that it actually has a historical significance and actually also has a probabilistic basis, so it's not something that's our."
        ],
        [
            "Actually, some things there are of course robust losses, which would be based on absolute deviations, but for the moment we."
        ],
        [
            "We will only consider the squared error loss just to get to where we're going, but please bear in mind that if you if you choose a loss function then of course there are a number of other factors that have to be considered, but in any case for the sample mean squared data we can define it just like."
        ],
        [
            "This here.",
            "No, I believe that you were introduced or you were.",
            "If you memories were refreshed as far as linear algebra was concerned, so you should be familiar with matrix notation, and I will use matrix notation all the time with in these lecture notes.",
            "So for the next couple of slides, or just introduce you to the matrix notation and then we'll use it.",
            "As normal, so we have this very simple linear model which has two parameters and intercept and slope parameter.",
            "We can therefore define the two dimensional column vector lowercase W in bold, which stacks up the parameters of our model and likewise all of the target values from one to N can be stacked up in an end by 1 dimensional column vector."
        ],
        [
            "To call T. Will define.",
            "An end by two dimensional matrix X, as in this right hand column.",
            "Here we will stack up all of our attribute values.",
            "Of course these are the.",
            "Elapsed times from one to N and then we'll append in this column here a column of ones and we'll define this two by N matrix X."
        ],
        [
            "So that means then that."
        ],
        [
            "This means squared error loss for our little.",
            "Linear model.",
            "Can be written.",
            "And compact notation."
        ],
        [
            "Is this here?",
            "So it's just the inner product of the deviation between T -- X WXW is the response of our model.",
            "And clearly this is just.",
            "This inner product is just the square term.",
            "No, for those of you who recognize this then.",
            "Good for you and for those of you who don't recognize this.",
            "I'll leave it as a tutorial exercise for you to actually derive this, and during the lab session, for those of you who are not familiar with this, try and work through it and then grab me and I can show you how to get to this."
        ],
        [
            "OK, so what's the object of the exercise?",
            "Well, there's a number of objects, but the one that we have to tackle at the moment is identify the model based on the functional class that we've assumed.",
            "In other words, we have to identify W which is going to be optimal in the sense that we want to minimize this mean squared error loss, right?",
            "So we want to minimize the squared deviation.",
            "Between for our model predicts and what are our observations actually give us?",
            "So how do we do that?",
            "Someone shoot out quickly.",
            "We take the first derivative with respect to the parameters."
        ],
        [
            "And we solve.",
            "We find the roots so again, this is all schoolboy."
        ],
        [
            "Algebra, so we find the stationary point of the mean squared error and again just for those of you who are unfamiliar with this.",
            "We take, we take the vector of 1st order derivatives with respect to each element of W. Like this and again I will leave it as an exercise for the lab.",
            "For those of you who are not familiar with this.",
            "So we just take the first derivative of this mean square data and we end up with this.",
            "The sun in by 1 dimensional vector.",
            "And of course, we set this to 0."
        ],
        [
            "So I'll just use matrix notation and the derivative of the mean squared error loss is just the inner product of X and the error between T and what the model predicts.",
            "And then we have some theorem here.",
            "So we."
        ],
        [
            "To set this to zero and hopefully solve for W. So again, for those of you who are not familiar with this sort of notation and the manipulation of.",
            "Derivatives are calculus vector calculus.",
            "Then it's it's a good idea to become familiar with."
        ],
        [
            "If you're going to be doing any serious machine learning, and before we go ahead, we just need to sign a fire sales that this is a stationary point as a minimum."
        ],
        [
            "And again, if you remember your schoolboy calculus, then for a single variable.",
            "The second derivatives at the stationary point have to be strictly positive.",
            "For that stationary point to the minimum of the function and the multi Param."
        ],
        [
            "So generalization is that."
        ],
        [
            "The matrix of partial derivatives partial second derivatives are called H requires to be positive definite, so in other words, if we take any random vector A, then the inner product orders this quadratic term here."
        ],
        [
            "This has to be positive.",
            "So.",
            "We typically refer to this metric."
        ],
        [
            "Is that the Hessian matrix?",
            "So we need the expression."
        ],
        [
            "Alright, so again we will just do some more schoolboy calculus and we can stack all of these second derivatives into a two by two matrix in this case and will general case if we have the parameters it would be D by D. And I'll leave it as an exercise for you.",
            "You can see that each of these elements basically corresponds to a constant.",
            "The sum of the actual values of the attributes in.",
            "In this.",
            "Elements here and then.",
            "This final diagonal term is the sum of the squared attribute value."
        ],
        [
            "Again, we can write this matrix.",
            "We can write this.",
            "Matrix of 2nd derivatives in matrix format and it turns out to be X transpose X, so this is actually a very important.",
            "Matrix and I'll talk a little bit more about it later.",
            "The interesting thing is that.",
            "The target values don't appear in here at all.",
            "The only thing that appears are the attribute values.",
            "So we have this inner product of the attribute value."
        ],
        [
            "This will return to this later on.",
            "So.",
            "If X transpose X can be inverted, can be then it will be poor."
        ],
        [
            "List of definite.",
            "So providing that N where N is the number of observations is greater than D, where D is the number of parameters, then they hasten.",
            "Will be positive definite and can be in."
        ],
        [
            "Acted in this case.",
            "And as in is whatever 25 and is only two then did.",
            "This is positive definite, so the stationary point.",
            "Of the mean squared error as a minimum, and."
        ],
        [
            "Everything's good.",
            "So because this can be inverted, then we can obtain the estimator of our parameter vector W hat wear.",
            "High corresponds to an estimate empirical estimate an.",
            "We obtain.",
            "The estimate which minimize minimizes the squared error, so it yields the least squared error.",
            "And if you remember.",
            "Let me just quickly dive back to this here.",
            "The express."
        ],
        [
            "And for the.",
            "Gradient.",
            "Was this so?",
            "We set this to 0, do some again schoolboy algebra and we can.",
            "Immediately solve for W. And it turns out to be.",
            "The.",
            "The well known least squares."
        ],
        [
            "Tomator which we all know and love.",
            "So W heart are our estimate for the parameters of our model is the inverse of this miss here.",
            "Like which is the the the second derivative of the loss, the mean squared error, and then this X transpose, TT being the target values.",
            "OK."
        ],
        [
            "K. Peters, so if we solve for the long jump data then we find that our Intercept is about 276 and the slope is about 3/4 of a of a unit.",
            "Right, so we've identified the model, which will yield the least squares error."
        ],
        [
            "Good, but what we're really interested in is making predictions on this, right?",
            "So what we want to know do is predict what the distance in the long jump that wins the gold medal will be in the 2012 Olympics in London, and then with that prediction we're all going to go to the bookies and we're going to put some money on whether that distance will actually be the one that wins the gold medal or not.",
            "How many of you are very confident about winning some money if we use this least squares estimator?"
        ],
        [
            "OK, not too many or few.",
            "Let's see why.",
            "Well, we can.",
            "We can predict all our target values by using our our estimator.",
            "And what we end up with, of course, is a straight line, so nothing new there.",
            "But it seems to be not a bad, not a bad model.",
            "Seems to fit the data reasonably well.",
            "Seems to explain the sort of gradual improvement overtime.",
            "I'm so this is all fine.",
            "But again, from a machine learning perspective, this isn't really particularly important.",
            "What's important is how good our predictions are.",
            "So."
        ],
        [
            "As money is at stake."
        ],
        [
            "Let's make a prediction.",
            "So what's the distance T heart 2012?"
        ],
        [
            "Well, our model predicts a winning distance of 360.5 inches.",
            "Now that's all it tells us that's that's the answer, OK?",
            "But Sir.",
            "360.5 inches doesn't see anything about?",
            "That's a sure bet.",
            "Or maybe this isn't such a sure bit.",
            "That's all that we get."
        ],
        [
            "Well, let's just think about this at the moment.",
            "The current Olympic record stands at 350 inches.",
            "And the current world record, which was set a long time ago 1991.",
            "Is a distance of 352 inches.",
            "So what does that say about our prediction?",
            "How confident are we in our production?",
            "Will you be running off to the bookies in Edinburgh to put a 10 pound they don't know?",
            "OK, so."
        ],
        [
            "It seems that our prediction is rather optimistic.",
            "I'm so just."
        ],
        [
            "Not basis alone, we probably wouldn't trust us.",
            "So we've used a linear model."
        ],
        [
            "What?",
            "Maybe what we need is a model which captures a lot of these.",
            "If you look here, what we've basically done is we've just looked."
        ],
        [
            "That we've just modeled the trend.",
            "In this data, but you know the there seems to be some sort of oscillation about the trend.",
            "So so maybe maybe we should be looking at a more rich class of functions to model this data, which will give us a more useful prediction.",
            "So rather than using a linear model, why don't we?"
        ],
        [
            "So nonlinear model.",
            "Will will use our nonlinear model, but we'll stick with a more."
        ],
        [
            "Which is linear in terms of the parameters.",
            "And Florence will be talking about creating care, knows and so forth, and how you can still have a linear model within the parameters and get employer nonlinear transformation.",
            "And therefore have a known linear class of model.",
            "So we can apply some sort of nonlinear transformation too."
        ],
        [
            "Attribute values and this will provide us with a much more flexible model, so although.",
            "We will have a much more flexible model.",
            "The model is still linear in the parameters.",
            "That's provided that we do not add any additional parameters associated with the nonlinear transformations.",
            "And we'll talk about that."
        ],
        [
            "Later on in the week.",
            "So.",
            "So one thing that we could use, for example, is we could look at a very, very simple cubic polynomial.",
            "So our function X is just a polynomial expansion of our elapsed time.",
            "So now we have four parameters to think about, and more generally speaking we could use a case order polynomial."
        ],
        [
            "The results, as far as least squares are concerned, still hold.",
            "No, for the simple polynomial expansion our matrix X will still of course consist of a column of ones and then each other column will correspond to the polynomial term that we've used up to whatever case order, so now.",
            "Alright.",
            "Our data matrix design matrix will be an end by key plus one dimensional matrix for key is obviously the order of."
        ],
        [
            "The polynomial that we're using.",
            "So that means that the least squares machinery still holds for no W hat.",
            "Rather than being in the simple case 2 by 1 is not going to be a key plus one.",
            "Column vector."
        ],
        [
            "So well, it let's.",
            "Let's look at our.",
            "A very highly nonlinear model of order K = 9."
        ],
        [
            "And here's what we get.",
            "No it to.",
            "This model right 9th order polynomial.",
            "It still captures the ongoing trend, but it also seems to capture these oscillations.",
            "Now the question."
        ],
        [
            "As.",
            "Is this a better model or not than the linear model?",
            "Would you be prepared to go to the bookies?",
            "I don't know if you book is that's not a European term, that's.",
            "Can I put some money or on any predictions that this 9th order polynomial model would use?",
            "Is this a better model?",
            "Someone shaking their head and on what basis do you make that judgement?",
            "So what we're going to do is we're now going to.",
            "What are we going to do now?",
            "What time do we finish at 11?",
            "1053 OK, so we've got plenty of time.",
            "Anne.",
            "To answer that question, we need to look at the whole notion of generalization theory.",
            "And I have one lecture looking at how the the loss function can be decomposed into bias various and then sort of residual error term.",
            "But what I'm now going to do is move on and look at this whole problem of creating models like this from a probabilistic basis.",
            "So this has been rather.",
            "Well, one could argue it's been rather arbitrary that we have used.",
            "Oh"
        ],
        [
            "OK, so.",
            "We've started off and we've shown how we can identify a linear model using this squared error loss empirical squared error loss.",
            "But that's all that we can really do.",
            "We can't really judge.",
            "How good the model is?",
            "How confident should we be in the estimates that we've obtained?",
            "So what we're now going to do?",
            "You keep it down.",
            "Is."
        ],
        [
            "Is look at this problem of modeling.",
            "Linear linear models within the probabilistic framework, and you've all been exposed to that, and the number of hands that were raised when you were asked.",
            "If you knew about hidden Markov models leading to suggest that probabilistic view is something."
        ],
        [
            "Which a lot of you are familiar with."
        ],
        [
            "So we."
        ],
        [
            "Going to.",
            "Look specifically at the likelihood principle and maximizing the likelihood.",
            "As a method of parameter estimation.",
            "But what we also obtained by employing this probabilistic perspective is, we know can assess the levels of certainty or confidence that we can place in our estimates and in our subsequent predictions.",
            "And this now moves as much more forward in a way that allows us to rationally consider the validity of our models and the validity of the predictions that our model."
        ],
        [
            "Will make.",
            "So basically what we've done is we have defined a very simple model, potentially some linear, some nonlinear function over attributes, and some set of parameters associated with this function.",
            "An which Maps to our our actual target values.",
            "And there is also some error term associated."
        ],
        [
            "With this some noise Tatham.",
            "So the model is based on at the terministic function over outputs.",
            "So."
        ],
        [
            "How about inputs an that deterministic function is then contaminated in some way?",
            "You can view this as contamination by noise, or you could view it as a residual error.",
            "Due to the fact that the class of function that we're using is never going to be able to to create a perfect fit to our."
        ],
        [
            "Ovations.",
            "Note in this particular case here we can make some assumptions or we.",
            "Yeah, we can make some assumptions about this statistical model that we are now going to be developing.",
            "And in this case here.",
            "It seems reasonable, and we could argue that the noise term.",
            "Would be normally distributed.",
            "It's a bit difficult to argue any other way in fight for this particular data, because we have no idea of what this noise term actually means, but certainly as far as the errors are concerned, then we could say that the distribution of these errors in this case would be normal.",
            "That is not always going to be the case, and again we will look at that subsequently, but for the time being, we can assume that.",
            "Are noise term or errors are normally distributed?",
            "On the mean is going to be 0, so over a period of time with this noise is just going to waggle about and the variance is going.",
            "The mean is going to be 0, so there's going to be a trend at all in the noise, and it's going to have some variance will just call that Sigma.",
            "We don't know what that is."
        ],
        [
            "So the typical notation is that our noise, this random variable E, is distributed as a normal distribution with a mean of zero and a standard deviation of Sigma.",
            "So when we are thinking about this, is that the noise epsilon term sits in top off and it can have corrupts the deterministic output of our model F of X.",
            "To give our observations."
        ],
        [
            "And the way that we write this as we see that T. Given X so T conditioned on the value of X is distributed as.",
            "A gaussian.",
            "It will be centered at well.",
            "Clearly the mean is now going to be the deterministic functional value.",
            "And the variance.",
            "Over target values is no clearly going to take on the variance of the noise.",
            "OK, so we're seeing the T conditioned on X.",
            "Is normally distributed.",
            "What's the meaning of functional response and?"
        ],
        [
            "Some some variance term.",
            "So another way that you may see this in the literature is the probability of T given X is equal to.",
            "A normal distribution with mean and variance."
        ],
        [
            "No.",
            "What's the question that we want to ask?",
            "Well the question we want to ask is, how likely is it that we're going to win some money if we make a prediction?",
            "But before that we get to that?",
            "Anne.",
            "How likely is it that I would have observed?",
            "The target values the T values given the X values.",
            "And.",
            "The functional class that I've assumed and the associated model parameters.",
            "So this is this is the leading up to the whole likelihood."
        ],
        [
            "Simple.",
            "So in other words, the likelihood of probable is it that I observe this data.",
            "Let's think of one data point T. Well, the likelihood is clearly going to be the conditional probability of making that observation.",
            "The probability of T given my inputs given my attribute values and the parameters of the model, I could also explicitly condition on the fact that I have assumed some functional class of model as well, but I'll leave that otherwise things will just get cluttered.",
            "So that's the likelihood of."
        ],
        [
            "Having one data point but an actual fight, what I have observed are I've made any observations X one T1 up to XNTN, so my little.",
            "Vectors X&T"
        ],
        [
            "So what I again I am asking the question, how likely is it that I would have made these any observations?",
            "So I want the probability of T1 and T2 and T3 up to TN given.",
            "X1 up to XN and the model parameters and the functional class of the model.",
            "So in other words, I want the probability of T. Right at the end by 1 dimensional vector T given X&W.",
            "And that smile."
        ],
        [
            "Likelihood.",
            "So this joint probability is the likelihood of the day."
        ],
        [
            "So.",
            "Now let's make some assumptions.",
            "So for those of you who are considering a career in machine learning, then assumptions will follow you for the rest of your career.",
            "So let's now assume.",
            "But these observations are made independently of each other.",
            "So in other words, the measurements that we've just made do not affect the following measurement to be made, and in this particular example, that's perfectly reasonable.",
            "So we are assuming statistical independence between our measurements.",
            "So what Michaela just?"
        ],
        [
            "Told you about Factor graph an.",
            "Will come into play here and the way that we define our likelihood function.",
            "The other assumption that will make is that the noise or this error term which corrupts our measurements always comes from the same distribution.",
            "So this is an assumption that we're making for this particular modeling example.",
            "This will not always hold, but in this case we are making this assumption.",
            "So in other words, the outputs the target values are going to be identically distributed, so they're all going to be drawn from the same Gaussian distribution."
        ],
        [
            "And.",
            "That means that we assume that the data is independent.",
            "And it's identically distributed.",
            "And you see this.",
            "This ID will haunt you for the rest of your career."
        ],
        [
            "So in other words, we factored the whole graph of all possible interactions between in between the N observations, which means that the joint likelihood this joint probability can just be written in a factored form and cause we have made the identically distributed assumption.",
            "It's just a product of these Gaussians.",
            "Where each of the means corresponds to the deterministic response of armor."
        ],
        [
            "So."
        ],
        [
            "So there's our likelihood function.",
            "And we can see that the likelihood depends on the model, the model parameters now.",
            "We should just remember here that.",
            "Our model codispot know corresponds to not only the deterministic component but also the stochastic component.",
            "So we have a statistical part of our model now which captures the uncertainty in our observations and this is where we now move forward from a least squares estimator.",
            "Just just giving a prediction, which is just a point prediction to the fact that now we have defined a statistical model which captures and the stochastic component of our model.",
            "Means that hopefully we're going to be able to fit more.",
            "Sensible in terms of characterizing any residual uncertainty as far as predictions are made."
        ],
        [
            "OK.",
            "So what this means then, is that we can use this function to tune the parameters.",
            "Of our model, which now correspond to W and Sigma.",
            "To make.",
            "The data maker observations more likely under our model."
        ],
        [
            "So let's do that and."
        ],
        [
            "So do we do it?",
            "Well, we find the maximum of the likelihood function with respect."
        ],
        [
            "The model parameters.",
            "Then we can use the logarithm of the likelihood.",
            "The logarithm is are a convex function, so the argument which maximizes the likelihood will be the same argument which maximizes the logarithm of the likelihood an this is, well, there are number of reasons why."
        ],
        [
            "We use the log of the likelihood, so again we use the same schoolboy algebra.",
            "We turn the crank we need to take derivatives of the log likelihood function.",
            "So like."
        ],
        [
            "Do that.",
            "So what is the log likelihood well?",
            "Remember that the likelihood is written as a product or of these individual components, so we don't have a some of the log of the components.",
            "Some of log of these Gaussians, you were all introduced to Goshen's by Joaquin, and I believe so.",
            "Our log likelihood function corresponds to this.",
            "This is just a constant term irrespective of the value of the parameters.",
            "We have a term here related to the statistical component of our model, the noise variance.",
            "And then we have this component here which should look familiar to those of you who are still awake.",
            "Ann.",
            "This is basically just the squared mismatch between our deterministic component of our model and our data observations."
        ],
        [
            "So.",
            "We take the stationary points with respect to W again, for those of you familiar with this.",
            "Very good, and for those of you who are not, grab me and again I'll show you how we can make these derivations in the laboratory, but basically the stationary point of the likelihood with respect to the deterministic component of the model."
        ],
        [
            "Is defined as this.",
            "And the matrix of 2nd order partial derivatives of the log likelihood function turns out to be.",
            "Well, this X transpose X term which you saw previously and we now have one over the noise variance and this happens to be strictly negative.",
            "So indeed by finding this stationary point we found the."
        ],
        [
            "Maximum or the likelihood?",
            "And the maximum likelihood solution is then just simply this term here we."
        ],
        [
            "Not enclosed forum and that should look familiar as it is indeed just the least squares estimator.",
            "So for those of you, again, you're not familiar with this.",
            "Let me just label the point that we have now have an equivalence between the maximum likelihood estimator under a linear model and the least squares estimator under R-squared error loss."
        ],
        [
            "Deal well it is a big deal now.",
            "One other thing of course we have to do to fully identify our statistical model is get the stationary points of Sigma Anne and I'll leave that as a tutorial exercise for you and I would even those of you who know this by the back of your hand I would.",
            "I would suggest that you just try it to convince yourself that you are Masters of this."
        ],
        [
            "OK.",
            "But does this bias anything more?",
            "Down the simple least squares estimator.",
            "One of the things that we.",
            "Those of us who work in machine learning lose sleep over is assessing certainty and uncertainty, which is more the case.",
            "So how certain are we in our maximum likelihood estimates?"
        ],
        [
            "Well, if W heart is on maximum likelihood estimate then we want to know what the variance of that estimate is.",
            "So what variability is there around W height?",
            "So clearly the variability is low.",
            "Then we have high confidence available."
        ],
        [
            "It is high, then we won't be as confident, so it can be characterized us."
        ],
        [
            "Well, we can.",
            "No, let's just remember that this is a vector, right?",
            "So W hard is a vector.",
            "So what we want to do is we want to obtain the covariance matrix associated with the covariance.",
            "Of this, this estimate here."
        ],
        [
            "I'm seeing remember that covariance vector is defined as this here if you don't remember then let me just remind you that we take the outer product of our maximum likelihood estimator minus the expected value of a maximum likelihood estimator, right?",
            "So we take the outer product of that and take that expectation.",
            "The expectation is with respect to the stochastic component of our statistical model.",
            "And again, for those of you familiar with second year undergraduate, statistics will know that the covariance can be written as the expectation of the outer product over random variable or random vector minus the outer product of the mean of our random vector.",
            "So we want to get an expression.",
            "For these terms here."
        ],
        [
            "An no, the maximum likelihood estimator, and this is one of the reasons why maximum likelihood and the likelihood principle is a very general principle, and.",
            "It's one which you will use and more or less all of the time.",
            "If you want to.",
            "Well, the forward backward recursions of a hidden Markov model follow from maximizing the likelihood of your sequence given the Markov property or the hidden Markov properties of your model.",
            "So the likelihood principle is 1, which is very general, and for good reason.",
            "It provides an unbiased estimator.",
            "What does that mean?",
            "Well, it means that if you have enough observations.",
            "And your model happens to be true in the sense that it accurately reflects the process by which the data you have observed has been produced.",
            "Then the parameters of that model.",
            "Estimates of the parameters of that model.",
            "If you use maximum likelihood, will be unbiased and otherwise you will tend towards the true parameter values in the asymptotic limit, so it's comforting to know that this estimator will be unbiased.",
            "So in other words, the expectation of our estimator W hat.",
            "Will actually be W where W are the two model parameters.",
            "Convince yourself the expectation of W heart is equal to W, and if you can't convince yourself again, grab me in the lecture in the.",
            "Now there's a philosophical question here.",
            "Of course, George Box famous well.",
            "Chemist under statistician is quoted as saying all models are wrong.",
            "But some are useful, so if all models are wrong.",
            "Right, and all models that you will use and machine learning will typically be wrong in the sense that they do not accurately reflect the process by which the data was was observed.",
            "Then does it make any sense to have an unbiased estimator?",
            "Will in the next lecture when I go on to the Bayesian methodology, then this whole question will arise yet again and we'll see that a natural fact.",
            "It's quite a sensible question to ask, but for the time being."
        ],
        [
            "Let's take comfort in the fact that if our model does indeed reflect the truth, then an unbiased estimator is a good one to have.",
            "OK, so already we have this part of the expression here, so this is just going to be WW transpose or W other two model parameters, so we know just require this matrix of 2nd order moments for W."
        ],
        [
            "So how do we get that?",
            "Well, we know that W heart is this thing here, so let's just take the outer product of these two things.",
            "OK, so this is baby linear algebra, so we just have this this here.",
            "Now.",
            "Remember that these here these axes are given.",
            "There's no stochastic under this model element here.",
            "The only stochastic component that we have or the other target values right?",
            "So when we take."
        ],
        [
            "Take the expectation with respect to W heart double, heart transpose.",
            "Because this whole thing is is basically linear, then our expectation operator is going to operate on TT transpose."
        ],
        [
            "Again, I'm going to run through this quite quickly so that I don't hold you back from your coffee, but.",
            "For those of you who don't quite follow this again, grab me in the laboratory or download the corresponding lecture notes which go into this and a lot more detail.",
            "But in any case, we now need to get an expression for the expectation of TT."
        ],
        [
            "Transpose.",
            "Well, our model our statistical model is basically T plus this deterministic component and then on noise component.",
            "So again we take the outer product and apply the expectation operator.",
            "So when we expand this we see that we have to remember this W is the true model parameter.",
            "So this is the terministic so there will be no expectation this expectation operator will operate on this.",
            "This is a stochastic element, this is a noise or error.",
            "Again, this is the terministic and then we have the outer product of our error terms, so the expectation will just operate on E&E transpose.",
            "Now.",
            "What is the expectation of epsilon zero and the expectation of Y transpose?",
            "I Sigma squared OK so this drops out.",
            "This just becomes an identity scaled by Sigma squared and this is just a deterministic component.",
            "So we have this good."
        ],
        [
            "So we can plug this.",
            "Indigo into EW hard W hard transpose and we end up with again.",
            "If you want to go through, it is fairly trivial, but.",
            "So we have this sort of product of the true parameter values, and then this oddly familiar looking matrix here."
        ],
        [
            "And if we plug everything together, then the overall covariance.",
            "So we have our.",
            "A term I corresponding to the expectation of the outer product of the.",
            "Maximum likelihood estimator.",
            "We have this component here which cancels and we find that the covariance matrix associated with our maximum likelihood estimator is equal to this thing here.",
            "No remember.",
            "OK, where does this come from?"
        ],
        [
            "Anne.",
            "Well, you should remember that."
        ],
        [
            "Some.",
            "This is actually the second derivative of the likelihood with respect to the model parameters.",
            "So the covariance.",
            "Of W. Is equal to the variance of a noise, which of course we have to estimate.",
            "And this matrix here, which just corresponds to our input or attribute values.",
            "And it's the inverse of that.",
            "So in other words, the so we can take the diagonal terms of this here to get the associated variance, marginal variance of each of the elements of our estimator.",
            "But the interesting thing here is that the covariance.",
            "Over maximum likelihood estimator is equal to the negative of the inverse of the Hessian matrix of the likelihood.",
            "No, what this does is it ties and the likelihood surface.",
            "And how the likelihood is determined by our attributes and the levels of certainty and certainty in our estimates.",
            "So if you look at this here, I mean clearly if we've got a very small curvature and our likelihood surface.",
            "So if this is small or the elements of this are smaller than the inverse is going to be large.",
            "Which means that we're going to have a very high variance, high uncertainty.",
            "And the estimate of a particular parameter and that may well suggest that that parameter.",
            "Well, it's suggesting that we can't get a good confident estimate of that parameter, and that's one thing.",
            "It tells us, which means that perhaps we don't have enough data, or it tells us that that parameter may be irrelevant and we have little explanatory value as far as the.",
            "The functional relationship between the attributes and the target value is concerned."
        ],
        [
            "So if we want to make a new prediction, then we can now use our maximum likelihood estimate and we can obtain the variance around this so we can get our T hot new plus Sigma squared.",
            "Associated with."
        ],
        [
            "Our prediction.",
            "Anti hot is just going to be well Xu transpose times on maximum likelihood estimate.",
            "And the variance associated with that prediction is just going to be this quadratic term X transpose multiplied by.",
            "This this thing here and X new and again.",
            "I'll go over the details of that in the laboratory.",
            "For those of you who don't already know it."
        ],
        [
            "So now.",
            "Here's our data.",
            "Alright, so this is.",
            "OK.",
            "Here's some data.",
            "Here is the two.",
            "Function.",
            "Which generated the data, and there's been some noise added to it, so you can see that the data jet is about the true function.",
            "If we use a linear model.",
            "K The number of loads of polynomial is 1, then our maximum likelihood predictions and associated variances are given here.",
            "So pretty wide.",
            "If, on the other hand, we use K = 3, the user cubic polynomial, 3rd order polynomial, then our maximum likelihood predictions.",
            "No certainly seem to have a much better match to our.",
            "Are deterministic the true function?",
            "But we can see that the there are still a fair amount of residual variance associated with the subsequent predictions, certainly less than what we have for the linear model.",
            "OK, so this is good, so we've made a step forward.",
            "We now have a way of characterizing our certainty and uncertainty in our estimates of the parameters of the model and in the subsequent predictions that we make.",
            "So we've made a fairly massive step forward in terms of doing something which is rational and useful.",
            "What?",
            "Let's just think about this.",
            "We know that the true model is basically just a.",
            "A cubic polynomial.",
            "So if we measure the likelihood.",
            "All the data.",
            "Under a model which has K equal to 1 K equal to 2K equal to three, and so on."
        ],
        [
            "Then what do we find?",
            "Well, we find that the maximum of the likelihood as the order of the model increases also increases.",
            "And this is not good.",
            "Because it's saying that.",
            "The true model.",
            "Has a lower likelihood of generating the observed data than the.",
            "An overly complex model.",
            "So on the one hand we know are able to characterize uncertainty in our estimates and our predictions.",
            "But we have no way of assessing.",
            "The complexity of our model and how good that model actually is for the the prediction task.",
            "Cause the maximum likelihood just keeps increasing, so in the subsequent Late Show I'm going to introduce you to the.",
            "The Bayesian principle and I believe you already been introduced to that, and it will be in relation to these classes of linear models.",
            "I'll then introduce you to generalized linear models, which will then take us on to Gaussian process models.",
            "So I think I've kept you away from your coffee for far too long, but I'm glad I got through all of this.",
            "Any questions?",
            "So for those of you who knew all of that, did you learn anything new at all?",
            "Good or the question?",
            "If you could.",
            "So you're going to have like more uncertainty in like boundaries of curves, just because there's no further data beyond that.",
            "So it has to be higher.",
            "OK, well that was all self explanatory coffee."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I was asked to.",
                    "label": 0
                },
                {
                    "sent": "To give some lectures to this course, I had to look into my crystal ball and try and figure out who was going to be here and at what level of expertise they had.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I didn't get much information from my crystal ball, so.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to be talking about over the next three, possibly 4 lectures.",
                    "label": 0
                },
                {
                    "sent": "Would be starting off at a very gentle level where I'll introduce the whole notion of linear regression models from a very classical loss based approach.",
                    "label": 0
                },
                {
                    "sent": "From there I'll move on and introduce you to the whole notion of linear models within a probabilistic framework.",
                    "label": 0
                },
                {
                    "sent": "So within the likelihood framework and we'll be looking at maximum likelihood estimators of linear and generalized linear models.",
                    "label": 0
                },
                {
                    "sent": "From there, we'll take a step forward and I'll introduce you to the whole notion of the Bayesian paradigm within the linear and generalized linear modeling framework.",
                    "label": 0
                },
                {
                    "sent": "And then from there I will introduce you to the whole notion of Gaussian process priors over functions and how these priors can be used for regression.",
                    "label": 1
                },
                {
                    "sent": "And more Interestingly, for classification problems.",
                    "label": 0
                },
                {
                    "sent": "So be warned, I will be starting at a very basic and simple level.",
                    "label": 0
                },
                {
                    "sent": "And by the time we get to the final lecture, I would imagine most off you will be getting a lot out of what I'm talking about, so I'll be introducing you to mean field approximations, variational approximations, alternative likelihood functions such as the multinomial probit and so forth for Gaussian process is OK, so don't think that because this lecture is going to be simple over estar, and although this is going to be simple, I hope that you'll get some insights, perhaps into things that.",
                    "label": 0
                },
                {
                    "sent": "Maybe you already know and for those of you who don't know all of this work, then hopefully and it will give you a good foundation too.",
                    "label": 0
                },
                {
                    "sent": "Develop your own methods or to study this field a little bit more now.",
                    "label": 0
                },
                {
                    "sent": "I have I'm using some lecture slides from a class which I teach at Glasgow.",
                    "label": 0
                },
                {
                    "sent": "And this is the web address.",
                    "label": 0
                },
                {
                    "sent": "So you'll be able to download some of the lecture slides in PDF, and there are also sets of notes which give you a lot more detail about the material in the slides.",
                    "label": 0
                },
                {
                    "sent": "And then there are also some laboratory exercises, predominantly laboratory exercises, and a whole load of a little MATLAB scripts which you can use to study the various methods which are being considered and will be using some of these in the lab.",
                    "label": 0
                },
                {
                    "sent": "So if you just take a note of that web address and we will use some of these scripts in the laboratories stations.",
                    "label": 0
                },
                {
                    "sent": "OK. Great so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's get started now.",
                    "label": 0
                },
                {
                    "sent": "Why am I starting off with linear regression when I'm meant to be talking about Kernow methods and Gaussian process is, well, primarily because these are the.",
                    "label": 0
                },
                {
                    "sent": "The whole basis upon which all of these other methods are built.",
                    "label": 0
                },
                {
                    "sent": "So I think if we recover these and we have a good understanding of what these methods are, then we can.",
                    "label": 0
                },
                {
                    "sent": "Better appreciate the most sophisticated or certainly more so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sticky, thin sounding methods which will come later on.",
                    "label": 0
                },
                {
                    "sent": "On OK.",
                    "label": 0
                },
                {
                    "sent": "Linear regression again, you probably some of you have probably covered this, and if it feels familiar to you, so hopefully this will be a refresher.",
                    "label": 0
                },
                {
                    "sent": "And for those of you who haven't, then hopefully you'll find this somewhat interesting.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to try and cram two lectures into one, so let's see how we get on.",
                    "label": 0
                },
                {
                    "sent": "So the whole idea of machine learning at the end of the day is that we want to learn or as a losers who are most statistically minded would say we want to infer a functional relationship between a set of attribute variables and some response or target variables.",
                    "label": 0
                },
                {
                    "sent": "And for those of you who are listening to Watkins lectures or some of his discussions about the work he's doing in Microsoft.",
                    "label": 0
                },
                {
                    "sent": "One of the things that we lose sleep about an awful lot of things like click through rates and well click through rate.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Well, it's an response.",
                    "label": 0
                },
                {
                    "sent": "It's a target variable and the attribute variables that they have for that particular problem are attributes such as the set of web pages that have been visited, the amount of time that has been spent in those web pages, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is a very general.",
                    "label": 0
                },
                {
                    "sent": "Problem and is 1 which is quite universal, so learning a functional relationship.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between attribute values and target or response variables.",
                    "label": 0
                },
                {
                    "sent": "And of course, the whole motivation is that if we have a good model.",
                    "label": 0
                },
                {
                    "sent": "Of the relationship between the attributes between your predictors and our our target values, then we can use this model to predict target values in an unknown setting.",
                    "label": 0
                },
                {
                    "sent": "So given a new set of attributes, what is the prediction that we can make about our particular target value?",
                    "label": 0
                },
                {
                    "sent": "So given a new user who's been browsing on our website, what is our prediction for the click through rate of this individual?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the question of course is how do we learn this relationship given that we only have a finite set of observations?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And more importantly, once we have learned this relationship.",
                    "label": 0
                },
                {
                    "sent": "So once we have inferred some functional relationship, how can we assess how good the model is as a predictor?",
                    "label": 0
                },
                {
                    "sent": "How much trust can be placed in the model when we go into an unknown environment and we start making predictions?",
                    "label": 0
                },
                {
                    "sent": "So we need to be able to objectively assess this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is basically what we're going to be talking about in this lecture.",
                    "label": 0
                },
                {
                    "sent": "Now in the UK, or certainly in London an.",
                    "label": 0
                },
                {
                    "sent": "We are very proud that the Olympic Games are coming to the UK in 2012.",
                    "label": 0
                },
                {
                    "sent": "At this courts are quite pleased as well, but the English probably more so, but.",
                    "label": 0
                },
                {
                    "sent": "So it struck me that the the the bookies the gambling shops will probably be taking lots of bets for the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The time that wins the gold medal in the 100 meters, or in this case, the distance in the long jump, right?",
                    "label": 0
                },
                {
                    "sent": "So what is the distance that wins the gold medal in the long jump?",
                    "label": 0
                },
                {
                    "sent": "And so I thought that what we would do is we would try and use some very basic machine learning to learn predictor of what the gold medal distance is going to be in the long jump.",
                    "label": 0
                },
                {
                    "sent": "And once we've done that.",
                    "label": 0
                },
                {
                    "sent": "We can make a prediction and then we can go and put some money and bet or input that distance will be OK.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we've got some data.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to the winning distance and the year that the games took place.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now of course there are lots of other attributes that we could have.",
                    "label": 0
                },
                {
                    "sent": "The current form of all of the top athletes that potentially may be competing with what the weather may well be like on and so forth.",
                    "label": 0
                },
                {
                    "sent": "Anne, and clearly, if we were really going to put any money down and making a bet as to what the actual distance would be, then we would want to use as much information as we possibly could.",
                    "label": 0
                },
                {
                    "sent": "So we would gather as as many attributes as possible as many indicators of what the likely winning disk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "But from a just from from our little perspective here from the boot counts for perspective, what we want to do is we'll see what sort of predictions we can make if, although we take into account is the amount of time that has elapsed from the very first modern Olympic Games.",
                    "label": 0
                },
                {
                    "sent": "So in many ways we're discarding a lot of valuable information, but will use time or elapsed time as a surrogate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all of that information that potentially could be available and also on clutters, it doesn't clutter the what I'm going to develop here.",
                    "label": 0
                },
                {
                    "sent": "So as one of the speakers said last week, always look at your data.",
                    "label": 0
                },
                {
                    "sent": "So let's just do that.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the data that we have.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will plot the gold medal winning distance against the time.",
                    "label": 0
                },
                {
                    "sent": "So the amount of time that elapsed from the start of the first modern games.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to use this laser pointer too much because you can see I've got a little shaky hand becausw.",
                    "label": 0
                },
                {
                    "sent": "The Rio Ha was drinking last night was a bit stronger than anticipated, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's look at the data and see what information that is.",
                    "label": 0
                },
                {
                    "sent": "So the first question is our attribute, which is the.",
                    "label": 0
                },
                {
                    "sent": "The time elapsed from the first modern games and the winning distance is there a functional relationship between them.",
                    "label": 0
                },
                {
                    "sent": "It is a lapse time.",
                    "label": 0
                },
                {
                    "sent": "A predictor of distance.",
                    "label": 0
                },
                {
                    "sent": "Someone show to please.",
                    "label": 0
                },
                {
                    "sent": "Yes or no.",
                    "label": 0
                },
                {
                    "sent": "Yes, you only have to look at the data and you can see there is a relationship.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a number of things, so those of you who are really keen on the long long jump.",
                    "label": 0
                },
                {
                    "sent": "Well, of course the games were interrupted during the two wars and you can see this little.",
                    "label": 0
                },
                {
                    "sent": "This thing here was of course Bob Beamon in 1969 games held in.",
                    "label": 0
                },
                {
                    "sent": "Mexico.",
                    "label": 0
                },
                {
                    "sent": "OK, but anyway, there's certainly a functional relation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they want to make this sort of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Qnective visually we can see there's something, so from a formal perspective, what we want to do is we want to find a class of functions which will.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Op are attributes which in this case are integers.",
                    "label": 0
                },
                {
                    "sent": "Onto the real line.",
                    "label": 0
                },
                {
                    "sent": "Well, it's an actual fact on to the the positive half of the real line, so we're looking for some function.",
                    "label": 0
                },
                {
                    "sent": "And given what we've observed, it would seem sort of reasonable that a linear relationship exists.",
                    "label": 0
                },
                {
                    "sent": "So we could say that the class of functional that we have is going to be from the linear class, which is parameterized by an intercept value W nought, and some slope value W one.",
                    "label": 0
                },
                {
                    "sent": "So here's our model.",
                    "label": 0
                },
                {
                    "sent": "The sort of thing that we learned in second year high school linear model.",
                    "label": 0
                },
                {
                    "sent": "So the WSR, the three parameters of our model within this linear funk.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Class.",
                    "label": 0
                },
                {
                    "sent": "Now what we have to do is we have to identify.",
                    "label": 0
                },
                {
                    "sent": "The model parameters.",
                    "label": 0
                },
                {
                    "sent": "And one way of doing this.",
                    "label": 0
                },
                {
                    "sent": "Is by considering what's called a loss function, and I think someone may be talking in a bit more detail and a bit more depth about loss functions and classes of loss functions, but in essence, what we're going to do is we're just going to define a loss function which will enumerate the mismatch between what the model predicts and the data that we have actually observed, which is our target value T the distance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "No, clearly I've loss should be averaged over all possible observations, so we would take an expectation, but here we're going to take.",
                    "label": 0
                },
                {
                    "sent": "An empirical expectation.",
                    "label": 0
                },
                {
                    "sent": "So we would define our loss for all of the available input output or attribute target example pairs XN&TN, and in this case we've got 25.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "So we could say that the empirical sample average based was is just going to be the loss between our target value and our model prediction given the attributes and well.",
                    "label": 0
                },
                {
                    "sent": "We just take the sample average.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what sort of loss could we use?",
                    "label": 0
                },
                {
                    "sent": "And again, I think.",
                    "label": 0
                },
                {
                    "sent": "Various classes of losses will be discussed later on this week, but the most obvious one that we could consider.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be a squared error loss.",
                    "label": 0
                },
                {
                    "sent": "No, this is quite a sensible choice, and as we'll see in my subsequent lectures that it actually has a historical significance and actually also has a probabilistic basis, so it's not something that's our.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, some things there are of course robust losses, which would be based on absolute deviations, but for the moment we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will only consider the squared error loss just to get to where we're going, but please bear in mind that if you if you choose a loss function then of course there are a number of other factors that have to be considered, but in any case for the sample mean squared data we can define it just like.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This here.",
                    "label": 0
                },
                {
                    "sent": "No, I believe that you were introduced or you were.",
                    "label": 0
                },
                {
                    "sent": "If you memories were refreshed as far as linear algebra was concerned, so you should be familiar with matrix notation, and I will use matrix notation all the time with in these lecture notes.",
                    "label": 0
                },
                {
                    "sent": "So for the next couple of slides, or just introduce you to the matrix notation and then we'll use it.",
                    "label": 0
                },
                {
                    "sent": "As normal, so we have this very simple linear model which has two parameters and intercept and slope parameter.",
                    "label": 0
                },
                {
                    "sent": "We can therefore define the two dimensional column vector lowercase W in bold, which stacks up the parameters of our model and likewise all of the target values from one to N can be stacked up in an end by 1 dimensional column vector.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To call T. Will define.",
                    "label": 0
                },
                {
                    "sent": "An end by two dimensional matrix X, as in this right hand column.",
                    "label": 0
                },
                {
                    "sent": "Here we will stack up all of our attribute values.",
                    "label": 0
                },
                {
                    "sent": "Of course these are the.",
                    "label": 0
                },
                {
                    "sent": "Elapsed times from one to N and then we'll append in this column here a column of ones and we'll define this two by N matrix X.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that means then that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This means squared error loss for our little.",
                    "label": 0
                },
                {
                    "sent": "Linear model.",
                    "label": 0
                },
                {
                    "sent": "Can be written.",
                    "label": 0
                },
                {
                    "sent": "And compact notation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this here?",
                    "label": 0
                },
                {
                    "sent": "So it's just the inner product of the deviation between T -- X WXW is the response of our model.",
                    "label": 0
                },
                {
                    "sent": "And clearly this is just.",
                    "label": 0
                },
                {
                    "sent": "This inner product is just the square term.",
                    "label": 0
                },
                {
                    "sent": "No, for those of you who recognize this then.",
                    "label": 0
                },
                {
                    "sent": "Good for you and for those of you who don't recognize this.",
                    "label": 0
                },
                {
                    "sent": "I'll leave it as a tutorial exercise for you to actually derive this, and during the lab session, for those of you who are not familiar with this, try and work through it and then grab me and I can show you how to get to this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's the object of the exercise?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a number of objects, but the one that we have to tackle at the moment is identify the model based on the functional class that we've assumed.",
                    "label": 0
                },
                {
                    "sent": "In other words, we have to identify W which is going to be optimal in the sense that we want to minimize this mean squared error loss, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize the squared deviation.",
                    "label": 0
                },
                {
                    "sent": "Between for our model predicts and what are our observations actually give us?",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Someone shoot out quickly.",
                    "label": 0
                },
                {
                    "sent": "We take the first derivative with respect to the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we solve.",
                    "label": 0
                },
                {
                    "sent": "We find the roots so again, this is all schoolboy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algebra, so we find the stationary point of the mean squared error and again just for those of you who are unfamiliar with this.",
                    "label": 0
                },
                {
                    "sent": "We take, we take the vector of 1st order derivatives with respect to each element of W. Like this and again I will leave it as an exercise for the lab.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are not familiar with this.",
                    "label": 0
                },
                {
                    "sent": "So we just take the first derivative of this mean square data and we end up with this.",
                    "label": 0
                },
                {
                    "sent": "The sun in by 1 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And of course, we set this to 0.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just use matrix notation and the derivative of the mean squared error loss is just the inner product of X and the error between T and what the model predicts.",
                    "label": 0
                },
                {
                    "sent": "And then we have some theorem here.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To set this to zero and hopefully solve for W. So again, for those of you who are not familiar with this sort of notation and the manipulation of.",
                    "label": 0
                },
                {
                    "sent": "Derivatives are calculus vector calculus.",
                    "label": 0
                },
                {
                    "sent": "Then it's it's a good idea to become familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're going to be doing any serious machine learning, and before we go ahead, we just need to sign a fire sales that this is a stationary point as a minimum.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, if you remember your schoolboy calculus, then for a single variable.",
                    "label": 0
                },
                {
                    "sent": "The second derivatives at the stationary point have to be strictly positive.",
                    "label": 0
                },
                {
                    "sent": "For that stationary point to the minimum of the function and the multi Param.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So generalization is that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The matrix of partial derivatives partial second derivatives are called H requires to be positive definite, so in other words, if we take any random vector A, then the inner product orders this quadratic term here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This has to be positive.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We typically refer to this metric.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the Hessian matrix?",
                    "label": 0
                },
                {
                    "sent": "So we need the expression.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so again we will just do some more schoolboy calculus and we can stack all of these second derivatives into a two by two matrix in this case and will general case if we have the parameters it would be D by D. And I'll leave it as an exercise for you.",
                    "label": 0
                },
                {
                    "sent": "You can see that each of these elements basically corresponds to a constant.",
                    "label": 0
                },
                {
                    "sent": "The sum of the actual values of the attributes in.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "Elements here and then.",
                    "label": 0
                },
                {
                    "sent": "This final diagonal term is the sum of the squared attribute value.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we can write this matrix.",
                    "label": 0
                },
                {
                    "sent": "We can write this.",
                    "label": 0
                },
                {
                    "sent": "Matrix of 2nd derivatives in matrix format and it turns out to be X transpose X, so this is actually a very important.",
                    "label": 0
                },
                {
                    "sent": "Matrix and I'll talk a little bit more about it later.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "The target values don't appear in here at all.",
                    "label": 0
                },
                {
                    "sent": "The only thing that appears are the attribute values.",
                    "label": 0
                },
                {
                    "sent": "So we have this inner product of the attribute value.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This will return to this later on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If X transpose X can be inverted, can be then it will be poor.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "List of definite.",
                    "label": 0
                },
                {
                    "sent": "So providing that N where N is the number of observations is greater than D, where D is the number of parameters, then they hasten.",
                    "label": 0
                },
                {
                    "sent": "Will be positive definite and can be in.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acted in this case.",
                    "label": 0
                },
                {
                    "sent": "And as in is whatever 25 and is only two then did.",
                    "label": 0
                },
                {
                    "sent": "This is positive definite, so the stationary point.",
                    "label": 0
                },
                {
                    "sent": "Of the mean squared error as a minimum, and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything's good.",
                    "label": 0
                },
                {
                    "sent": "So because this can be inverted, then we can obtain the estimator of our parameter vector W hat wear.",
                    "label": 0
                },
                {
                    "sent": "High corresponds to an estimate empirical estimate an.",
                    "label": 0
                },
                {
                    "sent": "We obtain.",
                    "label": 0
                },
                {
                    "sent": "The estimate which minimize minimizes the squared error, so it yields the least squared error.",
                    "label": 0
                },
                {
                    "sent": "And if you remember.",
                    "label": 0
                },
                {
                    "sent": "Let me just quickly dive back to this here.",
                    "label": 0
                },
                {
                    "sent": "The express.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the.",
                    "label": 0
                },
                {
                    "sent": "Gradient.",
                    "label": 0
                },
                {
                    "sent": "Was this so?",
                    "label": 0
                },
                {
                    "sent": "We set this to 0, do some again schoolboy algebra and we can.",
                    "label": 0
                },
                {
                    "sent": "Immediately solve for W. And it turns out to be.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The well known least squares.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tomator which we all know and love.",
                    "label": 0
                },
                {
                    "sent": "So W heart are our estimate for the parameters of our model is the inverse of this miss here.",
                    "label": 0
                },
                {
                    "sent": "Like which is the the the second derivative of the loss, the mean squared error, and then this X transpose, TT being the target values.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. Peters, so if we solve for the long jump data then we find that our Intercept is about 276 and the slope is about 3/4 of a of a unit.",
                    "label": 0
                },
                {
                    "sent": "Right, so we've identified the model, which will yield the least squares error.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, but what we're really interested in is making predictions on this, right?",
                    "label": 0
                },
                {
                    "sent": "So what we want to know do is predict what the distance in the long jump that wins the gold medal will be in the 2012 Olympics in London, and then with that prediction we're all going to go to the bookies and we're going to put some money on whether that distance will actually be the one that wins the gold medal or not.",
                    "label": 0
                },
                {
                    "sent": "How many of you are very confident about winning some money if we use this least squares estimator?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, not too many or few.",
                    "label": 0
                },
                {
                    "sent": "Let's see why.",
                    "label": 0
                },
                {
                    "sent": "Well, we can.",
                    "label": 0
                },
                {
                    "sent": "We can predict all our target values by using our our estimator.",
                    "label": 0
                },
                {
                    "sent": "And what we end up with, of course, is a straight line, so nothing new there.",
                    "label": 0
                },
                {
                    "sent": "But it seems to be not a bad, not a bad model.",
                    "label": 0
                },
                {
                    "sent": "Seems to fit the data reasonably well.",
                    "label": 0
                },
                {
                    "sent": "Seems to explain the sort of gradual improvement overtime.",
                    "label": 0
                },
                {
                    "sent": "I'm so this is all fine.",
                    "label": 0
                },
                {
                    "sent": "But again, from a machine learning perspective, this isn't really particularly important.",
                    "label": 0
                },
                {
                    "sent": "What's important is how good our predictions are.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As money is at stake.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's make a prediction.",
                    "label": 0
                },
                {
                    "sent": "So what's the distance T heart 2012?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, our model predicts a winning distance of 360.5 inches.",
                    "label": 0
                },
                {
                    "sent": "Now that's all it tells us that's that's the answer, OK?",
                    "label": 0
                },
                {
                    "sent": "But Sir.",
                    "label": 0
                },
                {
                    "sent": "360.5 inches doesn't see anything about?",
                    "label": 0
                },
                {
                    "sent": "That's a sure bet.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this isn't such a sure bit.",
                    "label": 0
                },
                {
                    "sent": "That's all that we get.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's just think about this at the moment.",
                    "label": 0
                },
                {
                    "sent": "The current Olympic record stands at 350 inches.",
                    "label": 0
                },
                {
                    "sent": "And the current world record, which was set a long time ago 1991.",
                    "label": 0
                },
                {
                    "sent": "Is a distance of 352 inches.",
                    "label": 0
                },
                {
                    "sent": "So what does that say about our prediction?",
                    "label": 0
                },
                {
                    "sent": "How confident are we in our production?",
                    "label": 0
                },
                {
                    "sent": "Will you be running off to the bookies in Edinburgh to put a 10 pound they don't know?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It seems that our prediction is rather optimistic.",
                    "label": 0
                },
                {
                    "sent": "I'm so just.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not basis alone, we probably wouldn't trust us.",
                    "label": 0
                },
                {
                    "sent": "So we've used a linear model.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Maybe what we need is a model which captures a lot of these.",
                    "label": 0
                },
                {
                    "sent": "If you look here, what we've basically done is we've just looked.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we've just modeled the trend.",
                    "label": 0
                },
                {
                    "sent": "In this data, but you know the there seems to be some sort of oscillation about the trend.",
                    "label": 0
                },
                {
                    "sent": "So so maybe maybe we should be looking at a more rich class of functions to model this data, which will give us a more useful prediction.",
                    "label": 0
                },
                {
                    "sent": "So rather than using a linear model, why don't we?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So nonlinear model.",
                    "label": 0
                },
                {
                    "sent": "Will will use our nonlinear model, but we'll stick with a more.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is linear in terms of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And Florence will be talking about creating care, knows and so forth, and how you can still have a linear model within the parameters and get employer nonlinear transformation.",
                    "label": 0
                },
                {
                    "sent": "And therefore have a known linear class of model.",
                    "label": 0
                },
                {
                    "sent": "So we can apply some sort of nonlinear transformation too.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attribute values and this will provide us with a much more flexible model, so although.",
                    "label": 0
                },
                {
                    "sent": "We will have a much more flexible model.",
                    "label": 0
                },
                {
                    "sent": "The model is still linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "That's provided that we do not add any additional parameters associated with the nonlinear transformations.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk about that.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later on in the week.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So one thing that we could use, for example, is we could look at a very, very simple cubic polynomial.",
                    "label": 0
                },
                {
                    "sent": "So our function X is just a polynomial expansion of our elapsed time.",
                    "label": 0
                },
                {
                    "sent": "So now we have four parameters to think about, and more generally speaking we could use a case order polynomial.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results, as far as least squares are concerned, still hold.",
                    "label": 0
                },
                {
                    "sent": "No, for the simple polynomial expansion our matrix X will still of course consist of a column of ones and then each other column will correspond to the polynomial term that we've used up to whatever case order, so now.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Our data matrix design matrix will be an end by key plus one dimensional matrix for key is obviously the order of.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The polynomial that we're using.",
                    "label": 0
                },
                {
                    "sent": "So that means that the least squares machinery still holds for no W hat.",
                    "label": 0
                },
                {
                    "sent": "Rather than being in the simple case 2 by 1 is not going to be a key plus one.",
                    "label": 0
                },
                {
                    "sent": "Column vector.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So well, it let's.",
                    "label": 0
                },
                {
                    "sent": "Let's look at our.",
                    "label": 0
                },
                {
                    "sent": "A very highly nonlinear model of order K = 9.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's what we get.",
                    "label": 0
                },
                {
                    "sent": "No it to.",
                    "label": 0
                },
                {
                    "sent": "This model right 9th order polynomial.",
                    "label": 0
                },
                {
                    "sent": "It still captures the ongoing trend, but it also seems to capture these oscillations.",
                    "label": 0
                },
                {
                    "sent": "Now the question.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "Is this a better model or not than the linear model?",
                    "label": 0
                },
                {
                    "sent": "Would you be prepared to go to the bookies?",
                    "label": 0
                },
                {
                    "sent": "I don't know if you book is that's not a European term, that's.",
                    "label": 0
                },
                {
                    "sent": "Can I put some money or on any predictions that this 9th order polynomial model would use?",
                    "label": 0
                },
                {
                    "sent": "Is this a better model?",
                    "label": 0
                },
                {
                    "sent": "Someone shaking their head and on what basis do you make that judgement?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're now going to.",
                    "label": 0
                },
                {
                    "sent": "What are we going to do now?",
                    "label": 0
                },
                {
                    "sent": "What time do we finish at 11?",
                    "label": 0
                },
                {
                    "sent": "1053 OK, so we've got plenty of time.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "To answer that question, we need to look at the whole notion of generalization theory.",
                    "label": 0
                },
                {
                    "sent": "And I have one lecture looking at how the the loss function can be decomposed into bias various and then sort of residual error term.",
                    "label": 0
                },
                {
                    "sent": "But what I'm now going to do is move on and look at this whole problem of creating models like this from a probabilistic basis.",
                    "label": 0
                },
                {
                    "sent": "So this has been rather.",
                    "label": 0
                },
                {
                    "sent": "Well, one could argue it's been rather arbitrary that we have used.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We've started off and we've shown how we can identify a linear model using this squared error loss empirical squared error loss.",
                    "label": 0
                },
                {
                    "sent": "But that's all that we can really do.",
                    "label": 0
                },
                {
                    "sent": "We can't really judge.",
                    "label": 0
                },
                {
                    "sent": "How good the model is?",
                    "label": 0
                },
                {
                    "sent": "How confident should we be in the estimates that we've obtained?",
                    "label": 0
                },
                {
                    "sent": "So what we're now going to do?",
                    "label": 0
                },
                {
                    "sent": "You keep it down.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is look at this problem of modeling.",
                    "label": 0
                },
                {
                    "sent": "Linear linear models within the probabilistic framework, and you've all been exposed to that, and the number of hands that were raised when you were asked.",
                    "label": 0
                },
                {
                    "sent": "If you knew about hidden Markov models leading to suggest that probabilistic view is something.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which a lot of you are familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to.",
                    "label": 0
                },
                {
                    "sent": "Look specifically at the likelihood principle and maximizing the likelihood.",
                    "label": 0
                },
                {
                    "sent": "As a method of parameter estimation.",
                    "label": 0
                },
                {
                    "sent": "But what we also obtained by employing this probabilistic perspective is, we know can assess the levels of certainty or confidence that we can place in our estimates and in our subsequent predictions.",
                    "label": 0
                },
                {
                    "sent": "And this now moves as much more forward in a way that allows us to rationally consider the validity of our models and the validity of the predictions that our model.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will make.",
                    "label": 0
                },
                {
                    "sent": "So basically what we've done is we have defined a very simple model, potentially some linear, some nonlinear function over attributes, and some set of parameters associated with this function.",
                    "label": 0
                },
                {
                    "sent": "An which Maps to our our actual target values.",
                    "label": 0
                },
                {
                    "sent": "And there is also some error term associated.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this some noise Tatham.",
                    "label": 0
                },
                {
                    "sent": "So the model is based on at the terministic function over outputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How about inputs an that deterministic function is then contaminated in some way?",
                    "label": 0
                },
                {
                    "sent": "You can view this as contamination by noise, or you could view it as a residual error.",
                    "label": 0
                },
                {
                    "sent": "Due to the fact that the class of function that we're using is never going to be able to to create a perfect fit to our.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ovations.",
                    "label": 0
                },
                {
                    "sent": "Note in this particular case here we can make some assumptions or we.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can make some assumptions about this statistical model that we are now going to be developing.",
                    "label": 0
                },
                {
                    "sent": "And in this case here.",
                    "label": 0
                },
                {
                    "sent": "It seems reasonable, and we could argue that the noise term.",
                    "label": 0
                },
                {
                    "sent": "Would be normally distributed.",
                    "label": 0
                },
                {
                    "sent": "It's a bit difficult to argue any other way in fight for this particular data, because we have no idea of what this noise term actually means, but certainly as far as the errors are concerned, then we could say that the distribution of these errors in this case would be normal.",
                    "label": 0
                },
                {
                    "sent": "That is not always going to be the case, and again we will look at that subsequently, but for the time being, we can assume that.",
                    "label": 0
                },
                {
                    "sent": "Are noise term or errors are normally distributed?",
                    "label": 0
                },
                {
                    "sent": "On the mean is going to be 0, so over a period of time with this noise is just going to waggle about and the variance is going.",
                    "label": 0
                },
                {
                    "sent": "The mean is going to be 0, so there's going to be a trend at all in the noise, and it's going to have some variance will just call that Sigma.",
                    "label": 0
                },
                {
                    "sent": "We don't know what that is.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the typical notation is that our noise, this random variable E, is distributed as a normal distribution with a mean of zero and a standard deviation of Sigma.",
                    "label": 0
                },
                {
                    "sent": "So when we are thinking about this, is that the noise epsilon term sits in top off and it can have corrupts the deterministic output of our model F of X.",
                    "label": 0
                },
                {
                    "sent": "To give our observations.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way that we write this as we see that T. Given X so T conditioned on the value of X is distributed as.",
                    "label": 0
                },
                {
                    "sent": "A gaussian.",
                    "label": 0
                },
                {
                    "sent": "It will be centered at well.",
                    "label": 0
                },
                {
                    "sent": "Clearly the mean is now going to be the deterministic functional value.",
                    "label": 0
                },
                {
                    "sent": "And the variance.",
                    "label": 0
                },
                {
                    "sent": "Over target values is no clearly going to take on the variance of the noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're seeing the T conditioned on X.",
                    "label": 0
                },
                {
                    "sent": "Is normally distributed.",
                    "label": 0
                },
                {
                    "sent": "What's the meaning of functional response and?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some some variance term.",
                    "label": 0
                },
                {
                    "sent": "So another way that you may see this in the literature is the probability of T given X is equal to.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution with mean and variance.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "What's the question that we want to ask?",
                    "label": 0
                },
                {
                    "sent": "Well the question we want to ask is, how likely is it that we're going to win some money if we make a prediction?",
                    "label": 0
                },
                {
                    "sent": "But before that we get to that?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "How likely is it that I would have observed?",
                    "label": 0
                },
                {
                    "sent": "The target values the T values given the X values.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The functional class that I've assumed and the associated model parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the leading up to the whole likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the likelihood of probable is it that I observe this data.",
                    "label": 0
                },
                {
                    "sent": "Let's think of one data point T. Well, the likelihood is clearly going to be the conditional probability of making that observation.",
                    "label": 0
                },
                {
                    "sent": "The probability of T given my inputs given my attribute values and the parameters of the model, I could also explicitly condition on the fact that I have assumed some functional class of model as well, but I'll leave that otherwise things will just get cluttered.",
                    "label": 0
                },
                {
                    "sent": "So that's the likelihood of.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having one data point but an actual fight, what I have observed are I've made any observations X one T1 up to XNTN, so my little.",
                    "label": 0
                },
                {
                    "sent": "Vectors X&T",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I again I am asking the question, how likely is it that I would have made these any observations?",
                    "label": 0
                },
                {
                    "sent": "So I want the probability of T1 and T2 and T3 up to TN given.",
                    "label": 0
                },
                {
                    "sent": "X1 up to XN and the model parameters and the functional class of the model.",
                    "label": 0
                },
                {
                    "sent": "So in other words, I want the probability of T. Right at the end by 1 dimensional vector T given X&W.",
                    "label": 0
                },
                {
                    "sent": "And that smile.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this joint probability is the likelihood of the day.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now let's make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who are considering a career in machine learning, then assumptions will follow you for the rest of your career.",
                    "label": 0
                },
                {
                    "sent": "So let's now assume.",
                    "label": 0
                },
                {
                    "sent": "But these observations are made independently of each other.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the measurements that we've just made do not affect the following measurement to be made, and in this particular example, that's perfectly reasonable.",
                    "label": 0
                },
                {
                    "sent": "So we are assuming statistical independence between our measurements.",
                    "label": 0
                },
                {
                    "sent": "So what Michaela just?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Told you about Factor graph an.",
                    "label": 0
                },
                {
                    "sent": "Will come into play here and the way that we define our likelihood function.",
                    "label": 0
                },
                {
                    "sent": "The other assumption that will make is that the noise or this error term which corrupts our measurements always comes from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is an assumption that we're making for this particular modeling example.",
                    "label": 0
                },
                {
                    "sent": "This will not always hold, but in this case we are making this assumption.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the outputs the target values are going to be identically distributed, so they're all going to be drawn from the same Gaussian distribution.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That means that we assume that the data is independent.",
                    "label": 0
                },
                {
                    "sent": "And it's identically distributed.",
                    "label": 0
                },
                {
                    "sent": "And you see this.",
                    "label": 0
                },
                {
                    "sent": "This ID will haunt you for the rest of your career.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in other words, we factored the whole graph of all possible interactions between in between the N observations, which means that the joint likelihood this joint probability can just be written in a factored form and cause we have made the identically distributed assumption.",
                    "label": 0
                },
                {
                    "sent": "It's just a product of these Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Where each of the means corresponds to the deterministic response of armor.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's our likelihood function.",
                    "label": 0
                },
                {
                    "sent": "And we can see that the likelihood depends on the model, the model parameters now.",
                    "label": 0
                },
                {
                    "sent": "We should just remember here that.",
                    "label": 0
                },
                {
                    "sent": "Our model codispot know corresponds to not only the deterministic component but also the stochastic component.",
                    "label": 0
                },
                {
                    "sent": "So we have a statistical part of our model now which captures the uncertainty in our observations and this is where we now move forward from a least squares estimator.",
                    "label": 0
                },
                {
                    "sent": "Just just giving a prediction, which is just a point prediction to the fact that now we have defined a statistical model which captures and the stochastic component of our model.",
                    "label": 0
                },
                {
                    "sent": "Means that hopefully we're going to be able to fit more.",
                    "label": 0
                },
                {
                    "sent": "Sensible in terms of characterizing any residual uncertainty as far as predictions are made.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what this means then, is that we can use this function to tune the parameters.",
                    "label": 0
                },
                {
                    "sent": "Of our model, which now correspond to W and Sigma.",
                    "label": 0
                },
                {
                    "sent": "To make.",
                    "label": 0
                },
                {
                    "sent": "The data maker observations more likely under our model.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's do that and.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do we do it?",
                    "label": 0
                },
                {
                    "sent": "Well, we find the maximum of the likelihood function with respect.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model parameters.",
                    "label": 0
                },
                {
                    "sent": "Then we can use the logarithm of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "The logarithm is are a convex function, so the argument which maximizes the likelihood will be the same argument which maximizes the logarithm of the likelihood an this is, well, there are number of reasons why.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use the log of the likelihood, so again we use the same schoolboy algebra.",
                    "label": 0
                },
                {
                    "sent": "We turn the crank we need to take derivatives of the log likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So like.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that.",
                    "label": 0
                },
                {
                    "sent": "So what is the log likelihood well?",
                    "label": 0
                },
                {
                    "sent": "Remember that the likelihood is written as a product or of these individual components, so we don't have a some of the log of the components.",
                    "label": 0
                },
                {
                    "sent": "Some of log of these Gaussians, you were all introduced to Goshen's by Joaquin, and I believe so.",
                    "label": 0
                },
                {
                    "sent": "Our log likelihood function corresponds to this.",
                    "label": 0
                },
                {
                    "sent": "This is just a constant term irrespective of the value of the parameters.",
                    "label": 0
                },
                {
                    "sent": "We have a term here related to the statistical component of our model, the noise variance.",
                    "label": 0
                },
                {
                    "sent": "And then we have this component here which should look familiar to those of you who are still awake.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "This is basically just the squared mismatch between our deterministic component of our model and our data observations.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We take the stationary points with respect to W again, for those of you familiar with this.",
                    "label": 0
                },
                {
                    "sent": "Very good, and for those of you who are not, grab me and again I'll show you how we can make these derivations in the laboratory, but basically the stationary point of the likelihood with respect to the deterministic component of the model.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is defined as this.",
                    "label": 0
                },
                {
                    "sent": "And the matrix of 2nd order partial derivatives of the log likelihood function turns out to be.",
                    "label": 0
                },
                {
                    "sent": "Well, this X transpose X term which you saw previously and we now have one over the noise variance and this happens to be strictly negative.",
                    "label": 0
                },
                {
                    "sent": "So indeed by finding this stationary point we found the.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximum or the likelihood?",
                    "label": 0
                },
                {
                    "sent": "And the maximum likelihood solution is then just simply this term here we.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not enclosed forum and that should look familiar as it is indeed just the least squares estimator.",
                    "label": 0
                },
                {
                    "sent": "So for those of you, again, you're not familiar with this.",
                    "label": 0
                },
                {
                    "sent": "Let me just label the point that we have now have an equivalence between the maximum likelihood estimator under a linear model and the least squares estimator under R-squared error loss.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deal well it is a big deal now.",
                    "label": 0
                },
                {
                    "sent": "One other thing of course we have to do to fully identify our statistical model is get the stationary points of Sigma Anne and I'll leave that as a tutorial exercise for you and I would even those of you who know this by the back of your hand I would.",
                    "label": 0
                },
                {
                    "sent": "I would suggest that you just try it to convince yourself that you are Masters of this.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But does this bias anything more?",
                    "label": 0
                },
                {
                    "sent": "Down the simple least squares estimator.",
                    "label": 0
                },
                {
                    "sent": "One of the things that we.",
                    "label": 0
                },
                {
                    "sent": "Those of us who work in machine learning lose sleep over is assessing certainty and uncertainty, which is more the case.",
                    "label": 0
                },
                {
                    "sent": "So how certain are we in our maximum likelihood estimates?",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if W heart is on maximum likelihood estimate then we want to know what the variance of that estimate is.",
                    "label": 0
                },
                {
                    "sent": "So what variability is there around W height?",
                    "label": 0
                },
                {
                    "sent": "So clearly the variability is low.",
                    "label": 0
                },
                {
                    "sent": "Then we have high confidence available.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is high, then we won't be as confident, so it can be characterized us.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we can.",
                    "label": 0
                },
                {
                    "sent": "No, let's just remember that this is a vector, right?",
                    "label": 0
                },
                {
                    "sent": "So W hard is a vector.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to obtain the covariance matrix associated with the covariance.",
                    "label": 0
                },
                {
                    "sent": "Of this, this estimate here.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm seeing remember that covariance vector is defined as this here if you don't remember then let me just remind you that we take the outer product of our maximum likelihood estimator minus the expected value of a maximum likelihood estimator, right?",
                    "label": 0
                },
                {
                    "sent": "So we take the outer product of that and take that expectation.",
                    "label": 0
                },
                {
                    "sent": "The expectation is with respect to the stochastic component of our statistical model.",
                    "label": 0
                },
                {
                    "sent": "And again, for those of you familiar with second year undergraduate, statistics will know that the covariance can be written as the expectation of the outer product over random variable or random vector minus the outer product of the mean of our random vector.",
                    "label": 0
                },
                {
                    "sent": "So we want to get an expression.",
                    "label": 0
                },
                {
                    "sent": "For these terms here.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An no, the maximum likelihood estimator, and this is one of the reasons why maximum likelihood and the likelihood principle is a very general principle, and.",
                    "label": 0
                },
                {
                    "sent": "It's one which you will use and more or less all of the time.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Well, the forward backward recursions of a hidden Markov model follow from maximizing the likelihood of your sequence given the Markov property or the hidden Markov properties of your model.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood principle is 1, which is very general, and for good reason.",
                    "label": 0
                },
                {
                    "sent": "It provides an unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "Well, it means that if you have enough observations.",
                    "label": 0
                },
                {
                    "sent": "And your model happens to be true in the sense that it accurately reflects the process by which the data you have observed has been produced.",
                    "label": 0
                },
                {
                    "sent": "Then the parameters of that model.",
                    "label": 0
                },
                {
                    "sent": "Estimates of the parameters of that model.",
                    "label": 0
                },
                {
                    "sent": "If you use maximum likelihood, will be unbiased and otherwise you will tend towards the true parameter values in the asymptotic limit, so it's comforting to know that this estimator will be unbiased.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the expectation of our estimator W hat.",
                    "label": 0
                },
                {
                    "sent": "Will actually be W where W are the two model parameters.",
                    "label": 0
                },
                {
                    "sent": "Convince yourself the expectation of W heart is equal to W, and if you can't convince yourself again, grab me in the lecture in the.",
                    "label": 0
                },
                {
                    "sent": "Now there's a philosophical question here.",
                    "label": 0
                },
                {
                    "sent": "Of course, George Box famous well.",
                    "label": 0
                },
                {
                    "sent": "Chemist under statistician is quoted as saying all models are wrong.",
                    "label": 0
                },
                {
                    "sent": "But some are useful, so if all models are wrong.",
                    "label": 0
                },
                {
                    "sent": "Right, and all models that you will use and machine learning will typically be wrong in the sense that they do not accurately reflect the process by which the data was was observed.",
                    "label": 0
                },
                {
                    "sent": "Then does it make any sense to have an unbiased estimator?",
                    "label": 0
                },
                {
                    "sent": "Will in the next lecture when I go on to the Bayesian methodology, then this whole question will arise yet again and we'll see that a natural fact.",
                    "label": 0
                },
                {
                    "sent": "It's quite a sensible question to ask, but for the time being.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's take comfort in the fact that if our model does indeed reflect the truth, then an unbiased estimator is a good one to have.",
                    "label": 0
                },
                {
                    "sent": "OK, so already we have this part of the expression here, so this is just going to be WW transpose or W other two model parameters, so we know just require this matrix of 2nd order moments for W.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we get that?",
                    "label": 0
                },
                {
                    "sent": "Well, we know that W heart is this thing here, so let's just take the outer product of these two things.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is baby linear algebra, so we just have this this here.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Remember that these here these axes are given.",
                    "label": 0
                },
                {
                    "sent": "There's no stochastic under this model element here.",
                    "label": 0
                },
                {
                    "sent": "The only stochastic component that we have or the other target values right?",
                    "label": 0
                },
                {
                    "sent": "So when we take.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the expectation with respect to W heart double, heart transpose.",
                    "label": 0
                },
                {
                    "sent": "Because this whole thing is is basically linear, then our expectation operator is going to operate on TT transpose.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I'm going to run through this quite quickly so that I don't hold you back from your coffee, but.",
                    "label": 0
                },
                {
                    "sent": "For those of you who don't quite follow this again, grab me in the laboratory or download the corresponding lecture notes which go into this and a lot more detail.",
                    "label": 0
                },
                {
                    "sent": "But in any case, we now need to get an expression for the expectation of TT.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transpose.",
                    "label": 0
                },
                {
                    "sent": "Well, our model our statistical model is basically T plus this deterministic component and then on noise component.",
                    "label": 0
                },
                {
                    "sent": "So again we take the outer product and apply the expectation operator.",
                    "label": 0
                },
                {
                    "sent": "So when we expand this we see that we have to remember this W is the true model parameter.",
                    "label": 0
                },
                {
                    "sent": "So this is the terministic so there will be no expectation this expectation operator will operate on this.",
                    "label": 0
                },
                {
                    "sent": "This is a stochastic element, this is a noise or error.",
                    "label": 0
                },
                {
                    "sent": "Again, this is the terministic and then we have the outer product of our error terms, so the expectation will just operate on E&E transpose.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What is the expectation of epsilon zero and the expectation of Y transpose?",
                    "label": 0
                },
                {
                    "sent": "I Sigma squared OK so this drops out.",
                    "label": 0
                },
                {
                    "sent": "This just becomes an identity scaled by Sigma squared and this is just a deterministic component.",
                    "label": 0
                },
                {
                    "sent": "So we have this good.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can plug this.",
                    "label": 0
                },
                {
                    "sent": "Indigo into EW hard W hard transpose and we end up with again.",
                    "label": 0
                },
                {
                    "sent": "If you want to go through, it is fairly trivial, but.",
                    "label": 0
                },
                {
                    "sent": "So we have this sort of product of the true parameter values, and then this oddly familiar looking matrix here.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we plug everything together, then the overall covariance.",
                    "label": 0
                },
                {
                    "sent": "So we have our.",
                    "label": 0
                },
                {
                    "sent": "A term I corresponding to the expectation of the outer product of the.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "We have this component here which cancels and we find that the covariance matrix associated with our maximum likelihood estimator is equal to this thing here.",
                    "label": 0
                },
                {
                    "sent": "No remember.",
                    "label": 0
                },
                {
                    "sent": "OK, where does this come from?",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, you should remember that.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "This is actually the second derivative of the likelihood with respect to the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So the covariance.",
                    "label": 0
                },
                {
                    "sent": "Of W. Is equal to the variance of a noise, which of course we have to estimate.",
                    "label": 0
                },
                {
                    "sent": "And this matrix here, which just corresponds to our input or attribute values.",
                    "label": 0
                },
                {
                    "sent": "And it's the inverse of that.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the so we can take the diagonal terms of this here to get the associated variance, marginal variance of each of the elements of our estimator.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing here is that the covariance.",
                    "label": 0
                },
                {
                    "sent": "Over maximum likelihood estimator is equal to the negative of the inverse of the Hessian matrix of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "No, what this does is it ties and the likelihood surface.",
                    "label": 0
                },
                {
                    "sent": "And how the likelihood is determined by our attributes and the levels of certainty and certainty in our estimates.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this here, I mean clearly if we've got a very small curvature and our likelihood surface.",
                    "label": 0
                },
                {
                    "sent": "So if this is small or the elements of this are smaller than the inverse is going to be large.",
                    "label": 0
                },
                {
                    "sent": "Which means that we're going to have a very high variance, high uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And the estimate of a particular parameter and that may well suggest that that parameter.",
                    "label": 0
                },
                {
                    "sent": "Well, it's suggesting that we can't get a good confident estimate of that parameter, and that's one thing.",
                    "label": 0
                },
                {
                    "sent": "It tells us, which means that perhaps we don't have enough data, or it tells us that that parameter may be irrelevant and we have little explanatory value as far as the.",
                    "label": 0
                },
                {
                    "sent": "The functional relationship between the attributes and the target value is concerned.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we want to make a new prediction, then we can now use our maximum likelihood estimate and we can obtain the variance around this so we can get our T hot new plus Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Associated with.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our prediction.",
                    "label": 0
                },
                {
                    "sent": "Anti hot is just going to be well Xu transpose times on maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "And the variance associated with that prediction is just going to be this quadratic term X transpose multiplied by.",
                    "label": 0
                },
                {
                    "sent": "This this thing here and X new and again.",
                    "label": 0
                },
                {
                    "sent": "I'll go over the details of that in the laboratory.",
                    "label": 0
                },
                {
                    "sent": "For those of you who don't already know it.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Here's our data.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Here's some data.",
                    "label": 0
                },
                {
                    "sent": "Here is the two.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Which generated the data, and there's been some noise added to it, so you can see that the data jet is about the true function.",
                    "label": 0
                },
                {
                    "sent": "If we use a linear model.",
                    "label": 0
                },
                {
                    "sent": "K The number of loads of polynomial is 1, then our maximum likelihood predictions and associated variances are given here.",
                    "label": 0
                },
                {
                    "sent": "So pretty wide.",
                    "label": 0
                },
                {
                    "sent": "If, on the other hand, we use K = 3, the user cubic polynomial, 3rd order polynomial, then our maximum likelihood predictions.",
                    "label": 0
                },
                {
                    "sent": "No certainly seem to have a much better match to our.",
                    "label": 0
                },
                {
                    "sent": "Are deterministic the true function?",
                    "label": 0
                },
                {
                    "sent": "But we can see that the there are still a fair amount of residual variance associated with the subsequent predictions, certainly less than what we have for the linear model.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is good, so we've made a step forward.",
                    "label": 0
                },
                {
                    "sent": "We now have a way of characterizing our certainty and uncertainty in our estimates of the parameters of the model and in the subsequent predictions that we make.",
                    "label": 0
                },
                {
                    "sent": "So we've made a fairly massive step forward in terms of doing something which is rational and useful.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Let's just think about this.",
                    "label": 0
                },
                {
                    "sent": "We know that the true model is basically just a.",
                    "label": 0
                },
                {
                    "sent": "A cubic polynomial.",
                    "label": 0
                },
                {
                    "sent": "So if we measure the likelihood.",
                    "label": 0
                },
                {
                    "sent": "All the data.",
                    "label": 0
                },
                {
                    "sent": "Under a model which has K equal to 1 K equal to 2K equal to three, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then what do we find?",
                    "label": 0
                },
                {
                    "sent": "Well, we find that the maximum of the likelihood as the order of the model increases also increases.",
                    "label": 0
                },
                {
                    "sent": "And this is not good.",
                    "label": 0
                },
                {
                    "sent": "Because it's saying that.",
                    "label": 0
                },
                {
                    "sent": "The true model.",
                    "label": 0
                },
                {
                    "sent": "Has a lower likelihood of generating the observed data than the.",
                    "label": 0
                },
                {
                    "sent": "An overly complex model.",
                    "label": 0
                },
                {
                    "sent": "So on the one hand we know are able to characterize uncertainty in our estimates and our predictions.",
                    "label": 0
                },
                {
                    "sent": "But we have no way of assessing.",
                    "label": 0
                },
                {
                    "sent": "The complexity of our model and how good that model actually is for the the prediction task.",
                    "label": 0
                },
                {
                    "sent": "Cause the maximum likelihood just keeps increasing, so in the subsequent Late Show I'm going to introduce you to the.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian principle and I believe you already been introduced to that, and it will be in relation to these classes of linear models.",
                    "label": 0
                },
                {
                    "sent": "I'll then introduce you to generalized linear models, which will then take us on to Gaussian process models.",
                    "label": 0
                },
                {
                    "sent": "So I think I've kept you away from your coffee for far too long, but I'm glad I got through all of this.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So for those of you who knew all of that, did you learn anything new at all?",
                    "label": 0
                },
                {
                    "sent": "Good or the question?",
                    "label": 0
                },
                {
                    "sent": "If you could.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have like more uncertainty in like boundaries of curves, just because there's no further data beyond that.",
                    "label": 0
                },
                {
                    "sent": "So it has to be higher.",
                    "label": 0
                },
                {
                    "sent": "OK, well that was all self explanatory coffee.",
                    "label": 0
                }
            ]
        }
    }
}