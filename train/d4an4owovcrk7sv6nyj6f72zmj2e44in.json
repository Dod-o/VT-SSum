{
    "id": "d4an4owovcrk7sv6nyj6f72zmj2e44in",
    "title": "Consistent Structured Estimation for Weighted Bipartite Matching",
    "info": {
        "author": [
            "Tib\u00e9rio Caetano, National ICT Australia",
            "James Petterson, National ICT Australia",
            "Julian McAuley, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Mathematics->Graph Theory",
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/aml08_caetano_csewbm/",
    "segmentation": [
        [
            "I'll be talking about an exponential family model for bipartite matching.",
            "Yeah, so this is work with James Patterson, Julian, McAuley and Gene you from Mixer."
        ],
        [
            "So.",
            "Describe the problem which is type of problem with study model.",
            "How we going to estimate this model and finally some experiments.",
            "Show some experiments, voting computer vision, domain for matching images and also.",
            "How we can use this bipartite matching algorithm to actually do ranking?"
        ],
        [
            "OK, so let's see the problem."
        ],
        [
            "So this is a set of men.",
            "This is a set of woman I need to marry them.",
            "OK, marry them up.",
            "Yeah."
        ],
        [
            "Something like that?",
            "OK, that's our task needs to marry these people, but we have a few constraints."
        ],
        [
            "Who first?",
            "For every pair of men and women will have a happiness score CIJ we will enforce monogamy.",
            "And no person can be unmatched.",
            "So basically this is a one to one map is basically bijection or permutation.",
            "And the goal is to maximize the overall happiness."
        ],
        [
            "Applications OK, we can apply this to people if you want, but if we want to be more Orthodox we can apply to computer vision.",
            "For example, when you want to match up features of different images right for constructing 3D geometry of the scene and things like that."
        ],
        [
            "We can also do machine translation where we have here a bunch of words from one language.",
            "Here's a bunch of formula and other language and we want basically to assign which corresponds to each."
        ],
        [
            "Now let's see how we model this mathematically, OK?",
            "We're going to model this basically as finding a perfect match in a complete bipartite graph, so this is a complete bipartite graph.",
            "Complete bipartite graph.",
            "We have all the possible edges.",
            "That's a bipartite graph.",
            "Can have.",
            "This is a match.",
            "In particular is a perfect match because it is a set of edges in this graph, such that no node.",
            "Is not connected by an edge, so every node is connected by on and only one edge.",
            "So you see that if you have a matching this perfect matching this graph, this would be an analogous to 111 to one map from basically the first set of features to the second set of features, which is basically a permutation of the first set of.",
            "It can be seen as a permutation for its set of node."
        ],
        [
            "OK.",
            "So the solution is a permutation basically bijection here between this set in this set.",
            "The aggregate pairwise happiness or for collective marriage.",
            "We are going to marry all the men to all.",
            "The woman is simply the sum.",
            "Off the weights of.",
            "The edges that indicate the match.",
            "What so CIJ where J is Y of I?",
            "Some over.",
            "The best color collective marriage is the one that maximizes this score.",
            "So basically this is the overall score for an aggregate match of these people and we want to find which match maximizes aggregate score.",
            "OK, this is called the maximum weight perfect bipartite matching problem.",
            "So big name, but you can just call it the assignment problem or linear assignment problem.",
            "This is a very well known problem in commentary optimization and very well studied.",
            "And you can actually solve this exactly in cubic time.",
            "There are well known algorithms, efficient algorithms and nice implementations available on the web.",
            "OK, so this is a well known well studied problem.",
            "How to find the maximum weight perfect bipartite matching?",
            "If you have this?",
            "Scores between corresponding to every edge.",
            "If you have the weights of this graph.",
            "Assuming you have the weights of this graph."
        ],
        [
            "Let's proceed ahead.",
            "So what we're going to do here?",
            "Well, we're going to create an exponential family model for this problem.",
            "What does that mean?"
        ],
        [
            "So essentially we will relax the assumption that we know this course.",
            "We don't know the scores.",
            "You don't need to know the scores.",
            "Instead, what we really have edge features, which is typically what you measure in reality, right?",
            "In reality, you just measure a bunch of features associated to edges.",
            "For example, in the case of computer vision, you may have a bunch of features associated to that particular pair of points, but it's very difficult to tell what's the score of similarity between 2 features, but it's very easy just to extract, for example, Sift features or shape, context, visual, whatever is the feature that you have.",
            "You are going to have a complex feature vector here, and we're going to parameterized this complex feature vector with some parameter Theta.",
            "And then we're going to estimate these parameters so as to obtain the weights of the graph.",
            "OK."
        ],
        [
            "OK.",
            "The way we're going to do that is basically by using a very simple model.",
            "One of the simplest things you can do, which is an exponential family model.",
            "OK, regular exponential family model.",
            "So why here remember why is a match on our graph Y OK?",
            "We're going to create a conditional model of matches on the features of the graph X.",
            "So X is the graph actually.",
            "Technically those features that they showed previously, right?",
            "Those features that actually it's what we obtain from real measurements from your data.",
            "And then here we have exponential family where this is basically our feature map, which takes basically a sufficient statistics which take X&Y and embeds is true to our feature space.",
            "In some linear permission in theater in here is the log partition function.",
            "The mode of this distribution is this, right?",
            "It's the argument is the why that maximizes this entire quantity, which also happens to be the Y that maximizes only this inner product.",
            "So this is the mode of this system of distribution.",
            "So what we're going to do is very simple.",
            "We're going to construct this feature map such that the mode.",
            "Of this distribution.",
            "Corresponds to the solution of the assignment problem.",
            "In other words, is that this core of?",
            "A given match corresponds to this inner product.",
            "That's basically what we're doing.",
            "So we're going to equate this in the product of sufficient statistic with natural parameter with this core of the linear silent problem.",
            "That we have presented previously.",
            "OK.",
            "Note that."
        ],
        [
            "So very simple.",
            "So here is our model so far.",
            "Well, now do another make another assumption well.",
            "We have this equation here, so the next step is, well, we can make really a simple assumption.",
            "Just assume that this feature map is additive on the edges.",
            "Because at the end of the day, that's really what you want.",
            "'cause you want to.",
            "Parameterized a given pair.",
            "Or features somehow right?",
            "So we just we're just going to make this feature map, which takes an entire graph an entire match and make it additive on the edges?",
            "That's a natural assumption, and here you have at the end of the day you have your CIJ parameter, which is the way that you want to learn, which is basically in a product of a new feature vector.",
            "Associated with a particular edge and theater.",
            "In other words, now the pairwise happiness or the weights of edges is parameterized.",
            "Our goal is just to learn which features will maximize the likelihood.",
            "So we want to do more."
        ],
        [
            "Like with this timation.",
            "OK, now we're going to start to see bad news now because this is the log likelihood function OK?",
            "The negative log like this is a convex function.",
            "After this is a linear function of these convex function Theta log partition function.",
            "In practice, we use a priori here as well.",
            "Just to make it simple, let's look at this.",
            "So let's look at the partition function.",
            "The partition function is exponential of the log partition function, which is this quantity here.",
            "OK. Well, you open this thing.",
            "You realize that this is the permanent of matrix be.",
            "So the permanent is similar to the determinant in definition.",
            "But algorithmically speaking it's a sharply complete problem to compute the permanent off of an arbitrary matrix, nonnegative entries.",
            "This matrix is negative entries.",
            "Um, so that's not good.",
            "That's probably why I guess I haven't found this modeling delicious so far in."
        ],
        [
            "Right?",
            "Well.",
            "If we want to do learning well, we have a nice convex problem.",
            "We unconstrained, we just do great, do some, some sort of gradient descent here, and we should be able to do well.",
            "The obvious bad news is that you know partition function not computable, so we want to compute expectations of the partition function, which are in the case.",
            "The gradient of the log partition function exponential form is well known to be the expectation of your sufficient statistics, which will be no no easier than computing the partition function, because this feature.",
            "This can be anything, so you don't have control over these fees here.",
            "In the particular case where fee is one is the partition function, so this is at least as difficult partition functions for arbitrary fee.",
            "So the really bad news."
        ],
        [
            "So now we're going to to show how we try to circumvent this bad news."
        ],
        [
            "So the present I received this year was a present by Mark Huber and Huber Law insulted this year.",
            "It's a very recent result.",
            "Basically what they?",
            "Designed was an algorithm which generates samples.",
            "4 Perfect matches of bipartite graphs.",
            "Um?",
            "To my understanding, this is going to be something big.",
            "It's very recent, just a few months.",
            "I guess this year.",
            "And The thing is that there are two important properties about this algorithm.",
            "First.",
            "In order to generate a sample of these distribution here.",
            "Well, it takes polynomial time in four log North time to produce 1 sample.",
            "Well, you may think it's bad.",
            "Well, it's a little bit bad, yes?",
            "But this sample is exact.",
            "When you drain the sample, you know that you're generating sample from precise distribution that I have created before.",
            "In the previous best sample we had for this problem.",
            "Was of this complex.",
            "So basically it was far more inefficient than this one.",
            "And in practice.",
            "You cannot actually.",
            "Make use of this algorithm.",
            "For any reasonable problems.",
            "And the other thing is that this was a Markov chain Monte Carlo algorithm, so at some point you have to truncate this Markov chain and you would incur bias.",
            "So this is an exact.",
            "You would generate a symbol which would be a biased sample.",
            "So this is exact, in which faster than the other one."
        ],
        [
            "Well, it just happens to be the tool I need to solve the problem.",
            "OK, I'll just give the general idea of the sample not go any more details when how much time.",
            "Our on on hold on.",
            "I won't go much more in details on the algorithms, so just give the OK. That's fine, that's fine.",
            "So basically we give the basic general idea of the sampler because I find this amazing because the sample is extremely simple.",
            "Oh, really, very simple.",
            "The idea of the sample is to construct an upper bound on the partition function.",
            "Which is computable, this upper bound is really easily computable OK?",
            "And use the fact that basically you know.",
            "Self reducibility of primitives to generate successive upper bounds on partial partition forms.",
            "So basically if you fix one particular.",
            "Assignment of a permutation.",
            "What remains is also permitted.",
            "We have Co sets of the original permutation group.",
            "And use sequence of these upper bounds on this.",
            "On this sequence of sets that are subsets of the other set.",
            "In order to generate an accept reject algorithm and therefore an exact sample, you obtain the, so let."
        ],
        [
            "Quickly see basically how this sample works.",
            "So basically very simple case.",
            "Here you have a simple model and here you have six possible permutations, so six possible matches.",
            "So the size of your space is 6.",
            "OK. Now if you fix one of these assignments here, now there are only two.",
            "Remaining permutations consistent with this with this assignment.",
            "And finally here.",
            "If you fix this to assign, there's only one final permutation consistent with these two assignments.",
            "OK, the nice thing is that this set the subset of of valid permutations.",
            "Here is a subset of this set of valid permutations, and so on.",
            "So basically we have a sequence of subset.",
            "And here if you have an upper bound on this partition function, you compute an upper bound on this partition function.",
            "This partition function and then you do accept reject."
        ],
        [
            "Algorithm, it's basically the phone.",
            "So for the entire set, your partition function is your original partition function.",
            "If you fix one of your assignments, then, well, you have the corresponding partition function restricted to that sign, and so on until until you arrive at the last.",
            "Assign which is the partition function, just a single.",
            "A single match, so you just take the weight of your final permutation.",
            "So basically all what you have to do is compute these bounds for each one of these subsets and sample from these proportions here.",
            "And then.",
            "You can show at the end that all these these terms will cancel.",
            "And at the end you only have the weight of the final permutation divided by the entire weight of all the permutations aggregated, which is the upper bound.",
            "Who is the upper bound on the partition function?",
            "But it turns out that the probability of a set accepting a sample is the size of the partition function divided by the upper bound on it.",
            "So if you take the ratio of this with this, you obtain precisely the weight of a given match divided by the partition function, which is your probability distribution.",
            "So basically, by sampling from this scheme here, you will generate a sample that symbolize it's exactly a sample from the original distribution.",
            "And you see that it can't get much simpler than this stuff here.",
            "And that's the beauty about it.",
            "Much, much more simple than the previous algorithm, much faster and produce an exact.",
            "Some."
        ],
        [
            "OK, so just showing here quickly, the upper bound, the form of the upper ball.",
            "I won't go into details of this, but you should look at this paper because I think this is big news."
        ],
        [
            "So why it's good to generate samples well?",
            "Those who like randomness, right?",
            "You just feel computer expectations.",
            "You estimated spectation using multiple approximations."
        ],
        [
            "In terms of optimization, we just use a quasi Newton optimization method to obtain approximate maximum, like this image in case map is."
        ],
        [
            "Nations as well.",
            "OK, so that's a description of what we do.",
            "Very large, very big picture of what we're doing.",
            "So basically what we're trying to do is really deal with this huge space of the symmetric group in a way that, well, we're not exploiting any structure of of the.",
            "Symmetric key instead.",
            "We're just here using this nice upper bound on this partition function.",
            "And this very simple scheme of sampling to generate exact samples and then compute expect approximate expectations.",
            "Now experiments well, it turns out that this thing really works."
        ],
        [
            "OK, just a few examples from the mayor of computer vision, 'cause that's traditionally where I use it to run experiment with these little houses.",
            "So basically here if you don't do the learning here, if you do the learning so basically read our mistakes in the correspondences."
        ],
        [
            "And here if you have larger baseline so you have less mistakes than you have here, this is when you do learning.",
            "And here's when you didn't do learning right."
        ],
        [
            "And also here but this examples.",
            "But now I'm more excited about some new exam."
        ],
        [
            "As we've obtained in in ranking problems, well, it turns out that you can formulate a ranking problem as a matching problem.",
            "OK, so OK, let's assume we want to do ranking for example webpage ranking here have a query."
        ],
        [
            "Example.",
            "Nips.",
            "And OK, we have, for example 123 given order that comes up.",
            "Um, you know how do we learn a ranking function?",
            "This is topic of research at the moment intensive research we've heard of these today, right?",
            "So we've been trying to apply."
        ],
        [
            "These two these ranking problem.",
            "So basically how does it work?",
            "Well the ranking problem can be formulated as a matching problem.",
            "First attempt that was some previous work last year by Lee and smaller.",
            "Using Max margin methods.",
            "OK so here we are using exponential family which is basically maximum.",
            "So obtain consistent consistent estimate.",
            "So here you have a set of documents.",
            "OK, we traveled by a given query.",
            "And here have the ranking of those documents.",
            "So if you see if you encode these things this way, you are going to obtain a permutation.",
            "Here will correspond to a ranking of your.",
            "Documents."
        ],
        [
            "OK.",
            "The idea is the following.",
            "You have a query.",
            "You have a set of documents that respond to that query.",
            "OK, and you also have a set of scores for that document in that query.",
            "So for every query in every document you have a score compatibility OK, which is what you want to learn at the end of the day, but you have training data for this guy.",
            "OK so I have this query have set of documents retrieval by the query and we have labels score for documents with everybody query and typically these scores they you know they are 01 or like in the case of the Netflix data set from one to five and something like that.",
            "You know usually vary from bed."
        ],
        [
            "Excellent.",
            "OK, now the idea is very simple.",
            "You just do the following.",
            "You define your weight of an edge in your graph as a product of ICE core.",
            "Between the document and the query, which is what you want to learn.",
            "In some function of your permutation.",
            "Now you just create any function that's not increasing here.",
            "You create a monotonically decreasing function.",
            "Why do you do that?",
            "Well, because then a test time you can solve this by just sorting.",
            "So the idea is the following.",
            "If this quantity is monotonically decreasing.",
            "The argmax the solution of the linear assignment problem, the assignment problem, which is cubic in general in this case, will be in log.",
            "Be cause.",
            "If you want to optimize an inner product between a vector and another vector, you need to.",
            "How do you do that?",
            "You just permute?",
            "The two vectors with the same order.",
            "That's a well known lemma by, you know.",
            "Paulie little wooden Hardy.",
            "So that's exactly what we're doing here.",
            "This allows us to do training with linear assignment, which could be.",
            "That's fine, but to do testing.",
            "Just by sorting, which is extremely important in practice, so have a very fast.",
            "You know in France, algorithm for for large scale problems."
        ],
        [
            "OK, so here results and then I will finish.",
            "So basically, this letter that is unfortunately apparently is the only.",
            "Only standard data set for benchmarking rankean.",
            "Our claims it has some issues of selection bias and stuff, but that's the only thing we can compare and many people have been criticizing this data set but but that's what we have and let's do it so.",
            "Basically there are three parts of the data set.",
            "This is the first part and we're doing pretty bad, so this is this brew.",
            "This blue here is our method.",
            "OK, and this is in DCG.",
            "Basically score of how good it matches.",
            "We were doing really bad in this in this particular case.",
            "These are.",
            "At least of.",
            "All the state of the art algorithms that that we could find.",
            "Huh?",
            "Um?",
            "It varies because this data set here is different from this data set.",
            "Which is different from this data set.",
            "So basically each of them have different sizes."
        ],
        [
            "Queries for something like 100 queries or something like that, and then every query will return a number of documents that varies between 50 and 200 in one case, or 1000, or the case.",
            "Things like that.",
            "So this is the other case.",
            "OK for the other data set.",
            "So here we are doing extremely well.",
            "OK, so we're doing extremely well compared to the state of the art.",
            "And there is this algorithm just published this here in CR.",
            "That seems to be very competitive as well without."
        ],
        [
            "And here we should be doing as well as well.",
            "OK so basically in this curve here.",
            "It seems to be having these two things, but but I want to call your attention for something here."
        ],
        [
            "Right?",
            "We are using a linear model.",
            "Right, using a linear model.",
            "These models here.",
            "In here.",
            "OK. Are very complicated neural network models extremely?",
            "Nonlinear models with very difficult optimization.",
            "We are using a linear model linear map.",
            "We're not even using a kernel there.",
            "And unimodal distribution basically exponential family.",
            "So it's clear that next step.",
            "Is just a penalizes algorithm.",
            "We just haven't nonparametric exponential family instead of basically parameterising directly feet.",
            "Using theaters are parameters, we just use represented theorem and then we kernelized this algorithm.",
            "Right, if we are getting these results.",
            "I mean with unimodal distribution, which is, you know, linear parameterization.",
            "So we're very hopeful that we may improve on this.",
            "With by make the algorithm nonlinear.",
            "And this is working for."
        ],
        [
            "As you probably realize, so that's my message for today.",
            "Experiments you were saying that you know people have some misgivings about that data sets.",
            "Yes, yes.",
            "A selection bias.",
            "Basically, the way the data sets were produced.",
            "There are two papers in workshop on CR this year.",
            "That basically only talk about this data set an raise issues regarding the, so it's really a big big problem to have good benchmarks for ranking at the moment because as you know you know, I mean big changes, they don't disclose the data sets for research, so that's big big scientific problem, right?",
            "Because many people want to solve these problems.",
            "And yet we don't have good benchmarks, so I wish I could get my hands on Yahoo's or Google's databases for these things.",
            "To be able to test this stuff.",
            "Yeah.",
            "Using it for information retrieval application.",
            "Or see how well you treat the top ten documents correct.",
            "Well, if this.",
            "Well, basically it learns from the order, right but but but it can be easily adapted to weight differently.",
            "Remember we have a we have a bipartite graph here, so every edge.",
            "Every edge is has a feature vector and these parameters.",
            "So basically so.",
            "Basically you can.",
            "You can do whatever you want, basically right.",
            "Captures that absolutely.",
            "And, well, in this case here I was showing in this issue which already already takes into account the first.",
            "That's this plots here take into account, you know, only this in this jet one."
        ],
        [
            "Two or three or four or five.",
            "This basically is information retrieval measure that is only taking into account the order of these guys.",
            "Yeah.",
            "Have you considered the report about stuff using maximum likelihood a different yes.",
            "Yeah, yeah, not maximum silver like with yet, but we did Max margin on this other type of estimate which is not consistent but still maxilla like this.",
            "Since it's consistently interesting at least.",
            "I am not sure.",
            "Oh yeah, yeah yeah that's true.",
            "I mean we, we should probably use that as a benchmark.",
            "Yeah.",
            "This application is really you have more information about.",
            "Possible matches.",
            "What do you mean?",
            "Every.",
            "Oh, you know I'm saying a complete in the sense that it doesn't need to be complete because you can define the edges.",
            "You kind of other features as being 0 or something like that.",
            "I'm just saying complete just just to make it easier to do.",
            "In principle, there's no features can be 0, so the word completes, not really.",
            "Maybe miss lady.",
            "Yeah, maybe a bit misleading, yes.",
            "So how do you spend to the case where the features on each image aren't the same number?",
            "Oh yeah, that's easy.",
            "I just I just wrote the basic model to make it simple to understand for the purpose of the talk.",
            "But you can basically extend create dummy variables and then you define those weights as being zero.",
            "You know?",
            "I mean, this is well well known tricks to do that.",
            "What about if?",
            "You wanted points to just not mentioning.",
            "Is that it?",
            "Well, it's possible, you know, I mean so if you have a smaller number of points on one side, then does your method force every point on the smaller side to mesh to something?",
            "Oh well, you were going to.",
            "OK.",
            "So basically you have this parameter Theta that you know you're going to learn that parameter, but you can engineer your your features as you wish, right?",
            "So if you engineer a feature of a given pair as being infinite coordinate, it doesn't matter what theater will be that that match will be forbidden.",
            "The nice thing about here, since it's a complete since you can use a complete bipartite graph, you can just engineer every edge as you wish according to their features.",
            "If you have prior knowledge.",
            "Speaker as well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be talking about an exponential family model for bipartite matching.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is work with James Patterson, Julian, McAuley and Gene you from Mixer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Describe the problem which is type of problem with study model.",
                    "label": 1
                },
                {
                    "sent": "How we going to estimate this model and finally some experiments.",
                    "label": 0
                },
                {
                    "sent": "Show some experiments, voting computer vision, domain for matching images and also.",
                    "label": 1
                },
                {
                    "sent": "How we can use this bipartite matching algorithm to actually do ranking?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's see the problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a set of men.",
                    "label": 0
                },
                {
                    "sent": "This is a set of woman I need to marry them.",
                    "label": 0
                },
                {
                    "sent": "OK, marry them up.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something like that?",
                    "label": 0
                },
                {
                    "sent": "OK, that's our task needs to marry these people, but we have a few constraints.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who first?",
                    "label": 0
                },
                {
                    "sent": "For every pair of men and women will have a happiness score CIJ we will enforce monogamy.",
                    "label": 0
                },
                {
                    "sent": "And no person can be unmatched.",
                    "label": 1
                },
                {
                    "sent": "So basically this is a one to one map is basically bijection or permutation.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to maximize the overall happiness.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applications OK, we can apply this to people if you want, but if we want to be more Orthodox we can apply to computer vision.",
                    "label": 0
                },
                {
                    "sent": "For example, when you want to match up features of different images right for constructing 3D geometry of the scene and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also do machine translation where we have here a bunch of words from one language.",
                    "label": 0
                },
                {
                    "sent": "Here's a bunch of formula and other language and we want basically to assign which corresponds to each.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see how we model this mathematically, OK?",
                    "label": 0
                },
                {
                    "sent": "We're going to model this basically as finding a perfect match in a complete bipartite graph, so this is a complete bipartite graph.",
                    "label": 1
                },
                {
                    "sent": "Complete bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "We have all the possible edges.",
                    "label": 0
                },
                {
                    "sent": "That's a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "Can have.",
                    "label": 0
                },
                {
                    "sent": "This is a match.",
                    "label": 0
                },
                {
                    "sent": "In particular is a perfect match because it is a set of edges in this graph, such that no node.",
                    "label": 0
                },
                {
                    "sent": "Is not connected by an edge, so every node is connected by on and only one edge.",
                    "label": 0
                },
                {
                    "sent": "So you see that if you have a matching this perfect matching this graph, this would be an analogous to 111 to one map from basically the first set of features to the second set of features, which is basically a permutation of the first set of.",
                    "label": 0
                },
                {
                    "sent": "It can be seen as a permutation for its set of node.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the solution is a permutation basically bijection here between this set in this set.",
                    "label": 1
                },
                {
                    "sent": "The aggregate pairwise happiness or for collective marriage.",
                    "label": 1
                },
                {
                    "sent": "We are going to marry all the men to all.",
                    "label": 0
                },
                {
                    "sent": "The woman is simply the sum.",
                    "label": 0
                },
                {
                    "sent": "Off the weights of.",
                    "label": 0
                },
                {
                    "sent": "The edges that indicate the match.",
                    "label": 0
                },
                {
                    "sent": "What so CIJ where J is Y of I?",
                    "label": 0
                },
                {
                    "sent": "Some over.",
                    "label": 0
                },
                {
                    "sent": "The best color collective marriage is the one that maximizes this score.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the overall score for an aggregate match of these people and we want to find which match maximizes aggregate score.",
                    "label": 1
                },
                {
                    "sent": "OK, this is called the maximum weight perfect bipartite matching problem.",
                    "label": 0
                },
                {
                    "sent": "So big name, but you can just call it the assignment problem or linear assignment problem.",
                    "label": 0
                },
                {
                    "sent": "This is a very well known problem in commentary optimization and very well studied.",
                    "label": 0
                },
                {
                    "sent": "And you can actually solve this exactly in cubic time.",
                    "label": 1
                },
                {
                    "sent": "There are well known algorithms, efficient algorithms and nice implementations available on the web.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a well known well studied problem.",
                    "label": 0
                },
                {
                    "sent": "How to find the maximum weight perfect bipartite matching?",
                    "label": 0
                },
                {
                    "sent": "If you have this?",
                    "label": 0
                },
                {
                    "sent": "Scores between corresponding to every edge.",
                    "label": 0
                },
                {
                    "sent": "If you have the weights of this graph.",
                    "label": 0
                },
                {
                    "sent": "Assuming you have the weights of this graph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's proceed ahead.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do here?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to create an exponential family model for this problem.",
                    "label": 1
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So essentially we will relax the assumption that we know this course.",
                    "label": 1
                },
                {
                    "sent": "We don't know the scores.",
                    "label": 1
                },
                {
                    "sent": "You don't need to know the scores.",
                    "label": 0
                },
                {
                    "sent": "Instead, what we really have edge features, which is typically what you measure in reality, right?",
                    "label": 0
                },
                {
                    "sent": "In reality, you just measure a bunch of features associated to edges.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of computer vision, you may have a bunch of features associated to that particular pair of points, but it's very difficult to tell what's the score of similarity between 2 features, but it's very easy just to extract, for example, Sift features or shape, context, visual, whatever is the feature that you have.",
                    "label": 0
                },
                {
                    "sent": "You are going to have a complex feature vector here, and we're going to parameterized this complex feature vector with some parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to estimate these parameters so as to obtain the weights of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The way we're going to do that is basically by using a very simple model.",
                    "label": 0
                },
                {
                    "sent": "One of the simplest things you can do, which is an exponential family model.",
                    "label": 0
                },
                {
                    "sent": "OK, regular exponential family model.",
                    "label": 1
                },
                {
                    "sent": "So why here remember why is a match on our graph Y OK?",
                    "label": 0
                },
                {
                    "sent": "We're going to create a conditional model of matches on the features of the graph X.",
                    "label": 0
                },
                {
                    "sent": "So X is the graph actually.",
                    "label": 0
                },
                {
                    "sent": "Technically those features that they showed previously, right?",
                    "label": 0
                },
                {
                    "sent": "Those features that actually it's what we obtain from real measurements from your data.",
                    "label": 0
                },
                {
                    "sent": "And then here we have exponential family where this is basically our feature map, which takes basically a sufficient statistics which take X&Y and embeds is true to our feature space.",
                    "label": 0
                },
                {
                    "sent": "In some linear permission in theater in here is the log partition function.",
                    "label": 0
                },
                {
                    "sent": "The mode of this distribution is this, right?",
                    "label": 0
                },
                {
                    "sent": "It's the argument is the why that maximizes this entire quantity, which also happens to be the Y that maximizes only this inner product.",
                    "label": 0
                },
                {
                    "sent": "So this is the mode of this system of distribution.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is very simple.",
                    "label": 1
                },
                {
                    "sent": "We're going to construct this feature map such that the mode.",
                    "label": 0
                },
                {
                    "sent": "Of this distribution.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to the solution of the assignment problem.",
                    "label": 0
                },
                {
                    "sent": "In other words, is that this core of?",
                    "label": 0
                },
                {
                    "sent": "A given match corresponds to this inner product.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we're doing.",
                    "label": 0
                },
                {
                    "sent": "So we're going to equate this in the product of sufficient statistic with natural parameter with this core of the linear silent problem.",
                    "label": 0
                },
                {
                    "sent": "That we have presented previously.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Note that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very simple.",
                    "label": 0
                },
                {
                    "sent": "So here is our model so far.",
                    "label": 0
                },
                {
                    "sent": "Well, now do another make another assumption well.",
                    "label": 0
                },
                {
                    "sent": "We have this equation here, so the next step is, well, we can make really a simple assumption.",
                    "label": 0
                },
                {
                    "sent": "Just assume that this feature map is additive on the edges.",
                    "label": 0
                },
                {
                    "sent": "Because at the end of the day, that's really what you want.",
                    "label": 0
                },
                {
                    "sent": "'cause you want to.",
                    "label": 0
                },
                {
                    "sent": "Parameterized a given pair.",
                    "label": 0
                },
                {
                    "sent": "Or features somehow right?",
                    "label": 0
                },
                {
                    "sent": "So we just we're just going to make this feature map, which takes an entire graph an entire match and make it additive on the edges?",
                    "label": 0
                },
                {
                    "sent": "That's a natural assumption, and here you have at the end of the day you have your CIJ parameter, which is the way that you want to learn, which is basically in a product of a new feature vector.",
                    "label": 0
                },
                {
                    "sent": "Associated with a particular edge and theater.",
                    "label": 0
                },
                {
                    "sent": "In other words, now the pairwise happiness or the weights of edges is parameterized.",
                    "label": 1
                },
                {
                    "sent": "Our goal is just to learn which features will maximize the likelihood.",
                    "label": 1
                },
                {
                    "sent": "So we want to do more.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like with this timation.",
                    "label": 0
                },
                {
                    "sent": "OK, now we're going to start to see bad news now because this is the log likelihood function OK?",
                    "label": 0
                },
                {
                    "sent": "The negative log like this is a convex function.",
                    "label": 0
                },
                {
                    "sent": "After this is a linear function of these convex function Theta log partition function.",
                    "label": 0
                },
                {
                    "sent": "In practice, we use a priori here as well.",
                    "label": 0
                },
                {
                    "sent": "Just to make it simple, let's look at this.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the partition function.",
                    "label": 0
                },
                {
                    "sent": "The partition function is exponential of the log partition function, which is this quantity here.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, you open this thing.",
                    "label": 0
                },
                {
                    "sent": "You realize that this is the permanent of matrix be.",
                    "label": 0
                },
                {
                    "sent": "So the permanent is similar to the determinant in definition.",
                    "label": 0
                },
                {
                    "sent": "But algorithmically speaking it's a sharply complete problem to compute the permanent off of an arbitrary matrix, nonnegative entries.",
                    "label": 0
                },
                {
                    "sent": "This matrix is negative entries.",
                    "label": 0
                },
                {
                    "sent": "Um, so that's not good.",
                    "label": 0
                },
                {
                    "sent": "That's probably why I guess I haven't found this modeling delicious so far in.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If we want to do learning well, we have a nice convex problem.",
                    "label": 0
                },
                {
                    "sent": "We unconstrained, we just do great, do some, some sort of gradient descent here, and we should be able to do well.",
                    "label": 1
                },
                {
                    "sent": "The obvious bad news is that you know partition function not computable, so we want to compute expectations of the partition function, which are in the case.",
                    "label": 0
                },
                {
                    "sent": "The gradient of the log partition function exponential form is well known to be the expectation of your sufficient statistics, which will be no no easier than computing the partition function, because this feature.",
                    "label": 0
                },
                {
                    "sent": "This can be anything, so you don't have control over these fees here.",
                    "label": 0
                },
                {
                    "sent": "In the particular case where fee is one is the partition function, so this is at least as difficult partition functions for arbitrary fee.",
                    "label": 1
                },
                {
                    "sent": "So the really bad news.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to to show how we try to circumvent this bad news.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the present I received this year was a present by Mark Huber and Huber Law insulted this year.",
                    "label": 0
                },
                {
                    "sent": "It's a very recent result.",
                    "label": 0
                },
                {
                    "sent": "Basically what they?",
                    "label": 0
                },
                {
                    "sent": "Designed was an algorithm which generates samples.",
                    "label": 0
                },
                {
                    "sent": "4 Perfect matches of bipartite graphs.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To my understanding, this is going to be something big.",
                    "label": 0
                },
                {
                    "sent": "It's very recent, just a few months.",
                    "label": 0
                },
                {
                    "sent": "I guess this year.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that there are two important properties about this algorithm.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "In order to generate a sample of these distribution here.",
                    "label": 1
                },
                {
                    "sent": "Well, it takes polynomial time in four log North time to produce 1 sample.",
                    "label": 0
                },
                {
                    "sent": "Well, you may think it's bad.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a little bit bad, yes?",
                    "label": 1
                },
                {
                    "sent": "But this sample is exact.",
                    "label": 0
                },
                {
                    "sent": "When you drain the sample, you know that you're generating sample from precise distribution that I have created before.",
                    "label": 0
                },
                {
                    "sent": "In the previous best sample we had for this problem.",
                    "label": 0
                },
                {
                    "sent": "Was of this complex.",
                    "label": 0
                },
                {
                    "sent": "So basically it was far more inefficient than this one.",
                    "label": 0
                },
                {
                    "sent": "And in practice.",
                    "label": 0
                },
                {
                    "sent": "You cannot actually.",
                    "label": 0
                },
                {
                    "sent": "Make use of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "For any reasonable problems.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that this was a Markov chain Monte Carlo algorithm, so at some point you have to truncate this Markov chain and you would incur bias.",
                    "label": 0
                },
                {
                    "sent": "So this is an exact.",
                    "label": 0
                },
                {
                    "sent": "You would generate a symbol which would be a biased sample.",
                    "label": 0
                },
                {
                    "sent": "So this is exact, in which faster than the other one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it just happens to be the tool I need to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll just give the general idea of the sample not go any more details when how much time.",
                    "label": 0
                },
                {
                    "sent": "Our on on hold on.",
                    "label": 0
                },
                {
                    "sent": "I won't go much more in details on the algorithms, so just give the OK. That's fine, that's fine.",
                    "label": 0
                },
                {
                    "sent": "So basically we give the basic general idea of the sampler because I find this amazing because the sample is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "Oh, really, very simple.",
                    "label": 0
                },
                {
                    "sent": "The idea of the sample is to construct an upper bound on the partition function.",
                    "label": 1
                },
                {
                    "sent": "Which is computable, this upper bound is really easily computable OK?",
                    "label": 0
                },
                {
                    "sent": "And use the fact that basically you know.",
                    "label": 1
                },
                {
                    "sent": "Self reducibility of primitives to generate successive upper bounds on partial partition forms.",
                    "label": 0
                },
                {
                    "sent": "So basically if you fix one particular.",
                    "label": 0
                },
                {
                    "sent": "Assignment of a permutation.",
                    "label": 0
                },
                {
                    "sent": "What remains is also permitted.",
                    "label": 1
                },
                {
                    "sent": "We have Co sets of the original permutation group.",
                    "label": 0
                },
                {
                    "sent": "And use sequence of these upper bounds on this.",
                    "label": 0
                },
                {
                    "sent": "On this sequence of sets that are subsets of the other set.",
                    "label": 0
                },
                {
                    "sent": "In order to generate an accept reject algorithm and therefore an exact sample, you obtain the, so let.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly see basically how this sample works.",
                    "label": 0
                },
                {
                    "sent": "So basically very simple case.",
                    "label": 0
                },
                {
                    "sent": "Here you have a simple model and here you have six possible permutations, so six possible matches.",
                    "label": 0
                },
                {
                    "sent": "So the size of your space is 6.",
                    "label": 0
                },
                {
                    "sent": "OK. Now if you fix one of these assignments here, now there are only two.",
                    "label": 0
                },
                {
                    "sent": "Remaining permutations consistent with this with this assignment.",
                    "label": 0
                },
                {
                    "sent": "And finally here.",
                    "label": 0
                },
                {
                    "sent": "If you fix this to assign, there's only one final permutation consistent with these two assignments.",
                    "label": 0
                },
                {
                    "sent": "OK, the nice thing is that this set the subset of of valid permutations.",
                    "label": 0
                },
                {
                    "sent": "Here is a subset of this set of valid permutations, and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a sequence of subset.",
                    "label": 0
                },
                {
                    "sent": "And here if you have an upper bound on this partition function, you compute an upper bound on this partition function.",
                    "label": 0
                },
                {
                    "sent": "This partition function and then you do accept reject.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm, it's basically the phone.",
                    "label": 0
                },
                {
                    "sent": "So for the entire set, your partition function is your original partition function.",
                    "label": 0
                },
                {
                    "sent": "If you fix one of your assignments, then, well, you have the corresponding partition function restricted to that sign, and so on until until you arrive at the last.",
                    "label": 0
                },
                {
                    "sent": "Assign which is the partition function, just a single.",
                    "label": 0
                },
                {
                    "sent": "A single match, so you just take the weight of your final permutation.",
                    "label": 0
                },
                {
                    "sent": "So basically all what you have to do is compute these bounds for each one of these subsets and sample from these proportions here.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "You can show at the end that all these these terms will cancel.",
                    "label": 0
                },
                {
                    "sent": "And at the end you only have the weight of the final permutation divided by the entire weight of all the permutations aggregated, which is the upper bound.",
                    "label": 0
                },
                {
                    "sent": "Who is the upper bound on the partition function?",
                    "label": 0
                },
                {
                    "sent": "But it turns out that the probability of a set accepting a sample is the size of the partition function divided by the upper bound on it.",
                    "label": 1
                },
                {
                    "sent": "So if you take the ratio of this with this, you obtain precisely the weight of a given match divided by the partition function, which is your probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So basically, by sampling from this scheme here, you will generate a sample that symbolize it's exactly a sample from the original distribution.",
                    "label": 0
                },
                {
                    "sent": "And you see that it can't get much simpler than this stuff here.",
                    "label": 0
                },
                {
                    "sent": "And that's the beauty about it.",
                    "label": 0
                },
                {
                    "sent": "Much, much more simple than the previous algorithm, much faster and produce an exact.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just showing here quickly, the upper bound, the form of the upper ball.",
                    "label": 0
                },
                {
                    "sent": "I won't go into details of this, but you should look at this paper because I think this is big news.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why it's good to generate samples well?",
                    "label": 0
                },
                {
                    "sent": "Those who like randomness, right?",
                    "label": 0
                },
                {
                    "sent": "You just feel computer expectations.",
                    "label": 0
                },
                {
                    "sent": "You estimated spectation using multiple approximations.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of optimization, we just use a quasi Newton optimization method to obtain approximate maximum, like this image in case map is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a description of what we do.",
                    "label": 0
                },
                {
                    "sent": "Very large, very big picture of what we're doing.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're trying to do is really deal with this huge space of the symmetric group in a way that, well, we're not exploiting any structure of of the.",
                    "label": 0
                },
                {
                    "sent": "Symmetric key instead.",
                    "label": 0
                },
                {
                    "sent": "We're just here using this nice upper bound on this partition function.",
                    "label": 0
                },
                {
                    "sent": "And this very simple scheme of sampling to generate exact samples and then compute expect approximate expectations.",
                    "label": 0
                },
                {
                    "sent": "Now experiments well, it turns out that this thing really works.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just a few examples from the mayor of computer vision, 'cause that's traditionally where I use it to run experiment with these little houses.",
                    "label": 0
                },
                {
                    "sent": "So basically here if you don't do the learning here, if you do the learning so basically read our mistakes in the correspondences.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here if you have larger baseline so you have less mistakes than you have here, this is when you do learning.",
                    "label": 0
                },
                {
                    "sent": "And here's when you didn't do learning right.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also here but this examples.",
                    "label": 0
                },
                {
                    "sent": "But now I'm more excited about some new exam.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we've obtained in in ranking problems, well, it turns out that you can formulate a ranking problem as a matching problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK, let's assume we want to do ranking for example webpage ranking here have a query.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "Nips.",
                    "label": 0
                },
                {
                    "sent": "And OK, we have, for example 123 given order that comes up.",
                    "label": 0
                },
                {
                    "sent": "Um, you know how do we learn a ranking function?",
                    "label": 0
                },
                {
                    "sent": "This is topic of research at the moment intensive research we've heard of these today, right?",
                    "label": 0
                },
                {
                    "sent": "So we've been trying to apply.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These two these ranking problem.",
                    "label": 0
                },
                {
                    "sent": "So basically how does it work?",
                    "label": 0
                },
                {
                    "sent": "Well the ranking problem can be formulated as a matching problem.",
                    "label": 1
                },
                {
                    "sent": "First attempt that was some previous work last year by Lee and smaller.",
                    "label": 0
                },
                {
                    "sent": "Using Max margin methods.",
                    "label": 0
                },
                {
                    "sent": "OK so here we are using exponential family which is basically maximum.",
                    "label": 0
                },
                {
                    "sent": "So obtain consistent consistent estimate.",
                    "label": 0
                },
                {
                    "sent": "So here you have a set of documents.",
                    "label": 0
                },
                {
                    "sent": "OK, we traveled by a given query.",
                    "label": 0
                },
                {
                    "sent": "And here have the ranking of those documents.",
                    "label": 0
                },
                {
                    "sent": "So if you see if you encode these things this way, you are going to obtain a permutation.",
                    "label": 0
                },
                {
                    "sent": "Here will correspond to a ranking of your.",
                    "label": 0
                },
                {
                    "sent": "Documents.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following.",
                    "label": 0
                },
                {
                    "sent": "You have a query.",
                    "label": 0
                },
                {
                    "sent": "You have a set of documents that respond to that query.",
                    "label": 1
                },
                {
                    "sent": "OK, and you also have a set of scores for that document in that query.",
                    "label": 0
                },
                {
                    "sent": "So for every query in every document you have a score compatibility OK, which is what you want to learn at the end of the day, but you have training data for this guy.",
                    "label": 0
                },
                {
                    "sent": "OK so I have this query have set of documents retrieval by the query and we have labels score for documents with everybody query and typically these scores they you know they are 01 or like in the case of the Netflix data set from one to five and something like that.",
                    "label": 0
                },
                {
                    "sent": "You know usually vary from bed.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "OK, now the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "You just do the following.",
                    "label": 0
                },
                {
                    "sent": "You define your weight of an edge in your graph as a product of ICE core.",
                    "label": 0
                },
                {
                    "sent": "Between the document and the query, which is what you want to learn.",
                    "label": 0
                },
                {
                    "sent": "In some function of your permutation.",
                    "label": 0
                },
                {
                    "sent": "Now you just create any function that's not increasing here.",
                    "label": 0
                },
                {
                    "sent": "You create a monotonically decreasing function.",
                    "label": 0
                },
                {
                    "sent": "Why do you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, because then a test time you can solve this by just sorting.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "If this quantity is monotonically decreasing.",
                    "label": 1
                },
                {
                    "sent": "The argmax the solution of the linear assignment problem, the assignment problem, which is cubic in general in this case, will be in log.",
                    "label": 0
                },
                {
                    "sent": "Be cause.",
                    "label": 0
                },
                {
                    "sent": "If you want to optimize an inner product between a vector and another vector, you need to.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "You just permute?",
                    "label": 0
                },
                {
                    "sent": "The two vectors with the same order.",
                    "label": 0
                },
                {
                    "sent": "That's a well known lemma by, you know.",
                    "label": 0
                },
                {
                    "sent": "Paulie little wooden Hardy.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "This allows us to do training with linear assignment, which could be.",
                    "label": 0
                },
                {
                    "sent": "That's fine, but to do testing.",
                    "label": 1
                },
                {
                    "sent": "Just by sorting, which is extremely important in practice, so have a very fast.",
                    "label": 0
                },
                {
                    "sent": "You know in France, algorithm for for large scale problems.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here results and then I will finish.",
                    "label": 0
                },
                {
                    "sent": "So basically, this letter that is unfortunately apparently is the only.",
                    "label": 0
                },
                {
                    "sent": "Only standard data set for benchmarking rankean.",
                    "label": 0
                },
                {
                    "sent": "Our claims it has some issues of selection bias and stuff, but that's the only thing we can compare and many people have been criticizing this data set but but that's what we have and let's do it so.",
                    "label": 0
                },
                {
                    "sent": "Basically there are three parts of the data set.",
                    "label": 0
                },
                {
                    "sent": "This is the first part and we're doing pretty bad, so this is this brew.",
                    "label": 0
                },
                {
                    "sent": "This blue here is our method.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is in DCG.",
                    "label": 0
                },
                {
                    "sent": "Basically score of how good it matches.",
                    "label": 0
                },
                {
                    "sent": "We were doing really bad in this in this particular case.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "At least of.",
                    "label": 0
                },
                {
                    "sent": "All the state of the art algorithms that that we could find.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It varies because this data set here is different from this data set.",
                    "label": 0
                },
                {
                    "sent": "Which is different from this data set.",
                    "label": 0
                },
                {
                    "sent": "So basically each of them have different sizes.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Queries for something like 100 queries or something like that, and then every query will return a number of documents that varies between 50 and 200 in one case, or 1000, or the case.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "So this is the other case.",
                    "label": 0
                },
                {
                    "sent": "OK for the other data set.",
                    "label": 0
                },
                {
                    "sent": "So here we are doing extremely well.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're doing extremely well compared to the state of the art.",
                    "label": 0
                },
                {
                    "sent": "And there is this algorithm just published this here in CR.",
                    "label": 0
                },
                {
                    "sent": "That seems to be very competitive as well without.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we should be doing as well as well.",
                    "label": 0
                },
                {
                    "sent": "OK so basically in this curve here.",
                    "label": 0
                },
                {
                    "sent": "It seems to be having these two things, but but I want to call your attention for something here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We are using a linear model.",
                    "label": 0
                },
                {
                    "sent": "Right, using a linear model.",
                    "label": 1
                },
                {
                    "sent": "These models here.",
                    "label": 0
                },
                {
                    "sent": "In here.",
                    "label": 0
                },
                {
                    "sent": "OK. Are very complicated neural network models extremely?",
                    "label": 0
                },
                {
                    "sent": "Nonlinear models with very difficult optimization.",
                    "label": 0
                },
                {
                    "sent": "We are using a linear model linear map.",
                    "label": 0
                },
                {
                    "sent": "We're not even using a kernel there.",
                    "label": 1
                },
                {
                    "sent": "And unimodal distribution basically exponential family.",
                    "label": 0
                },
                {
                    "sent": "So it's clear that next step.",
                    "label": 0
                },
                {
                    "sent": "Is just a penalizes algorithm.",
                    "label": 0
                },
                {
                    "sent": "We just haven't nonparametric exponential family instead of basically parameterising directly feet.",
                    "label": 0
                },
                {
                    "sent": "Using theaters are parameters, we just use represented theorem and then we kernelized this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, if we are getting these results.",
                    "label": 0
                },
                {
                    "sent": "I mean with unimodal distribution, which is, you know, linear parameterization.",
                    "label": 0
                },
                {
                    "sent": "So we're very hopeful that we may improve on this.",
                    "label": 0
                },
                {
                    "sent": "With by make the algorithm nonlinear.",
                    "label": 0
                },
                {
                    "sent": "And this is working for.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you probably realize, so that's my message for today.",
                    "label": 0
                },
                {
                    "sent": "Experiments you were saying that you know people have some misgivings about that data sets.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "A selection bias.",
                    "label": 0
                },
                {
                    "sent": "Basically, the way the data sets were produced.",
                    "label": 0
                },
                {
                    "sent": "There are two papers in workshop on CR this year.",
                    "label": 0
                },
                {
                    "sent": "That basically only talk about this data set an raise issues regarding the, so it's really a big big problem to have good benchmarks for ranking at the moment because as you know you know, I mean big changes, they don't disclose the data sets for research, so that's big big scientific problem, right?",
                    "label": 0
                },
                {
                    "sent": "Because many people want to solve these problems.",
                    "label": 0
                },
                {
                    "sent": "And yet we don't have good benchmarks, so I wish I could get my hands on Yahoo's or Google's databases for these things.",
                    "label": 0
                },
                {
                    "sent": "To be able to test this stuff.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Using it for information retrieval application.",
                    "label": 0
                },
                {
                    "sent": "Or see how well you treat the top ten documents correct.",
                    "label": 0
                },
                {
                    "sent": "Well, if this.",
                    "label": 0
                },
                {
                    "sent": "Well, basically it learns from the order, right but but but it can be easily adapted to weight differently.",
                    "label": 0
                },
                {
                    "sent": "Remember we have a we have a bipartite graph here, so every edge.",
                    "label": 0
                },
                {
                    "sent": "Every edge is has a feature vector and these parameters.",
                    "label": 0
                },
                {
                    "sent": "So basically so.",
                    "label": 0
                },
                {
                    "sent": "Basically you can.",
                    "label": 0
                },
                {
                    "sent": "You can do whatever you want, basically right.",
                    "label": 0
                },
                {
                    "sent": "Captures that absolutely.",
                    "label": 0
                },
                {
                    "sent": "And, well, in this case here I was showing in this issue which already already takes into account the first.",
                    "label": 0
                },
                {
                    "sent": "That's this plots here take into account, you know, only this in this jet one.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two or three or four or five.",
                    "label": 0
                },
                {
                    "sent": "This basically is information retrieval measure that is only taking into account the order of these guys.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Have you considered the report about stuff using maximum likelihood a different yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, not maximum silver like with yet, but we did Max margin on this other type of estimate which is not consistent but still maxilla like this.",
                    "label": 0
                },
                {
                    "sent": "Since it's consistently interesting at least.",
                    "label": 0
                },
                {
                    "sent": "I am not sure.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah yeah that's true.",
                    "label": 0
                },
                {
                    "sent": "I mean we, we should probably use that as a benchmark.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This application is really you have more information about.",
                    "label": 0
                },
                {
                    "sent": "Possible matches.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "Every.",
                    "label": 0
                },
                {
                    "sent": "Oh, you know I'm saying a complete in the sense that it doesn't need to be complete because you can define the edges.",
                    "label": 0
                },
                {
                    "sent": "You kind of other features as being 0 or something like that.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying complete just just to make it easier to do.",
                    "label": 0
                },
                {
                    "sent": "In principle, there's no features can be 0, so the word completes, not really.",
                    "label": 0
                },
                {
                    "sent": "Maybe miss lady.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe a bit misleading, yes.",
                    "label": 0
                },
                {
                    "sent": "So how do you spend to the case where the features on each image aren't the same number?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, that's easy.",
                    "label": 0
                },
                {
                    "sent": "I just I just wrote the basic model to make it simple to understand for the purpose of the talk.",
                    "label": 0
                },
                {
                    "sent": "But you can basically extend create dummy variables and then you define those weights as being zero.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is well well known tricks to do that.",
                    "label": 0
                },
                {
                    "sent": "What about if?",
                    "label": 0
                },
                {
                    "sent": "You wanted points to just not mentioning.",
                    "label": 0
                },
                {
                    "sent": "Is that it?",
                    "label": 0
                },
                {
                    "sent": "Well, it's possible, you know, I mean so if you have a smaller number of points on one side, then does your method force every point on the smaller side to mesh to something?",
                    "label": 0
                },
                {
                    "sent": "Oh well, you were going to.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically you have this parameter Theta that you know you're going to learn that parameter, but you can engineer your your features as you wish, right?",
                    "label": 0
                },
                {
                    "sent": "So if you engineer a feature of a given pair as being infinite coordinate, it doesn't matter what theater will be that that match will be forbidden.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about here, since it's a complete since you can use a complete bipartite graph, you can just engineer every edge as you wish according to their features.",
                    "label": 0
                },
                {
                    "sent": "If you have prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Speaker as well.",
                    "label": 0
                }
            ]
        }
    }
}