{
    "id": "hvocpxvrhubhpyktk6rt6hqlad7vmyub",
    "title": "Visual features II",
    "info": {
        "author": [
            "Roland Memisevic, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_memisevic_visual_features/",
    "segmentation": [
        [
            "I left off camera at the end of last lecture.",
            "As you might remember, so we looked at Fourier transforms on images and why they are relevant, probably to us and why neural Nets figure that out as well when they are trained and so on.",
            "Not really fully transforms more like a bar transforms which is fully transforms.",
            "Modulated by something like a Gaussian window which allows you to deal with.",
            "Engineering problems like leakage and so on and so curiously neural Nets figure that out as well.",
            "On a very high level, one could say that, well, a lot of this has to do with invariants, so if there's invariants structure in the data, which is a prerequisite for being able to learn anything, of course.",
            "Then these kind of features have to emerge becausw you're going to have to be able to kind of summarize things according to that invariant structure.",
            "So on a very high level, this talk is going to.",
            "Look at this.",
            "These kind of features from a different perspective, which you might call equivariance or something just more like we're going to lead these features not only if you want to be invariant to a certain types of transformation, but we're going to need these features also if we want to be able to represent transformations.",
            "So if you have something that moves around and you want to know how much did it move or something like that, then it's going to be the exact same features that are going to be required for that."
        ],
        [
            "So there has been a really long history.",
            "Of models looking at these kinds of ideas and problems, including Fourier transforms and stuff like that originally handcrafted and more recently learned.",
            "And almost all of them are motivated by the question how you might extend.",
            "Simple computer vision tasks, which are sort of solved now like object recognition.",
            "To more elaborate tasks that computer vision people like to think about like depth inference, motion inference and activity analysis, and stuff like that.",
            "So this is a very still, even though most of the models that I'm going to talk about in the beginning a bit outdated now because they're unsupervised.",
            "Outdated currently because the unsupervised and don't really beat any continent on any task.",
            "This is still relevant today because, well, object recognition you might argue solved, right?",
            "So this is from Alex Crisci's 2012 paper, but there are still a lot of tasks out there.",
            "Which are not solved."
        ],
        [
            "And so some of these tasks are things like geometric geometric inference, like figuring something out about this 3D structure of the world around you and stuff like that.",
            "Stereo vision is an example of that structure.",
            "For motion is another one, but also motion, understanding, activity recognition and videos and stuff.",
            "All of this requires understanding motion independent of the thing that is moving.",
            "So you want to know this motion happened and you don't care what the color of my shirt is and stuff like that, right?",
            "That allows you to represent things like.",
            "Activities.",
            "But there are lots of other things like optical flow tracking.",
            "Modeling relationships between objects in an image, for example, and visual odometry.",
            "Another task that I'm going to mention a little bit later and and maybe most of all analogy making, which is in really cool task that well we talked about in the context of NLP.",
            "Which means something like transferring a transformation from one object pair to some other object.",
            "And there are cognitive scientists who argue that analogy making is.",
            "The one thing that you're going to have to solve if you want to solve AI, maybe I'll get back to that a little bit later."
        ],
        [
            "So here's just a cartoon example.",
            "The standard example that one image might not be enough.",
            "To solve certain vision tasks, so stereo vision requires at least two images, and even if none of the images contains any information whatsoever, so it's just pure random dots.",
            "The pair of them with the random dots appropriately displaced.",
            "Might contain information, so this particular pair contains bar.",
            "If you squeeze kind of squeeze these images on top of each other by squinting and.",
            "Some people can do that when they see there's something in the center.",
            "So even if there's no structure whatsoever."
        ],
        [
            "It might be structure hidden in the relations with across pixels or across images even.",
            "Who has seen this video?",
            "Few of you.",
            "So apparently these are St bumps that are being used in Canada.",
            "Just I think a really bad idea, probably.",
            "Just.",
            "So you're laughing because you probably got fooled yourself the first time there, right?",
            "So I'm just going to play this one one more time.",
            "And I'm gonna use one of these.",
            "So if I stop somewhere here and take any any frame whatsoever, probably any reasonably image net trained content is going to say there is a person that's a girl they are in the ball or something like that.",
            "For every single frame, probably up to maybe when it gets distorted too much.",
            "But the fact that it's not, it's just encoded in the relation across frames, which allows you to infer motion an from that into infer 3D structure, and you see that the 3D structure is really boring.",
            "It's just descending kinda plain, and there's nothing else there, yes?",
            "Yeah, there are hints that you could use.",
            "That's right, that's right, there are some things that you could use.",
            "I would assume that a conflict on this image on this particular image here would get fooled on one of them.",
            "Not.",
            "Get that hint that shadow maybe but but there is a much much, much stronger Q which is structure that goes across the images of course."
        ],
        [
            "Sort of, yeah, that's right, that's right.",
            "Even though that's an adversarial example that's very different from the ones that he talked about.",
            "It gets at some very specific property and not.",
            "It's not so much.",
            "It's not at all caused by high dimensionality of the input space and stuff like that.",
            "So you can extend that further and further and further.",
            "Ultimately, maybe you need robots, of course to get.",
            "To solve AI becausw.",
            "You were able as a child to pick up objects like this by just picking this part and then lifting it up, and you saw that this piece of object deformed in certain ways, and then it has certain properties and that it falls on your foot.",
            "It doesn't hurt and all kinds of stuff.",
            "And you use videos of course.",
            "For that I mean your personal video going through the world, but you also use your activators for that and so on.",
            "And so this object is very different, has very different properties, very different texture and all that kind of stuff.",
            "And you feel that if you touch it and stuff, and so ultimately you need to go even beyond videos.",
            "But videos themselves probably already teach you much, much more than just isolated images, and in particular relationships across images, and maybe ultimately between images and videos and actuators, and the way the world reacts and so on.",
            "It's going to.",
            "Tell you even more."
        ],
        [
            "So as I said, this is a very there has been a very long research agenda.",
            "People trying to encode relationships to get at such tasks, little bit better.",
            "There are many ways to motivate why you should use certain types of model and not others, even though this view is being superseded a little bit by some more modern variants of it now.",
            "The main thing that people argued for was that you need multiplicative interactions and this is still somewhat true I think.",
            "So here is one way of to motivate why you need multiplicative interactions.",
            "If you have a neural network.",
            "Pair of layers like this and your two images for which you want to encode relations, are here encoded as the concatenation simply of the two images concatenated and you have features on top that learn about that, say the next layer in the neural net, or say a layer in an auto encoder or an IBM or any other kind of model.",
            "So as you know from our BMS for example.",
            "But you can kind of extend that argument across all kinds of models.",
            "If I condition on zed then everything here gets conditionally independent given zed and so that's a really weird assumption to make if you want that to encode relations between these.",
            "Obviously cause that cannot capture any such relations.",
            "If if I told you that these guys would be independent, so there wouldn't be related at all.",
            "So what you want is zed, which includes the relationship between 2 images and.",
            "So if I told you that and X for example, since you know the input image, say that that one and you know the transformation, then you should be able to know something more.",
            "Again, a few extra bits about why, but if they are independent, then knowing X and that doesn't tell you anything about why, so that doesn't match our idea of a random variable that includes relationships.",
            "And.",
            "Yeah, as I said, you can relate that to the IBM.",
            "Yeah yes yes please.",
            "Yes.",
            "Yes.",
            "Right?",
            "Right?",
            "Right, so higher level hidden units or something like that, that's right.",
            "Which.",
            "Right?",
            "That's sort of true.",
            "Yeah, and actually.",
            "Yeah, that's that.",
            "So that says basically a single hidden layer.",
            "Autoencoder cannot do that.",
            "OK, so that's just slightly well, somewhat weaker statement.",
            "You could even say even an autoencoder would still be fine.",
            "If you define the energy function appropriately.",
            "So this is sort of in an IBM.",
            "This is true.",
            "And in autoencoders typically, if these are sigmoid hidden units, you can show it's also true.",
            "But you could define an energy function for the auto encoder where this coupling kind of naturally happens.",
            "And I'll talk about that a little bit later, but yeah, so this is kind of true of two layer models only.",
            "That's true, so there's a way though to fix this two layer model even in itself.",
            "And so maybe I should also mention the reason why a lot of people who worked on this over the last 30 years still considered this, even though maybe higher layers might fix it.",
            "Is that they were also interested in local learning rules like heavy and updates on these parameters and stuff, and those typically matched very well.",
            "There's two layer kind of view rather than a deep network trend with back problem stuff.",
            "But yeah, back problems now around and everything kind of changes and a lot of theory is kind of outdated because of that this is somewhat true of this as well."
        ],
        [
            "So one simple way to fix this one layer model is to just make these into one click, right?",
            "So you want to have a clique in your probabilistic model that couples all of these three guys.",
            "Now they're not going to be.",
            "None of them is going to be independent.",
            "Of any other given the third guy in particular, these guys are not going to be independent.",
            "If I told you that so that matches now, our desire to capture relations in some sense.",
            "So that requires though a typical yes please.",
            "OK.",
            "Here, for example any.",
            "Any pixel, some pixel in the input image is connected to some pixel in the output image.",
            "At the same time, these both guys are connected to 1 hidden unit, so that means that these three are in one click and if you write down the probability distribution you see that these in dependencies don't hold, and so in particular, this particular unit said if it was on or something could encode something about X&Y.",
            "So if I told UXI and that K that would allow you to infer something about YJ so that in other words encode something about the relationship between this pixel and that mixer.",
            "In general, so this is just one.",
            "Way to fix?",
            "These three, these two pixels appear, but of course that means you want to throw in all kinds of three way connections that can relate all pixels that you might want to relate and stuff like that.",
            "This is just a kind of prototype view of the kinds of units that you would need for that."
        ],
        [
            "That is a very sensible thing to go for from the perspective of biology, probably.",
            "Here is a figure from an old paper by Bartlett Mail that just visualizes some dendritic trees and some neurons of some animal.",
            "And you see that's incredible amount of variety and extremely strong structure in the way that these neurons branch off to collect information from other neurons, and so the way that we always model these great variety.",
            "In basically any kind of model that we ever consider is W transpose X right, you basically compute a weighted summation.",
            "A weighted summation of the incoming.",
            "Neurons and then you have an activation function and so on.",
            "So he argued already back then, some 20 years ago that.",
            "What you called active neurons would be the right thing to go, which is sorry.",
            "Active dendrites would be great thing to go which is.",
            "Trees that do some stuff themselves that are not just wireless cables between neurons, but things that can do more elaborate computation, and so the most the simplest kind of thing you can possibly do to go beyond W transpose X is to include into the sum the way that some a little product of course.",
            "So as a scientist I mean maybe it's actually not a bad thing to make such a ridiculous.",
            "Approximation abstraction of reality because it actually did bring us very far and it captures a lot of the tensions that are there in the world, right?",
            "These networks work and so that taught us that distributed representations matter and learning matters and all kinds of stuff.",
            "So it's not bad to make extreme abstractions.",
            "But if you wanted to go one step away from this extreme abstraction, maybe it's."
        ],
        [
            "Putting in products would be just one way to do that, so that's what motivated a lot of people are looking at.",
            "These kind of things.",
            "Out of that came.",
            "Long research agenda called Bilinear Models, which I'm just going to motivate from the perspective of these manifolds.",
            "So typically you have a set of.",
            "Neurons and some layer that represent some image or something like that.",
            "Now if your goal is no longer to represent these images, but say the relationship between 2 images.",
            "Well, you can only represent that if there's some structure in the relationship between those images.",
            "So for example, if I gave you this X and say you apply the transformation, then they would kind of move this X around.",
            "And so if these transformations are not completely random, but somehow structured so that the.",
            "The X as I transform it is going to trace out something that you might want to call a manifold or in orbit, or subspace or something.",
            "Then maybe you might go after trying to identify what these subspaces are and learning about them.",
            "So now imagine you transform this X and travel along your orbit and now you swap in another X and that means you're going to travel around another orbit.",
            "But of course that orbit is going to be somewhat related to this orbit.",
            "If the transformation sequence is the same.",
            "So if I just rotate.",
            "Right, if I rotate this object, it's going to follow some manifold.",
            "If I rotate a slightly different images.",
            "Also going to follow some manifold, and presumably these manifolds are kind of parallel in somewhere there's some structure in which these things evolve and you might want to discover this.",
            "So the idea then is to just let's use completely standard feature learning model like an IBM.",
            "But let's just turn the parameters of the model into a function of the other image X.",
            "So we learn an auto encoder that encodes Y.",
            "But we just say let WJC is sitting in that model.",
            "Be a function of the other image, 'cause that should exactly capture.",
            "Kind of this intuition."
        ],
        [
            "And so that's what people did and what comes out is a model that looks like this.",
            "Which is kinda typically now called bilinear model.",
            "You have your image Y and you want to have, say in GBM or autoencoder that models that and now you just say OK, let's make every wait here a function of the other image.",
            "Every day WJK in that graph is going to be a WJC of X now.",
            "Right, and so if the simplest thing you can do is say it's a linear function, so every WJC should be a linear function of X.",
            "So what does it mean?",
            "A linear function of X?",
            "You have a whole bunch of values that you multiply X that the pixels in X with.",
            "And so that's the linear function.",
            "And now you have a model which is a function of X.",
            "Right now if I plug this into my model and I do inference, for example, I compute the value for one unit set here.",
            "Well, what is that is going to be?",
            "The weighted sum of the wise?",
            "Of course, that's the usual inference in GBM.",
            "The preactivation here at least now I plug in this form here that tells me it's supposed to be a function of X, and now I take away the brackets and.",
            "Some put together some sums and then I see that what I have to do is take all pairwise products between all these pixels.",
            "Have a wait for them which came from this linear function and then just summed it up right.",
            "Alright, so so that's a bilinear relationship here.",
            "Between X&Y, all pairwise products, weighted sum of them, and that's going to do inference for me, and so the same is true for the."
        ],
        [
            "Inferring the Y given zed and all kinds of other computations in this model.",
            "So a lot of people have been playing around with these models for many, many years.",
            "Some notable ones, maybe this year.",
            "Um?"
        ],
        [
            "I played with these models.",
            "Great deal during my PhD in Toronto and I kind of developed all kinds of variations like you can turn the bots in IBM into this kind of scheme into a bilinear PBM by just doing that.",
            "To this to the to the energy function, just replacing it with this 3 way thing here.",
            "The exact following, the exact same argumentation here.",
            "And then go through the math and then you get a 3 way IBM which I called gated Baltimore machine at that time and."
        ],
        [
            "You can also do that with an autoencoder or ICA or whatever model that you like the best.",
            "In fact, you can do that to any layer in a neural net if you want.",
            "Using the same mechanics."
        ],
        [
            "So as I said, this idea is not new at all and dates back as most things maybe.",
            "To a paper by at least 1981 two paper by Geoff Hinton who not only kind of came up with this idea of having 3 way interactions between triplets of neurons but also introduced this notation here that we now use all the time like this little triangle.",
            "So the handwritten at the time and.",
            "And and the reason he introduced it was also actually basically the same idea, same same thing.",
            "It's if you have features and you want to recognize things, but you want to be able to rotate these features around to make them match what you see and stuff like that.",
            "Then you need these control units which are able to take a feature and rotate it a little bit to make it match stuff like that Christopher Mods book is another one of the.",
            "Early believe us in this kind of scheme and he has also been pushing that since.",
            "Late 70s and then this is one of the first papers there.",
            "But there has been a ton of other work on that.",
            "As you can see here.",
            "Following that, I'm not going to go into detail.",
            "Just maybe mention some tensor product binding.",
            "Their tensor always pops up if you look at these kind of models because you have these.",
            "This matrix, this tensor that connects every.",
            "Unit in one vector with every unit and another vector with every unit in another vector.",
            "So you have a 3D grid of parameters.",
            "Naturally because of that.",
            "Turns on your Nets are just another instance iation of this.",
            "If you want the things that Chris for example, was talking about, Bruno 1000 is one of the people who has been pushing this quite a lot.",
            "Also, during his PhD he also looked at things that he called routing circuits, which is basically a way to route information around.",
            "In a brain or in a neural net.",
            "And stuff like that and.",
            "And then there's been."
        ],
        [
            "Gotta follow up work.",
            "One thing that I should still mention before I move on to some other things.",
            "Pushing this a little bit further is what we called factor gated Boltzmann machine at that time.",
            "Graham Taylor, I think mentioned that in his presentation and he also came.",
            "I studied this kind of thing in the same time as I did and.",
            "So the idea there is you have your 3D grid of parameters and that is an awful lot of parameters to learn, and so one way to fix that is to take your 3D block and apply what's called central tensor factorization, which just means that you take.",
            "This 3D block and represent it using an outer product or something of lower dimensional things like matrices, right?",
            "Imagine your matrix or matrix and matrix.",
            "Now if you cannot compute.",
            "Weighted sums in some way.",
            "It doesn't really matter in which way on all these matrices you can imagine a way to just fill out this tensor that way.",
            "Um?",
            "If you draw this in this autoencoder for example, then maybe it gets a bit clearer what's actually going to happen, and that's kind of important, because this is the first step towards eliminating those products altogether in the end.",
            "But what we left with then is something much cleaner and supply and so on.",
            "So what happens in there if you do this tensor factorization effectively so you replace WIJK using some product of matrices.",
            "What the model is going to look like is very similar as before, so you have two images, But what you're going to do is project onto some features using some matrix so you just do a matrix standard neural network layer.",
            "And then you do an elementwise product between triplets of units.",
            "Here are actually 2 units here to get the activation for some third unit here and then you have another neural net layer, and that was going to compute the hidden's for you, so that just falls out if you replace the 3D 10s or by matrices.",
            "And So what really happens inside this tensor factorization is that it takes the WIJK everyone multiplied with everyone else and replace it with some restricted product.",
            "That says I'm only going to multiply the ice guy with the ice guy to get the I TH output guy.",
            "And that in itself is not going to work because you want to relate pixels.",
            "You want to see structure across pixels, and So what you end up doing is first multiplying this matrix to project into another space and then using this restricted kind of scheme of multiplying.",
            "So that's what these models end up looking like.",
            "And the reason this is important is that.",
            "This is basically nothing other than saying you're doing a Fourier transform in order to perform your transformations.",
            "Or to infer the transformations, so this part would be inferring a transformation.",
            "Maybe the decoder would be.",
            "Applying a transformation.",
            "So why is that?",
            "Well, you project onto a bunch of features and then.",
            "Shifting say an image.",
            "By three pixels to the left.",
            "In the full year basis is nothing other than a phase shift.",
            "Of your free components and in complex notation, this is nothing other than an elementwise product with a phasor that's just going to rotate that projection a little bit.",
            "So projecting onto features followed by elementwise multiplication is essentially the same thing that a four year transform does, in order to perform transformations.",
            "So that wasn't clear at all in the beginning, and so we were really surprised to see that the model learns for."
        ],
        [
            "Transform so for example, if you train on pairs of random dot images that are shifted around just to the left to the right, up, down whatever and you look at the."
        ],
        [
            "Teach us that is these matrices here that come out.",
            "They're exactly a free transform and and that would be the feature on the input image X, and that would be the features on the output image, so therefore."
        ],
        [
            "Shifted versions of Fourier components.",
            "And.",
            "We were really pleased to see that 'cause you know these models were just no one had an idea what they would do and if that would work and stuff and then we started to play with this and suddenly saw.",
            "Well actually we didn't really suddenly see a few Transformers more small like you train your network for one week on your CPU back then.",
            "And just see random noise as you usually see in the beginning, your initialization and then after one week.",
            "It looked as though they were like little bass coming up a little bit maybe and we weren't really sure.",
            "So you train for another week and then you see that it's actually afraid transforming.",
            "I remember that I was running to Jeff into the office and showed him this and we were super excited and so we couldn't believe."
        ],
        [
            "How is it distinguished?",
            "Transforms and stuff now honestly on a GPU.",
            "This training.",
            "This takes about 5 seconds or so and you have your.",
            "So, but then you can say, OK, well, let's we transform, but what if I throw other transformations, edit and then it turns out, yeah?"
        ],
        [
            "And then it learns that.",
            "Generalized Fourier components of all these other kinds of transformation.",
            "So, for example, rotations have their own subspaces in which they happen, and the model figures out that if you train it on rotated images, then it's supposed to represent the images in these subspaces.",
            "I go a little bit.",
            "Deeper into the mathematics.",
            "So if you're a little bit confused right now, just bear with me for a little."
        ],
        [
            "You can do all kinds of other stuff like shifting around 1/2 of the image and then in dependently shifting around the other half of the image and so on."
        ],
        [
            "And the model figures out that it should devote part of a filter to this part of the image and other filters to model another part of the image, and so on, yes?"
        ],
        [
            "Yes.",
            "Yes.",
            "So in this case, not know in this case this wasn't with wrap around, this was with.",
            "I shift the image and that means stuff falls out here and random stuff comes in here.",
            "If you don't fill in with random stuff then you will always see kind of edges here or something because you can infer something from the fact that there's nothing here and then the model can kind of cheat and say there must have been a left shift because this is this line here is black and stuff, so we figured that out and then we started to just make this like crop from a very large image and move around and you're not subject to such problems.",
            "And then this is without wrap around.",
            "With wrap around it would just be easier to train becausw.",
            "The fact that stuff falls out as you shift just means that you get noise essentially right."
        ],
        [
            "Same thing here.",
            "So at the edge of the corners and stuff, so then."
        ],
        [
            "You can go further and just trainer natural videos and then you see that you still get this shifted components.",
            "But in that case they're really localized little for your components, which is couple of features which phase shift around from one frame to the next in the video.",
            "Oh yeah, so actually this figure here that happen.",
            "They happen to be organized in some way so you can play all kinds of tricks to make sure that nearby units here end up representing a similar frequency.",
            "Opposition.",
            "Which essentially amounts to just arranging them on a grid, say in 2D, and then constraining in some ways such that similar units get active together and stuff like that.",
            "That's a whole other interesting question, so you know that this is true in parts of the brain in some brains at least, and not others.",
            "And people have.",
            "This has been a spot color in this feature.",
            "Learning community rights like learning these typographic feature Maps.",
            "An interesting thing is that.",
            "All the models that are able to do this to get a Topa graphic or organization.",
            "This is not a great one, but they are much better.",
            "Once as you see in the literature, all of the models use either squaring nonlinearities or these kind of products that we have in these models.",
            "If you don't do that, you basically never get this topographic organization, even if you arrange your finger units in some way and stuff, and so you can actually explain why this has to emerge.",
            "If you have products or squares.",
            "And that has to do with the fact that this shifts the model wants to shift this around right?",
            "Because that's the data tells you to do this shifting around, but the model would never want to shift.",
            "This particular component to try to replace this component by something completely different that has completely different frequency orientation and so you want to pair things which have a phase shift but not.",
            "Don't do not differ in frequency and orientation and so in fact in the brain.",
            "It is also the case that you get this structure with respect to frequency and position and orientation of your Gabor features.",
            "But if you look at the face, if you if I would have a color plot that shows me the phases, that's entirely random.",
            "That's obvious completely.",
            "That would just be a noisy random plot, so there are some reasons behind this that we now understand pretty well.",
            "I don't want to go."
        ],
        [
            "Detail too much.",
            "About this so.",
            "So that's very pleasing.",
            "It's pleasing to see that the network learns free transforms and generalizations of that, and so we can go one step further and try to understand what these things are really doing mathematically and why they learn for you transforms and stuff.",
            "So I'm going to do that in the next three or four slides.",
            "Um?",
            "So the task that we should consider is you take 2 images X&Y and you want to infer from them the transformation that relates them.",
            "That would turn X into Y.",
            "So we assume that Y is some transform version of X.",
            "It's much easier mathematically if you assume orthogonal transformations L. Um?",
            "And in some sense, it's the only kind of transformation that really made us becausw.",
            "Anything that's not orthogonal is basically just going to increase or decrease the overall brightness or something like that, and so orthogonal transformations as I refer to in the end of flex.",
            "The last lecture are basically the kind of transformation that shifting around from one place in your MNIST digit to another, right that really move stuff around.",
            "Turns out any matrix can be decomposed any square matrix into a so-called symmetric and skew symmetric matrix and skew.",
            "Symmetric matrices are basically the.",
            "The general so code generator of orthogonal transformations.",
            "So basically we're going to want to focus on the orthogonal stuff on the moving stuff and not on the brightness stuff.",
            "Why is that still a very very large class of transformations?",
            "Because we're going to apply this transformation to vectorized images X, so X is going to be a stack of pixels.",
            "It's not going to be a 2D position of image, right?",
            "Just vectorize your image so you have a 784 dimensional Ms digit or something.",
            "And then also going to transformations in this 784 dimensions space are incredibly rich, right?",
            "So they can do basically anything to your digit.",
            "That's a huge class of transformations, couldn't care about more than that.",
            "In fact, any permutation matrix so shuffling the pixels around in any way you want.",
            "Would be an orthogonal transformations.",
            "That's how rich these."
        ],
        [
            "So mathematically, orthogonal transformations are good 'cause you can show that they.",
            "Have a peculiar eigen decomposition so you can actually always write any orthogonal matrix L. As a matrix, by doing appropriate transformation of your space.",
            "Which consists of nothing but two dimensional blocks which are two dimensional rotation matrices.",
            "Well, that's true of any high dimensional orthogonal matrix, so it's a block diagonal matrix.",
            "And if you look at it in the right basis, it's going to be a block diagonal matrix where every block is a two by two matrix and that two by two matrix does nothing other than rotate around in their two dimensional space.",
            "That's one interesting aspect of orthogonal transformations.",
            "Another mathematical property that's useful to know about is that.",
            "If you have a bunch of transformations that have this that sorry that commute, then they have the same eigen basis so this.",
            "Eigenvectors onto which you have to project to see that they just rotate so the eigen spaces in which this transformation is just going to spinning around.",
            "Are the same, and so translations commute.",
            "Becausw if I take this image and then shift it and then shift it again, that's the same as if I had taken that image.",
            "And shifted the 2nd way 1st and then the 1st way.",
            "So that proves that translations commute, and so every the whole set of translation matrices has the same eigen basis the same eigenvectors."
        ],
        [
            "And so I alluded to that.",
            "Also, in my last lecture shift.",
            "In one day at least can be represented by this kind of matrix, which is known as a circulant.",
            "So it's going to take a vector and just shift the components around, and so if I shift if I move this set of after this, the off diagonal, once here around 2 here or two here, just going to be different shift.",
            "But if you do like of this in Python you will see that you will every time for all of these different circulant matrices that you could try.",
            "You will always get the same eigenvectors, only the eigen values will be different.",
            "So the amount of rotation you're going to apply is going to differ.",
            "So this is talking about 1D, of course.",
            "Images can be still represented within this framework using something called block circulant matrices."
        ],
        [
            "So in a nutshell, if you take an image and you transform it by shifting it, so applying an orthogonal transformation like a kind of circulant, that's the same as taking that image.",
            "Rotating the space so that you represented in the eigen basis of this matrix and then just spinning around.",
            "In those two D subspaces and then projecting back right?",
            "So equivalently in in Python, you can either go at times X to apply a shift, or you could just do rotation, rotation, rotation, rotation, rotation, bunch of times in the appropriate directions, and that would also shift the image.",
            "And in fact, not only shift the image, but permute the pixels in any way you wish."
        ],
        [
            "So to get back to the question given to images X&Y.",
            "You would like to have a mechanism that takes these two images and tells you what the transformation is.",
            "If you had a training set of transformations, and those transformations are commuted.",
            "Say they were all translations, then this fact here tells us what we have."
        ],
        [
            "Do all we have to do is project onto the this these eigenspaces.",
            "Each of which is actually a 2D space, right?",
            "You guys going to do 2D rotations, so these will come in pairs.",
            "So you have to tip, take your ex input image projected onto the eigen spaces you have to take your second image projected onto the eigen spaces and then if you don't know which of the transformations you're dealing with, all you have to do is compute the angle of rotation by how much did you rotate in that eigenspace, and that's going to identify uniquely which transformation you're dealing with cause.",
            "Thanks to commutativity, the only way that any two translations differ is by the amount of rotation that they're going to apply in those subspaces.",
            "And so how do you do that in practice while you take an image?",
            "We project it onto those components.",
            "How do you compute a 2D in our product which would give you the angle between those two 2D vectors in that space?",
            "Well into the inner product is just this component times that component plus this component times that component right?",
            "That's how you computed 2D inner product between two vectors in 2D.",
            "X1 times Y 1 + X two times Y 2.",
            "So you compute this elementwise product, and then sum them up.",
            "And that's how you would go about doing this.",
            "And so what's interesting is that this is exactly the graph that I drew here was exactly affected by linear model, the way that we introduced them without knowing anything about eigenspaces and transformations in commuting matrices and stuff like that."
        ],
        [
            "So this is how you would do that anyway.",
            "There's one subtle difference, which actually is kind of important.",
            "We don't actually have some.",
            "Here we have a matrix here that's learned, so this."
        ],
        [
            "Fully connected matrix that's just learned and the whole network is just learned from the data, but it figures out that the features it should use pairwise fully components which are just phase shifted appropriately.",
            "Um?",
            "OK, any questions about that?",
            "Yes.",
            "That's right.",
            "That's right, so this is kind of the neural network way of writing down an algebraic expression so.",
            "So basically this is the X is a vector vectorized image.",
            "This just means do matrix multiplication.",
            "And the same here to get these.",
            "So what we called factors at that time and then this circle is just a dimension typically circles when you're on your own.",
            "It's a circle means a dimension in a vector.",
            "That's the semantics of that right?",
            "And so then this little triangle connection here just means that this guy gets its value by multiplying this guy and this guy together.",
            "And accordingly, this guy multiplies this and that and so on.",
            "So to make this analogy work between the fact that models and this system that you would engineer what you would have to put here if you would engineer this system, what you would put here is some eigenvectors.",
            "Say you are you T whatever right?",
            "And what you should put here is not the eigenvectors, but what you should use in order to figure out whether there was a rotation happening.",
            "Is the phase shifted version of that vector of Eigen?",
            "Sorry matrix of eigenvectors.",
            "So you should put eigenvectors.",
            "Here you should put phase shifted eigenvectors here and then you should compute an inner product and if this gives you a large value then the shift by which the X&Y are related exactly matches the shift that these.",
            "Matrices by which these matrices are related.",
            "So this is a subtlety that comes then later into play when you think about that these aren't actually sigmoid units, and they actually are just going to detect whether a certain rotation actually happened or not.",
            "And stuff like that.",
            "Um?",
            "Yeah, I'm just going to leave it a little bit loose this analogy because I don't think it's that important, maybe anymore.",
            "Yes.",
            "Yeah.",
            "So if you had only one image pair.",
            "An image and it's shifted version or many image pairs which were shifted in exactly the same way, so you had only one transformation.",
            "Then you could do something like an SVD and figure out what the transformation is.",
            "Canonical correlations analysis.",
            "CCA is also a way of getting at that it's sort of a way to analyze a transformation L between 2:00."
        ],
        [
            "Two vectors.",
            "But that's not what we're here.",
            "What we're after here?",
            "What we're after here is assuming that there's a whole class of transformations, say any shift, and I don't know which one and what I want to do is figure out which shift I'm actually dealing with.",
            "And so you could extend CCA to something like a mixture of CCA's or something like that.",
            "And that's essentially what these things are."
        ],
        [
            "So you need a mixture model at the end of the day.",
            "Does that answer your question?",
            "Yes.",
            "Yeah, so actually maybe I'm going to."
        ],
        [
            "I'm going to go over that very quick.",
            "So there's an interesting problem in vision that you might heard about.",
            "Might have heard about.",
            "It's called the aperture problem.",
            "Who knows who has heard about what the aperture problem is?",
            "Some, not many.",
            "So actually this view from the perspective of orthogonal transformations allows you to kind of understand what the aperture problem is in a much more concise and in my opinion simple simpler way then."
        ],
        [
            "The way that vision people usually think about this.",
            "Say you want to detect the transformation, which is a shift as you see on the very left, a shift by a certain number of pixels to the left from a position to the left, right, just a global left shift.",
            "What is this shift going to express itself as in those subspaces in which we can detect it?",
            "Well, in this subspace spanned by these two failure components.",
            "So in this one subspace.",
            "It's just going to be a rotation cause everything is just rotations and so you can think of it as going from this position here to this."
        ],
        [
            "Position here you're just.",
            "You're just going to rotate around as you apply this shift there, going to be a whole bunch of other subspaces like this one here."
        ],
        [
            "Which also your matrix U in which you won't see anything actually becausw.",
            "Maybe your image doesn't have any components in that subspace, right?",
            "In particular, this is."
        ],
        [
            "True of this image, it doesn't have any horizontal structure.",
            "And so these two components won't respond if you compute the inner product with this.",
            "And so you won't see anything at all.",
            "Now take another image like the superposition of two bars.",
            "Then the overall energy, say, is normalized, so the whole the overall intensity of this has to go down accordingly.",
            "So this is going to be less represented in the subspace in which it can detect the left right shift.",
            "But it's this image is going to have some energy in this subspace, and so if you now want to see what happens in each of the subspaces, you just wiggle back in force and you see uh-huh it's going to phase shift here.",
            "And it's just doing nothing here, right?",
            "So you get your signature of.",
            "Rotation angles that you need to identify the transformations.",
            "You can collect it from your subspaces, but unfortunately you cannot collect it from all subspaces, because if your image."
        ],
        [
            "Doesn't have any energy in some of them.",
            "Then you're screwed, then you won't see anything if you just see zeros.",
            "And so maybe there was actually an upshift.",
            "Which these guys would detect, but unfortunately I'm not able to see it because I don't have any energy in that subspace.",
            "And in fact in this image here.",
            "You won't be able to tell the difference between a bar that moves between left and right and the bar that moves to the left and at the same time up.",
            "Actually, you won't be able to tell, and the reason is exactly that there is a component which is just not represented in that image."
        ],
        [
            "So, but I go once."
        ],
        [
            "Step further and I just look just at this image and I keep applying my same transformation right?",
            "I shifted left to right.",
            "Then it turns out this image doesn't change at all, so you don't detect the fact that there is a left right shift.",
            "So why is that?",
            "Because this component in this for this image is not detectable and so on.",
            "The fact that there can be emotion, say a left shift of this image that you cannot see, that's called aperture problem in vision and it's called aperture problem because you're looking at this image only through a finite window, and if there is structure beyond this window that you cannot see if you had a larger window eventually.",
            "There are no infinitely long bars in the world, so eventually this bar would end, and that would immediately give you some free component in this space, right?",
            "If this by ends at some point you have something vertical, and so it's now you would be able to detect the motion.",
            "So so you could re late the problem to the fact that you have a finite window, but it's a bit of a strange way to name this problem.",
            "It's basically a subspace motion."
        ],
        [
            "Detection problem that exists.",
            "So do you want to repeat your question I think."
        ],
        [
            "Oh yeah, OK, so OK, right?",
            "So?",
            "So because of that it might not be a good idea to use as a car."
        ],
        [
            "Good for your transformation, these two D. Detections this panel, this number of hidden units here, which might detect some motion or not, might be better to pull across multiple subspaces to get a code which is always going to be the same for any left shift, no matter which components are present or not.",
            "You know what I mean?",
            "So in practice, it's much better to pull across me."
        ],
        [
            "The subspaces, rather than leaving them alone here.",
            "So this so no, it's not going to."
        ],
        [
            "Of that problem, and in fact you could say this.",
            "I mean, you could say there is no left motion here, right?",
            "It's you could say there is, or there isn't.",
            "It's sort of meaning a meaningless statement to make.",
            "What this illustration doesn't doesn't illustrate is that there can be fully components with other frequencies.",
            "For example, right, so I could have a twice S. Fast wave here and maybe I have an image which has that fast for your component but not this particular one and so then you could actually from that other full company could.",
            "You could still figure out that there was a left shift.",
            "But if you have this image, you would see the left shift popping up in one component.",
            "If you had a higher frequency image, you would see it pop up in another component, and if you just want a hidden unit that just says there was a left shift or not, then you should pull across all of them.",
            "Yeah."
        ],
        [
            "Any other questions?",
            "OK, so for a long time."
        ],
        [
            "Him people were really excited about these models.",
            "They were state of the art and activity analysis for some time.",
            "Graham mentioned that's actually his paper had a convolutional version of such a model.",
            "And and then a little bit later, quickly and others introduced a model that they called hierarchical ISA, which was also state of the art for quite some time.",
            "An interesting observation is that you can."
        ],
        [
            "Make is that these models are equivalent to these ISA models.",
            "If you take a video.",
            "Rather than a single image, so a whole long video and then plug in the same video left and right here.",
            "And the reason is that computing the product of two components is the same as squaring that component.",
            "If they are the same.",
            "Well, if you also set the parameters here to be the same and then summing over squares is."
        ],
        [
            "It turns out, is what Isa is doing this model here.",
            "And the theory then can also explain why both these skating models as well as the squaring models, also called square pulling models, are both able to represent the motion in videos.",
            "Essentially because the square of a sum is contains the products by binomial identity and then you can show that hidden units that respond to a shift are also going to respond to a square and stuff.",
            "But it's kind of technical that it's not so interesting anymore, yes.",
            "Yes, plugging this."
        ],
        [
            "Which is.",
            "No no no no no no.",
            "Just the exact same XO X = Y.",
            "Just plugging in the same thing here and here, which ultimately comes down to just having a model that computes features and those features get squared.",
            "And then you do pooling another other stuff.",
            "So it's basically a model with a squaring nonlinearity as the transfer function in neuron with that computer square basically.",
            "You can, you can motivate this square in other ways because it gives you invariants and stuff like that, but."
        ],
        [
            "It is.",
            "It is not exactly the same, but what is going to be the same is the tuning of the."
        ],
        [
            "Iran, so the stimulus here.",
            "The video that's going to maximally activate one unit here.",
            "It's going to be the same in across those kinds of model across squaring and products, but that's.",
            "Right?",
            "Right, right?",
            "OK, alright, OK, so there's an intermediate step.",
            "So instead of taking this XY thing you can take.",
            "XY concatenated and XY concatenated and then computer square.",
            "And that's that I'm claiming is the same as this.",
            "Right now, if you take 10 frames at a time, then that's still going to be the same as if you had taken all parallel path of by linear models or something."
        ],
        [
            "So, but people have moved on since then, right?",
            "This is unsupervised learning.",
            "Essentially, it's learning features that represent motions and stuff.",
            "And also Graham talked about that as well.",
            "So if you just take a huge continent and train it on a million videos and stuff, you typically outperform anything that.",
            "Anyone has ever done and so these kind of models have gone a little bit out of favor because of that?",
            "I don't think that means that you shouldn't be looking at these models and that we should kind of give up on them and just train.",
            "Big Nets using back prop to solve everything.",
            "I think there's an interest in understanding what models do.",
            "But as you also mentioned, unsupervised learning is currently not used in any company or in any practical.",
            "Situation.",
            "Right, OK, fair enough.",
            "Even though.",
            "It shouldn't, probably.",
            "It is, it is, but you can also just replace that word embedding model and just train a translation network end to end to solve their task and don't go through intermediate embeddings and then you're back in back problem.",
            "Right, right, right, right, right, right?",
            "So actually yeah, so there is actually a situation in which unsupervised learning is still valuable and probably even used in companies.",
            "And that is in this situation where you do not have tons of labeled data.",
            "And in that setup these models still are."
        ],
        [
            "Currently still the state of the art in various tasks are the only things that people have even tried so far.",
            "There so the applications maybe that you could derive from this, like for example getting invariants from videos.",
            "That's also not a supervised kind of problem.",
            "You can show that if you have videos, you naturally get invariants to the transformations that you see in the video, and that false.",
            "Basically out of this analysis you get these rotation angles and encoding.",
            "Those rotation angles doesn't depend on the actual phase of every stimulus.",
            "So if I see an angle like this.",
            "Or another image pair or whatever is an angle like this.",
            "I can figure out what the transformation is, so that's this angle, but it doesn't matter what the initial phase of the image was, and so invariants comes for free.",
            "Essentially from these models.",
            "Another way of saying that is that the transformation between 2 images is sort of the derivative of the position, and derivatives do not depend on the actual position or something, and so there are various papers that played with that a little bit and then.",
            "Kishor Conda, PhD student, played with various variations of this model, most notably a variation that drops this top layer that does the pooling altogether and justice sort of a single layer network.",
            "Graham talked about this and explained why Synchrony is one way to think about these models, and so he had some success on things like depth inference and and he replied that a visual odometry, which is also really cool task.",
            "Really important in robotics and automotive and stuff.",
            "From a video you want to know how far you were driving something like this, right?",
            "So you want to infer your ego motion from the video.",
            "I take one step forward, then all the pixels in my visual field are going to move around a little bit.",
            "The question is from that.",
            "Just that video of pixels moving this far, can I infer?",
            "How far will I walked on things like that?",
            "So what he did in that work was that he pre trained a consonant using these kinds of feature.",
            "So that way that model could infer depth and motion and stuff and then solve the odometry task.",
            "Had we more data, which we don't, but if he maybe had 100 times as much data for doing odometry, which no one I think has maybe just training a continent would have been sufficient.",
            "Yes.",
            "You should talk to Kishor.",
            "He's probably somewhere here.",
            "Yeah there is.",
            "There are lots of subtleties like that that come up, and so you also, you haven't even have to define what you want to do.",
            "Do you want to dictate rotations and more forward motion separately and stuff like that?",
            "And how do you want to represent that and?",
            "I think the that question can probably be answered by the training data that the small training set that exists.",
            "This, the Kitty database which says.",
            "Which kind of prescribes which units you should be using?",
            "But yeah, there are lots of subtleties that can come into that."
        ],
        [
            "So so are there any questions about this so far?"
        ],
        [
            "Yes.",
            "Yes.",
            "Yes, that's right.",
            "Yeah, that's true I guess.",
            "Probably yes.",
            "A fully 3 way model with this parameter 10s or with all those numbers would have a ton of capacity.",
            "The factored model, the one that does it first, the projection, and then elementwise product.",
            "So the full year model, basically.",
            "Doesn't have as much capacity.",
            "So that's one way to reduce that.",
            "But in practice, as these filters show and so on, it just learns happily learns the transformations from the data, and that's also not something that's maybe strange.",
            "That's something that we found all the way through the neural network literature, right?",
            "You scale up your model like crazy.",
            "And you train it on a lot of data and then it's fine.",
            "You cannot overparameterized neural net, almost just doesn't happen right, at least if you have a lot of data available unless and we have a lot of data, we have unlabeled videos.",
            "More than we could possibly want to have.",
            "What we do not have is labeled data, but unlabeled data like videos from which we can infer rotations, transformations and stuff.",
            "It's just infinite infinite amounts.",
            "Any other questions?",
            "So."
        ],
        [
            "So, um.",
            "So for the remainder of the talk, I'm going to talk about some more modern perspective onto this.",
            "I'm going to talk about two things that are somewhat.",
            "Come up from this view of Fourier basis and orthogonal transformations and stuff like that.",
            "Both of these are related to the vanishing gradient problem that people already talked about before, which comes up in deep Nets, but also in recurrent Nets so.",
            "So what does it mean?",
            "It just means if you have a deep net, say in recurrent net that goes through many time steps.",
            "And the backward pass, at least you're going to multiply matrix times matrix times matrix times matrix.",
            "And if the eigenvalues of that matrix are not nice.",
            "Things are going to decay exponentially or blow up exponentially and.",
            "You have a problem if they decay exponentially, that means the derivatives that arrive here are basically zero, and so you're not going to be able to learn anything over here anymore.",
            "If they explored, it's sort of the same problem, it means you're going to have to use a smaller learning rate, and so.",
            "Then again, the learning rate might be too small to be able to learn something here or somewhere else.",
            "Even then, that would be even here.",
            "I guess.",
            "As has been suggested and discussed in the summer school before, you could use orthogonal layers.",
            "Try to make this lower layers orthogonal or initialize them using orthogonal matrices and stuff.",
            "Um?",
            "Why would that fix it?",
            "Well?",
            "We know now from this analysis before what an orthogonal matrix is.",
            "Its eigenvalues have absolute value, one right?",
            "So it's eigenvalues aren't one.",
            "Its eigenvalues have absolute value one.",
            "They can still spin around the unit circle.",
            "There just never going to decay towards 0R explode.",
            "So the eigenvalues in fact in particular are complex numbers.",
            "And if you take any random matrix with which you will probably ever initializing neural net here and you do, I cough that matrix in Python you will see that you get real and imaginary values, so these are naturally complex, right?",
            "That's something to keep in mind when you think about these orthogonal layers in neural Nets and stuff like that.",
            "So they're going to spin around a lot.",
            "And why do they do that?",
            "Well, by the are they still going to do that after training?",
            "Well, 'cause you want to shift weights around, right?",
            "You want to left this hidden unit, activate that hidden unit and stuff you want to do this kind of moving around of ink inside the network all the time.",
            "So also discussed has been this identity initialization, which also helps.",
            "So you just initialize the identity matrix and, but that's kind of a really strange thing to do and.",
            "Maybe I'll."
        ],
        [
            "Stick to that a little bit later, so intuitively sine waves and fully components are great to understand what this orthogonal transformation means.",
            "Because the sine wave is kind of well, it's generated by your free component, it's just a phase are spinning around forever, and so it would be really easy to take the beginning of a few isava free component of a sine wave and infer what it's frequency is in phases, and then continue that sine wave.",
            "In fact, a sine wave is kind of the kind of signal that doesn't contain any information besides frequency and phase, right?",
            "It just does the same thing again and again and again and just never stops.",
            "And so even though in a sine wave that extremely strong correlations between the value at 398 and the value that."
        ],
        [
            "You saw here in the beginning, maybe?",
            "It doesn't introduce any new information along the way.",
            "Something so sine waves exhibit extremely strong long range structure like you wish you were heading an SDM or something.",
            "Um?"
        ],
        [
            "Interestingly, this little game of taking the beginning of a sine wave and continuing it forever, which we would want a neural net to be able to do, I think it's independent of phase.",
            "And the reason is that, well, it doesn't matter where you start.",
            "If I tell you the beginning of the sine wave and I ask you to continue it, all that you have to do is take the last value in the 2D subspace in which this guy is just spinning.",
            "And then you know say which phase are you dealing with.",
            "You know which component dealing with, so you just multiply by E to the I5.",
            "And keep doing that, and so you're going to spin forever, right?",
            "It doesn't matter where you start, so it doesn't matter if you start here or here.",
            "You can just keep spinning around and generate the rest of the same way forever easily.",
            "So there will be an easy thing to do.",
            "Trivial thing to do, a trivial task to go for using a neural net.",
            "Well, if that neural net would have components that are able to spend these two subspaces right, so those components should be able to do this spinning operation.",
            "And so that precludes already something like a sigmoid unit.",
            "You need something like a relevant, because otherwise you're not going to be able to infer this angle to play with these three values, as you cannot travel through the network.",
            "There was a really nice paper by Matthias Bittker and coauthors in 2007 where they called this.",
            "Well.",
            "Basically, the fact that you can.",
            "Look at it at any phase.",
            "Their coded, steerable simply and they'll try to learn features that are steerable in that sense, and I'm going to go back to that in a few minutes, I think."
        ],
        [
            "Um?",
            "So how does an STM solve the long range structure problem?",
            "Here is an Atlas TM unit.",
            "Well, the core of the STM is just.",
            "A neuron connected to itself with a one.",
            "And so as you go through time this well, you will just stay there and we won't do anything.",
            "That in itself is completely useless computation.",
            "It's like there's no way, and there's no reason to do this right.",
            "There's no reason to take a value and just leave it there is.",
            "It doesn't do any computation.",
            "Nothing.",
            "The reason it becomes useful, of course, and it's the M is big 'cause there's a whole lot of logic around that one component which can shovel things into it.",
            "Read things out of it, and kind of control it in a way such that it becomes useful to keep around the value.",
            "But the other units, like the gated recurrent unit, which has some other products and greetings and stuff.",
            "Around such a unit and works just as well on some tasks may be better on some may be worse on some, but it's essentially also just a useless component surrounded by control logic, which makes that useless component useful."
        ],
        [
            "Um?",
            "So going back to the sine wave example, imagine I give you the beginning of a sine wave.",
            "I want you to continue it forever.",
            "That would be easy to do.",
            "You need a neural net that has maybe relative units or linear units even, and an appropriately designed matrix that just spins at the component, that's it.",
            "Well, if I change the frequency that won't work anymore.",
            "You would have another, you would need another.",
            "Piece of network that can spin around that faster signal accordingly, right?",
            "So what you could do though is take a mixture model like a mixture of experts or something that goes from one time frame to the next and that mixture component should just be told what's the frequency.",
            "We don't need phase.",
            "We already covering all phases because we know now that spinning around is just the same.",
            "So we just have to tell it which frequency we're dealing with, and that network could then a simple mixture model could then.",
            "Continue sine waves for you forever."
        ],
        [
            "So.",
            "Vincent Michalski was also somewhere here I think did play with a model that can do that in his master thesis, and we call that Grandma cells because we made that hierarchical as I'm going to show in a second.",
            "The way that this model works is that it takes 2 frames in a video, for example, and it assumes that these two frames tell you everything about that video, and then it assumes that the transformation that in Ferd are going to be constant across time forever.",
            "And so if you make that assumption, you will be able to generate the rest of the video.",
            "Ad infinitum, you can just do the same thing again and again and again, so that's not interesting for real videos.",
            "Becausw no transformation just gets repeated again and again.",
            "That wouldn't give you an interesting video for sign."
        ],
        [
            "If it works though, for one thing.",
            "So if you show it to the beginning of a sine wave and then you ask it, please continue that sine wave that you infer.",
            "Then it figures out the phase.",
            "Sorry, that doesn't need to fail.",
            "It figures out the frequency and it just continues that sine wave and numerically."
        ],
        [
            "Things would maybe go unstable, but thanks to the way that this model is kind of doing its job.",
            "It."
        ],
        [
            "Can just go on and on and on and on and it will never never blow up and so you."
        ],
        [
            "Just generate signed book forever and so notice that between this component here at step 10,000.",
            "And the component that it actually inferred from the seat that it got is extremely strong structural correlation, right?",
            "It has to set the exact right value at the exact right position here.",
            "So this is a really really long range structure kind of problem if you like cause.",
            "This value here determines that value here.",
            "But of course it can solve it because it just carries forward a single computation which doesn't do anything fundamentally.",
            "As I said, it's just a sine wave, right?",
            "So it can sort of like on a letter carry across and do the right kind of transformation such that it will arrive here, giving you the right value at the right moment in time that it has two in LCM is exactly the same thing in exactly the same way, it just say puts a value there and keeps it there until it needs it, so it carries it forward using A1 without doing anything, yes?",
            "So you do, and actually this model doesn't actually."
        ],
        [
            "That I'm showing he doesn't actually do what it's supposed to do, and so this is not the plot generated from this type of model."
        ],
        [
            "We as I said, because this model gram ourselves because we made it hierarchical and the way we made it hierarchical is by saying.",
            "Well, a single transformation applied again and again is boring, so meh."
        ],
        [
            "We can do it."
        ],
        [
            "Slightly more, better slightly better model by assuming that the transformations can change themselves, right?",
            "So now you can do deep learning 101.",
            "You take your model in further transformation from 2 images and then you have further next transformation from two other images.",
            "Now you get 2 transformations that you infer and now you can just stack the same model on top and look at how the transformations change overtime, and that allows you now to change dynamically to model things.",
            "Say sine waves whose frequencies say change overtime or videos where some motion is followed by another motion in some regular way and stuff."
        ],
        [
            "Numerical problems that might occur here.",
            "Just because you have finite precision that might cause a blow up, even though you just multiply by E to the IFI.",
            "Fall another category and it turns out that this tool."
        ],
        [
            "Your model is Abe."
        ],
        [
            "To generate the sign."
        ],
        [
            "Yes forever."
        ],
        [
            "So this is a little bit more control logic.",
            "If you want right?",
            "So this is just a mixture of experts with another layer of mixture components, that's it, and so it turns out that.",
            "Piece of control logic up here is enough to generate a sine wave forever.",
            "Because presumably what happens is if there is something like a blow up thanks to an American precision.",
            "We gave it enough training sequence so that it can sense those things.",
            "The kinds of things that might happen on that particular machine or whatnot, and so it was trained to generate long enough to cope with that."
        ],
        [
            "And that was maybe."
        ],
        [
            "Like this, this is maybe the lengths that we train it for, but this is a test.",
            "Wave, incidentally, and so on.",
            "But this is maybe the length of sequence that we trained in foreign."
        ],
        [
            "Then it can go on forever becausw it won't see any numerical issues beyond the ones that it's on the training data."
        ],
        [
            "So I'm."
        ],
        [
            "Run."
        ],
        [
            "A little bit of time."
        ],
        [
            "So just to mention, we did all kinds of variations of that you can actually go in and crazy and just take more and more and so go from velocity to acceleration and then from acceleration to what's called SNAP from snap to add.",
            "Sorry, jerk and from jerk to snap and crackle and pop.",
            "This is what these harder derivatives are called in physics and fun models.",
            "That kind of learn hierarchical temporal structure in various ways and so on."
        ],
        [
            "That works nicely on."
        ],
        [
            "Videos that can learn shops, which is kind of sine waves that changes as frequency, which you would hope it can."
        ],
        [
            "And it can learn very."
        ],
        [
            "Other things like harmonics and.",
            "Videos."
        ],
        [
            "To some degree."
        ],
        [
            "I was."
        ],
        [
            "Really excited to see that it was able to learn to nail the bouncing balls task because I wasted three months of my time.",
            "As a professor.",
            "Trying to get this to work.",
            "And then I got it to work with the help of the two students.",
            "Kishor and Vincent were also involved in this, and so I was really excited.",
            "It works, it can.",
            "Model videos that are throw balls that are bouncing around in the box.",
            "Exciting enough for a scientist I guess.",
            "It was exciting because no one else has been had been able at that point to do that.",
            "Shortly after I was able to get these videos generated and it generates no problem.",
            "It doesn't matter how many balls and how long in things that hasn't seen before and stuff.",
            "But then June younger student here in the lab just joined in STM on the same task and.",
            "I guess you can imagine what happened.",
            "So it was disappointing to see that LCMS work as well.",
            "As it stands, it's LST MSN grammar cells that are able to solve this task and.",
            "My take home message from this is that.",
            "What you need to get this?"
        ],
        [
            "Task solved is basically potentially useless computation.",
            "Like a rotation or an STM, nothing computation.",
            "Surrounded by some control logic which can turn that useless thing into something useful, and so so I'm at least happy about that that we at least got this to work and get sort of head."
        ],
        [
            "At this time there was one question there."
        ],
        [
            "Oops.",
            "Yeah, probably there is.",
            "We talked about that a little bit there.",
            "There might be only a finite set of such videos actually, unfortunately becausw.",
            "Well, because of the periodicity that appear here and stuff like that right?",
            "We don't know how many.",
            "How many of these videos really exist in reality?",
            "Maybe not so many.",
            "Maybe you can just memorize all of them or something.",
            "So at the end of the day, I think this is probably not a very interesting data set because of that and.",
            "Yeah, because of that mainly yes.",
            "Well, you're up to numerical precision, but then things that are really similar can be summarized by a model like LCM, at least.",
            "Nicely, it can just.",
            "You know it can.",
            "If it takes this image, it knows what it has to do next.",
            "If it takes like take any snapshot, it knows what it has to do next.",
            "If it sees a slightly different snapshot, maybe it's contract, if enough so it's going to snap to the same thing, so there are whole bunch of nearby videos frames at time T that get mapped to the same frame at time T + 1, and then you're back to the tragic Tory that the model learns.",
            "And then you're kind of.",
            "You can solve this qualitatively what we should be doing, I guess, is study this quantitatively, because what you see?",
            "Is after sometime this video diverges from the test video that you see did it with.",
            "And the amount of divergences going to tell you how well it can really model this exact test video that you're dealing with, right?",
            "So I saw that at least for the LCM and and so you see this beginning and then if you see them side by side then after like 10 seconds they're just going to look completely different, right?",
            "So it learns kind of it knows what it has to do, but it doesn't really model the exact test case.",
            "Yes.",
            "Yes, yes, yes, that's correct.",
            "That's correct, yeah.",
            "That's right.",
            "Outside right?",
            "But making that statement that basically so basically what you say is used on the manifold of those videos.",
            "You don't want to be contract if you want to be orthogonal in some sense, you just want to move shift around the pixels as the class of videos does.",
            "That basically, I would argue asks for.",
            "Something like."
        ],
        [
            "This you want to be able to model any orthogonal transformations between 2 frames or hidden states or whatever.",
            "And two stuff accordingly rather than contracting.",
            "And I my hunch is that in a lot of neural Nets you do not see those orthogonal transformations.",
            "You rather contract and learn things by heart, where what you really do should be doing is transforms like travel around manifold and stuff like that.",
            "So the last thing I was going to talk about goes a little bit in that direction.",
            "I don't have a lot of time left.",
            "10 minutes I'm going to just give a summary of that.",
            "It's another thing that just falls out of this full year.",
            "Kind of you are just to mention you can extend this grammar cell idea.",
            "And change the logic that's happening here a little bit together, things like.",
            "Memory units and stuff like that very easily, but you make some of the units on which it operates hidden, and now the matrix that operates on these that also going to transformation can shovel stuff from the say visible pixels into the hidden's and it can shovel stuff out of those hidden's into the video and it can have things happening in those hidden's, and so it's fairly easy to just take this general idea of a mixture model if you want and turn it into something like little.",
            "Call compute circuit that has a little bit of memory and stuff like that.",
            "None of this is explored.",
            "We played with this a little."
        ],
        [
            "And so that the model can kind of continue things in its mind and stuff like that."
        ],
        [
            "But maybe I'll talk about that some other time.",
            "So the last thing I want to talk about that comes out of this phooey view is again related to vanishing gradients and stuff.",
            "So."
        ],
        [
            "I said there are some approaches.",
            "To deal with these problems.",
            "Saying that you want to have this matrix being orthogonal matrix in itself is a strange thing to want 'cause you don't want the whole transformation from the whole hidden state to the next hidden state be a global big orthogonal transformation, right?",
            "What you really want is little orthogonal transformations that can do things in some parts of the hidden state.",
            "Maybe, and in particular if you if you run such a model, you will see that many of these units are actually zero 'cause the sparsity pattern of any reasonably trained neural net is very very.",
            "Hi yeah, so you have a lot of debt units per training case, so typically only a few units here are going to be active and you don't want to global orthogonal transformation operating on this vector in which there are many zeros on.",
            "So on what you really want is having those components orthogonal which are actually active together often right?",
            "So what you want is orthogonal paths a through the network and not just distinct big orthogonal transformation that just helps you from gradient blowups."
        ],
        [
            "So here's the 2D subspace from the full from the Gabor components once again.",
            "That's right, that's right."
        ],
        [
            "That's right.",
            "Right, right?",
            "Right, that's sort of an orthogonal reason why you do not want an orthogonal matrix globally on this state.",
            "On the hidden state, operating on the in state."
        ],
        [
            "So here is a component which is spanned by two orthogonal vectors, WKW air that you are familiar with now, so expensive little 2D subspace.",
            "Of the image space and say you want to project into that subspace.",
            "We talked about that before.",
            "So say you have an image X and you want to approximate it in this 2D subspace using these two components using two coefficients AK&W.",
            "Sorry L that you apply.",
            "Can someone tell me what the coefficients coefficients should be for this particular pair of?",
            "Components here what's the optimal?",
            "Projection into that subspace.",
            "If the WSR forming little orthogonal basis of that subspace.",
            "Right, so exactly so the orthogonal projection into that subspace.",
            "It's the dot product, right?",
            "It's just W transpose XWK transpose, XW, transpose X that's the right thing to do.",
            "That's the best approximation you can do if you change this a little bit by adding or multiplying or something.",
            "This value you're going to move away from the best projection, right?",
            "But curiously, a lot of neural networks do not do this.",
            "They do not use this kind of projection, even though we know these are the right components.",
            "We know they're orthogonal in theory and in practice they have to be orthogonal anyway.",
            "Becausw.",
            "Of all those things that I talked before, and yet we use W transpose X plus bias typically to represent."
        ],
        [
            "The projection into that subspace.",
            "So let's take as an example an autoencoder.",
            "An autoencoder literally minimizes the reconstruction error from such a subspace.",
            "So it's the squared error, and the autoencoder is just a sum of decoder weight vectors.",
            "Using the hiddens as the encoding.",
            "So it's exactly that situation.",
            "If these students are reluz 'cause then you're just going to have a coefficient times the weight vector, and that's going to be the reconstruction.",
            "Um?",
            "So we know the optimal coefficients would just be the inner product.",
            "That's the best projection you can do, and what we also know is that in autoencoders, basically, whenever it was trained properly and it works and you get keyboard features is incredibly sparse.",
            "So out of your 500 hidden's something like 50 hidden's are active, so you always do a projection into a subspace for every input example.",
            "So we know we do a projection, we know we minimize squared error.",
            "The best thing we can do is this.",
            "Yet what do we do in erelu?",
            "We do this, we compute.",
            "What we should do and then add a constant and shift the reconstruction by that constant so we project into that subspace correctly and then move around in that subspace.",
            "And then we train that whole network and hope that the components that we train become orthogonal.",
            "Of course they won't, because we're not allowing them to become orthogonal because we are messing with the encoding it should be using to become orthogonal.",
            "Anne."
        ],
        [
            "And so one could hope that the biases are always zero and the problem kind of disappears.",
            "But that's absolutely not the case, and in fact the biases have already another job to do, they cannot do.",
            "Something that's good for the encoding, but the biases have to do almost always in any autoencoder RBM.",
            "Many other models is they have to be very, very negative.",
            "2 examples, just that I trained a couple of months ago.",
            "You learn your basis and stuff, but devices are very negative and the reason is you have to be sparse, otherwise you will not learn as reasonable model.",
            "If you force the biases to be positive.",
            "These filters break and nothing works and the model just doesn't learn.",
            "So the bias is already have a job.",
            "They have to make the hiddens get sparse and in order to be able to do that they have to be very negative no matter which model you train.",
            "And so."
        ],
        [
            "How do we use this bias whose role we know is enforcing sparsity of the vector of responses and pack that into the encoding with which we reconstruct?",
            "That doesn't seem to make sense specially."
        ],
        [
            "From the perspective of orthogonal subspaces and learning about the relationships and being steerable and whatnot.",
            "So."
        ],
        [
            "But most of us so I'm not going to go into."
        ],
        [
            "Some of."
        ],
        [
            "Other details here."
        ],
        [
            "That motivated us to look at an activation function which.",
            "Doesn't have a bias yet.",
            "Gives you sparse responses, so we take erelu and we just cut off its response.",
            "If it's smaller than a certain value, that makes sure that only a small fraction of them are going to be alive after they saw an image X.",
            "And then we say after you compute after you determine that you're going to be on, use W transpose X as the.",
            "Coefficient for the decoder and nothing other than that.",
            "That's very similar to Spike and slab models, which also try to separate the decision of being on from the encoding and stuff like that.",
            "So similar to something called hard threshold.",
            "I think this function is called hard thresholding function and it's also called coring and all kinds of stuff.",
            "But The funny thing is if you take this.",
            "Activation function training autoencoder using this activation function and do nothing other than that, no regularization, nothing.",
            "Just train this.",
            "It's 3 lines of code for the autoencoder, basically just a simple autoencoder is funny."
        ],
        [
            "Salvation function."
        ],
        [
            "And their variation."
        ],
        [
            "Of that"
        ],
        [
            "Then you learn these beautiful features.",
            "For example on 10,000,000 images.",
            "So basically it just works.",
            "That's the take home message.",
            "And I really like to show this.",
            "This results because you also get features that look like this.",
            "Um?",
            "Which some of you remember from the New York Times.",
            "Maybe when Google used gigantic cluster to train a eight layer big convolutional autoencoder on YouTube videos and then showed that the model was able to develop features that look like this.",
            "Now for us it's much easier to visualize those features.",
            "We just have to show them right.",
            "We don't have to do any inference, this is just a single layer autoencoder.",
            "You can just look at the features and this is what some of them look like.",
            "It also."
        ],
        [
            "Looks very nicely, so you can actually use those features in various pipelines.",
            "Turns out on C5, so this is by the way of work, joint work with quiche or condo again, and David Krueger was sitting there.",
            "And initially it didn't outperform a contractive autoencoder.",
            "If you take your contractive autoencoder.",
            "Rip out its activation function and replace it by a no bias Relu activation function.",
            "Then the contractive autoencoder actually outperforms this model a little bit, but if you use it, if you try to train a deep network and use an autoencoder to initialize that deep network layer by layer pretraining and so on, fully connected big network on Cifar 10, which is kind of an insane thing to do.",
            "But we had reasons to do that because of hardware and.",
            "Problems with confidence and stuff."
        ],
        [
            "Then you."
        ],
        [
            "Did too much better."
        ],
        [
            "On this and so we were able to push the performance on this task."
        ],
        [
            "Twoo"
        ],
        [
            "Beyond the state of the art currently, and that's actually work by Joanne Lynn, who is also so."
        ],
        [
            "We're here.",
            "So, so I'm not going to go into all the details of this, but the set of that recently was something like 63% on the permutation invariant, and we pushed it to 65 and.",
            "And then if you give up on."
        ],
        [
            "Mutation invariants and train on deformations and stuff.",
            "You go up to something like 80%.",
            "The reason we did that is because fully connected networks.",
            "If you remember Adam Coates talk are really important.",
            "To study becausw of hardware cause dense high intensity arithmetic computation is much more.",
            "Easily mapped to hardware than a convolution, which is a complete mess because of long range communication and stuff like that.",
            "And so it was kind of pleasing to see that we get to something like 10%, ten, 15%.",
            "Below are confident can do currently on this task is simply a fully connected net.",
            "Sorry I'm rushing a little bit because I'm running out of time.",
            "So I'm just going to stop here and take any other questions.",
            "I already saw some over there, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I left off camera at the end of last lecture.",
                    "label": 0
                },
                {
                    "sent": "As you might remember, so we looked at Fourier transforms on images and why they are relevant, probably to us and why neural Nets figure that out as well when they are trained and so on.",
                    "label": 0
                },
                {
                    "sent": "Not really fully transforms more like a bar transforms which is fully transforms.",
                    "label": 0
                },
                {
                    "sent": "Modulated by something like a Gaussian window which allows you to deal with.",
                    "label": 0
                },
                {
                    "sent": "Engineering problems like leakage and so on and so curiously neural Nets figure that out as well.",
                    "label": 0
                },
                {
                    "sent": "On a very high level, one could say that, well, a lot of this has to do with invariants, so if there's invariants structure in the data, which is a prerequisite for being able to learn anything, of course.",
                    "label": 0
                },
                {
                    "sent": "Then these kind of features have to emerge becausw you're going to have to be able to kind of summarize things according to that invariant structure.",
                    "label": 0
                },
                {
                    "sent": "So on a very high level, this talk is going to.",
                    "label": 0
                },
                {
                    "sent": "Look at this.",
                    "label": 0
                },
                {
                    "sent": "These kind of features from a different perspective, which you might call equivariance or something just more like we're going to lead these features not only if you want to be invariant to a certain types of transformation, but we're going to need these features also if we want to be able to represent transformations.",
                    "label": 0
                },
                {
                    "sent": "So if you have something that moves around and you want to know how much did it move or something like that, then it's going to be the exact same features that are going to be required for that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there has been a really long history.",
                    "label": 0
                },
                {
                    "sent": "Of models looking at these kinds of ideas and problems, including Fourier transforms and stuff like that originally handcrafted and more recently learned.",
                    "label": 0
                },
                {
                    "sent": "And almost all of them are motivated by the question how you might extend.",
                    "label": 0
                },
                {
                    "sent": "Simple computer vision tasks, which are sort of solved now like object recognition.",
                    "label": 0
                },
                {
                    "sent": "To more elaborate tasks that computer vision people like to think about like depth inference, motion inference and activity analysis, and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So this is a very still, even though most of the models that I'm going to talk about in the beginning a bit outdated now because they're unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Outdated currently because the unsupervised and don't really beat any continent on any task.",
                    "label": 0
                },
                {
                    "sent": "This is still relevant today because, well, object recognition you might argue solved, right?",
                    "label": 0
                },
                {
                    "sent": "So this is from Alex Crisci's 2012 paper, but there are still a lot of tasks out there.",
                    "label": 0
                },
                {
                    "sent": "Which are not solved.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so some of these tasks are things like geometric geometric inference, like figuring something out about this 3D structure of the world around you and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Stereo vision is an example of that structure.",
                    "label": 0
                },
                {
                    "sent": "For motion is another one, but also motion, understanding, activity recognition and videos and stuff.",
                    "label": 1
                },
                {
                    "sent": "All of this requires understanding motion independent of the thing that is moving.",
                    "label": 0
                },
                {
                    "sent": "So you want to know this motion happened and you don't care what the color of my shirt is and stuff like that, right?",
                    "label": 0
                },
                {
                    "sent": "That allows you to represent things like.",
                    "label": 0
                },
                {
                    "sent": "Activities.",
                    "label": 1
                },
                {
                    "sent": "But there are lots of other things like optical flow tracking.",
                    "label": 0
                },
                {
                    "sent": "Modeling relationships between objects in an image, for example, and visual odometry.",
                    "label": 0
                },
                {
                    "sent": "Another task that I'm going to mention a little bit later and and maybe most of all analogy making, which is in really cool task that well we talked about in the context of NLP.",
                    "label": 0
                },
                {
                    "sent": "Which means something like transferring a transformation from one object pair to some other object.",
                    "label": 0
                },
                {
                    "sent": "And there are cognitive scientists who argue that analogy making is.",
                    "label": 0
                },
                {
                    "sent": "The one thing that you're going to have to solve if you want to solve AI, maybe I'll get back to that a little bit later.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's just a cartoon example.",
                    "label": 0
                },
                {
                    "sent": "The standard example that one image might not be enough.",
                    "label": 0
                },
                {
                    "sent": "To solve certain vision tasks, so stereo vision requires at least two images, and even if none of the images contains any information whatsoever, so it's just pure random dots.",
                    "label": 0
                },
                {
                    "sent": "The pair of them with the random dots appropriately displaced.",
                    "label": 0
                },
                {
                    "sent": "Might contain information, so this particular pair contains bar.",
                    "label": 0
                },
                {
                    "sent": "If you squeeze kind of squeeze these images on top of each other by squinting and.",
                    "label": 0
                },
                {
                    "sent": "Some people can do that when they see there's something in the center.",
                    "label": 0
                },
                {
                    "sent": "So even if there's no structure whatsoever.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It might be structure hidden in the relations with across pixels or across images even.",
                    "label": 0
                },
                {
                    "sent": "Who has seen this video?",
                    "label": 0
                },
                {
                    "sent": "Few of you.",
                    "label": 0
                },
                {
                    "sent": "So apparently these are St bumps that are being used in Canada.",
                    "label": 0
                },
                {
                    "sent": "Just I think a really bad idea, probably.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "So you're laughing because you probably got fooled yourself the first time there, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to play this one one more time.",
                    "label": 0
                },
                {
                    "sent": "And I'm gonna use one of these.",
                    "label": 0
                },
                {
                    "sent": "So if I stop somewhere here and take any any frame whatsoever, probably any reasonably image net trained content is going to say there is a person that's a girl they are in the ball or something like that.",
                    "label": 0
                },
                {
                    "sent": "For every single frame, probably up to maybe when it gets distorted too much.",
                    "label": 0
                },
                {
                    "sent": "But the fact that it's not, it's just encoded in the relation across frames, which allows you to infer motion an from that into infer 3D structure, and you see that the 3D structure is really boring.",
                    "label": 0
                },
                {
                    "sent": "It's just descending kinda plain, and there's nothing else there, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there are hints that you could use.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right, there are some things that you could use.",
                    "label": 1
                },
                {
                    "sent": "I would assume that a conflict on this image on this particular image here would get fooled on one of them.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "Get that hint that shadow maybe but but there is a much much, much stronger Q which is structure that goes across the images of course.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of, yeah, that's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Even though that's an adversarial example that's very different from the ones that he talked about.",
                    "label": 0
                },
                {
                    "sent": "It gets at some very specific property and not.",
                    "label": 0
                },
                {
                    "sent": "It's not so much.",
                    "label": 0
                },
                {
                    "sent": "It's not at all caused by high dimensionality of the input space and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So you can extend that further and further and further.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, maybe you need robots, of course to get.",
                    "label": 0
                },
                {
                    "sent": "To solve AI becausw.",
                    "label": 0
                },
                {
                    "sent": "You were able as a child to pick up objects like this by just picking this part and then lifting it up, and you saw that this piece of object deformed in certain ways, and then it has certain properties and that it falls on your foot.",
                    "label": 0
                },
                {
                    "sent": "It doesn't hurt and all kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "And you use videos of course.",
                    "label": 0
                },
                {
                    "sent": "For that I mean your personal video going through the world, but you also use your activators for that and so on.",
                    "label": 0
                },
                {
                    "sent": "And so this object is very different, has very different properties, very different texture and all that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "And you feel that if you touch it and stuff, and so ultimately you need to go even beyond videos.",
                    "label": 0
                },
                {
                    "sent": "But videos themselves probably already teach you much, much more than just isolated images, and in particular relationships across images, and maybe ultimately between images and videos and actuators, and the way the world reacts and so on.",
                    "label": 0
                },
                {
                    "sent": "It's going to.",
                    "label": 0
                },
                {
                    "sent": "Tell you even more.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, this is a very there has been a very long research agenda.",
                    "label": 0
                },
                {
                    "sent": "People trying to encode relationships to get at such tasks, little bit better.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to motivate why you should use certain types of model and not others, even though this view is being superseded a little bit by some more modern variants of it now.",
                    "label": 0
                },
                {
                    "sent": "The main thing that people argued for was that you need multiplicative interactions and this is still somewhat true I think.",
                    "label": 0
                },
                {
                    "sent": "So here is one way of to motivate why you need multiplicative interactions.",
                    "label": 0
                },
                {
                    "sent": "If you have a neural network.",
                    "label": 0
                },
                {
                    "sent": "Pair of layers like this and your two images for which you want to encode relations, are here encoded as the concatenation simply of the two images concatenated and you have features on top that learn about that, say the next layer in the neural net, or say a layer in an auto encoder or an IBM or any other kind of model.",
                    "label": 0
                },
                {
                    "sent": "So as you know from our BMS for example.",
                    "label": 0
                },
                {
                    "sent": "But you can kind of extend that argument across all kinds of models.",
                    "label": 0
                },
                {
                    "sent": "If I condition on zed then everything here gets conditionally independent given zed and so that's a really weird assumption to make if you want that to encode relations between these.",
                    "label": 0
                },
                {
                    "sent": "Obviously cause that cannot capture any such relations.",
                    "label": 0
                },
                {
                    "sent": "If if I told you that these guys would be independent, so there wouldn't be related at all.",
                    "label": 0
                },
                {
                    "sent": "So what you want is zed, which includes the relationship between 2 images and.",
                    "label": 0
                },
                {
                    "sent": "So if I told you that and X for example, since you know the input image, say that that one and you know the transformation, then you should be able to know something more.",
                    "label": 0
                },
                {
                    "sent": "Again, a few extra bits about why, but if they are independent, then knowing X and that doesn't tell you anything about why, so that doesn't match our idea of a random variable that includes relationships.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as I said, you can relate that to the IBM.",
                    "label": 0
                },
                {
                    "sent": "Yeah yes yes please.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right, so higher level hidden units or something like that, that's right.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "That's sort of true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that.",
                    "label": 0
                },
                {
                    "sent": "So that says basically a single hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder cannot do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just slightly well, somewhat weaker statement.",
                    "label": 0
                },
                {
                    "sent": "You could even say even an autoencoder would still be fine.",
                    "label": 0
                },
                {
                    "sent": "If you define the energy function appropriately.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of in an IBM.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 0
                },
                {
                    "sent": "And in autoencoders typically, if these are sigmoid hidden units, you can show it's also true.",
                    "label": 0
                },
                {
                    "sent": "But you could define an energy function for the auto encoder where this coupling kind of naturally happens.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about that a little bit later, but yeah, so this is kind of true of two layer models only.",
                    "label": 0
                },
                {
                    "sent": "That's true, so there's a way though to fix this two layer model even in itself.",
                    "label": 0
                },
                {
                    "sent": "And so maybe I should also mention the reason why a lot of people who worked on this over the last 30 years still considered this, even though maybe higher layers might fix it.",
                    "label": 0
                },
                {
                    "sent": "Is that they were also interested in local learning rules like heavy and updates on these parameters and stuff, and those typically matched very well.",
                    "label": 0
                },
                {
                    "sent": "There's two layer kind of view rather than a deep network trend with back problem stuff.",
                    "label": 0
                },
                {
                    "sent": "But yeah, back problems now around and everything kind of changes and a lot of theory is kind of outdated because of that this is somewhat true of this as well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one simple way to fix this one layer model is to just make these into one click, right?",
                    "label": 0
                },
                {
                    "sent": "So you want to have a clique in your probabilistic model that couples all of these three guys.",
                    "label": 0
                },
                {
                    "sent": "Now they're not going to be.",
                    "label": 1
                },
                {
                    "sent": "None of them is going to be independent.",
                    "label": 0
                },
                {
                    "sent": "Of any other given the third guy in particular, these guys are not going to be independent.",
                    "label": 0
                },
                {
                    "sent": "If I told you that so that matches now, our desire to capture relations in some sense.",
                    "label": 0
                },
                {
                    "sent": "So that requires though a typical yes please.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Here, for example any.",
                    "label": 0
                },
                {
                    "sent": "Any pixel, some pixel in the input image is connected to some pixel in the output image.",
                    "label": 0
                },
                {
                    "sent": "At the same time, these both guys are connected to 1 hidden unit, so that means that these three are in one click and if you write down the probability distribution you see that these in dependencies don't hold, and so in particular, this particular unit said if it was on or something could encode something about X&Y.",
                    "label": 0
                },
                {
                    "sent": "So if I told UXI and that K that would allow you to infer something about YJ so that in other words encode something about the relationship between this pixel and that mixer.",
                    "label": 1
                },
                {
                    "sent": "In general, so this is just one.",
                    "label": 0
                },
                {
                    "sent": "Way to fix?",
                    "label": 0
                },
                {
                    "sent": "These three, these two pixels appear, but of course that means you want to throw in all kinds of three way connections that can relate all pixels that you might want to relate and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "This is just a kind of prototype view of the kinds of units that you would need for that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is a very sensible thing to go for from the perspective of biology, probably.",
                    "label": 0
                },
                {
                    "sent": "Here is a figure from an old paper by Bartlett Mail that just visualizes some dendritic trees and some neurons of some animal.",
                    "label": 0
                },
                {
                    "sent": "And you see that's incredible amount of variety and extremely strong structure in the way that these neurons branch off to collect information from other neurons, and so the way that we always model these great variety.",
                    "label": 0
                },
                {
                    "sent": "In basically any kind of model that we ever consider is W transpose X right, you basically compute a weighted summation.",
                    "label": 0
                },
                {
                    "sent": "A weighted summation of the incoming.",
                    "label": 0
                },
                {
                    "sent": "Neurons and then you have an activation function and so on.",
                    "label": 0
                },
                {
                    "sent": "So he argued already back then, some 20 years ago that.",
                    "label": 0
                },
                {
                    "sent": "What you called active neurons would be the right thing to go, which is sorry.",
                    "label": 0
                },
                {
                    "sent": "Active dendrites would be great thing to go which is.",
                    "label": 0
                },
                {
                    "sent": "Trees that do some stuff themselves that are not just wireless cables between neurons, but things that can do more elaborate computation, and so the most the simplest kind of thing you can possibly do to go beyond W transpose X is to include into the sum the way that some a little product of course.",
                    "label": 0
                },
                {
                    "sent": "So as a scientist I mean maybe it's actually not a bad thing to make such a ridiculous.",
                    "label": 0
                },
                {
                    "sent": "Approximation abstraction of reality because it actually did bring us very far and it captures a lot of the tensions that are there in the world, right?",
                    "label": 0
                },
                {
                    "sent": "These networks work and so that taught us that distributed representations matter and learning matters and all kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "So it's not bad to make extreme abstractions.",
                    "label": 0
                },
                {
                    "sent": "But if you wanted to go one step away from this extreme abstraction, maybe it's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Putting in products would be just one way to do that, so that's what motivated a lot of people are looking at.",
                    "label": 0
                },
                {
                    "sent": "These kind of things.",
                    "label": 0
                },
                {
                    "sent": "Out of that came.",
                    "label": 0
                },
                {
                    "sent": "Long research agenda called Bilinear Models, which I'm just going to motivate from the perspective of these manifolds.",
                    "label": 0
                },
                {
                    "sent": "So typically you have a set of.",
                    "label": 0
                },
                {
                    "sent": "Neurons and some layer that represent some image or something like that.",
                    "label": 0
                },
                {
                    "sent": "Now if your goal is no longer to represent these images, but say the relationship between 2 images.",
                    "label": 0
                },
                {
                    "sent": "Well, you can only represent that if there's some structure in the relationship between those images.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I gave you this X and say you apply the transformation, then they would kind of move this X around.",
                    "label": 0
                },
                {
                    "sent": "And so if these transformations are not completely random, but somehow structured so that the.",
                    "label": 0
                },
                {
                    "sent": "The X as I transform it is going to trace out something that you might want to call a manifold or in orbit, or subspace or something.",
                    "label": 0
                },
                {
                    "sent": "Then maybe you might go after trying to identify what these subspaces are and learning about them.",
                    "label": 0
                },
                {
                    "sent": "So now imagine you transform this X and travel along your orbit and now you swap in another X and that means you're going to travel around another orbit.",
                    "label": 0
                },
                {
                    "sent": "But of course that orbit is going to be somewhat related to this orbit.",
                    "label": 0
                },
                {
                    "sent": "If the transformation sequence is the same.",
                    "label": 0
                },
                {
                    "sent": "So if I just rotate.",
                    "label": 0
                },
                {
                    "sent": "Right, if I rotate this object, it's going to follow some manifold.",
                    "label": 0
                },
                {
                    "sent": "If I rotate a slightly different images.",
                    "label": 0
                },
                {
                    "sent": "Also going to follow some manifold, and presumably these manifolds are kind of parallel in somewhere there's some structure in which these things evolve and you might want to discover this.",
                    "label": 0
                },
                {
                    "sent": "So the idea then is to just let's use completely standard feature learning model like an IBM.",
                    "label": 0
                },
                {
                    "sent": "But let's just turn the parameters of the model into a function of the other image X.",
                    "label": 0
                },
                {
                    "sent": "So we learn an auto encoder that encodes Y.",
                    "label": 0
                },
                {
                    "sent": "But we just say let WJC is sitting in that model.",
                    "label": 0
                },
                {
                    "sent": "Be a function of the other image, 'cause that should exactly capture.",
                    "label": 1
                },
                {
                    "sent": "Kind of this intuition.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that's what people did and what comes out is a model that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Which is kinda typically now called bilinear model.",
                    "label": 0
                },
                {
                    "sent": "You have your image Y and you want to have, say in GBM or autoencoder that models that and now you just say OK, let's make every wait here a function of the other image.",
                    "label": 0
                },
                {
                    "sent": "Every day WJK in that graph is going to be a WJC of X now.",
                    "label": 0
                },
                {
                    "sent": "Right, and so if the simplest thing you can do is say it's a linear function, so every WJC should be a linear function of X.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "A linear function of X?",
                    "label": 0
                },
                {
                    "sent": "You have a whole bunch of values that you multiply X that the pixels in X with.",
                    "label": 0
                },
                {
                    "sent": "And so that's the linear function.",
                    "label": 0
                },
                {
                    "sent": "And now you have a model which is a function of X.",
                    "label": 0
                },
                {
                    "sent": "Right now if I plug this into my model and I do inference, for example, I compute the value for one unit set here.",
                    "label": 0
                },
                {
                    "sent": "Well, what is that is going to be?",
                    "label": 0
                },
                {
                    "sent": "The weighted sum of the wise?",
                    "label": 0
                },
                {
                    "sent": "Of course, that's the usual inference in GBM.",
                    "label": 0
                },
                {
                    "sent": "The preactivation here at least now I plug in this form here that tells me it's supposed to be a function of X, and now I take away the brackets and.",
                    "label": 0
                },
                {
                    "sent": "Some put together some sums and then I see that what I have to do is take all pairwise products between all these pixels.",
                    "label": 0
                },
                {
                    "sent": "Have a wait for them which came from this linear function and then just summed it up right.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so that's a bilinear relationship here.",
                    "label": 0
                },
                {
                    "sent": "Between X&Y, all pairwise products, weighted sum of them, and that's going to do inference for me, and so the same is true for the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inferring the Y given zed and all kinds of other computations in this model.",
                    "label": 0
                },
                {
                    "sent": "So a lot of people have been playing around with these models for many, many years.",
                    "label": 0
                },
                {
                    "sent": "Some notable ones, maybe this year.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I played with these models.",
                    "label": 0
                },
                {
                    "sent": "Great deal during my PhD in Toronto and I kind of developed all kinds of variations like you can turn the bots in IBM into this kind of scheme into a bilinear PBM by just doing that.",
                    "label": 0
                },
                {
                    "sent": "To this to the to the energy function, just replacing it with this 3 way thing here.",
                    "label": 0
                },
                {
                    "sent": "The exact following, the exact same argumentation here.",
                    "label": 0
                },
                {
                    "sent": "And then go through the math and then you get a 3 way IBM which I called gated Baltimore machine at that time and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do that with an autoencoder or ICA or whatever model that you like the best.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can do that to any layer in a neural net if you want.",
                    "label": 0
                },
                {
                    "sent": "Using the same mechanics.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, this idea is not new at all and dates back as most things maybe.",
                    "label": 0
                },
                {
                    "sent": "To a paper by at least 1981 two paper by Geoff Hinton who not only kind of came up with this idea of having 3 way interactions between triplets of neurons but also introduced this notation here that we now use all the time like this little triangle.",
                    "label": 0
                },
                {
                    "sent": "So the handwritten at the time and.",
                    "label": 0
                },
                {
                    "sent": "And and the reason he introduced it was also actually basically the same idea, same same thing.",
                    "label": 0
                },
                {
                    "sent": "It's if you have features and you want to recognize things, but you want to be able to rotate these features around to make them match what you see and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Then you need these control units which are able to take a feature and rotate it a little bit to make it match stuff like that Christopher Mods book is another one of the.",
                    "label": 0
                },
                {
                    "sent": "Early believe us in this kind of scheme and he has also been pushing that since.",
                    "label": 0
                },
                {
                    "sent": "Late 70s and then this is one of the first papers there.",
                    "label": 0
                },
                {
                    "sent": "But there has been a ton of other work on that.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "Following that, I'm not going to go into detail.",
                    "label": 0
                },
                {
                    "sent": "Just maybe mention some tensor product binding.",
                    "label": 1
                },
                {
                    "sent": "Their tensor always pops up if you look at these kind of models because you have these.",
                    "label": 0
                },
                {
                    "sent": "This matrix, this tensor that connects every.",
                    "label": 0
                },
                {
                    "sent": "Unit in one vector with every unit and another vector with every unit in another vector.",
                    "label": 0
                },
                {
                    "sent": "So you have a 3D grid of parameters.",
                    "label": 0
                },
                {
                    "sent": "Naturally because of that.",
                    "label": 0
                },
                {
                    "sent": "Turns on your Nets are just another instance iation of this.",
                    "label": 0
                },
                {
                    "sent": "If you want the things that Chris for example, was talking about, Bruno 1000 is one of the people who has been pushing this quite a lot.",
                    "label": 1
                },
                {
                    "sent": "Also, during his PhD he also looked at things that he called routing circuits, which is basically a way to route information around.",
                    "label": 0
                },
                {
                    "sent": "In a brain or in a neural net.",
                    "label": 0
                },
                {
                    "sent": "And stuff like that and.",
                    "label": 0
                },
                {
                    "sent": "And then there's been.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gotta follow up work.",
                    "label": 0
                },
                {
                    "sent": "One thing that I should still mention before I move on to some other things.",
                    "label": 0
                },
                {
                    "sent": "Pushing this a little bit further is what we called factor gated Boltzmann machine at that time.",
                    "label": 0
                },
                {
                    "sent": "Graham Taylor, I think mentioned that in his presentation and he also came.",
                    "label": 0
                },
                {
                    "sent": "I studied this kind of thing in the same time as I did and.",
                    "label": 0
                },
                {
                    "sent": "So the idea there is you have your 3D grid of parameters and that is an awful lot of parameters to learn, and so one way to fix that is to take your 3D block and apply what's called central tensor factorization, which just means that you take.",
                    "label": 0
                },
                {
                    "sent": "This 3D block and represent it using an outer product or something of lower dimensional things like matrices, right?",
                    "label": 0
                },
                {
                    "sent": "Imagine your matrix or matrix and matrix.",
                    "label": 0
                },
                {
                    "sent": "Now if you cannot compute.",
                    "label": 0
                },
                {
                    "sent": "Weighted sums in some way.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter in which way on all these matrices you can imagine a way to just fill out this tensor that way.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If you draw this in this autoencoder for example, then maybe it gets a bit clearer what's actually going to happen, and that's kind of important, because this is the first step towards eliminating those products altogether in the end.",
                    "label": 0
                },
                {
                    "sent": "But what we left with then is something much cleaner and supply and so on.",
                    "label": 0
                },
                {
                    "sent": "So what happens in there if you do this tensor factorization effectively so you replace WIJK using some product of matrices.",
                    "label": 0
                },
                {
                    "sent": "What the model is going to look like is very similar as before, so you have two images, But what you're going to do is project onto some features using some matrix so you just do a matrix standard neural network layer.",
                    "label": 0
                },
                {
                    "sent": "And then you do an elementwise product between triplets of units.",
                    "label": 0
                },
                {
                    "sent": "Here are actually 2 units here to get the activation for some third unit here and then you have another neural net layer, and that was going to compute the hidden's for you, so that just falls out if you replace the 3D 10s or by matrices.",
                    "label": 0
                },
                {
                    "sent": "And So what really happens inside this tensor factorization is that it takes the WIJK everyone multiplied with everyone else and replace it with some restricted product.",
                    "label": 0
                },
                {
                    "sent": "That says I'm only going to multiply the ice guy with the ice guy to get the I TH output guy.",
                    "label": 0
                },
                {
                    "sent": "And that in itself is not going to work because you want to relate pixels.",
                    "label": 0
                },
                {
                    "sent": "You want to see structure across pixels, and So what you end up doing is first multiplying this matrix to project into another space and then using this restricted kind of scheme of multiplying.",
                    "label": 0
                },
                {
                    "sent": "So that's what these models end up looking like.",
                    "label": 0
                },
                {
                    "sent": "And the reason this is important is that.",
                    "label": 1
                },
                {
                    "sent": "This is basically nothing other than saying you're doing a Fourier transform in order to perform your transformations.",
                    "label": 0
                },
                {
                    "sent": "Or to infer the transformations, so this part would be inferring a transformation.",
                    "label": 0
                },
                {
                    "sent": "Maybe the decoder would be.",
                    "label": 0
                },
                {
                    "sent": "Applying a transformation.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, you project onto a bunch of features and then.",
                    "label": 0
                },
                {
                    "sent": "Shifting say an image.",
                    "label": 0
                },
                {
                    "sent": "By three pixels to the left.",
                    "label": 0
                },
                {
                    "sent": "In the full year basis is nothing other than a phase shift.",
                    "label": 0
                },
                {
                    "sent": "Of your free components and in complex notation, this is nothing other than an elementwise product with a phasor that's just going to rotate that projection a little bit.",
                    "label": 1
                },
                {
                    "sent": "So projecting onto features followed by elementwise multiplication is essentially the same thing that a four year transform does, in order to perform transformations.",
                    "label": 0
                },
                {
                    "sent": "So that wasn't clear at all in the beginning, and so we were really surprised to see that the model learns for.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transform so for example, if you train on pairs of random dot images that are shifted around just to the left to the right, up, down whatever and you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teach us that is these matrices here that come out.",
                    "label": 0
                },
                {
                    "sent": "They're exactly a free transform and and that would be the feature on the input image X, and that would be the features on the output image, so therefore.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shifted versions of Fourier components.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We were really pleased to see that 'cause you know these models were just no one had an idea what they would do and if that would work and stuff and then we started to play with this and suddenly saw.",
                    "label": 0
                },
                {
                    "sent": "Well actually we didn't really suddenly see a few Transformers more small like you train your network for one week on your CPU back then.",
                    "label": 0
                },
                {
                    "sent": "And just see random noise as you usually see in the beginning, your initialization and then after one week.",
                    "label": 0
                },
                {
                    "sent": "It looked as though they were like little bass coming up a little bit maybe and we weren't really sure.",
                    "label": 0
                },
                {
                    "sent": "So you train for another week and then you see that it's actually afraid transforming.",
                    "label": 0
                },
                {
                    "sent": "I remember that I was running to Jeff into the office and showed him this and we were super excited and so we couldn't believe.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How is it distinguished?",
                    "label": 0
                },
                {
                    "sent": "Transforms and stuff now honestly on a GPU.",
                    "label": 0
                },
                {
                    "sent": "This training.",
                    "label": 0
                },
                {
                    "sent": "This takes about 5 seconds or so and you have your.",
                    "label": 0
                },
                {
                    "sent": "So, but then you can say, OK, well, let's we transform, but what if I throw other transformations, edit and then it turns out, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it learns that.",
                    "label": 0
                },
                {
                    "sent": "Generalized Fourier components of all these other kinds of transformation.",
                    "label": 0
                },
                {
                    "sent": "So, for example, rotations have their own subspaces in which they happen, and the model figures out that if you train it on rotated images, then it's supposed to represent the images in these subspaces.",
                    "label": 0
                },
                {
                    "sent": "I go a little bit.",
                    "label": 0
                },
                {
                    "sent": "Deeper into the mathematics.",
                    "label": 0
                },
                {
                    "sent": "So if you're a little bit confused right now, just bear with me for a little.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do all kinds of other stuff like shifting around 1/2 of the image and then in dependently shifting around the other half of the image and so on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the model figures out that it should devote part of a filter to this part of the image and other filters to model another part of the image, and so on, yes?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So in this case, not know in this case this wasn't with wrap around, this was with.",
                    "label": 0
                },
                {
                    "sent": "I shift the image and that means stuff falls out here and random stuff comes in here.",
                    "label": 0
                },
                {
                    "sent": "If you don't fill in with random stuff then you will always see kind of edges here or something because you can infer something from the fact that there's nothing here and then the model can kind of cheat and say there must have been a left shift because this is this line here is black and stuff, so we figured that out and then we started to just make this like crop from a very large image and move around and you're not subject to such problems.",
                    "label": 0
                },
                {
                    "sent": "And then this is without wrap around.",
                    "label": 0
                },
                {
                    "sent": "With wrap around it would just be easier to train becausw.",
                    "label": 0
                },
                {
                    "sent": "The fact that stuff falls out as you shift just means that you get noise essentially right.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing here.",
                    "label": 0
                },
                {
                    "sent": "So at the edge of the corners and stuff, so then.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can go further and just trainer natural videos and then you see that you still get this shifted components.",
                    "label": 0
                },
                {
                    "sent": "But in that case they're really localized little for your components, which is couple of features which phase shift around from one frame to the next in the video.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so actually this figure here that happen.",
                    "label": 0
                },
                {
                    "sent": "They happen to be organized in some way so you can play all kinds of tricks to make sure that nearby units here end up representing a similar frequency.",
                    "label": 0
                },
                {
                    "sent": "Opposition.",
                    "label": 0
                },
                {
                    "sent": "Which essentially amounts to just arranging them on a grid, say in 2D, and then constraining in some ways such that similar units get active together and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "That's a whole other interesting question, so you know that this is true in parts of the brain in some brains at least, and not others.",
                    "label": 0
                },
                {
                    "sent": "And people have.",
                    "label": 0
                },
                {
                    "sent": "This has been a spot color in this feature.",
                    "label": 0
                },
                {
                    "sent": "Learning community rights like learning these typographic feature Maps.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "All the models that are able to do this to get a Topa graphic or organization.",
                    "label": 0
                },
                {
                    "sent": "This is not a great one, but they are much better.",
                    "label": 0
                },
                {
                    "sent": "Once as you see in the literature, all of the models use either squaring nonlinearities or these kind of products that we have in these models.",
                    "label": 0
                },
                {
                    "sent": "If you don't do that, you basically never get this topographic organization, even if you arrange your finger units in some way and stuff, and so you can actually explain why this has to emerge.",
                    "label": 0
                },
                {
                    "sent": "If you have products or squares.",
                    "label": 0
                },
                {
                    "sent": "And that has to do with the fact that this shifts the model wants to shift this around right?",
                    "label": 0
                },
                {
                    "sent": "Because that's the data tells you to do this shifting around, but the model would never want to shift.",
                    "label": 0
                },
                {
                    "sent": "This particular component to try to replace this component by something completely different that has completely different frequency orientation and so you want to pair things which have a phase shift but not.",
                    "label": 0
                },
                {
                    "sent": "Don't do not differ in frequency and orientation and so in fact in the brain.",
                    "label": 0
                },
                {
                    "sent": "It is also the case that you get this structure with respect to frequency and position and orientation of your Gabor features.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the face, if you if I would have a color plot that shows me the phases, that's entirely random.",
                    "label": 0
                },
                {
                    "sent": "That's obvious completely.",
                    "label": 0
                },
                {
                    "sent": "That would just be a noisy random plot, so there are some reasons behind this that we now understand pretty well.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detail too much.",
                    "label": 0
                },
                {
                    "sent": "About this so.",
                    "label": 0
                },
                {
                    "sent": "So that's very pleasing.",
                    "label": 0
                },
                {
                    "sent": "It's pleasing to see that the network learns free transforms and generalizations of that, and so we can go one step further and try to understand what these things are really doing mathematically and why they learn for you transforms and stuff.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do that in the next three or four slides.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the task that we should consider is you take 2 images X&Y and you want to infer from them the transformation that relates them.",
                    "label": 0
                },
                {
                    "sent": "That would turn X into Y.",
                    "label": 0
                },
                {
                    "sent": "So we assume that Y is some transform version of X.",
                    "label": 0
                },
                {
                    "sent": "It's much easier mathematically if you assume orthogonal transformations L. Um?",
                    "label": 0
                },
                {
                    "sent": "And in some sense, it's the only kind of transformation that really made us becausw.",
                    "label": 0
                },
                {
                    "sent": "Anything that's not orthogonal is basically just going to increase or decrease the overall brightness or something like that, and so orthogonal transformations as I refer to in the end of flex.",
                    "label": 0
                },
                {
                    "sent": "The last lecture are basically the kind of transformation that shifting around from one place in your MNIST digit to another, right that really move stuff around.",
                    "label": 0
                },
                {
                    "sent": "Turns out any matrix can be decomposed any square matrix into a so-called symmetric and skew symmetric matrix and skew.",
                    "label": 0
                },
                {
                    "sent": "Symmetric matrices are basically the.",
                    "label": 0
                },
                {
                    "sent": "The general so code generator of orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "So basically we're going to want to focus on the orthogonal stuff on the moving stuff and not on the brightness stuff.",
                    "label": 0
                },
                {
                    "sent": "Why is that still a very very large class of transformations?",
                    "label": 0
                },
                {
                    "sent": "Because we're going to apply this transformation to vectorized images X, so X is going to be a stack of pixels.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be a 2D position of image, right?",
                    "label": 0
                },
                {
                    "sent": "Just vectorize your image so you have a 784 dimensional Ms digit or something.",
                    "label": 0
                },
                {
                    "sent": "And then also going to transformations in this 784 dimensions space are incredibly rich, right?",
                    "label": 0
                },
                {
                    "sent": "So they can do basically anything to your digit.",
                    "label": 0
                },
                {
                    "sent": "That's a huge class of transformations, couldn't care about more than that.",
                    "label": 0
                },
                {
                    "sent": "In fact, any permutation matrix so shuffling the pixels around in any way you want.",
                    "label": 0
                },
                {
                    "sent": "Would be an orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "That's how rich these.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So mathematically, orthogonal transformations are good 'cause you can show that they.",
                    "label": 0
                },
                {
                    "sent": "Have a peculiar eigen decomposition so you can actually always write any orthogonal matrix L. As a matrix, by doing appropriate transformation of your space.",
                    "label": 0
                },
                {
                    "sent": "Which consists of nothing but two dimensional blocks which are two dimensional rotation matrices.",
                    "label": 0
                },
                {
                    "sent": "Well, that's true of any high dimensional orthogonal matrix, so it's a block diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "And if you look at it in the right basis, it's going to be a block diagonal matrix where every block is a two by two matrix and that two by two matrix does nothing other than rotate around in their two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "That's one interesting aspect of orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "Another mathematical property that's useful to know about is that.",
                    "label": 0
                },
                {
                    "sent": "If you have a bunch of transformations that have this that sorry that commute, then they have the same eigen basis so this.",
                    "label": 0
                },
                {
                    "sent": "Eigenvectors onto which you have to project to see that they just rotate so the eigen spaces in which this transformation is just going to spinning around.",
                    "label": 0
                },
                {
                    "sent": "Are the same, and so translations commute.",
                    "label": 0
                },
                {
                    "sent": "Becausw if I take this image and then shift it and then shift it again, that's the same as if I had taken that image.",
                    "label": 0
                },
                {
                    "sent": "And shifted the 2nd way 1st and then the 1st way.",
                    "label": 0
                },
                {
                    "sent": "So that proves that translations commute, and so every the whole set of translation matrices has the same eigen basis the same eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I alluded to that.",
                    "label": 0
                },
                {
                    "sent": "Also, in my last lecture shift.",
                    "label": 0
                },
                {
                    "sent": "In one day at least can be represented by this kind of matrix, which is known as a circulant.",
                    "label": 0
                },
                {
                    "sent": "So it's going to take a vector and just shift the components around, and so if I shift if I move this set of after this, the off diagonal, once here around 2 here or two here, just going to be different shift.",
                    "label": 0
                },
                {
                    "sent": "But if you do like of this in Python you will see that you will every time for all of these different circulant matrices that you could try.",
                    "label": 0
                },
                {
                    "sent": "You will always get the same eigenvectors, only the eigen values will be different.",
                    "label": 0
                },
                {
                    "sent": "So the amount of rotation you're going to apply is going to differ.",
                    "label": 0
                },
                {
                    "sent": "So this is talking about 1D, of course.",
                    "label": 0
                },
                {
                    "sent": "Images can be still represented within this framework using something called block circulant matrices.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in a nutshell, if you take an image and you transform it by shifting it, so applying an orthogonal transformation like a kind of circulant, that's the same as taking that image.",
                    "label": 0
                },
                {
                    "sent": "Rotating the space so that you represented in the eigen basis of this matrix and then just spinning around.",
                    "label": 0
                },
                {
                    "sent": "In those two D subspaces and then projecting back right?",
                    "label": 0
                },
                {
                    "sent": "So equivalently in in Python, you can either go at times X to apply a shift, or you could just do rotation, rotation, rotation, rotation, rotation, bunch of times in the appropriate directions, and that would also shift the image.",
                    "label": 0
                },
                {
                    "sent": "And in fact, not only shift the image, but permute the pixels in any way you wish.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to get back to the question given to images X&Y.",
                    "label": 0
                },
                {
                    "sent": "You would like to have a mechanism that takes these two images and tells you what the transformation is.",
                    "label": 0
                },
                {
                    "sent": "If you had a training set of transformations, and those transformations are commuted.",
                    "label": 0
                },
                {
                    "sent": "Say they were all translations, then this fact here tells us what we have.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do all we have to do is project onto the this these eigenspaces.",
                    "label": 0
                },
                {
                    "sent": "Each of which is actually a 2D space, right?",
                    "label": 0
                },
                {
                    "sent": "You guys going to do 2D rotations, so these will come in pairs.",
                    "label": 0
                },
                {
                    "sent": "So you have to tip, take your ex input image projected onto the eigen spaces you have to take your second image projected onto the eigen spaces and then if you don't know which of the transformations you're dealing with, all you have to do is compute the angle of rotation by how much did you rotate in that eigenspace, and that's going to identify uniquely which transformation you're dealing with cause.",
                    "label": 0
                },
                {
                    "sent": "Thanks to commutativity, the only way that any two translations differ is by the amount of rotation that they're going to apply in those subspaces.",
                    "label": 0
                },
                {
                    "sent": "And so how do you do that in practice while you take an image?",
                    "label": 0
                },
                {
                    "sent": "We project it onto those components.",
                    "label": 0
                },
                {
                    "sent": "How do you compute a 2D in our product which would give you the angle between those two 2D vectors in that space?",
                    "label": 0
                },
                {
                    "sent": "Well into the inner product is just this component times that component plus this component times that component right?",
                    "label": 0
                },
                {
                    "sent": "That's how you computed 2D inner product between two vectors in 2D.",
                    "label": 1
                },
                {
                    "sent": "X1 times Y 1 + X two times Y 2.",
                    "label": 0
                },
                {
                    "sent": "So you compute this elementwise product, and then sum them up.",
                    "label": 0
                },
                {
                    "sent": "And that's how you would go about doing this.",
                    "label": 0
                },
                {
                    "sent": "And so what's interesting is that this is exactly the graph that I drew here was exactly affected by linear model, the way that we introduced them without knowing anything about eigenspaces and transformations in commuting matrices and stuff like that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is how you would do that anyway.",
                    "label": 0
                },
                {
                    "sent": "There's one subtle difference, which actually is kind of important.",
                    "label": 0
                },
                {
                    "sent": "We don't actually have some.",
                    "label": 0
                },
                {
                    "sent": "Here we have a matrix here that's learned, so this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fully connected matrix that's just learned and the whole network is just learned from the data, but it figures out that the features it should use pairwise fully components which are just phase shifted appropriately.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about that?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "That's right, so this is kind of the neural network way of writing down an algebraic expression so.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the X is a vector vectorized image.",
                    "label": 0
                },
                {
                    "sent": "This just means do matrix multiplication.",
                    "label": 0
                },
                {
                    "sent": "And the same here to get these.",
                    "label": 0
                },
                {
                    "sent": "So what we called factors at that time and then this circle is just a dimension typically circles when you're on your own.",
                    "label": 0
                },
                {
                    "sent": "It's a circle means a dimension in a vector.",
                    "label": 0
                },
                {
                    "sent": "That's the semantics of that right?",
                    "label": 0
                },
                {
                    "sent": "And so then this little triangle connection here just means that this guy gets its value by multiplying this guy and this guy together.",
                    "label": 0
                },
                {
                    "sent": "And accordingly, this guy multiplies this and that and so on.",
                    "label": 0
                },
                {
                    "sent": "So to make this analogy work between the fact that models and this system that you would engineer what you would have to put here if you would engineer this system, what you would put here is some eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Say you are you T whatever right?",
                    "label": 0
                },
                {
                    "sent": "And what you should put here is not the eigenvectors, but what you should use in order to figure out whether there was a rotation happening.",
                    "label": 0
                },
                {
                    "sent": "Is the phase shifted version of that vector of Eigen?",
                    "label": 0
                },
                {
                    "sent": "Sorry matrix of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So you should put eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Here you should put phase shifted eigenvectors here and then you should compute an inner product and if this gives you a large value then the shift by which the X&Y are related exactly matches the shift that these.",
                    "label": 0
                },
                {
                    "sent": "Matrices by which these matrices are related.",
                    "label": 0
                },
                {
                    "sent": "So this is a subtlety that comes then later into play when you think about that these aren't actually sigmoid units, and they actually are just going to detect whether a certain rotation actually happened or not.",
                    "label": 0
                },
                {
                    "sent": "And stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm just going to leave it a little bit loose this analogy because I don't think it's that important, maybe anymore.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you had only one image pair.",
                    "label": 0
                },
                {
                    "sent": "An image and it's shifted version or many image pairs which were shifted in exactly the same way, so you had only one transformation.",
                    "label": 0
                },
                {
                    "sent": "Then you could do something like an SVD and figure out what the transformation is.",
                    "label": 0
                },
                {
                    "sent": "Canonical correlations analysis.",
                    "label": 0
                },
                {
                    "sent": "CCA is also a way of getting at that it's sort of a way to analyze a transformation L between 2:00.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two vectors.",
                    "label": 0
                },
                {
                    "sent": "But that's not what we're here.",
                    "label": 0
                },
                {
                    "sent": "What we're after here?",
                    "label": 0
                },
                {
                    "sent": "What we're after here is assuming that there's a whole class of transformations, say any shift, and I don't know which one and what I want to do is figure out which shift I'm actually dealing with.",
                    "label": 0
                },
                {
                    "sent": "And so you could extend CCA to something like a mixture of CCA's or something like that.",
                    "label": 0
                },
                {
                    "sent": "And that's essentially what these things are.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you need a mixture model at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually maybe I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to go over that very quick.",
                    "label": 0
                },
                {
                    "sent": "So there's an interesting problem in vision that you might heard about.",
                    "label": 0
                },
                {
                    "sent": "Might have heard about.",
                    "label": 0
                },
                {
                    "sent": "It's called the aperture problem.",
                    "label": 1
                },
                {
                    "sent": "Who knows who has heard about what the aperture problem is?",
                    "label": 0
                },
                {
                    "sent": "Some, not many.",
                    "label": 0
                },
                {
                    "sent": "So actually this view from the perspective of orthogonal transformations allows you to kind of understand what the aperture problem is in a much more concise and in my opinion simple simpler way then.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way that vision people usually think about this.",
                    "label": 0
                },
                {
                    "sent": "Say you want to detect the transformation, which is a shift as you see on the very left, a shift by a certain number of pixels to the left from a position to the left, right, just a global left shift.",
                    "label": 0
                },
                {
                    "sent": "What is this shift going to express itself as in those subspaces in which we can detect it?",
                    "label": 0
                },
                {
                    "sent": "Well, in this subspace spanned by these two failure components.",
                    "label": 1
                },
                {
                    "sent": "So in this one subspace.",
                    "label": 0
                },
                {
                    "sent": "It's just going to be a rotation cause everything is just rotations and so you can think of it as going from this position here to this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Position here you're just.",
                    "label": 0
                },
                {
                    "sent": "You're just going to rotate around as you apply this shift there, going to be a whole bunch of other subspaces like this one here.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which also your matrix U in which you won't see anything actually becausw.",
                    "label": 0
                },
                {
                    "sent": "Maybe your image doesn't have any components in that subspace, right?",
                    "label": 0
                },
                {
                    "sent": "In particular, this is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True of this image, it doesn't have any horizontal structure.",
                    "label": 0
                },
                {
                    "sent": "And so these two components won't respond if you compute the inner product with this.",
                    "label": 0
                },
                {
                    "sent": "And so you won't see anything at all.",
                    "label": 0
                },
                {
                    "sent": "Now take another image like the superposition of two bars.",
                    "label": 0
                },
                {
                    "sent": "Then the overall energy, say, is normalized, so the whole the overall intensity of this has to go down accordingly.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be less represented in the subspace in which it can detect the left right shift.",
                    "label": 0
                },
                {
                    "sent": "But it's this image is going to have some energy in this subspace, and so if you now want to see what happens in each of the subspaces, you just wiggle back in force and you see uh-huh it's going to phase shift here.",
                    "label": 0
                },
                {
                    "sent": "And it's just doing nothing here, right?",
                    "label": 0
                },
                {
                    "sent": "So you get your signature of.",
                    "label": 0
                },
                {
                    "sent": "Rotation angles that you need to identify the transformations.",
                    "label": 0
                },
                {
                    "sent": "You can collect it from your subspaces, but unfortunately you cannot collect it from all subspaces, because if your image.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't have any energy in some of them.",
                    "label": 0
                },
                {
                    "sent": "Then you're screwed, then you won't see anything if you just see zeros.",
                    "label": 0
                },
                {
                    "sent": "And so maybe there was actually an upshift.",
                    "label": 0
                },
                {
                    "sent": "Which these guys would detect, but unfortunately I'm not able to see it because I don't have any energy in that subspace.",
                    "label": 0
                },
                {
                    "sent": "And in fact in this image here.",
                    "label": 0
                },
                {
                    "sent": "You won't be able to tell the difference between a bar that moves between left and right and the bar that moves to the left and at the same time up.",
                    "label": 0
                },
                {
                    "sent": "Actually, you won't be able to tell, and the reason is exactly that there is a component which is just not represented in that image.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but I go once.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step further and I just look just at this image and I keep applying my same transformation right?",
                    "label": 0
                },
                {
                    "sent": "I shifted left to right.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out this image doesn't change at all, so you don't detect the fact that there is a left right shift.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "Because this component in this for this image is not detectable and so on.",
                    "label": 0
                },
                {
                    "sent": "The fact that there can be emotion, say a left shift of this image that you cannot see, that's called aperture problem in vision and it's called aperture problem because you're looking at this image only through a finite window, and if there is structure beyond this window that you cannot see if you had a larger window eventually.",
                    "label": 1
                },
                {
                    "sent": "There are no infinitely long bars in the world, so eventually this bar would end, and that would immediately give you some free component in this space, right?",
                    "label": 0
                },
                {
                    "sent": "If this by ends at some point you have something vertical, and so it's now you would be able to detect the motion.",
                    "label": 0
                },
                {
                    "sent": "So so you could re late the problem to the fact that you have a finite window, but it's a bit of a strange way to name this problem.",
                    "label": 0
                },
                {
                    "sent": "It's basically a subspace motion.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detection problem that exists.",
                    "label": 0
                },
                {
                    "sent": "So do you want to repeat your question I think.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, OK, so OK, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So because of that it might not be a good idea to use as a car.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good for your transformation, these two D. Detections this panel, this number of hidden units here, which might detect some motion or not, might be better to pull across multiple subspaces to get a code which is always going to be the same for any left shift, no matter which components are present or not.",
                    "label": 0
                },
                {
                    "sent": "You know what I mean?",
                    "label": 0
                },
                {
                    "sent": "So in practice, it's much better to pull across me.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The subspaces, rather than leaving them alone here.",
                    "label": 0
                },
                {
                    "sent": "So this so no, it's not going to.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that problem, and in fact you could say this.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could say there is no left motion here, right?",
                    "label": 0
                },
                {
                    "sent": "It's you could say there is, or there isn't.",
                    "label": 0
                },
                {
                    "sent": "It's sort of meaning a meaningless statement to make.",
                    "label": 0
                },
                {
                    "sent": "What this illustration doesn't doesn't illustrate is that there can be fully components with other frequencies.",
                    "label": 0
                },
                {
                    "sent": "For example, right, so I could have a twice S. Fast wave here and maybe I have an image which has that fast for your component but not this particular one and so then you could actually from that other full company could.",
                    "label": 0
                },
                {
                    "sent": "You could still figure out that there was a left shift.",
                    "label": 0
                },
                {
                    "sent": "But if you have this image, you would see the left shift popping up in one component.",
                    "label": 0
                },
                {
                    "sent": "If you had a higher frequency image, you would see it pop up in another component, and if you just want a hidden unit that just says there was a left shift or not, then you should pull across all of them.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so for a long time.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Him people were really excited about these models.",
                    "label": 0
                },
                {
                    "sent": "They were state of the art and activity analysis for some time.",
                    "label": 0
                },
                {
                    "sent": "Graham mentioned that's actually his paper had a convolutional version of such a model.",
                    "label": 0
                },
                {
                    "sent": "And and then a little bit later, quickly and others introduced a model that they called hierarchical ISA, which was also state of the art for quite some time.",
                    "label": 0
                },
                {
                    "sent": "An interesting observation is that you can.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make is that these models are equivalent to these ISA models.",
                    "label": 0
                },
                {
                    "sent": "If you take a video.",
                    "label": 0
                },
                {
                    "sent": "Rather than a single image, so a whole long video and then plug in the same video left and right here.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that computing the product of two components is the same as squaring that component.",
                    "label": 1
                },
                {
                    "sent": "If they are the same.",
                    "label": 0
                },
                {
                    "sent": "Well, if you also set the parameters here to be the same and then summing over squares is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out, is what Isa is doing this model here.",
                    "label": 0
                },
                {
                    "sent": "And the theory then can also explain why both these skating models as well as the squaring models, also called square pulling models, are both able to represent the motion in videos.",
                    "label": 0
                },
                {
                    "sent": "Essentially because the square of a sum is contains the products by binomial identity and then you can show that hidden units that respond to a shift are also going to respond to a square and stuff.",
                    "label": 0
                },
                {
                    "sent": "But it's kind of technical that it's not so interesting anymore, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, plugging this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "No no no no no no.",
                    "label": 0
                },
                {
                    "sent": "Just the exact same XO X = Y.",
                    "label": 0
                },
                {
                    "sent": "Just plugging in the same thing here and here, which ultimately comes down to just having a model that computes features and those features get squared.",
                    "label": 0
                },
                {
                    "sent": "And then you do pooling another other stuff.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a model with a squaring nonlinearity as the transfer function in neuron with that computer square basically.",
                    "label": 0
                },
                {
                    "sent": "You can, you can motivate this square in other ways because it gives you invariants and stuff like that, but.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "It is not exactly the same, but what is going to be the same is the tuning of the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Iran, so the stimulus here.",
                    "label": 0
                },
                {
                    "sent": "The video that's going to maximally activate one unit here.",
                    "label": 0
                },
                {
                    "sent": "It's going to be the same in across those kinds of model across squaring and products, but that's.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "OK, alright, OK, so there's an intermediate step.",
                    "label": 0
                },
                {
                    "sent": "So instead of taking this XY thing you can take.",
                    "label": 0
                },
                {
                    "sent": "XY concatenated and XY concatenated and then computer square.",
                    "label": 0
                },
                {
                    "sent": "And that's that I'm claiming is the same as this.",
                    "label": 1
                },
                {
                    "sent": "Right now, if you take 10 frames at a time, then that's still going to be the same as if you had taken all parallel path of by linear models or something.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but people have moved on since then, right?",
                    "label": 0
                },
                {
                    "sent": "This is unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's learning features that represent motions and stuff.",
                    "label": 0
                },
                {
                    "sent": "And also Graham talked about that as well.",
                    "label": 0
                },
                {
                    "sent": "So if you just take a huge continent and train it on a million videos and stuff, you typically outperform anything that.",
                    "label": 0
                },
                {
                    "sent": "Anyone has ever done and so these kind of models have gone a little bit out of favor because of that?",
                    "label": 0
                },
                {
                    "sent": "I don't think that means that you shouldn't be looking at these models and that we should kind of give up on them and just train.",
                    "label": 0
                },
                {
                    "sent": "Big Nets using back prop to solve everything.",
                    "label": 0
                },
                {
                    "sent": "I think there's an interest in understanding what models do.",
                    "label": 0
                },
                {
                    "sent": "But as you also mentioned, unsupervised learning is currently not used in any company or in any practical.",
                    "label": 0
                },
                {
                    "sent": "Situation.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, fair enough.",
                    "label": 0
                },
                {
                    "sent": "Even though.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't, probably.",
                    "label": 0
                },
                {
                    "sent": "It is, it is, but you can also just replace that word embedding model and just train a translation network end to end to solve their task and don't go through intermediate embeddings and then you're back in back problem.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right, right, right, right?",
                    "label": 0
                },
                {
                    "sent": "So actually yeah, so there is actually a situation in which unsupervised learning is still valuable and probably even used in companies.",
                    "label": 0
                },
                {
                    "sent": "And that is in this situation where you do not have tons of labeled data.",
                    "label": 0
                },
                {
                    "sent": "And in that setup these models still are.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Currently still the state of the art in various tasks are the only things that people have even tried so far.",
                    "label": 0
                },
                {
                    "sent": "There so the applications maybe that you could derive from this, like for example getting invariants from videos.",
                    "label": 0
                },
                {
                    "sent": "That's also not a supervised kind of problem.",
                    "label": 0
                },
                {
                    "sent": "You can show that if you have videos, you naturally get invariants to the transformations that you see in the video, and that false.",
                    "label": 0
                },
                {
                    "sent": "Basically out of this analysis you get these rotation angles and encoding.",
                    "label": 0
                },
                {
                    "sent": "Those rotation angles doesn't depend on the actual phase of every stimulus.",
                    "label": 0
                },
                {
                    "sent": "So if I see an angle like this.",
                    "label": 0
                },
                {
                    "sent": "Or another image pair or whatever is an angle like this.",
                    "label": 0
                },
                {
                    "sent": "I can figure out what the transformation is, so that's this angle, but it doesn't matter what the initial phase of the image was, and so invariants comes for free.",
                    "label": 0
                },
                {
                    "sent": "Essentially from these models.",
                    "label": 0
                },
                {
                    "sent": "Another way of saying that is that the transformation between 2 images is sort of the derivative of the position, and derivatives do not depend on the actual position or something, and so there are various papers that played with that a little bit and then.",
                    "label": 0
                },
                {
                    "sent": "Kishor Conda, PhD student, played with various variations of this model, most notably a variation that drops this top layer that does the pooling altogether and justice sort of a single layer network.",
                    "label": 0
                },
                {
                    "sent": "Graham talked about this and explained why Synchrony is one way to think about these models, and so he had some success on things like depth inference and and he replied that a visual odometry, which is also really cool task.",
                    "label": 0
                },
                {
                    "sent": "Really important in robotics and automotive and stuff.",
                    "label": 0
                },
                {
                    "sent": "From a video you want to know how far you were driving something like this, right?",
                    "label": 0
                },
                {
                    "sent": "So you want to infer your ego motion from the video.",
                    "label": 0
                },
                {
                    "sent": "I take one step forward, then all the pixels in my visual field are going to move around a little bit.",
                    "label": 0
                },
                {
                    "sent": "The question is from that.",
                    "label": 0
                },
                {
                    "sent": "Just that video of pixels moving this far, can I infer?",
                    "label": 0
                },
                {
                    "sent": "How far will I walked on things like that?",
                    "label": 0
                },
                {
                    "sent": "So what he did in that work was that he pre trained a consonant using these kinds of feature.",
                    "label": 0
                },
                {
                    "sent": "So that way that model could infer depth and motion and stuff and then solve the odometry task.",
                    "label": 0
                },
                {
                    "sent": "Had we more data, which we don't, but if he maybe had 100 times as much data for doing odometry, which no one I think has maybe just training a continent would have been sufficient.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You should talk to Kishor.",
                    "label": 0
                },
                {
                    "sent": "He's probably somewhere here.",
                    "label": 0
                },
                {
                    "sent": "Yeah there is.",
                    "label": 0
                },
                {
                    "sent": "There are lots of subtleties like that that come up, and so you also, you haven't even have to define what you want to do.",
                    "label": 0
                },
                {
                    "sent": "Do you want to dictate rotations and more forward motion separately and stuff like that?",
                    "label": 0
                },
                {
                    "sent": "And how do you want to represent that and?",
                    "label": 0
                },
                {
                    "sent": "I think the that question can probably be answered by the training data that the small training set that exists.",
                    "label": 0
                },
                {
                    "sent": "This, the Kitty database which says.",
                    "label": 0
                },
                {
                    "sent": "Which kind of prescribes which units you should be using?",
                    "label": 0
                },
                {
                    "sent": "But yeah, there are lots of subtleties that can come into that.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so are there any questions about this so far?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true I guess.",
                    "label": 0
                },
                {
                    "sent": "Probably yes.",
                    "label": 0
                },
                {
                    "sent": "A fully 3 way model with this parameter 10s or with all those numbers would have a ton of capacity.",
                    "label": 0
                },
                {
                    "sent": "The factored model, the one that does it first, the projection, and then elementwise product.",
                    "label": 0
                },
                {
                    "sent": "So the full year model, basically.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have as much capacity.",
                    "label": 0
                },
                {
                    "sent": "So that's one way to reduce that.",
                    "label": 0
                },
                {
                    "sent": "But in practice, as these filters show and so on, it just learns happily learns the transformations from the data, and that's also not something that's maybe strange.",
                    "label": 0
                },
                {
                    "sent": "That's something that we found all the way through the neural network literature, right?",
                    "label": 0
                },
                {
                    "sent": "You scale up your model like crazy.",
                    "label": 0
                },
                {
                    "sent": "And you train it on a lot of data and then it's fine.",
                    "label": 0
                },
                {
                    "sent": "You cannot overparameterized neural net, almost just doesn't happen right, at least if you have a lot of data available unless and we have a lot of data, we have unlabeled videos.",
                    "label": 0
                },
                {
                    "sent": "More than we could possibly want to have.",
                    "label": 0
                },
                {
                    "sent": "What we do not have is labeled data, but unlabeled data like videos from which we can infer rotations, transformations and stuff.",
                    "label": 0
                },
                {
                    "sent": "It's just infinite infinite amounts.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So for the remainder of the talk, I'm going to talk about some more modern perspective onto this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about two things that are somewhat.",
                    "label": 0
                },
                {
                    "sent": "Come up from this view of Fourier basis and orthogonal transformations and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Both of these are related to the vanishing gradient problem that people already talked about before, which comes up in deep Nets, but also in recurrent Nets so.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It just means if you have a deep net, say in recurrent net that goes through many time steps.",
                    "label": 0
                },
                {
                    "sent": "And the backward pass, at least you're going to multiply matrix times matrix times matrix times matrix.",
                    "label": 0
                },
                {
                    "sent": "And if the eigenvalues of that matrix are not nice.",
                    "label": 0
                },
                {
                    "sent": "Things are going to decay exponentially or blow up exponentially and.",
                    "label": 0
                },
                {
                    "sent": "You have a problem if they decay exponentially, that means the derivatives that arrive here are basically zero, and so you're not going to be able to learn anything over here anymore.",
                    "label": 0
                },
                {
                    "sent": "If they explored, it's sort of the same problem, it means you're going to have to use a smaller learning rate, and so.",
                    "label": 0
                },
                {
                    "sent": "Then again, the learning rate might be too small to be able to learn something here or somewhere else.",
                    "label": 0
                },
                {
                    "sent": "Even then, that would be even here.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "As has been suggested and discussed in the summer school before, you could use orthogonal layers.",
                    "label": 1
                },
                {
                    "sent": "Try to make this lower layers orthogonal or initialize them using orthogonal matrices and stuff.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Why would that fix it?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "We know now from this analysis before what an orthogonal matrix is.",
                    "label": 0
                },
                {
                    "sent": "Its eigenvalues have absolute value, one right?",
                    "label": 0
                },
                {
                    "sent": "So it's eigenvalues aren't one.",
                    "label": 0
                },
                {
                    "sent": "Its eigenvalues have absolute value one.",
                    "label": 1
                },
                {
                    "sent": "They can still spin around the unit circle.",
                    "label": 0
                },
                {
                    "sent": "There just never going to decay towards 0R explode.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvalues in fact in particular are complex numbers.",
                    "label": 0
                },
                {
                    "sent": "And if you take any random matrix with which you will probably ever initializing neural net here and you do, I cough that matrix in Python you will see that you get real and imaginary values, so these are naturally complex, right?",
                    "label": 0
                },
                {
                    "sent": "That's something to keep in mind when you think about these orthogonal layers in neural Nets and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So they're going to spin around a lot.",
                    "label": 0
                },
                {
                    "sent": "And why do they do that?",
                    "label": 0
                },
                {
                    "sent": "Well, by the are they still going to do that after training?",
                    "label": 0
                },
                {
                    "sent": "Well, 'cause you want to shift weights around, right?",
                    "label": 1
                },
                {
                    "sent": "You want to left this hidden unit, activate that hidden unit and stuff you want to do this kind of moving around of ink inside the network all the time.",
                    "label": 0
                },
                {
                    "sent": "So also discussed has been this identity initialization, which also helps.",
                    "label": 0
                },
                {
                    "sent": "So you just initialize the identity matrix and, but that's kind of a really strange thing to do and.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stick to that a little bit later, so intuitively sine waves and fully components are great to understand what this orthogonal transformation means.",
                    "label": 1
                },
                {
                    "sent": "Because the sine wave is kind of well, it's generated by your free component, it's just a phase are spinning around forever, and so it would be really easy to take the beginning of a few isava free component of a sine wave and infer what it's frequency is in phases, and then continue that sine wave.",
                    "label": 0
                },
                {
                    "sent": "In fact, a sine wave is kind of the kind of signal that doesn't contain any information besides frequency and phase, right?",
                    "label": 1
                },
                {
                    "sent": "It just does the same thing again and again and again and just never stops.",
                    "label": 1
                },
                {
                    "sent": "And so even though in a sine wave that extremely strong correlations between the value at 398 and the value that.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You saw here in the beginning, maybe?",
                    "label": 0
                },
                {
                    "sent": "It doesn't introduce any new information along the way.",
                    "label": 0
                },
                {
                    "sent": "Something so sine waves exhibit extremely strong long range structure like you wish you were heading an SDM or something.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interestingly, this little game of taking the beginning of a sine wave and continuing it forever, which we would want a neural net to be able to do, I think it's independent of phase.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that, well, it doesn't matter where you start.",
                    "label": 0
                },
                {
                    "sent": "If I tell you the beginning of the sine wave and I ask you to continue it, all that you have to do is take the last value in the 2D subspace in which this guy is just spinning.",
                    "label": 0
                },
                {
                    "sent": "And then you know say which phase are you dealing with.",
                    "label": 0
                },
                {
                    "sent": "You know which component dealing with, so you just multiply by E to the I5.",
                    "label": 0
                },
                {
                    "sent": "And keep doing that, and so you're going to spin forever, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter where you start, so it doesn't matter if you start here or here.",
                    "label": 0
                },
                {
                    "sent": "You can just keep spinning around and generate the rest of the same way forever easily.",
                    "label": 1
                },
                {
                    "sent": "So there will be an easy thing to do.",
                    "label": 0
                },
                {
                    "sent": "Trivial thing to do, a trivial task to go for using a neural net.",
                    "label": 0
                },
                {
                    "sent": "Well, if that neural net would have components that are able to spend these two subspaces right, so those components should be able to do this spinning operation.",
                    "label": 0
                },
                {
                    "sent": "And so that precludes already something like a sigmoid unit.",
                    "label": 0
                },
                {
                    "sent": "You need something like a relevant, because otherwise you're not going to be able to infer this angle to play with these three values, as you cannot travel through the network.",
                    "label": 0
                },
                {
                    "sent": "There was a really nice paper by Matthias Bittker and coauthors in 2007 where they called this.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Basically, the fact that you can.",
                    "label": 0
                },
                {
                    "sent": "Look at it at any phase.",
                    "label": 0
                },
                {
                    "sent": "Their coded, steerable simply and they'll try to learn features that are steerable in that sense, and I'm going to go back to that in a few minutes, I think.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So how does an STM solve the long range structure problem?",
                    "label": 0
                },
                {
                    "sent": "Here is an Atlas TM unit.",
                    "label": 0
                },
                {
                    "sent": "Well, the core of the STM is just.",
                    "label": 0
                },
                {
                    "sent": "A neuron connected to itself with a one.",
                    "label": 0
                },
                {
                    "sent": "And so as you go through time this well, you will just stay there and we won't do anything.",
                    "label": 0
                },
                {
                    "sent": "That in itself is completely useless computation.",
                    "label": 0
                },
                {
                    "sent": "It's like there's no way, and there's no reason to do this right.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to take a value and just leave it there is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do any computation.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "The reason it becomes useful, of course, and it's the M is big 'cause there's a whole lot of logic around that one component which can shovel things into it.",
                    "label": 0
                },
                {
                    "sent": "Read things out of it, and kind of control it in a way such that it becomes useful to keep around the value.",
                    "label": 0
                },
                {
                    "sent": "But the other units, like the gated recurrent unit, which has some other products and greetings and stuff.",
                    "label": 0
                },
                {
                    "sent": "Around such a unit and works just as well on some tasks may be better on some may be worse on some, but it's essentially also just a useless component surrounded by control logic, which makes that useless component useful.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So going back to the sine wave example, imagine I give you the beginning of a sine wave.",
                    "label": 0
                },
                {
                    "sent": "I want you to continue it forever.",
                    "label": 0
                },
                {
                    "sent": "That would be easy to do.",
                    "label": 0
                },
                {
                    "sent": "You need a neural net that has maybe relative units or linear units even, and an appropriately designed matrix that just spins at the component, that's it.",
                    "label": 0
                },
                {
                    "sent": "Well, if I change the frequency that won't work anymore.",
                    "label": 0
                },
                {
                    "sent": "You would have another, you would need another.",
                    "label": 0
                },
                {
                    "sent": "Piece of network that can spin around that faster signal accordingly, right?",
                    "label": 0
                },
                {
                    "sent": "So what you could do though is take a mixture model like a mixture of experts or something that goes from one time frame to the next and that mixture component should just be told what's the frequency.",
                    "label": 0
                },
                {
                    "sent": "We don't need phase.",
                    "label": 0
                },
                {
                    "sent": "We already covering all phases because we know now that spinning around is just the same.",
                    "label": 0
                },
                {
                    "sent": "So we just have to tell it which frequency we're dealing with, and that network could then a simple mixture model could then.",
                    "label": 0
                },
                {
                    "sent": "Continue sine waves for you forever.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Vincent Michalski was also somewhere here I think did play with a model that can do that in his master thesis, and we call that Grandma cells because we made that hierarchical as I'm going to show in a second.",
                    "label": 0
                },
                {
                    "sent": "The way that this model works is that it takes 2 frames in a video, for example, and it assumes that these two frames tell you everything about that video, and then it assumes that the transformation that in Ferd are going to be constant across time forever.",
                    "label": 1
                },
                {
                    "sent": "And so if you make that assumption, you will be able to generate the rest of the video.",
                    "label": 0
                },
                {
                    "sent": "Ad infinitum, you can just do the same thing again and again and again, so that's not interesting for real videos.",
                    "label": 0
                },
                {
                    "sent": "Becausw no transformation just gets repeated again and again.",
                    "label": 0
                },
                {
                    "sent": "That wouldn't give you an interesting video for sign.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it works though, for one thing.",
                    "label": 0
                },
                {
                    "sent": "So if you show it to the beginning of a sine wave and then you ask it, please continue that sine wave that you infer.",
                    "label": 0
                },
                {
                    "sent": "Then it figures out the phase.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that doesn't need to fail.",
                    "label": 0
                },
                {
                    "sent": "It figures out the frequency and it just continues that sine wave and numerically.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things would maybe go unstable, but thanks to the way that this model is kind of doing its job.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can just go on and on and on and on and it will never never blow up and so you.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just generate signed book forever and so notice that between this component here at step 10,000.",
                    "label": 0
                },
                {
                    "sent": "And the component that it actually inferred from the seat that it got is extremely strong structural correlation, right?",
                    "label": 0
                },
                {
                    "sent": "It has to set the exact right value at the exact right position here.",
                    "label": 0
                },
                {
                    "sent": "So this is a really really long range structure kind of problem if you like cause.",
                    "label": 0
                },
                {
                    "sent": "This value here determines that value here.",
                    "label": 0
                },
                {
                    "sent": "But of course it can solve it because it just carries forward a single computation which doesn't do anything fundamentally.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's just a sine wave, right?",
                    "label": 0
                },
                {
                    "sent": "So it can sort of like on a letter carry across and do the right kind of transformation such that it will arrive here, giving you the right value at the right moment in time that it has two in LCM is exactly the same thing in exactly the same way, it just say puts a value there and keeps it there until it needs it, so it carries it forward using A1 without doing anything, yes?",
                    "label": 0
                },
                {
                    "sent": "So you do, and actually this model doesn't actually.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I'm showing he doesn't actually do what it's supposed to do, and so this is not the plot generated from this type of model.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We as I said, because this model gram ourselves because we made it hierarchical and the way we made it hierarchical is by saying.",
                    "label": 0
                },
                {
                    "sent": "Well, a single transformation applied again and again is boring, so meh.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can do it.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slightly more, better slightly better model by assuming that the transformations can change themselves, right?",
                    "label": 0
                },
                {
                    "sent": "So now you can do deep learning 101.",
                    "label": 0
                },
                {
                    "sent": "You take your model in further transformation from 2 images and then you have further next transformation from two other images.",
                    "label": 0
                },
                {
                    "sent": "Now you get 2 transformations that you infer and now you can just stack the same model on top and look at how the transformations change overtime, and that allows you now to change dynamically to model things.",
                    "label": 0
                },
                {
                    "sent": "Say sine waves whose frequencies say change overtime or videos where some motion is followed by another motion in some regular way and stuff.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Numerical problems that might occur here.",
                    "label": 0
                },
                {
                    "sent": "Just because you have finite precision that might cause a blow up, even though you just multiply by E to the IFI.",
                    "label": 0
                },
                {
                    "sent": "Fall another category and it turns out that this tool.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your model is Abe.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To generate the sign.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes forever.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a little bit more control logic.",
                    "label": 0
                },
                {
                    "sent": "If you want right?",
                    "label": 0
                },
                {
                    "sent": "So this is just a mixture of experts with another layer of mixture components, that's it, and so it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Piece of control logic up here is enough to generate a sine wave forever.",
                    "label": 0
                },
                {
                    "sent": "Because presumably what happens is if there is something like a blow up thanks to an American precision.",
                    "label": 0
                },
                {
                    "sent": "We gave it enough training sequence so that it can sense those things.",
                    "label": 0
                },
                {
                    "sent": "The kinds of things that might happen on that particular machine or whatnot, and so it was trained to generate long enough to cope with that.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that was maybe.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, this is maybe the lengths that we train it for, but this is a test.",
                    "label": 0
                },
                {
                    "sent": "Wave, incidentally, and so on.",
                    "label": 0
                },
                {
                    "sent": "But this is maybe the length of sequence that we trained in foreign.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then it can go on forever becausw it won't see any numerical issues beyond the ones that it's on the training data.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit of time.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to mention, we did all kinds of variations of that you can actually go in and crazy and just take more and more and so go from velocity to acceleration and then from acceleration to what's called SNAP from snap to add.",
                    "label": 0
                },
                {
                    "sent": "Sorry, jerk and from jerk to snap and crackle and pop.",
                    "label": 0
                },
                {
                    "sent": "This is what these harder derivatives are called in physics and fun models.",
                    "label": 0
                },
                {
                    "sent": "That kind of learn hierarchical temporal structure in various ways and so on.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That works nicely on.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Videos that can learn shops, which is kind of sine waves that changes as frequency, which you would hope it can.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it can learn very.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other things like harmonics and.",
                    "label": 0
                },
                {
                    "sent": "Videos.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To some degree.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really excited to see that it was able to learn to nail the bouncing balls task because I wasted three months of my time.",
                    "label": 0
                },
                {
                    "sent": "As a professor.",
                    "label": 0
                },
                {
                    "sent": "Trying to get this to work.",
                    "label": 0
                },
                {
                    "sent": "And then I got it to work with the help of the two students.",
                    "label": 0
                },
                {
                    "sent": "Kishor and Vincent were also involved in this, and so I was really excited.",
                    "label": 0
                },
                {
                    "sent": "It works, it can.",
                    "label": 0
                },
                {
                    "sent": "Model videos that are throw balls that are bouncing around in the box.",
                    "label": 0
                },
                {
                    "sent": "Exciting enough for a scientist I guess.",
                    "label": 0
                },
                {
                    "sent": "It was exciting because no one else has been had been able at that point to do that.",
                    "label": 0
                },
                {
                    "sent": "Shortly after I was able to get these videos generated and it generates no problem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how many balls and how long in things that hasn't seen before and stuff.",
                    "label": 0
                },
                {
                    "sent": "But then June younger student here in the lab just joined in STM on the same task and.",
                    "label": 0
                },
                {
                    "sent": "I guess you can imagine what happened.",
                    "label": 0
                },
                {
                    "sent": "So it was disappointing to see that LCMS work as well.",
                    "label": 0
                },
                {
                    "sent": "As it stands, it's LST MSN grammar cells that are able to solve this task and.",
                    "label": 0
                },
                {
                    "sent": "My take home message from this is that.",
                    "label": 0
                },
                {
                    "sent": "What you need to get this?",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task solved is basically potentially useless computation.",
                    "label": 0
                },
                {
                    "sent": "Like a rotation or an STM, nothing computation.",
                    "label": 0
                },
                {
                    "sent": "Surrounded by some control logic which can turn that useless thing into something useful, and so so I'm at least happy about that that we at least got this to work and get sort of head.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this time there was one question there.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably there is.",
                    "label": 0
                },
                {
                    "sent": "We talked about that a little bit there.",
                    "label": 0
                },
                {
                    "sent": "There might be only a finite set of such videos actually, unfortunately becausw.",
                    "label": 0
                },
                {
                    "sent": "Well, because of the periodicity that appear here and stuff like that right?",
                    "label": 0
                },
                {
                    "sent": "We don't know how many.",
                    "label": 0
                },
                {
                    "sent": "How many of these videos really exist in reality?",
                    "label": 0
                },
                {
                    "sent": "Maybe not so many.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can just memorize all of them or something.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, I think this is probably not a very interesting data set because of that and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because of that mainly yes.",
                    "label": 0
                },
                {
                    "sent": "Well, you're up to numerical precision, but then things that are really similar can be summarized by a model like LCM, at least.",
                    "label": 0
                },
                {
                    "sent": "Nicely, it can just.",
                    "label": 0
                },
                {
                    "sent": "You know it can.",
                    "label": 0
                },
                {
                    "sent": "If it takes this image, it knows what it has to do next.",
                    "label": 0
                },
                {
                    "sent": "If it takes like take any snapshot, it knows what it has to do next.",
                    "label": 0
                },
                {
                    "sent": "If it sees a slightly different snapshot, maybe it's contract, if enough so it's going to snap to the same thing, so there are whole bunch of nearby videos frames at time T that get mapped to the same frame at time T + 1, and then you're back to the tragic Tory that the model learns.",
                    "label": 0
                },
                {
                    "sent": "And then you're kind of.",
                    "label": 0
                },
                {
                    "sent": "You can solve this qualitatively what we should be doing, I guess, is study this quantitatively, because what you see?",
                    "label": 0
                },
                {
                    "sent": "Is after sometime this video diverges from the test video that you see did it with.",
                    "label": 0
                },
                {
                    "sent": "And the amount of divergences going to tell you how well it can really model this exact test video that you're dealing with, right?",
                    "label": 0
                },
                {
                    "sent": "So I saw that at least for the LCM and and so you see this beginning and then if you see them side by side then after like 10 seconds they're just going to look completely different, right?",
                    "label": 0
                },
                {
                    "sent": "So it learns kind of it knows what it has to do, but it doesn't really model the exact test case.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, yes, that's correct.",
                    "label": 0
                },
                {
                    "sent": "That's correct, yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "Outside right?",
                    "label": 0
                },
                {
                    "sent": "But making that statement that basically so basically what you say is used on the manifold of those videos.",
                    "label": 0
                },
                {
                    "sent": "You don't want to be contract if you want to be orthogonal in some sense, you just want to move shift around the pixels as the class of videos does.",
                    "label": 0
                },
                {
                    "sent": "That basically, I would argue asks for.",
                    "label": 0
                },
                {
                    "sent": "Something like.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This you want to be able to model any orthogonal transformations between 2 frames or hidden states or whatever.",
                    "label": 0
                },
                {
                    "sent": "And two stuff accordingly rather than contracting.",
                    "label": 0
                },
                {
                    "sent": "And I my hunch is that in a lot of neural Nets you do not see those orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "You rather contract and learn things by heart, where what you really do should be doing is transforms like travel around manifold and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So the last thing I was going to talk about goes a little bit in that direction.",
                    "label": 0
                },
                {
                    "sent": "I don't have a lot of time left.",
                    "label": 0
                },
                {
                    "sent": "10 minutes I'm going to just give a summary of that.",
                    "label": 0
                },
                {
                    "sent": "It's another thing that just falls out of this full year.",
                    "label": 0
                },
                {
                    "sent": "Kind of you are just to mention you can extend this grammar cell idea.",
                    "label": 0
                },
                {
                    "sent": "And change the logic that's happening here a little bit together, things like.",
                    "label": 0
                },
                {
                    "sent": "Memory units and stuff like that very easily, but you make some of the units on which it operates hidden, and now the matrix that operates on these that also going to transformation can shovel stuff from the say visible pixels into the hidden's and it can shovel stuff out of those hidden's into the video and it can have things happening in those hidden's, and so it's fairly easy to just take this general idea of a mixture model if you want and turn it into something like little.",
                    "label": 0
                },
                {
                    "sent": "Call compute circuit that has a little bit of memory and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "None of this is explored.",
                    "label": 0
                },
                {
                    "sent": "We played with this a little.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that the model can kind of continue things in its mind and stuff like that.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But maybe I'll talk about that some other time.",
                    "label": 0
                },
                {
                    "sent": "So the last thing I want to talk about that comes out of this phooey view is again related to vanishing gradients and stuff.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said there are some approaches.",
                    "label": 0
                },
                {
                    "sent": "To deal with these problems.",
                    "label": 0
                },
                {
                    "sent": "Saying that you want to have this matrix being orthogonal matrix in itself is a strange thing to want 'cause you don't want the whole transformation from the whole hidden state to the next hidden state be a global big orthogonal transformation, right?",
                    "label": 0
                },
                {
                    "sent": "What you really want is little orthogonal transformations that can do things in some parts of the hidden state.",
                    "label": 0
                },
                {
                    "sent": "Maybe, and in particular if you if you run such a model, you will see that many of these units are actually zero 'cause the sparsity pattern of any reasonably trained neural net is very very.",
                    "label": 0
                },
                {
                    "sent": "Hi yeah, so you have a lot of debt units per training case, so typically only a few units here are going to be active and you don't want to global orthogonal transformation operating on this vector in which there are many zeros on.",
                    "label": 0
                },
                {
                    "sent": "So on what you really want is having those components orthogonal which are actually active together often right?",
                    "label": 0
                },
                {
                    "sent": "So what you want is orthogonal paths a through the network and not just distinct big orthogonal transformation that just helps you from gradient blowups.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the 2D subspace from the full from the Gabor components once again.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Right, that's sort of an orthogonal reason why you do not want an orthogonal matrix globally on this state.",
                    "label": 0
                },
                {
                    "sent": "On the hidden state, operating on the in state.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a component which is spanned by two orthogonal vectors, WKW air that you are familiar with now, so expensive little 2D subspace.",
                    "label": 0
                },
                {
                    "sent": "Of the image space and say you want to project into that subspace.",
                    "label": 0
                },
                {
                    "sent": "We talked about that before.",
                    "label": 0
                },
                {
                    "sent": "So say you have an image X and you want to approximate it in this 2D subspace using these two components using two coefficients AK&W.",
                    "label": 0
                },
                {
                    "sent": "Sorry L that you apply.",
                    "label": 0
                },
                {
                    "sent": "Can someone tell me what the coefficients coefficients should be for this particular pair of?",
                    "label": 0
                },
                {
                    "sent": "Components here what's the optimal?",
                    "label": 0
                },
                {
                    "sent": "Projection into that subspace.",
                    "label": 0
                },
                {
                    "sent": "If the WSR forming little orthogonal basis of that subspace.",
                    "label": 0
                },
                {
                    "sent": "Right, so exactly so the orthogonal projection into that subspace.",
                    "label": 0
                },
                {
                    "sent": "It's the dot product, right?",
                    "label": 0
                },
                {
                    "sent": "It's just W transpose XWK transpose, XW, transpose X that's the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "That's the best approximation you can do if you change this a little bit by adding or multiplying or something.",
                    "label": 0
                },
                {
                    "sent": "This value you're going to move away from the best projection, right?",
                    "label": 0
                },
                {
                    "sent": "But curiously, a lot of neural networks do not do this.",
                    "label": 0
                },
                {
                    "sent": "They do not use this kind of projection, even though we know these are the right components.",
                    "label": 0
                },
                {
                    "sent": "We know they're orthogonal in theory and in practice they have to be orthogonal anyway.",
                    "label": 0
                },
                {
                    "sent": "Becausw.",
                    "label": 0
                },
                {
                    "sent": "Of all those things that I talked before, and yet we use W transpose X plus bias typically to represent.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The projection into that subspace.",
                    "label": 0
                },
                {
                    "sent": "So let's take as an example an autoencoder.",
                    "label": 0
                },
                {
                    "sent": "An autoencoder literally minimizes the reconstruction error from such a subspace.",
                    "label": 0
                },
                {
                    "sent": "So it's the squared error, and the autoencoder is just a sum of decoder weight vectors.",
                    "label": 0
                },
                {
                    "sent": "Using the hiddens as the encoding.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly that situation.",
                    "label": 0
                },
                {
                    "sent": "If these students are reluz 'cause then you're just going to have a coefficient times the weight vector, and that's going to be the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we know the optimal coefficients would just be the inner product.",
                    "label": 0
                },
                {
                    "sent": "That's the best projection you can do, and what we also know is that in autoencoders, basically, whenever it was trained properly and it works and you get keyboard features is incredibly sparse.",
                    "label": 0
                },
                {
                    "sent": "So out of your 500 hidden's something like 50 hidden's are active, so you always do a projection into a subspace for every input example.",
                    "label": 0
                },
                {
                    "sent": "So we know we do a projection, we know we minimize squared error.",
                    "label": 0
                },
                {
                    "sent": "The best thing we can do is this.",
                    "label": 0
                },
                {
                    "sent": "Yet what do we do in erelu?",
                    "label": 0
                },
                {
                    "sent": "We do this, we compute.",
                    "label": 0
                },
                {
                    "sent": "What we should do and then add a constant and shift the reconstruction by that constant so we project into that subspace correctly and then move around in that subspace.",
                    "label": 0
                },
                {
                    "sent": "And then we train that whole network and hope that the components that we train become orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Of course they won't, because we're not allowing them to become orthogonal because we are messing with the encoding it should be using to become orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so one could hope that the biases are always zero and the problem kind of disappears.",
                    "label": 0
                },
                {
                    "sent": "But that's absolutely not the case, and in fact the biases have already another job to do, they cannot do.",
                    "label": 0
                },
                {
                    "sent": "Something that's good for the encoding, but the biases have to do almost always in any autoencoder RBM.",
                    "label": 0
                },
                {
                    "sent": "Many other models is they have to be very, very negative.",
                    "label": 0
                },
                {
                    "sent": "2 examples, just that I trained a couple of months ago.",
                    "label": 0
                },
                {
                    "sent": "You learn your basis and stuff, but devices are very negative and the reason is you have to be sparse, otherwise you will not learn as reasonable model.",
                    "label": 0
                },
                {
                    "sent": "If you force the biases to be positive.",
                    "label": 0
                },
                {
                    "sent": "These filters break and nothing works and the model just doesn't learn.",
                    "label": 0
                },
                {
                    "sent": "So the bias is already have a job.",
                    "label": 0
                },
                {
                    "sent": "They have to make the hiddens get sparse and in order to be able to do that they have to be very negative no matter which model you train.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we use this bias whose role we know is enforcing sparsity of the vector of responses and pack that into the encoding with which we reconstruct?",
                    "label": 0
                },
                {
                    "sent": "That doesn't seem to make sense specially.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the perspective of orthogonal subspaces and learning about the relationships and being steerable and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But most of us so I'm not going to go into.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other details here.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That motivated us to look at an activation function which.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have a bias yet.",
                    "label": 0
                },
                {
                    "sent": "Gives you sparse responses, so we take erelu and we just cut off its response.",
                    "label": 0
                },
                {
                    "sent": "If it's smaller than a certain value, that makes sure that only a small fraction of them are going to be alive after they saw an image X.",
                    "label": 0
                },
                {
                    "sent": "And then we say after you compute after you determine that you're going to be on, use W transpose X as the.",
                    "label": 0
                },
                {
                    "sent": "Coefficient for the decoder and nothing other than that.",
                    "label": 0
                },
                {
                    "sent": "That's very similar to Spike and slab models, which also try to separate the decision of being on from the encoding and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So similar to something called hard threshold.",
                    "label": 0
                },
                {
                    "sent": "I think this function is called hard thresholding function and it's also called coring and all kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "But The funny thing is if you take this.",
                    "label": 0
                },
                {
                    "sent": "Activation function training autoencoder using this activation function and do nothing other than that, no regularization, nothing.",
                    "label": 0
                },
                {
                    "sent": "Just train this.",
                    "label": 0
                },
                {
                    "sent": "It's 3 lines of code for the autoencoder, basically just a simple autoencoder is funny.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Salvation function.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And their variation.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you learn these beautiful features.",
                    "label": 0
                },
                {
                    "sent": "For example on 10,000,000 images.",
                    "label": 0
                },
                {
                    "sent": "So basically it just works.",
                    "label": 0
                },
                {
                    "sent": "That's the take home message.",
                    "label": 0
                },
                {
                    "sent": "And I really like to show this.",
                    "label": 0
                },
                {
                    "sent": "This results because you also get features that look like this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which some of you remember from the New York Times.",
                    "label": 0
                },
                {
                    "sent": "Maybe when Google used gigantic cluster to train a eight layer big convolutional autoencoder on YouTube videos and then showed that the model was able to develop features that look like this.",
                    "label": 0
                },
                {
                    "sent": "Now for us it's much easier to visualize those features.",
                    "label": 0
                },
                {
                    "sent": "We just have to show them right.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do any inference, this is just a single layer autoencoder.",
                    "label": 0
                },
                {
                    "sent": "You can just look at the features and this is what some of them look like.",
                    "label": 0
                },
                {
                    "sent": "It also.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks very nicely, so you can actually use those features in various pipelines.",
                    "label": 0
                },
                {
                    "sent": "Turns out on C5, so this is by the way of work, joint work with quiche or condo again, and David Krueger was sitting there.",
                    "label": 0
                },
                {
                    "sent": "And initially it didn't outperform a contractive autoencoder.",
                    "label": 0
                },
                {
                    "sent": "If you take your contractive autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Rip out its activation function and replace it by a no bias Relu activation function.",
                    "label": 0
                },
                {
                    "sent": "Then the contractive autoencoder actually outperforms this model a little bit, but if you use it, if you try to train a deep network and use an autoencoder to initialize that deep network layer by layer pretraining and so on, fully connected big network on Cifar 10, which is kind of an insane thing to do.",
                    "label": 0
                },
                {
                    "sent": "But we had reasons to do that because of hardware and.",
                    "label": 0
                },
                {
                    "sent": "Problems with confidence and stuff.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did too much better.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this and so we were able to push the performance on this task.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Twoo",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beyond the state of the art currently, and that's actually work by Joanne Lynn, who is also so.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're here.",
                    "label": 0
                },
                {
                    "sent": "So, so I'm not going to go into all the details of this, but the set of that recently was something like 63% on the permutation invariant, and we pushed it to 65 and.",
                    "label": 0
                },
                {
                    "sent": "And then if you give up on.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mutation invariants and train on deformations and stuff.",
                    "label": 0
                },
                {
                    "sent": "You go up to something like 80%.",
                    "label": 0
                },
                {
                    "sent": "The reason we did that is because fully connected networks.",
                    "label": 0
                },
                {
                    "sent": "If you remember Adam Coates talk are really important.",
                    "label": 0
                },
                {
                    "sent": "To study becausw of hardware cause dense high intensity arithmetic computation is much more.",
                    "label": 0
                },
                {
                    "sent": "Easily mapped to hardware than a convolution, which is a complete mess because of long range communication and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And so it was kind of pleasing to see that we get to something like 10%, ten, 15%.",
                    "label": 0
                },
                {
                    "sent": "Below are confident can do currently on this task is simply a fully connected net.",
                    "label": 0
                },
                {
                    "sent": "Sorry I'm rushing a little bit because I'm running out of time.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to stop here and take any other questions.",
                    "label": 0
                },
                {
                    "sent": "I already saw some over there, thank you.",
                    "label": 0
                }
            ]
        }
    }
}