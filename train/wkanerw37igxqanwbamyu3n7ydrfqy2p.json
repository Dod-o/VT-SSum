{
    "id": "wkanerw37igxqanwbamyu3n7ydrfqy2p",
    "title": "Building Machines that Imagine and Reason: Principles and Applications of Deep Generative Models",
    "info": {
        "author": [
            "Shakir Mohamed, Google, Inc."
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_mohamed_generative_models/",
    "segmentation": [
        [
            "Thank you, I'm very excited to be here today to get to share with you and re explore this part of deep learning and machine learning and I wanted to use this title of imagining and reasoning to sort of excite you and maybe to stimulate your creative juices about what the future of deep learning can be, and so maybe to start with, I would like you to all think about the reasons that excited you to be part of machine learning and deep learning.",
            "What is the reason that brought you here today?",
            "The ultimate goals that you want to achieve through machine learning and I think having met a number of you.",
            "For men"
        ],
        [
            "The sum of you want to understand the statistical and mathematical foundations of learning systems and what it means to learn from data for others.",
            "You are part of a new era of scientific discovery where we have very large scientific experiments and as a result we need even larger machine learning.",
            "Even more impressive machine learning to really understand the nature of the world.",
            "Some of you are innovators and technologists and entrepreneurs of tomorrow.",
            "And you want to disrupt and create new markets and.",
            "Some of you are on a quest to solve intelligence, and I'd like you now to think about what the components you think I needed that will form your ideal machine learning system.",
            "Thanks for thinking thinking.",
            "Obviously you are definitely in mind.",
            "What are the components of your ideal machine learning system?",
            "The components that will help you reach your ambition, and I think throughout this week.",
            "Definitely part of the solution involves the powerful supervised deep learning systems that you've learned earlier in the week which are now state of the art in machine translation object recognition speech to text.",
            "They involve the things he learned about yesterday, which involve reinforcement learning, learning from rewards, and you know, but they are."
        ],
        [
            "Some other things which haven't been.",
            "How can we learn systems that move beyond associating inputs to outputs or associating actions to rewards?",
            "We build systems that really understand the world that can synthesize different sources of information, fuse them together to really unpack how the world will evolve.",
            "Can we build a system that understands that there are objects in the world that there are factors of variation underlying these objects and they can be used to be manipulated and reason against?",
            "Can we establish concepts that are in the world and use these concepts for high level reasoning and decision making?",
            "Can we build ways that let our models know that something is new or surprising, and ultimately, can we build systems that help us imagine and generate rich plans for the future?",
            "And these are the places of chairman.",
            "Some models that I want to share with you today, but when we look at all the things you've learned about this week, supervised learning, recurrent networks, reasoning and attention and generative models, these will be part of the suite of what we'll call complementary learning systems.",
            "Systems that work together to really help us achieve.",
            "Whatever your ambition for machine learning was."
        ],
        [
            "So just some keynotes.",
            "When I write a function F the function F is always going to be a deep network of some form.",
            "I won't go into too much detail about its exact form, but that's what the function F is going to be, and you see all the topics that I'm going to speak about today.",
            "So what I wanted to do was leave you with some principles, some tools, and some tricks that you can use not just for generative models, but throughout machine learning, and give you a sort of different perspective.",
            "So some of the themes that I want to leave with you are the design of probabilistic models.",
            "Asian deep learning, stochastic optimization methods for reasoning and control and in some way everything.",
            "I'm going to talk about is about the problem of density estimation.",
            "So whatever you know about the problem of statistical density estimation, bring back to the front of your mind and then we'll use that to go for."
        ],
        [
            "So I have sort of six parts we might not go through all six parts, but the first part will be a Birds Eye view of generative models.",
            "What is the current state of the art of generative models then?",
            "I love dealing with models and there is a model for every occasion for whatever kind of system you want.",
            "There is a model that we can build.",
            "We're going to explore all the different classes of models that we have.",
            "Once you have a model and you have data, the way you integrate data into your model is through a principle of learning and inference.",
            "And there are two principles that I wanted to discuss.",
            "One will be about evidence estimation and the other one is going to be about 2 sample testing, and then we're going to take our models.",
            "We're going to take our inference.",
            "We're going to combine them to build different kinds of algorithms, which there are many ways to do that, and then we're going to use a case study of variational autoencoders to explore all the different ways you might want to build models, and they will try and summarize and look at things."
        ],
        [
            "So, OK, there is a huge diversity of generative models and."
        ],
        [
            "Want to take you through all of them so the simplest application is this one.",
            "It's data imputation.",
            "If you really understand the data of your world then you will be able to understand its underlying statistics and the classical problem is missing data imputation or removing noise 'cause data in the real world is not clean and so this is an example from a model called pixel RNN, which if you need to compete complete or fill in the missing half, these are the kind of completions it can do and you can see that good models allow you to.",
            "Explore the variation in the world that is actually plausible, and this kind of variation is actually what you need for powerful reasoning and probabilistic systems.",
            "But this might be the simplest one, and I'm going to go from simple applications to more complicated ones."
        ],
        [
            "The next type of thing is semi supervised classification.",
            "Generative models have a very important role to act in the service of supervised learning models.",
            "Ways to actually make them more data efficient or to be more active, better regularizers and so in the semi supervised learning problem you have very few labeled data points and lots of unlabeled data point.",
            "And how can you do this transfer of knowledge from the labeled data point to the unlabeled data points and the generative approaches?",
            "Right now one of the state of the art methods for semi supervised classification."
        ],
        [
            "Then we have generative models are closely related to the problem of compression and communication, and I'd like you to imagine we all have phones we all deal with videos and large images which we need to communicate over long distances.",
            "Generative models have the role for us to communicate and compress those data very efficiently and then send them, and we can even control to what extent we want to have this kind of compression.",
            "That's another useful."
        ],
        [
            "Application if we really want to understand scenes in the world, then one way of dealing with this is to understand its 3D structure.",
            "Understand that there are certain physical constraints, some intuitive physical notions of the world, and so this bottom video is the classic example of the Necker cube, where there are certain ambiguities in how you can perceive, and.",
            "View this kind of cube and then there's a completion program process where you have to take the left half an you have to see how it fits and it does very much, but a human does.",
            "It forms the rest of the cube and then realize it to fit with the rest of the queue.",
            "But you know, we can also move to other things of more complex 3 dimensional scenes.",
            "Think of a generative virtual reality where you can build and generate these kind of scenes and really interact with the world."
        ],
        [
            "So an important other aspect that generative models have is in the problem of rapid scene understand.",
            "So here is the scene of a camera looking over set and when you do reasoning in the scene, you want to very quickly know how many objects are in the scene, what kind of objects are in this scene and the kind of factors of variation associated with those objects.",
            "And so you can see the reconstruction.",
            "Here is the red part and the camera which shows that it's in the right position.",
            "It knows how many scenes they are, how many objects they are, and this bottom video is of.",
            "The learning process of such a model that is both trying to simultaneously learn the identity of digits.",
            "It's also trying to learn where these digits are, and it's also counting the number of digits and letting you know that in this case there are three digits in all of these images, so this kind of scene understanding will be very important."
        ],
        [
            "Like humans do, we want to have machines that when they seen you data one time or very few times, they can explore the data, understand their variations and use prior knowledge to really do that, and so in this task we are looking to explore the different ways generate alternative examples of the images at the top row, which the model has never seen before.",
            "A task as we call 1 chart generalization."
        ],
        [
            "So you saw a bit of this question yesterday that we can now use a generative model to actually build simulators of an environment.",
            "And if we have a simulator of an environment, then we can do very long term rollout.",
            "We can make predictions about what will happen.",
            "We can do counterfactual reasoning in some way, and then we can actually account for this when we do our decision-making as we go forward."
        ],
        [
            "Another kind of things this is the video of a 3 dimensional labyrinth, and if you and I operate in environments like this all the time, So what you need to do for control is form this state representation that was called yesterday.",
            "How can you form a representation of these States and use these states to actually do control?",
            "There's actually a control system happening here that uses a generative model to form the state representation, and based on this state representation, then actually does the control and the task is actually defined.",
            "The green Apple, which gives you a reward so."
        ],
        [
            "This kind of representation, though we wanted to actually be a little bit more structured, and so can we actually say that there are concepts within our representations that factor in interesting ways, and so in this video, this is of the Atari game called Seaquest.",
            "We explore a few different.",
            "You can see that generative models help you unpack different concepts.",
            "There is the concept of the oxygen bar, which is being filled out.",
            "There's the concept of the submarine moving up and down.",
            "A concept of the submarine moving left to right, a concept of lives which matter in this kind of game.",
            "And the ability to detect these kind of meaningful representations that allow us to do reasoning at the concept level will be very important."
        ],
        [
            "So yesterday Joel mentioned the topic of exploration, and in the typical everything that was mentioned about exploration in this game.",
            "In that article Montezumas Revenge, we usually can only explore 2 rooms.",
            "This is actually the structure of the game.",
            "There are 26 rooms and usually can only see these two by using a generative model.",
            "You can actually see and learn what is new and what is interesting in the world.",
            "You can equip your agents with that knowledge and using that using that information.",
            "You can then build a system like this that now let us explore 15 rooms in the world and so now Montezuma's Revenge is getting much better through the power of equipping generative models within the reinforcement learning setting."
        ],
        [
            "And finally, there's yes policy.",
            "Yes, the policy comes to key learning in this case.",
            "And finally there's the problem of how can we do temporal extra abstractions and learn make plans into the future, and you can think of generative models that actually have a role to play in helping you generate plans into the future.",
            "So this is a model of Miss Pacman an on the Y axis is the number of actions.",
            "There are discrete number of actions, and then this X axis days time.",
            "The white line sort of indicates where we are in the current point in time in this generative model actually generates plans into the future.",
            "You can see beyond the white line.",
            "There's a bit of a plan that is already formed, and then it has a way of switching between plans on the fly, so this is sort of the."
        ],
        [
            "State of the art of Generative models.",
            "We have models that allow you to do simple tasks from missing data, which helps you understand the structure of your data.",
            "To do semisupervised classification, density based exploration, representation, learning and concept learning, one child, generalization simulating environments, compression macro actions and planning.",
            "And I think there are many many other applications which are not listed here, but these are some of the ones that I think we've seen a lot of progress in recent time, so."
        ],
        [
            "Just to really emphasize the kind of progress that we've had in the last few years that Russ mentioned earlier, this is on the Emnace benchmark using the log likelihood measure an we would start off with simple models like mixtures of Bernoulli's with 500 clusters which weren't that good.",
            "But overtime we've done much much better models and under any measure we're just getting better an under any different kind of model.",
            "And today we have really good models that actually do really impress people.",
            "They inspire people you really think and then under different generative model, different data set.",
            "We have the same kind of this.",
            "The omniglot datasets.",
            "And so we're really making progress.",
            "Of course.",
            "Not every model can be measured against this log likelihood metric.",
            "So if you want to look at the actual kind of generations that they can do again the same thing."
        ],
        [
            "We really are making a lot of progress from simple things like VE that weren't very good three years ago when we created them to now many new models which actually represent the statistics.",
            "Then images have the good kind of structure, sharpness.",
            "So that paint."
        ],
        [
            "The picture, so for the rest of the talk I'm going to describe to you the way I understand and use machine learning.",
            "So there are three parts of my machine learning framework.",
            "It always starts off with the model.",
            "The model will embody everything you know about your system and that you want to work with.",
            "Once you have a model, you will choose a system of learning principles and you're learning.",
            "Principles will then get used to help you deal with your data and for any combination of learning principle and model you can form very different algorithms, and so we're going to do 3 parts to explore different kinds."
        ],
        [
            "So under models, we're going to consider three different types of models.",
            "The first one will be fully observed models, and you saw many of these models when you looked at our own ends earlier in the week.",
            "Then we're going to look at transformation models.",
            "You've all used these if you generated random numbers already, but they are very important class, and then the third class of models will look at our latent variable models that help you establish hidden causes and the structure of your of the data generating process."
        ],
        [
            "Then there's a huge smorgasbord of learning principles which you get to choose from, and different models make different principles more applicable, and you know you've used maximum likelihood and map estimation this week.",
            "You see lots of variational methods, and I'm going to try and put them together in a different."
        ],
        [
            "And then once you have a model and inference, you can combine them in many different ways.",
            "For example that you just saw with the restricted Boltzmann machine and maximum likelihood.",
            "There are many different algorithms to combine those two contrastive divergent parallel tempering natural gradient example."
        ],
        [
            "So OK, that's sort of just the first part an I think we'll just dive straight into into different kinds of models."
        ],
        [
            "So.",
            "The three types of generative models that I mentioned, fully observed, models, transformation models, and latent variable models, and you really need to think about what the problem is that you are dealing with, and depending on the problem, you will have to choose one of these.",
            "These kind of models and there are many design dimensions that you actually will deal with.",
            "You have certain data types, what kind of data do you have?",
            "Binary data, real valued nominal?",
            "Are these strings?",
            "Are the images?",
            "Are the graphs?",
            "There will be certain dependencies structures which you might want to model.",
            "Whether they are independent, whether they're sequential, where their temporal, spatial, temporal, etc.",
            "There will be different representations you might want to have you under have discrete or continuous or some kind of mixed representations.",
            "Is there certain dimensionality that you want to consider?",
            "Do you want to have parametric or nonparametric modeling?",
            "And then there are other aspects aspects of computational complexity modeling capacity, bias uncertainty, calibration of your datasets, interpretability.",
            "All these are things we sort of have to think about, and different models will trade these off in different ways."
        ],
        [
            "So the first class of models are called fully observed models and what defines a fully observed model is that there are models that use the data directly and they do not introduce other UN observed and random local variables.",
            "So parameters in a model are global variables because they are properties of the data set in total, but a local variable is something which is particular to an individual data point and so fully observed models do not introduce other variables that are particular to data points, right?",
            "And then.",
            "They need to be stochastic, otherwise you know.",
            "So that's the definition.",
            "So you've seen Markov models earlier in the week.",
            "This sort of generalize the entire class and the generative process is very simple.",
            "Here you generate the first dimension of your data from some distribution of categorical and this place this case given some parameters pie, you generate the second dimension given the first one, and so on.",
            "You generate the I TH dimension given all the other ones before.",
            "So this is the image of generating Pixar I in this image.",
            "Given everything in the blue case.",
            "And then that is the probability models.",
            "And when I write this function F you can put any kind of deep neural network with.",
            "In this case they can be fully connected, recurrent, etc.",
            "So there are many properties of these kind of fully observed model."
        ],
        [
            "And some of these properties include that you can directly encode the structure of the data that you have, which can be very powerful if you have certain knowledge.",
            "If you are modeling astronomical phenomena, then you have this kind of knowledge.",
            "Any data type can be used which is very flexible and powerful, and for directed graphical models.",
            "The parameter learning is extremely simple in these models because the log likelihood is immediately available and you do not need to introduce other approximations, and it's very easy to scale these kind of models up to very large systems.",
            "Over multiple GPS, very large datasets and we have many good tools for optimization after this in the UN directed the negative is that they have this ordering sensitivity which comes into play in the undirected case, though the parameter learning can be difficult.",
            "Yes, there is also a disadvantage sometimes like in pixel RNN, which is computation becomes very sequential.",
            "An on GPU's for example, that's not really convenient.",
            "Yes yeah.",
            "So there's this last point I'll make which is about this loan is.",
            "So for undirected models, just to complete that, the parameter learning can be difficult.",
            "The problem of the normalizing constant that we just saw in the earlier session and for all these models this generation can be slow.",
            "You have to iterate sequentially through the elements, use the Markov chain and it's not efficient of your computing resources.",
            "So depending on what your problem is, you have to consider these kind of things.",
            "These are some of the generations you can get from a pixel CNN model conditional on labels.",
            "So."
        ],
        [
            "So I wanted to explore the different classes of models in two dimensions, 1 dimensions, directed and undirected and the other dimension is discrete and continuous.",
            "And for any combination of these you'll probably find a model and this morning we saw about log linear models in Gaussian MRF's.",
            "The models I just mentioned are nonlinear autoregressive models and real valued made.",
            "Many people are working in this block of discrete and directed, made fully visible sigmoid belief networks.",
            "Pixar, NN, and their other models as well.",
            "So."
        ],
        [
            "Confirmation model let's take a very different approach.",
            "They say that you have an underlying source of noise, and this source of noise is going to be transformed through some parametric function.",
            "So this model basically describes everything and the image.",
            "I sort of want you to keep in your mind is of a sort of a system of pipes, and the pipes represent this function and what you can do with this function is you can adjust some properties of these pipes, the thickness and their length, and once you put into this pipe system source of noise and what comes out of the pipes at the end is going to be some new distribution.",
            "And if the function represented by these pipes is invertible, then you can just apply the rule for change of variables and you'll be able to actually know the distribution at the end.",
            "But you don't necessarily need to know this.",
            "We have the ability to create very arbitrary functions, and so one example of these kind of models would be what they call generating networks.",
            "This very simple degenerative procedure.",
            "You have some source of noise, a Gaussian for example.",
            "Then you push this through some nonlinear function and out at the end you get our samples from a new distribution.",
            "And this transformation can again be any kind of function, so these kind of models have it very different.",
            "Kind of set of properties."
        ],
        [
            "The sampling is very easy as you, so I just needed to take some random source of numbers and I get them out at the end.",
            "It's very easy to compute expectations in these models.",
            "There's the property that we call the low of the unconscious statistician that uses this, that if you want to compute expectation with respect to the final distribution, you just need samples.",
            "And again, because these models are going to allow us to use classifiers another very large scale systems, they can be scaled up to very large scale.",
            "But there are certain constraints which these models.",
            "Imposing a switch can be difficult to satisfy.",
            "It is very difficult to learn invertible neural networks or the optimization can be very challenging.",
            "And then there's an issue that unlike the fully observed models, there is no noise model at the end which may be an issue.",
            "So not having a noise model on the final output means that it's very difficult to extend to generic data types.",
            "If you wanted to generate binary data, this will be very difficult to do data in the real world is not.",
            "So if you needed to account for noise in the data.",
            "Then this is a bit hard to do, and since you do not have access to the final distribution, computing the marginalized likelihood which lets you do model comparison scoring can be very hard to do.",
            "Down is very easy to add noise in the visible if you want, but we don't do it because we don't want it because the real images don't have that kind of effect.",
            "In fact, we have done it, but then it becomes a VE laser variable models with noise models are directed models.",
            "Put it at the top, but you could easily put it anywhere you want.",
            "Is nothing in that in the algorithm that prevents you, but we've got.",
            "We've also done experiments where you put it in the top and the bottom, and what happens is it just learns to put the variance, and then at the visible light at the visible layer, OK. Yeah, so again, yeah this is a good point.",
            "You need to know, sort of what you actually want to do an depending on what you want to do, that kind of model you use will actually affect your choice.",
            "So this is from the conditional generative adversarial model.",
            "Understanding this is the way I understand it is images, sounds and things like that on the low dimensional manifold very near low dimensional manifold.",
            "As soon as you add isotropic noise like Gaussian noise, you get something that's not going to be on that manifold.",
            "Yes, and so.",
            "You should not have noise available, and for me it's it's a defect of things like both machines, Yankees and things like that.",
            "But you have to add noise in order to get a meeting with you.",
            "If you if you don't have that and you model generates pixel which is not just like you put zero probabilities and which can be maybe like it is not the right objective here.",
            "I think this is a very good point that we actually need to debate the fact that some points can have zero likelihood is a questionable statement, right?",
            "It's it can be dangerous.",
            "Again, you need to think about what you're doing.",
            "I like this class of models.",
            "As I said, we've all used them before."
        ],
        [
            "So the classic, there's two ways to really think about these.",
            "You can think about them in discrete time, or you can think about them in continuous time.",
            "If you have generated random numbers before, then those kind of models usually take a uniform random number and then give you a sample from a Gaussian.",
            "Those are called one liners and then you have the generation networks Gans and then you have other volume and non volume preserving transformations and in continuous time this rule for change of law variables is called the Fokker Planck equation or Kolmogorov equation.",
            "And then you get the equivalent in the continuous time domain, so there's a lot of good models to explore in this room."
        ],
        [
            "And then you have the last case, which is sort of yes."
        ],
        [
            "One more before.",
            "So the kind of a diffusion that you have to think is that the diffusion gives you a mechanism to do a flow of the probability.",
            "So they Gan defines this flow away of reshaping the probability to give you the thing at the end, and so these are all white diffusion models.",
            "Do Diffusions take an initial source of randomness, shape them over some path, and then out at the end you get, obviously, Diffusions will add extra noise at different points in time, but the thinking is at least conceptually the same."
        ],
        [
            "OK, latent variable models are going to actually specify a set of underlying causes that you think represent how your data is generated.",
            "So this is an example of what is called a deep latent Gaussian model, and degenerative process is again very simple.",
            "You start at the very top, generate some Gaussian noise, come down to generate some new random variables condition in the previous one, generate some new ones condition on the one before, and eventually generate some data from a noise source, which is what is not in maybe some of the other model, and you can have lots of other kind of skip connections in various ways.",
            "And all these functions, mu and Sigma are also deep networks, right?",
            "So there's this is what Russ mentioned by having these deterministic layers in between these stochastic layers."
        ],
        [
            "And so these ones also have a different set of properties.",
            "Again, like all the others, the sampling is very easy.",
            "It's a very easy way of including hierarchy and depth, and it's you know it's very natural to want to encode the structure you think underlying your model in this way, and the issue of the order dependency which came up maybe in the fully observed model is removed here, because integrating out latent variables induces dependencies, which means you do not need to think about this right.",
            "Latent variables provide some compression and representation of the data.",
            "And again you have access to the likelihood, which means that you can do scoring model comparison and model selection, right?",
            "But when you have an observed random variables in a model, then you have to compute the probability of that random variable given data and this task can be very difficult to do in general.",
            "The marginalized probability is itself not easy to compute an you might need to have some clever kind of approximations and the kind of approximations themselves need a lot of creativity.",
            "The kind of posterior distributions need to be approximated."
        ],
        [
            "So, but this is probably the largest class of models that's been explored more than 150 years of research, starting with Spearman when he created PCA, and you can think of a useful and interesting dimensions for me are the parametric and nonparametric distinction deep versus linear or just more direct non deep models and discrete and continuous?",
            "Most of us are sort of working around here.",
            "We're continuous, deep in parametric models where you have things like the ease and draw, but then you have other interesting.",
            "Models in various other cases, like you know, nonparametric deep and discrete.",
            "You know, hierarchical, dear slave process.",
            "All of these.",
            "The point is just to show that these are all the same class of model and the reasoning system you use for one can be used for the other.",
            "So there's a lot of different models to explore and wherever you find your interest, you can find a model that helps you fill that category."
        ],
        [
            "OK, so that's everything about models.",
            "If there are any questions about modeling, yes.",
            "Harbor freight in elaborate.",
            "So typically what you have to do?",
            "I mean there's a lot of work on an invertible neural network from the 90s, maybe you know more of this for me, So what you have to do then is there a set of constraints in the parameters that you have to have to make them so even in simple univariate functions this is difficult to do and the optimization can be very hard.",
            "You have to add extra constraints to make sure the gradients behave well, so we've tried it in some previous work and it's not great to do so.",
            "Yes."
        ],
        [
            "Yes.",
            "Yes, it's sort of in an ICP bills at nonparametric private, discrete random variables.",
            "An it is.",
            "The typical formulation is directed.",
            "You just have this one layer of parametric prior.",
            "Then that gives you the data, but you can have multiple layers of stacking this nonparametric discrete product.",
            "Think of a sigmoid belief.",
            "Natural, but the nonparametric equivalent maybe I have the wrong name, but they definitely exist, so I'll say I'm going to Add all the references in the back for all of these models and things.",
            "OK, any other modeling?"
        ],
        [
            "Question.",
            "No OK, so once you have a model to the process of combining data into your model, is the process of learning and inference, and I wanted to explore two principles of inference with you.",
            "So common inferential problems that you'll find in machine learning."
        ],
        [
            "Statistics include evidence estimation.",
            "Once you have data, you want to know what is the probability of your data.",
            "And once you know this probability, you can do a lot of things.",
            "Then there's sort of moments computation.",
            "They're sort of statistical quantities that you want to do for summarization, and then you want to sort of compute these moments in different ways.",
            "Prediction is the common one that everyone wants to do given some old data up to XT, you want to make a prediction of XT plus one or they can be entire datasets and sometimes you want to do testing as another inferential problem.",
            "You want to be able to look at two hypothesis and say which one should you actually use in the case of model selection for example, so first."
        ],
        [
            "Kind of principle for inference that I wanted to go about is on the model evidence, so the model evidence is the way of computing the probability of the data that you have, and we're going to look at the model evidence in the class of latent variable models, and you've seen much of this already, so maybe we can go through it faster and the model evidences which will use to score your model to do model comparison to do model selection to do moment estimation to do normalization to posterior compute computation and prediction.",
            "Once you have this single quantity, you basically know as much as you need to know about your data to do anything you want to do.",
            "So the principle involves taking steps to improve the model evidence.",
            "Given the data that you have.",
            "So the principle is very simple.",
            "Given data integrate out anything that is UN observed and use the value of X.",
            "Now this integral in general is not known to you, but the main principle is to transform this integral into an expectation over something simpler.",
            "An while you've seen that trick yesterday."
        ],
        [
            "And even this morning, so I'm going to start with important sampling and we're going to do a little generation.",
            "So this is the integral problem.",
            "There is common to almost every inferential problem that you have.",
            "Can we integrate out latent variables, Ed.",
            "But we don't know how to do this.",
            "What I'll do is introduce the proposal distribution, which will be called Q, and I'm just introduced a one, so it's just multiply by one, then I re wait this thing to introduce the importance weight that you saw earlier mentioned.",
            "And now this itself is an expectation of a different quantities and expectation.",
            "Under the distribution queue.",
            "So I can form this important weight which is WI can easily sample from Q and then the principle of important sampling is to evaluate this integral by Monte Carlo integration, which means just averaging the importance weights given these samples.",
            "So just a little bit of notation for the rest.",
            "Everywhere I'm going to write queue of Zed, but I actually mean Q of Z given X, but I'm just going to leave out the X for convenience and there are some conditions for important sampling.",
            "So important sampling is the simplest way and the most fundamental way of computing integrals.",
            "We use them everywhere."
        ],
        [
            "You can do a slightly different trick at the end, so here's our integral problem again, which we need to compute.",
            "We introduced the proposal.",
            "We look, read, match the terms to introduce the importance weight, and now instead of applying Monte Carlo integration, I'm going to apply Jensen's inequality that you saw this morning that says that the log of an expectation is greater than the expectation of the law, and now this gives a new quantity which I can rewrite this way, which is an expectation of cubes Ed under this.",
            "Log likelihood term and an expectation of QZ under this log ratio term, and this is the famous variational lower bound that we describe everywhere and that you've seen throughout the week, yes.",
            "No OK, OK, so if I could have seen the derivation a few times, so I'm."
        ],
        [
            "I just unpacked this.",
            "Quantity is called the variational free energy and it has a number of useful quantities.",
            "There's this distribution Q event which will be the approximate posterior distribution.",
            "This Q of zed represents your current understanding of the posterior distribution of the latent variables given the data.",
            "What makes this other term, which is the reconstruction term log likelihood terms, are reconstruction terms that this helps you measure how well you are matching the data that you have.",
            "And then there's this penalty term which naturally arises.",
            "You do not have to design.",
            "It is derived for you and it is the way of introducing Occam's razor into your problem and dealing with model complexity and penalization.",
            "So."
        ],
        [
            "So this one here is being very familiar to you this week, but there are various other kinds of variational bounds.",
            "The one you just saw from Russ this morning was the multisample variational objective of the importance weighted objective.",
            "Can see the top one is a special case of this.",
            "Then you get sort of linear variational objectives, which is a generalization of the previous two, and then you have other kinds of variational families as well.",
            "But the point of these kind of variational families is that the solution for all of them, the solution Q of Z, is the same for all of them, which means find the queue of zed, which is as close as possible to the true P of Z given X.",
            "So you can choose any of these kind of bonds that you like.",
            "The solution in the end is the same.",
            "Some of them make optimization easier than others, or let you unify different ways of thinking so.",
            "OK, are there any questions and sort of variational inference and this sort of part of the store?",
            "OK, so the second principle for inference that I wanted."
        ],
        [
            "Look at is on 2 sample testing, so for many models you do not have access.",
            "You only have access to an unnormalized probability or you have just partial knowledge of the distribution.",
            "You really don't know anything you know the noise source at the beginning and you know know the function but not much else.",
            "So a different way to do this is then just to compare two distributions.",
            "And so the principle of learning here is that we want to compare a distribution of training data with the distribution of some other data source and can be a test data set.",
            "It can be a data set that we generated from our model.",
            "It can be anything, right?",
            "So the principle of two sample testing will be to say that I want the probability of my training data set to be the same as my distribution under the data set I generated, for example.",
            "So you want these two probabilities to be one.",
            "Or you want them to be the same right?",
            "And the reason you want to do this is because you might not be interested in computing the marginal probability.",
            "And if you aren't interested in computing the marginal probability, then this problem here of computing a density ratio is a far easier problem, right?",
            "And so the trick of density ratio estimation is going to be to transform the problem of density ratio estimation into the problem of class probability estimation.",
            "And that is something we know very well how to do so OK."
        ],
        [
            "A bit of notation just to set up.",
            "I have two datasets X hat an X~ which I'm going to combine together into some new data set, which I'm just going to call X. I'm going to use the random variable X to denote that combined data set.",
            "I'm going to assign some labels also to this data set, so I'm going to introduce plus one to everything from the data set that was X hat and minus one to everything from the data set there was X~ and then there is this equivalence that the probability of X~ is just the probability of the combined data set condition on the label one, and similarly for X~ is the probability of the total data set condition on the fact that you have the label minus one right?",
            "So this is just the setup of our problem.",
            "I'm going to write two things.",
            "I'm going to remind you about problem, which is the problem of density ratio estimation.",
            "And the second problem, which is just going to be based rule which you need for everything.",
            "So let's start the conditional problem that we started with is to compute this conditional density rate.",
            "This density ratio, which you can right because of this equivalence that we have as the ratio of two conditional probabilities P of X given Y equals class one and P of X / P of X given Y equals class minus one.",
            "Now using Bayes rule I can replace the terms of these conditional distributions.",
            "So I do a base substitution and now I'm going to get these two terms here and now I want to do a bit of a cancellation, so P of X is the same on both sides, so I'm just going to remove those two terms.",
            "Now I'm going to assume that the data set that you have from X~ and the data set that you have from X half equally balanced.",
            "But if they are not equally balanced, you can address that imbalance by these two probabilities at the bottom P of y = + 1 and P of Y = -- 1 but.",
            "Let's assume it's equal, so they also cancel and then basically what is left is this this quantity.",
            "It says that the density ratio we are actually interested in is actually equal to the ratio of the two cloud conditional probabilities, right?",
            "And because so that is the main point, I want to leave with you the problem of computing density ratio estimation is equal to the problem of class probability estimation.",
            "And now what is the point of all of this?",
            "So basically, let's try and learn this classifier at the top so."
        ],
        [
            "There's a scoring function now I need to learn the probability P of y = 1 given X, so I'm going to create a scoring function D which is theater of X.",
            "Then obviously 1 minus that quantity is, the other is the class of the negative term because I'm doing binary classification, the Bernoulli outcome is going to tell me that the log likelihood is going to be log of D of the ex hatch data set minus log 1 -- D of the other data set, right?",
            "And so I'm going to develop basically a 2 sample criterion.",
            "Which is going to tell you, you can optimize this loss function where X hat is one data set, an X~ is another data set.",
            "These two can be any different data.",
            "One can be a training data set and the other one can be a data set which you generated from your model and so now how can we use this kind of criterion in different ways?",
            "So typically when we do generative, I've assumed up to this point that xgen and X abzar fixed datasets, but when we do learning we xgen is not fixed.",
            "It is also something that we are learning on the fly.",
            "So in the case of transformation models, for example, where you have Z pass through some function and you get X Ed, we can replace X Gen in this 2 sample criterion.",
            "With this function F directly right?",
            "So now the two sample criterion leads us to what ended up in the adversarial networks, which is that you get this loss function, which tells you to take the expectation of the log of your scoring function under observed data an under the other other day.",
            "Yes, the second equation is missing some indicator functions to say that.",
            "You're talking about the coming from one class with another class, right?",
            "Well, this equation here.",
            "Second, yeah, yes, I've written here X, as in X~ which already is.",
            "The indicator is implied in there.",
            "If I written it up in the previous notation of X given Y one, then there would be a D in front in a 1 -- T at the front.",
            "But yes, it's just simplify Rota XNXX tilted directly so you would see the connection OK, so now this loss functions are very different beasts from the variational loss function you saw before.",
            "The variational loss function was a lower bound, which means that any optimization you do on that bond always guaranteed to improve the bound, so it gives you a tool for debugging this kind of loss function is sort of playing a game with itself.",
            "A classifier always wants to find a clear decision boundary to tell you what is generated and what is not.",
            "But we actually do not want the classifier to work this way.",
            "We want our classifier to have probability .5 for generated and two data points, so then.",
            "Basically, we attain this alternating optimization principle, where we maximize the classifiers parameters Theta and we minimize the classifier parameters Phi, right?",
            "But this is just in the case of these transformation models.",
            "If you are working other classes of models like undirected models are directed models this same 2 sample criterion will be called noise contrastive estimation.",
            "Of course, if you are doing it more statistical theory then 2 sample ratio density ratio estimation with something you do a lot of the time important estimation that you see in reinforcement learning.",
            "Comes up all the time and you can use this as a solution to that problem as well."
        ],
        [
            "OK, so are there any questions on maybe 2 sample testing evidence estimation or learning algorithms in general?",
            "OK, so let's go on.",
            "So now that we have models and we have inference methods, we're going to put them together and we can build different kinds of algorithms, and I want to look at about 3 different principles for building algorithms.",
            "There will be stochastic approximation, amortized inference, and stochastic optimization.",
            "You've seen all of these already, yes?",
            "Fit like CD training of GBM from that.",
            "Yes, I think you could.",
            "But well, yes you can because they do it in the noise contrastive estimation.",
            "That is exactly what you do for log linear models, so I don't know if it works that well, right?",
            "It's it's not bad, it's actually as good as doing anything else, but typically people do it in the language modeling case, which is, which is a good good use case for that.",
            "OK, so the classic thing."
        ],
        [
            "That you learned in the textbooks of machine learning was about the EM algorithm and the EM algorithm will be the basis of everything.",
            "If you know this, you'll know almost everything in machine learning, so.",
            "Yes.",
            "I'm going to do exactly that, so the EM algorithm basically operates.",
            "This way.",
            "You have a loss function as follows.",
            "An EM algorithm is just an alternating optimization between two sets of parameters, right?",
            "So the variational EM is actually a super set of the standard EM algorithm, so I just need to talk about only variational Emmanuel, understand them so the alternating optimization of model parameters and distribution parameters or variational parameters, right?",
            "So any EM algorithm always works as follows.",
            "You write a for loop and then you repeat eastep computation, compute the gradients of the function with respect to the variational parameters Phi.",
            "Then you go to an M step.",
            "You compute the gradient with respect to the model parameters Theta, and then you do this in a loop and you iterate and then because the EM algorithm is always optimizing abound every step improves this free energy up until you can't learn anymore from the data.",
            "So."
        ],
        [
            "The let me go and unpack this EM algorithm in a little bit more detail.",
            "So again they EM algorithm always says repeat the following steps and then you do your eastep you say for I = 110.",
            "If you've written in EM algorithm, you can remember writing this in your code.",
            "Then you do the gradient update step for the variational parameters 5, which is compute the gradient of this quantity F with respect to Phi Phi lives in the term Q.",
            "Then you do that for all the datasets and once you are done you store all the files in a table.",
            "M Step update which says computes the Theta using all the files.",
            "This expectation of a fire requires you to have all that knowledge.",
            "So the key point that I wanted to bring out here is that here you do a sum for all the data in your data set.",
            "And here there's another sum for all the data in the data set.",
            "So in the 90s we could do because N 100 or 1000, but in 2016 an is 10,000,000.",
            "This sum is not possible, but this is the exact gradient.",
            "So instead what you can do is replace N with something smaller.",
            "A mini batch of data instead, so this is called the stochastic approximation technique and because you re sample N data points from the full data set, this introduces noise, which is why it's called a stochastic approximation, and this has various names online.",
            "EMS the classic approximation, am stochastic variational inference and it is the main tool that you use for stochastic gradient descent, right?",
            "Annual do this now.",
            "Basically this is doing stochastic gradient descent for these two terms, so there's one other sort of problem with this with this setup.",
            "In the eastep so.",
            "If you."
        ],
        [
            "Look here at the eastep.",
            "This eastep is what I typically Cora memory list step.",
            "So for every data point equals one to N you compute 5 and buy some optimization an you stored at five in the table.",
            "When you compute five of one, you compute that optimization computing Phi 2 does not use any knowledge of fire one.",
            "The fact that you've computed this before you redo it again and you do this for all the data points.",
            "Every file that you compute users.",
            "No knowledge of the fact that you've already computed some files before."
        ],
        [
            "So what you can do is now replace this entire East step by using a model instead.",
            "So this is the principle of amortizing some computation into a model, and so this is how the principle of an inference network kinda get introduced into a probabilistic modeling scenario, right?",
            "So you have you introduce AQ Network, which is now an encoder which will basically summarize all the data points and allow you to reuse and transfer the fact that you've done eastep computations between them.",
            "This inference network introduces a set of global parameters into the model that's used for testing train, and I call this amortizing because there's this dark knowledge.",
            "If you remember this dark knowledge notation that gets entered into this network that helps you spread the cost of learning eastep learning files for every data point, right, you can now do a joint optimization, which is very nice, and so the point is that inference networks provide you an efficient mechanism for posterior inference with memory.",
            "In these kind of latent variable models, for example, or any kind of Bayes net."
        ],
        [
            "So OK, amortized variational inference then basically takes the idea of using these kind of inference network within the variational inference setup.",
            "So just to remind you again of this free energy bound we have expectation of QZ, which is the approximate posterior.",
            "We have our reconstruction term in our penalty term then basically by introducing an encoder network we form a stochastic encoder decoder network.",
            "So basically any stochastic encoded decoded system will effectively implement variational inference.",
            "And so basically what you have the model, you can now call a decoder, which is your likelihood function.",
            "Your inference network is an encoder which takes data X and gives you said.",
            "And basically you can also see this as a way of transforming an autoencoder into a generative model.",
            "Right, so when we have a specific combination of variational inference in latent variable models using inference networks, this specific combination of these three things gives you an algorithm that we refer to as a variational autoencoder, but always remember what the model is that you are using and what kind of inference for any model you can replace it with a different inference and you get a different kind of algorithm.",
            "So."
        ],
        [
            "So I wanted to just connect this to the problem of compression.",
            "Since we've established these encoder decoder systems, so in minimum description length compression you basically have exactly the same loss function where Q XD will be a stochastic encoder.",
            "Then you have a term which represents the data code length, which is what you want to measure and then you have a hypothesis code which is what you want to produce.",
            "And again this is a different kind of stochastic encoder decoder system and the principle of minimum description length ask you or requires you to.",
            "Except that because there's regularity in the data, the latent variables can be used to compress this data, and so the minimum description length ask you to find the shortest message for your given data and the shortest message is this marginal likelihood bound.",
            "So there's the encoder decoder system and the approximation to the ideal message, which you can't compute is this variational bound."
        ],
        [
            "I just wanted to connect to other kinds of models that you might be using, so message passing is another very important inferential approach, which is very different from the approach that I just described.",
            "And typically you have a factor graph of this kind and the factor graph has certain factors in their assumptions, usually a factorized assumption on these graphs and the memoryless inference approach is just to basically do exactly what we are doing in the East step of the EM algorithm, which means compute this argument, which in the case of cavity methods involves computing the projection and there re optimization of the distribution.",
            "The amortized inference is very simple.",
            "Replace their computation within neural network and then you know things will workout the function.",
            "In this case can be anything can be a tree, can be a neural network, can be a set of basis functions, but the principle of amortization is more general than that, again, you."
        ],
        [
            "Use the same kind of principle for predictive distributions.",
            "If you are thinking about building Bayesian neural networks, Bayesian neural networks are different.",
            "You have data X which you observed and why we have observed and what is random in the parameters of that distribution.",
            "You typically need to compute some predicted probabilities and the memory list prediction basically asked you to do Monte Carlo integration.",
            "Again, the amortized inference is related to the idea of distillation if that has come up during the week of user deep network distill this Monte Carlo computation into a neural network so you can do that as well."
        ],
        [
            "OK, so the last bit in this section I want to talk about was stochastic optimization, so this common gradient problem you have seen everywhere this week it is compute the gradient with respect to some parameters of an expectation of a function.",
            "This is the problem of variational inference.",
            "It is the problem of the importance weighted autoencoders.",
            "It is the same problem that you have in reinforcement learning in variational inference is the log likelihood.",
            "In iOS it's also the log likelihood in reinforcement learning, F is the return of the reward function.",
            "It's basically the same problem and in other areas you know this probably is one of the biggest problems you have in computational science, so this sort of two terms is really difficult to compute this thing a you don't actually know this expectation.",
            "If you knew the expectation, then things are very simple and be the expectation is with respect to the gradient you want is with respect to these parameters.",
            "Phi, which live in the distribution you want to take the expectation with.",
            "So as I said that lots of problem areas, generative models.",
            "As I'm describing, is one of them reinforcement learning that you saw yesterday is another one.",
            "Operations research and inventory control has this Monte Carlo simulations of weather phenomena has this problem.",
            "If you want to do asset pricing and finance, then you will also see this problem.",
            "So it is really the same problem everywhere, so there are typically two approaches.",
            "You can take a deterministic approach, which means introducing some deterministic bound on F, and if the bond is deterministic and you can integrate each of these things, then the problem is very simple.",
            "In the 2000s they would call these local variational methods.",
            "People don't like them as much anymore because.",
            "Finding good deterministic approximations can be quite hard, but the one that we prefer these days are stochastic methods, so they compute this expectation by Monte Carlo by exploiting properties of these distributions and these functions right?",
            "So there are two ways of doing this.",
            "You can either take the derivative of the function F, in which case we're going to call that the pathwise estimator, or you can take the derivative of the function Q, in which case we're going to call that this core function estimator, and they have very different properties and different ways in which you can use them so."
        ],
        [
            "So you seem all of these yesterdays.",
            "I'll go through very quickly.",
            "The score function estimator says I'm not going to make any assumptions about FF can be non differentiable, but Q should be something that is easy to sample from.",
            "So what I can do is then.",
            "Transform this quantity into this this new gradient estimator is actually very simple graded estimator.",
            "You saw it yesterday by using the likelihood ratio trick.",
            "This gradient is a is very interpretable.",
            "It's very intuitive.",
            "Actually, it's a very randomized search, So what the gradient tells you to do is choose some zed at random from everywhere that you see.",
            "And now I'm just going to evaluate how good this random said is, so I'm going to look what the cost of doing that zed is.",
            "So take F of dead and look how good it was.",
            "If it was good, F is going to be high.",
            "If that said was bad, F is going to be low, and then I'm going to say, well just take the gradient with respect to that, then and then re weighted by the cost.",
            "So it's a very randomized.",
            "Sort of search, but you know this is what you have to do if you don't want to make assumptions and so you can see that the variance of these kind of estimate is going to be very high right?",
            "F of Zed is typically going to be a sum of K terms and so the variance of this kind of estimate is going to scale linearly around order of K. So but you can do a lot of tricks to make this manageable, so it has many different names in the literature.",
            "The likelihood ratio methods is what you will see in reinforcement learning, reinforcing policy gradients.",
            "Automated inference is 1 black.",
            "Box inference is another method one.",
            "The other way is instead to use properties of Q&F directly, so F is now a differentiable function if F is differentiable, then you can actually use a little bit more information, so I want you to recall those transformation models again, which was this piping system that we have that takes noise and gives you random variables.",
            "So if you have this, you can re express this in rather in the system of pipes are saying zed can be expressed in terms of a deterministic functions Ed with some random noise epsilon so.",
            "Then what you can do is wherever you see zed in this aspect expression, just replace it with G, right?",
            "And this is the expression that you get, and it's very.",
            "It's very intuitive because all this expression tells you to do is just do back prop.",
            "Just take the gradient and push it all the way through all the functions that you have.",
            "And because of that reason in an earlier paper, we thought you called this stochastic backpropagation, but it has many different names.",
            "If you look in this finance literature or stochastic optimization literature, they will typically call this perturbation analysis.",
            "The reparameterization trick which Russ mentioned, I find independent inference methods right?",
            "So these are the two estimators that you get to choose from.",
            "There are other classes, at least two other classes of stochastic gradient estimators, but these two are the most general ones in the most useful ones for problems in machine learning.",
            "And so when you combine these kind of stochastic gradient estimators with estimators that use mini batches, then you get what are called doubly stochastic estimation problems, right?",
            "And you have to sort of control."
        ],
        [
            "The noise.",
            "So great, I have half an hour left.",
            "Are there any questions on that section?",
            "Yes.",
            "Five, yes, both estimators are biased in this case.",
            "And then."
        ],
        [
            "Very easy.",
            "To see to see in both cases, because in the in the pathways estimated because you can undo, you get back the same the same integral that you had before, and in this case here, because you can always re weight.",
            "In both cases there we actually use the property there unbiased to do that you can create unbiased estimators which also work.",
            "But you know people don't like unbiased estimators.",
            "To know that.",
            "You need to.",
            "You need to make you need to know what Q is and you need to exploit that knowledge.",
            "If you aren't willing to exploit that knowledge then you can only use the other estimator, right?",
            "It's all again about knowing your problem and deciding what is best for your problem, OK?",
            "So I wanted to use a case study."
        ],
        [
            "Our variational autoencoders.",
            "But this can be for any kind of generative models to explore different ways of building these kind of models and different assumptions that she can make."
        ],
        [
            "So as I said, a variational autoencoder is the principle of amortized inference applied to latent variable models, and this is the loss function that we have, which is this variational free energy and we've already introduced that way of dealing with this is by building a model and an inference network.",
            "Right and there are many design choices that you have to make.",
            "You can decide what kind of latent variables you're going to use.",
            "Are they discrete?",
            "Are they continuous?",
            "Are they gaussians?",
            "Are there bernoulli's?",
            "Are there some mixtures of things?",
            "This is something you have to decide.",
            "You have to choose what kind of model or likelihood function you have.",
            "Are you happy to deal with ID data?",
            "Is there some sequential aspect to this data?",
            "Are there temporal properties you need to model?",
            "Are there spatial properties and all of this knowledge you need to build in, and then this inference network you need to alter disid some properties.",
            "What kind of distribution does it have?",
            "Can it be multimodal?",
            "Is it sequential?",
            "Does it have spatial properties?",
            "So all of these things you need to do, but the things you do not need to choose because this is what we'll do for large scale deep learning is will always use stochastic gradient descent and will always use stochastic gradient estimation.",
            "These two properties of inference that we just did."
        ],
        [
            "So I just wanted to make a quick mention about how you implement these things.",
            "These days we have many different tools which make implementation easier, so we have Theano and Torch and Tensorflow and stand.",
            "All of them are easy and if you are doing message passing then infer.net is a good one and so you have a forward pass which is sort of as follows.",
            "You take data X and you pass it through a box which is some function.",
            "And out of this function needs to come a sample of the latent variable and the computation of the entropy.",
            "So this is basically the function that you need to compute.",
            "Which will call the recognition model or the encoder.",
            "Then you need to implement two other things.",
            "You implement another box which is the prior.",
            "Typically the prior doesn't have any parameters, so it just needs to evaluate the log log probability of a Gaussian and then you need to implement one last function, which will be the model and the model.",
            "We're actually put all this likelihood computation and structure in there.",
            "Yes, in principle you don't need to have another big expression for the entropy.",
            "You could also sample it if you have Q.",
            "Yes, yeah it can, this how HQ gets computed can be by Monte Carlo or.",
            "If you know it, it can be done exactly so, but once you put these things into a computational framework like torture, Theano, Tensorflow, you immediately get back the backward graph and the backward grapher.",
            "Then when you go backwards, computes the gradient computer grading on the product gives you the inference, so all the pictures that I'm going to draw are drawn in boxes of these three types.",
            "Of that you see exactly how you implement it and what the flow of the gradient is like.",
            "Because the flow of the gradient matches depends on the type of estimated for the gradient shoes.",
            "Ideally what you wanted some kind of.",
            "Probabilistic programming that uses variational inference in this case, and I think we're very close to that given the tools that we have, right?",
            "So the standard things are always the case.",
            "Stochastic gradient descent and precondition optimization you'll use, like RMS proper Adam, you implement your code in Jeep's of some distributed clusters.",
            "And because these probabilistic models are modular, when you code it up, this prior itself can be an entire via E, and so you can build via modules.",
            "You can use that in a prior of another via module.",
            "And you can make a very complicated codebase this way."
        ],
        [
            "OK, the simplest kind of VE that you can deal with this sort of this latent Gaussian VE an the prior is just a Gaussian distribution and so basically you implement and evaluate of a Gaussian probability which is easy to do.",
            "Then you have a model P of X given zed, which is very simple.",
            "It says just take give me the probability of X given some function of those latent variables and the function.",
            "In the simplest case being via Gaussian.",
            "But this can be any distribution, exponential, family or otherwise, and it has some parameters Theta and the recognition network is going to be equally simple.",
            "It's just going to give you a Gaussian with some nonlinear functions from yuan for Sigma, and when you plug this into automatic differentiator, you'll get the gradients of all of these things, their model, and the inference can be fully connected, or it can be convolutional.",
            "This is easy to do, and so when you read in supervised learning all the latest advances they have in building models, when you see things like batch normalization, dilated convolutions, and skip recurrent connections and whatever you plug them.",
            "Into these models, and you build better models that way, and so there is sort of this cycle of life which happens in machine learning through all parts, right?",
            "So all the functions are dinner."
        ],
        [
            "So these kind of latent Gaussian vieze can do a lot of interesting things.",
            "So the first one I mentioned was about this visual concept learning, so they can dissent.",
            "Angle input data in interesting ways.",
            "I showed you the video, it's not playing here.",
            "About how it learns, separate the fact that there is an oxygen bar from the motion of the of the submarine, which can go up or down.",
            "These latent variables can help you do visualization of very high dimensional data.",
            "Explain how these images were obtained by moving in that space, yes?",
            "So yeah, basically you take zed with 10 latent variables.",
            "You clamp nine of them, and then you very one, and then you look to see exactly what happens and then you see one of them deals with oxygen bar.",
            "Another one is just with the motion up and down.",
            "And I think this is a nice property that you want out of systems that reason yes.",
            "How can you do it if it's.",
            "If it's a prior, the Gaussian once again arbitral big things.",
            "So yeah, so in that particular case what you do is you introduce an additional penalty onto the scale constraint that reduces its contribution, and then what comes out is sort of these kind of contributions.",
            "But yeah, Gaussians, but once you have a fixed model that when the parameters are fixed, then this problem disappears, right?",
            "'cause then, for a fixed model then permutations don't exist anymore.",
            "I mentioned visualization of very high dimensional data.",
            "And in this particular graph, what is very nice with these kind of models when you can actually evaluate the probability is that you can plot all your data and you can resize the shapes by the contribution of their marginal likelihood.",
            "So data points that have a great deal of contribution have very large shape and things near the decision boundary have very small contribution to the probability.",
            "And in this video here is sort of just exploring this dimensions of MNIST videos.",
            "Let me see if it plays yes.",
            "I just I didn't quite understand your answer to this question is we know the Villa already does this in tangling all on its own.",
            "Yeah, so that wasn't enough, so for that that's not enough.",
            "'cause face these kind of games have a bit more other properties.",
            "They are very deterministic.",
            "So then you want to sort of emphasize the deterministic aspect more so you down weighted scale term relative to the to the reconstruction term and then you can sort of see these kind of structures come out.",
            "So was that your answer to get?",
            "Yeah, different dimensions to not be rotation variance.",
            "Yes, at least in this case that's just one simple way of doing it.",
            "Otherwise, as you said, they can do them themselves and faces data you don't have to do anything special.",
            "Are there any other questions?",
            "While we understand why playing with the strength of the kilt room has anything to do with the rotation in that space?",
            "So what this happens?",
            "Yeah, this is, uh, I'll tell you, we could talk about this often.",
            "A big debate.",
            "Hey."
        ],
        [
            "So I wanted to mention VA representation so you can learn a latent variable model of these data set.",
            "This environment is called Labyrinth and the task that you have to do is called forage and avoid.",
            "You need to go and collect the green Apple Dan you need to avoid the lemons right and what you do is you learn of EAE and these latent variables now form the representation and you store these representations in what is called an episodic memory, and episodic memory is basically AK nearest neighbor.",
            "And what gets stored in this K nearest neighbor memory is the actual representation.",
            "The return that you got from that episode in the data as well as the action that you took.",
            "And it always stores the best return.",
            "So this is a system of episodic control, and this is one way that generative models can help in the problem of control, yes.",
            "So in this case that you know, one thing that the NRL people are grappling with is that if you just train good models that they produce the dynamics of the environment is not enough to really have anything with models where the reward is yes.",
            "Right here as opposed to just the distribution of the data information.",
            "So the generative model is only used to give you the state representation.",
            "Once you have the state representation that gets stored along with the reward, not, there would actually the return for that episode, right?",
            "The return does not inform the state representation.",
            "In this case, it would be nice to have maybe something that does inform the state representation, but you also sort of an exploration to see if how modular kind of system we can build in this case, But in this particular case the return is very tight.",
            "Yeah yeah, because at least in this kind of simple or in that I've made navigation environment.",
            "In this case projections, random projections also work, they just don't work as well.",
            "The have an experiment that compares random projections versus VH to look at exactly what other kind of sources you can do.",
            "Propagate through the state representative.",
            "Yeah, you want to have a model that's part of the learning system and you know it's very difficult to differentiate through K nearest neighbors, and so this is the problem.",
            "But because we're doing lots of research that you saw earlier in the week about memory systems, I think this will also get addressed very soon, so this is the problem that we call episodic control just to explore.",
            "One way of using generative models for control prop."
        ],
        [
            "So one of the problems that comes up from these latent Gaussian models is that the real kind of posterior distributions that you see are very complicated.",
            "They have distributions that look like this, and like this, and if you use a Gaussian that might be OK.",
            "But real distributions actually look more like this, or like this, or like this, the grey stuff is the true posterior in this case.",
            "OK, good.",
            "Yes, everything has published.",
            "I'll put the references, I just haven't had time so.",
            "What was I saying?",
            "Power of good posteriors.",
            "OK, so real posteriors look more like things like this if you put a 10H in your model all of a sudden there's a donut with a hole, and so really you need to build a Gaussian for posterior distribution is not going to be enough, so we're going to have to do a lot of work in this this domain, so we'll come back to that a little bit more."
        ],
        [
            "Let's replace the Gaussian and put a binary latent variable instead, and so same model.",
            "You implement a box that gives you an auto regressive Bernoulli distribution is 1 example.",
            "You build a model may be the output is also an autoregressive Bernoulli distribution.",
            "And then you build the same thing through.",
            "Prince Network that gives you binary data.",
            "Everything is binary in this case very similar to sigmoid belief natural.",
            "This is called the deep auto regressive network.",
            "And again you can derive the gradient estimation in this case by using the score score function technique, and in fact in that paper there was a slightly biased estimator, but."
        ],
        [
            "Anyway, so this is some of the things that you can do with the earliest versions of this kind of model with binary latent variables, But I just wanted to point it out to show that if you are interested in having binary latent variables because they offer you interpret ability in some way or other ways of disentangle in your data, then that is possible to do so.",
            "These ones on Binarized Atari games."
        ],
        [
            "The problem of semi supervised variational autoencoders is another good one to look at.",
            "Here we can do a little bit more.",
            "We build our prior function and we also include a prior over classes which will assume it's an observe these two things feed into the model and then we build an inference network that gives you both the posterior distribution on latent variables as well as the posterior distribution on the class probabilities.",
            "And again like we showed for the generative models, the progress in semi supervised learning has really gone well.",
            "And you know the generative approach really is the state of the art.",
            "For Semi supervised learning right now.",
            "And I think there will be a lot more work coming in this front and they let you explore the data in various different ways."
        ],
        [
            "So the problem of these posteriors that I mentioned can be addressed by a different kind of way of building up your posterior, and this is a model that we call draw.",
            "So draw itself is very much like a latent Gaussian.",
            "It is a latent Gaussian VE, but I'll call it a sequential latent Gaussian VE.",
            "You have a prior box, and the prior itself will have a sequential structure.",
            "It will say forms that I given all zeds that have come before, and how this conditioning I'm going to leave to detail for the next slide.",
            "Then.",
            "Similarly, you can build a model which is going to.",
            "Form this distribution based on all these ads that you have generated and then the inference is also going to generate these beds in a sequential fashion so."
        ],
        [
            "In more detail how we actually do that.",
            "You do this sequentially.",
            "You take time .1 you generate some prior and then you fuse this using some RNN and LTM.",
            "In this case you do it again, generate a new source of randomness and fuse it this way and eventually you can do this for as many steps as you like and then you can generate the final state and then you can generate your model and generate the data.",
            "And the inference is where this comes in.",
            "You can take your data X with some state and generate this Gaussian and then you do the inference sequentially.",
            "And because of the conditioning of how the distributions Q of Z is related to Q. XD I in a very non linear way the final distribution which is the collection of all the zeds is something multimodal can be very complicated and it's actually the thing that will help you get very good results.",
            "So these things can be LTM or Gru units if you want to include attention like we heard this morning.",
            "Through spatial Transformers or harder soft attention, then that's easy to do.",
            "Let's see, you wanted to show you.",
            "Yeah, you can have different last names or you can share them in this case and you can even introduce additional canvases.",
            "So here's just two videos."
        ],
        [
            "Are we generating faces on the multiply data set with the attention window in this case?",
            "And then there's a different kind of?"
        ],
        [
            "Love is generating omniglot digits and I just wanted to show this video to show the How the drawing process changes depending on the type of LTM that you actually use.",
            "So if you have this additive canvas which is just adding things then it's more more drawing like but if you have the SGR you kind of canvas is more like a gas that is remolding itself and changing overtime.",
            "But in the end you get equally good generation so additive canvas is good enough."
        ],
        [
            "So if you wanted to build other kinds of even more structured and sequential vasc, then here's a different model of the kind to do scene interpretation.",
            "So I'm going to structure my prior in such a case that is going to tell me three things.",
            "It's going to have a prior over what is in the image of prior of where those things are in the image and the prior of whether this is the last.",
            "If I'm still counting how many images they are objects they are in the image, so I'm going to have a prior for as many objects as they are in the image, and then I'm going to use them together and generate the data right so?",
            "Probably the prior probability is P of YP of where NP of whether I should continue to generate a new.",
            "A new object, and then the likelihood is that way, and then the inference is sequential as well.",
            "You take data through some LST mRNA and you form an inference over what and where and whether to continue or not and the point I want to mention about this model is that this is one of those models that combines binary and latent Gaussian Bernoulli latent variables.",
            "So the binary or the continuous ones at the washing away and the discrete variable.",
            "Is this probability of continuing, which helps us count how many objects they are in the image so.",
            "In this video here, BASIC."
        ],
        [
            "Get the demo which is going online and you get to draw MNIST digits in a little window.",
            "So when I draw one, basically the object knows that there's one object, and that's how to generate it.",
            "As soon as I draw 4 then he knows, oh, there's a new object, it knows that it's their wages in the image an what it is.",
            "These are some limitations that you see of this kind of module.",
            "There's only trained on MNIST digits of a certain size, so a very long aimless digit becomes two amnist digits and eight gets recognized but very large 8.",
            "Is seen AS20 so these are the kind of things of how your data is used affects your training."
        ],
        [
            "OK, you can extend all of this to be more than spatial more than Temple.",
            "You can even do volumetric so again in the sequential VE setup generate all the priors and you build your model based on volumetric data itself.",
            "And here the what I wanted to point out is that this kind of model now need not be a model that you learn through a confident or volumetric convolution.",
            "The model itself can also be a graphics engine and you can take derivatives through this graphics engine using that score function estimation technique.",
            "Right, and so you can use volumetric convolutions and volumetric canvas.",
            "In this case you can have 3D attention using 3 dimensional spatial Transformers.",
            "Volume can use this third dimension to represent color channels or some volume in space, or it can even be time if you want to be do that.",
            "And I mentioned the nondifferentiable aspect."
        ],
        [
            "So here's some results on this volumetric via ease of generating some 3D shapes in the shape net data set, which does really well, and in at least in this case we can actually also produce a set of benchmark results using the likelihood function to actually test how well will do on this model.",
            "Here we actually use a rendering engine as the generative model itself, and then we look at the kind of reconstructions that can happen because what the model needs to do is learn a mesh representation internally that matches through the renderer.",
            "Yeah, is that.",
            "3D rendering using like a 3D complement structure in the top image.",
            "Yes, it's using a 3 dimensional 3D convolution effectively and in the bottom one it's open GL render inaccurate."
        ],
        [
            "OK, I think this is the last one.",
            "This is the model that we call straw and this is the one where I wanted you to think a bit more about reinforcement learning again, and how we can form temporary extended action.",
            "So again, you're going to have a prior and the price is going to be split in two parts.",
            "We also saw something similar to this this morning and yesterday the prior is going to include a latent variable of Gaussians and then it's going to include a set of actions which is going to be conditioned on that set of latent variables.",
            "The actions are UN observed.",
            "They're also late, and so because they are later, we're going to have to integrate them out later on.",
            "But you can implement these two boxes.",
            "The action is discrete, the prior is continuous.",
            "Then you have the model but in reinforcement learning if you want to be model free, you won't have access to the model.",
            "You will use the environment and this will just be the reward of the return directly.",
            "If you wanted to do model based R. Oh, then you can put some kind of another generative model to model their environment of the kind that I showed you earlier here and then what you can get is the log probability of these returns and then you need to do an inference of some sequential form of taking data, giving you inference of zed giving zed and then giving you the sequence of actions.",
            "And I'm not going to just generate one action at a time, I'm going to generate an action from now to T points in the future as well, and so this is how I'm going to do the temporally extended planning.",
            "Right by generating these kind of actions and I'm sort of writing it this way because I wanted to introduce you a slightly different concept, which is the concept of the variational MDP.",
            "It's also very familiar and the kind of loss function that you get out can be derived using the techniques of important sampling.",
            "Then I showed you earlier and yesterday Peter mentioned a bit about these sort of entropy penalties that can be introduced, and you can see through the variational MDP framework how entropy penalties can be derived, and other ways of deriving alternative kind of penalties.",
            "Right, so this kind of loss function lets you maximize the return, which is the value subject to some Cal constraints on a embedding space, but also ensure that you do some exploration.",
            "So this is a very nice principle for doing reinforcement learning and."
        ],
        [
            "So in this kind of model we tried it out to Miss PAC Man in a number of other games that the number of actions the main one is this action plan, which is the number of actions which is discrete and then time is in this case I said, why is the current point in time?",
            "And if you look there's always some grey parts in the future, white is sort of high probability of taking a particular action and you can sort of you know, and then this value function at the bottom is what we use is the baseline and it actually does pretty well and it was a good way of exploring how generative models can be easiest.",
            "How you doing?",
            "Well, next round I think yeah it was just the first set up to try it out and to see how these things combined together.",
            "But closely planning is the thing that we actually want to do, but then you have to build more complicated feedback system and if you look at you know you have to do a lot of other."
        ],
        [
            "Things to actually make the optimization work.",
            "This kind of optimization requires you to use everything you know about stochastic gradient estimation.",
            "You have to apply the score function estimator.",
            "Here you have to apply the pathwise derivative here and there.",
            "So you have to do a lot of work to actually make the estimation work.",
            "So, but open loop closed loop would be the best way to do OK, so that."
        ],
        [
            "Action actions in learning.",
            "So let's summarize."
        ],
        [
            "I thought I wanted to demonstrate.",
            "I think I have 11 or 13.",
            "I don't know different ways of building generative models this world of generative models which I wanted to share.",
            "The reason that I love working in this area with you there although."
        ],
        [
            "Different ways of working in it, and we described three different kinds of generative models.",
            "Fully observed models, transformation models, latent variable models.",
            "We looked at all the progress that has happened over many years to really improving these models and making them usable in different kinds of setups.",
            "We can look at the quality of the images that we have."
        ],
        [
            "There were two learning principles that we looked at the principle of model evidence.",
            "If you are interested in knowing the marginal probabilities or the principle of two sample testing when you aren't interested in there but still want to do some kind of probabilistic reasoning."
        ],
        [
            "We looked at amortized inference.",
            "The way of introducing inference networks, stochastic optimization, and different kinds of estimation procedures.",
            "Different families of EAS and different assumptions that you might want to make."
        ],
        [
            "Yeah, then what is the future of generative models?",
            "Then?",
            "There's a lot.",
            "I the first one will be to come to the aid of supervised learning and reward based systems.",
            "There is a strong role to have calibration confidence interval, more robustness, some sort of interpret, ability to form more data, efficient learning systems to help us be more semiparametric to combine nonparametric and parametric systems to do this new age of scientific discovery, exploratory analysis, synthesis and simulation.",
            "And ultimately, to build these kind of complementary learning systems that build rich scene understanding are self directed and curious agents have conceptual reasoning and integrate planning and control."
        ],
        [
            "So I have lots of people to thank you.",
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, I'm very excited to be here today to get to share with you and re explore this part of deep learning and machine learning and I wanted to use this title of imagining and reasoning to sort of excite you and maybe to stimulate your creative juices about what the future of deep learning can be, and so maybe to start with, I would like you to all think about the reasons that excited you to be part of machine learning and deep learning.",
                    "label": 0
                },
                {
                    "sent": "What is the reason that brought you here today?",
                    "label": 0
                },
                {
                    "sent": "The ultimate goals that you want to achieve through machine learning and I think having met a number of you.",
                    "label": 0
                },
                {
                    "sent": "For men",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sum of you want to understand the statistical and mathematical foundations of learning systems and what it means to learn from data for others.",
                    "label": 0
                },
                {
                    "sent": "You are part of a new era of scientific discovery where we have very large scientific experiments and as a result we need even larger machine learning.",
                    "label": 1
                },
                {
                    "sent": "Even more impressive machine learning to really understand the nature of the world.",
                    "label": 0
                },
                {
                    "sent": "Some of you are innovators and technologists and entrepreneurs of tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And you want to disrupt and create new markets and.",
                    "label": 1
                },
                {
                    "sent": "Some of you are on a quest to solve intelligence, and I'd like you now to think about what the components you think I needed that will form your ideal machine learning system.",
                    "label": 0
                },
                {
                    "sent": "Thanks for thinking thinking.",
                    "label": 0
                },
                {
                    "sent": "Obviously you are definitely in mind.",
                    "label": 1
                },
                {
                    "sent": "What are the components of your ideal machine learning system?",
                    "label": 0
                },
                {
                    "sent": "The components that will help you reach your ambition, and I think throughout this week.",
                    "label": 0
                },
                {
                    "sent": "Definitely part of the solution involves the powerful supervised deep learning systems that you've learned earlier in the week which are now state of the art in machine translation object recognition speech to text.",
                    "label": 0
                },
                {
                    "sent": "They involve the things he learned about yesterday, which involve reinforcement learning, learning from rewards, and you know, but they are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some other things which haven't been.",
                    "label": 0
                },
                {
                    "sent": "How can we learn systems that move beyond associating inputs to outputs or associating actions to rewards?",
                    "label": 1
                },
                {
                    "sent": "We build systems that really understand the world that can synthesize different sources of information, fuse them together to really unpack how the world will evolve.",
                    "label": 1
                },
                {
                    "sent": "Can we build a system that understands that there are objects in the world that there are factors of variation underlying these objects and they can be used to be manipulated and reason against?",
                    "label": 1
                },
                {
                    "sent": "Can we establish concepts that are in the world and use these concepts for high level reasoning and decision making?",
                    "label": 1
                },
                {
                    "sent": "Can we build ways that let our models know that something is new or surprising, and ultimately, can we build systems that help us imagine and generate rich plans for the future?",
                    "label": 0
                },
                {
                    "sent": "And these are the places of chairman.",
                    "label": 0
                },
                {
                    "sent": "Some models that I want to share with you today, but when we look at all the things you've learned about this week, supervised learning, recurrent networks, reasoning and attention and generative models, these will be part of the suite of what we'll call complementary learning systems.",
                    "label": 0
                },
                {
                    "sent": "Systems that work together to really help us achieve.",
                    "label": 0
                },
                {
                    "sent": "Whatever your ambition for machine learning was.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just some keynotes.",
                    "label": 0
                },
                {
                    "sent": "When I write a function F the function F is always going to be a deep network of some form.",
                    "label": 0
                },
                {
                    "sent": "I won't go into too much detail about its exact form, but that's what the function F is going to be, and you see all the topics that I'm going to speak about today.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to do was leave you with some principles, some tools, and some tricks that you can use not just for generative models, but throughout machine learning, and give you a sort of different perspective.",
                    "label": 0
                },
                {
                    "sent": "So some of the themes that I want to leave with you are the design of probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "Asian deep learning, stochastic optimization methods for reasoning and control and in some way everything.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about is about the problem of density estimation.",
                    "label": 0
                },
                {
                    "sent": "So whatever you know about the problem of statistical density estimation, bring back to the front of your mind and then we'll use that to go for.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have sort of six parts we might not go through all six parts, but the first part will be a Birds Eye view of generative models.",
                    "label": 1
                },
                {
                    "sent": "What is the current state of the art of generative models then?",
                    "label": 1
                },
                {
                    "sent": "I love dealing with models and there is a model for every occasion for whatever kind of system you want.",
                    "label": 0
                },
                {
                    "sent": "There is a model that we can build.",
                    "label": 0
                },
                {
                    "sent": "We're going to explore all the different classes of models that we have.",
                    "label": 0
                },
                {
                    "sent": "Once you have a model and you have data, the way you integrate data into your model is through a principle of learning and inference.",
                    "label": 0
                },
                {
                    "sent": "And there are two principles that I wanted to discuss.",
                    "label": 1
                },
                {
                    "sent": "One will be about evidence estimation and the other one is going to be about 2 sample testing, and then we're going to take our models.",
                    "label": 0
                },
                {
                    "sent": "We're going to take our inference.",
                    "label": 0
                },
                {
                    "sent": "We're going to combine them to build different kinds of algorithms, which there are many ways to do that, and then we're going to use a case study of variational autoencoders to explore all the different ways you might want to build models, and they will try and summarize and look at things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, OK, there is a huge diversity of generative models and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to take you through all of them so the simplest application is this one.",
                    "label": 0
                },
                {
                    "sent": "It's data imputation.",
                    "label": 0
                },
                {
                    "sent": "If you really understand the data of your world then you will be able to understand its underlying statistics and the classical problem is missing data imputation or removing noise 'cause data in the real world is not clean and so this is an example from a model called pixel RNN, which if you need to compete complete or fill in the missing half, these are the kind of completions it can do and you can see that good models allow you to.",
                    "label": 0
                },
                {
                    "sent": "Explore the variation in the world that is actually plausible, and this kind of variation is actually what you need for powerful reasoning and probabilistic systems.",
                    "label": 0
                },
                {
                    "sent": "But this might be the simplest one, and I'm going to go from simple applications to more complicated ones.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next type of thing is semi supervised classification.",
                    "label": 0
                },
                {
                    "sent": "Generative models have a very important role to act in the service of supervised learning models.",
                    "label": 0
                },
                {
                    "sent": "Ways to actually make them more data efficient or to be more active, better regularizers and so in the semi supervised learning problem you have very few labeled data points and lots of unlabeled data point.",
                    "label": 0
                },
                {
                    "sent": "And how can you do this transfer of knowledge from the labeled data point to the unlabeled data points and the generative approaches?",
                    "label": 0
                },
                {
                    "sent": "Right now one of the state of the art methods for semi supervised classification.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we have generative models are closely related to the problem of compression and communication, and I'd like you to imagine we all have phones we all deal with videos and large images which we need to communicate over long distances.",
                    "label": 0
                },
                {
                    "sent": "Generative models have the role for us to communicate and compress those data very efficiently and then send them, and we can even control to what extent we want to have this kind of compression.",
                    "label": 0
                },
                {
                    "sent": "That's another useful.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application if we really want to understand scenes in the world, then one way of dealing with this is to understand its 3D structure.",
                    "label": 0
                },
                {
                    "sent": "Understand that there are certain physical constraints, some intuitive physical notions of the world, and so this bottom video is the classic example of the Necker cube, where there are certain ambiguities in how you can perceive, and.",
                    "label": 0
                },
                {
                    "sent": "View this kind of cube and then there's a completion program process where you have to take the left half an you have to see how it fits and it does very much, but a human does.",
                    "label": 0
                },
                {
                    "sent": "It forms the rest of the cube and then realize it to fit with the rest of the queue.",
                    "label": 0
                },
                {
                    "sent": "But you know, we can also move to other things of more complex 3 dimensional scenes.",
                    "label": 0
                },
                {
                    "sent": "Think of a generative virtual reality where you can build and generate these kind of scenes and really interact with the world.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an important other aspect that generative models have is in the problem of rapid scene understand.",
                    "label": 0
                },
                {
                    "sent": "So here is the scene of a camera looking over set and when you do reasoning in the scene, you want to very quickly know how many objects are in the scene, what kind of objects are in this scene and the kind of factors of variation associated with those objects.",
                    "label": 0
                },
                {
                    "sent": "And so you can see the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Here is the red part and the camera which shows that it's in the right position.",
                    "label": 0
                },
                {
                    "sent": "It knows how many scenes they are, how many objects they are, and this bottom video is of.",
                    "label": 0
                },
                {
                    "sent": "The learning process of such a model that is both trying to simultaneously learn the identity of digits.",
                    "label": 0
                },
                {
                    "sent": "It's also trying to learn where these digits are, and it's also counting the number of digits and letting you know that in this case there are three digits in all of these images, so this kind of scene understanding will be very important.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like humans do, we want to have machines that when they seen you data one time or very few times, they can explore the data, understand their variations and use prior knowledge to really do that, and so in this task we are looking to explore the different ways generate alternative examples of the images at the top row, which the model has never seen before.",
                    "label": 0
                },
                {
                    "sent": "A task as we call 1 chart generalization.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you saw a bit of this question yesterday that we can now use a generative model to actually build simulators of an environment.",
                    "label": 0
                },
                {
                    "sent": "And if we have a simulator of an environment, then we can do very long term rollout.",
                    "label": 0
                },
                {
                    "sent": "We can make predictions about what will happen.",
                    "label": 0
                },
                {
                    "sent": "We can do counterfactual reasoning in some way, and then we can actually account for this when we do our decision-making as we go forward.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another kind of things this is the video of a 3 dimensional labyrinth, and if you and I operate in environments like this all the time, So what you need to do for control is form this state representation that was called yesterday.",
                    "label": 0
                },
                {
                    "sent": "How can you form a representation of these States and use these states to actually do control?",
                    "label": 0
                },
                {
                    "sent": "There's actually a control system happening here that uses a generative model to form the state representation, and based on this state representation, then actually does the control and the task is actually defined.",
                    "label": 0
                },
                {
                    "sent": "The green Apple, which gives you a reward so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kind of representation, though we wanted to actually be a little bit more structured, and so can we actually say that there are concepts within our representations that factor in interesting ways, and so in this video, this is of the Atari game called Seaquest.",
                    "label": 0
                },
                {
                    "sent": "We explore a few different.",
                    "label": 0
                },
                {
                    "sent": "You can see that generative models help you unpack different concepts.",
                    "label": 0
                },
                {
                    "sent": "There is the concept of the oxygen bar, which is being filled out.",
                    "label": 0
                },
                {
                    "sent": "There's the concept of the submarine moving up and down.",
                    "label": 1
                },
                {
                    "sent": "A concept of the submarine moving left to right, a concept of lives which matter in this kind of game.",
                    "label": 0
                },
                {
                    "sent": "And the ability to detect these kind of meaningful representations that allow us to do reasoning at the concept level will be very important.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yesterday Joel mentioned the topic of exploration, and in the typical everything that was mentioned about exploration in this game.",
                    "label": 0
                },
                {
                    "sent": "In that article Montezumas Revenge, we usually can only explore 2 rooms.",
                    "label": 0
                },
                {
                    "sent": "This is actually the structure of the game.",
                    "label": 0
                },
                {
                    "sent": "There are 26 rooms and usually can only see these two by using a generative model.",
                    "label": 0
                },
                {
                    "sent": "You can actually see and learn what is new and what is interesting in the world.",
                    "label": 0
                },
                {
                    "sent": "You can equip your agents with that knowledge and using that using that information.",
                    "label": 0
                },
                {
                    "sent": "You can then build a system like this that now let us explore 15 rooms in the world and so now Montezuma's Revenge is getting much better through the power of equipping generative models within the reinforcement learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, there's yes policy.",
                    "label": 0
                },
                {
                    "sent": "Yes, the policy comes to key learning in this case.",
                    "label": 0
                },
                {
                    "sent": "And finally there's the problem of how can we do temporal extra abstractions and learn make plans into the future, and you can think of generative models that actually have a role to play in helping you generate plans into the future.",
                    "label": 0
                },
                {
                    "sent": "So this is a model of Miss Pacman an on the Y axis is the number of actions.",
                    "label": 0
                },
                {
                    "sent": "There are discrete number of actions, and then this X axis days time.",
                    "label": 0
                },
                {
                    "sent": "The white line sort of indicates where we are in the current point in time in this generative model actually generates plans into the future.",
                    "label": 0
                },
                {
                    "sent": "You can see beyond the white line.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of a plan that is already formed, and then it has a way of switching between plans on the fly, so this is sort of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "State of the art of Generative models.",
                    "label": 0
                },
                {
                    "sent": "We have models that allow you to do simple tasks from missing data, which helps you understand the structure of your data.",
                    "label": 0
                },
                {
                    "sent": "To do semisupervised classification, density based exploration, representation, learning and concept learning, one child, generalization simulating environments, compression macro actions and planning.",
                    "label": 0
                },
                {
                    "sent": "And I think there are many many other applications which are not listed here, but these are some of the ones that I think we've seen a lot of progress in recent time, so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to really emphasize the kind of progress that we've had in the last few years that Russ mentioned earlier, this is on the Emnace benchmark using the log likelihood measure an we would start off with simple models like mixtures of Bernoulli's with 500 clusters which weren't that good.",
                    "label": 0
                },
                {
                    "sent": "But overtime we've done much much better models and under any measure we're just getting better an under any different kind of model.",
                    "label": 0
                },
                {
                    "sent": "And today we have really good models that actually do really impress people.",
                    "label": 0
                },
                {
                    "sent": "They inspire people you really think and then under different generative model, different data set.",
                    "label": 0
                },
                {
                    "sent": "We have the same kind of this.",
                    "label": 0
                },
                {
                    "sent": "The omniglot datasets.",
                    "label": 0
                },
                {
                    "sent": "And so we're really making progress.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Not every model can be measured against this log likelihood metric.",
                    "label": 0
                },
                {
                    "sent": "So if you want to look at the actual kind of generations that they can do again the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We really are making a lot of progress from simple things like VE that weren't very good three years ago when we created them to now many new models which actually represent the statistics.",
                    "label": 0
                },
                {
                    "sent": "Then images have the good kind of structure, sharpness.",
                    "label": 0
                },
                {
                    "sent": "So that paint.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The picture, so for the rest of the talk I'm going to describe to you the way I understand and use machine learning.",
                    "label": 0
                },
                {
                    "sent": "So there are three parts of my machine learning framework.",
                    "label": 1
                },
                {
                    "sent": "It always starts off with the model.",
                    "label": 0
                },
                {
                    "sent": "The model will embody everything you know about your system and that you want to work with.",
                    "label": 0
                },
                {
                    "sent": "Once you have a model, you will choose a system of learning principles and you're learning.",
                    "label": 0
                },
                {
                    "sent": "Principles will then get used to help you deal with your data and for any combination of learning principle and model you can form very different algorithms, and so we're going to do 3 parts to explore different kinds.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So under models, we're going to consider three different types of models.",
                    "label": 1
                },
                {
                    "sent": "The first one will be fully observed models, and you saw many of these models when you looked at our own ends earlier in the week.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to look at transformation models.",
                    "label": 1
                },
                {
                    "sent": "You've all used these if you generated random numbers already, but they are very important class, and then the third class of models will look at our latent variable models that help you establish hidden causes and the structure of your of the data generating process.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then there's a huge smorgasbord of learning principles which you get to choose from, and different models make different principles more applicable, and you know you've used maximum likelihood and map estimation this week.",
                    "label": 0
                },
                {
                    "sent": "You see lots of variational methods, and I'm going to try and put them together in a different.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then once you have a model and inference, you can combine them in many different ways.",
                    "label": 1
                },
                {
                    "sent": "For example that you just saw with the restricted Boltzmann machine and maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "There are many different algorithms to combine those two contrastive divergent parallel tempering natural gradient example.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, that's sort of just the first part an I think we'll just dive straight into into different kinds of models.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The three types of generative models that I mentioned, fully observed, models, transformation models, and latent variable models, and you really need to think about what the problem is that you are dealing with, and depending on the problem, you will have to choose one of these.",
                    "label": 1
                },
                {
                    "sent": "These kind of models and there are many design dimensions that you actually will deal with.",
                    "label": 0
                },
                {
                    "sent": "You have certain data types, what kind of data do you have?",
                    "label": 1
                },
                {
                    "sent": "Binary data, real valued nominal?",
                    "label": 0
                },
                {
                    "sent": "Are these strings?",
                    "label": 0
                },
                {
                    "sent": "Are the images?",
                    "label": 0
                },
                {
                    "sent": "Are the graphs?",
                    "label": 0
                },
                {
                    "sent": "There will be certain dependencies structures which you might want to model.",
                    "label": 1
                },
                {
                    "sent": "Whether they are independent, whether they're sequential, where their temporal, spatial, temporal, etc.",
                    "label": 0
                },
                {
                    "sent": "There will be different representations you might want to have you under have discrete or continuous or some kind of mixed representations.",
                    "label": 1
                },
                {
                    "sent": "Is there certain dimensionality that you want to consider?",
                    "label": 0
                },
                {
                    "sent": "Do you want to have parametric or nonparametric modeling?",
                    "label": 1
                },
                {
                    "sent": "And then there are other aspects aspects of computational complexity modeling capacity, bias uncertainty, calibration of your datasets, interpretability.",
                    "label": 0
                },
                {
                    "sent": "All these are things we sort of have to think about, and different models will trade these off in different ways.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first class of models are called fully observed models and what defines a fully observed model is that there are models that use the data directly and they do not introduce other UN observed and random local variables.",
                    "label": 1
                },
                {
                    "sent": "So parameters in a model are global variables because they are properties of the data set in total, but a local variable is something which is particular to an individual data point and so fully observed models do not introduce other variables that are particular to data points, right?",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 1
                },
                {
                    "sent": "They need to be stochastic, otherwise you know.",
                    "label": 1
                },
                {
                    "sent": "So that's the definition.",
                    "label": 0
                },
                {
                    "sent": "So you've seen Markov models earlier in the week.",
                    "label": 0
                },
                {
                    "sent": "This sort of generalize the entire class and the generative process is very simple.",
                    "label": 0
                },
                {
                    "sent": "Here you generate the first dimension of your data from some distribution of categorical and this place this case given some parameters pie, you generate the second dimension given the first one, and so on.",
                    "label": 0
                },
                {
                    "sent": "You generate the I TH dimension given all the other ones before.",
                    "label": 0
                },
                {
                    "sent": "So this is the image of generating Pixar I in this image.",
                    "label": 0
                },
                {
                    "sent": "Given everything in the blue case.",
                    "label": 0
                },
                {
                    "sent": "And then that is the probability models.",
                    "label": 0
                },
                {
                    "sent": "And when I write this function F you can put any kind of deep neural network with.",
                    "label": 0
                },
                {
                    "sent": "In this case they can be fully connected, recurrent, etc.",
                    "label": 0
                },
                {
                    "sent": "So there are many properties of these kind of fully observed model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And some of these properties include that you can directly encode the structure of the data that you have, which can be very powerful if you have certain knowledge.",
                    "label": 0
                },
                {
                    "sent": "If you are modeling astronomical phenomena, then you have this kind of knowledge.",
                    "label": 0
                },
                {
                    "sent": "Any data type can be used which is very flexible and powerful, and for directed graphical models.",
                    "label": 1
                },
                {
                    "sent": "The parameter learning is extremely simple in these models because the log likelihood is immediately available and you do not need to introduce other approximations, and it's very easy to scale these kind of models up to very large systems.",
                    "label": 0
                },
                {
                    "sent": "Over multiple GPS, very large datasets and we have many good tools for optimization after this in the UN directed the negative is that they have this ordering sensitivity which comes into play in the undirected case, though the parameter learning can be difficult.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is also a disadvantage sometimes like in pixel RNN, which is computation becomes very sequential.",
                    "label": 0
                },
                {
                    "sent": "An on GPU's for example, that's not really convenient.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So there's this last point I'll make which is about this loan is.",
                    "label": 1
                },
                {
                    "sent": "So for undirected models, just to complete that, the parameter learning can be difficult.",
                    "label": 0
                },
                {
                    "sent": "The problem of the normalizing constant that we just saw in the earlier session and for all these models this generation can be slow.",
                    "label": 0
                },
                {
                    "sent": "You have to iterate sequentially through the elements, use the Markov chain and it's not efficient of your computing resources.",
                    "label": 0
                },
                {
                    "sent": "So depending on what your problem is, you have to consider these kind of things.",
                    "label": 0
                },
                {
                    "sent": "These are some of the generations you can get from a pixel CNN model conditional on labels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I wanted to explore the different classes of models in two dimensions, 1 dimensions, directed and undirected and the other dimension is discrete and continuous.",
                    "label": 0
                },
                {
                    "sent": "And for any combination of these you'll probably find a model and this morning we saw about log linear models in Gaussian MRF's.",
                    "label": 0
                },
                {
                    "sent": "The models I just mentioned are nonlinear autoregressive models and real valued made.",
                    "label": 0
                },
                {
                    "sent": "Many people are working in this block of discrete and directed, made fully visible sigmoid belief networks.",
                    "label": 0
                },
                {
                    "sent": "Pixar, NN, and their other models as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Confirmation model let's take a very different approach.",
                    "label": 0
                },
                {
                    "sent": "They say that you have an underlying source of noise, and this source of noise is going to be transformed through some parametric function.",
                    "label": 0
                },
                {
                    "sent": "So this model basically describes everything and the image.",
                    "label": 0
                },
                {
                    "sent": "I sort of want you to keep in your mind is of a sort of a system of pipes, and the pipes represent this function and what you can do with this function is you can adjust some properties of these pipes, the thickness and their length, and once you put into this pipe system source of noise and what comes out of the pipes at the end is going to be some new distribution.",
                    "label": 0
                },
                {
                    "sent": "And if the function represented by these pipes is invertible, then you can just apply the rule for change of variables and you'll be able to actually know the distribution at the end.",
                    "label": 0
                },
                {
                    "sent": "But you don't necessarily need to know this.",
                    "label": 0
                },
                {
                    "sent": "We have the ability to create very arbitrary functions, and so one example of these kind of models would be what they call generating networks.",
                    "label": 0
                },
                {
                    "sent": "This very simple degenerative procedure.",
                    "label": 0
                },
                {
                    "sent": "You have some source of noise, a Gaussian for example.",
                    "label": 0
                },
                {
                    "sent": "Then you push this through some nonlinear function and out at the end you get our samples from a new distribution.",
                    "label": 0
                },
                {
                    "sent": "And this transformation can again be any kind of function, so these kind of models have it very different.",
                    "label": 0
                },
                {
                    "sent": "Kind of set of properties.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sampling is very easy as you, so I just needed to take some random source of numbers and I get them out at the end.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to compute expectations in these models.",
                    "label": 1
                },
                {
                    "sent": "There's the property that we call the low of the unconscious statistician that uses this, that if you want to compute expectation with respect to the final distribution, you just need samples.",
                    "label": 0
                },
                {
                    "sent": "And again, because these models are going to allow us to use classifiers another very large scale systems, they can be scaled up to very large scale.",
                    "label": 0
                },
                {
                    "sent": "But there are certain constraints which these models.",
                    "label": 1
                },
                {
                    "sent": "Imposing a switch can be difficult to satisfy.",
                    "label": 0
                },
                {
                    "sent": "It is very difficult to learn invertible neural networks or the optimization can be very challenging.",
                    "label": 0
                },
                {
                    "sent": "And then there's an issue that unlike the fully observed models, there is no noise model at the end which may be an issue.",
                    "label": 0
                },
                {
                    "sent": "So not having a noise model on the final output means that it's very difficult to extend to generic data types.",
                    "label": 1
                },
                {
                    "sent": "If you wanted to generate binary data, this will be very difficult to do data in the real world is not.",
                    "label": 1
                },
                {
                    "sent": "So if you needed to account for noise in the data.",
                    "label": 0
                },
                {
                    "sent": "Then this is a bit hard to do, and since you do not have access to the final distribution, computing the marginalized likelihood which lets you do model comparison scoring can be very hard to do.",
                    "label": 0
                },
                {
                    "sent": "Down is very easy to add noise in the visible if you want, but we don't do it because we don't want it because the real images don't have that kind of effect.",
                    "label": 0
                },
                {
                    "sent": "In fact, we have done it, but then it becomes a VE laser variable models with noise models are directed models.",
                    "label": 0
                },
                {
                    "sent": "Put it at the top, but you could easily put it anywhere you want.",
                    "label": 0
                },
                {
                    "sent": "Is nothing in that in the algorithm that prevents you, but we've got.",
                    "label": 0
                },
                {
                    "sent": "We've also done experiments where you put it in the top and the bottom, and what happens is it just learns to put the variance, and then at the visible light at the visible layer, OK. Yeah, so again, yeah this is a good point.",
                    "label": 0
                },
                {
                    "sent": "You need to know, sort of what you actually want to do an depending on what you want to do, that kind of model you use will actually affect your choice.",
                    "label": 1
                },
                {
                    "sent": "So this is from the conditional generative adversarial model.",
                    "label": 0
                },
                {
                    "sent": "Understanding this is the way I understand it is images, sounds and things like that on the low dimensional manifold very near low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "As soon as you add isotropic noise like Gaussian noise, you get something that's not going to be on that manifold.",
                    "label": 0
                },
                {
                    "sent": "Yes, and so.",
                    "label": 0
                },
                {
                    "sent": "You should not have noise available, and for me it's it's a defect of things like both machines, Yankees and things like that.",
                    "label": 0
                },
                {
                    "sent": "But you have to add noise in order to get a meeting with you.",
                    "label": 0
                },
                {
                    "sent": "If you if you don't have that and you model generates pixel which is not just like you put zero probabilities and which can be maybe like it is not the right objective here.",
                    "label": 0
                },
                {
                    "sent": "I think this is a very good point that we actually need to debate the fact that some points can have zero likelihood is a questionable statement, right?",
                    "label": 0
                },
                {
                    "sent": "It's it can be dangerous.",
                    "label": 0
                },
                {
                    "sent": "Again, you need to think about what you're doing.",
                    "label": 0
                },
                {
                    "sent": "I like this class of models.",
                    "label": 0
                },
                {
                    "sent": "As I said, we've all used them before.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the classic, there's two ways to really think about these.",
                    "label": 0
                },
                {
                    "sent": "You can think about them in discrete time, or you can think about them in continuous time.",
                    "label": 0
                },
                {
                    "sent": "If you have generated random numbers before, then those kind of models usually take a uniform random number and then give you a sample from a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Those are called one liners and then you have the generation networks Gans and then you have other volume and non volume preserving transformations and in continuous time this rule for change of law variables is called the Fokker Planck equation or Kolmogorov equation.",
                    "label": 0
                },
                {
                    "sent": "And then you get the equivalent in the continuous time domain, so there's a lot of good models to explore in this room.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you have the last case, which is sort of yes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One more before.",
                    "label": 0
                },
                {
                    "sent": "So the kind of a diffusion that you have to think is that the diffusion gives you a mechanism to do a flow of the probability.",
                    "label": 0
                },
                {
                    "sent": "So they Gan defines this flow away of reshaping the probability to give you the thing at the end, and so these are all white diffusion models.",
                    "label": 0
                },
                {
                    "sent": "Do Diffusions take an initial source of randomness, shape them over some path, and then out at the end you get, obviously, Diffusions will add extra noise at different points in time, but the thinking is at least conceptually the same.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, latent variable models are going to actually specify a set of underlying causes that you think represent how your data is generated.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of what is called a deep latent Gaussian model, and degenerative process is again very simple.",
                    "label": 1
                },
                {
                    "sent": "You start at the very top, generate some Gaussian noise, come down to generate some new random variables condition in the previous one, generate some new ones condition on the one before, and eventually generate some data from a noise source, which is what is not in maybe some of the other model, and you can have lots of other kind of skip connections in various ways.",
                    "label": 0
                },
                {
                    "sent": "And all these functions, mu and Sigma are also deep networks, right?",
                    "label": 0
                },
                {
                    "sent": "So there's this is what Russ mentioned by having these deterministic layers in between these stochastic layers.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so these ones also have a different set of properties.",
                    "label": 0
                },
                {
                    "sent": "Again, like all the others, the sampling is very easy.",
                    "label": 0
                },
                {
                    "sent": "It's a very easy way of including hierarchy and depth, and it's you know it's very natural to want to encode the structure you think underlying your model in this way, and the issue of the order dependency which came up maybe in the fully observed model is removed here, because integrating out latent variables induces dependencies, which means you do not need to think about this right.",
                    "label": 1
                },
                {
                    "sent": "Latent variables provide some compression and representation of the data.",
                    "label": 0
                },
                {
                    "sent": "And again you have access to the likelihood, which means that you can do scoring model comparison and model selection, right?",
                    "label": 0
                },
                {
                    "sent": "But when you have an observed random variables in a model, then you have to compute the probability of that random variable given data and this task can be very difficult to do in general.",
                    "label": 1
                },
                {
                    "sent": "The marginalized probability is itself not easy to compute an you might need to have some clever kind of approximations and the kind of approximations themselves need a lot of creativity.",
                    "label": 0
                },
                {
                    "sent": "The kind of posterior distributions need to be approximated.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but this is probably the largest class of models that's been explored more than 150 years of research, starting with Spearman when he created PCA, and you can think of a useful and interesting dimensions for me are the parametric and nonparametric distinction deep versus linear or just more direct non deep models and discrete and continuous?",
                    "label": 1
                },
                {
                    "sent": "Most of us are sort of working around here.",
                    "label": 1
                },
                {
                    "sent": "We're continuous, deep in parametric models where you have things like the ease and draw, but then you have other interesting.",
                    "label": 1
                },
                {
                    "sent": "Models in various other cases, like you know, nonparametric deep and discrete.",
                    "label": 0
                },
                {
                    "sent": "You know, hierarchical, dear slave process.",
                    "label": 0
                },
                {
                    "sent": "All of these.",
                    "label": 0
                },
                {
                    "sent": "The point is just to show that these are all the same class of model and the reasoning system you use for one can be used for the other.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of different models to explore and wherever you find your interest, you can find a model that helps you fill that category.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's everything about models.",
                    "label": 0
                },
                {
                    "sent": "If there are any questions about modeling, yes.",
                    "label": 0
                },
                {
                    "sent": "Harbor freight in elaborate.",
                    "label": 0
                },
                {
                    "sent": "So typically what you have to do?",
                    "label": 0
                },
                {
                    "sent": "I mean there's a lot of work on an invertible neural network from the 90s, maybe you know more of this for me, So what you have to do then is there a set of constraints in the parameters that you have to have to make them so even in simple univariate functions this is difficult to do and the optimization can be very hard.",
                    "label": 0
                },
                {
                    "sent": "You have to add extra constraints to make sure the gradients behave well, so we've tried it in some previous work and it's not great to do so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's sort of in an ICP bills at nonparametric private, discrete random variables.",
                    "label": 1
                },
                {
                    "sent": "An it is.",
                    "label": 0
                },
                {
                    "sent": "The typical formulation is directed.",
                    "label": 1
                },
                {
                    "sent": "You just have this one layer of parametric prior.",
                    "label": 0
                },
                {
                    "sent": "Then that gives you the data, but you can have multiple layers of stacking this nonparametric discrete product.",
                    "label": 1
                },
                {
                    "sent": "Think of a sigmoid belief.",
                    "label": 1
                },
                {
                    "sent": "Natural, but the nonparametric equivalent maybe I have the wrong name, but they definitely exist, so I'll say I'm going to Add all the references in the back for all of these models and things.",
                    "label": 0
                },
                {
                    "sent": "OK, any other modeling?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "No OK, so once you have a model to the process of combining data into your model, is the process of learning and inference, and I wanted to explore two principles of inference with you.",
                    "label": 0
                },
                {
                    "sent": "So common inferential problems that you'll find in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statistics include evidence estimation.",
                    "label": 0
                },
                {
                    "sent": "Once you have data, you want to know what is the probability of your data.",
                    "label": 0
                },
                {
                    "sent": "And once you know this probability, you can do a lot of things.",
                    "label": 0
                },
                {
                    "sent": "Then there's sort of moments computation.",
                    "label": 0
                },
                {
                    "sent": "They're sort of statistical quantities that you want to do for summarization, and then you want to sort of compute these moments in different ways.",
                    "label": 0
                },
                {
                    "sent": "Prediction is the common one that everyone wants to do given some old data up to XT, you want to make a prediction of XT plus one or they can be entire datasets and sometimes you want to do testing as another inferential problem.",
                    "label": 1
                },
                {
                    "sent": "You want to be able to look at two hypothesis and say which one should you actually use in the case of model selection for example, so first.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of principle for inference that I wanted to go about is on the model evidence, so the model evidence is the way of computing the probability of the data that you have, and we're going to look at the model evidence in the class of latent variable models, and you've seen much of this already, so maybe we can go through it faster and the model evidences which will use to score your model to do model comparison to do model selection to do moment estimation to do normalization to posterior compute computation and prediction.",
                    "label": 1
                },
                {
                    "sent": "Once you have this single quantity, you basically know as much as you need to know about your data to do anything you want to do.",
                    "label": 1
                },
                {
                    "sent": "So the principle involves taking steps to improve the model evidence.",
                    "label": 0
                },
                {
                    "sent": "Given the data that you have.",
                    "label": 1
                },
                {
                    "sent": "So the principle is very simple.",
                    "label": 0
                },
                {
                    "sent": "Given data integrate out anything that is UN observed and use the value of X.",
                    "label": 1
                },
                {
                    "sent": "Now this integral in general is not known to you, but the main principle is to transform this integral into an expectation over something simpler.",
                    "label": 0
                },
                {
                    "sent": "An while you've seen that trick yesterday.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even this morning, so I'm going to start with important sampling and we're going to do a little generation.",
                    "label": 0
                },
                {
                    "sent": "So this is the integral problem.",
                    "label": 0
                },
                {
                    "sent": "There is common to almost every inferential problem that you have.",
                    "label": 0
                },
                {
                    "sent": "Can we integrate out latent variables, Ed.",
                    "label": 0
                },
                {
                    "sent": "But we don't know how to do this.",
                    "label": 0
                },
                {
                    "sent": "What I'll do is introduce the proposal distribution, which will be called Q, and I'm just introduced a one, so it's just multiply by one, then I re wait this thing to introduce the importance weight that you saw earlier mentioned.",
                    "label": 0
                },
                {
                    "sent": "And now this itself is an expectation of a different quantities and expectation.",
                    "label": 0
                },
                {
                    "sent": "Under the distribution queue.",
                    "label": 0
                },
                {
                    "sent": "So I can form this important weight which is WI can easily sample from Q and then the principle of important sampling is to evaluate this integral by Monte Carlo integration, which means just averaging the importance weights given these samples.",
                    "label": 0
                },
                {
                    "sent": "So just a little bit of notation for the rest.",
                    "label": 0
                },
                {
                    "sent": "Everywhere I'm going to write queue of Zed, but I actually mean Q of Z given X, but I'm just going to leave out the X for convenience and there are some conditions for important sampling.",
                    "label": 0
                },
                {
                    "sent": "So important sampling is the simplest way and the most fundamental way of computing integrals.",
                    "label": 0
                },
                {
                    "sent": "We use them everywhere.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do a slightly different trick at the end, so here's our integral problem again, which we need to compute.",
                    "label": 0
                },
                {
                    "sent": "We introduced the proposal.",
                    "label": 0
                },
                {
                    "sent": "We look, read, match the terms to introduce the importance weight, and now instead of applying Monte Carlo integration, I'm going to apply Jensen's inequality that you saw this morning that says that the log of an expectation is greater than the expectation of the law, and now this gives a new quantity which I can rewrite this way, which is an expectation of cubes Ed under this.",
                    "label": 0
                },
                {
                    "sent": "Log likelihood term and an expectation of QZ under this log ratio term, and this is the famous variational lower bound that we describe everywhere and that you've seen throughout the week, yes.",
                    "label": 0
                },
                {
                    "sent": "No OK, OK, so if I could have seen the derivation a few times, so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I just unpacked this.",
                    "label": 0
                },
                {
                    "sent": "Quantity is called the variational free energy and it has a number of useful quantities.",
                    "label": 1
                },
                {
                    "sent": "There's this distribution Q event which will be the approximate posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "This Q of zed represents your current understanding of the posterior distribution of the latent variables given the data.",
                    "label": 1
                },
                {
                    "sent": "What makes this other term, which is the reconstruction term log likelihood terms, are reconstruction terms that this helps you measure how well you are matching the data that you have.",
                    "label": 0
                },
                {
                    "sent": "And then there's this penalty term which naturally arises.",
                    "label": 0
                },
                {
                    "sent": "You do not have to design.",
                    "label": 0
                },
                {
                    "sent": "It is derived for you and it is the way of introducing Occam's razor into your problem and dealing with model complexity and penalization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this one here is being very familiar to you this week, but there are various other kinds of variational bounds.",
                    "label": 1
                },
                {
                    "sent": "The one you just saw from Russ this morning was the multisample variational objective of the importance weighted objective.",
                    "label": 1
                },
                {
                    "sent": "Can see the top one is a special case of this.",
                    "label": 0
                },
                {
                    "sent": "Then you get sort of linear variational objectives, which is a generalization of the previous two, and then you have other kinds of variational families as well.",
                    "label": 0
                },
                {
                    "sent": "But the point of these kind of variational families is that the solution for all of them, the solution Q of Z, is the same for all of them, which means find the queue of zed, which is as close as possible to the true P of Z given X.",
                    "label": 1
                },
                {
                    "sent": "So you can choose any of these kind of bonds that you like.",
                    "label": 1
                },
                {
                    "sent": "The solution in the end is the same.",
                    "label": 0
                },
                {
                    "sent": "Some of them make optimization easier than others, or let you unify different ways of thinking so.",
                    "label": 0
                },
                {
                    "sent": "OK, are there any questions and sort of variational inference and this sort of part of the store?",
                    "label": 0
                },
                {
                    "sent": "OK, so the second principle for inference that I wanted.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at is on 2 sample testing, so for many models you do not have access.",
                    "label": 0
                },
                {
                    "sent": "You only have access to an unnormalized probability or you have just partial knowledge of the distribution.",
                    "label": 1
                },
                {
                    "sent": "You really don't know anything you know the noise source at the beginning and you know know the function but not much else.",
                    "label": 0
                },
                {
                    "sent": "So a different way to do this is then just to compare two distributions.",
                    "label": 0
                },
                {
                    "sent": "And so the principle of learning here is that we want to compare a distribution of training data with the distribution of some other data source and can be a test data set.",
                    "label": 0
                },
                {
                    "sent": "It can be a data set that we generated from our model.",
                    "label": 0
                },
                {
                    "sent": "It can be anything, right?",
                    "label": 0
                },
                {
                    "sent": "So the principle of two sample testing will be to say that I want the probability of my training data set to be the same as my distribution under the data set I generated, for example.",
                    "label": 1
                },
                {
                    "sent": "So you want these two probabilities to be one.",
                    "label": 0
                },
                {
                    "sent": "Or you want them to be the same right?",
                    "label": 0
                },
                {
                    "sent": "And the reason you want to do this is because you might not be interested in computing the marginal probability.",
                    "label": 0
                },
                {
                    "sent": "And if you aren't interested in computing the marginal probability, then this problem here of computing a density ratio is a far easier problem, right?",
                    "label": 1
                },
                {
                    "sent": "And so the trick of density ratio estimation is going to be to transform the problem of density ratio estimation into the problem of class probability estimation.",
                    "label": 0
                },
                {
                    "sent": "And that is something we know very well how to do so OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit of notation just to set up.",
                    "label": 0
                },
                {
                    "sent": "I have two datasets X hat an X~ which I'm going to combine together into some new data set, which I'm just going to call X. I'm going to use the random variable X to denote that combined data set.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assign some labels also to this data set, so I'm going to introduce plus one to everything from the data set that was X hat and minus one to everything from the data set there was X~ and then there is this equivalence that the probability of X~ is just the probability of the combined data set condition on the label one, and similarly for X~ is the probability of the total data set condition on the fact that you have the label minus one right?",
                    "label": 0
                },
                {
                    "sent": "So this is just the setup of our problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to write two things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to remind you about problem, which is the problem of density ratio estimation.",
                    "label": 0
                },
                {
                    "sent": "And the second problem, which is just going to be based rule which you need for everything.",
                    "label": 0
                },
                {
                    "sent": "So let's start the conditional problem that we started with is to compute this conditional density rate.",
                    "label": 0
                },
                {
                    "sent": "This density ratio, which you can right because of this equivalence that we have as the ratio of two conditional probabilities P of X given Y equals class one and P of X / P of X given Y equals class minus one.",
                    "label": 0
                },
                {
                    "sent": "Now using Bayes rule I can replace the terms of these conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So I do a base substitution and now I'm going to get these two terms here and now I want to do a bit of a cancellation, so P of X is the same on both sides, so I'm just going to remove those two terms.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to assume that the data set that you have from X~ and the data set that you have from X half equally balanced.",
                    "label": 0
                },
                {
                    "sent": "But if they are not equally balanced, you can address that imbalance by these two probabilities at the bottom P of y = + 1 and P of Y = -- 1 but.",
                    "label": 0
                },
                {
                    "sent": "Let's assume it's equal, so they also cancel and then basically what is left is this this quantity.",
                    "label": 0
                },
                {
                    "sent": "It says that the density ratio we are actually interested in is actually equal to the ratio of the two cloud conditional probabilities, right?",
                    "label": 0
                },
                {
                    "sent": "And because so that is the main point, I want to leave with you the problem of computing density ratio estimation is equal to the problem of class probability estimation.",
                    "label": 0
                },
                {
                    "sent": "And now what is the point of all of this?",
                    "label": 0
                },
                {
                    "sent": "So basically, let's try and learn this classifier at the top so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a scoring function now I need to learn the probability P of y = 1 given X, so I'm going to create a scoring function D which is theater of X.",
                    "label": 1
                },
                {
                    "sent": "Then obviously 1 minus that quantity is, the other is the class of the negative term because I'm doing binary classification, the Bernoulli outcome is going to tell me that the log likelihood is going to be log of D of the ex hatch data set minus log 1 -- D of the other data set, right?",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to develop basically a 2 sample criterion.",
                    "label": 0
                },
                {
                    "sent": "Which is going to tell you, you can optimize this loss function where X hat is one data set, an X~ is another data set.",
                    "label": 0
                },
                {
                    "sent": "These two can be any different data.",
                    "label": 0
                },
                {
                    "sent": "One can be a training data set and the other one can be a data set which you generated from your model and so now how can we use this kind of criterion in different ways?",
                    "label": 1
                },
                {
                    "sent": "So typically when we do generative, I've assumed up to this point that xgen and X abzar fixed datasets, but when we do learning we xgen is not fixed.",
                    "label": 0
                },
                {
                    "sent": "It is also something that we are learning on the fly.",
                    "label": 0
                },
                {
                    "sent": "So in the case of transformation models, for example, where you have Z pass through some function and you get X Ed, we can replace X Gen in this 2 sample criterion.",
                    "label": 0
                },
                {
                    "sent": "With this function F directly right?",
                    "label": 0
                },
                {
                    "sent": "So now the two sample criterion leads us to what ended up in the adversarial networks, which is that you get this loss function, which tells you to take the expectation of the log of your scoring function under observed data an under the other other day.",
                    "label": 1
                },
                {
                    "sent": "Yes, the second equation is missing some indicator functions to say that.",
                    "label": 0
                },
                {
                    "sent": "You're talking about the coming from one class with another class, right?",
                    "label": 0
                },
                {
                    "sent": "Well, this equation here.",
                    "label": 0
                },
                {
                    "sent": "Second, yeah, yes, I've written here X, as in X~ which already is.",
                    "label": 0
                },
                {
                    "sent": "The indicator is implied in there.",
                    "label": 0
                },
                {
                    "sent": "If I written it up in the previous notation of X given Y one, then there would be a D in front in a 1 -- T at the front.",
                    "label": 0
                },
                {
                    "sent": "But yes, it's just simplify Rota XNXX tilted directly so you would see the connection OK, so now this loss functions are very different beasts from the variational loss function you saw before.",
                    "label": 0
                },
                {
                    "sent": "The variational loss function was a lower bound, which means that any optimization you do on that bond always guaranteed to improve the bound, so it gives you a tool for debugging this kind of loss function is sort of playing a game with itself.",
                    "label": 0
                },
                {
                    "sent": "A classifier always wants to find a clear decision boundary to tell you what is generated and what is not.",
                    "label": 0
                },
                {
                    "sent": "But we actually do not want the classifier to work this way.",
                    "label": 0
                },
                {
                    "sent": "We want our classifier to have probability .5 for generated and two data points, so then.",
                    "label": 1
                },
                {
                    "sent": "Basically, we attain this alternating optimization principle, where we maximize the classifiers parameters Theta and we minimize the classifier parameters Phi, right?",
                    "label": 0
                },
                {
                    "sent": "But this is just in the case of these transformation models.",
                    "label": 0
                },
                {
                    "sent": "If you are working other classes of models like undirected models are directed models this same 2 sample criterion will be called noise contrastive estimation.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you are doing it more statistical theory then 2 sample ratio density ratio estimation with something you do a lot of the time important estimation that you see in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Comes up all the time and you can use this as a solution to that problem as well.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so are there any questions on maybe 2 sample testing evidence estimation or learning algorithms in general?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go on.",
                    "label": 0
                },
                {
                    "sent": "So now that we have models and we have inference methods, we're going to put them together and we can build different kinds of algorithms, and I want to look at about 3 different principles for building algorithms.",
                    "label": 0
                },
                {
                    "sent": "There will be stochastic approximation, amortized inference, and stochastic optimization.",
                    "label": 1
                },
                {
                    "sent": "You've seen all of these already, yes?",
                    "label": 0
                },
                {
                    "sent": "Fit like CD training of GBM from that.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think you could.",
                    "label": 0
                },
                {
                    "sent": "But well, yes you can because they do it in the noise contrastive estimation.",
                    "label": 0
                },
                {
                    "sent": "That is exactly what you do for log linear models, so I don't know if it works that well, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's not bad, it's actually as good as doing anything else, but typically people do it in the language modeling case, which is, which is a good good use case for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the classic thing.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you learned in the textbooks of machine learning was about the EM algorithm and the EM algorithm will be the basis of everything.",
                    "label": 0
                },
                {
                    "sent": "If you know this, you'll know almost everything in machine learning, so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do exactly that, so the EM algorithm basically operates.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "You have a loss function as follows.",
                    "label": 0
                },
                {
                    "sent": "An EM algorithm is just an alternating optimization between two sets of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So the variational EM is actually a super set of the standard EM algorithm, so I just need to talk about only variational Emmanuel, understand them so the alternating optimization of model parameters and distribution parameters or variational parameters, right?",
                    "label": 1
                },
                {
                    "sent": "So any EM algorithm always works as follows.",
                    "label": 0
                },
                {
                    "sent": "You write a for loop and then you repeat eastep computation, compute the gradients of the function with respect to the variational parameters Phi.",
                    "label": 0
                },
                {
                    "sent": "Then you go to an M step.",
                    "label": 0
                },
                {
                    "sent": "You compute the gradient with respect to the model parameters Theta, and then you do this in a loop and you iterate and then because the EM algorithm is always optimizing abound every step improves this free energy up until you can't learn anymore from the data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The let me go and unpack this EM algorithm in a little bit more detail.",
                    "label": 0
                },
                {
                    "sent": "So again they EM algorithm always says repeat the following steps and then you do your eastep you say for I = 110.",
                    "label": 1
                },
                {
                    "sent": "If you've written in EM algorithm, you can remember writing this in your code.",
                    "label": 0
                },
                {
                    "sent": "Then you do the gradient update step for the variational parameters 5, which is compute the gradient of this quantity F with respect to Phi Phi lives in the term Q.",
                    "label": 0
                },
                {
                    "sent": "Then you do that for all the datasets and once you are done you store all the files in a table.",
                    "label": 0
                },
                {
                    "sent": "M Step update which says computes the Theta using all the files.",
                    "label": 0
                },
                {
                    "sent": "This expectation of a fire requires you to have all that knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the key point that I wanted to bring out here is that here you do a sum for all the data in your data set.",
                    "label": 0
                },
                {
                    "sent": "And here there's another sum for all the data in the data set.",
                    "label": 0
                },
                {
                    "sent": "So in the 90s we could do because N 100 or 1000, but in 2016 an is 10,000,000.",
                    "label": 0
                },
                {
                    "sent": "This sum is not possible, but this is the exact gradient.",
                    "label": 0
                },
                {
                    "sent": "So instead what you can do is replace N with something smaller.",
                    "label": 0
                },
                {
                    "sent": "A mini batch of data instead, so this is called the stochastic approximation technique and because you re sample N data points from the full data set, this introduces noise, which is why it's called a stochastic approximation, and this has various names online.",
                    "label": 1
                },
                {
                    "sent": "EMS the classic approximation, am stochastic variational inference and it is the main tool that you use for stochastic gradient descent, right?",
                    "label": 0
                },
                {
                    "sent": "Annual do this now.",
                    "label": 0
                },
                {
                    "sent": "Basically this is doing stochastic gradient descent for these two terms, so there's one other sort of problem with this with this setup.",
                    "label": 0
                },
                {
                    "sent": "In the eastep so.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look here at the eastep.",
                    "label": 0
                },
                {
                    "sent": "This eastep is what I typically Cora memory list step.",
                    "label": 0
                },
                {
                    "sent": "So for every data point equals one to N you compute 5 and buy some optimization an you stored at five in the table.",
                    "label": 0
                },
                {
                    "sent": "When you compute five of one, you compute that optimization computing Phi 2 does not use any knowledge of fire one.",
                    "label": 0
                },
                {
                    "sent": "The fact that you've computed this before you redo it again and you do this for all the data points.",
                    "label": 0
                },
                {
                    "sent": "Every file that you compute users.",
                    "label": 0
                },
                {
                    "sent": "No knowledge of the fact that you've already computed some files before.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you can do is now replace this entire East step by using a model instead.",
                    "label": 1
                },
                {
                    "sent": "So this is the principle of amortizing some computation into a model, and so this is how the principle of an inference network kinda get introduced into a probabilistic modeling scenario, right?",
                    "label": 0
                },
                {
                    "sent": "So you have you introduce AQ Network, which is now an encoder which will basically summarize all the data points and allow you to reuse and transfer the fact that you've done eastep computations between them.",
                    "label": 1
                },
                {
                    "sent": "This inference network introduces a set of global parameters into the model that's used for testing train, and I call this amortizing because there's this dark knowledge.",
                    "label": 1
                },
                {
                    "sent": "If you remember this dark knowledge notation that gets entered into this network that helps you spread the cost of learning eastep learning files for every data point, right, you can now do a joint optimization, which is very nice, and so the point is that inference networks provide you an efficient mechanism for posterior inference with memory.",
                    "label": 1
                },
                {
                    "sent": "In these kind of latent variable models, for example, or any kind of Bayes net.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, amortized variational inference then basically takes the idea of using these kind of inference network within the variational inference setup.",
                    "label": 1
                },
                {
                    "sent": "So just to remind you again of this free energy bound we have expectation of QZ, which is the approximate posterior.",
                    "label": 1
                },
                {
                    "sent": "We have our reconstruction term in our penalty term then basically by introducing an encoder network we form a stochastic encoder decoder network.",
                    "label": 1
                },
                {
                    "sent": "So basically any stochastic encoded decoded system will effectively implement variational inference.",
                    "label": 0
                },
                {
                    "sent": "And so basically what you have the model, you can now call a decoder, which is your likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Your inference network is an encoder which takes data X and gives you said.",
                    "label": 1
                },
                {
                    "sent": "And basically you can also see this as a way of transforming an autoencoder into a generative model.",
                    "label": 0
                },
                {
                    "sent": "Right, so when we have a specific combination of variational inference in latent variable models using inference networks, this specific combination of these three things gives you an algorithm that we refer to as a variational autoencoder, but always remember what the model is that you are using and what kind of inference for any model you can replace it with a different inference and you get a different kind of algorithm.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I wanted to just connect this to the problem of compression.",
                    "label": 1
                },
                {
                    "sent": "Since we've established these encoder decoder systems, so in minimum description length compression you basically have exactly the same loss function where Q XD will be a stochastic encoder.",
                    "label": 1
                },
                {
                    "sent": "Then you have a term which represents the data code length, which is what you want to measure and then you have a hypothesis code which is what you want to produce.",
                    "label": 1
                },
                {
                    "sent": "And again this is a different kind of stochastic encoder decoder system and the principle of minimum description length ask you or requires you to.",
                    "label": 1
                },
                {
                    "sent": "Except that because there's regularity in the data, the latent variables can be used to compress this data, and so the minimum description length ask you to find the shortest message for your given data and the shortest message is this marginal likelihood bound.",
                    "label": 1
                },
                {
                    "sent": "So there's the encoder decoder system and the approximation to the ideal message, which you can't compute is this variational bound.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just wanted to connect to other kinds of models that you might be using, so message passing is another very important inferential approach, which is very different from the approach that I just described.",
                    "label": 0
                },
                {
                    "sent": "And typically you have a factor graph of this kind and the factor graph has certain factors in their assumptions, usually a factorized assumption on these graphs and the memoryless inference approach is just to basically do exactly what we are doing in the East step of the EM algorithm, which means compute this argument, which in the case of cavity methods involves computing the projection and there re optimization of the distribution.",
                    "label": 0
                },
                {
                    "sent": "The amortized inference is very simple.",
                    "label": 0
                },
                {
                    "sent": "Replace their computation within neural network and then you know things will workout the function.",
                    "label": 0
                },
                {
                    "sent": "In this case can be anything can be a tree, can be a neural network, can be a set of basis functions, but the principle of amortization is more general than that, again, you.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use the same kind of principle for predictive distributions.",
                    "label": 1
                },
                {
                    "sent": "If you are thinking about building Bayesian neural networks, Bayesian neural networks are different.",
                    "label": 1
                },
                {
                    "sent": "You have data X which you observed and why we have observed and what is random in the parameters of that distribution.",
                    "label": 0
                },
                {
                    "sent": "You typically need to compute some predicted probabilities and the memory list prediction basically asked you to do Monte Carlo integration.",
                    "label": 1
                },
                {
                    "sent": "Again, the amortized inference is related to the idea of distillation if that has come up during the week of user deep network distill this Monte Carlo computation into a neural network so you can do that as well.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the last bit in this section I want to talk about was stochastic optimization, so this common gradient problem you have seen everywhere this week it is compute the gradient with respect to some parameters of an expectation of a function.",
                    "label": 1
                },
                {
                    "sent": "This is the problem of variational inference.",
                    "label": 0
                },
                {
                    "sent": "It is the problem of the importance weighted autoencoders.",
                    "label": 0
                },
                {
                    "sent": "It is the same problem that you have in reinforcement learning in variational inference is the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "In iOS it's also the log likelihood in reinforcement learning, F is the return of the reward function.",
                    "label": 0
                },
                {
                    "sent": "It's basically the same problem and in other areas you know this probably is one of the biggest problems you have in computational science, so this sort of two terms is really difficult to compute this thing a you don't actually know this expectation.",
                    "label": 0
                },
                {
                    "sent": "If you knew the expectation, then things are very simple and be the expectation is with respect to the gradient you want is with respect to these parameters.",
                    "label": 1
                },
                {
                    "sent": "Phi, which live in the distribution you want to take the expectation with.",
                    "label": 0
                },
                {
                    "sent": "So as I said that lots of problem areas, generative models.",
                    "label": 1
                },
                {
                    "sent": "As I'm describing, is one of them reinforcement learning that you saw yesterday is another one.",
                    "label": 0
                },
                {
                    "sent": "Operations research and inventory control has this Monte Carlo simulations of weather phenomena has this problem.",
                    "label": 1
                },
                {
                    "sent": "If you want to do asset pricing and finance, then you will also see this problem.",
                    "label": 0
                },
                {
                    "sent": "So it is really the same problem everywhere, so there are typically two approaches.",
                    "label": 0
                },
                {
                    "sent": "You can take a deterministic approach, which means introducing some deterministic bound on F, and if the bond is deterministic and you can integrate each of these things, then the problem is very simple.",
                    "label": 1
                },
                {
                    "sent": "In the 2000s they would call these local variational methods.",
                    "label": 0
                },
                {
                    "sent": "People don't like them as much anymore because.",
                    "label": 0
                },
                {
                    "sent": "Finding good deterministic approximations can be quite hard, but the one that we prefer these days are stochastic methods, so they compute this expectation by Monte Carlo by exploiting properties of these distributions and these functions right?",
                    "label": 1
                },
                {
                    "sent": "So there are two ways of doing this.",
                    "label": 1
                },
                {
                    "sent": "You can either take the derivative of the function F, in which case we're going to call that the pathwise estimator, or you can take the derivative of the function Q, in which case we're going to call that this core function estimator, and they have very different properties and different ways in which you can use them so.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you seem all of these yesterdays.",
                    "label": 0
                },
                {
                    "sent": "I'll go through very quickly.",
                    "label": 0
                },
                {
                    "sent": "The score function estimator says I'm not going to make any assumptions about FF can be non differentiable, but Q should be something that is easy to sample from.",
                    "label": 1
                },
                {
                    "sent": "So what I can do is then.",
                    "label": 0
                },
                {
                    "sent": "Transform this quantity into this this new gradient estimator is actually very simple graded estimator.",
                    "label": 0
                },
                {
                    "sent": "You saw it yesterday by using the likelihood ratio trick.",
                    "label": 0
                },
                {
                    "sent": "This gradient is a is very interpretable.",
                    "label": 0
                },
                {
                    "sent": "It's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a very randomized search, So what the gradient tells you to do is choose some zed at random from everywhere that you see.",
                    "label": 0
                },
                {
                    "sent": "And now I'm just going to evaluate how good this random said is, so I'm going to look what the cost of doing that zed is.",
                    "label": 0
                },
                {
                    "sent": "So take F of dead and look how good it was.",
                    "label": 0
                },
                {
                    "sent": "If it was good, F is going to be high.",
                    "label": 0
                },
                {
                    "sent": "If that said was bad, F is going to be low, and then I'm going to say, well just take the gradient with respect to that, then and then re weighted by the cost.",
                    "label": 0
                },
                {
                    "sent": "So it's a very randomized.",
                    "label": 0
                },
                {
                    "sent": "Sort of search, but you know this is what you have to do if you don't want to make assumptions and so you can see that the variance of these kind of estimate is going to be very high right?",
                    "label": 0
                },
                {
                    "sent": "F of Zed is typically going to be a sum of K terms and so the variance of this kind of estimate is going to scale linearly around order of K. So but you can do a lot of tricks to make this manageable, so it has many different names in the literature.",
                    "label": 1
                },
                {
                    "sent": "The likelihood ratio methods is what you will see in reinforcement learning, reinforcing policy gradients.",
                    "label": 0
                },
                {
                    "sent": "Automated inference is 1 black.",
                    "label": 0
                },
                {
                    "sent": "Box inference is another method one.",
                    "label": 0
                },
                {
                    "sent": "The other way is instead to use properties of Q&F directly, so F is now a differentiable function if F is differentiable, then you can actually use a little bit more information, so I want you to recall those transformation models again, which was this piping system that we have that takes noise and gives you random variables.",
                    "label": 0
                },
                {
                    "sent": "So if you have this, you can re express this in rather in the system of pipes are saying zed can be expressed in terms of a deterministic functions Ed with some random noise epsilon so.",
                    "label": 0
                },
                {
                    "sent": "Then what you can do is wherever you see zed in this aspect expression, just replace it with G, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the expression that you get, and it's very.",
                    "label": 1
                },
                {
                    "sent": "It's very intuitive because all this expression tells you to do is just do back prop.",
                    "label": 0
                },
                {
                    "sent": "Just take the gradient and push it all the way through all the functions that you have.",
                    "label": 1
                },
                {
                    "sent": "And because of that reason in an earlier paper, we thought you called this stochastic backpropagation, but it has many different names.",
                    "label": 1
                },
                {
                    "sent": "If you look in this finance literature or stochastic optimization literature, they will typically call this perturbation analysis.",
                    "label": 0
                },
                {
                    "sent": "The reparameterization trick which Russ mentioned, I find independent inference methods right?",
                    "label": 0
                },
                {
                    "sent": "So these are the two estimators that you get to choose from.",
                    "label": 0
                },
                {
                    "sent": "There are other classes, at least two other classes of stochastic gradient estimators, but these two are the most general ones in the most useful ones for problems in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And so when you combine these kind of stochastic gradient estimators with estimators that use mini batches, then you get what are called doubly stochastic estimation problems, right?",
                    "label": 1
                },
                {
                    "sent": "And you have to sort of control.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The noise.",
                    "label": 0
                },
                {
                    "sent": "So great, I have half an hour left.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions on that section?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Five, yes, both estimators are biased in this case.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very easy.",
                    "label": 0
                },
                {
                    "sent": "To see to see in both cases, because in the in the pathways estimated because you can undo, you get back the same the same integral that you had before, and in this case here, because you can always re weight.",
                    "label": 0
                },
                {
                    "sent": "In both cases there we actually use the property there unbiased to do that you can create unbiased estimators which also work.",
                    "label": 1
                },
                {
                    "sent": "But you know people don't like unbiased estimators.",
                    "label": 1
                },
                {
                    "sent": "To know that.",
                    "label": 0
                },
                {
                    "sent": "You need to.",
                    "label": 0
                },
                {
                    "sent": "You need to make you need to know what Q is and you need to exploit that knowledge.",
                    "label": 1
                },
                {
                    "sent": "If you aren't willing to exploit that knowledge then you can only use the other estimator, right?",
                    "label": 0
                },
                {
                    "sent": "It's all again about knowing your problem and deciding what is best for your problem, OK?",
                    "label": 0
                },
                {
                    "sent": "So I wanted to use a case study.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our variational autoencoders.",
                    "label": 0
                },
                {
                    "sent": "But this can be for any kind of generative models to explore different ways of building these kind of models and different assumptions that she can make.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, a variational autoencoder is the principle of amortized inference applied to latent variable models, and this is the loss function that we have, which is this variational free energy and we've already introduced that way of dealing with this is by building a model and an inference network.",
                    "label": 1
                },
                {
                    "sent": "Right and there are many design choices that you have to make.",
                    "label": 0
                },
                {
                    "sent": "You can decide what kind of latent variables you're going to use.",
                    "label": 0
                },
                {
                    "sent": "Are they discrete?",
                    "label": 0
                },
                {
                    "sent": "Are they continuous?",
                    "label": 0
                },
                {
                    "sent": "Are they gaussians?",
                    "label": 0
                },
                {
                    "sent": "Are there bernoulli's?",
                    "label": 0
                },
                {
                    "sent": "Are there some mixtures of things?",
                    "label": 1
                },
                {
                    "sent": "This is something you have to decide.",
                    "label": 0
                },
                {
                    "sent": "You have to choose what kind of model or likelihood function you have.",
                    "label": 0
                },
                {
                    "sent": "Are you happy to deal with ID data?",
                    "label": 0
                },
                {
                    "sent": "Is there some sequential aspect to this data?",
                    "label": 0
                },
                {
                    "sent": "Are there temporal properties you need to model?",
                    "label": 0
                },
                {
                    "sent": "Are there spatial properties and all of this knowledge you need to build in, and then this inference network you need to alter disid some properties.",
                    "label": 0
                },
                {
                    "sent": "What kind of distribution does it have?",
                    "label": 0
                },
                {
                    "sent": "Can it be multimodal?",
                    "label": 0
                },
                {
                    "sent": "Is it sequential?",
                    "label": 0
                },
                {
                    "sent": "Does it have spatial properties?",
                    "label": 0
                },
                {
                    "sent": "So all of these things you need to do, but the things you do not need to choose because this is what we'll do for large scale deep learning is will always use stochastic gradient descent and will always use stochastic gradient estimation.",
                    "label": 1
                },
                {
                    "sent": "These two properties of inference that we just did.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just wanted to make a quick mention about how you implement these things.",
                    "label": 0
                },
                {
                    "sent": "These days we have many different tools which make implementation easier, so we have Theano and Torch and Tensorflow and stand.",
                    "label": 0
                },
                {
                    "sent": "All of them are easy and if you are doing message passing then infer.net is a good one and so you have a forward pass which is sort of as follows.",
                    "label": 1
                },
                {
                    "sent": "You take data X and you pass it through a box which is some function.",
                    "label": 0
                },
                {
                    "sent": "And out of this function needs to come a sample of the latent variable and the computation of the entropy.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the function that you need to compute.",
                    "label": 0
                },
                {
                    "sent": "Which will call the recognition model or the encoder.",
                    "label": 0
                },
                {
                    "sent": "Then you need to implement two other things.",
                    "label": 0
                },
                {
                    "sent": "You implement another box which is the prior.",
                    "label": 0
                },
                {
                    "sent": "Typically the prior doesn't have any parameters, so it just needs to evaluate the log log probability of a Gaussian and then you need to implement one last function, which will be the model and the model.",
                    "label": 0
                },
                {
                    "sent": "We're actually put all this likelihood computation and structure in there.",
                    "label": 0
                },
                {
                    "sent": "Yes, in principle you don't need to have another big expression for the entropy.",
                    "label": 0
                },
                {
                    "sent": "You could also sample it if you have Q.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah it can, this how HQ gets computed can be by Monte Carlo or.",
                    "label": 0
                },
                {
                    "sent": "If you know it, it can be done exactly so, but once you put these things into a computational framework like torture, Theano, Tensorflow, you immediately get back the backward graph and the backward grapher.",
                    "label": 1
                },
                {
                    "sent": "Then when you go backwards, computes the gradient computer grading on the product gives you the inference, so all the pictures that I'm going to draw are drawn in boxes of these three types.",
                    "label": 0
                },
                {
                    "sent": "Of that you see exactly how you implement it and what the flow of the gradient is like.",
                    "label": 0
                },
                {
                    "sent": "Because the flow of the gradient matches depends on the type of estimated for the gradient shoes.",
                    "label": 0
                },
                {
                    "sent": "Ideally what you wanted some kind of.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic programming that uses variational inference in this case, and I think we're very close to that given the tools that we have, right?",
                    "label": 1
                },
                {
                    "sent": "So the standard things are always the case.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient descent and precondition optimization you'll use, like RMS proper Adam, you implement your code in Jeep's of some distributed clusters.",
                    "label": 1
                },
                {
                    "sent": "And because these probabilistic models are modular, when you code it up, this prior itself can be an entire via E, and so you can build via modules.",
                    "label": 0
                },
                {
                    "sent": "You can use that in a prior of another via module.",
                    "label": 0
                },
                {
                    "sent": "And you can make a very complicated codebase this way.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the simplest kind of VE that you can deal with this sort of this latent Gaussian VE an the prior is just a Gaussian distribution and so basically you implement and evaluate of a Gaussian probability which is easy to do.",
                    "label": 1
                },
                {
                    "sent": "Then you have a model P of X given zed, which is very simple.",
                    "label": 0
                },
                {
                    "sent": "It says just take give me the probability of X given some function of those latent variables and the function.",
                    "label": 0
                },
                {
                    "sent": "In the simplest case being via Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But this can be any distribution, exponential, family or otherwise, and it has some parameters Theta and the recognition network is going to be equally simple.",
                    "label": 0
                },
                {
                    "sent": "It's just going to give you a Gaussian with some nonlinear functions from yuan for Sigma, and when you plug this into automatic differentiator, you'll get the gradients of all of these things, their model, and the inference can be fully connected, or it can be convolutional.",
                    "label": 0
                },
                {
                    "sent": "This is easy to do, and so when you read in supervised learning all the latest advances they have in building models, when you see things like batch normalization, dilated convolutions, and skip recurrent connections and whatever you plug them.",
                    "label": 0
                },
                {
                    "sent": "Into these models, and you build better models that way, and so there is sort of this cycle of life which happens in machine learning through all parts, right?",
                    "label": 1
                },
                {
                    "sent": "So all the functions are dinner.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these kind of latent Gaussian vieze can do a lot of interesting things.",
                    "label": 1
                },
                {
                    "sent": "So the first one I mentioned was about this visual concept learning, so they can dissent.",
                    "label": 1
                },
                {
                    "sent": "Angle input data in interesting ways.",
                    "label": 0
                },
                {
                    "sent": "I showed you the video, it's not playing here.",
                    "label": 1
                },
                {
                    "sent": "About how it learns, separate the fact that there is an oxygen bar from the motion of the of the submarine, which can go up or down.",
                    "label": 0
                },
                {
                    "sent": "These latent variables can help you do visualization of very high dimensional data.",
                    "label": 1
                },
                {
                    "sent": "Explain how these images were obtained by moving in that space, yes?",
                    "label": 0
                },
                {
                    "sent": "So yeah, basically you take zed with 10 latent variables.",
                    "label": 0
                },
                {
                    "sent": "You clamp nine of them, and then you very one, and then you look to see exactly what happens and then you see one of them deals with oxygen bar.",
                    "label": 0
                },
                {
                    "sent": "Another one is just with the motion up and down.",
                    "label": 0
                },
                {
                    "sent": "And I think this is a nice property that you want out of systems that reason yes.",
                    "label": 0
                },
                {
                    "sent": "How can you do it if it's.",
                    "label": 0
                },
                {
                    "sent": "If it's a prior, the Gaussian once again arbitral big things.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so in that particular case what you do is you introduce an additional penalty onto the scale constraint that reduces its contribution, and then what comes out is sort of these kind of contributions.",
                    "label": 0
                },
                {
                    "sent": "But yeah, Gaussians, but once you have a fixed model that when the parameters are fixed, then this problem disappears, right?",
                    "label": 0
                },
                {
                    "sent": "'cause then, for a fixed model then permutations don't exist anymore.",
                    "label": 0
                },
                {
                    "sent": "I mentioned visualization of very high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "And in this particular graph, what is very nice with these kind of models when you can actually evaluate the probability is that you can plot all your data and you can resize the shapes by the contribution of their marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So data points that have a great deal of contribution have very large shape and things near the decision boundary have very small contribution to the probability.",
                    "label": 1
                },
                {
                    "sent": "And in this video here is sort of just exploring this dimensions of MNIST videos.",
                    "label": 0
                },
                {
                    "sent": "Let me see if it plays yes.",
                    "label": 0
                },
                {
                    "sent": "I just I didn't quite understand your answer to this question is we know the Villa already does this in tangling all on its own.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that wasn't enough, so for that that's not enough.",
                    "label": 0
                },
                {
                    "sent": "'cause face these kind of games have a bit more other properties.",
                    "label": 0
                },
                {
                    "sent": "They are very deterministic.",
                    "label": 0
                },
                {
                    "sent": "So then you want to sort of emphasize the deterministic aspect more so you down weighted scale term relative to the to the reconstruction term and then you can sort of see these kind of structures come out.",
                    "label": 0
                },
                {
                    "sent": "So was that your answer to get?",
                    "label": 0
                },
                {
                    "sent": "Yeah, different dimensions to not be rotation variance.",
                    "label": 0
                },
                {
                    "sent": "Yes, at least in this case that's just one simple way of doing it.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, as you said, they can do them themselves and faces data you don't have to do anything special.",
                    "label": 0
                },
                {
                    "sent": "Are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "While we understand why playing with the strength of the kilt room has anything to do with the rotation in that space?",
                    "label": 0
                },
                {
                    "sent": "So what this happens?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is, uh, I'll tell you, we could talk about this often.",
                    "label": 0
                },
                {
                    "sent": "A big debate.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I wanted to mention VA representation so you can learn a latent variable model of these data set.",
                    "label": 0
                },
                {
                    "sent": "This environment is called Labyrinth and the task that you have to do is called forage and avoid.",
                    "label": 0
                },
                {
                    "sent": "You need to go and collect the green Apple Dan you need to avoid the lemons right and what you do is you learn of EAE and these latent variables now form the representation and you store these representations in what is called an episodic memory, and episodic memory is basically AK nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And what gets stored in this K nearest neighbor memory is the actual representation.",
                    "label": 0
                },
                {
                    "sent": "The return that you got from that episode in the data as well as the action that you took.",
                    "label": 0
                },
                {
                    "sent": "And it always stores the best return.",
                    "label": 0
                },
                {
                    "sent": "So this is a system of episodic control, and this is one way that generative models can help in the problem of control, yes.",
                    "label": 1
                },
                {
                    "sent": "So in this case that you know, one thing that the NRL people are grappling with is that if you just train good models that they produce the dynamics of the environment is not enough to really have anything with models where the reward is yes.",
                    "label": 0
                },
                {
                    "sent": "Right here as opposed to just the distribution of the data information.",
                    "label": 0
                },
                {
                    "sent": "So the generative model is only used to give you the state representation.",
                    "label": 0
                },
                {
                    "sent": "Once you have the state representation that gets stored along with the reward, not, there would actually the return for that episode, right?",
                    "label": 0
                },
                {
                    "sent": "The return does not inform the state representation.",
                    "label": 0
                },
                {
                    "sent": "In this case, it would be nice to have maybe something that does inform the state representation, but you also sort of an exploration to see if how modular kind of system we can build in this case, But in this particular case the return is very tight.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, because at least in this kind of simple or in that I've made navigation environment.",
                    "label": 0
                },
                {
                    "sent": "In this case projections, random projections also work, they just don't work as well.",
                    "label": 0
                },
                {
                    "sent": "The have an experiment that compares random projections versus VH to look at exactly what other kind of sources you can do.",
                    "label": 0
                },
                {
                    "sent": "Propagate through the state representative.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you want to have a model that's part of the learning system and you know it's very difficult to differentiate through K nearest neighbors, and so this is the problem.",
                    "label": 0
                },
                {
                    "sent": "But because we're doing lots of research that you saw earlier in the week about memory systems, I think this will also get addressed very soon, so this is the problem that we call episodic control just to explore.",
                    "label": 0
                },
                {
                    "sent": "One way of using generative models for control prop.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the problems that comes up from these latent Gaussian models is that the real kind of posterior distributions that you see are very complicated.",
                    "label": 1
                },
                {
                    "sent": "They have distributions that look like this, and like this, and if you use a Gaussian that might be OK.",
                    "label": 0
                },
                {
                    "sent": "But real distributions actually look more like this, or like this, or like this, the grey stuff is the true posterior in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "Yes, everything has published.",
                    "label": 0
                },
                {
                    "sent": "I'll put the references, I just haven't had time so.",
                    "label": 0
                },
                {
                    "sent": "What was I saying?",
                    "label": 0
                },
                {
                    "sent": "Power of good posteriors.",
                    "label": 0
                },
                {
                    "sent": "OK, so real posteriors look more like things like this if you put a 10H in your model all of a sudden there's a donut with a hole, and so really you need to build a Gaussian for posterior distribution is not going to be enough, so we're going to have to do a lot of work in this this domain, so we'll come back to that a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's replace the Gaussian and put a binary latent variable instead, and so same model.",
                    "label": 0
                },
                {
                    "sent": "You implement a box that gives you an auto regressive Bernoulli distribution is 1 example.",
                    "label": 0
                },
                {
                    "sent": "You build a model may be the output is also an autoregressive Bernoulli distribution.",
                    "label": 0
                },
                {
                    "sent": "And then you build the same thing through.",
                    "label": 0
                },
                {
                    "sent": "Prince Network that gives you binary data.",
                    "label": 0
                },
                {
                    "sent": "Everything is binary in this case very similar to sigmoid belief natural.",
                    "label": 0
                },
                {
                    "sent": "This is called the deep auto regressive network.",
                    "label": 0
                },
                {
                    "sent": "And again you can derive the gradient estimation in this case by using the score score function technique, and in fact in that paper there was a slightly biased estimator, but.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so this is some of the things that you can do with the earliest versions of this kind of model with binary latent variables, But I just wanted to point it out to show that if you are interested in having binary latent variables because they offer you interpret ability in some way or other ways of disentangle in your data, then that is possible to do so.",
                    "label": 0
                },
                {
                    "sent": "These ones on Binarized Atari games.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem of semi supervised variational autoencoders is another good one to look at.",
                    "label": 0
                },
                {
                    "sent": "Here we can do a little bit more.",
                    "label": 0
                },
                {
                    "sent": "We build our prior function and we also include a prior over classes which will assume it's an observe these two things feed into the model and then we build an inference network that gives you both the posterior distribution on latent variables as well as the posterior distribution on the class probabilities.",
                    "label": 0
                },
                {
                    "sent": "And again like we showed for the generative models, the progress in semi supervised learning has really gone well.",
                    "label": 0
                },
                {
                    "sent": "And you know the generative approach really is the state of the art.",
                    "label": 0
                },
                {
                    "sent": "For Semi supervised learning right now.",
                    "label": 0
                },
                {
                    "sent": "And I think there will be a lot more work coming in this front and they let you explore the data in various different ways.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem of these posteriors that I mentioned can be addressed by a different kind of way of building up your posterior, and this is a model that we call draw.",
                    "label": 0
                },
                {
                    "sent": "So draw itself is very much like a latent Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It is a latent Gaussian VE, but I'll call it a sequential latent Gaussian VE.",
                    "label": 1
                },
                {
                    "sent": "You have a prior box, and the prior itself will have a sequential structure.",
                    "label": 0
                },
                {
                    "sent": "It will say forms that I given all zeds that have come before, and how this conditioning I'm going to leave to detail for the next slide.",
                    "label": 0
                },
                {
                    "sent": "Then.",
                    "label": 0
                },
                {
                    "sent": "Similarly, you can build a model which is going to.",
                    "label": 0
                },
                {
                    "sent": "Form this distribution based on all these ads that you have generated and then the inference is also going to generate these beds in a sequential fashion so.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In more detail how we actually do that.",
                    "label": 0
                },
                {
                    "sent": "You do this sequentially.",
                    "label": 0
                },
                {
                    "sent": "You take time .1 you generate some prior and then you fuse this using some RNN and LTM.",
                    "label": 1
                },
                {
                    "sent": "In this case you do it again, generate a new source of randomness and fuse it this way and eventually you can do this for as many steps as you like and then you can generate the final state and then you can generate your model and generate the data.",
                    "label": 0
                },
                {
                    "sent": "And the inference is where this comes in.",
                    "label": 0
                },
                {
                    "sent": "You can take your data X with some state and generate this Gaussian and then you do the inference sequentially.",
                    "label": 1
                },
                {
                    "sent": "And because of the conditioning of how the distributions Q of Z is related to Q. XD I in a very non linear way the final distribution which is the collection of all the zeds is something multimodal can be very complicated and it's actually the thing that will help you get very good results.",
                    "label": 1
                },
                {
                    "sent": "So these things can be LTM or Gru units if you want to include attention like we heard this morning.",
                    "label": 1
                },
                {
                    "sent": "Through spatial Transformers or harder soft attention, then that's easy to do.",
                    "label": 0
                },
                {
                    "sent": "Let's see, you wanted to show you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can have different last names or you can share them in this case and you can even introduce additional canvases.",
                    "label": 0
                },
                {
                    "sent": "So here's just two videos.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are we generating faces on the multiply data set with the attention window in this case?",
                    "label": 0
                },
                {
                    "sent": "And then there's a different kind of?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love is generating omniglot digits and I just wanted to show this video to show the How the drawing process changes depending on the type of LTM that you actually use.",
                    "label": 0
                },
                {
                    "sent": "So if you have this additive canvas which is just adding things then it's more more drawing like but if you have the SGR you kind of canvas is more like a gas that is remolding itself and changing overtime.",
                    "label": 0
                },
                {
                    "sent": "But in the end you get equally good generation so additive canvas is good enough.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you wanted to build other kinds of even more structured and sequential vasc, then here's a different model of the kind to do scene interpretation.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to structure my prior in such a case that is going to tell me three things.",
                    "label": 0
                },
                {
                    "sent": "It's going to have a prior over what is in the image of prior of where those things are in the image and the prior of whether this is the last.",
                    "label": 0
                },
                {
                    "sent": "If I'm still counting how many images they are objects they are in the image, so I'm going to have a prior for as many objects as they are in the image, and then I'm going to use them together and generate the data right so?",
                    "label": 0
                },
                {
                    "sent": "Probably the prior probability is P of YP of where NP of whether I should continue to generate a new.",
                    "label": 0
                },
                {
                    "sent": "A new object, and then the likelihood is that way, and then the inference is sequential as well.",
                    "label": 0
                },
                {
                    "sent": "You take data through some LST mRNA and you form an inference over what and where and whether to continue or not and the point I want to mention about this model is that this is one of those models that combines binary and latent Gaussian Bernoulli latent variables.",
                    "label": 0
                },
                {
                    "sent": "So the binary or the continuous ones at the washing away and the discrete variable.",
                    "label": 0
                },
                {
                    "sent": "Is this probability of continuing, which helps us count how many objects they are in the image so.",
                    "label": 0
                },
                {
                    "sent": "In this video here, BASIC.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get the demo which is going online and you get to draw MNIST digits in a little window.",
                    "label": 0
                },
                {
                    "sent": "So when I draw one, basically the object knows that there's one object, and that's how to generate it.",
                    "label": 0
                },
                {
                    "sent": "As soon as I draw 4 then he knows, oh, there's a new object, it knows that it's their wages in the image an what it is.",
                    "label": 0
                },
                {
                    "sent": "These are some limitations that you see of this kind of module.",
                    "label": 0
                },
                {
                    "sent": "There's only trained on MNIST digits of a certain size, so a very long aimless digit becomes two amnist digits and eight gets recognized but very large 8.",
                    "label": 0
                },
                {
                    "sent": "Is seen AS20 so these are the kind of things of how your data is used affects your training.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, you can extend all of this to be more than spatial more than Temple.",
                    "label": 0
                },
                {
                    "sent": "You can even do volumetric so again in the sequential VE setup generate all the priors and you build your model based on volumetric data itself.",
                    "label": 0
                },
                {
                    "sent": "And here the what I wanted to point out is that this kind of model now need not be a model that you learn through a confident or volumetric convolution.",
                    "label": 0
                },
                {
                    "sent": "The model itself can also be a graphics engine and you can take derivatives through this graphics engine using that score function estimation technique.",
                    "label": 1
                },
                {
                    "sent": "Right, and so you can use volumetric convolutions and volumetric canvas.",
                    "label": 1
                },
                {
                    "sent": "In this case you can have 3D attention using 3 dimensional spatial Transformers.",
                    "label": 0
                },
                {
                    "sent": "Volume can use this third dimension to represent color channels or some volume in space, or it can even be time if you want to be do that.",
                    "label": 0
                },
                {
                    "sent": "And I mentioned the nondifferentiable aspect.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some results on this volumetric via ease of generating some 3D shapes in the shape net data set, which does really well, and in at least in this case we can actually also produce a set of benchmark results using the likelihood function to actually test how well will do on this model.",
                    "label": 0
                },
                {
                    "sent": "Here we actually use a rendering engine as the generative model itself, and then we look at the kind of reconstructions that can happen because what the model needs to do is learn a mesh representation internally that matches through the renderer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, is that.",
                    "label": 0
                },
                {
                    "sent": "3D rendering using like a 3D complement structure in the top image.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's using a 3 dimensional 3D convolution effectively and in the bottom one it's open GL render inaccurate.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think this is the last one.",
                    "label": 0
                },
                {
                    "sent": "This is the model that we call straw and this is the one where I wanted you to think a bit more about reinforcement learning again, and how we can form temporary extended action.",
                    "label": 0
                },
                {
                    "sent": "So again, you're going to have a prior and the price is going to be split in two parts.",
                    "label": 0
                },
                {
                    "sent": "We also saw something similar to this this morning and yesterday the prior is going to include a latent variable of Gaussians and then it's going to include a set of actions which is going to be conditioned on that set of latent variables.",
                    "label": 0
                },
                {
                    "sent": "The actions are UN observed.",
                    "label": 0
                },
                {
                    "sent": "They're also late, and so because they are later, we're going to have to integrate them out later on.",
                    "label": 0
                },
                {
                    "sent": "But you can implement these two boxes.",
                    "label": 0
                },
                {
                    "sent": "The action is discrete, the prior is continuous.",
                    "label": 0
                },
                {
                    "sent": "Then you have the model but in reinforcement learning if you want to be model free, you won't have access to the model.",
                    "label": 0
                },
                {
                    "sent": "You will use the environment and this will just be the reward of the return directly.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to do model based R. Oh, then you can put some kind of another generative model to model their environment of the kind that I showed you earlier here and then what you can get is the log probability of these returns and then you need to do an inference of some sequential form of taking data, giving you inference of zed giving zed and then giving you the sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to just generate one action at a time, I'm going to generate an action from now to T points in the future as well, and so this is how I'm going to do the temporally extended planning.",
                    "label": 0
                },
                {
                    "sent": "Right by generating these kind of actions and I'm sort of writing it this way because I wanted to introduce you a slightly different concept, which is the concept of the variational MDP.",
                    "label": 0
                },
                {
                    "sent": "It's also very familiar and the kind of loss function that you get out can be derived using the techniques of important sampling.",
                    "label": 0
                },
                {
                    "sent": "Then I showed you earlier and yesterday Peter mentioned a bit about these sort of entropy penalties that can be introduced, and you can see through the variational MDP framework how entropy penalties can be derived, and other ways of deriving alternative kind of penalties.",
                    "label": 0
                },
                {
                    "sent": "Right, so this kind of loss function lets you maximize the return, which is the value subject to some Cal constraints on a embedding space, but also ensure that you do some exploration.",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice principle for doing reinforcement learning and.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this kind of model we tried it out to Miss PAC Man in a number of other games that the number of actions the main one is this action plan, which is the number of actions which is discrete and then time is in this case I said, why is the current point in time?",
                    "label": 0
                },
                {
                    "sent": "And if you look there's always some grey parts in the future, white is sort of high probability of taking a particular action and you can sort of you know, and then this value function at the bottom is what we use is the baseline and it actually does pretty well and it was a good way of exploring how generative models can be easiest.",
                    "label": 0
                },
                {
                    "sent": "How you doing?",
                    "label": 0
                },
                {
                    "sent": "Well, next round I think yeah it was just the first set up to try it out and to see how these things combined together.",
                    "label": 0
                },
                {
                    "sent": "But closely planning is the thing that we actually want to do, but then you have to build more complicated feedback system and if you look at you know you have to do a lot of other.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things to actually make the optimization work.",
                    "label": 0
                },
                {
                    "sent": "This kind of optimization requires you to use everything you know about stochastic gradient estimation.",
                    "label": 0
                },
                {
                    "sent": "You have to apply the score function estimator.",
                    "label": 0
                },
                {
                    "sent": "Here you have to apply the pathwise derivative here and there.",
                    "label": 0
                },
                {
                    "sent": "So you have to do a lot of work to actually make the estimation work.",
                    "label": 0
                },
                {
                    "sent": "So, but open loop closed loop would be the best way to do OK, so that.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action actions in learning.",
                    "label": 0
                },
                {
                    "sent": "So let's summarize.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I thought I wanted to demonstrate.",
                    "label": 0
                },
                {
                    "sent": "I think I have 11 or 13.",
                    "label": 0
                },
                {
                    "sent": "I don't know different ways of building generative models this world of generative models which I wanted to share.",
                    "label": 0
                },
                {
                    "sent": "The reason that I love working in this area with you there although.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different ways of working in it, and we described three different kinds of generative models.",
                    "label": 0
                },
                {
                    "sent": "Fully observed models, transformation models, latent variable models.",
                    "label": 1
                },
                {
                    "sent": "We looked at all the progress that has happened over many years to really improving these models and making them usable in different kinds of setups.",
                    "label": 0
                },
                {
                    "sent": "We can look at the quality of the images that we have.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There were two learning principles that we looked at the principle of model evidence.",
                    "label": 0
                },
                {
                    "sent": "If you are interested in knowing the marginal probabilities or the principle of two sample testing when you aren't interested in there but still want to do some kind of probabilistic reasoning.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at amortized inference.",
                    "label": 0
                },
                {
                    "sent": "The way of introducing inference networks, stochastic optimization, and different kinds of estimation procedures.",
                    "label": 0
                },
                {
                    "sent": "Different families of EAS and different assumptions that you might want to make.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, then what is the future of generative models?",
                    "label": 1
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "There's a lot.",
                    "label": 1
                },
                {
                    "sent": "I the first one will be to come to the aid of supervised learning and reward based systems.",
                    "label": 0
                },
                {
                    "sent": "There is a strong role to have calibration confidence interval, more robustness, some sort of interpret, ability to form more data, efficient learning systems to help us be more semiparametric to combine nonparametric and parametric systems to do this new age of scientific discovery, exploratory analysis, synthesis and simulation.",
                    "label": 1
                },
                {
                    "sent": "And ultimately, to build these kind of complementary learning systems that build rich scene understanding are self directed and curious agents have conceptual reasoning and integrate planning and control.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have lots of people to thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        }
    }
}