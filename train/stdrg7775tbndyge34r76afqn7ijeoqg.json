{
    "id": "stdrg7775tbndyge34r76afqn7ijeoqg",
    "title": "An Efficient P300-based Brain-Computer Interface with Minimal Calibration Time",
    "info": {
        "author": [
            "Fabien Lotte, INRIA Bordeaux - Sud-Ouest"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/nips09_lotte_epb/",
    "segmentation": [
        [
            "Good afternoon everybody.",
            "So I will be also talking about BCI.",
            "So first I'd like to thank the previous speaker for their very nice talk because it was very interesting and it will also help me to introduce the my work.",
            "So I'm not.",
            "I'm working in Singapore.",
            "The Brain Computer interface laboratory.",
            "And today I will speak about P300 based beside that Jeremy Hill mentioned in his talk.",
            "So which is a busy I?",
            "I guess I don't have to introduce it anymore because you know what it is.",
            "So it's communication system based on activity."
        ],
        [
            "Again, here we are also considering leader.",
            "BCI based on e.g signals, electrons, photography.",
            "So more particularly here we are interesting in the peace friendship based BCI.",
            "So the the beehive based on the brain signals which occurs when the subject is pursuing a rare and relevant stimulus as she will mention.",
            "So just to recall three call you at.",
            "If the subjects perceived stimulus which is rare and relevant.",
            "Then we have a specific waveform specific positive potential which will occur in the brain signals that you can see in this picture.",
            "So basically it's a increase in activity which occurs roughly 200 milliseconds after the stimuli."
        ],
        [
            "And it's specifically interesting to design assistive technologies based on PCI.",
            "For instance, for spellers, so the basic application that you've seen here and also mentioned in the previous talk is the speller in which the subject can can watch a grid of letters and all the columns.",
            "The rows and columns of this letter, our randomly flashing.",
            "And so generally we ask the subject to count the number of appearance of the flash on the letter.",
            "I want to spell.",
            "And so each time a flash, so a row or column flash and contains the desired letter.",
            "It will trigger happy translating this user brain signals.",
            "So by detecting the speech translator we can infer which is the rule or column that contains desire, character and so we can infer what is the desired character.",
            "So that is 1 main application and another one is to to control wheelchair.",
            "So some some groups have design will share which the user can select some destination in a in a room or in an environment by focusing that is attention on a specific rooms for which stimulus are generated."
        ],
        [
            "So as mentioned in the previous post, there still a lot of limitation for currency systems.",
            "And one of which and we are fixing it in this talk is the long calibration time.",
            "So generally we need some example of the signals to calibrate the system properly.",
            "And the problem is that.",
            "It can be relatively longer and especially forgeable user.",
            "With that we have limited attention span, so for them we cannot ask them to be in front of a computer for too long.",
            "There, they cannot do that.",
            "So because it's inconvenient and not comfortable, so it's quite important to be able to reduce this calibration time and to design the system with only a few training examples.",
            "So that's the objective of our work.",
            "When we want to reduce the calibration time of vision based on the pitch."
        ],
        [
            "100 so far there have been only a few works which try to reduce this collaboration time.",
            "All of them are based on online adaptation.",
            "So in this design, they initially trend herbicide system on a few training examples using standard website design techniques and then the user semi supervised learning to adapt the Visa system online.",
            "When you that are coming.",
            "So this would work quite well, the limitation being that as they used standard training techniques, the initial performance of the system were quite low.",
            "So at the beginning the the system was making a lot of mistakes and only after a significant adaptation it was working properly.",
            "So here we propose a quite a simple approach, but which appears to be 2 to acquire in practice to learn a little bit shy from a few examples.",
            "So how does it work so fast?",
            "We do, quite simply classical purposes."
        ],
        [
            "So we are considering segments of Fiji that are located from 150 milliseconds to 500 milliseconds after the stimulus, so it's roughly the time window around the pitch 100.",
            "If there is any translator, because it should appear 300 milliseconds after the stimulus.",
            "We lowpass filter signals here below 25 heads because we know the pictures in the slow wave.",
            "So by doing some filtering we can remove all the unnecessary information and keep the relevant one.",
            "And finally we do downsampling, so here to 50th than equivalency to have a first dimensionality reduction because the more dimension we have, the more training examples we need.",
            "So we need to reduce that.",
            "That's quite a classical proposing that is used in Maine.",
            "Mostly all pictures of baby CI, and generally this this proposing is such purpose.",
            "STR vectorized and give as input to any classifier, and that's how it is designer.",
            "So here we propose to."
        ],
        [
            "So I've lost my equation, never mind.",
            "So here we propose to do another feature extraction steps which is based on Canonical correlation analysis, CCA.",
            "So what you see she ate some machine learning algorithms, which ends at looking for finding some direction WX&WI which maximize the correlation between the two variables X&Y.",
            "Once projection is the in this directions.",
            "So I just put in the objective function, but it's not that important.",
            "So basically it can be simply solved by using eigenvalues decomposition.",
            "So here we are, using it to to perform some feature extraction and uses here to find the direction WEG which is maximized.",
            "The correlation between the purpose, STD signals and the class labels.",
            "So in this in this way we will find some linear combination of the edge signals which best explains the class labels.",
            "So first things we're thinking because we give us.",
            "More, most likely more discriminative features, and the good point also is that we see are you can select the number of directions that you want.",
            "So basically you can perform dimensionality reduction and keep on your small numbers of directions.",
            "So it's interesting because."
        ],
        [
            "I think she wanted the dimension you need less data through to learn.",
            "For the problem that we have is that when we want to learn from very few examples, you have the problem with estimation and CC are required to estimate the covariance matrix of the data.",
            "The two and naturally declines metrics may be not properly estimated if you have two training examples.",
            "So to solve that we.",
            "We were set to regularization.",
            "So simply.",
            "Adding a given amount of the identity matrix to the covariance matrix and the nice thing that we can use the method proposed by law, involve to automatically regularize the covariance matrix, so no need to manually define the hyperparameter it can be done all automatically with the this math."
        ],
        [
            "Then for classification we used simple LDA which is quite simple and classic classifier for periods BC I.",
            "He also, as we are still in the context of huge running examples, we need some regularization.",
            "And here again we are using."
        ],
        [
            "The method of the involved to automatically recognize the covariance matrix of the LDA classifier.",
            "But does the methods question simple methods in fact?",
            "So to assess if it was useful, we classified some pictures and data from a picture under speller experiments.",
            "From 10 subjects.",
            "So here the subject to spell letters using the.",
            "The great visual spoiler that I described before.",
            "So the signals were coming out ATG Center for for each subject.",
            "And that our dividing into a training and testing set.",
            "So in each in each set we had 41 character.",
            "So which means that the user should try to spell 31 character and for each character.",
            "There are each row and each column of the matrix were flashing 10 times, so we turned it for one character.",
            "We had 120 e.g segments, twenty of which contain actually appear under the signal.",
            "So here we try to evaluate the approach in the single trial.",
            "That is, we for each segment we want to know whether it contains the pyramid or not.",
            "So to to assess the performance.",
            "We classically used Roc analysis and the area under the Roc curve because it's a detection problem.",
            "So we tried our approach using values variance.",
            "So weather season we improve their results.",
            "Well, regularization or not improve the results and you also compare sit here with the PCA which was also quite popularly used for designer pitch under the BCI.",
            "So for both these feature extraction."
        ],
        [
            "We expected 50 features from the initial 136 examples that were available after preprocessing.",
            "So these are the result.",
            "There are under curve for the standard PCI design, so just the standard is a classical.",
            "Let's say that is often used where the examples are directly fed to the LDA classifier after some preprocessing.",
            "So you can see that the performance of the system are dropping quite rapidly when the number of new characters are decreasing.",
            "This is the design which is PCA for feature extraction.",
            "So initially the performance are not as good as just an RBI design.",
            "Probably because when we have sufficiently sufficient number of training examples here will lose some information.",
            "But as the number of training data decreases, peer becomes better, probably because of the dimensionality reduction properties.",
            "No, if used regularization for the LDA.",
            "We can see the performance."
        ],
        [
            "Arbiter so when you use all the training data.",
            "It's slightly less efficient than the standard PCI design, but when the data are small, the rigorous daily is better.",
            "No."
        ],
        [
            "If we further add the CCF feature extraction, the performance are again better if we all the time, whatever the number of in character, the performance are better, and especially when there are very few training characters, only three characters, the performance Arbiter."
        ],
        [
            "And finally, if you regularize the CCF feature extraction, we get even better performance.",
            "And especially again when there are very few training examples.",
            "So what is interesting to notice here is that if you look at the performance.",
            "So approach using."
        ],
        [
            "Turning collectors, it's actually the same performance or the PCA designer using all different character so we can achieve the same performance by using roughly 10 times less training data.",
            "And comparing to the standard PCI design.",
            "I'll get down.",
            "Basically we need only 5 or 10 characters to reach the same performance, so we can decrease the number of training data by by 4 or something like this.",
            "So from roughly 20 minutes of calibration we can drop to 5 minutes of collaboration.",
            "With that we have our approach."
        ],
        [
            "So that's basically our approach.",
            "So we propose a new design build and rigorous Hessian LDA 4 official extraction and classification.",
            "So it's quite a simple approach, but the good thing is that actually it's simple, so simple to implement, and it's also very compassionate, computationally efficient, because it's all linear and automatic methods.",
            "So no, I parameters select.",
            "And the good thing is that you require so much testing that had to learn the system, so you can significantly reduce the calibration line of the system.",
            "So in future work we tried to use the such kind of methods for online evaluation.",
            "We've actually disabled users and also to do deeper machine learning by Johnny Optimized CCN LDA to find proper feature that are adapted."
        ],
        [
            "India classifier.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "So I will be also talking about BCI.",
                    "label": 0
                },
                {
                    "sent": "So first I'd like to thank the previous speaker for their very nice talk because it was very interesting and it will also help me to introduce the my work.",
                    "label": 0
                },
                {
                    "sent": "So I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm working in Singapore.",
                    "label": 0
                },
                {
                    "sent": "The Brain Computer interface laboratory.",
                    "label": 0
                },
                {
                    "sent": "And today I will speak about P300 based beside that Jeremy Hill mentioned in his talk.",
                    "label": 0
                },
                {
                    "sent": "So which is a busy I?",
                    "label": 0
                },
                {
                    "sent": "I guess I don't have to introduce it anymore because you know what it is.",
                    "label": 0
                },
                {
                    "sent": "So it's communication system based on activity.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, here we are also considering leader.",
                    "label": 0
                },
                {
                    "sent": "BCI based on e.g signals, electrons, photography.",
                    "label": 1
                },
                {
                    "sent": "So more particularly here we are interesting in the peace friendship based BCI.",
                    "label": 1
                },
                {
                    "sent": "So the the beehive based on the brain signals which occurs when the subject is pursuing a rare and relevant stimulus as she will mention.",
                    "label": 0
                },
                {
                    "sent": "So just to recall three call you at.",
                    "label": 0
                },
                {
                    "sent": "If the subjects perceived stimulus which is rare and relevant.",
                    "label": 0
                },
                {
                    "sent": "Then we have a specific waveform specific positive potential which will occur in the brain signals that you can see in this picture.",
                    "label": 0
                },
                {
                    "sent": "So basically it's a increase in activity which occurs roughly 200 milliseconds after the stimuli.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's specifically interesting to design assistive technologies based on PCI.",
                    "label": 0
                },
                {
                    "sent": "For instance, for spellers, so the basic application that you've seen here and also mentioned in the previous talk is the speller in which the subject can can watch a grid of letters and all the columns.",
                    "label": 0
                },
                {
                    "sent": "The rows and columns of this letter, our randomly flashing.",
                    "label": 0
                },
                {
                    "sent": "And so generally we ask the subject to count the number of appearance of the flash on the letter.",
                    "label": 0
                },
                {
                    "sent": "I want to spell.",
                    "label": 0
                },
                {
                    "sent": "And so each time a flash, so a row or column flash and contains the desired letter.",
                    "label": 0
                },
                {
                    "sent": "It will trigger happy translating this user brain signals.",
                    "label": 0
                },
                {
                    "sent": "So by detecting the speech translator we can infer which is the rule or column that contains desire, character and so we can infer what is the desired character.",
                    "label": 0
                },
                {
                    "sent": "So that is 1 main application and another one is to to control wheelchair.",
                    "label": 0
                },
                {
                    "sent": "So some some groups have design will share which the user can select some destination in a in a room or in an environment by focusing that is attention on a specific rooms for which stimulus are generated.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as mentioned in the previous post, there still a lot of limitation for currency systems.",
                    "label": 0
                },
                {
                    "sent": "And one of which and we are fixing it in this talk is the long calibration time.",
                    "label": 0
                },
                {
                    "sent": "So generally we need some example of the signals to calibrate the system properly.",
                    "label": 1
                },
                {
                    "sent": "And the problem is that.",
                    "label": 0
                },
                {
                    "sent": "It can be relatively longer and especially forgeable user.",
                    "label": 1
                },
                {
                    "sent": "With that we have limited attention span, so for them we cannot ask them to be in front of a computer for too long.",
                    "label": 0
                },
                {
                    "sent": "There, they cannot do that.",
                    "label": 1
                },
                {
                    "sent": "So because it's inconvenient and not comfortable, so it's quite important to be able to reduce this calibration time and to design the system with only a few training examples.",
                    "label": 0
                },
                {
                    "sent": "So that's the objective of our work.",
                    "label": 0
                },
                {
                    "sent": "When we want to reduce the calibration time of vision based on the pitch.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "100 so far there have been only a few works which try to reduce this collaboration time.",
                    "label": 0
                },
                {
                    "sent": "All of them are based on online adaptation.",
                    "label": 1
                },
                {
                    "sent": "So in this design, they initially trend herbicide system on a few training examples using standard website design techniques and then the user semi supervised learning to adapt the Visa system online.",
                    "label": 0
                },
                {
                    "sent": "When you that are coming.",
                    "label": 0
                },
                {
                    "sent": "So this would work quite well, the limitation being that as they used standard training techniques, the initial performance of the system were quite low.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning the the system was making a lot of mistakes and only after a significant adaptation it was working properly.",
                    "label": 0
                },
                {
                    "sent": "So here we propose a quite a simple approach, but which appears to be 2 to acquire in practice to learn a little bit shy from a few examples.",
                    "label": 1
                },
                {
                    "sent": "So how does it work so fast?",
                    "label": 0
                },
                {
                    "sent": "We do, quite simply classical purposes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are considering segments of Fiji that are located from 150 milliseconds to 500 milliseconds after the stimulus, so it's roughly the time window around the pitch 100.",
                    "label": 1
                },
                {
                    "sent": "If there is any translator, because it should appear 300 milliseconds after the stimulus.",
                    "label": 1
                },
                {
                    "sent": "We lowpass filter signals here below 25 heads because we know the pictures in the slow wave.",
                    "label": 0
                },
                {
                    "sent": "So by doing some filtering we can remove all the unnecessary information and keep the relevant one.",
                    "label": 1
                },
                {
                    "sent": "And finally we do downsampling, so here to 50th than equivalency to have a first dimensionality reduction because the more dimension we have, the more training examples we need.",
                    "label": 0
                },
                {
                    "sent": "So we need to reduce that.",
                    "label": 0
                },
                {
                    "sent": "That's quite a classical proposing that is used in Maine.",
                    "label": 0
                },
                {
                    "sent": "Mostly all pictures of baby CI, and generally this this proposing is such purpose.",
                    "label": 0
                },
                {
                    "sent": "STR vectorized and give as input to any classifier, and that's how it is designer.",
                    "label": 0
                },
                {
                    "sent": "So here we propose to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've lost my equation, never mind.",
                    "label": 0
                },
                {
                    "sent": "So here we propose to do another feature extraction steps which is based on Canonical correlation analysis, CCA.",
                    "label": 1
                },
                {
                    "sent": "So what you see she ate some machine learning algorithms, which ends at looking for finding some direction WX&WI which maximize the correlation between the two variables X&Y.",
                    "label": 1
                },
                {
                    "sent": "Once projection is the in this directions.",
                    "label": 0
                },
                {
                    "sent": "So I just put in the objective function, but it's not that important.",
                    "label": 0
                },
                {
                    "sent": "So basically it can be simply solved by using eigenvalues decomposition.",
                    "label": 0
                },
                {
                    "sent": "So here we are, using it to to perform some feature extraction and uses here to find the direction WEG which is maximized.",
                    "label": 0
                },
                {
                    "sent": "The correlation between the purpose, STD signals and the class labels.",
                    "label": 1
                },
                {
                    "sent": "So in this in this way we will find some linear combination of the edge signals which best explains the class labels.",
                    "label": 0
                },
                {
                    "sent": "So first things we're thinking because we give us.",
                    "label": 0
                },
                {
                    "sent": "More, most likely more discriminative features, and the good point also is that we see are you can select the number of directions that you want.",
                    "label": 0
                },
                {
                    "sent": "So basically you can perform dimensionality reduction and keep on your small numbers of directions.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting because.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think she wanted the dimension you need less data through to learn.",
                    "label": 0
                },
                {
                    "sent": "For the problem that we have is that when we want to learn from very few examples, you have the problem with estimation and CC are required to estimate the covariance matrix of the data.",
                    "label": 1
                },
                {
                    "sent": "The two and naturally declines metrics may be not properly estimated if you have two training examples.",
                    "label": 0
                },
                {
                    "sent": "So to solve that we.",
                    "label": 0
                },
                {
                    "sent": "We were set to regularization.",
                    "label": 0
                },
                {
                    "sent": "So simply.",
                    "label": 0
                },
                {
                    "sent": "Adding a given amount of the identity matrix to the covariance matrix and the nice thing that we can use the method proposed by law, involve to automatically regularize the covariance matrix, so no need to manually define the hyperparameter it can be done all automatically with the this math.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then for classification we used simple LDA which is quite simple and classic classifier for periods BC I.",
                    "label": 0
                },
                {
                    "sent": "He also, as we are still in the context of huge running examples, we need some regularization.",
                    "label": 0
                },
                {
                    "sent": "And here again we are using.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The method of the involved to automatically recognize the covariance matrix of the LDA classifier.",
                    "label": 0
                },
                {
                    "sent": "But does the methods question simple methods in fact?",
                    "label": 0
                },
                {
                    "sent": "So to assess if it was useful, we classified some pictures and data from a picture under speller experiments.",
                    "label": 0
                },
                {
                    "sent": "From 10 subjects.",
                    "label": 0
                },
                {
                    "sent": "So here the subject to spell letters using the.",
                    "label": 0
                },
                {
                    "sent": "The great visual spoiler that I described before.",
                    "label": 0
                },
                {
                    "sent": "So the signals were coming out ATG Center for for each subject.",
                    "label": 0
                },
                {
                    "sent": "And that our dividing into a training and testing set.",
                    "label": 0
                },
                {
                    "sent": "So in each in each set we had 41 character.",
                    "label": 0
                },
                {
                    "sent": "So which means that the user should try to spell 31 character and for each character.",
                    "label": 0
                },
                {
                    "sent": "There are each row and each column of the matrix were flashing 10 times, so we turned it for one character.",
                    "label": 0
                },
                {
                    "sent": "We had 120 e.g segments, twenty of which contain actually appear under the signal.",
                    "label": 0
                },
                {
                    "sent": "So here we try to evaluate the approach in the single trial.",
                    "label": 0
                },
                {
                    "sent": "That is, we for each segment we want to know whether it contains the pyramid or not.",
                    "label": 0
                },
                {
                    "sent": "So to to assess the performance.",
                    "label": 0
                },
                {
                    "sent": "We classically used Roc analysis and the area under the Roc curve because it's a detection problem.",
                    "label": 1
                },
                {
                    "sent": "So we tried our approach using values variance.",
                    "label": 0
                },
                {
                    "sent": "So weather season we improve their results.",
                    "label": 0
                },
                {
                    "sent": "Well, regularization or not improve the results and you also compare sit here with the PCA which was also quite popularly used for designer pitch under the BCI.",
                    "label": 0
                },
                {
                    "sent": "So for both these feature extraction.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We expected 50 features from the initial 136 examples that were available after preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So these are the result.",
                    "label": 0
                },
                {
                    "sent": "There are under curve for the standard PCI design, so just the standard is a classical.",
                    "label": 0
                },
                {
                    "sent": "Let's say that is often used where the examples are directly fed to the LDA classifier after some preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the performance of the system are dropping quite rapidly when the number of new characters are decreasing.",
                    "label": 0
                },
                {
                    "sent": "This is the design which is PCA for feature extraction.",
                    "label": 0
                },
                {
                    "sent": "So initially the performance are not as good as just an RBI design.",
                    "label": 0
                },
                {
                    "sent": "Probably because when we have sufficiently sufficient number of training examples here will lose some information.",
                    "label": 0
                },
                {
                    "sent": "But as the number of training data decreases, peer becomes better, probably because of the dimensionality reduction properties.",
                    "label": 0
                },
                {
                    "sent": "No, if used regularization for the LDA.",
                    "label": 0
                },
                {
                    "sent": "We can see the performance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arbiter so when you use all the training data.",
                    "label": 0
                },
                {
                    "sent": "It's slightly less efficient than the standard PCI design, but when the data are small, the rigorous daily is better.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we further add the CCF feature extraction, the performance are again better if we all the time, whatever the number of in character, the performance are better, and especially when there are very few training characters, only three characters, the performance Arbiter.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, if you regularize the CCF feature extraction, we get even better performance.",
                    "label": 0
                },
                {
                    "sent": "And especially again when there are very few training examples.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting to notice here is that if you look at the performance.",
                    "label": 0
                },
                {
                    "sent": "So approach using.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turning collectors, it's actually the same performance or the PCA designer using all different character so we can achieve the same performance by using roughly 10 times less training data.",
                    "label": 0
                },
                {
                    "sent": "And comparing to the standard PCI design.",
                    "label": 0
                },
                {
                    "sent": "I'll get down.",
                    "label": 0
                },
                {
                    "sent": "Basically we need only 5 or 10 characters to reach the same performance, so we can decrease the number of training data by by 4 or something like this.",
                    "label": 0
                },
                {
                    "sent": "So from roughly 20 minutes of calibration we can drop to 5 minutes of collaboration.",
                    "label": 0
                },
                {
                    "sent": "With that we have our approach.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's basically our approach.",
                    "label": 0
                },
                {
                    "sent": "So we propose a new design build and rigorous Hessian LDA 4 official extraction and classification.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a simple approach, but the good thing is that actually it's simple, so simple to implement, and it's also very compassionate, computationally efficient, because it's all linear and automatic methods.",
                    "label": 1
                },
                {
                    "sent": "So no, I parameters select.",
                    "label": 0
                },
                {
                    "sent": "And the good thing is that you require so much testing that had to learn the system, so you can significantly reduce the calibration line of the system.",
                    "label": 1
                },
                {
                    "sent": "So in future work we tried to use the such kind of methods for online evaluation.",
                    "label": 0
                },
                {
                    "sent": "We've actually disabled users and also to do deeper machine learning by Johnny Optimized CCN LDA to find proper feature that are adapted.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "India classifier.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}