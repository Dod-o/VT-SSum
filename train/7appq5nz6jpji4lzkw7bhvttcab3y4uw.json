{
    "id": "7appq5nz6jpji4lzkw7bhvttcab3y4uw",
    "title": "Language Detection and Tracking in Multilingual Documents Using Weak Estimators",
    "info": {
        "author": [
            "Aleksander M. Stensby, University of Agder"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_stensby_ldtm/",
    "segmentation": [
        [
            "My name is Alexander Stensby.",
            "I'm working in a company called into Grasco.",
            "I'm also associated with the University of Auditor in Norway.",
            "My Co.",
            "Authors of this paper is Professor John Woman and Christopher Grandma from University of there and Carleton University.",
            "So today I'm going to talk about language classification in multilingual documents and more particular online conversation.",
            "Using a family of weak estimators, which was introduced by Professor Woman."
        ],
        [
            "So the outline for this talk, I'll give you a little bit of introduction.",
            "Also, looking at what's been done previously in language classification will have deeper look at the SLWE or the Stochastic Learning Week estimator.",
            "Then we'll see how this is applied to language classification and will look at some results and the summary."
        ],
        [
            "Everything.",
            "So the theory of estimation have obviously been studied for hundreds of years, and it's an essential part in the learning or training phase of statistical pattern recognition systems.",
            "As you all know, the traditional methods like maximum likelihood and Bashan, family of estimators.",
            "But although these show good computational and statistical properties, they all work under the assumption that the paramaters that we're estimating do not change, so that the environment is stationary."
        ],
        [
            "In this case, we will consider the case where the program parameters we are going to estimate actually do change with time, so the class conditional distributions are nonstationary and one example of such an environment is online conversations where you have multiple authors expressing expressing multiple opinions.",
            "Even writing the text in different languages.",
            "So for this kind of environment we need to have estimation and strategies that are able to deal with the changes to the class conditional distributions, and that's what we're going to look at."
        ],
        [
            "Today so traditionally to overcome this problem with the traditional estimators, we normally use a sliding window.",
            "So for instance you can apply a sliding window to the maximum likelihood estimates and.",
            "It do, it does work, but the problem here is that the windows which is crucial to the problem.",
            "So if you use a two small window, the estimates tend to be poor an if the window with is too large then any estimates prior to the change in the environment will affect the new estimates, which is obviously not what we want.",
            "In this case.",
            "There is also the computational aspect where you actually have to maintain the entire width of the observations to update.",
            "Throughout the win."
        ],
        [
            "So.",
            "So.",
            "The fascinating problem that we have investigated in our paper is that of language detection in real electronic applications, such as online conversations.",
            "So the company that I work for Integral scope does something called analysis of word of mouth, which is consumers exchanging opinions online.",
            "Um, the fascinating problem with these type of conversations and the difference between classifying Journal articles or newspaper articles is that these pieces of text that we analyze often contain very short segments.",
            "Their chatty in the sense that the people that are discussing online often use their own Internet slang and no formalism.",
            "Well, to some extent, but they are prone to spelling and grammatical errors.",
            "In addition to that, the nonstationarity of the problem occurs because there are multiple authors which express multiple opinions.",
            "An often discuss several different topics even within the same conversation.",
            "So we we need to find training and classification strategies that are able to deal with the non stationarity of the problem."
        ],
        [
            "Just to give you an example, I don't know if you can read this, but it doesn't really matter that much.",
            "The whole point is that this is a conversation from an online discussion board, a German discussion board where you see several authors discussing in the same conversation and."
        ],
        [
            "They are actually expressing themselves both in German and in the middle of a comment is actually saying something in English, so that is the nature of the problem."
        ],
        [
            "So what we want to achieve is to treat each of these conversations as one contiguous document.",
            "And then segment this discussion and classify each segment.",
            "Now I've written topics, sentiment or language because this the basis of this problem does not necessarily have to be language classification.",
            "But that is what we have investigated.",
            "In this case.",
            "It can also be applied to other type of classes of text classification and the goal is obviously to find an efficient solution which can deal with this non stationary environment."
        ],
        [
            "So, just briefly, on the state of the art language classification in itself has been widely studied.",
            "Anne received quite a lot of attention in the literature, but most of these approaches focus on monolingual documents, and they also focus on documents that are formalized and they've gone through a pre processing where they actually avoid most of the spelling and grammatical errors.",
            "There are some approaches to multi lingual classification.",
            "But I'm not going to go into depth of that.",
            "You can read that in the paper.",
            "The the biggest difference between the approaches that we find in the literature is the choice of discrimina.",
            "Tori features an most commonly the use of a dictionary trained set of words for each language is often used.",
            "There are also approaches where you use particular characters or N grams, which is actually what we will use.",
            "Our discrimina Tori features.",
            "There are also more complex approaches where they use the natural.",
            "Morphological features of a language, but again that requires trained expert and we want to make a simple approach that do not require any previous knowledge about the languages."
        ],
        [
            "So I want to introduce you to the Stochastic Learning Week estimator, which was introduced by Umn Andrew Ada.",
            "It is based on the principles of stochastic learning in the sense that.",
            "It uses a multiplicative updating scheme very similar to the linear action probability schemes which you find in reinforcement learning.",
            "The convergence of the estimator is weak in the sense that it only converges in the limit.",
            "Anne.",
            "And it is based on a user defined learning coefficient Lambda, which we will see later.",
            "Is is very important in this case, so we will look into that.",
            "The concept is that the SLW maintains a running estimate and that at every timestep or every sample it is updated based on the value of the current sample."
        ],
        [
            "So.",
            "To explain, I don't have a point there, some just have to bear with me.",
            "So let's say we have a multinomial random variable X and that X takes on the values from the set 1 to R. S is the probability distributions that governs the value of X.",
            "So X takes on the value of I with probability XI and so on.",
            "Let's say that XN is the concrete realization of X at time N, and then our goal here is to estimate S for all for all the different probabilities."
        ],
        [
            "So.",
            "How do we do that with ESL?",
            "WE we have a running estimate P of N where Pi event is the estimate of SI at time N. And then we use this fairly simple updating rules which you might recognize.",
            "There are very similar to a penalty and reward in reinforcement learning.",
            "And for other values of Pi the rules are similar.",
            "So what is interesting with these rules is that they are governed by Markov chain or a stochastic matrix.",
            "And what woman and radar have proved in their paper is that the rate of convergence of P is fully determined by Lambda, so."
        ],
        [
            "It's just look at these properties.",
            "The proof of these you can find in woman's paper.",
            "You might ask yourself, how does this work?",
            "Since we are dealing with a non stationary environment and we're actually saying that the convergence is weak.",
            "Well empirically they have shown that determined only by Lambda, the variance and the speed of the convergence can be manipulated and it can also be controlled in a way that it actually adapts to the changes quickly.",
            "So we have tried different values of Lambda in our experiments.",
            "And the results are are very good as you will see soon, but in general a small value of Lambda leads to false convergence but a larger variance and vice versa.",
            "So a large value of Lambda leads to a slower convergence but lower variance."
        ],
        [
            "So in our solution.",
            "In our solution we have chosen to use mixed order N grams as the discrimina Tori features so.",
            "For each language we train the ngram characteristics of the language itself based on a training set of monolingual documents.",
            "And mixed order means that we combine unigrams, bigrams and trigrams.",
            "We've also tried N grams with N = 4, But the accuracy didn't change much.",
            "So we use a combination of Uni, BI and trigrams in addition to that, we also take each word and we pad with whitespace is the reason why we do that is to preserve the knowledge of beginning and ending of words, which is often frequent to certain languages.",
            "So in the testing in the testing phase, we utilized the SLW will go into more detail on how that is done.",
            "The advantage here is obviously that we can utilize the properties of the SLW to overcome the nonstationarity of the problem, and we also make use of sips law where it states that there in any language there are certain number of words that dominates all other words in.",
            "Any given language.",
            "So what we have done to reduce the feature space of our classifier is actually 2.",
            "Train the ngram profiles for each language and then use a cut off threshold.",
            "So that means we discards all the words or N grams that have a lower frequency and we just maintain the top, let's say 400 for each language.",
            "And so.",
            "Like I said, the training of these language profiles is done on pre classified monolingual documents."
        ],
        [
            "So to show you an example of one of the language profiles, these are selected in grams from the English language profile and as you can see, we've padded.",
            "Words with white spaces would just represent them as an_here, and the reason for that is you'll see that in the English language there are.",
            "More words that start with a T and there are also several words that end with a T and the same with the trigram TH.",
            "So for the total profile of the English language we trained, we trained it on approximately 600 monolingual documents.",
            "We had a total of 41,000 unique engrams.",
            "Now, maintaining only the top 400, discarding the rest and training our classifier for eight different languages.",
            "We would have a worst case scenario of 400 by 8, but as the alphabet of the most languages are very similar, you'll also have similar engrams, although the order will be different.",
            "So that means that the actual feature space is severely reduced."
        ],
        [
            "So for the classification we utilized the running estimate of the SLW, which is P hat.",
            "Of all the unique engrams in all the trained language profiles, so that means we take we maintain all the unique language, the engrams in all the language profiles, and that is then initialized as the running estimate and then as we do the classification, we measure the distance between the estimate and each of the trained language profiles which are in essence probability distributions and the measure.",
            "We did use words the Euclidean distance, but.",
            "Obviously we can use other similarity measures as well.",
            "The benefit of doing this is that we can classify on the engram level.",
            "We can collect engrams and classify whenever we want to.",
            "We can collect words and collect and classify individual words.",
            "Or if we have sentence boundary algorithm, we can classify sentences and in essence you can obviously classify the entire document.",
            "If you want to do that.",
            "AM.",
            "So.",
            "Keeping that in mind."
        ],
        [
            "I'm not going to go into detail of the algorithms, but you can read this in the in the paper."
        ],
        [
            "Let's just have a look at how we did the testing so to be able to do large scale testing an not having to manually label each and every document, we took our set of pre labeled monolingual documents and we generated a test set of multilingual documents, meaning that we actually extracted random sentences from the model monolingual documents and combine them.",
            "Into segments, so obviously it's easy for a human to see that these are three different segments to the classifier.",
            "It's treated as one document, and the classifier's aim is to identify individual words or individual sentences which we tested with that as well by using a very simple sentence algorithm, we just appended.",
            "At the end of 10 words.",
            "So in this case you will see that.",
            "You have a German segment of French segment and an English segment.",
            "I've highlighted some of the words here because.",
            "You'll see that the sentences are rather short, which then stimulates the environment we want to work with online conversations.",
            "Also see that there are certain slang or short and chatty sentences, and there are also some industry specific words that might not even be of the current language we are identifying."
        ],
        [
            "So for our experiments.",
            "We train the classifier on eight different languages.",
            "Now this was fascinating.",
            "The choice of languages becausw.",
            "As you can see we have Norwegian and Swedish, which are very similar languages.",
            "So for many non Scandinavian IAM from Norway by the way, they are literally impossible to see the difference of.",
            "Spanish, obviously Italian.",
            "There's a resemblance there.",
            "Dutch German, so we have a.",
            "We've made our classification problem quite hard in this case.",
            "Like I said, for each of the language profiles we had approximately 600 sample documents, and each of those documents where had an average length of 15 to 20 KB.",
            "We then did our experiments on three different language sets.",
            "So instead of running all the 8 languages, we actually divided it up and said, let's try with three languages and then let's see how the classifier accuracy changes the dimensionality changes and how the scalability of the classifier itself actually adapts to adding more languages.",
            "So we have three different sets with three, five and eight languages."
        ],
        [
            "So I'm not going to spend too much time on this.",
            "You can read about this in the paper as well.",
            "These are the languages in the different sets, and we used different test sets with different sentence lengths and also corpus sizes.",
            "Then to actually experiment with the learning parameter, which is Lambda and the cutoff, which was how many engrams we maintain per language profile, we had two different variations."
        ],
        [
            "So just looking at the results, this is the plot of the Euclidean distance between the running estimate of the classifier and.",
            "Each of the three language profiles that we initialize the SLW with, so we had English, German and French and Now the interesting thing here is that this document was actually not multilingual.",
            "It's a monolingual document written in French and it contains 300 words, and as you can see.",
            "It rapidly.",
            "Identifies that the document is written in French and throughout the entire document.",
            "Although high variance, the distance to the other language profiles are significantly higher.",
            "So there's no doubt anywhere in the document that this is a French document."
        ],
        [
            "Now, looking at accuracies that we achieved when dealing with only three languages, we had accuracies for the English language as high as 99.6%.",
            "The German we achieved 98.7.",
            "And 99.2 for the French one, and the difference between set 1, two and three is the segment length.",
            "So the first one has short sentences of 10 words.",
            "The next one is 15 and the last one is 20.",
            "So as you can see the accuracy."
        ],
        [
            "Obviously increases with the segment lengths."
        ],
        [
            "So just looking at one particular test case from our results.",
            "What's interesting here is that.",
            "Most of the mistakes that the French and the German classifier makes are classified as English, and by investigating the results we actually see that these languages do have words that are English, so that will obviously confuse the classifier and then the question or philosophical question is, should it be classified as English, or should it be classified as German?",
            "But still, the average classifier accuracy of 98.96% is."
        ],
        [
            "Very good now.",
            "The details of the last Test set where we used all 8 languages you can find in the paper.",
            "What is interesting is that the classifier accuracy does not significantly get dragged down by the fact that you're dealing with more languages.",
            "What we do see is that in all of our different test sets, the Norwegian.",
            "The classification showed the poorest results because we had Swedish in the mix as well, and the majority of these that were misclassified were actually classified as Swedish."
        ],
        [
            "So to summarize, we have developed a classifier that is able to classify multilingual documents with high accuracy.",
            "It performs extremely well on moderate side segments an we've achieved good results with shorter segments as short as 10 words per sentence for the first language set, which only contain three different languages, we achieved 99.6%, which is the highest accuracy that.",
            "I have seen in any of the monolingual multilingual approaches that we have investigated, we've also shown that it scales well with in regards to the number of supported languages, and also the size of the corpus, and we are able to discriminate similar languages, although there are some improvements to be made.",
            "It also deals with so-called chatty or slangy and short and error prone sentences, which we find in online conversations.",
            "Anne."
        ],
        [
            "Yeah, there are quite a few possible directions for further work.",
            "Be happy to speak with anyone of you if."
        ],
        [
            "Find this interesting.",
            "Yeah."
        ],
        [
            "Thank you.",
            "Thank you very much for this very nice presentation are there."
        ],
        [
            "Questions or comments?",
            "You said that your estimator is based on multiplicative updating mechanisms, and can you say some words about difference in practice with estimators based on attitudes?",
            "Yeah, so in this case why it's a benefit of having a multiplicative updating rule in this environment.",
            "This obviously that you want the classifier to rapidly adapt to changes, so if you were using a multiplicative updating rule, the convergence would be too slow to deal with this particular problem so.",
            "If you have rapid changes, so in our in our case we didn't have that rapid changes, we forced, well, we force the training samples to be 3 segments of one language and then three segments of another language.",
            "So you had quite a long window with to actually be able to identify it, but with the multiplicative updating rule, obviously penalty and the reward would actually significantly change the probabilities of the distribution.",
            "Which then again allows you to move quickly to the next, so we've seen examples when we've done our testing that you can actually have one word, and it will actually identify the change, even in that one word.",
            "And that's obviously also because of the benefit of using N grams and not whole words as the discrimina Tori features, because you can have well all the information bearing contents of each individual engram and then do the classification on the word.",
            "Did you compare the two approaches just for the curious on synthetic data, we have compared the SLWON Bernoulli trials, an also on multinomial parameters, where we compared it to the window based maximum likelihood method and in all of the tests we did and we have a lot of tests that we can, I can show you later the slowly converges faster and it's able to scale better in terms of the dimensionality, which is obviously very important in this application domain.",
            "Or you have all the engrams that you need to maintain in your in your probability distribution.",
            "So yeah, we have not compared this to other approaches to language classification other than.",
            "What we've seen in the literature.",
            "Another question, yes.",
            "Training data from the destination translator.",
            "Clearly monolingual text, yes.",
            "That they are the same in the sense that the monolingual documents we used for training is gathered from online conversations.",
            "So they are actually in itself.",
            "They are chatty and slangy and short.",
            "And then So what we did was we gathered 1200 monolingual documents and we used 600 of those to do the training and then the other 600 to generate the test set.",
            "So for for these results that would you see how we did an ensemble average as well, so they.",
            "On this on those documents.",
            "Very good, I don't see any further questions or young one people here.",
            "How many languages do you actually need to get in touch inputs classifier?",
            "So yeah.",
            "OK, selecting the engrams by frequency needed using the most frequent engrams.",
            "It may be that the most frequent angles are not the most selective ones, as they're coming to our languages.",
            "That's true, and that's also the benefit of combining unigrams, bigrams and trigrams becausw becausw you maintain.",
            "In our case, we had 400 and 500 and we didn't see a significant loss of accuracy by going down to 400, which actually reduces the feature space quite a lot so.",
            "If you were just doing unigrams, obviously you would not get the same accuracy at all.",
            "It would be.",
            "I don't think you would get good results at all with only using unigrams.",
            "I don't know if I answered your question, but I hope so.",
            "OK, so let's take."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Alexander Stensby.",
                    "label": 0
                },
                {
                    "sent": "I'm working in a company called into Grasco.",
                    "label": 0
                },
                {
                    "sent": "I'm also associated with the University of Auditor in Norway.",
                    "label": 1
                },
                {
                    "sent": "My Co.",
                    "label": 0
                },
                {
                    "sent": "Authors of this paper is Professor John Woman and Christopher Grandma from University of there and Carleton University.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk about language classification in multilingual documents and more particular online conversation.",
                    "label": 1
                },
                {
                    "sent": "Using a family of weak estimators, which was introduced by Professor Woman.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline for this talk, I'll give you a little bit of introduction.",
                    "label": 0
                },
                {
                    "sent": "Also, looking at what's been done previously in language classification will have deeper look at the SLWE or the Stochastic Learning Week estimator.",
                    "label": 1
                },
                {
                    "sent": "Then we'll see how this is applied to language classification and will look at some results and the summary.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything.",
                    "label": 0
                },
                {
                    "sent": "So the theory of estimation have obviously been studied for hundreds of years, and it's an essential part in the learning or training phase of statistical pattern recognition systems.",
                    "label": 1
                },
                {
                    "sent": "As you all know, the traditional methods like maximum likelihood and Bashan, family of estimators.",
                    "label": 1
                },
                {
                    "sent": "But although these show good computational and statistical properties, they all work under the assumption that the paramaters that we're estimating do not change, so that the environment is stationary.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we will consider the case where the program parameters we are going to estimate actually do change with time, so the class conditional distributions are nonstationary and one example of such an environment is online conversations where you have multiple authors expressing expressing multiple opinions.",
                    "label": 0
                },
                {
                    "sent": "Even writing the text in different languages.",
                    "label": 0
                },
                {
                    "sent": "So for this kind of environment we need to have estimation and strategies that are able to deal with the changes to the class conditional distributions, and that's what we're going to look at.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today so traditionally to overcome this problem with the traditional estimators, we normally use a sliding window.",
                    "label": 0
                },
                {
                    "sent": "So for instance you can apply a sliding window to the maximum likelihood estimates and.",
                    "label": 1
                },
                {
                    "sent": "It do, it does work, but the problem here is that the windows which is crucial to the problem.",
                    "label": 0
                },
                {
                    "sent": "So if you use a two small window, the estimates tend to be poor an if the window with is too large then any estimates prior to the change in the environment will affect the new estimates, which is obviously not what we want.",
                    "label": 1
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "There is also the computational aspect where you actually have to maintain the entire width of the observations to update.",
                    "label": 0
                },
                {
                    "sent": "Throughout the win.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The fascinating problem that we have investigated in our paper is that of language detection in real electronic applications, such as online conversations.",
                    "label": 1
                },
                {
                    "sent": "So the company that I work for Integral scope does something called analysis of word of mouth, which is consumers exchanging opinions online.",
                    "label": 0
                },
                {
                    "sent": "Um, the fascinating problem with these type of conversations and the difference between classifying Journal articles or newspaper articles is that these pieces of text that we analyze often contain very short segments.",
                    "label": 0
                },
                {
                    "sent": "Their chatty in the sense that the people that are discussing online often use their own Internet slang and no formalism.",
                    "label": 1
                },
                {
                    "sent": "Well, to some extent, but they are prone to spelling and grammatical errors.",
                    "label": 1
                },
                {
                    "sent": "In addition to that, the nonstationarity of the problem occurs because there are multiple authors which express multiple opinions.",
                    "label": 1
                },
                {
                    "sent": "An often discuss several different topics even within the same conversation.",
                    "label": 0
                },
                {
                    "sent": "So we we need to find training and classification strategies that are able to deal with the non stationarity of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you an example, I don't know if you can read this, but it doesn't really matter that much.",
                    "label": 0
                },
                {
                    "sent": "The whole point is that this is a conversation from an online discussion board, a German discussion board where you see several authors discussing in the same conversation and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are actually expressing themselves both in German and in the middle of a comment is actually saying something in English, so that is the nature of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to achieve is to treat each of these conversations as one contiguous document.",
                    "label": 1
                },
                {
                    "sent": "And then segment this discussion and classify each segment.",
                    "label": 0
                },
                {
                    "sent": "Now I've written topics, sentiment or language because this the basis of this problem does not necessarily have to be language classification.",
                    "label": 1
                },
                {
                    "sent": "But that is what we have investigated.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "It can also be applied to other type of classes of text classification and the goal is obviously to find an efficient solution which can deal with this non stationary environment.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just briefly, on the state of the art language classification in itself has been widely studied.",
                    "label": 1
                },
                {
                    "sent": "Anne received quite a lot of attention in the literature, but most of these approaches focus on monolingual documents, and they also focus on documents that are formalized and they've gone through a pre processing where they actually avoid most of the spelling and grammatical errors.",
                    "label": 0
                },
                {
                    "sent": "There are some approaches to multi lingual classification.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to go into depth of that.",
                    "label": 0
                },
                {
                    "sent": "You can read that in the paper.",
                    "label": 0
                },
                {
                    "sent": "The the biggest difference between the approaches that we find in the literature is the choice of discrimina.",
                    "label": 0
                },
                {
                    "sent": "Tori features an most commonly the use of a dictionary trained set of words for each language is often used.",
                    "label": 1
                },
                {
                    "sent": "There are also approaches where you use particular characters or N grams, which is actually what we will use.",
                    "label": 0
                },
                {
                    "sent": "Our discrimina Tori features.",
                    "label": 0
                },
                {
                    "sent": "There are also more complex approaches where they use the natural.",
                    "label": 0
                },
                {
                    "sent": "Morphological features of a language, but again that requires trained expert and we want to make a simple approach that do not require any previous knowledge about the languages.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to introduce you to the Stochastic Learning Week estimator, which was introduced by Umn Andrew Ada.",
                    "label": 1
                },
                {
                    "sent": "It is based on the principles of stochastic learning in the sense that.",
                    "label": 1
                },
                {
                    "sent": "It uses a multiplicative updating scheme very similar to the linear action probability schemes which you find in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "The convergence of the estimator is weak in the sense that it only converges in the limit.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And it is based on a user defined learning coefficient Lambda, which we will see later.",
                    "label": 0
                },
                {
                    "sent": "Is is very important in this case, so we will look into that.",
                    "label": 0
                },
                {
                    "sent": "The concept is that the SLW maintains a running estimate and that at every timestep or every sample it is updated based on the value of the current sample.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To explain, I don't have a point there, some just have to bear with me.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have a multinomial random variable X and that X takes on the values from the set 1 to R. S is the probability distributions that governs the value of X.",
                    "label": 1
                },
                {
                    "sent": "So X takes on the value of I with probability XI and so on.",
                    "label": 1
                },
                {
                    "sent": "Let's say that XN is the concrete realization of X at time N, and then our goal here is to estimate S for all for all the different probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do we do that with ESL?",
                    "label": 0
                },
                {
                    "sent": "WE we have a running estimate P of N where Pi event is the estimate of SI at time N. And then we use this fairly simple updating rules which you might recognize.",
                    "label": 1
                },
                {
                    "sent": "There are very similar to a penalty and reward in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And for other values of Pi the rules are similar.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting with these rules is that they are governed by Markov chain or a stochastic matrix.",
                    "label": 0
                },
                {
                    "sent": "And what woman and radar have proved in their paper is that the rate of convergence of P is fully determined by Lambda, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's just look at these properties.",
                    "label": 0
                },
                {
                    "sent": "The proof of these you can find in woman's paper.",
                    "label": 0
                },
                {
                    "sent": "You might ask yourself, how does this work?",
                    "label": 0
                },
                {
                    "sent": "Since we are dealing with a non stationary environment and we're actually saying that the convergence is weak.",
                    "label": 0
                },
                {
                    "sent": "Well empirically they have shown that determined only by Lambda, the variance and the speed of the convergence can be manipulated and it can also be controlled in a way that it actually adapts to the changes quickly.",
                    "label": 1
                },
                {
                    "sent": "So we have tried different values of Lambda in our experiments.",
                    "label": 0
                },
                {
                    "sent": "And the results are are very good as you will see soon, but in general a small value of Lambda leads to false convergence but a larger variance and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So a large value of Lambda leads to a slower convergence but lower variance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our solution.",
                    "label": 0
                },
                {
                    "sent": "In our solution we have chosen to use mixed order N grams as the discrimina Tori features so.",
                    "label": 0
                },
                {
                    "sent": "For each language we train the ngram characteristics of the language itself based on a training set of monolingual documents.",
                    "label": 0
                },
                {
                    "sent": "And mixed order means that we combine unigrams, bigrams and trigrams.",
                    "label": 0
                },
                {
                    "sent": "We've also tried N grams with N = 4, But the accuracy didn't change much.",
                    "label": 0
                },
                {
                    "sent": "So we use a combination of Uni, BI and trigrams in addition to that, we also take each word and we pad with whitespace is the reason why we do that is to preserve the knowledge of beginning and ending of words, which is often frequent to certain languages.",
                    "label": 0
                },
                {
                    "sent": "So in the testing in the testing phase, we utilized the SLW will go into more detail on how that is done.",
                    "label": 0
                },
                {
                    "sent": "The advantage here is obviously that we can utilize the properties of the SLW to overcome the nonstationarity of the problem, and we also make use of sips law where it states that there in any language there are certain number of words that dominates all other words in.",
                    "label": 0
                },
                {
                    "sent": "Any given language.",
                    "label": 0
                },
                {
                    "sent": "So what we have done to reduce the feature space of our classifier is actually 2.",
                    "label": 0
                },
                {
                    "sent": "Train the ngram profiles for each language and then use a cut off threshold.",
                    "label": 0
                },
                {
                    "sent": "So that means we discards all the words or N grams that have a lower frequency and we just maintain the top, let's say 400 for each language.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Like I said, the training of these language profiles is done on pre classified monolingual documents.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to show you an example of one of the language profiles, these are selected in grams from the English language profile and as you can see, we've padded.",
                    "label": 1
                },
                {
                    "sent": "Words with white spaces would just represent them as an_here, and the reason for that is you'll see that in the English language there are.",
                    "label": 0
                },
                {
                    "sent": "More words that start with a T and there are also several words that end with a T and the same with the trigram TH.",
                    "label": 0
                },
                {
                    "sent": "So for the total profile of the English language we trained, we trained it on approximately 600 monolingual documents.",
                    "label": 1
                },
                {
                    "sent": "We had a total of 41,000 unique engrams.",
                    "label": 0
                },
                {
                    "sent": "Now, maintaining only the top 400, discarding the rest and training our classifier for eight different languages.",
                    "label": 0
                },
                {
                    "sent": "We would have a worst case scenario of 400 by 8, but as the alphabet of the most languages are very similar, you'll also have similar engrams, although the order will be different.",
                    "label": 0
                },
                {
                    "sent": "So that means that the actual feature space is severely reduced.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the classification we utilized the running estimate of the SLW, which is P hat.",
                    "label": 0
                },
                {
                    "sent": "Of all the unique engrams in all the trained language profiles, so that means we take we maintain all the unique language, the engrams in all the language profiles, and that is then initialized as the running estimate and then as we do the classification, we measure the distance between the estimate and each of the trained language profiles which are in essence probability distributions and the measure.",
                    "label": 0
                },
                {
                    "sent": "We did use words the Euclidean distance, but.",
                    "label": 0
                },
                {
                    "sent": "Obviously we can use other similarity measures as well.",
                    "label": 0
                },
                {
                    "sent": "The benefit of doing this is that we can classify on the engram level.",
                    "label": 0
                },
                {
                    "sent": "We can collect engrams and classify whenever we want to.",
                    "label": 0
                },
                {
                    "sent": "We can collect words and collect and classify individual words.",
                    "label": 0
                },
                {
                    "sent": "Or if we have sentence boundary algorithm, we can classify sentences and in essence you can obviously classify the entire document.",
                    "label": 0
                },
                {
                    "sent": "If you want to do that.",
                    "label": 0
                },
                {
                    "sent": "AM.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Keeping that in mind.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to go into detail of the algorithms, but you can read this in the in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just have a look at how we did the testing so to be able to do large scale testing an not having to manually label each and every document, we took our set of pre labeled monolingual documents and we generated a test set of multilingual documents, meaning that we actually extracted random sentences from the model monolingual documents and combine them.",
                    "label": 0
                },
                {
                    "sent": "Into segments, so obviously it's easy for a human to see that these are three different segments to the classifier.",
                    "label": 0
                },
                {
                    "sent": "It's treated as one document, and the classifier's aim is to identify individual words or individual sentences which we tested with that as well by using a very simple sentence algorithm, we just appended.",
                    "label": 0
                },
                {
                    "sent": "At the end of 10 words.",
                    "label": 0
                },
                {
                    "sent": "So in this case you will see that.",
                    "label": 0
                },
                {
                    "sent": "You have a German segment of French segment and an English segment.",
                    "label": 0
                },
                {
                    "sent": "I've highlighted some of the words here because.",
                    "label": 0
                },
                {
                    "sent": "You'll see that the sentences are rather short, which then stimulates the environment we want to work with online conversations.",
                    "label": 0
                },
                {
                    "sent": "Also see that there are certain slang or short and chatty sentences, and there are also some industry specific words that might not even be of the current language we are identifying.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for our experiments.",
                    "label": 0
                },
                {
                    "sent": "We train the classifier on eight different languages.",
                    "label": 0
                },
                {
                    "sent": "Now this was fascinating.",
                    "label": 0
                },
                {
                    "sent": "The choice of languages becausw.",
                    "label": 0
                },
                {
                    "sent": "As you can see we have Norwegian and Swedish, which are very similar languages.",
                    "label": 0
                },
                {
                    "sent": "So for many non Scandinavian IAM from Norway by the way, they are literally impossible to see the difference of.",
                    "label": 0
                },
                {
                    "sent": "Spanish, obviously Italian.",
                    "label": 0
                },
                {
                    "sent": "There's a resemblance there.",
                    "label": 0
                },
                {
                    "sent": "Dutch German, so we have a.",
                    "label": 0
                },
                {
                    "sent": "We've made our classification problem quite hard in this case.",
                    "label": 0
                },
                {
                    "sent": "Like I said, for each of the language profiles we had approximately 600 sample documents, and each of those documents where had an average length of 15 to 20 KB.",
                    "label": 0
                },
                {
                    "sent": "We then did our experiments on three different language sets.",
                    "label": 0
                },
                {
                    "sent": "So instead of running all the 8 languages, we actually divided it up and said, let's try with three languages and then let's see how the classifier accuracy changes the dimensionality changes and how the scalability of the classifier itself actually adapts to adding more languages.",
                    "label": 0
                },
                {
                    "sent": "So we have three different sets with three, five and eight languages.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to spend too much time on this.",
                    "label": 0
                },
                {
                    "sent": "You can read about this in the paper as well.",
                    "label": 0
                },
                {
                    "sent": "These are the languages in the different sets, and we used different test sets with different sentence lengths and also corpus sizes.",
                    "label": 0
                },
                {
                    "sent": "Then to actually experiment with the learning parameter, which is Lambda and the cutoff, which was how many engrams we maintain per language profile, we had two different variations.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just looking at the results, this is the plot of the Euclidean distance between the running estimate of the classifier and.",
                    "label": 0
                },
                {
                    "sent": "Each of the three language profiles that we initialize the SLW with, so we had English, German and French and Now the interesting thing here is that this document was actually not multilingual.",
                    "label": 0
                },
                {
                    "sent": "It's a monolingual document written in French and it contains 300 words, and as you can see.",
                    "label": 0
                },
                {
                    "sent": "It rapidly.",
                    "label": 0
                },
                {
                    "sent": "Identifies that the document is written in French and throughout the entire document.",
                    "label": 0
                },
                {
                    "sent": "Although high variance, the distance to the other language profiles are significantly higher.",
                    "label": 0
                },
                {
                    "sent": "So there's no doubt anywhere in the document that this is a French document.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, looking at accuracies that we achieved when dealing with only three languages, we had accuracies for the English language as high as 99.6%.",
                    "label": 0
                },
                {
                    "sent": "The German we achieved 98.7.",
                    "label": 0
                },
                {
                    "sent": "And 99.2 for the French one, and the difference between set 1, two and three is the segment length.",
                    "label": 0
                },
                {
                    "sent": "So the first one has short sentences of 10 words.",
                    "label": 0
                },
                {
                    "sent": "The next one is 15 and the last one is 20.",
                    "label": 0
                },
                {
                    "sent": "So as you can see the accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously increases with the segment lengths.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just looking at one particular test case from our results.",
                    "label": 1
                },
                {
                    "sent": "What's interesting here is that.",
                    "label": 0
                },
                {
                    "sent": "Most of the mistakes that the French and the German classifier makes are classified as English, and by investigating the results we actually see that these languages do have words that are English, so that will obviously confuse the classifier and then the question or philosophical question is, should it be classified as English, or should it be classified as German?",
                    "label": 0
                },
                {
                    "sent": "But still, the average classifier accuracy of 98.96% is.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very good now.",
                    "label": 0
                },
                {
                    "sent": "The details of the last Test set where we used all 8 languages you can find in the paper.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is that the classifier accuracy does not significantly get dragged down by the fact that you're dealing with more languages.",
                    "label": 0
                },
                {
                    "sent": "What we do see is that in all of our different test sets, the Norwegian.",
                    "label": 0
                },
                {
                    "sent": "The classification showed the poorest results because we had Swedish in the mix as well, and the majority of these that were misclassified were actually classified as Swedish.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we have developed a classifier that is able to classify multilingual documents with high accuracy.",
                    "label": 1
                },
                {
                    "sent": "It performs extremely well on moderate side segments an we've achieved good results with shorter segments as short as 10 words per sentence for the first language set, which only contain three different languages, we achieved 99.6%, which is the highest accuracy that.",
                    "label": 1
                },
                {
                    "sent": "I have seen in any of the monolingual multilingual approaches that we have investigated, we've also shown that it scales well with in regards to the number of supported languages, and also the size of the corpus, and we are able to discriminate similar languages, although there are some improvements to be made.",
                    "label": 0
                },
                {
                    "sent": "It also deals with so-called chatty or slangy and short and error prone sentences, which we find in online conversations.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, there are quite a few possible directions for further work.",
                    "label": 0
                },
                {
                    "sent": "Be happy to speak with anyone of you if.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find this interesting.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for this very nice presentation are there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Questions or comments?",
                    "label": 0
                },
                {
                    "sent": "You said that your estimator is based on multiplicative updating mechanisms, and can you say some words about difference in practice with estimators based on attitudes?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in this case why it's a benefit of having a multiplicative updating rule in this environment.",
                    "label": 0
                },
                {
                    "sent": "This obviously that you want the classifier to rapidly adapt to changes, so if you were using a multiplicative updating rule, the convergence would be too slow to deal with this particular problem so.",
                    "label": 0
                },
                {
                    "sent": "If you have rapid changes, so in our in our case we didn't have that rapid changes, we forced, well, we force the training samples to be 3 segments of one language and then three segments of another language.",
                    "label": 0
                },
                {
                    "sent": "So you had quite a long window with to actually be able to identify it, but with the multiplicative updating rule, obviously penalty and the reward would actually significantly change the probabilities of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Which then again allows you to move quickly to the next, so we've seen examples when we've done our testing that you can actually have one word, and it will actually identify the change, even in that one word.",
                    "label": 0
                },
                {
                    "sent": "And that's obviously also because of the benefit of using N grams and not whole words as the discrimina Tori features, because you can have well all the information bearing contents of each individual engram and then do the classification on the word.",
                    "label": 0
                },
                {
                    "sent": "Did you compare the two approaches just for the curious on synthetic data, we have compared the SLWON Bernoulli trials, an also on multinomial parameters, where we compared it to the window based maximum likelihood method and in all of the tests we did and we have a lot of tests that we can, I can show you later the slowly converges faster and it's able to scale better in terms of the dimensionality, which is obviously very important in this application domain.",
                    "label": 0
                },
                {
                    "sent": "Or you have all the engrams that you need to maintain in your in your probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we have not compared this to other approaches to language classification other than.",
                    "label": 1
                },
                {
                    "sent": "What we've seen in the literature.",
                    "label": 0
                },
                {
                    "sent": "Another question, yes.",
                    "label": 1
                },
                {
                    "sent": "Training data from the destination translator.",
                    "label": 0
                },
                {
                    "sent": "Clearly monolingual text, yes.",
                    "label": 0
                },
                {
                    "sent": "That they are the same in the sense that the monolingual documents we used for training is gathered from online conversations.",
                    "label": 0
                },
                {
                    "sent": "So they are actually in itself.",
                    "label": 0
                },
                {
                    "sent": "They are chatty and slangy and short.",
                    "label": 0
                },
                {
                    "sent": "And then So what we did was we gathered 1200 monolingual documents and we used 600 of those to do the training and then the other 600 to generate the test set.",
                    "label": 0
                },
                {
                    "sent": "So for for these results that would you see how we did an ensemble average as well, so they.",
                    "label": 0
                },
                {
                    "sent": "On this on those documents.",
                    "label": 0
                },
                {
                    "sent": "Very good, I don't see any further questions or young one people here.",
                    "label": 0
                },
                {
                    "sent": "How many languages do you actually need to get in touch inputs classifier?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, selecting the engrams by frequency needed using the most frequent engrams.",
                    "label": 0
                },
                {
                    "sent": "It may be that the most frequent angles are not the most selective ones, as they're coming to our languages.",
                    "label": 0
                },
                {
                    "sent": "That's true, and that's also the benefit of combining unigrams, bigrams and trigrams becausw becausw you maintain.",
                    "label": 0
                },
                {
                    "sent": "In our case, we had 400 and 500 and we didn't see a significant loss of accuracy by going down to 400, which actually reduces the feature space quite a lot so.",
                    "label": 0
                },
                {
                    "sent": "If you were just doing unigrams, obviously you would not get the same accuracy at all.",
                    "label": 0
                },
                {
                    "sent": "It would be.",
                    "label": 0
                },
                {
                    "sent": "I don't think you would get good results at all with only using unigrams.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I answered your question, but I hope so.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take.",
                    "label": 0
                }
            ]
        }
    }
}