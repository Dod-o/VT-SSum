{
    "id": "tsgufizecvci5vcgowmi7zqiqpx2sfdo",
    "title": "Mining Discrete Patterns via Binary Matrix Factorization",
    "info": {
        "author": [
            "Jieping Ye, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Data Mining->Pattern Mining Applications"
        ]
    },
    "url": "http://videolectures.net/kdd09_ye_mdpbmf/",
    "segmentation": [
        [
            "So this work is about binary matrix factorization and it's a theoretical investigation.",
            "Lease is a joint work with my postdoc for home and my student, so one."
        ],
        [
            "So we are given a data matrix.",
            "Each row is a sample and each calling the feature.",
            "And all the entries are.",
            "Finally it's either zero or one.",
            "So for example, is each rose can be a binary image.",
            "In this case the columns can be a pixel, it's zero or one.",
            "Any each summer commit transaction.",
            "In this case, number columns will be item set item.",
            "So we are interested in ranking one approximation which is a product of two vectors, the red vector times blue vector.",
            "So this will be a rank one approximation.",
            "So all entries in these two vectors we badly so input is badly an output.",
            "This ranking is also badly.",
            "So this why why this cause?",
            "Binary ranking One plus mission.",
            "At least can be used for data compression, clustering and pattern discovery.",
            "So have given example.",
            "So we have a 6.",
            "All samples.",
            "12346 and would like to computer ankle approximations.",
            "OK result.",
            "It's a product of two vectors, this one red vector and blue vector.",
            "So here we call a second vectors that dominated patterns.",
            "Because this pattern is shared by 4 in this case for vectors.",
            "For examples, the first one third one 4th or 5th one surely spread.",
            "This means these samples and this dominant patterns left very similar to each other.",
            "The mismatch is small, OK.",
            "So that's what we call.",
            "This is the dominant pattern of this collection of these examples examples, and we call the first vector.",
            "It's called indicate vector, cause these entries 01 indicate whether particular example surely spent or not.",
            "So for example, it's the first entries one because the first sample share this pattern and the second one is 0.",
            "Because this one does not show this pattern and the next we showed this pattern and the last one does not have this pattern.",
            "So the first vector is indicate vector.",
            "The second one is we called dominant vector of this.",
            "Some collection of samples.",
            "So while this can be useful, for example.",
            "The singular first vector is the Enigma vector, so we can naturally be used over clustering, partitioning the data into two clusters of the ones we belong to one cluster Ashura pattern, and all the zeros.",
            "They do not show this pattern, and the second vector can be used to compute the dominant patterns.",
            "And lazy just one rank one plus meeting and we can do recursively.",
            "For example, after we find this dominant patterns for all the ones learn, we apply rank one approximation to all the zero vectors.",
            "OK, and then we find in the second dominant vectors and so on.",
            "So this will can be used for."
        ],
        [
            "Pattern discovery OK, so next I will show some examples.",
            "Applications for least binary rank, one approximation, the first noise image compression or data compression.",
            "So we are given a bunch of images.",
            "It's a binary image, so we convert to vector is zero or one.",
            "So one means least Black, Zero is its least white and will follow matrix binding matrix.",
            "Learn will apply Lisa Binary rank one approximation recursively to get data."
        ],
        [
            "Well, the second one is for Lisa Tree hierarchy tree construction.",
            "At the root note, we have the training data set, which is a collection of binary feature vectors.",
            "Then we compute ranking one badly approximation, then over, so we get X * Y transport.",
            "Remember this axis at least indicate vectors, so we can put all the ones miss all the samples which share this pattern to the left subtree, right?",
            "So these are all examples.",
            "Surely dominant vector Y.",
            "Then the remaining vectors which are zeros which do not.",
            "Have this pattern will go to the right side and we learn we do this recursively.",
            "We get this hierarchal tree.",
            "OK, so here's example, we apply this list reconstruction for collection of biological images.",
            "So in this trailer live node are the images all input vectors in this case are binary images.",
            "Learn the internal nodes list images.",
            "These are not real images, but these are the dominant patterns.",
            "So for example, this image is the dominant pattern shared by all of the image in this subtree, OK?",
            "And similarly so 0 means less, no pattern, and one means less part OK.",
            "So actually in this particular location in all of these image, each image correspond to a dream.",
            "So potentially by constructing history we can finally see evolution, history of dreams and the second application is we can do image retrieval efficiently because the height of trees is much smaller.",
            "Learn a number of images."
        ],
        [
            "In data set.",
            "So otherwise, the pattern discovery.",
            "Let's assume we are given image button image.",
            "We have a clear pattern, least for block, and we have some noise.",
            "If we apply binary rank one portion we can get perfect recovery.",
            "In this case we do for ranking one plus mission.",
            "If you do SVD.",
            "You can find pattern, but you do not get perfect recovery pain."
        ],
        [
            "This case.",
            "So next I will formulate diphylla ranking one approximation.",
            "So we are given a data matrix A.",
            "The number of roses.",
            "I want the number of columns.",
            "I took a small entries are 01 and we would like to compute rank one approximation which is a product of two vectors X 1 * X Two Annex one is the length by one, X2 is less I2.",
            "Both of these are ankle badly 01 OK.",
            "So we would like to compute X one X2, select the difference between our original matrix A.",
            "An approximation is as small as possible.",
            "So since in this case A and at least a matrix is finally an X 1 * X Two is automatically, so the difference is essentially capture ELISA mismatch between these two matrices.",
            "So we would like to compute rank one approximation which minimize the mismatch between the original matrix an hour plus."
        ],
        [
            "Matrix.",
            "So because it's a discrete 0, one of these problems is NP hard OK, and so if you want to get efficient algorithm, you have to somehow apply security sticks so they already sound exciting work in this area.",
            "So the key observation is.",
            "So we are computing X one X2, But if we say X one is no it's fixed.",
            "Let's say XYZ fixed X2 is given by the close form easily.",
            "OK so then we have a simple updating algorithm.",
            "We first get initial guess of X1.",
            "So basically you need to guess when we will compute X2.",
            "And based on the computer takes too long way.",
            "Update X1 and then repeat any guarantee convergence and about its local solution OK?",
            "So this works well in many applications, but it's all is not guaranteed.",
            "And Allah resulting approximation may be far away from the optimal one, so there's no guarantee on on approximation error.",
            "So this leads to the question we try to address in this in this work.",
            "So can we compute an approach approach solution not exactly right, but with a guaranteed allaband.",
            "This means we want to compute the solution which is not optimal, but which, but it's not far away from optimal.",
            "Some bond.",
            "And Secondly, can we compute it efficiently?"
        ],
        [
            "And so you need to walk away as you consider more general problem.",
            "So we add a regularization.",
            "It's Lambda is the parameter is negative and these two norm of X one X2.",
            "So this more general because if we settle and not to be 0 learning introduced the original binary ranking proximation problem.",
            "So this more general problem."
        ],
        [
            "It turns out if we compute the first thing out and combine these two terms, we can easily convert the minimum problem to actually a simple maximization problem, I will not get in detail about these two.",
            "Actually, is not not difficult follow transformation.",
            "So in the original problem we have a matrix, a binary matrix wherein the reformulation we have a matrix U.",
            "So you is just two elements.",
            "A is just 01, but you just also two elements.",
            "It's either 1 minus Lambda.",
            "If AIG is 1, learn your EULA corresponding new entries, 1 minus Lambda, if AIG zero, let's minus one, plus Lambda.",
            "So here Lambda is between 01.",
            "So this means if this one and then you is positive if a zero is negative is negative here, and the simple objective, Now it's product over vector matrix U and another vector at least we call maximum weight problem and this is the one we will focus on in this talk."
        ],
        [
            "And he's our main contributions.",
            "So we only show original binary ranking.",
            "One proximation is equivalent to a maximum."
        ],
        [
            "Weight problem is least one OK."
        ],
        [
            "And we surely, Sir, maximum weight problem can be reformulated as integer linear programming.",
            "So this is actually."
        ],
        [
            "Intuitive becausw logic peers quadratic write an if you want to come convert to injury in the program, so the objective must be linear."
        ],
        [
            "So we show by using a nice check, we can convert MWP little maximum weight problem to integer linear programming.",
            "Of course, this is very hard to solve its integer constraint.",
            "And we propose a error bounded approximation with a guaranteed element.",
            "2 /, 1 plus minimal one plus Lambda for example.",
            "Even landing zero then Alabama is 2.",
            "So this means that all we get approximate solution and the quality is good 'cause it's in the worst case is double of the optimal error.",
            "And more importantly, we show this error.",
            "Bounded approximation can be solved efficiently either using linear programming or maximum flow.",
            "So here's the overview of our approach.",
            "We already show it convert ranking approximation to a maximum weight problem list by quivalent less you matrix and the objective here is quadratic.",
            "And we will convert this into integer linear program.",
            "We call IL P1 so it's at least one is also hard to solve because of integer constraint.",
            "And we convert list the first initial linear programming to the second policeman still initially in the program we call IO P2 and there's a error bounded approximation.",
            "This means if we are able to compute the solution to LP two learn.",
            "This will be close to the optimal solution of IO P1.",
            "OK, so that's why we actually just focus on the second one, so you may ask why we get another IO P at least already.",
            "IOP at least not IOP.",
            "Why we where we define all?",
            "I hope it turns out the second IOP lesson nice property and we do a simple linear relaxation will relax all linear constraint to continuous constraint.",
            "So we got a linear programming relaxation for the second IO P and this can be solved efficiently by linear programming such as this simplex method.",
            "OK, it turns out if you solve a linear programming relaxation all the over the entries of the solution X1X2 are guaranteed to be integer.",
            "So this means that the LP tool can be solved efficiently and exactly by solving a simple linear programming.",
            "So with this long we get her efficient algorithm which solves bandolier ankle approximation with a guaranteed element.",
            "In this case, bodies at most two.",
            "However, the linear programming, also it's efficient.",
            "It's only limit to medium size problems, so it does not scale to large sets problems.",
            "So the last part we should at least I LP two or linear programming linear programming, relaxation.",
            "Is somehow equivalent to a minimum ethical problem so it can be solved efficiently by maximum flow?",
            "OK so to to my best knowledge leases, the first polynomial time algorithm list this linear program relaxation.",
            "Let's compute an approximate solution with this part galantine element.",
            "In this case, the bond is 2K.",
            "What follow minimalistic problem leases.",
            "The first work that connects Matrix factorization.",
            "Anna Minimum is minimized."
        ],
        [
            "Other problems?",
            "So next I will go through this all derivation in detail so we are given this a maximum weight problem.",
            "We have Lisa X1 transpose U X2 as objective and we want to convert to a linear program or any training program.",
            "OK so OK so here's a trick.",
            "Objectives quadratic OK and we want to convert to a linear program.",
            "So somewhere to convert the quadratic objective to a linear objective.",
            "OK, So what we do here is we combine X one X2.",
            "Into a joint available.",
            "OK so if your multiple lists lists three vectors.",
            "It's a submission of a bunch of entries and each entry is X1 the ice element.",
            "Times likes to dress element times the address element of you OK.",
            "So we do a simple valuable transformation.",
            "We define X.",
            "One eye is the I, central X1 and X2, J is the dress entry of X2 X one X2.",
            "These are we want to compute.",
            "Lower device Zi Dre is essentially the product of X y * X two J.",
            "So now if we replace X1 I times X 2G by the IG learn, we have a linear objective, which is you actually times the edge.",
            "OK, so now this part is done.",
            "We convert quadratic objective to a linear objective, but the tricky part is how do we make sure?",
            "ZIJ is equal to X 1 * X two JK.",
            "It turns out we can use these two simple linear constraint for AIG.",
            "Could 1A H O2 make sure.",
            "ZIJ always equal to XY and X2J.",
            "So this is the only slide I've actually give a pool, because this kind of important and it's it's not complicated.",
            "OK, so let's check.",
            "Wireless constraint.",
            "Guarantee I see actually is really XY attacks extra J.",
            "So let's look at this one.",
            "A address here is 1 and we know if a is 1 U is positive because Lambda is less than one, so you will be positive, OK.",
            "So lonely outlet 2 cases.",
            "Well actually 3 case, but let's let's 2 cases if both X one X2, G, R1 both are one.",
            "OK, so both lists are one.",
            "Then we move this.",
            "These are 2 -- 2 to the right side.",
            "So the idea is less than or equal to 1.",
            "Call Susie address like one so we have two choices.",
            "Is either zero or one because the idea is is a integer between 01.",
            "OK, so it's either zero or one.",
            "However, here we maximize UI three times the edge and the corresponding you is positive, so they will be the larger better OK.",
            "So we'll choose one.",
            "Z will be one, in this case the Andre is equal to X, one times extra days you'll learn the other cases even more.",
            "Volunteer, oh, let's say if.",
            "Normally 0.",
            "OK, in this case it's minus one.",
            "We moved to the right side, so two the Android less next one.",
            "So there's less than half.",
            "But there's no choice because the idea is integer sociology has to be 000.",
            "One man is this less than half, so if there is zero is equal to X 1 * X J, so it's still guaranteed.",
            "So.",
            "This simple linear linear inequality guaranteed.",
            "The idea is equal to X y * X to do the same thing for the second case.",
            "So now we convert N lower maximum weight problem to integer linear program OK?"
        ],
        [
            "So.",
            "I will call this IOP one the difficult part of IO.",
            "P1 is actually simple.",
            "Number 22 makes a lot of trouble, OK?",
            "So we have this key observations.",
            "We decompose or objective the submission of two tents corresponding to all the entries.",
            "Idea which AIG Zero is 1 or AIG OK. And the reason we do this is we try to get rid of these two.",
            "OK, so now let's look at the first time.",
            "It's all the summation of the age where the corresponding entries, AIG is 1 and from the first constraint.",
            "If I dress warm, Lindsey address bounded by the summation of these 2 + / 2.",
            "OK, so the idea is is a bounded and here we do maximum so.",
            "So what we do here is we remove the first constraint because this one cause some trouble because of these two will remove this constraint and then we replace the Android by its upper bound which is XY plus X 2 / 2.",
            "So we get."
        ],
        [
            "Our second initially napola IO P2, so the first constraint already removed and the first part of objective replaced by a bond.",
            "So flow I LP one too.",
            "I hope it with a simple relaxation solar objective of IO.",
            "P2 is no less than a lot of IO P1 because we do relaxation."
        ],
        [
            "So this slide shows.",
            "I hope you too has is a relaxation of IO P1, but the solution to the IO P2 is close to the optimal solution to IO P1.",
            "So here's the here's the result.",
            "The first part is the objective.",
            "It's a buyer proposed by LP 2K, so it's approximate objective.",
            "Well, the second part is the bond.",
            "With two over this one, and if not exist, so no larger than two bodies 2.",
            "Well, the first one because the minimum.",
            "So it's optimal objective.",
            "So this shows if we solve I LP 2.",
            "The error is no worse than double the error of optimal solution, so that's why it's called guaranteed bond approximation.",
            "Solar proof of this last request is not trivial, quite complex, but you can find me in."
        ],
        [
            "Paper.",
            "OK, so why do we derive IO P2 flow?",
            "Another IO P1, right?",
            "It turns out by removing this tool in the objective in the constraint.",
            "This has very nice property which we call totally unique modular.",
            "OK, so the coefficient matrix of all the constraint is totally unimodular.",
            "We actually prove.",
            "Becauses totally unimodular, based on results from this paper we can actually show if we solve this.",
            "IOP 2 by simple linear programming relaxation we the solution.",
            "It's guaranteed to be integer, so this means we can obtain an exact solution of IO P2 by solving its LP relaxation, and we know if we remove linear constraint, LP can be solved efficiently for medium size problems.",
            "So, however, LLP is still not scalable to large deployment 'cause there's a lot of large number of constraints.",
            "OK, so next week."
        ],
        [
            "So.",
            "Whole week actually faster so so far we have done the first part, it's MWP.",
            "Two linear program manager lowered or relaxation at least part is exact and wish you were in the last part.",
            "We show we can show the IOP to essentially is a minimalist color problem."
        ],
        [
            "So first we discussed is a generalized independent set problem.",
            "So we are given undirected graph G. So these are the set of vertices.",
            "E is the set of edges OK and we give two type of weight penalty.",
            "So that's a non negative weight W for each vertex V inset in V and we also give a penalty for each edge.",
            "So there's a.",
            "It's a graph right?",
            "So each node we give await so negative and each edge we also give a penalty.",
            "Solar journalistic independence at properties.",
            "Is here which would find a vertex subset.",
            "So we're given largest set V and we want to find subset which is called S which maximize this objective.",
            "Essentially we try to maximize the submission of all the weight of the vertices.",
            "OK, it's the submission of the vertices and we try to minimize because less negative we try to minimize the penalty on."
        ],
        [
            "Oval edges, so we're given a large graph.",
            "And we try to find a small sub graph.",
            "Which maximize the submission of Liberty says minimize the summation over the penalty over edges.",
            "So let's look."
        ],
        [
            "Generalized independent data problem OK."
        ],
        [
            "So it turns out our second IOP IOP two is a special case of generalized independent set problem by defining specific weight vector for viruses and specific penalty for address.",
            "So we call this is our IO P2.",
            "And clearly the first part define a weight for each of the vertices.",
            "So here we find we have X one X2 vectors.",
            "XY is the role vector, X2 is convector, and each entry in the X one, each entry in X2 divina node.",
            "OK.",
            "So before this graph each entry in the next one we have I-1 node in X1 and well I two nodes in the X2.",
            "So this divine node and the connection.",
            "If AIG is 1 if AIG is 1, this means the ice element of X1 dress element of actually store connected.",
            "That's why we found graph is undirected graph.",
            "And we can simply define.",
            "Wait for vex XY as this one.",
            "It's cause for this part, OK?",
            "And the wait for the 2nd.",
            "No X2 J is this part.",
            "So the first part.",
            "Define love with vector for the vertices because of vertices are essentially XY X2J.",
            "So here I see I enjoy learning, is the edge right?",
            "It's connecting X1 I 2X2 J so one plus Lambda will be the penalty for each edge.",
            "So that's a simple definition for weight vector for the penalties.",
            "With this definition, the LP Two Defiant special case of GIS Anna Nice property and which is actually important is this graph is bipartite graph OK?",
            "Which one side is all over?",
            "Entries in X1I.",
            "One note in the first part OK.",
            "Laying the second party is all over entries in X2.",
            "We have two entries.",
            "And as I drive, which connects between X1X2, these are the least of the penalty.",
            "So it's it's a bipartite graph."
        ],
        [
            "So unfortunately the generalized impedance other problem it's also and be hard for general graph.",
            "So this unfortunate however.",
            "For special case, will a graph is bipartite graph?",
            "It can be solved in polynomial time, so that's nice.",
            "And more importantly, there's a recent list paper in 1997, so if the graph is bipartite graph learn jazz essentially can be solved by finding the maximum flow, so which can be, which has a cubic time complexity and our results show this if we compute the GIS by maximum flow, this much more efficient.",
            "Learn linear programming by using simplex.",
            "If the problem size is large.",
            "So now we have two solutions.",
            "Still a linear program.",
            "Relaxation of IO P2.",
            "All this maximum flow and we choose the second one because this one is more more efficient."
        ],
        [
            "So next few slides I will show some results.",
            "So the first part is to, uh, verify our theoretical results, verify the album.",
            "OK, so we have several test cases.",
            "The green Line shows our theoretical band, which is 2 / 1 plus minimum between one Lambda.",
            "So the X axis is Lambda and the Y axis is all bound.",
            "So green line.",
            "And Alisa Blue line shows all the bond shoulder approximation error of the proposed algorithm either by linear programming or by finding maximum flow.",
            "And we can see in both cases the blue line is below the green line.",
            "So that's nice because learning is consistent with our theoretical results.",
            "So the error is bounded by this theoretical model."
        ],
        [
            "Similar following next two cases, the Blue line is always below green line and in these two cases went on a size large learn.",
            "Actually we can show we can see the bond is quite tight.",
            "These two curves are valid close like in this case it's almost identical, so this shows our bond.",
            "The bond with computers are very tight.",
            "It may be difficult to actually."
        ],
        [
            "Will improve.",
            "So this is running time.",
            "The first part is.",
            "We fix the.",
            "We fixed the sample size to be 1000, an increase dimension.",
            "So it's about linear time complexity and the 2nd is we fixed dimension to be 1000 and we increase the data points.",
            "So it's not quite linear OK.",
            "But still efficient."
        ],
        [
            "And here's our summary.",
            "OK yeah, thank you.",
            "Shoes.",
            "Yeah.",
            "Yes only by the original application images in front of clients or compressing them.",
            "Applying the discrete packets, and then I'll be handle rotation and segmentation annihilations.",
            "Nokia will only be online, so for example full application we fall by 1 means or standard.",
            "Same dimensions, same size, so we will do some quick assessment.",
            "Big Ben log in with the translation location.",
            "Oh, OK, see the tricky part of this division is the way we convert quadratic objective to linear objective.",
            "I think this only works 11 entries are binding 01 if we generalize to buy a general problem might be hard.",
            "OK, so this simple trick but works perfectly well for binding problem 01.",
            "OK, so for general case I think it may be difficult.",
            "Might be difficult.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this work is about binary matrix factorization and it's a theoretical investigation.",
                    "label": 0
                },
                {
                    "sent": "Lease is a joint work with my postdoc for home and my student, so one.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are given a data matrix.",
                    "label": 0
                },
                {
                    "sent": "Each row is a sample and each calling the feature.",
                    "label": 0
                },
                {
                    "sent": "And all the entries are.",
                    "label": 0
                },
                {
                    "sent": "Finally it's either zero or one.",
                    "label": 0
                },
                {
                    "sent": "So for example, is each rose can be a binary image.",
                    "label": 0
                },
                {
                    "sent": "In this case the columns can be a pixel, it's zero or one.",
                    "label": 0
                },
                {
                    "sent": "Any each summer commit transaction.",
                    "label": 0
                },
                {
                    "sent": "In this case, number columns will be item set item.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in ranking one approximation which is a product of two vectors, the red vector times blue vector.",
                    "label": 0
                },
                {
                    "sent": "So this will be a rank one approximation.",
                    "label": 0
                },
                {
                    "sent": "So all entries in these two vectors we badly so input is badly an output.",
                    "label": 0
                },
                {
                    "sent": "This ranking is also badly.",
                    "label": 0
                },
                {
                    "sent": "So this why why this cause?",
                    "label": 0
                },
                {
                    "sent": "Binary ranking One plus mission.",
                    "label": 0
                },
                {
                    "sent": "At least can be used for data compression, clustering and pattern discovery.",
                    "label": 1
                },
                {
                    "sent": "So have given example.",
                    "label": 0
                },
                {
                    "sent": "So we have a 6.",
                    "label": 0
                },
                {
                    "sent": "All samples.",
                    "label": 0
                },
                {
                    "sent": "12346 and would like to computer ankle approximations.",
                    "label": 0
                },
                {
                    "sent": "OK result.",
                    "label": 0
                },
                {
                    "sent": "It's a product of two vectors, this one red vector and blue vector.",
                    "label": 0
                },
                {
                    "sent": "So here we call a second vectors that dominated patterns.",
                    "label": 0
                },
                {
                    "sent": "Because this pattern is shared by 4 in this case for vectors.",
                    "label": 0
                },
                {
                    "sent": "For examples, the first one third one 4th or 5th one surely spread.",
                    "label": 0
                },
                {
                    "sent": "This means these samples and this dominant patterns left very similar to each other.",
                    "label": 0
                },
                {
                    "sent": "The mismatch is small, OK.",
                    "label": 0
                },
                {
                    "sent": "So that's what we call.",
                    "label": 1
                },
                {
                    "sent": "This is the dominant pattern of this collection of these examples examples, and we call the first vector.",
                    "label": 0
                },
                {
                    "sent": "It's called indicate vector, cause these entries 01 indicate whether particular example surely spent or not.",
                    "label": 0
                },
                {
                    "sent": "So for example, it's the first entries one because the first sample share this pattern and the second one is 0.",
                    "label": 0
                },
                {
                    "sent": "Because this one does not show this pattern and the next we showed this pattern and the last one does not have this pattern.",
                    "label": 0
                },
                {
                    "sent": "So the first vector is indicate vector.",
                    "label": 0
                },
                {
                    "sent": "The second one is we called dominant vector of this.",
                    "label": 0
                },
                {
                    "sent": "Some collection of samples.",
                    "label": 0
                },
                {
                    "sent": "So while this can be useful, for example.",
                    "label": 0
                },
                {
                    "sent": "The singular first vector is the Enigma vector, so we can naturally be used over clustering, partitioning the data into two clusters of the ones we belong to one cluster Ashura pattern, and all the zeros.",
                    "label": 0
                },
                {
                    "sent": "They do not show this pattern, and the second vector can be used to compute the dominant patterns.",
                    "label": 0
                },
                {
                    "sent": "And lazy just one rank one plus meeting and we can do recursively.",
                    "label": 0
                },
                {
                    "sent": "For example, after we find this dominant patterns for all the ones learn, we apply rank one approximation to all the zero vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we find in the second dominant vectors and so on.",
                    "label": 0
                },
                {
                    "sent": "So this will can be used for.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pattern discovery OK, so next I will show some examples.",
                    "label": 0
                },
                {
                    "sent": "Applications for least binary rank, one approximation, the first noise image compression or data compression.",
                    "label": 1
                },
                {
                    "sent": "So we are given a bunch of images.",
                    "label": 0
                },
                {
                    "sent": "It's a binary image, so we convert to vector is zero or one.",
                    "label": 0
                },
                {
                    "sent": "So one means least Black, Zero is its least white and will follow matrix binding matrix.",
                    "label": 0
                },
                {
                    "sent": "Learn will apply Lisa Binary rank one approximation recursively to get data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the second one is for Lisa Tree hierarchy tree construction.",
                    "label": 0
                },
                {
                    "sent": "At the root note, we have the training data set, which is a collection of binary feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Then we compute ranking one badly approximation, then over, so we get X * Y transport.",
                    "label": 0
                },
                {
                    "sent": "Remember this axis at least indicate vectors, so we can put all the ones miss all the samples which share this pattern to the left subtree, right?",
                    "label": 0
                },
                {
                    "sent": "So these are all examples.",
                    "label": 0
                },
                {
                    "sent": "Surely dominant vector Y.",
                    "label": 0
                },
                {
                    "sent": "Then the remaining vectors which are zeros which do not.",
                    "label": 0
                },
                {
                    "sent": "Have this pattern will go to the right side and we learn we do this recursively.",
                    "label": 0
                },
                {
                    "sent": "We get this hierarchal tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's example, we apply this list reconstruction for collection of biological images.",
                    "label": 0
                },
                {
                    "sent": "So in this trailer live node are the images all input vectors in this case are binary images.",
                    "label": 0
                },
                {
                    "sent": "Learn the internal nodes list images.",
                    "label": 0
                },
                {
                    "sent": "These are not real images, but these are the dominant patterns.",
                    "label": 0
                },
                {
                    "sent": "So for example, this image is the dominant pattern shared by all of the image in this subtree, OK?",
                    "label": 0
                },
                {
                    "sent": "And similarly so 0 means less, no pattern, and one means less part OK.",
                    "label": 0
                },
                {
                    "sent": "So actually in this particular location in all of these image, each image correspond to a dream.",
                    "label": 0
                },
                {
                    "sent": "So potentially by constructing history we can finally see evolution, history of dreams and the second application is we can do image retrieval efficiently because the height of trees is much smaller.",
                    "label": 0
                },
                {
                    "sent": "Learn a number of images.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In data set.",
                    "label": 0
                },
                {
                    "sent": "So otherwise, the pattern discovery.",
                    "label": 1
                },
                {
                    "sent": "Let's assume we are given image button image.",
                    "label": 0
                },
                {
                    "sent": "We have a clear pattern, least for block, and we have some noise.",
                    "label": 0
                },
                {
                    "sent": "If we apply binary rank one portion we can get perfect recovery.",
                    "label": 1
                },
                {
                    "sent": "In this case we do for ranking one plus mission.",
                    "label": 0
                },
                {
                    "sent": "If you do SVD.",
                    "label": 0
                },
                {
                    "sent": "You can find pattern, but you do not get perfect recovery pain.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This case.",
                    "label": 0
                },
                {
                    "sent": "So next I will formulate diphylla ranking one approximation.",
                    "label": 0
                },
                {
                    "sent": "So we are given a data matrix A.",
                    "label": 0
                },
                {
                    "sent": "The number of roses.",
                    "label": 0
                },
                {
                    "sent": "I want the number of columns.",
                    "label": 1
                },
                {
                    "sent": "I took a small entries are 01 and we would like to compute rank one approximation which is a product of two vectors X 1 * X Two Annex one is the length by one, X2 is less I2.",
                    "label": 0
                },
                {
                    "sent": "Both of these are ankle badly 01 OK.",
                    "label": 0
                },
                {
                    "sent": "So we would like to compute X one X2, select the difference between our original matrix A.",
                    "label": 0
                },
                {
                    "sent": "An approximation is as small as possible.",
                    "label": 0
                },
                {
                    "sent": "So since in this case A and at least a matrix is finally an X 1 * X Two is automatically, so the difference is essentially capture ELISA mismatch between these two matrices.",
                    "label": 1
                },
                {
                    "sent": "So we would like to compute rank one approximation which minimize the mismatch between the original matrix an hour plus.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "So because it's a discrete 0, one of these problems is NP hard OK, and so if you want to get efficient algorithm, you have to somehow apply security sticks so they already sound exciting work in this area.",
                    "label": 0
                },
                {
                    "sent": "So the key observation is.",
                    "label": 0
                },
                {
                    "sent": "So we are computing X one X2, But if we say X one is no it's fixed.",
                    "label": 0
                },
                {
                    "sent": "Let's say XYZ fixed X2 is given by the close form easily.",
                    "label": 0
                },
                {
                    "sent": "OK so then we have a simple updating algorithm.",
                    "label": 0
                },
                {
                    "sent": "We first get initial guess of X1.",
                    "label": 0
                },
                {
                    "sent": "So basically you need to guess when we will compute X2.",
                    "label": 0
                },
                {
                    "sent": "And based on the computer takes too long way.",
                    "label": 1
                },
                {
                    "sent": "Update X1 and then repeat any guarantee convergence and about its local solution OK?",
                    "label": 0
                },
                {
                    "sent": "So this works well in many applications, but it's all is not guaranteed.",
                    "label": 0
                },
                {
                    "sent": "And Allah resulting approximation may be far away from the optimal one, so there's no guarantee on on approximation error.",
                    "label": 0
                },
                {
                    "sent": "So this leads to the question we try to address in this in this work.",
                    "label": 0
                },
                {
                    "sent": "So can we compute an approach approach solution not exactly right, but with a guaranteed allaband.",
                    "label": 1
                },
                {
                    "sent": "This means we want to compute the solution which is not optimal, but which, but it's not far away from optimal.",
                    "label": 0
                },
                {
                    "sent": "Some bond.",
                    "label": 1
                },
                {
                    "sent": "And Secondly, can we compute it efficiently?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you need to walk away as you consider more general problem.",
                    "label": 0
                },
                {
                    "sent": "So we add a regularization.",
                    "label": 0
                },
                {
                    "sent": "It's Lambda is the parameter is negative and these two norm of X one X2.",
                    "label": 0
                },
                {
                    "sent": "So this more general because if we settle and not to be 0 learning introduced the original binary ranking proximation problem.",
                    "label": 0
                },
                {
                    "sent": "So this more general problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out if we compute the first thing out and combine these two terms, we can easily convert the minimum problem to actually a simple maximization problem, I will not get in detail about these two.",
                    "label": 0
                },
                {
                    "sent": "Actually, is not not difficult follow transformation.",
                    "label": 0
                },
                {
                    "sent": "So in the original problem we have a matrix, a binary matrix wherein the reformulation we have a matrix U.",
                    "label": 0
                },
                {
                    "sent": "So you is just two elements.",
                    "label": 0
                },
                {
                    "sent": "A is just 01, but you just also two elements.",
                    "label": 0
                },
                {
                    "sent": "It's either 1 minus Lambda.",
                    "label": 0
                },
                {
                    "sent": "If AIG is 1, learn your EULA corresponding new entries, 1 minus Lambda, if AIG zero, let's minus one, plus Lambda.",
                    "label": 0
                },
                {
                    "sent": "So here Lambda is between 01.",
                    "label": 0
                },
                {
                    "sent": "So this means if this one and then you is positive if a zero is negative is negative here, and the simple objective, Now it's product over vector matrix U and another vector at least we call maximum weight problem and this is the one we will focus on in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And he's our main contributions.",
                    "label": 1
                },
                {
                    "sent": "So we only show original binary ranking.",
                    "label": 0
                },
                {
                    "sent": "One proximation is equivalent to a maximum.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weight problem is least one OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we surely, Sir, maximum weight problem can be reformulated as integer linear programming.",
                    "label": 0
                },
                {
                    "sent": "So this is actually.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intuitive becausw logic peers quadratic write an if you want to come convert to injury in the program, so the objective must be linear.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we show by using a nice check, we can convert MWP little maximum weight problem to integer linear programming.",
                    "label": 1
                },
                {
                    "sent": "Of course, this is very hard to solve its integer constraint.",
                    "label": 0
                },
                {
                    "sent": "And we propose a error bounded approximation with a guaranteed element.",
                    "label": 0
                },
                {
                    "sent": "2 /, 1 plus minimal one plus Lambda for example.",
                    "label": 0
                },
                {
                    "sent": "Even landing zero then Alabama is 2.",
                    "label": 0
                },
                {
                    "sent": "So this means that all we get approximate solution and the quality is good 'cause it's in the worst case is double of the optimal error.",
                    "label": 0
                },
                {
                    "sent": "And more importantly, we show this error.",
                    "label": 0
                },
                {
                    "sent": "Bounded approximation can be solved efficiently either using linear programming or maximum flow.",
                    "label": 0
                },
                {
                    "sent": "So here's the overview of our approach.",
                    "label": 0
                },
                {
                    "sent": "We already show it convert ranking approximation to a maximum weight problem list by quivalent less you matrix and the objective here is quadratic.",
                    "label": 0
                },
                {
                    "sent": "And we will convert this into integer linear program.",
                    "label": 0
                },
                {
                    "sent": "We call IL P1 so it's at least one is also hard to solve because of integer constraint.",
                    "label": 0
                },
                {
                    "sent": "And we convert list the first initial linear programming to the second policeman still initially in the program we call IO P2 and there's a error bounded approximation.",
                    "label": 0
                },
                {
                    "sent": "This means if we are able to compute the solution to LP two learn.",
                    "label": 0
                },
                {
                    "sent": "This will be close to the optimal solution of IO P1.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we actually just focus on the second one, so you may ask why we get another IO P at least already.",
                    "label": 0
                },
                {
                    "sent": "IOP at least not IOP.",
                    "label": 0
                },
                {
                    "sent": "Why we where we define all?",
                    "label": 0
                },
                {
                    "sent": "I hope it turns out the second IOP lesson nice property and we do a simple linear relaxation will relax all linear constraint to continuous constraint.",
                    "label": 0
                },
                {
                    "sent": "So we got a linear programming relaxation for the second IO P and this can be solved efficiently by linear programming such as this simplex method.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out if you solve a linear programming relaxation all the over the entries of the solution X1X2 are guaranteed to be integer.",
                    "label": 0
                },
                {
                    "sent": "So this means that the LP tool can be solved efficiently and exactly by solving a simple linear programming.",
                    "label": 0
                },
                {
                    "sent": "So with this long we get her efficient algorithm which solves bandolier ankle approximation with a guaranteed element.",
                    "label": 0
                },
                {
                    "sent": "In this case, bodies at most two.",
                    "label": 0
                },
                {
                    "sent": "However, the linear programming, also it's efficient.",
                    "label": 0
                },
                {
                    "sent": "It's only limit to medium size problems, so it does not scale to large sets problems.",
                    "label": 0
                },
                {
                    "sent": "So the last part we should at least I LP two or linear programming linear programming, relaxation.",
                    "label": 1
                },
                {
                    "sent": "Is somehow equivalent to a minimum ethical problem so it can be solved efficiently by maximum flow?",
                    "label": 0
                },
                {
                    "sent": "OK so to to my best knowledge leases, the first polynomial time algorithm list this linear program relaxation.",
                    "label": 0
                },
                {
                    "sent": "Let's compute an approximate solution with this part galantine element.",
                    "label": 0
                },
                {
                    "sent": "In this case, the bond is 2K.",
                    "label": 1
                },
                {
                    "sent": "What follow minimalistic problem leases.",
                    "label": 0
                },
                {
                    "sent": "The first work that connects Matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Anna Minimum is minimized.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other problems?",
                    "label": 0
                },
                {
                    "sent": "So next I will go through this all derivation in detail so we are given this a maximum weight problem.",
                    "label": 0
                },
                {
                    "sent": "We have Lisa X1 transpose U X2 as objective and we want to convert to a linear program or any training program.",
                    "label": 0
                },
                {
                    "sent": "OK so OK so here's a trick.",
                    "label": 0
                },
                {
                    "sent": "Objectives quadratic OK and we want to convert to a linear program.",
                    "label": 0
                },
                {
                    "sent": "So somewhere to convert the quadratic objective to a linear objective.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do here is we combine X one X2.",
                    "label": 0
                },
                {
                    "sent": "Into a joint available.",
                    "label": 0
                },
                {
                    "sent": "OK so if your multiple lists lists three vectors.",
                    "label": 0
                },
                {
                    "sent": "It's a submission of a bunch of entries and each entry is X1 the ice element.",
                    "label": 0
                },
                {
                    "sent": "Times likes to dress element times the address element of you OK.",
                    "label": 0
                },
                {
                    "sent": "So we do a simple valuable transformation.",
                    "label": 0
                },
                {
                    "sent": "We define X.",
                    "label": 0
                },
                {
                    "sent": "One eye is the I, central X1 and X2, J is the dress entry of X2 X one X2.",
                    "label": 0
                },
                {
                    "sent": "These are we want to compute.",
                    "label": 0
                },
                {
                    "sent": "Lower device Zi Dre is essentially the product of X y * X two J.",
                    "label": 0
                },
                {
                    "sent": "So now if we replace X1 I times X 2G by the IG learn, we have a linear objective, which is you actually times the edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this part is done.",
                    "label": 0
                },
                {
                    "sent": "We convert quadratic objective to a linear objective, but the tricky part is how do we make sure?",
                    "label": 0
                },
                {
                    "sent": "ZIJ is equal to X 1 * X two JK.",
                    "label": 0
                },
                {
                    "sent": "It turns out we can use these two simple linear constraint for AIG.",
                    "label": 0
                },
                {
                    "sent": "Could 1A H O2 make sure.",
                    "label": 0
                },
                {
                    "sent": "ZIJ always equal to XY and X2J.",
                    "label": 0
                },
                {
                    "sent": "So this is the only slide I've actually give a pool, because this kind of important and it's it's not complicated.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's check.",
                    "label": 0
                },
                {
                    "sent": "Wireless constraint.",
                    "label": 0
                },
                {
                    "sent": "Guarantee I see actually is really XY attacks extra J.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this one.",
                    "label": 0
                },
                {
                    "sent": "A address here is 1 and we know if a is 1 U is positive because Lambda is less than one, so you will be positive, OK.",
                    "label": 0
                },
                {
                    "sent": "So lonely outlet 2 cases.",
                    "label": 0
                },
                {
                    "sent": "Well actually 3 case, but let's let's 2 cases if both X one X2, G, R1 both are one.",
                    "label": 0
                },
                {
                    "sent": "OK, so both lists are one.",
                    "label": 0
                },
                {
                    "sent": "Then we move this.",
                    "label": 0
                },
                {
                    "sent": "These are 2 -- 2 to the right side.",
                    "label": 0
                },
                {
                    "sent": "So the idea is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Call Susie address like one so we have two choices.",
                    "label": 0
                },
                {
                    "sent": "Is either zero or one because the idea is is a integer between 01.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's either zero or one.",
                    "label": 0
                },
                {
                    "sent": "However, here we maximize UI three times the edge and the corresponding you is positive, so they will be the larger better OK.",
                    "label": 0
                },
                {
                    "sent": "So we'll choose one.",
                    "label": 0
                },
                {
                    "sent": "Z will be one, in this case the Andre is equal to X, one times extra days you'll learn the other cases even more.",
                    "label": 0
                },
                {
                    "sent": "Volunteer, oh, let's say if.",
                    "label": 0
                },
                {
                    "sent": "Normally 0.",
                    "label": 0
                },
                {
                    "sent": "OK, in this case it's minus one.",
                    "label": 0
                },
                {
                    "sent": "We moved to the right side, so two the Android less next one.",
                    "label": 0
                },
                {
                    "sent": "So there's less than half.",
                    "label": 0
                },
                {
                    "sent": "But there's no choice because the idea is integer sociology has to be 000.",
                    "label": 0
                },
                {
                    "sent": "One man is this less than half, so if there is zero is equal to X 1 * X J, so it's still guaranteed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This simple linear linear inequality guaranteed.",
                    "label": 0
                },
                {
                    "sent": "The idea is equal to X y * X to do the same thing for the second case.",
                    "label": 0
                },
                {
                    "sent": "So now we convert N lower maximum weight problem to integer linear program OK?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I will call this IOP one the difficult part of IO.",
                    "label": 0
                },
                {
                    "sent": "P1 is actually simple.",
                    "label": 0
                },
                {
                    "sent": "Number 22 makes a lot of trouble, OK?",
                    "label": 0
                },
                {
                    "sent": "So we have this key observations.",
                    "label": 0
                },
                {
                    "sent": "We decompose or objective the submission of two tents corresponding to all the entries.",
                    "label": 0
                },
                {
                    "sent": "Idea which AIG Zero is 1 or AIG OK. And the reason we do this is we try to get rid of these two.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at the first time.",
                    "label": 0
                },
                {
                    "sent": "It's all the summation of the age where the corresponding entries, AIG is 1 and from the first constraint.",
                    "label": 0
                },
                {
                    "sent": "If I dress warm, Lindsey address bounded by the summation of these 2 + / 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is is a bounded and here we do maximum so.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is we remove the first constraint because this one cause some trouble because of these two will remove this constraint and then we replace the Android by its upper bound which is XY plus X 2 / 2.",
                    "label": 0
                },
                {
                    "sent": "So we get.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our second initially napola IO P2, so the first constraint already removed and the first part of objective replaced by a bond.",
                    "label": 0
                },
                {
                    "sent": "So flow I LP one too.",
                    "label": 0
                },
                {
                    "sent": "I hope it with a simple relaxation solar objective of IO.",
                    "label": 0
                },
                {
                    "sent": "P2 is no less than a lot of IO P1 because we do relaxation.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this slide shows.",
                    "label": 0
                },
                {
                    "sent": "I hope you too has is a relaxation of IO P1, but the solution to the IO P2 is close to the optimal solution to IO P1.",
                    "label": 0
                },
                {
                    "sent": "So here's the here's the result.",
                    "label": 0
                },
                {
                    "sent": "The first part is the objective.",
                    "label": 0
                },
                {
                    "sent": "It's a buyer proposed by LP 2K, so it's approximate objective.",
                    "label": 0
                },
                {
                    "sent": "Well, the second part is the bond.",
                    "label": 0
                },
                {
                    "sent": "With two over this one, and if not exist, so no larger than two bodies 2.",
                    "label": 0
                },
                {
                    "sent": "Well, the first one because the minimum.",
                    "label": 0
                },
                {
                    "sent": "So it's optimal objective.",
                    "label": 0
                },
                {
                    "sent": "So this shows if we solve I LP 2.",
                    "label": 0
                },
                {
                    "sent": "The error is no worse than double the error of optimal solution, so that's why it's called guaranteed bond approximation.",
                    "label": 0
                },
                {
                    "sent": "Solar proof of this last request is not trivial, quite complex, but you can find me in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do we derive IO P2 flow?",
                    "label": 0
                },
                {
                    "sent": "Another IO P1, right?",
                    "label": 0
                },
                {
                    "sent": "It turns out by removing this tool in the objective in the constraint.",
                    "label": 0
                },
                {
                    "sent": "This has very nice property which we call totally unique modular.",
                    "label": 0
                },
                {
                    "sent": "OK, so the coefficient matrix of all the constraint is totally unimodular.",
                    "label": 1
                },
                {
                    "sent": "We actually prove.",
                    "label": 0
                },
                {
                    "sent": "Becauses totally unimodular, based on results from this paper we can actually show if we solve this.",
                    "label": 1
                },
                {
                    "sent": "IOP 2 by simple linear programming relaxation we the solution.",
                    "label": 0
                },
                {
                    "sent": "It's guaranteed to be integer, so this means we can obtain an exact solution of IO P2 by solving its LP relaxation, and we know if we remove linear constraint, LP can be solved efficiently for medium size problems.",
                    "label": 1
                },
                {
                    "sent": "So, however, LLP is still not scalable to large deployment 'cause there's a lot of large number of constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so next week.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Whole week actually faster so so far we have done the first part, it's MWP.",
                    "label": 0
                },
                {
                    "sent": "Two linear program manager lowered or relaxation at least part is exact and wish you were in the last part.",
                    "label": 0
                },
                {
                    "sent": "We show we can show the IOP to essentially is a minimalist color problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we discussed is a generalized independent set problem.",
                    "label": 1
                },
                {
                    "sent": "So we are given undirected graph G. So these are the set of vertices.",
                    "label": 0
                },
                {
                    "sent": "E is the set of edges OK and we give two type of weight penalty.",
                    "label": 0
                },
                {
                    "sent": "So that's a non negative weight W for each vertex V inset in V and we also give a penalty for each edge.",
                    "label": 1
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "It's a graph right?",
                    "label": 0
                },
                {
                    "sent": "So each node we give await so negative and each edge we also give a penalty.",
                    "label": 0
                },
                {
                    "sent": "Solar journalistic independence at properties.",
                    "label": 1
                },
                {
                    "sent": "Is here which would find a vertex subset.",
                    "label": 0
                },
                {
                    "sent": "So we're given largest set V and we want to find subset which is called S which maximize this objective.",
                    "label": 0
                },
                {
                    "sent": "Essentially we try to maximize the submission of all the weight of the vertices.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the submission of the vertices and we try to minimize because less negative we try to minimize the penalty on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oval edges, so we're given a large graph.",
                    "label": 0
                },
                {
                    "sent": "And we try to find a small sub graph.",
                    "label": 0
                },
                {
                    "sent": "Which maximize the submission of Liberty says minimize the summation over the penalty over edges.",
                    "label": 0
                },
                {
                    "sent": "So let's look.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalized independent data problem OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out our second IOP IOP two is a special case of generalized independent set problem by defining specific weight vector for viruses and specific penalty for address.",
                    "label": 0
                },
                {
                    "sent": "So we call this is our IO P2.",
                    "label": 0
                },
                {
                    "sent": "And clearly the first part define a weight for each of the vertices.",
                    "label": 0
                },
                {
                    "sent": "So here we find we have X one X2 vectors.",
                    "label": 0
                },
                {
                    "sent": "XY is the role vector, X2 is convector, and each entry in the X one, each entry in X2 divina node.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So before this graph each entry in the next one we have I-1 node in X1 and well I two nodes in the X2.",
                    "label": 1
                },
                {
                    "sent": "So this divine node and the connection.",
                    "label": 0
                },
                {
                    "sent": "If AIG is 1 if AIG is 1, this means the ice element of X1 dress element of actually store connected.",
                    "label": 0
                },
                {
                    "sent": "That's why we found graph is undirected graph.",
                    "label": 0
                },
                {
                    "sent": "And we can simply define.",
                    "label": 0
                },
                {
                    "sent": "Wait for vex XY as this one.",
                    "label": 0
                },
                {
                    "sent": "It's cause for this part, OK?",
                    "label": 1
                },
                {
                    "sent": "And the wait for the 2nd.",
                    "label": 0
                },
                {
                    "sent": "No X2 J is this part.",
                    "label": 0
                },
                {
                    "sent": "So the first part.",
                    "label": 0
                },
                {
                    "sent": "Define love with vector for the vertices because of vertices are essentially XY X2J.",
                    "label": 0
                },
                {
                    "sent": "So here I see I enjoy learning, is the edge right?",
                    "label": 0
                },
                {
                    "sent": "It's connecting X1 I 2X2 J so one plus Lambda will be the penalty for each edge.",
                    "label": 0
                },
                {
                    "sent": "So that's a simple definition for weight vector for the penalties.",
                    "label": 0
                },
                {
                    "sent": "With this definition, the LP Two Defiant special case of GIS Anna Nice property and which is actually important is this graph is bipartite graph OK?",
                    "label": 1
                },
                {
                    "sent": "Which one side is all over?",
                    "label": 0
                },
                {
                    "sent": "Entries in X1I.",
                    "label": 0
                },
                {
                    "sent": "One note in the first part OK.",
                    "label": 0
                },
                {
                    "sent": "Laying the second party is all over entries in X2.",
                    "label": 0
                },
                {
                    "sent": "We have two entries.",
                    "label": 1
                },
                {
                    "sent": "And as I drive, which connects between X1X2, these are the least of the penalty.",
                    "label": 0
                },
                {
                    "sent": "So it's it's a bipartite graph.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So unfortunately the generalized impedance other problem it's also and be hard for general graph.",
                    "label": 0
                },
                {
                    "sent": "So this unfortunate however.",
                    "label": 0
                },
                {
                    "sent": "For special case, will a graph is bipartite graph?",
                    "label": 0
                },
                {
                    "sent": "It can be solved in polynomial time, so that's nice.",
                    "label": 1
                },
                {
                    "sent": "And more importantly, there's a recent list paper in 1997, so if the graph is bipartite graph learn jazz essentially can be solved by finding the maximum flow, so which can be, which has a cubic time complexity and our results show this if we compute the GIS by maximum flow, this much more efficient.",
                    "label": 0
                },
                {
                    "sent": "Learn linear programming by using simplex.",
                    "label": 0
                },
                {
                    "sent": "If the problem size is large.",
                    "label": 0
                },
                {
                    "sent": "So now we have two solutions.",
                    "label": 0
                },
                {
                    "sent": "Still a linear program.",
                    "label": 0
                },
                {
                    "sent": "Relaxation of IO P2.",
                    "label": 0
                },
                {
                    "sent": "All this maximum flow and we choose the second one because this one is more more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next few slides I will show some results.",
                    "label": 0
                },
                {
                    "sent": "So the first part is to, uh, verify our theoretical results, verify the album.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have several test cases.",
                    "label": 0
                },
                {
                    "sent": "The green Line shows our theoretical band, which is 2 / 1 plus minimum between one Lambda.",
                    "label": 0
                },
                {
                    "sent": "So the X axis is Lambda and the Y axis is all bound.",
                    "label": 0
                },
                {
                    "sent": "So green line.",
                    "label": 0
                },
                {
                    "sent": "And Alisa Blue line shows all the bond shoulder approximation error of the proposed algorithm either by linear programming or by finding maximum flow.",
                    "label": 0
                },
                {
                    "sent": "And we can see in both cases the blue line is below the green line.",
                    "label": 0
                },
                {
                    "sent": "So that's nice because learning is consistent with our theoretical results.",
                    "label": 0
                },
                {
                    "sent": "So the error is bounded by this theoretical model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar following next two cases, the Blue line is always below green line and in these two cases went on a size large learn.",
                    "label": 0
                },
                {
                    "sent": "Actually we can show we can see the bond is quite tight.",
                    "label": 0
                },
                {
                    "sent": "These two curves are valid close like in this case it's almost identical, so this shows our bond.",
                    "label": 0
                },
                {
                    "sent": "The bond with computers are very tight.",
                    "label": 0
                },
                {
                    "sent": "It may be difficult to actually.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will improve.",
                    "label": 0
                },
                {
                    "sent": "So this is running time.",
                    "label": 1
                },
                {
                    "sent": "The first part is.",
                    "label": 0
                },
                {
                    "sent": "We fix the.",
                    "label": 0
                },
                {
                    "sent": "We fixed the sample size to be 1000, an increase dimension.",
                    "label": 0
                },
                {
                    "sent": "So it's about linear time complexity and the 2nd is we fixed dimension to be 1000 and we increase the data points.",
                    "label": 0
                },
                {
                    "sent": "So it's not quite linear OK.",
                    "label": 0
                },
                {
                    "sent": "But still efficient.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's our summary.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Shoes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes only by the original application images in front of clients or compressing them.",
                    "label": 0
                },
                {
                    "sent": "Applying the discrete packets, and then I'll be handle rotation and segmentation annihilations.",
                    "label": 0
                },
                {
                    "sent": "Nokia will only be online, so for example full application we fall by 1 means or standard.",
                    "label": 0
                },
                {
                    "sent": "Same dimensions, same size, so we will do some quick assessment.",
                    "label": 0
                },
                {
                    "sent": "Big Ben log in with the translation location.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK, see the tricky part of this division is the way we convert quadratic objective to linear objective.",
                    "label": 0
                },
                {
                    "sent": "I think this only works 11 entries are binding 01 if we generalize to buy a general problem might be hard.",
                    "label": 0
                },
                {
                    "sent": "OK, so this simple trick but works perfectly well for binding problem 01.",
                    "label": 0
                },
                {
                    "sent": "OK, so for general case I think it may be difficult.",
                    "label": 0
                },
                {
                    "sent": "Might be difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}