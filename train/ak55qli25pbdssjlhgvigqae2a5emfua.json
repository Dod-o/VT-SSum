{
    "id": "ak55qli25pbdssjlhgvigqae2a5emfua",
    "title": "Gradient Weights help Nonparametric Regressors",
    "info": {
        "author": [
            "Samory Kpotufe, Department of Operations Research and Financial Engineering, Princeton University"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_kpotufe_regressors/",
    "segmentation": [
        [
            "I'll be presenting a.",
            "At this work on nonparametric regression, joint work with Absalom Bull areas from the Max Planck Institute.",
            "So we'll be presenting a simple approach to nonparametric regression, and it's a preprocessing approach that would, essentially that we can essentially apply to any non parametric regressor and help improve performance.",
            "So the setup of nonparametric regression is as follows.",
            "We have, we assume that Y is equal to F of X plus noise, where F the unknown function F belongs to some infinite dimensional class and by infinite dimensional class or we mean is that we don't make too many assumptions about F. We might for instance just assume that F is smooth."
        ],
        [
            "For instance, F is Lipschitz continuous.",
            "The goal here is to estimate F at a point X using random sample of XX&Y pairs of size."
        ],
        [
            "Dot N. So a common way to do nonparametric regression is local regression.",
            "It can also be called distance based regression and the idea here is to simply average the Y values using away away in function W and wait for the weighting function.",
            "W depends on the metric on the metric Ro Ann.",
            "Usually the metric rose the Euclidean distance in Rd.",
            "Here I'm assuming that the data comes from Euclidean Space, Rd of the dimension.",
            "So some examples, K nearest neighbor regression, kernel regression or local polynomial regression?",
            "So again, I will."
        ],
        [
            "Will present a simple way to significantly significantly improve the performance of local local regression."
        ],
        [
            "So the motivation behind the approach is as follows.",
            "The unknown function F often varies more in some coordinates and in other coordinates, and so will try to take advantage of this.",
            "So let FI prime be the derivative of F along coordinate I.",
            "Will be interested in the norm of a 5 prime the L1 norm of FI prime an essentially the L1 on my 5."
        ],
        [
            "Prime captures how F varies on average along coordinate I.",
            "So in practice, we can expect that this."
        ],
        [
            "Gradient norms will differ across coordinates.",
            "And, uh."
        ],
        [
            "Gradient weighting is what?"
        ],
        [
            "We call the approach and it's simple.",
            "We just re weight each core didn't I?",
            "According to the unknown gradient Norm FI prime?",
            "Elder 5 prime.",
            "In other words, we are changing the usual Euclidean metric to a simple metric row that is of Mahalanobis form, and where the?",
            "Where the diagonal elements of WI essentially the gradient, the norms of the of the coordinate wise derivatives."
        ],
        [
            "So now that we have transformed the space, we just run any local regressor FN on the new metric SpaceX row."
        ],
        [
            "So again, gradient weighting is simply doing regression on the new metric."
        ],
        [
            "Throw where row depends on the gradient norms.",
            "If I primes along all coordinates.",
            "So this is a bit similar to metric learning, but it."
        ],
        [
            "It's cheaper to estimate essentially, in metric learning you usually search over a space of all possible of many possible metrics, and you search for the best metric over space of possible metrics.",
            "Here all we do is we estimate a single metric metric rho.",
            "So it is also so."
        ],
        [
            "Miller to feature selection, but it works more generally in feature selection.",
            "Usually you assume that some features are almost irrelevant here.",
            "We see significant gains even when F depends on all features.",
            "Also, if F if all features are equally relevant, we actually don't see much loss in the.",
            "The experiments that we have."
        ],
        [
            "So just a bit of here.",
            "I'm going to explain a little bit why the intuition behind why it works so generally the performance of regression and nonparametric regression depends on the variance of of the regressor and all the bias of the regressor.",
            "So here let's assume that all the weights, all the gradient weights are not the same, that some gradient weights are much higher than some others.",
            "Then what we'll see."
        ],
        [
            "Is that the variance of regression decreases in the new SpaceX row while the bias remains relatively unaffected?",
            "So generally."
        ],
        [
            "It's well known that the variance is controlled by the mass of balls in the in the metric space, so.",
            "So this is fairly well known and what will show what we do?",
            "What we show in the paper is that the mass of balls is higher in the new metric space than in the original space.",
            "So in that sense, the variance of a regression will decrease in this new space."
        ],
        [
            "The bias of nonparametric regression is generally controlled by the smoothness of the unknown function F in the space in which the regressor is being run.",
            "And what we can show is that the smoothness properties of F in the new space extra remain relatively unaffected.",
            "So now how do?"
        ],
        [
            "Estimated these gradient weights because the gradient weights are.",
            "Essentially, on average, we don't need to estimate the gradients or the derivatives exactly.",
            "At any point, we just need to estimate it well on average.",
            "So what we do is we start with an initial rough estimate."
        ],
        [
            "It's of F and I call it F&H here an in in the paper.",
            "What we use is a kernel regressor where H here is the bandwidth of the kernel regressor.",
            "So the way we estimate the WI is as follows.",
            "We essentially just take differences of FNH at every sample point X.",
            "So we pick a sample point X an.",
            "We look along one coordinate and we take differences of F&H around X along the coordinate an we average over this.",
            "So Ian, here is the expectation is the empirical expectation.",
            "And the indicator function here is essentially a way to reject some sample points if we are not confident in the estimate at that particular sample point."
        ],
        [
            "So this is a fast to estimate and I can be estimated online because it's just an average on empirical average and we as you can see we only need about two.",
            "We only need to estimate of FNH at every sample point X."
        ],
        [
            "We also only have two parameters, T&HH being the bandwidth of the initial estimator and T being essentially how.",
            "How far around X we take differences of F&H?",
            "And just doing this allows us to essentially tune the whole algorithm to all the dimensions by taking advantage of the fact that the gradient weights.",
            "Are likely to be different or in all dimensions.",
            "This is also fairly."
        ],
        [
            "No, and that's what I said earlier.",
            "This preprocessing can be done for any distance based regressor.",
            "You do this.",
            "You learn this gradient weights you re weight the coordinates and then you run your favorite distance based regressor on the new SpaceX Row."
        ],
        [
            "So we can show also that Wi-Fi is a consistent estimator of this gradient norms.",
            "So we have the following theorem that essentially, so I'm just paraphrasing the theorem here.",
            "It's essentially just saying that all the general regularity conditions on MEW and MEW here is the marginal measure on X.",
            "On the general regular regularity conditions on mu an under uniform continuity assumption on the gradient of FWI converges in probability to the gradient norm.",
            "The L1 norm of FI Prime provided that she goes to zero H / T also goes to 0 and that HT don't go to 02 fast compared to 2 N. We also give a final sample bounds that."
        ],
        [
            "Give a that allows us to better understand how to tune T&H."
        ],
        [
            "So the main technical hurdle here to prove this theorem is the behavior of WI at the boundary of X.",
            "So we have a lot of boundary problems that we need to take care of in the in the analysis.",
            "So the short is."
        ],
        [
            "The talk is going to be somewhat short because essentially the approach is fairly easy and it works, so we see significant performance improvements in practice.",
            "What we picked a lot."
        ],
        [
            "That had a lot of different data sets.",
            "Maybe there are sets from robotics from material science, from agriculture, from Tele communication, from medicine and other areas, and we try this approach.",
            "So first, before I."
        ],
        [
            "Show the results of the experiment.",
            "What I'm showing here is the typical gradient weights that you see on real world data.",
            "So here on the essentially on the X axis are the dimensions and what what I'm showing here are the gradient weight calculated for every dimension, and we see that this gradient weights actually vary on average even though they are not.",
            "Almost, even though there we don't have any of 'em that almost 0, they're very well on average, and that's enough to get to see significant gains."
        ],
        [
            "So.",
            "This is the first set of results.",
            "What I'm showing here is results for kernel regression, and So what we comparing is kernel regression without gradient waiting and kernel regression with gradient weighing so the KR KR is just kernel regression NKR RO is kernel regression with gradient weighing an.",
            "Essentially every data sets that we tried.",
            "This seems to work on every data sets that we tried and you see here in bold is the method that works that worked best an if you see all day long essentially?",
            "We seem to always be doing better than without gradient weighing.",
            "In the case of one quality, it turned out that the gradient weights were almost essentially the same, and so in this case, when the green and white are essentially the same, oh, we just fall back to the Euclidean distance by default, and so we don't lose anything."
        ],
        [
            "So we see the same thing with nearest neighbor regression.",
            "And again here can any just nearest neighbor regression with algorithm weighing and can enroll is nearest neighbor regression with gradient weights an again on most of the data set we see significant improvement and for one quality same thing for in the case of telecom.",
            "Nearest neighbor regression did better and it turned out that the reason why is that nearest neighbor regression is already doing very well, and but will I'll show another in a different slide.",
            "I'll show that we eventually do better when we increase the sample size."
        ],
        [
            "So here this is the same sort of experiments, but while increasing the sample sizes and we see that for small sample sizes we always do much better as the sample size is increasing.",
            "Doing regression without gradient weights start getting also good.",
            "But we still up to about 5000 points or 7000 points in the case of Telecom, we are still doing much better."
        ],
        [
            "So in the case of nearest neighbor regression and we see the same behavior and now as I was saying earlier in the case of telecom, we start off with nearest neighbor regression doing better and then eventually when the sample size increases is large enough.",
            "Doing gradient weighing allows us to get better performance overtime.",
            "So that's a sign."
        ],
        [
            "Actually, it like I said, the talk was going to be short.",
            "The take home messages that gradient weights help in nonparametric regression.",
            "And the."
        ],
        [
            "Yes, very simple.",
            "So simple that we believe that there is a lot of room for improvement and.",
            "The approach."
        ],
        [
            "Super easy to implement and so I hope everybody tries it and that's it.",
            "Thank you.",
            "Thank you, thank you very much.",
            "OK questions, what happens if you pre normalize your features in different ways?",
            "In different ways, such as well if you stand in normalized before and then is your estimate of the gradient going to be different.",
            "So in all the experiments here, we actually standardize the data 1st and then learn the green Edwards.",
            "OK, this question is right.",
            "Yeah, are you able to show sort of faster convergence rates then then standard nonparametric regression estimators using these techniques or their conditions under which you are guaranteed to do better?",
            "OK, so so in the first part of the talk I tried to explain why gradient waiting as well an I didn't get into the math there, but in reality we actually show theoretically that the variance is expected to decrease.",
            "Generally, and that's your bias is expected to remain the same through by showing that the smooth by showing how the smoothness of the of the unknown function changes in the in the new space.",
            "So that implies for.",
            "So I guess what that implies is that in the minimax sense in a mini Mac sense, the problem is easier in the in the new metric space, and so that will essentially imply what you're asking imply that regression will be better will be easier.",
            "In this space, and so you should get better rates.",
            "At least the min Max rates will be smaller will be smaller, so that's assuming you know the metric.",
            "Because yeah, yeah yeah so so that's assuming, yeah, I agree.",
            "So that's assuming that I know the exact metric.",
            "OK, that's assuming that I know the exact gradient weight and so on.",
            "The If I knew the exact gradient wait, we can give, we can show that we will do better, but we don't know the exact exaggerating wait.",
            "So we estimated an we have a consistent estimator for it.",
            "OK. Any other questions?",
            "So I have a question.",
            "First, it's actually two question, one so to say.",
            "So first of all you could think of this of using the very same thing also just for density estimation.",
            "So as in, you know, send a parcel in Windows 10's destination where you would then have a dimension that Lt dependent rescaling.",
            "The second thing is saying the installation.",
            "Other parts.",
            "In Windows there's something called filaments trick where you basically use a crude density estimate and then adapt your kernel width and that will give you probably similar activity as what you have here for your dimension dependently scaling.",
            "So the obvious two questions are, well, first of all does what you're doing help for dense destination?",
            "Secondly, could use something like Solomon's trick to also improve the estimates for your regression, so I'm not familiar with the metric, but.",
            "Yeah, and I don't think I fully understand the question, because in this year's mission I don't.",
            "I don't really see.",
            "The benefits of the of the gradient weights the so basically the.",
            "So basically the trick is that you use that you want to use a big large kernel in any area of low density in a very narrow curled in an area of high density, and that does in a way very similar things to what you're doing.",
            "OK, so so you could.",
            "You could do something similar also in regression.",
            "So essentially what you're saying is this is similar to.",
            "To choosing the Bandwidth D bandwidth appropriately correct in some sense, yeah, so it's definitely similar to choosing the bandwidth appropriately.",
            "The one benefit we have here is that we only have two parameters.",
            "That's the first thing instead of the parameters to tune over, we only have two parameters and.",
            "Another thing is that usually the sort of approaches you have for kernel density estimation or kernel regression where you tuning the D bandwidth these approaches.",
            "Fine tuned to a particular type of regression, so here we just we have something that works that is preprocessing for any type of regression you want to do afterwards.",
            "So it's more general investments.",
            "Yeah, yes, it's very elegant work.",
            "So thank you very much again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll be presenting a.",
                    "label": 0
                },
                {
                    "sent": "At this work on nonparametric regression, joint work with Absalom Bull areas from the Max Planck Institute.",
                    "label": 1
                },
                {
                    "sent": "So we'll be presenting a simple approach to nonparametric regression, and it's a preprocessing approach that would, essentially that we can essentially apply to any non parametric regressor and help improve performance.",
                    "label": 0
                },
                {
                    "sent": "So the setup of nonparametric regression is as follows.",
                    "label": 0
                },
                {
                    "sent": "We have, we assume that Y is equal to F of X plus noise, where F the unknown function F belongs to some infinite dimensional class and by infinite dimensional class or we mean is that we don't make too many assumptions about F. We might for instance just assume that F is smooth.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance, F is Lipschitz continuous.",
                    "label": 0
                },
                {
                    "sent": "The goal here is to estimate F at a point X using random sample of XX&Y pairs of size.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dot N. So a common way to do nonparametric regression is local regression.",
                    "label": 1
                },
                {
                    "sent": "It can also be called distance based regression and the idea here is to simply average the Y values using away away in function W and wait for the weighting function.",
                    "label": 1
                },
                {
                    "sent": "W depends on the metric on the metric Ro Ann.",
                    "label": 1
                },
                {
                    "sent": "Usually the metric rose the Euclidean distance in Rd.",
                    "label": 1
                },
                {
                    "sent": "Here I'm assuming that the data comes from Euclidean Space, Rd of the dimension.",
                    "label": 0
                },
                {
                    "sent": "So some examples, K nearest neighbor regression, kernel regression or local polynomial regression?",
                    "label": 0
                },
                {
                    "sent": "So again, I will.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will present a simple way to significantly significantly improve the performance of local local regression.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the motivation behind the approach is as follows.",
                    "label": 1
                },
                {
                    "sent": "The unknown function F often varies more in some coordinates and in other coordinates, and so will try to take advantage of this.",
                    "label": 1
                },
                {
                    "sent": "So let FI prime be the derivative of F along coordinate I.",
                    "label": 0
                },
                {
                    "sent": "Will be interested in the norm of a 5 prime the L1 norm of FI prime an essentially the L1 on my 5.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prime captures how F varies on average along coordinate I.",
                    "label": 0
                },
                {
                    "sent": "So in practice, we can expect that this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient norms will differ across coordinates.",
                    "label": 0
                },
                {
                    "sent": "And, uh.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient weighting is what?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We call the approach and it's simple.",
                    "label": 0
                },
                {
                    "sent": "We just re weight each core didn't I?",
                    "label": 0
                },
                {
                    "sent": "According to the unknown gradient Norm FI prime?",
                    "label": 1
                },
                {
                    "sent": "Elder 5 prime.",
                    "label": 0
                },
                {
                    "sent": "In other words, we are changing the usual Euclidean metric to a simple metric row that is of Mahalanobis form, and where the?",
                    "label": 0
                },
                {
                    "sent": "Where the diagonal elements of WI essentially the gradient, the norms of the of the coordinate wise derivatives.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have transformed the space, we just run any local regressor FN on the new metric SpaceX row.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, gradient weighting is simply doing regression on the new metric.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Throw where row depends on the gradient norms.",
                    "label": 1
                },
                {
                    "sent": "If I primes along all coordinates.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit similar to metric learning, but it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's cheaper to estimate essentially, in metric learning you usually search over a space of all possible of many possible metrics, and you search for the best metric over space of possible metrics.",
                    "label": 1
                },
                {
                    "sent": "Here all we do is we estimate a single metric metric rho.",
                    "label": 0
                },
                {
                    "sent": "So it is also so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miller to feature selection, but it works more generally in feature selection.",
                    "label": 1
                },
                {
                    "sent": "Usually you assume that some features are almost irrelevant here.",
                    "label": 0
                },
                {
                    "sent": "We see significant gains even when F depends on all features.",
                    "label": 1
                },
                {
                    "sent": "Also, if F if all features are equally relevant, we actually don't see much loss in the.",
                    "label": 0
                },
                {
                    "sent": "The experiments that we have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a bit of here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to explain a little bit why the intuition behind why it works so generally the performance of regression and nonparametric regression depends on the variance of of the regressor and all the bias of the regressor.",
                    "label": 1
                },
                {
                    "sent": "So here let's assume that all the weights, all the gradient weights are not the same, that some gradient weights are much higher than some others.",
                    "label": 0
                },
                {
                    "sent": "Then what we'll see.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the variance of regression decreases in the new SpaceX row while the bias remains relatively unaffected?",
                    "label": 0
                },
                {
                    "sent": "So generally.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's well known that the variance is controlled by the mass of balls in the in the metric space, so.",
                    "label": 1
                },
                {
                    "sent": "So this is fairly well known and what will show what we do?",
                    "label": 0
                },
                {
                    "sent": "What we show in the paper is that the mass of balls is higher in the new metric space than in the original space.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, the variance of a regression will decrease in this new space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bias of nonparametric regression is generally controlled by the smoothness of the unknown function F in the space in which the regressor is being run.",
                    "label": 1
                },
                {
                    "sent": "And what we can show is that the smoothness properties of F in the new space extra remain relatively unaffected.",
                    "label": 1
                },
                {
                    "sent": "So now how do?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimated these gradient weights because the gradient weights are.",
                    "label": 0
                },
                {
                    "sent": "Essentially, on average, we don't need to estimate the gradients or the derivatives exactly.",
                    "label": 0
                },
                {
                    "sent": "At any point, we just need to estimate it well on average.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we start with an initial rough estimate.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's of F and I call it F&H here an in in the paper.",
                    "label": 0
                },
                {
                    "sent": "What we use is a kernel regressor where H here is the bandwidth of the kernel regressor.",
                    "label": 0
                },
                {
                    "sent": "So the way we estimate the WI is as follows.",
                    "label": 0
                },
                {
                    "sent": "We essentially just take differences of FNH at every sample point X.",
                    "label": 1
                },
                {
                    "sent": "So we pick a sample point X an.",
                    "label": 0
                },
                {
                    "sent": "We look along one coordinate and we take differences of F&H around X along the coordinate an we average over this.",
                    "label": 0
                },
                {
                    "sent": "So Ian, here is the expectation is the empirical expectation.",
                    "label": 0
                },
                {
                    "sent": "And the indicator function here is essentially a way to reject some sample points if we are not confident in the estimate at that particular sample point.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a fast to estimate and I can be estimated online because it's just an average on empirical average and we as you can see we only need about two.",
                    "label": 0
                },
                {
                    "sent": "We only need to estimate of FNH at every sample point X.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also only have two parameters, T&HH being the bandwidth of the initial estimator and T being essentially how.",
                    "label": 0
                },
                {
                    "sent": "How far around X we take differences of F&H?",
                    "label": 0
                },
                {
                    "sent": "And just doing this allows us to essentially tune the whole algorithm to all the dimensions by taking advantage of the fact that the gradient weights.",
                    "label": 0
                },
                {
                    "sent": "Are likely to be different or in all dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is also fairly.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, and that's what I said earlier.",
                    "label": 0
                },
                {
                    "sent": "This preprocessing can be done for any distance based regressor.",
                    "label": 0
                },
                {
                    "sent": "You do this.",
                    "label": 0
                },
                {
                    "sent": "You learn this gradient weights you re weight the coordinates and then you run your favorite distance based regressor on the new SpaceX Row.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can show also that Wi-Fi is a consistent estimator of this gradient norms.",
                    "label": 0
                },
                {
                    "sent": "So we have the following theorem that essentially, so I'm just paraphrasing the theorem here.",
                    "label": 0
                },
                {
                    "sent": "It's essentially just saying that all the general regularity conditions on MEW and MEW here is the marginal measure on X.",
                    "label": 1
                },
                {
                    "sent": "On the general regular regularity conditions on mu an under uniform continuity assumption on the gradient of FWI converges in probability to the gradient norm.",
                    "label": 0
                },
                {
                    "sent": "The L1 norm of FI Prime provided that she goes to zero H / T also goes to 0 and that HT don't go to 02 fast compared to 2 N. We also give a final sample bounds that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give a that allows us to better understand how to tune T&H.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main technical hurdle here to prove this theorem is the behavior of WI at the boundary of X.",
                    "label": 1
                },
                {
                    "sent": "So we have a lot of boundary problems that we need to take care of in the in the analysis.",
                    "label": 0
                },
                {
                    "sent": "So the short is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The talk is going to be somewhat short because essentially the approach is fairly easy and it works, so we see significant performance improvements in practice.",
                    "label": 0
                },
                {
                    "sent": "What we picked a lot.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That had a lot of different data sets.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are sets from robotics from material science, from agriculture, from Tele communication, from medicine and other areas, and we try this approach.",
                    "label": 1
                },
                {
                    "sent": "So first, before I.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show the results of the experiment.",
                    "label": 0
                },
                {
                    "sent": "What I'm showing here is the typical gradient weights that you see on real world data.",
                    "label": 0
                },
                {
                    "sent": "So here on the essentially on the X axis are the dimensions and what what I'm showing here are the gradient weight calculated for every dimension, and we see that this gradient weights actually vary on average even though they are not.",
                    "label": 0
                },
                {
                    "sent": "Almost, even though there we don't have any of 'em that almost 0, they're very well on average, and that's enough to get to see significant gains.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the first set of results.",
                    "label": 0
                },
                {
                    "sent": "What I'm showing here is results for kernel regression, and So what we comparing is kernel regression without gradient waiting and kernel regression with gradient weighing so the KR KR is just kernel regression NKR RO is kernel regression with gradient weighing an.",
                    "label": 0
                },
                {
                    "sent": "Essentially every data sets that we tried.",
                    "label": 0
                },
                {
                    "sent": "This seems to work on every data sets that we tried and you see here in bold is the method that works that worked best an if you see all day long essentially?",
                    "label": 0
                },
                {
                    "sent": "We seem to always be doing better than without gradient weighing.",
                    "label": 0
                },
                {
                    "sent": "In the case of one quality, it turned out that the gradient weights were almost essentially the same, and so in this case, when the green and white are essentially the same, oh, we just fall back to the Euclidean distance by default, and so we don't lose anything.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see the same thing with nearest neighbor regression.",
                    "label": 0
                },
                {
                    "sent": "And again here can any just nearest neighbor regression with algorithm weighing and can enroll is nearest neighbor regression with gradient weights an again on most of the data set we see significant improvement and for one quality same thing for in the case of telecom.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor regression did better and it turned out that the reason why is that nearest neighbor regression is already doing very well, and but will I'll show another in a different slide.",
                    "label": 0
                },
                {
                    "sent": "I'll show that we eventually do better when we increase the sample size.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here this is the same sort of experiments, but while increasing the sample sizes and we see that for small sample sizes we always do much better as the sample size is increasing.",
                    "label": 0
                },
                {
                    "sent": "Doing regression without gradient weights start getting also good.",
                    "label": 0
                },
                {
                    "sent": "But we still up to about 5000 points or 7000 points in the case of Telecom, we are still doing much better.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the case of nearest neighbor regression and we see the same behavior and now as I was saying earlier in the case of telecom, we start off with nearest neighbor regression doing better and then eventually when the sample size increases is large enough.",
                    "label": 0
                },
                {
                    "sent": "Doing gradient weighing allows us to get better performance overtime.",
                    "label": 0
                },
                {
                    "sent": "So that's a sign.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, it like I said, the talk was going to be short.",
                    "label": 0
                },
                {
                    "sent": "The take home messages that gradient weights help in nonparametric regression.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, very simple.",
                    "label": 0
                },
                {
                    "sent": "So simple that we believe that there is a lot of room for improvement and.",
                    "label": 1
                },
                {
                    "sent": "The approach.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Super easy to implement and so I hope everybody tries it and that's it.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "OK questions, what happens if you pre normalize your features in different ways?",
                    "label": 0
                },
                {
                    "sent": "In different ways, such as well if you stand in normalized before and then is your estimate of the gradient going to be different.",
                    "label": 0
                },
                {
                    "sent": "So in all the experiments here, we actually standardize the data 1st and then learn the green Edwards.",
                    "label": 0
                },
                {
                    "sent": "OK, this question is right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, are you able to show sort of faster convergence rates then then standard nonparametric regression estimators using these techniques or their conditions under which you are guaranteed to do better?",
                    "label": 0
                },
                {
                    "sent": "OK, so so in the first part of the talk I tried to explain why gradient waiting as well an I didn't get into the math there, but in reality we actually show theoretically that the variance is expected to decrease.",
                    "label": 0
                },
                {
                    "sent": "Generally, and that's your bias is expected to remain the same through by showing that the smooth by showing how the smoothness of the of the unknown function changes in the in the new space.",
                    "label": 0
                },
                {
                    "sent": "So that implies for.",
                    "label": 0
                },
                {
                    "sent": "So I guess what that implies is that in the minimax sense in a mini Mac sense, the problem is easier in the in the new metric space, and so that will essentially imply what you're asking imply that regression will be better will be easier.",
                    "label": 0
                },
                {
                    "sent": "In this space, and so you should get better rates.",
                    "label": 0
                },
                {
                    "sent": "At least the min Max rates will be smaller will be smaller, so that's assuming you know the metric.",
                    "label": 0
                },
                {
                    "sent": "Because yeah, yeah yeah so so that's assuming, yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "So that's assuming that I know the exact metric.",
                    "label": 0
                },
                {
                    "sent": "OK, that's assuming that I know the exact gradient weight and so on.",
                    "label": 0
                },
                {
                    "sent": "The If I knew the exact gradient wait, we can give, we can show that we will do better, but we don't know the exact exaggerating wait.",
                    "label": 0
                },
                {
                    "sent": "So we estimated an we have a consistent estimator for it.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So I have a question.",
                    "label": 0
                },
                {
                    "sent": "First, it's actually two question, one so to say.",
                    "label": 0
                },
                {
                    "sent": "So first of all you could think of this of using the very same thing also just for density estimation.",
                    "label": 0
                },
                {
                    "sent": "So as in, you know, send a parcel in Windows 10's destination where you would then have a dimension that Lt dependent rescaling.",
                    "label": 0
                },
                {
                    "sent": "The second thing is saying the installation.",
                    "label": 0
                },
                {
                    "sent": "Other parts.",
                    "label": 0
                },
                {
                    "sent": "In Windows there's something called filaments trick where you basically use a crude density estimate and then adapt your kernel width and that will give you probably similar activity as what you have here for your dimension dependently scaling.",
                    "label": 0
                },
                {
                    "sent": "So the obvious two questions are, well, first of all does what you're doing help for dense destination?",
                    "label": 0
                },
                {
                    "sent": "Secondly, could use something like Solomon's trick to also improve the estimates for your regression, so I'm not familiar with the metric, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I don't think I fully understand the question, because in this year's mission I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't really see.",
                    "label": 0
                },
                {
                    "sent": "The benefits of the of the gradient weights the so basically the.",
                    "label": 0
                },
                {
                    "sent": "So basically the trick is that you use that you want to use a big large kernel in any area of low density in a very narrow curled in an area of high density, and that does in a way very similar things to what you're doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so so you could.",
                    "label": 0
                },
                {
                    "sent": "You could do something similar also in regression.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you're saying is this is similar to.",
                    "label": 0
                },
                {
                    "sent": "To choosing the Bandwidth D bandwidth appropriately correct in some sense, yeah, so it's definitely similar to choosing the bandwidth appropriately.",
                    "label": 0
                },
                {
                    "sent": "The one benefit we have here is that we only have two parameters.",
                    "label": 0
                },
                {
                    "sent": "That's the first thing instead of the parameters to tune over, we only have two parameters and.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that usually the sort of approaches you have for kernel density estimation or kernel regression where you tuning the D bandwidth these approaches.",
                    "label": 0
                },
                {
                    "sent": "Fine tuned to a particular type of regression, so here we just we have something that works that is preprocessing for any type of regression you want to do afterwards.",
                    "label": 0
                },
                {
                    "sent": "So it's more general investments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes, it's very elegant work.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much again.",
                    "label": 0
                }
            ]
        }
    }
}