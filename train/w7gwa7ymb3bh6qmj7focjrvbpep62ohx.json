{
    "id": "w7gwa7ymb3bh6qmj7focjrvbpep62ohx",
    "title": "Bandit Convex Optimization: sqrt{T} Regret in One Dimension",
    "info": {
        "author": [
            "Tomer Koren, Technion - Israel Institute of Technology"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_koren_one_dimension/",
    "segmentation": [
        [
            "Thanks again.",
            "OK so hi everyone, this is a joint work with Sebastian Bubeck offer Dekel and Yuval Peres and."
        ],
        [
            "Even though I have made a great job, I will still show this slide, but I'll go over.",
            "I'll go over it very, very quickly.",
            "So the only difference is that the the functions in this case are convex, so we have this bandit convex optimization game.",
            "An adversary fixed fixes some sequence of convex functions from some subset K in and dimensional space, and I assume the value is there.",
            "These functions attain are bounded between zero and one.",
            "And the the game preceding rounds in each round, the player picks some point in this in this set and incurs.",
            ".16 reset and incurs a loss.",
            "FT of XT and and similarly to hide the setting he observes only this number and nothing else.",
            "OK, so here the functions are convex.",
            "And again, the goal is to minimize the expected regret.",
            "So this is the bandit convex optimization problem and our main result is the following.",
            "One dimension, namely, when N, when N is 1, there exists an algorithm for which the regret the expected regret is of order routing and I should know that there are no additional assumptions on the loss functions besides being convex.",
            "So not even for example, not even Lipschitz assumption.",
            "And this is in can't."
        ],
        [
            "Do most of their previous work in this on this problem in the last decade or so, so the only general bound that applies to general convex functions is that of the original that appeared in the original paper of Flaxman Kalliana common in 2002 thousand and five, and all of the rest of the work in the last decade imposes some additional regularity assumptions, or the loss functions, either lips Lipschitz assumption door.",
            "So oh strong convexity or smoothness and so on and so forth.",
            "So ours.",
            "Our result is kind of the first that you know settles the minimax regret for this problem, albeit in one dimension, without imposing further regularity assumptions."
        ],
        [
            "Let me just describe it.",
            "You know in few bullets the main the proof overview or the main ideas.",
            "So the first crucial idea is to treat, so not not try to find an explicit algorithm, because this appears to be very difficult.",
            "So instead of that.",
            "We use minimax duality to transform the problem to a dual problem which might be easier to deal with.",
            "So how does this do?",
            "It looks like it's it can be viewed as a beige and variant of bandit convex optimization, in which the loss functions F1 to F capital T are in fact random variables that are drawn from a known joint distribution over sequences of convex loss functions.",
            "So this distribution is adversarially chosen, but it's known to the player, so we can use it in his algorithm.",
            "And once we have a vision problem, it makes sense to try and use a variant or of Thompson sampling in order to to solve it.",
            "So that's what we do.",
            "And the basically get analysis of this algorithm relies on recent information theoretic arguments introduced by Russo and Van Roy last year and finally.",
            "But maybe most importantly, the way we use convexity in our analysis is a relies on certain new local to global property of convex functions, an that is essentially, roughly speaking, says that.",
            "Two, how does a local information that we know about certain convex function propagates to a global information about the same convex function, which is, you know, essentially kind of essential to bandit problems that we only observe local information.",
            "That's it.",
            "For more details, please come to talk with one of us or come to our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks again.",
                    "label": 0
                },
                {
                    "sent": "OK so hi everyone, this is a joint work with Sebastian Bubeck offer Dekel and Yuval Peres and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even though I have made a great job, I will still show this slide, but I'll go over.",
                    "label": 0
                },
                {
                    "sent": "I'll go over it very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "So the only difference is that the the functions in this case are convex, so we have this bandit convex optimization game.",
                    "label": 0
                },
                {
                    "sent": "An adversary fixed fixes some sequence of convex functions from some subset K in and dimensional space, and I assume the value is there.",
                    "label": 0
                },
                {
                    "sent": "These functions attain are bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And the the game preceding rounds in each round, the player picks some point in this in this set and incurs.",
                    "label": 0
                },
                {
                    "sent": ".16 reset and incurs a loss.",
                    "label": 0
                },
                {
                    "sent": "FT of XT and and similarly to hide the setting he observes only this number and nothing else.",
                    "label": 0
                },
                {
                    "sent": "OK, so here the functions are convex.",
                    "label": 0
                },
                {
                    "sent": "And again, the goal is to minimize the expected regret.",
                    "label": 0
                },
                {
                    "sent": "So this is the bandit convex optimization problem and our main result is the following.",
                    "label": 0
                },
                {
                    "sent": "One dimension, namely, when N, when N is 1, there exists an algorithm for which the regret the expected regret is of order routing and I should know that there are no additional assumptions on the loss functions besides being convex.",
                    "label": 1
                },
                {
                    "sent": "So not even for example, not even Lipschitz assumption.",
                    "label": 0
                },
                {
                    "sent": "And this is in can't.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do most of their previous work in this on this problem in the last decade or so, so the only general bound that applies to general convex functions is that of the original that appeared in the original paper of Flaxman Kalliana common in 2002 thousand and five, and all of the rest of the work in the last decade imposes some additional regularity assumptions, or the loss functions, either lips Lipschitz assumption door.",
                    "label": 0
                },
                {
                    "sent": "So oh strong convexity or smoothness and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So ours.",
                    "label": 0
                },
                {
                    "sent": "Our result is kind of the first that you know settles the minimax regret for this problem, albeit in one dimension, without imposing further regularity assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just describe it.",
                    "label": 0
                },
                {
                    "sent": "You know in few bullets the main the proof overview or the main ideas.",
                    "label": 0
                },
                {
                    "sent": "So the first crucial idea is to treat, so not not try to find an explicit algorithm, because this appears to be very difficult.",
                    "label": 0
                },
                {
                    "sent": "So instead of that.",
                    "label": 0
                },
                {
                    "sent": "We use minimax duality to transform the problem to a dual problem which might be easier to deal with.",
                    "label": 0
                },
                {
                    "sent": "So how does this do?",
                    "label": 0
                },
                {
                    "sent": "It looks like it's it can be viewed as a beige and variant of bandit convex optimization, in which the loss functions F1 to F capital T are in fact random variables that are drawn from a known joint distribution over sequences of convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "So this distribution is adversarially chosen, but it's known to the player, so we can use it in his algorithm.",
                    "label": 1
                },
                {
                    "sent": "And once we have a vision problem, it makes sense to try and use a variant or of Thompson sampling in order to to solve it.",
                    "label": 0
                },
                {
                    "sent": "So that's what we do.",
                    "label": 0
                },
                {
                    "sent": "And the basically get analysis of this algorithm relies on recent information theoretic arguments introduced by Russo and Van Roy last year and finally.",
                    "label": 0
                },
                {
                    "sent": "But maybe most importantly, the way we use convexity in our analysis is a relies on certain new local to global property of convex functions, an that is essentially, roughly speaking, says that.",
                    "label": 0
                },
                {
                    "sent": "Two, how does a local information that we know about certain convex function propagates to a global information about the same convex function, which is, you know, essentially kind of essential to bandit problems that we only observe local information.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "For more details, please come to talk with one of us or come to our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}