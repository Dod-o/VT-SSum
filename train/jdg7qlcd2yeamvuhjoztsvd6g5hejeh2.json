{
    "id": "jdg7qlcd2yeamvuhjoztsvd6g5hejeh2",
    "title": "Bolasso: Model Consistent Lasso Estimation through the Bootstrap",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/icml08_bach_bmcle/",
    "segmentation": [
        [
            "So I've used in the previous talk, so that's often used for variable selection and recent work has shown that it's not bother consistent in the sense that the variables that is going to select won't be the correct variables.",
            "So we're going to put in.",
            "Today is a very simple way to make that consistent, and this is the following.",
            "Instead of running one last, so estimation on your full data just simple many times, run many last estimation, intersect all the support cells and boom, you're done.",
            "Your mother consistent, so it's a very simple algorithm and the goal of this talk is simply to show you why you should do it.",
            "And whether it works or not."
        ],
        [
            "So I will review first aseptic properties of the soul with the current known visit any results.",
            "Then I present the bootstrap solution and then I already."
        ],
        [
            "Simple simulations.",
            "So the goal is to predict response.",
            "Why, given AP covariates exposed to XP and we only assume we only consider linear predictors defined by W, where W is vector in RP and it will assume that you have ID data SII for, while going to one from that when there are given in the usual matrices by why is vector of size or the labels whether responses and X is a design matrix which is random in my setting an I use a square loss function, the son of the squared error between why I as in the.",
            "And the estimate which is can be nicely written as a in matrix formulation and the lasso is simply L1 regularised estimator of the value where you simply add the square loss plus The Reg."
        ],
        [
            "Position term like that.",
            "And as you know, it does lead to sparsity, and this has led to many, many papers on trying to do this more efficiently trained to evaluate on many on many examples and trying to trying also to look at extensions like group, Lasso or Trace norm of multiple kernel learning.",
            "But again to talk today about is trying to see whether it works or not and try to improve on the problems of the lasso.",
            "So we have to do some."
        ],
        [
            "Sentences, so let's put the usual setup in statistics.",
            "We assume that the data are generated from the model, so why is assumed to be a linear function of X Anna plus some noise, and we consider the minimizer W head of the lasso and we lacked end.",
            "The number of solutions tend to Infinity and you want to look whether W had we tend to W board W and we have two types of three types of consistencies, so we have the regular one we want to estimate to be closed in norm to the true one.",
            "But we also have in this setting of of dress or the pattern consistency and the same consistency and the pattern consistency stability following you want your pattern of zeros of W hat to converge to the pattern of the rules of the true generating model, which will be called capital J for the end of the talk.",
            "And since we play with, will the values you also have the sign where I did find the sun as being 0 if you're zero and one if you positive and minus 1, two negative.",
            "And you can also look at the sign consistency you want the sign pattern to converge.",
            "So the correct thing and just a small note with my assumptions as soon as we have the regular consistency, those two or three equivalents.",
            "So I will come back.",
            "I will go back and forth between the two either sign consistency or pattern consistency."
        ],
        [
            "So to set up the analysis, you have to make some assumptions.",
            "So here let me be clear from the start I took.",
            "I take the simplest assumptions of the level, which is I fixed P and let North to tend to Infinity.",
            "And of course it is of interest allowed to extend that to cases by both P&N are growing growing.",
            "But this for this talk is simply a fixed P an.",
            "So assume this part in our model.",
            "Why is the linear function of W plus some noise?",
            "And we see that W is fast.",
            "We assume that the noise, essentially the noise sizer light tails.",
            "So I think Gaussian or any boundaries of things like that, and we assume she's classical in this fixed P setting, that the joint balance metrics as the data is invertible.",
            "So those are the only three assumptions that I'm considering."
        ],
        [
            "So let's go and start with the simple things.",
            "So what are the simple cases and everything will depend whether UN, which is my regularization parameter tends to zero and at what rate.",
            "So let's take the simple case when we intend to Infinity.",
            "This is pretty dumb, but let's still look at that case.",
            "In this situation, the volume will tend to 0, so you clearly non consistent and also the partner will tend to the empty set.",
            "So clearly this is a very bad case which is very simple to derive."
        ],
        [
            "And you can look at the similar situation when UN is attached to a constant.",
            "So just to be clear, I device occurs by N OK, so it's not exactly a receipt parameter or the Lambda parameter which is often used in learning, because often in learning you don't divide the cost by N, so just to be clear, when you tense with constant you realize too much, which means that at the end.",
            "The cost function we converse with that OK and you can easily see that the minimum of that function we never BW Bible W which means that you can never be consistent in terms of norm.",
            "So whatever you do, you always end up adding value but which is kind of funny.",
            "Sometimes it turns out that the limit, the minimum that guy as the correct sign pattern just just look.",
            "So sometimes you can have some consistency but not regular consistency.",
            "And what we look at.",
            "But I look at this paper is that all situation when you want regular consistency?",
            "You don't want the sun pattern is only interesting once you have good consistency, which means that you want new end to tend to 0."
        ],
        [
            "As you also simple cases when you intend to zero very fast, essentially the sparsity inducing effect or the loss of the L1 norm doesn't appear, which means that at the end you have all variables.",
            "OK, so you're consistent as soon as you intend to 0, because I have finite P, you always consistent in terms of norm, but in that case when you intend to zero too fast, it's totally useless in terms of sparsity and so.",
            "The goal now will be to look weather what's happening when you intend to zero it."
        ],
        [
            "Store and this has led to many, like many papers on that similar topic.",
            "So here you assume that new intense 0 faster than end to the minus 1/2.",
            "Of course, this new intends to 0.",
            "Then you have regular consistency.",
            "W had converges with W bar and you can prove it that setting that aside pattern will always converge to the same, the same pattern whatever UN is, as long as we intend to zero slowly enough, you will convert the side pattern of this many of the minimum of that optimization problem so.",
            "So whatever you and is, you can do that, and so it means that in this pattern of that guy is a correct one.",
            "You're consistent in terms of sign, and if the patterns that guy is not the correct one, you're not consistent in terms of sign as anybody could do is you could take that optimization problem and try to do this to see when you have exactly the correct pattern.",
            "And if you do that you will actually get to necessary and sufficient condition.",
            "That means there has been found by many authors in the last two or three years, which is the following.",
            "Those term essentially bounded away from zero and bounded, so they don't really count, and everything boils down to the covariance between soj compliment is all the variables that you should not take at the end, and J or the variable that you should take at the end.",
            "So this says that if you have a lot.",
            "You said only a little covariance between the 11 variables and the relevant variables.",
            "You will be consistent, but if you have a lot of correlation then you won't be consistent and I tend to believe that in practice we only look at cases where we have a lot of correlations and for me this is."
        ],
        [
            "Sorry what I call a disappointing result like you've been doing the last so for 10 years and in fact it doesn't work.",
            "OK, so this has led to a lot of work trying to make to make this work so we have worked up the type of putting weights in front of the.",
            "Or the absolute values like adaptive lasso type of thing.",
            "In this paper we tried to look at a different thing and we like to look exactly as a case that was left out by everybody, which is a case where you intend to zero exactly at the right and minus 1/2.",
            "So to remind you when you end tends to zero too fast, you have no profit, no sparsity when you intend to zero too slow, you have a fixed pattern, which might or might not be the correct one."
        ],
        [
            "And then when you enter exactly at Ray 10 -- 1/2, which means that the limit of L -- N. End the power 1/2 times Milan is.",
            "Constant which is non zero.",
            "Then you have consistency regular constantly because you are you tend to zero of course, but then the cool thing that all patterns which are consistent with you on your active set on you said available variables will be selected and all the other ones will or will not with positive probability.",
            "So then what you end up doing the good ones are stable and the bad words by one socially unstable."
        ],
        [
            "This can be precisely are shown.",
            "So those two propositions two propositions.",
            "So the first one state that if you consider assign pattern which is non consistent on the setter relevant variables, then the probability that you select it will converge to zero exponentially fast, which is exactly what you want.",
            "The good the good ones are stable on the opposite.",
            "As soon as you are consistent on the set of active relevant variables, then the probability of selecting the sign will converge to a constant.",
            "OK, as in Destiny Infinity and the good thing is that the constant is always between zero and one is never zero and it also never one."
        ],
        [
            "So to summarize, to summarize this behavior, so repeat again.",
            "You take all the good ones with positive with a high probability and all the bad ones with finite but non zero and non probability.",
            "So this leads to the simple."
        ],
        [
            "Argument if you had several data sets, let's say you're lucky you're being given a lot of data sets with the same distribution, then what you could do is to run the lasso.",
            "You could run the lasso on on data set 12345, and if we assume that this will be the active set which are represented here if you assume.",
            "That's the first viable for the other other relevant one.",
            "Then, as you run as well as the multiple times, you always get the good ones with high probability and all the other ones will come in and out of the model.",
            "Depending on the specific specific data set and if you just intersect everybody.",
            "If you have enough data sets, you will remove all of those and keep all of those, which is exactly what you want by intersecting the result of the last.",
            "So for multiple data sets you get consistent model estimation.",
            "The trick is we don't have a lot of data sets.",
            "OK, they've been given one data set and you don't have the opportunity of having new ones, and this is where the."
        ],
        [
            "Bootstraps, bootstrap come in, or the bootstrap essentially got the way I see.",
            "It is simply a way to mimic the fact to mimic the situation where you had multiple data sets.",
            "OK, if you want to study the additive behavior of an estimator, then nothing to do with the bootstrap and this actually say that.",
            "What happens if I hadn't set a new data set?",
            "And by doing multiple bootstrap samples you essentially mimic this.",
            "So how does it work?",
            "You're given here NID observations.",
            "OK, like we have another divine.",
            "M's, I could aim Booster application so that is 1 booster application just one that asset of any samples and how do you get those samples?",
            "We just select randomly from your original data, 1 sample and all of them with replacement.",
            "OK so if you select without replacement and samples from North data points without replacement, we just do a copy of your data set and then anything about the bootstrapped is that you use sampling with replacement to at the end, but you end up with will be a copy of the of the data.",
            "But with some data points which I never selected, some which are cited once and some which are selected many times, OK, you do that M times.",
            "So at the end you start from one big data set and you get M other big data sets.",
            "And as I told you earlier, it is a standard way of mimicking mimicking the availability of several data sets.",
            "So the goal is."
        ],
        [
            "This is pretty simple.",
            "So you just compute compute sample.",
            "Umm, bootstrap replications.",
            "Ann runs the lasso and times.",
            "And as I told you, you just compute the the set affective variables except all of them and you hope to get something like that.",
            "And of course, once you have your final data set, you just do regular estimation using least square together W OK.",
            "So here you could, you could play tricks on the application of the law.",
            "So because once you have applied the less over the first bootstrap data set, then you know that you should never select those one of those one again, because at the end you intersect.",
            "So you could use it.",
            "You can reduce the size of the following problems, but in the paper we just.",
            "I'm just playing M times in a in product.",
            "So you see, the algorithm is very simple, you just apply M times the same thing and it becomes can."
        ],
        [
            "As shown in the next slide, so he assumed that new end tends to zero at the correct rate and what he showed that the probability of not selecting the good active set will tell you 10 to 0 with that rate.",
            "So let me first emphasize the fact that this does not depend on any type of love, consistency, condition.",
            "OK, so for us, so this only works if you have this low correlation setting between relevant and irrelevant variables.",
            "Here it is always true under arm.",
            "Assumptions of course.",
            "So it's like look at the M&N is that in that in the bound?",
            "So this will tend to 0 is good as N tends to Infinity.",
            "This as well.",
            "And let's look at the number M. So M is the number of bootstrap replicates.",
            "So M 310 to Infinity to zero an here M should 10 to 15 but not too fast.",
            "OK in fact this bound will give you at least one way or set of setting up MM shoot should increase with North but less than less than exponentially fast OK. And if you look even more closely.",
            "If you believe the bound we say you shouldn't do.",
            "But let's say you believe the band that the band is tight.",
            "OK, the bond is true, but it is surely not tight as all bounds.",
            "But if you believe the bound then.",
            "Just enough to take M being end to the 1/2 OK because all you will be limited by that time anyway.",
            "OK, so let's say for the weather to take N = N to the minus 1/2.",
            "So this proved that this procedure of just taking an boost replicates and just intercepting the active sets will be."
        ],
        [
            "Assistant.",
            "So let's now see how it works.",
            "So here just took very simple setting where I have 16 variables in eight of them, eight of them are active, the first 8 ones and the other ones are irrelevant.",
            "That is the goal here.",
            "I plot when you varies, I plotted probability of selecting that given variable for the lasso and for the bulusu and the goal here is to be white on the top and black on the bottom.",
            "Or why it means that you have selected variables and black means that you haven't selected them.",
            "So we consider 2 cases when the consistency condition of the lasso is satisfied or not satisfied.",
            "And of course, when it is satisfied the Lasso will indeed select the correct pattern with positive probability.",
            "Then we split it into one super black, which means that you have a lot of white.",
            "Here a lot of black there and if possible so also does it and you can see which is not proved in the bound, but can be proven that we will also be increase the consistency between of the lasso in this favorable favorable case.",
            "But the nice thing is.",
            "When the condition is not satisfied.",
            "So here you can see that.",
            "So the so we never be consistent.",
            "OK, there's no point for which you are all white on the top and or black on the bottom, whereas for the bull's so we have this region, so the bulusu essentially created consistency vision for the formula.",
            "So so this is a simple illustration of the consistency result, whatever this is, satisfied or not, you will always get some model consistent estimates."
        ],
        [
            "Let's let's now look at the influence of the number of boost replication.",
            "This is my end parameter, so we added a user same setting the same data, same data an in black.",
            "We have the lasso and in red you have the bowler.",
            "So with several numbers of replicates and of course when the condition of the left satisfied, the probability of selecting the correct size setting to one for the lasso, and as you add and more and more replicates possible, so you get more and more better and better estimation.",
            "This is somewhat normal.",
            "As a cool thing is, when this is not satisfying.",
            "So the last sub contracted with some probability.",
            "This is consistent with my first analysis.",
            "You always get all patterns with final probability, so it's but less than one, and as you add more and more replicates, so we tend to increase probability up to one an.",
            "If you were to make M tend to Infinity, it would decrease again.",
            "OK, because you might get unlucky and select.",
            "So you always get the good ones with high probability if you're unlucky.",
            "If you miss one, you miss one forever because intersect OK.",
            "So if M is too big it won't work, but I couldn't have big enough on my laptop to do that."
        ],
        [
            "So now I think this is a another, let's say compared to other methods to do do variable selection.",
            "So you have a.",
            "So compared with two of them, or four of them, so all of those are essentially no one free parameter to select.",
            "No free parameters, select a given number of variables.",
            "So give, give give those methods the correct number of viable, and I see what they do, and I compare the selection error and you can see.",
            "So here I compare the last soldier Bledsoe adaptive lasso of zoo.",
            "And just the special dead thresholded least core estimate and a greedy algorithm to select the lasso to select the pattern, and you can see, as expected, that the Bola, so will do better than the lasso, but also those better than all the other ones and effectively, so doesn't seem to be so much better than all the other one, so the left.",
            "So is this setting.",
            "There also is no better than the thresholded list for estimate, which is kind of super disappointing, and this is subject of ongoing work.",
            "20 design when trying to decide whether less so is better than just thresholding the estimates, which is quite basic and simple.",
            "So."
        ],
        [
            "And loss."
        ],
        [
            "Stimulation.",
            "What happens when you don't assume sparsity?",
            "So here you have to.",
            "You have to.",
            "You have to keep in mind that I've took.",
            "I've taken examples where knew the generating data was passed.",
            "So you want to know what's happened, what's happening with the data are not necessarily sparse, so I took random datasets from the UCI repository and I compare like Ridge regression which is the L2 penalization solar.",
            "So enjoyable also.",
            "And you can see that.",
            "And how that so sometimes will perform very badly OK?",
            "And my interpretation of that is that it is far too aggressive in addressing intercepting the models.",
            "So what I said was that design is just bull.",
            "Also with an S, which we simply a subthreshold soft selection of the of the variables for.",
            "You just vote or just keep the ones which are in 90% of the bootstrap replicates automatically.",
            "Does it does better.",
            "And this is not the last crisis quite quite quite yet, but this is simply to show that it does not always work OK. Well the model is not sparse.",
            "In my computer.",
            "Fail because.",
            "Intersecting is is too aggressive and the goal will be to design something which is in between.",
            "When you only do a soft intersec."
        ],
        [
            "And.",
            "To conclude, I presented you a detailed analysis of the variable section properties of the Bootstrap lasso as the most important point is that in the case of finite there's no condition on the covariance matrix, but being invertible, and one thing which I didn't mention is that there's no additional free parameter.",
            "OK, just as the number of Bootstrap replicates, which can always take into the minus 1/2.",
            "So at the end you get better method at most for free.",
            "And so your comment extensions that we are considering all of course in the last World prefix is not very.",
            "They say not very whatever.",
            "It's not very good.",
            "It's not enough and you want to allow P2P to grow with the number that I said.",
            "Number of data points like myself and you.",
            "Also we were planning to extend that to the group lasso setting, which is somewhat trivial, at least algorithmically.",
            "Ann, I think there's another connection with other billing method here.",
            "We recent pull not to average the final predictors just to estimate the correct specific part, which is quite different from actual resampling methods.",
            "Thank you for your attention.",
            "Maybe?",
            "So one thing is bothering me a bit is we can take EM to be too large as he said, sure.",
            "Guidance theoretical about how large name is allowed here in order to get to what you have to send in.",
            "Also, if you get the bank will say that if M increases lesson exponentially fast in N, it will always work OK. As long as this tends to zero quickly, so you want these to tend to zero quicker than that, so you want them to be quicker than minus one and to the 1/2, but more is not very useful because you have exponential part over there, so this bond is very loose.",
            "OK, so this way I could be improved a lot.",
            "Different.",
            "From bagging on last so yeah so backing off so if I if I."
        ],
        [
            "Well, begging lasso is will take estimate several lasso from the from the result from the bootstrap samples, but we more average as a predictor instead of intercepting the intersecting the active sets.",
            "So the main difference is that.",
            "We don't do very good at Bootstrap where we just take the estimated average them.",
            "We just average or intersect the active sets it.",
            "Once you have good active sets to have the good W comes for free.",
            "Related variables and losses.",
            "For faster and just leave everything out.",
            "Then you take several such cases, intersect them.",
            "Sure, sure, here I think they generally doesn't hold when you have a fully correlated random variables.",
            "So here we assume that the metrics the junk metrics is invertible.",
            "So if you have tightly protectress, then the lowest I gotta do quickly, and in fact most analysis of the last.",
            "So we always assume that the covariance matrix of the active variables will be small in some sense.",
            "No thanks no.",
            "So now if you cluster exact you could you can just remove everybody so.",
            "Interesting region.",
            "Sure, yeah.",
            "OK, so so I said that there's no additional free parameter, but you still have the one for the rest, of course, and this I have.",
            "The band says that you should take him to the order of Ms. 1/2, but it is not very informative.",
            "And here I'll just do conversation right now.",
            "Now that.",
            "Outperform the rest should at shoebuy tend to be against like automatic selection of the parliamentary cause like validation just to make sure that you're not totally.",
            "Wrong, but I know it.",
            "The band will tell you, but give me a decent metrics.",
            "I don't know how to set Lambda automatically.",
            "Yes no.",
            "I have every intention that you look at the stability of the active set across the bootstrap samples and you know that it should stop when you start to be unstable.",
            "But a.",
            "But it's not really clear.",
            "So.",
            "Maybe you can integrate.",
            "No.",
            "No, yes, but no.",
            "But what I plan to do is to do like.",
            "Bootstrap the visuals, which is a common thing to do if you have a fix design, it's not.",
            "It's not allowed.",
            "We need to do a bootstrap of the X.",
            "You only do Bootstrap on the regular between Y and you predictor.",
            "This is easy to do what you mentioned.",
            "I don't know possibly, but then you will need to have gas and assumption on the many things so.",
            "Yeah.",
            "Inventor?",
            "Oh sure, if you put that fully gas and you, I guess you could do some something, but I don't know."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've used in the previous talk, so that's often used for variable selection and recent work has shown that it's not bother consistent in the sense that the variables that is going to select won't be the correct variables.",
                    "label": 0
                },
                {
                    "sent": "So we're going to put in.",
                    "label": 0
                },
                {
                    "sent": "Today is a very simple way to make that consistent, and this is the following.",
                    "label": 0
                },
                {
                    "sent": "Instead of running one last, so estimation on your full data just simple many times, run many last estimation, intersect all the support cells and boom, you're done.",
                    "label": 0
                },
                {
                    "sent": "Your mother consistent, so it's a very simple algorithm and the goal of this talk is simply to show you why you should do it.",
                    "label": 0
                },
                {
                    "sent": "And whether it works or not.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will review first aseptic properties of the soul with the current known visit any results.",
                    "label": 0
                },
                {
                    "sent": "Then I present the bootstrap solution and then I already.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple simulations.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to predict response.",
                    "label": 0
                },
                {
                    "sent": "Why, given AP covariates exposed to XP and we only assume we only consider linear predictors defined by W, where W is vector in RP and it will assume that you have ID data SII for, while going to one from that when there are given in the usual matrices by why is vector of size or the labels whether responses and X is a design matrix which is random in my setting an I use a square loss function, the son of the squared error between why I as in the.",
                    "label": 1
                },
                {
                    "sent": "And the estimate which is can be nicely written as a in matrix formulation and the lasso is simply L1 regularised estimator of the value where you simply add the square loss plus The Reg.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Position term like that.",
                    "label": 0
                },
                {
                    "sent": "And as you know, it does lead to sparsity, and this has led to many, many papers on trying to do this more efficiently trained to evaluate on many on many examples and trying to trying also to look at extensions like group, Lasso or Trace norm of multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "But again to talk today about is trying to see whether it works or not and try to improve on the problems of the lasso.",
                    "label": 0
                },
                {
                    "sent": "So we have to do some.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sentences, so let's put the usual setup in statistics.",
                    "label": 0
                },
                {
                    "sent": "We assume that the data are generated from the model, so why is assumed to be a linear function of X Anna plus some noise, and we consider the minimizer W head of the lasso and we lacked end.",
                    "label": 1
                },
                {
                    "sent": "The number of solutions tend to Infinity and you want to look whether W had we tend to W board W and we have two types of three types of consistencies, so we have the regular one we want to estimate to be closed in norm to the true one.",
                    "label": 1
                },
                {
                    "sent": "But we also have in this setting of of dress or the pattern consistency and the same consistency and the pattern consistency stability following you want your pattern of zeros of W hat to converge to the pattern of the rules of the true generating model, which will be called capital J for the end of the talk.",
                    "label": 1
                },
                {
                    "sent": "And since we play with, will the values you also have the sign where I did find the sun as being 0 if you're zero and one if you positive and minus 1, two negative.",
                    "label": 0
                },
                {
                    "sent": "And you can also look at the sign consistency you want the sign pattern to converge.",
                    "label": 0
                },
                {
                    "sent": "So the correct thing and just a small note with my assumptions as soon as we have the regular consistency, those two or three equivalents.",
                    "label": 1
                },
                {
                    "sent": "So I will come back.",
                    "label": 0
                },
                {
                    "sent": "I will go back and forth between the two either sign consistency or pattern consistency.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to set up the analysis, you have to make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "So here let me be clear from the start I took.",
                    "label": 0
                },
                {
                    "sent": "I take the simplest assumptions of the level, which is I fixed P and let North to tend to Infinity.",
                    "label": 1
                },
                {
                    "sent": "And of course it is of interest allowed to extend that to cases by both P&N are growing growing.",
                    "label": 0
                },
                {
                    "sent": "But this for this talk is simply a fixed P an.",
                    "label": 0
                },
                {
                    "sent": "So assume this part in our model.",
                    "label": 0
                },
                {
                    "sent": "Why is the linear function of W plus some noise?",
                    "label": 0
                },
                {
                    "sent": "And we see that W is fast.",
                    "label": 0
                },
                {
                    "sent": "We assume that the noise, essentially the noise sizer light tails.",
                    "label": 0
                },
                {
                    "sent": "So I think Gaussian or any boundaries of things like that, and we assume she's classical in this fixed P setting, that the joint balance metrics as the data is invertible.",
                    "label": 0
                },
                {
                    "sent": "So those are the only three assumptions that I'm considering.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's go and start with the simple things.",
                    "label": 0
                },
                {
                    "sent": "So what are the simple cases and everything will depend whether UN, which is my regularization parameter tends to zero and at what rate.",
                    "label": 1
                },
                {
                    "sent": "So let's take the simple case when we intend to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This is pretty dumb, but let's still look at that case.",
                    "label": 0
                },
                {
                    "sent": "In this situation, the volume will tend to 0, so you clearly non consistent and also the partner will tend to the empty set.",
                    "label": 0
                },
                {
                    "sent": "So clearly this is a very bad case which is very simple to derive.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can look at the similar situation when UN is attached to a constant.",
                    "label": 0
                },
                {
                    "sent": "So just to be clear, I device occurs by N OK, so it's not exactly a receipt parameter or the Lambda parameter which is often used in learning, because often in learning you don't divide the cost by N, so just to be clear, when you tense with constant you realize too much, which means that at the end.",
                    "label": 0
                },
                {
                    "sent": "The cost function we converse with that OK and you can easily see that the minimum of that function we never BW Bible W which means that you can never be consistent in terms of norm.",
                    "label": 1
                },
                {
                    "sent": "So whatever you do, you always end up adding value but which is kind of funny.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it turns out that the limit, the minimum that guy as the correct sign pattern just just look.",
                    "label": 1
                },
                {
                    "sent": "So sometimes you can have some consistency but not regular consistency.",
                    "label": 0
                },
                {
                    "sent": "And what we look at.",
                    "label": 0
                },
                {
                    "sent": "But I look at this paper is that all situation when you want regular consistency?",
                    "label": 0
                },
                {
                    "sent": "You don't want the sun pattern is only interesting once you have good consistency, which means that you want new end to tend to 0.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you also simple cases when you intend to zero very fast, essentially the sparsity inducing effect or the loss of the L1 norm doesn't appear, which means that at the end you have all variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so you're consistent as soon as you intend to 0, because I have finite P, you always consistent in terms of norm, but in that case when you intend to zero too fast, it's totally useless in terms of sparsity and so.",
                    "label": 0
                },
                {
                    "sent": "The goal now will be to look weather what's happening when you intend to zero it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Store and this has led to many, like many papers on that similar topic.",
                    "label": 0
                },
                {
                    "sent": "So here you assume that new intense 0 faster than end to the minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "Of course, this new intends to 0.",
                    "label": 0
                },
                {
                    "sent": "Then you have regular consistency.",
                    "label": 0
                },
                {
                    "sent": "W had converges with W bar and you can prove it that setting that aside pattern will always converge to the same, the same pattern whatever UN is, as long as we intend to zero slowly enough, you will convert the side pattern of this many of the minimum of that optimization problem so.",
                    "label": 1
                },
                {
                    "sent": "So whatever you and is, you can do that, and so it means that in this pattern of that guy is a correct one.",
                    "label": 0
                },
                {
                    "sent": "You're consistent in terms of sign, and if the patterns that guy is not the correct one, you're not consistent in terms of sign as anybody could do is you could take that optimization problem and try to do this to see when you have exactly the correct pattern.",
                    "label": 0
                },
                {
                    "sent": "And if you do that you will actually get to necessary and sufficient condition.",
                    "label": 1
                },
                {
                    "sent": "That means there has been found by many authors in the last two or three years, which is the following.",
                    "label": 0
                },
                {
                    "sent": "Those term essentially bounded away from zero and bounded, so they don't really count, and everything boils down to the covariance between soj compliment is all the variables that you should not take at the end, and J or the variable that you should take at the end.",
                    "label": 0
                },
                {
                    "sent": "So this says that if you have a lot.",
                    "label": 0
                },
                {
                    "sent": "You said only a little covariance between the 11 variables and the relevant variables.",
                    "label": 0
                },
                {
                    "sent": "You will be consistent, but if you have a lot of correlation then you won't be consistent and I tend to believe that in practice we only look at cases where we have a lot of correlations and for me this is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry what I call a disappointing result like you've been doing the last so for 10 years and in fact it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "OK, so this has led to a lot of work trying to make to make this work so we have worked up the type of putting weights in front of the.",
                    "label": 0
                },
                {
                    "sent": "Or the absolute values like adaptive lasso type of thing.",
                    "label": 0
                },
                {
                    "sent": "In this paper we tried to look at a different thing and we like to look exactly as a case that was left out by everybody, which is a case where you intend to zero exactly at the right and minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "So to remind you when you end tends to zero too fast, you have no profit, no sparsity when you intend to zero too slow, you have a fixed pattern, which might or might not be the correct one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then when you enter exactly at Ray 10 -- 1/2, which means that the limit of L -- N. End the power 1/2 times Milan is.",
                    "label": 0
                },
                {
                    "sent": "Constant which is non zero.",
                    "label": 0
                },
                {
                    "sent": "Then you have consistency regular constantly because you are you tend to zero of course, but then the cool thing that all patterns which are consistent with you on your active set on you said available variables will be selected and all the other ones will or will not with positive probability.",
                    "label": 1
                },
                {
                    "sent": "So then what you end up doing the good ones are stable and the bad words by one socially unstable.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This can be precisely are shown.",
                    "label": 0
                },
                {
                    "sent": "So those two propositions two propositions.",
                    "label": 0
                },
                {
                    "sent": "So the first one state that if you consider assign pattern which is non consistent on the setter relevant variables, then the probability that you select it will converge to zero exponentially fast, which is exactly what you want.",
                    "label": 0
                },
                {
                    "sent": "The good the good ones are stable on the opposite.",
                    "label": 0
                },
                {
                    "sent": "As soon as you are consistent on the set of active relevant variables, then the probability of selecting the sign will converge to a constant.",
                    "label": 0
                },
                {
                    "sent": "OK, as in Destiny Infinity and the good thing is that the constant is always between zero and one is never zero and it also never one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to summarize, to summarize this behavior, so repeat again.",
                    "label": 0
                },
                {
                    "sent": "You take all the good ones with positive with a high probability and all the bad ones with finite but non zero and non probability.",
                    "label": 0
                },
                {
                    "sent": "So this leads to the simple.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Argument if you had several data sets, let's say you're lucky you're being given a lot of data sets with the same distribution, then what you could do is to run the lasso.",
                    "label": 0
                },
                {
                    "sent": "You could run the lasso on on data set 12345, and if we assume that this will be the active set which are represented here if you assume.",
                    "label": 0
                },
                {
                    "sent": "That's the first viable for the other other relevant one.",
                    "label": 0
                },
                {
                    "sent": "Then, as you run as well as the multiple times, you always get the good ones with high probability and all the other ones will come in and out of the model.",
                    "label": 0
                },
                {
                    "sent": "Depending on the specific specific data set and if you just intersect everybody.",
                    "label": 0
                },
                {
                    "sent": "If you have enough data sets, you will remove all of those and keep all of those, which is exactly what you want by intersecting the result of the last.",
                    "label": 0
                },
                {
                    "sent": "So for multiple data sets you get consistent model estimation.",
                    "label": 0
                },
                {
                    "sent": "The trick is we don't have a lot of data sets.",
                    "label": 0
                },
                {
                    "sent": "OK, they've been given one data set and you don't have the opportunity of having new ones, and this is where the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bootstraps, bootstrap come in, or the bootstrap essentially got the way I see.",
                    "label": 0
                },
                {
                    "sent": "It is simply a way to mimic the fact to mimic the situation where you had multiple data sets.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to study the additive behavior of an estimator, then nothing to do with the bootstrap and this actually say that.",
                    "label": 0
                },
                {
                    "sent": "What happens if I hadn't set a new data set?",
                    "label": 0
                },
                {
                    "sent": "And by doing multiple bootstrap samples you essentially mimic this.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "You're given here NID observations.",
                    "label": 0
                },
                {
                    "sent": "OK, like we have another divine.",
                    "label": 0
                },
                {
                    "sent": "M's, I could aim Booster application so that is 1 booster application just one that asset of any samples and how do you get those samples?",
                    "label": 0
                },
                {
                    "sent": "We just select randomly from your original data, 1 sample and all of them with replacement.",
                    "label": 1
                },
                {
                    "sent": "OK so if you select without replacement and samples from North data points without replacement, we just do a copy of your data set and then anything about the bootstrapped is that you use sampling with replacement to at the end, but you end up with will be a copy of the of the data.",
                    "label": 0
                },
                {
                    "sent": "But with some data points which I never selected, some which are cited once and some which are selected many times, OK, you do that M times.",
                    "label": 0
                },
                {
                    "sent": "So at the end you start from one big data set and you get M other big data sets.",
                    "label": 0
                },
                {
                    "sent": "And as I told you earlier, it is a standard way of mimicking mimicking the availability of several data sets.",
                    "label": 1
                },
                {
                    "sent": "So the goal is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "So you just compute compute sample.",
                    "label": 0
                },
                {
                    "sent": "Umm, bootstrap replications.",
                    "label": 0
                },
                {
                    "sent": "Ann runs the lasso and times.",
                    "label": 0
                },
                {
                    "sent": "And as I told you, you just compute the the set affective variables except all of them and you hope to get something like that.",
                    "label": 0
                },
                {
                    "sent": "And of course, once you have your final data set, you just do regular estimation using least square together W OK.",
                    "label": 0
                },
                {
                    "sent": "So here you could, you could play tricks on the application of the law.",
                    "label": 1
                },
                {
                    "sent": "So because once you have applied the less over the first bootstrap data set, then you know that you should never select those one of those one again, because at the end you intersect.",
                    "label": 0
                },
                {
                    "sent": "So you could use it.",
                    "label": 0
                },
                {
                    "sent": "You can reduce the size of the following problems, but in the paper we just.",
                    "label": 0
                },
                {
                    "sent": "I'm just playing M times in a in product.",
                    "label": 0
                },
                {
                    "sent": "So you see, the algorithm is very simple, you just apply M times the same thing and it becomes can.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As shown in the next slide, so he assumed that new end tends to zero at the correct rate and what he showed that the probability of not selecting the good active set will tell you 10 to 0 with that rate.",
                    "label": 1
                },
                {
                    "sent": "So let me first emphasize the fact that this does not depend on any type of love, consistency, condition.",
                    "label": 0
                },
                {
                    "sent": "OK, so for us, so this only works if you have this low correlation setting between relevant and irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "Here it is always true under arm.",
                    "label": 0
                },
                {
                    "sent": "Assumptions of course.",
                    "label": 0
                },
                {
                    "sent": "So it's like look at the M&N is that in that in the bound?",
                    "label": 0
                },
                {
                    "sent": "So this will tend to 0 is good as N tends to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This as well.",
                    "label": 0
                },
                {
                    "sent": "And let's look at the number M. So M is the number of bootstrap replicates.",
                    "label": 0
                },
                {
                    "sent": "So M 310 to Infinity to zero an here M should 10 to 15 but not too fast.",
                    "label": 0
                },
                {
                    "sent": "OK in fact this bound will give you at least one way or set of setting up MM shoot should increase with North but less than less than exponentially fast OK. And if you look even more closely.",
                    "label": 0
                },
                {
                    "sent": "If you believe the bound we say you shouldn't do.",
                    "label": 0
                },
                {
                    "sent": "But let's say you believe the band that the band is tight.",
                    "label": 0
                },
                {
                    "sent": "OK, the bond is true, but it is surely not tight as all bounds.",
                    "label": 0
                },
                {
                    "sent": "But if you believe the bound then.",
                    "label": 0
                },
                {
                    "sent": "Just enough to take M being end to the 1/2 OK because all you will be limited by that time anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say for the weather to take N = N to the minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "So this proved that this procedure of just taking an boost replicates and just intercepting the active sets will be.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assistant.",
                    "label": 0
                },
                {
                    "sent": "So let's now see how it works.",
                    "label": 0
                },
                {
                    "sent": "So here just took very simple setting where I have 16 variables in eight of them, eight of them are active, the first 8 ones and the other ones are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "That is the goal here.",
                    "label": 0
                },
                {
                    "sent": "I plot when you varies, I plotted probability of selecting that given variable for the lasso and for the bulusu and the goal here is to be white on the top and black on the bottom.",
                    "label": 0
                },
                {
                    "sent": "Or why it means that you have selected variables and black means that you haven't selected them.",
                    "label": 0
                },
                {
                    "sent": "So we consider 2 cases when the consistency condition of the lasso is satisfied or not satisfied.",
                    "label": 1
                },
                {
                    "sent": "And of course, when it is satisfied the Lasso will indeed select the correct pattern with positive probability.",
                    "label": 0
                },
                {
                    "sent": "Then we split it into one super black, which means that you have a lot of white.",
                    "label": 0
                },
                {
                    "sent": "Here a lot of black there and if possible so also does it and you can see which is not proved in the bound, but can be proven that we will also be increase the consistency between of the lasso in this favorable favorable case.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing is.",
                    "label": 0
                },
                {
                    "sent": "When the condition is not satisfied.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that.",
                    "label": 0
                },
                {
                    "sent": "So the so we never be consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no point for which you are all white on the top and or black on the bottom, whereas for the bull's so we have this region, so the bulusu essentially created consistency vision for the formula.",
                    "label": 1
                },
                {
                    "sent": "So so this is a simple illustration of the consistency result, whatever this is, satisfied or not, you will always get some model consistent estimates.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's let's now look at the influence of the number of boost replication.",
                    "label": 1
                },
                {
                    "sent": "This is my end parameter, so we added a user same setting the same data, same data an in black.",
                    "label": 0
                },
                {
                    "sent": "We have the lasso and in red you have the bowler.",
                    "label": 0
                },
                {
                    "sent": "So with several numbers of replicates and of course when the condition of the left satisfied, the probability of selecting the correct size setting to one for the lasso, and as you add and more and more replicates possible, so you get more and more better and better estimation.",
                    "label": 0
                },
                {
                    "sent": "This is somewhat normal.",
                    "label": 0
                },
                {
                    "sent": "As a cool thing is, when this is not satisfying.",
                    "label": 0
                },
                {
                    "sent": "So the last sub contracted with some probability.",
                    "label": 0
                },
                {
                    "sent": "This is consistent with my first analysis.",
                    "label": 0
                },
                {
                    "sent": "You always get all patterns with final probability, so it's but less than one, and as you add more and more replicates, so we tend to increase probability up to one an.",
                    "label": 0
                },
                {
                    "sent": "If you were to make M tend to Infinity, it would decrease again.",
                    "label": 0
                },
                {
                    "sent": "OK, because you might get unlucky and select.",
                    "label": 0
                },
                {
                    "sent": "So you always get the good ones with high probability if you're unlucky.",
                    "label": 0
                },
                {
                    "sent": "If you miss one, you miss one forever because intersect OK.",
                    "label": 0
                },
                {
                    "sent": "So if M is too big it won't work, but I couldn't have big enough on my laptop to do that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I think this is a another, let's say compared to other methods to do do variable selection.",
                    "label": 1
                },
                {
                    "sent": "So you have a.",
                    "label": 0
                },
                {
                    "sent": "So compared with two of them, or four of them, so all of those are essentially no one free parameter to select.",
                    "label": 0
                },
                {
                    "sent": "No free parameters, select a given number of variables.",
                    "label": 1
                },
                {
                    "sent": "So give, give give those methods the correct number of viable, and I see what they do, and I compare the selection error and you can see.",
                    "label": 0
                },
                {
                    "sent": "So here I compare the last soldier Bledsoe adaptive lasso of zoo.",
                    "label": 1
                },
                {
                    "sent": "And just the special dead thresholded least core estimate and a greedy algorithm to select the lasso to select the pattern, and you can see, as expected, that the Bola, so will do better than the lasso, but also those better than all the other ones and effectively, so doesn't seem to be so much better than all the other one, so the left.",
                    "label": 0
                },
                {
                    "sent": "So is this setting.",
                    "label": 0
                },
                {
                    "sent": "There also is no better than the thresholded list for estimate, which is kind of super disappointing, and this is subject of ongoing work.",
                    "label": 0
                },
                {
                    "sent": "20 design when trying to decide whether less so is better than just thresholding the estimates, which is quite basic and simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And loss.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stimulation.",
                    "label": 0
                },
                {
                    "sent": "What happens when you don't assume sparsity?",
                    "label": 0
                },
                {
                    "sent": "So here you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You have to keep in mind that I've took.",
                    "label": 0
                },
                {
                    "sent": "I've taken examples where knew the generating data was passed.",
                    "label": 0
                },
                {
                    "sent": "So you want to know what's happened, what's happening with the data are not necessarily sparse, so I took random datasets from the UCI repository and I compare like Ridge regression which is the L2 penalization solar.",
                    "label": 0
                },
                {
                    "sent": "So enjoyable also.",
                    "label": 0
                },
                {
                    "sent": "And you can see that.",
                    "label": 0
                },
                {
                    "sent": "And how that so sometimes will perform very badly OK?",
                    "label": 0
                },
                {
                    "sent": "And my interpretation of that is that it is far too aggressive in addressing intercepting the models.",
                    "label": 0
                },
                {
                    "sent": "So what I said was that design is just bull.",
                    "label": 0
                },
                {
                    "sent": "Also with an S, which we simply a subthreshold soft selection of the of the variables for.",
                    "label": 0
                },
                {
                    "sent": "You just vote or just keep the ones which are in 90% of the bootstrap replicates automatically.",
                    "label": 0
                },
                {
                    "sent": "Does it does better.",
                    "label": 0
                },
                {
                    "sent": "And this is not the last crisis quite quite quite yet, but this is simply to show that it does not always work OK. Well the model is not sparse.",
                    "label": 0
                },
                {
                    "sent": "In my computer.",
                    "label": 0
                },
                {
                    "sent": "Fail because.",
                    "label": 0
                },
                {
                    "sent": "Intersecting is is too aggressive and the goal will be to design something which is in between.",
                    "label": 0
                },
                {
                    "sent": "When you only do a soft intersec.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To conclude, I presented you a detailed analysis of the variable section properties of the Bootstrap lasso as the most important point is that in the case of finite there's no condition on the covariance matrix, but being invertible, and one thing which I didn't mention is that there's no additional free parameter.",
                    "label": 1
                },
                {
                    "sent": "OK, just as the number of Bootstrap replicates, which can always take into the minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "So at the end you get better method at most for free.",
                    "label": 0
                },
                {
                    "sent": "And so your comment extensions that we are considering all of course in the last World prefix is not very.",
                    "label": 0
                },
                {
                    "sent": "They say not very whatever.",
                    "label": 0
                },
                {
                    "sent": "It's not very good.",
                    "label": 0
                },
                {
                    "sent": "It's not enough and you want to allow P2P to grow with the number that I said.",
                    "label": 0
                },
                {
                    "sent": "Number of data points like myself and you.",
                    "label": 1
                },
                {
                    "sent": "Also we were planning to extend that to the group lasso setting, which is somewhat trivial, at least algorithmically.",
                    "label": 0
                },
                {
                    "sent": "Ann, I think there's another connection with other billing method here.",
                    "label": 0
                },
                {
                    "sent": "We recent pull not to average the final predictors just to estimate the correct specific part, which is quite different from actual resampling methods.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "So one thing is bothering me a bit is we can take EM to be too large as he said, sure.",
                    "label": 0
                },
                {
                    "sent": "Guidance theoretical about how large name is allowed here in order to get to what you have to send in.",
                    "label": 0
                },
                {
                    "sent": "Also, if you get the bank will say that if M increases lesson exponentially fast in N, it will always work OK. As long as this tends to zero quickly, so you want these to tend to zero quicker than that, so you want them to be quicker than minus one and to the 1/2, but more is not very useful because you have exponential part over there, so this bond is very loose.",
                    "label": 0
                },
                {
                    "sent": "OK, so this way I could be improved a lot.",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "From bagging on last so yeah so backing off so if I if I.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, begging lasso is will take estimate several lasso from the from the result from the bootstrap samples, but we more average as a predictor instead of intercepting the intersecting the active sets.",
                    "label": 0
                },
                {
                    "sent": "So the main difference is that.",
                    "label": 0
                },
                {
                    "sent": "We don't do very good at Bootstrap where we just take the estimated average them.",
                    "label": 0
                },
                {
                    "sent": "We just average or intersect the active sets it.",
                    "label": 0
                },
                {
                    "sent": "Once you have good active sets to have the good W comes for free.",
                    "label": 0
                },
                {
                    "sent": "Related variables and losses.",
                    "label": 0
                },
                {
                    "sent": "For faster and just leave everything out.",
                    "label": 0
                },
                {
                    "sent": "Then you take several such cases, intersect them.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure, here I think they generally doesn't hold when you have a fully correlated random variables.",
                    "label": 0
                },
                {
                    "sent": "So here we assume that the metrics the junk metrics is invertible.",
                    "label": 0
                },
                {
                    "sent": "So if you have tightly protectress, then the lowest I gotta do quickly, and in fact most analysis of the last.",
                    "label": 0
                },
                {
                    "sent": "So we always assume that the covariance matrix of the active variables will be small in some sense.",
                    "label": 0
                },
                {
                    "sent": "No thanks no.",
                    "label": 0
                },
                {
                    "sent": "So now if you cluster exact you could you can just remove everybody so.",
                    "label": 0
                },
                {
                    "sent": "Interesting region.",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so so I said that there's no additional free parameter, but you still have the one for the rest, of course, and this I have.",
                    "label": 0
                },
                {
                    "sent": "The band says that you should take him to the order of Ms. 1/2, but it is not very informative.",
                    "label": 0
                },
                {
                    "sent": "And here I'll just do conversation right now.",
                    "label": 0
                },
                {
                    "sent": "Now that.",
                    "label": 0
                },
                {
                    "sent": "Outperform the rest should at shoebuy tend to be against like automatic selection of the parliamentary cause like validation just to make sure that you're not totally.",
                    "label": 1
                },
                {
                    "sent": "Wrong, but I know it.",
                    "label": 0
                },
                {
                    "sent": "The band will tell you, but give me a decent metrics.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to set Lambda automatically.",
                    "label": 0
                },
                {
                    "sent": "Yes no.",
                    "label": 0
                },
                {
                    "sent": "I have every intention that you look at the stability of the active set across the bootstrap samples and you know that it should stop when you start to be unstable.",
                    "label": 0
                },
                {
                    "sent": "But a.",
                    "label": 0
                },
                {
                    "sent": "But it's not really clear.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can integrate.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, yes, but no.",
                    "label": 0
                },
                {
                    "sent": "But what I plan to do is to do like.",
                    "label": 0
                },
                {
                    "sent": "Bootstrap the visuals, which is a common thing to do if you have a fix design, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not allowed.",
                    "label": 1
                },
                {
                    "sent": "We need to do a bootstrap of the X.",
                    "label": 0
                },
                {
                    "sent": "You only do Bootstrap on the regular between Y and you predictor.",
                    "label": 0
                },
                {
                    "sent": "This is easy to do what you mentioned.",
                    "label": 0
                },
                {
                    "sent": "I don't know possibly, but then you will need to have gas and assumption on the many things so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Inventor?",
                    "label": 0
                },
                {
                    "sent": "Oh sure, if you put that fully gas and you, I guess you could do some something, but I don't know.",
                    "label": 0
                }
            ]
        }
    }
}