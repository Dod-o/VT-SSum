{
    "id": "ilkogsbmosaebpalfwapnq3ztb67xruz",
    "title": "Variational Model Selection for Sparse Gaussian Process Regression",
    "info": {
        "author": [
            "Michalis K. Titsias, School of Mathematics, University of Manchester"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/bark08_titsias_vmsfsgpr/",
    "segmentation": [
        [
            "So I would like to thank nearly in quite in that, given the opportunity to speak around this meeting.",
            "So I'm going to speak about, as Neil said, the word variation and model selection for sparse Gaussian process regression.",
            "So this kind of very specific topic, so I'm not going to say anything about the Bayesian philosophy or."
        ],
        [
            "Things like this.",
            "So this is my outline of my talk.",
            "I will first describe briefly the problem of Gaussian process regression using Gaussian noise.",
            "And the problem we have in large data sets.",
            "And then I will discuss a method that I'm starting the last month and is related to variation inference based on reducing variables.",
            "So going through all these topics.",
            "The concept of auxiliary inducing virus, which was introduced by Snelson Ghahramani.",
            "I would compute lower bound for this kind of auxiliary validation method.",
            "I would compare it with.",
            "Currently use Magna likelihoods.",
            "I would have experiments in large datasets.",
            "Then I will discuss the case of reducing virus been being chosen from the training data.",
            "And if there is some time left, then am I so you some kind of?",
            "Variational formulation of of the current sparsity methods."
        ],
        [
            "So.",
            "That's the definition of the symbol.",
            "Model, so we assume that we have some data.",
            "And data.",
            "That is, the point consists of two objects, one object is.",
            "Is 1 high dimensional real vector and the 2nd is discolored?",
            "Is that the output?",
            "So we have a likelihood model which is a Gaussian.",
            "With why I do not hear all the vectors, the scalar vectors.",
            "The scarlet object vectors and we have.",
            "The set of latent function values.",
            "So when you use Gaussian process with this model, we play with you supplier replace supplier over the train latent functions.",
            "And we apply Bayesian inference, and the prior has this form is just a Gaussian distribution with a certain covariance matrix defined by.",
            "Some kernel function."
        ],
        [
            "So I'm not going to see that any full Bayesian methods here, so I'm just considering.",
            "Maximum likelihood type 2 methods.",
            "Most people do actually in their community.",
            "So this is here is you have two problems to solve.",
            "One is the prediction, so we assume that we have known that we know the parameters.",
            "So we have updated parameters from some previous estimation process.",
            "Some maximization of the marginal likelihood.",
            "So the question is giving some test inputs.",
            "To predict the latent latent function values.",
            "So this computation is given by.",
            "This expression boils down at the end, some kind of matrix inversion.",
            "Where?",
            "This is the test condition.",
            "Deep Fryer and this is the.",
            "The training the posterior and on the training later function values.",
            "I actually liked it.",
            "I prefer this type of equations are just not given the.",
            "The the usual equations that people give with the with inverse.",
            "Inverting this matrix because this is going to give me.",
            "This Bayesian edicola summer Havana posterior over some parameter which is actually the parameter grows with the data, and this is the kind of model that associate this parameter with the thing that I want to infer.",
            "So.",
            "To obtain the key parameters, the Noise Sigma and the and the current parameters, we maximize the marginal likelihood, which is.",
            "Theoretically it's computable is just a Gaussian distribution.",
            "But of course in practice is intractable.",
            "Becauses involves an invasion of a matrix.",
            "And by N. Where is the number of of the training points, so this has cubic complexity."
        ],
        [
            "Knowledge can only be falling for small data sets.",
            "So.",
            "Because of this, the cubic complexity in the number of turning points.",
            "We cannot actually perform this computation in large data sets, so we have two.",
            "So exact prediction is.",
            "It's impossible, and also the training using the maximum likelihood is also kind of the exact training is impossible, so we have to consider approximate methods.",
            "Also call it sparse message.",
            "So very, very similar way to get a sparse method is to use a subset of data approach.",
            "So we just keep.",
            "I'm training points and we throw away the rest of them and we just we train the model and we do predictions based on only only on this.",
            "Subject of the data.",
            "A second idea is kind of more advantages to using reducing variance.",
            "So when we use reducing violence, we use all the training points for prediction and training, but some some variables that might be actually some subset of the data might be kind of different latent function values.",
            "It will be specially treated the way that we will compute the marginal likelihood or the predicting solution.",
            "So in this talk we will focus on reducing variables.",
            "There are also other methods that are not going to disk."
        ],
        [
            "About these methods.",
            "So which is the idea of the kind of the using values?",
            "First of all, what are these using values so we have?",
            "Different types of reducing valves.",
            "The first type is to travel subject of the turning point.",
            "So who might choose?",
            "After my carefully choose the subject of the training points that capture the information about your problem and use this as reducing virus has begins a lot.",
            "Address for, but many people actually in this room.",
            "I tend to side here only people from the base and community rather than their frequencies.",
            "Did you notice?",
            "So.",
            "Another option is to use.",
            "The test points has been using Bayesian Committee machine or use auxiliary verbs which was.",
            "Introduce was used by its national and father, discussed by Queen on Arrow Kardelen Rasmussen there.",
            "Prior viewer for positive methods.",
            "So why do we want to do this?",
            "The training of the progression system using reducing values we have.",
            "To carry out two things, we have to select the inducing points.",
            "To choose this inducing variables and also to choose also the parameters, I would like to point out here that these problems are correlated.",
            "So I mean the set of reducing points that you are going to choose depends on the function.",
            "And the function depends on the parameters.",
            "So the best way probably to solve this problem is to solve it jointly.",
            "Simple tenuously choose reducing points.",
            "Also, the parameters people actually don't do this most of the time they just split into spaces, which I think is suboptimal.",
            "So the key question here.",
            "The key concept here is which is the objective function that we're going to use.",
            "So in order to remain with this maximal Type 2 approach.",
            "The natural choice is to use an approximate marginal likelihood.",
            "But we have several choices here.",
            "I mean, there are many approximates marginal likelihood out there.",
            "So which is the right one that you allow us for?",
            "For being efficient and being robust overfitting as well."
        ],
        [
            "So I would just briefly say what?",
            "What kind of nation have the current maximum likelihood used in the literature?",
            "So this might sound like it's actually derived in a kind of in a way that either we approximate the likelihood.",
            "Sorry, some sense we change this model here, replacing visually this self with some kind of.",
            "Pure.",
            "Using variables or changing the prior so this this actually gives the same both.",
            "Both ways give the same result actually and all this.",
            "Form.",
            "Which is default this form where you have replaced the true covariance matrix, which is this one by the skate till there was kind of a low rank approximation of this covariance matrix.",
            "As we will see later in the talk, this actually form of the marginal likelihood is problematic and there is something important missing there.",
            "Actually, there is another term that is quite important is actually is missing from this objective function.",
            "I mean another way to view this is that this type of objective function.",
            "Is not a lower bound in the maximum likelihood, which can cause overfitting when you trying to optimize simultaneously with respect to kernel parameters and using points.",
            "Will often I mean this magically become larger than the true marginal likelihood, so will achieve a better fitting in the training data.",
            "But the performance in taking the test that I work?"
        ],
        [
            "Actually, we must watch.",
            "So what we're going to do here is to do more selection of different ways, and we're going to apply.",
            "Started very strong inference.",
            "Like if you have read.",
            "Simply adorable version inference then this is the only thing you need to understand this method, so we never think about approximating electrical supplier.",
            "Just introduce a posterior to approximate.",
            "Studio.",
            "This is going to give us a lower bound and we're going to maximize this bound with reducing inputs and parameters using continuous optimization following the work of its natural."
        ],
        [
            "So the concept of auxiliary using variables was introduced in this paper.",
            "They, I think to call them auxiliary is kind of thing is more statistical be used for.",
            "It also makes you think about.",
            "Connections with auxiliary MCMC methods, which there are some connections there.",
            "At least this valuation method.",
            "So let me first define the concept of auxiliary variables and then I will tell you at the end of this slide how I'm going to use them.",
            "So imagine that I have M latent function values.",
            "FM.",
            "That are associated with any arbitrary inputs, not.",
            "They are not involved in the training or in the test data.",
            "So I'm going to do first what I say what I call model accommodation.",
            "So I have the original zip prior which is only over this training at the function.",
            "That kind of augmented the prior using reducing reducing values.",
            "I want to put it down here, don't do any approximations.",
            "Don't approximate the prior, just automate the prior.",
            "I have a new joint and the Magna computing that way, so actually in some sense I have done nothing so far right.",
            "The model is unchanged, the predicted solution is the same.",
            "The modern luxury is the same.",
            "In some sense I have reproduced.",
            "A set of parameters that play no active role at the moment.",
            "But I'm going to use later this parameters to play a very important role.",
            "They going to be the variational parameters, so in some sense they will determine the flexibility of.",
            "Of my variation distribution, so I'm going to do a similar thing with MCMC, for example with so this auxiliary values is going to use to facilitate inference about the real thing, and the real thing is.",
            "The function values in the training set."
        ],
        [
            "So what I want what I wish is to use auxiliary variables.",
            "Now going to facilitate inference about the training function function values but before before trying to do this, then it's good to think about what type of auxiliary variables are.",
            "It would be efficient.",
            "I mean if someone's it was about to give me some auxiliary variables, and what properties are divided should have in order to be useful.",
            "For my inference.",
            "So I'm going to give this definition.",
            "You might not agree about this.",
            "You might think of that better auxiliary values there, but this is what I'm going to use in this talk.",
            "So I called the auxiliary value of the ref and the inputs.",
            "Optimal when the data, the output data and the training later functions become conditional independent given the given that using variables.",
            "So this equations holds.",
            "So this means that when I condition on the reducing values and data.",
            "This becomes conditional dependent on the data, so another way to say this that now the inference problem about the problem and fair enough is kind of.",
            "Is transferred to inferring reducing values so its surface to infer that using values if I infer that using virus, then inferring F is very easy, I can do it just from the condition applier.",
            "So the key.",
            "Factorisation that holds.",
            "When we have optimal reducing variance, the true posterior in the augmented space factorize as follows.",
            "This is the key factor.",
            "Factorization based on on which I'm going to specify my variation distribution."
        ],
        [
            "So the question is now is how can discover this optimal reducing values?",
            "We need to kind of minimization procedures that will allow us to do this.",
            "So this is obvious now.",
            "Which kind of minimization procedures we are going to do?",
            "Is suitable to use?",
            "So.",
            "We are going to minimize the distance between the true posterior and in this augmented space, this auxiliary space.",
            "With some approximate posterior in the same space.",
            "And then we're going to do this over the visit using inputs.",
            "But it will actually reflect the flexibility of this violation distribution and also the number of this.",
            "This includes as well if we like optionally so that is of course that the variation distribution must satisfy the same factorization that holds for optimal reducing variables, so the factorisation that the true posterior it will satisfy if we have got this optimal reducing violence.",
            "So the true true factorization, I mean the real posterior.",
            "Is this one?",
            "And the vibrational posterior that we're going to use this one."
        ],
        [
            "So the only thing is left is a distant measure.",
            "We're going to apply standard variational inference and minimize the.",
            "We killed the versions the variational way.",
            "Not the woman messing killed Avengers, right?",
            "Does give us adaptable computation so we express our lower bound in this augmented space.",
            "That's the initial form of the bound.",
            "I would just."
        ],
        [
            "You just slide how to compute this?",
            "So the only the only thing the only important point here is actually the first line.",
            "Vertis here, that makes the computation tractable is that this term here cancels.",
            "If I now blocking the variation distribution here and inside the log, then this cancels out.",
            "That's the key action that makes all the computation tractable.",
            "So just from here you go here.",
            "Then you can split the diagonals.",
            "This now is is an expectation this this term is actually tractable.",
            "General compute this body can take expectations over this term.",
            "You can compute this is as defined.",
            "Is this the function which is the log of this Gaussian distribution, where this is the mean of the conditional prior and you have this trace term which is the covariance of this code of the conditional prior of given the inducing variables."
        ],
        [
            "So you follow like this computation, you merge the logs and in order to compute the best bound with respect to this field distribution.",
            "An easy way to do this.",
            "Is to reverse to put the log outside and reverse this inequality.",
            "So this is the final result.",
            "OK, we're going to analyze this.",
            "What the meaning?",
            "So at the moment, think about that you have got.",
            "A term that it was in the same function form like the term had before, but we have also this additional tracer which regularization term we're going to give a meaning to this person want to discuss the meaning of this term in a bit."
        ],
        [
            "But yeah, I mean this is what I'm going to say.",
            "That that's the initial thing.",
            "That's the traditional.",
            "Mother like to used before.",
            "But the project process approximation and we got.",
            "This this variational lower bound has this additional trace term."
        ],
        [
            "Yeah, before I continue with the comparison and explanation what this extra trays term means?",
            "Just briefly to say that as far as production is concerned, we have done nothing.",
            "Kind of new or something.",
            "I mean we got exactly the same prediction.",
            "The optimal fit distribution corresponds to the projected closer approximation.",
            "Or to the deterministic training conditional.",
            "But obviously we have done we have obtained."
        ],
        [
            "A lower bound.",
            "So.",
            "How can we use this lower bound for model selection away?",
            "The way that we're going to use it is this is now a continuous function over the inducing reboots that using reboots exist in this covariance term in this covariance term.",
            "And.",
            "And here in here as well, and we and also we are going very slow.",
            "So we want to optimize over the parameters.",
            "So what this objective function is going to do?",
            "Actually we minimize with respect to inducing.",
            "OK, the first term is a kind of supervised term.",
            "I mean, it's just a Gaussian, so in some sense the first time says feed feed the covariance of the data of the output data.",
            "The second trend is kind of unsupervised and says OK, you have to minimize this trace term in order to minimize.",
            "So in order to minimize this trace term, this trace that is actually the total values of the condition prior.",
            "Sometimes you have to put.",
            "You have to spread in some sense that using points.",
            "In order to.",
            "The.",
            "You know there's some sense to compress the prior to fit the prior some sense and also the Sigma.",
            "The noise plays a role in this regularization.",
            "For example, we will see later that.",
            "This is very robust overfeeding and reason that is very robust overfitting because the Sigma is very well regularize here.",
            "For example, this trace term is large because we don't have enough's enough it using variables.",
            "To describe, let's say the full GP prediction, this Sigma terms becomes also large in order to avoid overfitting.",
            "We will.",
            "This is actually very important.",
            "It will see actually the model parameters.",
            "Actually values of this segment.",
            "There is a very interesting behavior there.",
            "So just to mention that this exercise then can actually use.",
            "Can stand on its own as an objective function for possible learning.",
            "We can fix this, maybe some kind of auxiliary Mistral methods.",
            "I don't know because we're trying to fit this Mr of approximation to the real thing."
        ],
        [
            "So let's see kind of a theoretical sanity check if the whole procedure makes sense.",
            "So if the bound becomes exact.",
            "Then the trace is 0.",
            "Does the case.",
            "So in that race is here.",
            "Of course, then this Mr Approximation is exact to the to the train covariance matrix.",
            "This of course becomes a vector function, because this is actually the covariance matrix of this conditional prior, and we reproduce exactly the fully prediction.",
            "So in the optimal case, I mean if the bound becomes exact to the marginal to the true maximum likelihood, we have discovered optimal."
        ],
        [
            "Variance?",
            "So let's see how you straight in comparison.",
            "I'm going to use the data used by its natural in his paper and also in his thesis.",
            "For this conference registration.",
            "So I'm going to compare this objective function and the lower bound.",
            "We're going to minimize with respect to that using the brute inputs and these parameters.",
            "The inputs now just is 1 dimensional.",
            "So."
        ],
        [
            "If we do this in this data, so here we have 2000 training points.",
            "They use it inputs.",
            "We have used different values of their view of different numbers of reducing reboots 810 and 15, and we want to see how well we predict the full CP model so the full shipping model is.",
            "This is with the red dashed line and the sparse model.",
            "Is it with the blue line.",
            "This is the initialization of the inducing inputs and this is the final value of reducing reboots.",
            "As we can see, as we increase the number of of inducing variables from 10 to 15.",
            "We actually reproduce the almost would produce a fully prediction.",
            "So for example, in that case the prediction of the full DB and the variational way of training.",
            "Yeah, this party P is the same actually.",
            "This difference is really small, so if we do it with another objective function, we overfit.",
            "I want to say that this is really very robust initialization.",
            "I mean, for any random initialization in this area, you could get the same result.",
            "In there for the other objective function.",
            "We will not feel the full DP.",
            "We kind of actually massively over."
        ],
        [
            "So now I'm going to show you.",
            "Actually, I'm going to look at the actual parameters to see what is going on with the parameters is estimated parameters.",
            "So I have used before the exponential kernel.",
            "When I do the optimization and the tenant that are used in the prior.",
            "So what I'm showing here is the the parameters found by the variational method.",
            "48 reducing inputs 10 and 15, and this is the parameters by training the full DP.",
            "And this is also the marginal likelihood as you observe as you move as you increase the number of it using points, the parameters actually matched to the full GP.",
            "For example with 15, then the reducing points.",
            "The parameter also indestructibles.",
            "I mean almost the same.",
            "And there is a very interesting pattern here, but is related to the what I said about being robust, overfitting that the variance.",
            "Is getting decreases with the number of several inputs.",
            "For example, here is 0.08 something, then it becomes smaller, smaller until we must be full CP.",
            "So this is very intuitive and very important actually, because it says that when.",
            "Well, we don't.",
            "When we don't have enough reducing points.",
            "To match the full GP.",
            "So in some sense we are not allowed to use the information in the data.",
            "Only can use some partial information then.",
            "The model will prefer to explain some some signal as noise and this is what I mean if you think in terms of work on riser.",
            "This is actually what should do.",
            "An objective function that is robust overfitting if it's not allowed to use all the data should think OK.",
            "I'm not sure if there is signal there so I should actually explain some things by by noise.",
            "And this pattern here actually.",
            "We have privacy."
        ],
        [
            "Partition method so OK, what I'm going to do now is I'm going to make this problem all challenging and the reason I'm going to make it more challenging is that 200 points in one dimensional space actually had too many, right?",
            "I mean the high dimension.",
            "The data is going to look like this.",
            "Definitely not going to look like this.",
            "So so The thing is that what I'm getting what I'm going to.",
            "Is the method is going to feed well?",
            "I mean to work well in this case."
        ],
        [
            "Is any overfitting question?",
            "And also it will tell me this example to make some additional observations.",
            "So again, the same setting and I initialized it using points maximize over inducing points and key parameters, and I gave the I'm at the full prediction what we can observe here is that the prediction of the variational method is a bit smoother.",
            "When I cannot match the full CP, the valuation method will give me a bit smoother prediction that the full degree which again are related to the situation that when the valuation method cannot match the full GP in some sense.",
            "Since it is robust, overfitting will prefer to give you a smoother predicted distribution of the foods agree.",
            "Now there is another also interesting observational about the projected process marginal likelihood.",
            "In this sparse data case.",
            "This was not likely corresponded to projected process.",
            "You verbalize the training data.",
            "So this is clear here for just the mean value has passes from the training data, but there are bugs are very are very bad.",
            "We will see in the later on the large data sets that.",
            "This example actually will help us to give it the protection and meaning too."
        ],
        [
            "The results we're going to get in the large data sets.",
            "Just point out you didn't."
        ],
        [
            "Close the full TP on the two.",
            "It's not then you.",
            "Yeah yeah this browser, because then you want to.",
            "Actually, the figure you know because.",
            "This is so let's also.",
            "I wanted to keep clean the figures, that's why I didn't plot it.",
            "Yeah.",
            "What you're comparing with?",
            "Oh yeah.",
            "You are looking at a model that fits the model.",
            "And compared to that and you're saying OK, Now I have a new margin like you you have about the market like but this is not comparing to fixing it.",
            "No, not yet.",
            "I mean, I'm going to compare it to 50 in a moment in the next slide."
        ],
        [
            "It's just the BBC.",
            "I should actually use this terminology because it's more common so.",
            "Earn.",
            "So again, I mean the same pattern that this is become smaller than we fit in with the full DP."
        ],
        [
            "OK, this is some conclusions.",
            "It kind of holds more then I'll actually have observed in many data sets, so the variation method seems to converge and systematic way to the full GP.",
            "This is in terms of prediction model parameters.",
            "And also.",
            "It tends to find smoother predict distribution when we are not able to food to feed the full CP.",
            "The demise and likelihood.",
            "It will not converge to the full GP.",
            "The reason is that reducing points for the DTC is actually model parameters were actually for.",
            "The variational method are actually variation of parameters.",
            "That's why actually we don't overfit.",
            "And also an observation that we will see later in the last data set is that.",
            "This objective function that DC objective function has a tendency to."
        ],
        [
            "Depilate the training points.",
            "So let's see another marginal likelihood.",
            "That is used by introduced by money.",
            "So this is now I'm not going to speak about Fields as far as the prediction is concerned, only about the maximum likelihood, because this is actually that's what I mean.",
            "This thing to do here.",
            "So this should do in this pseudo.",
            "Censorious GP model likelihood.",
            "It uses some more advanced approximation to the real marginal likelihood, so if we see here that we have additional two Sigma and this mystro term here we have had this diagonal and this diagonal makes.",
            "The covariance.",
            "To be the.",
            "The approximation would be exactly the diagonal right?",
            "So it's kind of more more close to the truth.",
            "So this.",
            "This might sound like a stupid OK, this covariance that correspond to this model is actually not stationary.",
            "This is means that you will be able to model input dependent noise and it has been shown in practice that this marginal likelihood much more robust overfitting."
        ],
        [
            "Let's see how it's going to work in this example.",
            "So OK, in the first row is the result when you have 200 data sets and the 2nd row is the result when you when you reduce the number of training on training points.",
            "So in the first Genesee allies.",
            "This is the initialization.",
            "That's the final values.",
            "So in the first case, when I have a lot of training points, this works very well.",
            "It doesn't match the full zip prediction, but it's very close.",
            "In the other case where.",
            "The data are very few.",
            "It does quite.",
            "It works quite different than the full CP.",
            "And as we can see here is models.",
            "It's interesting noise.",
            "Or do I want to say that this is actually very natural for this model?",
            "I mean, this is the model of $0.50.",
            "So you will do different things.",
            "I mean someone that uses models.",
            "You know that if you're going to train the model by.",
            "The freedom is unlikely.",
            "We should expect not to.",
            "Full model.",
            "I mean at some cases you will fit, but some cases will not, especially when there is heteroscedastic noise or you have sparse data.",
            "So if you see here for example, there are bars are thinner than become wider, wider, wider, thinner again so."
        ],
        [
            "If you see also the actual parameters of.",
            "Yeah.",
            "Found by maximizing this machine learning we will see, for example, in the second case that the Sigma becomes.",
            "Very small to 0 so the actual noise.",
            "In the likelihood is actually explained by this characteristic term in the field, see marginal likelihood is."
        ],
        [
            "So the actual noise actually is explained by this term, which has the ability to model input dependent noise, while this Sigma.",
            "Becomes very close to 0."
        ],
        [
            "What I want to point out, I want to point out that."
        ],
        [
            "That we are not converting to the full submersion likelihood by by maximizing this.",
            "This approximate 50 Muslim likelihood.",
            "So although this is not bad actually, I mean someone is not see this like kind of disadvantage.",
            "I mean, but but you have to see that the fizzy.",
            "Is going to do a different thing that.",
            "You might expect.",
            "Alright, this is kind of some vision about this marginal likelihood.",
            "That is much more robust over fitting that the decision likely would join learning wise zone learning of reducing points, and your parents might cause some overfitting.",
            "There is a great advantage about this model that is ability to model heteroskedastic noise.",
            "And this will affect predictive measures that associated with the variance.",
            "So the average lower negative log probability density can give you have a very good, very good performance in the average negative log probability density.",
            "And also he will not."
        ],
        [
            "First, the full speed model.",
            "So another example is that both houses that are set by just using data set, because I think it's very common.",
            "I mean their existing software of Rasmussen and Williams are thing on Internet so I just took the software because kind of very easy to reproduce the result.",
            "So in order to help with the methods in the current state I will fix the hyperparameters to those obtained the full CPU.",
            "So the question is now the objective functions are going to.",
            "To feed the full submodel or not?",
            "So what I'm plotting here is the Cal divergences between the full CP.",
            "And the and what I'm getting from the sparse methods.",
            "In the case of is between the test.",
            "Posterior distribution and the approximate the posterior distribution.",
            "Well in that case this test posterior is a 51 dimensional object.",
            "I have 1500 points, so as you can see the calendar.",
            "This is for the variational methods are dropped to 0.",
            "Allowed.",
            "200 points you almost match the full speed model.",
            "The parameters also are very close to what you obtained by maximizing the action in that case are fixed too.",
            "What do you have from the food model?",
            "The other objective functions, I mean the 50 approximation that fits you much much better, but still it will not.",
            "It will not converge the physical model."
        ],
        [
            "That's the same data set, but now I'm going to.",
            "To learn, both reducing points and parameters.",
            "And I'm going.",
            "This is the results based on normalized mean square error.",
            "And normalized probability density.",
            "So these measures, the smaller the better.",
            "The smaller these measures are, the better is the performance.",
            "As you can see again the variational.",
            "Approximation, I mean the horizontal line is the value of the.",
            "Is the prediction of the full GP.",
            "So the red line converges to the Fuji prediction, both for both measures and also you can see you can see also the maximum likelihood.",
            "So this is the value of the marginal likelihood that this is the value of the variation about convergence with the full CP.",
            "The other objective functions I think they overfit.",
            "I mean the fits quite well.",
            "It has better actually normalized squarer than the full CP.",
            "But it does something different.",
            "It doesn't converge, yes?",
            "Price.",
            "But I thought they fit see approximation actually converges to the full DP when the numbers using inputs equals the number of.",
            "Yeah.",
            "No."
        ],
        [
            "The increasing inputs are in the same position, yeah, But if you optimize over them then you did something different, I mean.",
            "Aiming points that won't happen, yeah, so this is optimizing the yes and keeping fixed the parameters the razor.",
            "The reason GP with those number of reducing points that is the same as.",
            "Full GP if you're optimizing your inducing inputs you may find as regards showing a better likelihood.",
            "The point.",
            "If you think about this trace term, that race term is exactly what would encourage them to move to the training points, and it's not there."
        ],
        [
            "Yeah, they will not move to the year is because in some sense they disagree with your kind of model parameters is.",
            "There's no constraint that will remain true.",
            "Yeah.",
            "Partly explains the noise with this magnitude of discord exponential function.",
            "That's basically what you have only few industry, but you have to get a longer length scale on higher magnitude for the current function in order to.",
            "Induce all those correlations from danger noodles, and that's what I found actually.",
            "Number.",
            "Or yeah.",
            "And the landscape.",
            "Brenda.",
            "Yeah.",
            "If you have the diagonal and in the 50s, that's basically same as noise, but not now.",
            "Yeah, this document in some sense plays the role of.",
            "I mean, there's no going from.",
            "So I have to.",
            "The.",
            "Yeah yeah, SMNP seems to get worse or.",
            "For a while at the same time.",
            "Better, yeah, in that example I mean.",
            "Or something like that.",
            "Yeah, I mean.",
            "A friend well.",
            "When you have low low noise I mean like this robot data or something then the.",
            "The feed stream is lagging tends to give you better prediction with respect to, you know to anything related to the variance cause, but if you have a thing high noise I don't know.",
            "It's kind of the opposite, but I'm not sure.",
            "I mean, I can say that.",
            "In the nose case that.",
            "What is the schedule static noise?",
            "I mean, you get better results, I mean usually the physics gives better result with respect to this measure, not the not the squarer.",
            "As we will see."
        ],
        [
            "Actually, the."
        ],
        [
            "Here, for example, there is actually the same pattern.",
            "I mean I have two."
        ],
        [
            "Assets.",
            "The cleaner 40K data set in the circles used in the book."
        ],
        [
            "Williams so these are the same partner in this other set.",
            "So let's explain what is going on here.",
            "So so this uploading them in square error and the mean normalized negative log probability one depends only only on them in the other depends on the variance.",
            "So there is actually very low noise in this data set Iso's.",
            "I believe, and this seems unlikely, would with the blue line actually has the best performance because it has a tendency to over to interpolate the training data, and given that there is not too much noise in some sense is able to give good prediction in the test points as well.",
            "On the other hand.",
            "The 50 model likelihood because the the goal setter.",
            "Hey let me see error let me square error but has the best performance in the in the average non negative.",
            "Probability density because it has the ability to model healthy, drastic noise and this will matter a lot in the prediction.",
            "Of course in the prediction as far.",
            "In the low probability density.",
            "And the the variational method does something between.",
            "I mean never has the cost performance I mean.",
            "Drops there are.",
            "Systematically get down and have some.",
            "So I believe I don't know.",
            "I mean with the conclusion if if one is better than full CPU or the other, but I suspect that the reason that you get so good performance for the 50 miles, and likely because you have the ability to model healthy lasting noise."
        ],
        [
            "And you get the same pattern in that in the other data set in the circles, other set pretty much.",
            "Alright, so.",
            "I don't know how much time you have."
        ],
        [
            "No, I mean.",
            "OK, that's that other stuff about this variational method that in the case that you selected using points from the training points, I mean the variational method kind of simplifies because you don't have auxiliary variables.",
            "You have a similar bound here, but now the conditional prior is over.",
            "Remaining points and reducing point selecting from the training data.",
            "So this F N -- M is kind of.",
            "The set that is not selected to be part in there using points and Ephraim is the subject of the training data that is."
        ],
        [
            "Very using points and a very interesting result here.",
            "I mean I don't know is not a result, just.",
            "The algorithm is that.",
            "This violation about now when you try to maximize it with kind of grid selection, it becomes a variational EM algorithm.",
            "In this step you update the violation distribution by adding.",
            "So what does that mean?",
            "Actually greedy selection you means that you start with an active set with empty.",
            "You stand with our remaining set which is the whole training points and you.",
            "You do the following steps.",
            "You insert one training point to the active set and you make also an update of the hyperparameters.",
            "So then you the first table account of EM like step and the other one like kind of em like step.",
            "So for the for the current objective functions this is has not spoken versus.",
            "I mean, in this step the objective function as we can become.",
            "You will not want to nautically increase the objective function in these steps.",
            "You might actually become smaller.",
            "Now the current value.",
            "So this is not an algorithm, but if you consider with the variational bound as an objective function, is becomes precisely variational EM algorithm."
        ],
        [
            "And there is a.",
            "A proof of this, I mean, this is based on this proposition that says that if this is your current.",
            "Active set then any point doesn't matter how you choose this that you were going to add.",
            "It is active set will never decrease the lower bound and is very intuitive.",
            "I mean if you see it, I mean always you should be able to get to go closer to the true posterior if you add another point that reducing set very easily prove this.",
            "Another way to say this at any point inserted it will never decrease the calendar versions.",
            "So now it becomes.",
            "Besides operationally Margarita you have step, you update this distribution so in some sense you take a point from here and then you put it here or something.",
            "And you always get closer to the truth to the."
        ],
        [
            "And this holds for any criterion.",
            "Alright, so.",
            "This is kind of an effort to.",
            "To make us some kind of variational formulation, I mean there is no reason to actually.",
            "I mean the prior formulation.",
            "The prior unified view is very nice.",
            "Actually for me actually help me to understand these methods, but a reason to try to think about valuation methods just for model selection.",
            "I mean it will give you better objective functions that might be better for overfitting.",
            "So I'm not going to go through this slide."
        ],
        [
            "Just to show you that.",
            "For the fit, see we can actually derive a lower bound for the corresponds to the 50 objective function, but we have to start with a different likelihood model, I mean.",
            "Thinking different way, I mean for the for the for the likelihood with constant noise, the projected processes optimal one.",
            "So if we find we would like to find.",
            "Which is the lacking that correspond?",
            "To fix approximation, we have to use a different likelihood model and it turns out that.",
            "A full shipping model.",
            "That gives rise to 50.",
            "Is this one?",
            "So this is a kind of lack of the food model we have to add this characteristic term.",
            "Anyway, if we apply this variation method with end up with.",
            "Without bound for this for this join for this model, that is exactly that.",
            "Is bound consists exactly from one term, that is what naturally was is using at the moment.",
            "But we have plus this trace thermogen, or now the Lambda matrix.",
            "Is defined by this the land of the free and the K is exactly the same as before.",
            "So it's going to actually.",
            "I mean, it's quite nice in the sense that you just have to other trace term and it becomes a lower bound of some."
        ],
        [
            "Model.",
            "So that's my last slide, so there is related work in the little.",
            "So I mean there is a published draft for like a certain month report about variational learning.",
            "That is associated with this method and also Seagate also discusses valuation methods for specific classification, all although for aggression he doesn't use abound and he selects the using points using greedy heuristic criteria.",
            "So the conclusion is that the variational method can be provided with lower bounds, so this can be very useful as far as the joint learning of reducing points per parameter is concerned."
        ],
        [
            "And some acknowledgments.",
            "We now have benefit a lot from discussion from me alone as a magnet, so just hide the light near building more because he's really expert in this methods and he he gave you an amazing feeling.",
            "So.",
            "I mean, I'm not an author on this one.",
            "I love this work and the reason Michalis gotta talk is 'cause he showed me this work and I said, oh, we've got to put that in the workshop, 'cause I think it's much in the tradition of the GPR T where a lot of these methods were discussed and for me it's the most significant advance on those sparse GP approaches that we've had since.",
            "Maybe that workshop?",
            "So you didn't show any results for the corrected 50?",
            "Yeah this.",
            "I haven't actually investigated, I mean.",
            "Well this is not.",
            "Yeah, it's not clear actually for the variational bound for the physical be it will do different things.",
            "So it depends.",
            "I guess the ability of physical model, heater, static noise it will reduce.",
            "I mean.",
            "You will be.",
            "You should spread them using valid server.",
            "Well so you will be more careful, careful about modeling the function correlation.",
            "If you do run away but don't have actually complete conclusion about this, but that's nothing but.",
            "Xbox video one reason?",
            "Variational approach doesn't look so natural somehow is, I think because."
        ],
        [
            "Discussion.",
            "Can be sort of driving will naturally by looking at KLE loving.",
            "Looks like kind of after fix that by changing the likelihood.",
            "Here to get us to the very.",
            "Yeah, yeah.",
            "Yeah, I agree with you.",
            "I mean it's not that I mean this violation formulation doesn't.",
            "He's not naturalizing the CTS, but in general, but.",
            "Of those below 50.",
            "Yeah, so they will tend towards the true GT model like their disaster is only under control to the conditions and we do agree that if you're optimizing the high prices.",
            "You have to be.",
            "You have to accept that it's going to do certain things together, highlight it and it yeah.",
            "Giving people points and hyperactive, it has more flexibility to do some things which maybe you know which supposedly wouldn't wouldn't do so.",
            "It seems like your method is definitely more robust.",
            "But I think I mean The thing is that this variation is this kind of constraint.",
            "Better to feed the full CP I mean.",
            "The physical exam and you might actually work better.",
            "I mean, you might have better prediction error becausw.",
            "Scratch inflexible so.",
            "Yeah I I am.",
            "I find it very, very interesting.",
            "Thank you go.",
            "Competition.",
            "Processes and it sort of seems to imply that in the best case will be doing like a full GP, right?",
            "But we see in practice that actually simply are simply a different model in a way, right?",
            "And this makes I mean, I needed to say this makes empirical comparisons of computationally efficient methods extremely difficult, and some of us are actually struggling still.",
            "Whether we can get something like that out because you don't really know what you're trying to get, Christmas would be, or you just want to come over.",
            "Nude lived in like having dimensional space.",
            "How how is scale?",
            "You're replacing, yeah.",
            "Optimization with additional teaching point.",
            "That's a very good question.",
            "I mean there is an answer about this, but this other investigation I mean The thing is that you can construct much better optimization algorithm because now you optimize over in some high dimensional space right?",
            "So one way to get much better Alchemist to utilize the training points.",
            "So you should make line services between training points.",
            "So you might say OK, I will initialize it using points to some subset of data.",
            "Then I will take another subset I will mate.",
            "It's point from one subject with another point from other subject and then I will.",
            "I will make an optimization across the lines that connect this.",
            "So we're actually kind of.",
            "Investigate another thing.",
            "It will scale well with the dimension, but there is no results yet, so I don't know.",
            "The other answer to that is the subset of data use of the inducing input Nicholas Pass.",
            "Then really nice what he showed briefly at the end there allows you to one of the reasons Ed was proposing fits is because you couldn't optimize your active set from the training data and the hyperparameter at the same time without your objective bouncing around.",
            "The other really nice property that you guys didn't emphasize so much is if you choose decides to restrict yourself your training data.",
            "You can continuously optimize your objective function without worrying about bouncing around.",
            "And in a high dimensional space, that's probably the best thing to do was like this here.",
            "Dimensions I mean the choice in choosing using models from the training set, maybe something?",
            "We don't have the answers here, but.",
            "Talked about this when you know this refers to when you can incorporate in you data point, you can expand your.",
            "Juicy.",
            "It means that this point is when you.",
            "Actual criterion using there is no when they have the.",
            "Variational contains black beans and compare against other peoples.",
            "Criteria for inclusion appointments and bringing some idea maybe.",
            "The trace term is actually for the for the case of regression, that race term is the IBM selection criteria.",
            "It's just the accuracy of the.",
            "Data points so that does sort of validate in some sense the IBM regression criterion, 'cause you're actually trying to minimize this term only by the points you include, but you're ignoring your effect on this term, so it's very easy.",
            "Select selection.",
            "Well, I mean.",
            "Attack.",
            "It's hacking.",
            "Last line classification is that straightforward.",
            "No, I mean.",
            "Better ways of doing I mean like?",
            "Matthias, eager, and Moreover they have done.",
            "I mean, you have to do to introduce another approximation.",
            "I guess, like I don't know Laplace, but this is not the way that I would like to do it.",
            "I mean, I want to do it in a way that I will be going to optimize.",
            "Yeah.",
            "Yeah, just I mean.",
            "The variation approximation to that effectively truncated Gaussian, so it just becomes the Gaussian approximations about under the cube P is not good.",
            "If you take enough combined with the Gaussian variational, approximate it with the Gaussian, the variance of the approximation has to be very low, which is why EP is such a better idea because it peak use rather than kewpies.",
            "Anyway.",
            "Question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to thank nearly in quite in that, given the opportunity to speak around this meeting.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to speak about, as Neil said, the word variation and model selection for sparse Gaussian process regression.",
                    "label": 1
                },
                {
                    "sent": "So this kind of very specific topic, so I'm not going to say anything about the Bayesian philosophy or.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like this.",
                    "label": 0
                },
                {
                    "sent": "So this is my outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will first describe briefly the problem of Gaussian process regression using Gaussian noise.",
                    "label": 1
                },
                {
                    "sent": "And the problem we have in large data sets.",
                    "label": 0
                },
                {
                    "sent": "And then I will discuss a method that I'm starting the last month and is related to variation inference based on reducing variables.",
                    "label": 0
                },
                {
                    "sent": "So going through all these topics.",
                    "label": 0
                },
                {
                    "sent": "The concept of auxiliary inducing virus, which was introduced by Snelson Ghahramani.",
                    "label": 0
                },
                {
                    "sent": "I would compute lower bound for this kind of auxiliary validation method.",
                    "label": 0
                },
                {
                    "sent": "I would compare it with.",
                    "label": 0
                },
                {
                    "sent": "Currently use Magna likelihoods.",
                    "label": 0
                },
                {
                    "sent": "I would have experiments in large datasets.",
                    "label": 1
                },
                {
                    "sent": "Then I will discuss the case of reducing virus been being chosen from the training data.",
                    "label": 0
                },
                {
                    "sent": "And if there is some time left, then am I so you some kind of?",
                    "label": 0
                },
                {
                    "sent": "Variational formulation of of the current sparsity methods.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's the definition of the symbol.",
                    "label": 0
                },
                {
                    "sent": "Model, so we assume that we have some data.",
                    "label": 0
                },
                {
                    "sent": "And data.",
                    "label": 0
                },
                {
                    "sent": "That is, the point consists of two objects, one object is.",
                    "label": 1
                },
                {
                    "sent": "Is 1 high dimensional real vector and the 2nd is discolored?",
                    "label": 0
                },
                {
                    "sent": "Is that the output?",
                    "label": 0
                },
                {
                    "sent": "So we have a likelihood model which is a Gaussian.",
                    "label": 1
                },
                {
                    "sent": "With why I do not hear all the vectors, the scalar vectors.",
                    "label": 1
                },
                {
                    "sent": "The scarlet object vectors and we have.",
                    "label": 0
                },
                {
                    "sent": "The set of latent function values.",
                    "label": 0
                },
                {
                    "sent": "So when you use Gaussian process with this model, we play with you supplier replace supplier over the train latent functions.",
                    "label": 0
                },
                {
                    "sent": "And we apply Bayesian inference, and the prior has this form is just a Gaussian distribution with a certain covariance matrix defined by.",
                    "label": 0
                },
                {
                    "sent": "Some kernel function.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to see that any full Bayesian methods here, so I'm just considering.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood type 2 methods.",
                    "label": 1
                },
                {
                    "sent": "Most people do actually in their community.",
                    "label": 0
                },
                {
                    "sent": "So this is here is you have two problems to solve.",
                    "label": 0
                },
                {
                    "sent": "One is the prediction, so we assume that we have known that we know the parameters.",
                    "label": 0
                },
                {
                    "sent": "So we have updated parameters from some previous estimation process.",
                    "label": 0
                },
                {
                    "sent": "Some maximization of the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the question is giving some test inputs.",
                    "label": 0
                },
                {
                    "sent": "To predict the latent latent function values.",
                    "label": 0
                },
                {
                    "sent": "So this computation is given by.",
                    "label": 0
                },
                {
                    "sent": "This expression boils down at the end, some kind of matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "This is the test condition.",
                    "label": 0
                },
                {
                    "sent": "Deep Fryer and this is the.",
                    "label": 0
                },
                {
                    "sent": "The training the posterior and on the training later function values.",
                    "label": 0
                },
                {
                    "sent": "I actually liked it.",
                    "label": 0
                },
                {
                    "sent": "I prefer this type of equations are just not given the.",
                    "label": 0
                },
                {
                    "sent": "The the usual equations that people give with the with inverse.",
                    "label": 0
                },
                {
                    "sent": "Inverting this matrix because this is going to give me.",
                    "label": 0
                },
                {
                    "sent": "This Bayesian edicola summer Havana posterior over some parameter which is actually the parameter grows with the data, and this is the kind of model that associate this parameter with the thing that I want to infer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To obtain the key parameters, the Noise Sigma and the and the current parameters, we maximize the marginal likelihood, which is.",
                    "label": 1
                },
                {
                    "sent": "Theoretically it's computable is just a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "But of course in practice is intractable.",
                    "label": 0
                },
                {
                    "sent": "Becauses involves an invasion of a matrix.",
                    "label": 0
                },
                {
                    "sent": "And by N. Where is the number of of the training points, so this has cubic complexity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge can only be falling for small data sets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because of this, the cubic complexity in the number of turning points.",
                    "label": 0
                },
                {
                    "sent": "We cannot actually perform this computation in large data sets, so we have two.",
                    "label": 0
                },
                {
                    "sent": "So exact prediction is.",
                    "label": 0
                },
                {
                    "sent": "It's impossible, and also the training using the maximum likelihood is also kind of the exact training is impossible, so we have to consider approximate methods.",
                    "label": 0
                },
                {
                    "sent": "Also call it sparse message.",
                    "label": 0
                },
                {
                    "sent": "So very, very similar way to get a sparse method is to use a subset of data approach.",
                    "label": 0
                },
                {
                    "sent": "So we just keep.",
                    "label": 0
                },
                {
                    "sent": "I'm training points and we throw away the rest of them and we just we train the model and we do predictions based on only only on this.",
                    "label": 0
                },
                {
                    "sent": "Subject of the data.",
                    "label": 0
                },
                {
                    "sent": "A second idea is kind of more advantages to using reducing variance.",
                    "label": 0
                },
                {
                    "sent": "So when we use reducing violence, we use all the training points for prediction and training, but some some variables that might be actually some subset of the data might be kind of different latent function values.",
                    "label": 1
                },
                {
                    "sent": "It will be specially treated the way that we will compute the marginal likelihood or the predicting solution.",
                    "label": 1
                },
                {
                    "sent": "So in this talk we will focus on reducing variables.",
                    "label": 0
                },
                {
                    "sent": "There are also other methods that are not going to disk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About these methods.",
                    "label": 0
                },
                {
                    "sent": "So which is the idea of the kind of the using values?",
                    "label": 0
                },
                {
                    "sent": "First of all, what are these using values so we have?",
                    "label": 0
                },
                {
                    "sent": "Different types of reducing valves.",
                    "label": 0
                },
                {
                    "sent": "The first type is to travel subject of the turning point.",
                    "label": 0
                },
                {
                    "sent": "So who might choose?",
                    "label": 0
                },
                {
                    "sent": "After my carefully choose the subject of the training points that capture the information about your problem and use this as reducing virus has begins a lot.",
                    "label": 0
                },
                {
                    "sent": "Address for, but many people actually in this room.",
                    "label": 0
                },
                {
                    "sent": "I tend to side here only people from the base and community rather than their frequencies.",
                    "label": 0
                },
                {
                    "sent": "Did you notice?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another option is to use.",
                    "label": 0
                },
                {
                    "sent": "The test points has been using Bayesian Committee machine or use auxiliary verbs which was.",
                    "label": 1
                },
                {
                    "sent": "Introduce was used by its national and father, discussed by Queen on Arrow Kardelen Rasmussen there.",
                    "label": 0
                },
                {
                    "sent": "Prior viewer for positive methods.",
                    "label": 0
                },
                {
                    "sent": "So why do we want to do this?",
                    "label": 1
                },
                {
                    "sent": "The training of the progression system using reducing values we have.",
                    "label": 0
                },
                {
                    "sent": "To carry out two things, we have to select the inducing points.",
                    "label": 0
                },
                {
                    "sent": "To choose this inducing variables and also to choose also the parameters, I would like to point out here that these problems are correlated.",
                    "label": 0
                },
                {
                    "sent": "So I mean the set of reducing points that you are going to choose depends on the function.",
                    "label": 0
                },
                {
                    "sent": "And the function depends on the parameters.",
                    "label": 0
                },
                {
                    "sent": "So the best way probably to solve this problem is to solve it jointly.",
                    "label": 0
                },
                {
                    "sent": "Simple tenuously choose reducing points.",
                    "label": 0
                },
                {
                    "sent": "Also, the parameters people actually don't do this most of the time they just split into spaces, which I think is suboptimal.",
                    "label": 0
                },
                {
                    "sent": "So the key question here.",
                    "label": 0
                },
                {
                    "sent": "The key concept here is which is the objective function that we're going to use.",
                    "label": 1
                },
                {
                    "sent": "So in order to remain with this maximal Type 2 approach.",
                    "label": 1
                },
                {
                    "sent": "The natural choice is to use an approximate marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "But we have several choices here.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are many approximates marginal likelihood out there.",
                    "label": 0
                },
                {
                    "sent": "So which is the right one that you allow us for?",
                    "label": 0
                },
                {
                    "sent": "For being efficient and being robust overfitting as well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would just briefly say what?",
                    "label": 0
                },
                {
                    "sent": "What kind of nation have the current maximum likelihood used in the literature?",
                    "label": 1
                },
                {
                    "sent": "So this might sound like it's actually derived in a kind of in a way that either we approximate the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Sorry, some sense we change this model here, replacing visually this self with some kind of.",
                    "label": 0
                },
                {
                    "sent": "Pure.",
                    "label": 1
                },
                {
                    "sent": "Using variables or changing the prior so this this actually gives the same both.",
                    "label": 0
                },
                {
                    "sent": "Both ways give the same result actually and all this.",
                    "label": 0
                },
                {
                    "sent": "Form.",
                    "label": 1
                },
                {
                    "sent": "Which is default this form where you have replaced the true covariance matrix, which is this one by the skate till there was kind of a low rank approximation of this covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "As we will see later in the talk, this actually form of the marginal likelihood is problematic and there is something important missing there.",
                    "label": 1
                },
                {
                    "sent": "Actually, there is another term that is quite important is actually is missing from this objective function.",
                    "label": 0
                },
                {
                    "sent": "I mean another way to view this is that this type of objective function.",
                    "label": 0
                },
                {
                    "sent": "Is not a lower bound in the maximum likelihood, which can cause overfitting when you trying to optimize simultaneously with respect to kernel parameters and using points.",
                    "label": 1
                },
                {
                    "sent": "Will often I mean this magically become larger than the true marginal likelihood, so will achieve a better fitting in the training data.",
                    "label": 0
                },
                {
                    "sent": "But the performance in taking the test that I work?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, we must watch.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do here is to do more selection of different ways, and we're going to apply.",
                    "label": 1
                },
                {
                    "sent": "Started very strong inference.",
                    "label": 0
                },
                {
                    "sent": "Like if you have read.",
                    "label": 0
                },
                {
                    "sent": "Simply adorable version inference then this is the only thing you need to understand this method, so we never think about approximating electrical supplier.",
                    "label": 1
                },
                {
                    "sent": "Just introduce a posterior to approximate.",
                    "label": 1
                },
                {
                    "sent": "Studio.",
                    "label": 0
                },
                {
                    "sent": "This is going to give us a lower bound and we're going to maximize this bound with reducing inputs and parameters using continuous optimization following the work of its natural.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the concept of auxiliary using variables was introduced in this paper.",
                    "label": 0
                },
                {
                    "sent": "They, I think to call them auxiliary is kind of thing is more statistical be used for.",
                    "label": 0
                },
                {
                    "sent": "It also makes you think about.",
                    "label": 0
                },
                {
                    "sent": "Connections with auxiliary MCMC methods, which there are some connections there.",
                    "label": 0
                },
                {
                    "sent": "At least this valuation method.",
                    "label": 0
                },
                {
                    "sent": "So let me first define the concept of auxiliary variables and then I will tell you at the end of this slide how I'm going to use them.",
                    "label": 0
                },
                {
                    "sent": "So imagine that I have M latent function values.",
                    "label": 1
                },
                {
                    "sent": "FM.",
                    "label": 1
                },
                {
                    "sent": "That are associated with any arbitrary inputs, not.",
                    "label": 0
                },
                {
                    "sent": "They are not involved in the training or in the test data.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do first what I say what I call model accommodation.",
                    "label": 0
                },
                {
                    "sent": "So I have the original zip prior which is only over this training at the function.",
                    "label": 0
                },
                {
                    "sent": "That kind of augmented the prior using reducing reducing values.",
                    "label": 0
                },
                {
                    "sent": "I want to put it down here, don't do any approximations.",
                    "label": 0
                },
                {
                    "sent": "Don't approximate the prior, just automate the prior.",
                    "label": 0
                },
                {
                    "sent": "I have a new joint and the Magna computing that way, so actually in some sense I have done nothing so far right.",
                    "label": 0
                },
                {
                    "sent": "The model is unchanged, the predicted solution is the same.",
                    "label": 1
                },
                {
                    "sent": "The modern luxury is the same.",
                    "label": 0
                },
                {
                    "sent": "In some sense I have reproduced.",
                    "label": 1
                },
                {
                    "sent": "A set of parameters that play no active role at the moment.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to use later this parameters to play a very important role.",
                    "label": 0
                },
                {
                    "sent": "They going to be the variational parameters, so in some sense they will determine the flexibility of.",
                    "label": 0
                },
                {
                    "sent": "Of my variation distribution, so I'm going to do a similar thing with MCMC, for example with so this auxiliary values is going to use to facilitate inference about the real thing, and the real thing is.",
                    "label": 0
                },
                {
                    "sent": "The function values in the training set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I want what I wish is to use auxiliary variables.",
                    "label": 0
                },
                {
                    "sent": "Now going to facilitate inference about the training function function values but before before trying to do this, then it's good to think about what type of auxiliary variables are.",
                    "label": 1
                },
                {
                    "sent": "It would be efficient.",
                    "label": 0
                },
                {
                    "sent": "I mean if someone's it was about to give me some auxiliary variables, and what properties are divided should have in order to be useful.",
                    "label": 0
                },
                {
                    "sent": "For my inference.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give this definition.",
                    "label": 0
                },
                {
                    "sent": "You might not agree about this.",
                    "label": 0
                },
                {
                    "sent": "You might think of that better auxiliary values there, but this is what I'm going to use in this talk.",
                    "label": 0
                },
                {
                    "sent": "So I called the auxiliary value of the ref and the inputs.",
                    "label": 1
                },
                {
                    "sent": "Optimal when the data, the output data and the training later functions become conditional independent given the given that using variables.",
                    "label": 0
                },
                {
                    "sent": "So this equations holds.",
                    "label": 0
                },
                {
                    "sent": "So this means that when I condition on the reducing values and data.",
                    "label": 0
                },
                {
                    "sent": "This becomes conditional dependent on the data, so another way to say this that now the inference problem about the problem and fair enough is kind of.",
                    "label": 0
                },
                {
                    "sent": "Is transferred to inferring reducing values so its surface to infer that using values if I infer that using virus, then inferring F is very easy, I can do it just from the condition applier.",
                    "label": 0
                },
                {
                    "sent": "So the key.",
                    "label": 0
                },
                {
                    "sent": "Factorisation that holds.",
                    "label": 1
                },
                {
                    "sent": "When we have optimal reducing variance, the true posterior in the augmented space factorize as follows.",
                    "label": 0
                },
                {
                    "sent": "This is the key factor.",
                    "label": 0
                },
                {
                    "sent": "Factorization based on on which I'm going to specify my variation distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is now is how can discover this optimal reducing values?",
                    "label": 1
                },
                {
                    "sent": "We need to kind of minimization procedures that will allow us to do this.",
                    "label": 0
                },
                {
                    "sent": "So this is obvious now.",
                    "label": 0
                },
                {
                    "sent": "Which kind of minimization procedures we are going to do?",
                    "label": 0
                },
                {
                    "sent": "Is suitable to use?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We are going to minimize the distance between the true posterior and in this augmented space, this auxiliary space.",
                    "label": 1
                },
                {
                    "sent": "With some approximate posterior in the same space.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do this over the visit using inputs.",
                    "label": 0
                },
                {
                    "sent": "But it will actually reflect the flexibility of this violation distribution and also the number of this.",
                    "label": 0
                },
                {
                    "sent": "This includes as well if we like optionally so that is of course that the variation distribution must satisfy the same factorization that holds for optimal reducing variables, so the factorisation that the true posterior it will satisfy if we have got this optimal reducing violence.",
                    "label": 1
                },
                {
                    "sent": "So the true true factorization, I mean the real posterior.",
                    "label": 1
                },
                {
                    "sent": "Is this one?",
                    "label": 0
                },
                {
                    "sent": "And the vibrational posterior that we're going to use this one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the only thing is left is a distant measure.",
                    "label": 0
                },
                {
                    "sent": "We're going to apply standard variational inference and minimize the.",
                    "label": 1
                },
                {
                    "sent": "We killed the versions the variational way.",
                    "label": 0
                },
                {
                    "sent": "Not the woman messing killed Avengers, right?",
                    "label": 0
                },
                {
                    "sent": "Does give us adaptable computation so we express our lower bound in this augmented space.",
                    "label": 0
                },
                {
                    "sent": "That's the initial form of the bound.",
                    "label": 0
                },
                {
                    "sent": "I would just.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just slide how to compute this?",
                    "label": 0
                },
                {
                    "sent": "So the only the only thing the only important point here is actually the first line.",
                    "label": 0
                },
                {
                    "sent": "Vertis here, that makes the computation tractable is that this term here cancels.",
                    "label": 0
                },
                {
                    "sent": "If I now blocking the variation distribution here and inside the log, then this cancels out.",
                    "label": 0
                },
                {
                    "sent": "That's the key action that makes all the computation tractable.",
                    "label": 0
                },
                {
                    "sent": "So just from here you go here.",
                    "label": 0
                },
                {
                    "sent": "Then you can split the diagonals.",
                    "label": 0
                },
                {
                    "sent": "This now is is an expectation this this term is actually tractable.",
                    "label": 0
                },
                {
                    "sent": "General compute this body can take expectations over this term.",
                    "label": 0
                },
                {
                    "sent": "You can compute this is as defined.",
                    "label": 0
                },
                {
                    "sent": "Is this the function which is the log of this Gaussian distribution, where this is the mean of the conditional prior and you have this trace term which is the covariance of this code of the conditional prior of given the inducing variables.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you follow like this computation, you merge the logs and in order to compute the best bound with respect to this field distribution.",
                    "label": 0
                },
                {
                    "sent": "An easy way to do this.",
                    "label": 0
                },
                {
                    "sent": "Is to reverse to put the log outside and reverse this inequality.",
                    "label": 0
                },
                {
                    "sent": "So this is the final result.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to analyze this.",
                    "label": 0
                },
                {
                    "sent": "What the meaning?",
                    "label": 0
                },
                {
                    "sent": "So at the moment, think about that you have got.",
                    "label": 0
                },
                {
                    "sent": "A term that it was in the same function form like the term had before, but we have also this additional tracer which regularization term we're going to give a meaning to this person want to discuss the meaning of this term in a bit.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But yeah, I mean this is what I'm going to say.",
                    "label": 0
                },
                {
                    "sent": "That that's the initial thing.",
                    "label": 0
                },
                {
                    "sent": "That's the traditional.",
                    "label": 0
                },
                {
                    "sent": "Mother like to used before.",
                    "label": 0
                },
                {
                    "sent": "But the project process approximation and we got.",
                    "label": 1
                },
                {
                    "sent": "This this variational lower bound has this additional trace term.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, before I continue with the comparison and explanation what this extra trays term means?",
                    "label": 0
                },
                {
                    "sent": "Just briefly to say that as far as production is concerned, we have done nothing.",
                    "label": 0
                },
                {
                    "sent": "Kind of new or something.",
                    "label": 0
                },
                {
                    "sent": "I mean we got exactly the same prediction.",
                    "label": 0
                },
                {
                    "sent": "The optimal fit distribution corresponds to the projected closer approximation.",
                    "label": 1
                },
                {
                    "sent": "Or to the deterministic training conditional.",
                    "label": 0
                },
                {
                    "sent": "But obviously we have done we have obtained.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lower bound.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How can we use this lower bound for model selection away?",
                    "label": 1
                },
                {
                    "sent": "The way that we're going to use it is this is now a continuous function over the inducing reboots that using reboots exist in this covariance term in this covariance term.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And here in here as well, and we and also we are going very slow.",
                    "label": 0
                },
                {
                    "sent": "So we want to optimize over the parameters.",
                    "label": 0
                },
                {
                    "sent": "So what this objective function is going to do?",
                    "label": 0
                },
                {
                    "sent": "Actually we minimize with respect to inducing.",
                    "label": 1
                },
                {
                    "sent": "OK, the first term is a kind of supervised term.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just a Gaussian, so in some sense the first time says feed feed the covariance of the data of the output data.",
                    "label": 1
                },
                {
                    "sent": "The second trend is kind of unsupervised and says OK, you have to minimize this trace term in order to minimize.",
                    "label": 0
                },
                {
                    "sent": "So in order to minimize this trace term, this trace that is actually the total values of the condition prior.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have to put.",
                    "label": 0
                },
                {
                    "sent": "You have to spread in some sense that using points.",
                    "label": 0
                },
                {
                    "sent": "In order to.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "You know there's some sense to compress the prior to fit the prior some sense and also the Sigma.",
                    "label": 0
                },
                {
                    "sent": "The noise plays a role in this regularization.",
                    "label": 0
                },
                {
                    "sent": "For example, we will see later that.",
                    "label": 0
                },
                {
                    "sent": "This is very robust overfeeding and reason that is very robust overfitting because the Sigma is very well regularize here.",
                    "label": 0
                },
                {
                    "sent": "For example, this trace term is large because we don't have enough's enough it using variables.",
                    "label": 0
                },
                {
                    "sent": "To describe, let's say the full GP prediction, this Sigma terms becomes also large in order to avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                },
                {
                    "sent": "This is actually very important.",
                    "label": 0
                },
                {
                    "sent": "It will see actually the model parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually values of this segment.",
                    "label": 0
                },
                {
                    "sent": "There is a very interesting behavior there.",
                    "label": 0
                },
                {
                    "sent": "So just to mention that this exercise then can actually use.",
                    "label": 0
                },
                {
                    "sent": "Can stand on its own as an objective function for possible learning.",
                    "label": 1
                },
                {
                    "sent": "We can fix this, maybe some kind of auxiliary Mistral methods.",
                    "label": 0
                },
                {
                    "sent": "I don't know because we're trying to fit this Mr of approximation to the real thing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see kind of a theoretical sanity check if the whole procedure makes sense.",
                    "label": 0
                },
                {
                    "sent": "So if the bound becomes exact.",
                    "label": 0
                },
                {
                    "sent": "Then the trace is 0.",
                    "label": 0
                },
                {
                    "sent": "Does the case.",
                    "label": 0
                },
                {
                    "sent": "So in that race is here.",
                    "label": 0
                },
                {
                    "sent": "Of course, then this Mr Approximation is exact to the to the train covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This of course becomes a vector function, because this is actually the covariance matrix of this conditional prior, and we reproduce exactly the fully prediction.",
                    "label": 0
                },
                {
                    "sent": "So in the optimal case, I mean if the bound becomes exact to the marginal to the true maximum likelihood, we have discovered optimal.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance?",
                    "label": 0
                },
                {
                    "sent": "So let's see how you straight in comparison.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use the data used by its natural in his paper and also in his thesis.",
                    "label": 0
                },
                {
                    "sent": "For this conference registration.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to compare this objective function and the lower bound.",
                    "label": 0
                },
                {
                    "sent": "We're going to minimize with respect to that using the brute inputs and these parameters.",
                    "label": 0
                },
                {
                    "sent": "The inputs now just is 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we do this in this data, so here we have 2000 training points.",
                    "label": 1
                },
                {
                    "sent": "They use it inputs.",
                    "label": 0
                },
                {
                    "sent": "We have used different values of their view of different numbers of reducing reboots 810 and 15, and we want to see how well we predict the full CP model so the full shipping model is.",
                    "label": 1
                },
                {
                    "sent": "This is with the red dashed line and the sparse model.",
                    "label": 1
                },
                {
                    "sent": "Is it with the blue line.",
                    "label": 0
                },
                {
                    "sent": "This is the initialization of the inducing inputs and this is the final value of reducing reboots.",
                    "label": 0
                },
                {
                    "sent": "As we can see, as we increase the number of of inducing variables from 10 to 15.",
                    "label": 0
                },
                {
                    "sent": "We actually reproduce the almost would produce a fully prediction.",
                    "label": 1
                },
                {
                    "sent": "So for example, in that case the prediction of the full DB and the variational way of training.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this party P is the same actually.",
                    "label": 0
                },
                {
                    "sent": "This difference is really small, so if we do it with another objective function, we overfit.",
                    "label": 0
                },
                {
                    "sent": "I want to say that this is really very robust initialization.",
                    "label": 0
                },
                {
                    "sent": "I mean, for any random initialization in this area, you could get the same result.",
                    "label": 0
                },
                {
                    "sent": "In there for the other objective function.",
                    "label": 0
                },
                {
                    "sent": "We will not feel the full DP.",
                    "label": 0
                },
                {
                    "sent": "We kind of actually massively over.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm going to look at the actual parameters to see what is going on with the parameters is estimated parameters.",
                    "label": 0
                },
                {
                    "sent": "So I have used before the exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "When I do the optimization and the tenant that are used in the prior.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here is the the parameters found by the variational method.",
                    "label": 1
                },
                {
                    "sent": "48 reducing inputs 10 and 15, and this is the parameters by training the full DP.",
                    "label": 0
                },
                {
                    "sent": "And this is also the marginal likelihood as you observe as you move as you increase the number of it using points, the parameters actually matched to the full GP.",
                    "label": 0
                },
                {
                    "sent": "For example with 15, then the reducing points.",
                    "label": 0
                },
                {
                    "sent": "The parameter also indestructibles.",
                    "label": 0
                },
                {
                    "sent": "I mean almost the same.",
                    "label": 0
                },
                {
                    "sent": "And there is a very interesting pattern here, but is related to the what I said about being robust, overfitting that the variance.",
                    "label": 1
                },
                {
                    "sent": "Is getting decreases with the number of several inputs.",
                    "label": 1
                },
                {
                    "sent": "For example, here is 0.08 something, then it becomes smaller, smaller until we must be full CP.",
                    "label": 0
                },
                {
                    "sent": "So this is very intuitive and very important actually, because it says that when.",
                    "label": 1
                },
                {
                    "sent": "Well, we don't.",
                    "label": 0
                },
                {
                    "sent": "When we don't have enough reducing points.",
                    "label": 0
                },
                {
                    "sent": "To match the full GP.",
                    "label": 0
                },
                {
                    "sent": "So in some sense we are not allowed to use the information in the data.",
                    "label": 0
                },
                {
                    "sent": "Only can use some partial information then.",
                    "label": 0
                },
                {
                    "sent": "The model will prefer to explain some some signal as noise and this is what I mean if you think in terms of work on riser.",
                    "label": 1
                },
                {
                    "sent": "This is actually what should do.",
                    "label": 0
                },
                {
                    "sent": "An objective function that is robust overfitting if it's not allowed to use all the data should think OK.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if there is signal there so I should actually explain some things by by noise.",
                    "label": 0
                },
                {
                    "sent": "And this pattern here actually.",
                    "label": 0
                },
                {
                    "sent": "We have privacy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partition method so OK, what I'm going to do now is I'm going to make this problem all challenging and the reason I'm going to make it more challenging is that 200 points in one dimensional space actually had too many, right?",
                    "label": 0
                },
                {
                    "sent": "I mean the high dimension.",
                    "label": 0
                },
                {
                    "sent": "The data is going to look like this.",
                    "label": 0
                },
                {
                    "sent": "Definitely not going to look like this.",
                    "label": 0
                },
                {
                    "sent": "So so The thing is that what I'm getting what I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Is the method is going to feed well?",
                    "label": 0
                },
                {
                    "sent": "I mean to work well in this case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is any overfitting question?",
                    "label": 0
                },
                {
                    "sent": "And also it will tell me this example to make some additional observations.",
                    "label": 0
                },
                {
                    "sent": "So again, the same setting and I initialized it using points maximize over inducing points and key parameters, and I gave the I'm at the full prediction what we can observe here is that the prediction of the variational method is a bit smoother.",
                    "label": 0
                },
                {
                    "sent": "When I cannot match the full CP, the valuation method will give me a bit smoother prediction that the full degree which again are related to the situation that when the valuation method cannot match the full GP in some sense.",
                    "label": 0
                },
                {
                    "sent": "Since it is robust, overfitting will prefer to give you a smoother predicted distribution of the foods agree.",
                    "label": 0
                },
                {
                    "sent": "Now there is another also interesting observational about the projected process marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "In this sparse data case.",
                    "label": 0
                },
                {
                    "sent": "This was not likely corresponded to projected process.",
                    "label": 0
                },
                {
                    "sent": "You verbalize the training data.",
                    "label": 0
                },
                {
                    "sent": "So this is clear here for just the mean value has passes from the training data, but there are bugs are very are very bad.",
                    "label": 0
                },
                {
                    "sent": "We will see in the later on the large data sets that.",
                    "label": 0
                },
                {
                    "sent": "This example actually will help us to give it the protection and meaning too.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results we're going to get in the large data sets.",
                    "label": 0
                },
                {
                    "sent": "Just point out you didn't.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Close the full TP on the two.",
                    "label": 0
                },
                {
                    "sent": "It's not then you.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah this browser, because then you want to.",
                    "label": 0
                },
                {
                    "sent": "Actually, the figure you know because.",
                    "label": 0
                },
                {
                    "sent": "This is so let's also.",
                    "label": 0
                },
                {
                    "sent": "I wanted to keep clean the figures, that's why I didn't plot it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What you're comparing with?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "You are looking at a model that fits the model.",
                    "label": 0
                },
                {
                    "sent": "And compared to that and you're saying OK, Now I have a new margin like you you have about the market like but this is not comparing to fixing it.",
                    "label": 0
                },
                {
                    "sent": "No, not yet.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm going to compare it to 50 in a moment in the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just the BBC.",
                    "label": 0
                },
                {
                    "sent": "I should actually use this terminology because it's more common so.",
                    "label": 0
                },
                {
                    "sent": "Earn.",
                    "label": 0
                },
                {
                    "sent": "So again, I mean the same pattern that this is become smaller than we fit in with the full DP.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is some conclusions.",
                    "label": 0
                },
                {
                    "sent": "It kind of holds more then I'll actually have observed in many data sets, so the variation method seems to converge and systematic way to the full GP.",
                    "label": 1
                },
                {
                    "sent": "This is in terms of prediction model parameters.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                },
                {
                    "sent": "It tends to find smoother predict distribution when we are not able to food to feed the full CP.",
                    "label": 1
                },
                {
                    "sent": "The demise and likelihood.",
                    "label": 1
                },
                {
                    "sent": "It will not converge to the full GP.",
                    "label": 1
                },
                {
                    "sent": "The reason is that reducing points for the DTC is actually model parameters were actually for.",
                    "label": 0
                },
                {
                    "sent": "The variational method are actually variation of parameters.",
                    "label": 0
                },
                {
                    "sent": "That's why actually we don't overfit.",
                    "label": 0
                },
                {
                    "sent": "And also an observation that we will see later in the last data set is that.",
                    "label": 0
                },
                {
                    "sent": "This objective function that DC objective function has a tendency to.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Depilate the training points.",
                    "label": 0
                },
                {
                    "sent": "So let's see another marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "That is used by introduced by money.",
                    "label": 0
                },
                {
                    "sent": "So this is now I'm not going to speak about Fields as far as the prediction is concerned, only about the maximum likelihood, because this is actually that's what I mean.",
                    "label": 0
                },
                {
                    "sent": "This thing to do here.",
                    "label": 0
                },
                {
                    "sent": "So this should do in this pseudo.",
                    "label": 0
                },
                {
                    "sent": "Censorious GP model likelihood.",
                    "label": 0
                },
                {
                    "sent": "It uses some more advanced approximation to the real marginal likelihood, so if we see here that we have additional two Sigma and this mystro term here we have had this diagonal and this diagonal makes.",
                    "label": 1
                },
                {
                    "sent": "The covariance.",
                    "label": 0
                },
                {
                    "sent": "To be the.",
                    "label": 0
                },
                {
                    "sent": "The approximation would be exactly the diagonal right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of more more close to the truth.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 1
                },
                {
                    "sent": "This might sound like a stupid OK, this covariance that correspond to this model is actually not stationary.",
                    "label": 0
                },
                {
                    "sent": "This is means that you will be able to model input dependent noise and it has been shown in practice that this marginal likelihood much more robust overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how it's going to work in this example.",
                    "label": 0
                },
                {
                    "sent": "So OK, in the first row is the result when you have 200 data sets and the 2nd row is the result when you when you reduce the number of training on training points.",
                    "label": 0
                },
                {
                    "sent": "So in the first Genesee allies.",
                    "label": 0
                },
                {
                    "sent": "This is the initialization.",
                    "label": 0
                },
                {
                    "sent": "That's the final values.",
                    "label": 0
                },
                {
                    "sent": "So in the first case, when I have a lot of training points, this works very well.",
                    "label": 0
                },
                {
                    "sent": "It doesn't match the full zip prediction, but it's very close.",
                    "label": 0
                },
                {
                    "sent": "In the other case where.",
                    "label": 0
                },
                {
                    "sent": "The data are very few.",
                    "label": 0
                },
                {
                    "sent": "It does quite.",
                    "label": 0
                },
                {
                    "sent": "It works quite different than the full CP.",
                    "label": 0
                },
                {
                    "sent": "And as we can see here is models.",
                    "label": 0
                },
                {
                    "sent": "It's interesting noise.",
                    "label": 0
                },
                {
                    "sent": "Or do I want to say that this is actually very natural for this model?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is the model of $0.50.",
                    "label": 0
                },
                {
                    "sent": "So you will do different things.",
                    "label": 0
                },
                {
                    "sent": "I mean someone that uses models.",
                    "label": 0
                },
                {
                    "sent": "You know that if you're going to train the model by.",
                    "label": 0
                },
                {
                    "sent": "The freedom is unlikely.",
                    "label": 0
                },
                {
                    "sent": "We should expect not to.",
                    "label": 0
                },
                {
                    "sent": "Full model.",
                    "label": 0
                },
                {
                    "sent": "I mean at some cases you will fit, but some cases will not, especially when there is heteroscedastic noise or you have sparse data.",
                    "label": 0
                },
                {
                    "sent": "So if you see here for example, there are bars are thinner than become wider, wider, wider, thinner again so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you see also the actual parameters of.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Found by maximizing this machine learning we will see, for example, in the second case that the Sigma becomes.",
                    "label": 0
                },
                {
                    "sent": "Very small to 0 so the actual noise.",
                    "label": 0
                },
                {
                    "sent": "In the likelihood is actually explained by this characteristic term in the field, see marginal likelihood is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the actual noise actually is explained by this term, which has the ability to model input dependent noise, while this Sigma.",
                    "label": 0
                },
                {
                    "sent": "Becomes very close to 0.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I want to point out, I want to point out that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we are not converting to the full submersion likelihood by by maximizing this.",
                    "label": 1
                },
                {
                    "sent": "This approximate 50 Muslim likelihood.",
                    "label": 0
                },
                {
                    "sent": "So although this is not bad actually, I mean someone is not see this like kind of disadvantage.",
                    "label": 0
                },
                {
                    "sent": "I mean, but but you have to see that the fizzy.",
                    "label": 0
                },
                {
                    "sent": "Is going to do a different thing that.",
                    "label": 0
                },
                {
                    "sent": "You might expect.",
                    "label": 0
                },
                {
                    "sent": "Alright, this is kind of some vision about this marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "That is much more robust over fitting that the decision likely would join learning wise zone learning of reducing points, and your parents might cause some overfitting.",
                    "label": 1
                },
                {
                    "sent": "There is a great advantage about this model that is ability to model heteroskedastic noise.",
                    "label": 1
                },
                {
                    "sent": "And this will affect predictive measures that associated with the variance.",
                    "label": 0
                },
                {
                    "sent": "So the average lower negative log probability density can give you have a very good, very good performance in the average negative log probability density.",
                    "label": 0
                },
                {
                    "sent": "And also he will not.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, the full speed model.",
                    "label": 0
                },
                {
                    "sent": "So another example is that both houses that are set by just using data set, because I think it's very common.",
                    "label": 0
                },
                {
                    "sent": "I mean their existing software of Rasmussen and Williams are thing on Internet so I just took the software because kind of very easy to reproduce the result.",
                    "label": 0
                },
                {
                    "sent": "So in order to help with the methods in the current state I will fix the hyperparameters to those obtained the full CPU.",
                    "label": 1
                },
                {
                    "sent": "So the question is now the objective functions are going to.",
                    "label": 0
                },
                {
                    "sent": "To feed the full submodel or not?",
                    "label": 0
                },
                {
                    "sent": "So what I'm plotting here is the Cal divergences between the full CP.",
                    "label": 0
                },
                {
                    "sent": "And the and what I'm getting from the sparse methods.",
                    "label": 0
                },
                {
                    "sent": "In the case of is between the test.",
                    "label": 1
                },
                {
                    "sent": "Posterior distribution and the approximate the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Well in that case this test posterior is a 51 dimensional object.",
                    "label": 1
                },
                {
                    "sent": "I have 1500 points, so as you can see the calendar.",
                    "label": 0
                },
                {
                    "sent": "This is for the variational methods are dropped to 0.",
                    "label": 0
                },
                {
                    "sent": "Allowed.",
                    "label": 0
                },
                {
                    "sent": "200 points you almost match the full speed model.",
                    "label": 0
                },
                {
                    "sent": "The parameters also are very close to what you obtained by maximizing the action in that case are fixed too.",
                    "label": 0
                },
                {
                    "sent": "What do you have from the food model?",
                    "label": 0
                },
                {
                    "sent": "The other objective functions, I mean the 50 approximation that fits you much much better, but still it will not.",
                    "label": 0
                },
                {
                    "sent": "It will not converge the physical model.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the same data set, but now I'm going to.",
                    "label": 0
                },
                {
                    "sent": "To learn, both reducing points and parameters.",
                    "label": 0
                },
                {
                    "sent": "And I'm going.",
                    "label": 0
                },
                {
                    "sent": "This is the results based on normalized mean square error.",
                    "label": 0
                },
                {
                    "sent": "And normalized probability density.",
                    "label": 0
                },
                {
                    "sent": "So these measures, the smaller the better.",
                    "label": 0
                },
                {
                    "sent": "The smaller these measures are, the better is the performance.",
                    "label": 0
                },
                {
                    "sent": "As you can see again the variational.",
                    "label": 1
                },
                {
                    "sent": "Approximation, I mean the horizontal line is the value of the.",
                    "label": 1
                },
                {
                    "sent": "Is the prediction of the full GP.",
                    "label": 0
                },
                {
                    "sent": "So the red line converges to the Fuji prediction, both for both measures and also you can see you can see also the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this is the value of the marginal likelihood that this is the value of the variation about convergence with the full CP.",
                    "label": 1
                },
                {
                    "sent": "The other objective functions I think they overfit.",
                    "label": 0
                },
                {
                    "sent": "I mean the fits quite well.",
                    "label": 0
                },
                {
                    "sent": "It has better actually normalized squarer than the full CP.",
                    "label": 0
                },
                {
                    "sent": "But it does something different.",
                    "label": 0
                },
                {
                    "sent": "It doesn't converge, yes?",
                    "label": 0
                },
                {
                    "sent": "Price.",
                    "label": 0
                },
                {
                    "sent": "But I thought they fit see approximation actually converges to the full DP when the numbers using inputs equals the number of.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The increasing inputs are in the same position, yeah, But if you optimize over them then you did something different, I mean.",
                    "label": 0
                },
                {
                    "sent": "Aiming points that won't happen, yeah, so this is optimizing the yes and keeping fixed the parameters the razor.",
                    "label": 0
                },
                {
                    "sent": "The reason GP with those number of reducing points that is the same as.",
                    "label": 0
                },
                {
                    "sent": "Full GP if you're optimizing your inducing inputs you may find as regards showing a better likelihood.",
                    "label": 0
                },
                {
                    "sent": "The point.",
                    "label": 0
                },
                {
                    "sent": "If you think about this trace term, that race term is exactly what would encourage them to move to the training points, and it's not there.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, they will not move to the year is because in some sense they disagree with your kind of model parameters is.",
                    "label": 0
                },
                {
                    "sent": "There's no constraint that will remain true.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Partly explains the noise with this magnitude of discord exponential function.",
                    "label": 0
                },
                {
                    "sent": "That's basically what you have only few industry, but you have to get a longer length scale on higher magnitude for the current function in order to.",
                    "label": 0
                },
                {
                    "sent": "Induce all those correlations from danger noodles, and that's what I found actually.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Or yeah.",
                    "label": 0
                },
                {
                    "sent": "And the landscape.",
                    "label": 0
                },
                {
                    "sent": "Brenda.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "If you have the diagonal and in the 50s, that's basically same as noise, but not now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this document in some sense plays the role of.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no going from.",
                    "label": 0
                },
                {
                    "sent": "So I have to.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, SMNP seems to get worse or.",
                    "label": 0
                },
                {
                    "sent": "For a while at the same time.",
                    "label": 0
                },
                {
                    "sent": "Better, yeah, in that example I mean.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "A friend well.",
                    "label": 0
                },
                {
                    "sent": "When you have low low noise I mean like this robot data or something then the.",
                    "label": 0
                },
                {
                    "sent": "The feed stream is lagging tends to give you better prediction with respect to, you know to anything related to the variance cause, but if you have a thing high noise I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the opposite, but I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, I can say that.",
                    "label": 0
                },
                {
                    "sent": "In the nose case that.",
                    "label": 0
                },
                {
                    "sent": "What is the schedule static noise?",
                    "label": 0
                },
                {
                    "sent": "I mean, you get better results, I mean usually the physics gives better result with respect to this measure, not the not the squarer.",
                    "label": 0
                },
                {
                    "sent": "As we will see.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, for example, there is actually the same pattern.",
                    "label": 0
                },
                {
                    "sent": "I mean I have two.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assets.",
                    "label": 0
                },
                {
                    "sent": "The cleaner 40K data set in the circles used in the book.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Williams so these are the same partner in this other set.",
                    "label": 0
                },
                {
                    "sent": "So let's explain what is going on here.",
                    "label": 0
                },
                {
                    "sent": "So so this uploading them in square error and the mean normalized negative log probability one depends only only on them in the other depends on the variance.",
                    "label": 1
                },
                {
                    "sent": "So there is actually very low noise in this data set Iso's.",
                    "label": 0
                },
                {
                    "sent": "I believe, and this seems unlikely, would with the blue line actually has the best performance because it has a tendency to over to interpolate the training data, and given that there is not too much noise in some sense is able to give good prediction in the test points as well.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "The 50 model likelihood because the the goal setter.",
                    "label": 0
                },
                {
                    "sent": "Hey let me see error let me square error but has the best performance in the in the average non negative.",
                    "label": 0
                },
                {
                    "sent": "Probability density because it has the ability to model healthy, drastic noise and this will matter a lot in the prediction.",
                    "label": 0
                },
                {
                    "sent": "Of course in the prediction as far.",
                    "label": 1
                },
                {
                    "sent": "In the low probability density.",
                    "label": 0
                },
                {
                    "sent": "And the the variational method does something between.",
                    "label": 0
                },
                {
                    "sent": "I mean never has the cost performance I mean.",
                    "label": 0
                },
                {
                    "sent": "Drops there are.",
                    "label": 0
                },
                {
                    "sent": "Systematically get down and have some.",
                    "label": 0
                },
                {
                    "sent": "So I believe I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean with the conclusion if if one is better than full CPU or the other, but I suspect that the reason that you get so good performance for the 50 miles, and likely because you have the ability to model healthy lasting noise.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get the same pattern in that in the other data set in the circles, other set pretty much.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much time you have.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that other stuff about this variational method that in the case that you selected using points from the training points, I mean the variational method kind of simplifies because you don't have auxiliary variables.",
                    "label": 1
                },
                {
                    "sent": "You have a similar bound here, but now the conditional prior is over.",
                    "label": 0
                },
                {
                    "sent": "Remaining points and reducing point selecting from the training data.",
                    "label": 1
                },
                {
                    "sent": "So this F N -- M is kind of.",
                    "label": 0
                },
                {
                    "sent": "The set that is not selected to be part in there using points and Ephraim is the subject of the training data that is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very using points and a very interesting result here.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't know is not a result, just.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is that.",
                    "label": 0
                },
                {
                    "sent": "This violation about now when you try to maximize it with kind of grid selection, it becomes a variational EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "In this step you update the violation distribution by adding.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Actually greedy selection you means that you start with an active set with empty.",
                    "label": 0
                },
                {
                    "sent": "You stand with our remaining set which is the whole training points and you.",
                    "label": 0
                },
                {
                    "sent": "You do the following steps.",
                    "label": 0
                },
                {
                    "sent": "You insert one training point to the active set and you make also an update of the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So then you the first table account of EM like step and the other one like kind of em like step.",
                    "label": 0
                },
                {
                    "sent": "So for the for the current objective functions this is has not spoken versus.",
                    "label": 0
                },
                {
                    "sent": "I mean, in this step the objective function as we can become.",
                    "label": 0
                },
                {
                    "sent": "You will not want to nautically increase the objective function in these steps.",
                    "label": 0
                },
                {
                    "sent": "You might actually become smaller.",
                    "label": 0
                },
                {
                    "sent": "Now the current value.",
                    "label": 0
                },
                {
                    "sent": "So this is not an algorithm, but if you consider with the variational bound as an objective function, is becomes precisely variational EM algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is a.",
                    "label": 0
                },
                {
                    "sent": "A proof of this, I mean, this is based on this proposition that says that if this is your current.",
                    "label": 0
                },
                {
                    "sent": "Active set then any point doesn't matter how you choose this that you were going to add.",
                    "label": 0
                },
                {
                    "sent": "It is active set will never decrease the lower bound and is very intuitive.",
                    "label": 1
                },
                {
                    "sent": "I mean if you see it, I mean always you should be able to get to go closer to the true posterior if you add another point that reducing set very easily prove this.",
                    "label": 1
                },
                {
                    "sent": "Another way to say this at any point inserted it will never decrease the calendar versions.",
                    "label": 0
                },
                {
                    "sent": "So now it becomes.",
                    "label": 0
                },
                {
                    "sent": "Besides operationally Margarita you have step, you update this distribution so in some sense you take a point from here and then you put it here or something.",
                    "label": 0
                },
                {
                    "sent": "And you always get closer to the truth to the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this holds for any criterion.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of an effort to.",
                    "label": 0
                },
                {
                    "sent": "To make us some kind of variational formulation, I mean there is no reason to actually.",
                    "label": 1
                },
                {
                    "sent": "I mean the prior formulation.",
                    "label": 0
                },
                {
                    "sent": "The prior unified view is very nice.",
                    "label": 0
                },
                {
                    "sent": "Actually for me actually help me to understand these methods, but a reason to try to think about valuation methods just for model selection.",
                    "label": 1
                },
                {
                    "sent": "I mean it will give you better objective functions that might be better for overfitting.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go through this slide.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to show you that.",
                    "label": 0
                },
                {
                    "sent": "For the fit, see we can actually derive a lower bound for the corresponds to the 50 objective function, but we have to start with a different likelihood model, I mean.",
                    "label": 0
                },
                {
                    "sent": "Thinking different way, I mean for the for the for the likelihood with constant noise, the projected processes optimal one.",
                    "label": 0
                },
                {
                    "sent": "So if we find we would like to find.",
                    "label": 0
                },
                {
                    "sent": "Which is the lacking that correspond?",
                    "label": 0
                },
                {
                    "sent": "To fix approximation, we have to use a different likelihood model and it turns out that.",
                    "label": 0
                },
                {
                    "sent": "A full shipping model.",
                    "label": 0
                },
                {
                    "sent": "That gives rise to 50.",
                    "label": 0
                },
                {
                    "sent": "Is this one?",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of lack of the food model we have to add this characteristic term.",
                    "label": 0
                },
                {
                    "sent": "Anyway, if we apply this variation method with end up with.",
                    "label": 0
                },
                {
                    "sent": "Without bound for this for this join for this model, that is exactly that.",
                    "label": 1
                },
                {
                    "sent": "Is bound consists exactly from one term, that is what naturally was is using at the moment.",
                    "label": 0
                },
                {
                    "sent": "But we have plus this trace thermogen, or now the Lambda matrix.",
                    "label": 0
                },
                {
                    "sent": "Is defined by this the land of the free and the K is exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "So it's going to actually.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's quite nice in the sense that you just have to other trace term and it becomes a lower bound of some.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So that's my last slide, so there is related work in the little.",
                    "label": 0
                },
                {
                    "sent": "So I mean there is a published draft for like a certain month report about variational learning.",
                    "label": 1
                },
                {
                    "sent": "That is associated with this method and also Seagate also discusses valuation methods for specific classification, all although for aggression he doesn't use abound and he selects the using points using greedy heuristic criteria.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is that the variational method can be provided with lower bounds, so this can be very useful as far as the joint learning of reducing points per parameter is concerned.",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some acknowledgments.",
                    "label": 0
                },
                {
                    "sent": "We now have benefit a lot from discussion from me alone as a magnet, so just hide the light near building more because he's really expert in this methods and he he gave you an amazing feeling.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not an author on this one.",
                    "label": 0
                },
                {
                    "sent": "I love this work and the reason Michalis gotta talk is 'cause he showed me this work and I said, oh, we've got to put that in the workshop, 'cause I think it's much in the tradition of the GPR T where a lot of these methods were discussed and for me it's the most significant advance on those sparse GP approaches that we've had since.",
                    "label": 0
                },
                {
                    "sent": "Maybe that workshop?",
                    "label": 0
                },
                {
                    "sent": "So you didn't show any results for the corrected 50?",
                    "label": 0
                },
                {
                    "sent": "Yeah this.",
                    "label": 0
                },
                {
                    "sent": "I haven't actually investigated, I mean.",
                    "label": 0
                },
                {
                    "sent": "Well this is not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not clear actually for the variational bound for the physical be it will do different things.",
                    "label": 0
                },
                {
                    "sent": "So it depends.",
                    "label": 0
                },
                {
                    "sent": "I guess the ability of physical model, heater, static noise it will reduce.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "You will be.",
                    "label": 0
                },
                {
                    "sent": "You should spread them using valid server.",
                    "label": 0
                },
                {
                    "sent": "Well so you will be more careful, careful about modeling the function correlation.",
                    "label": 0
                },
                {
                    "sent": "If you do run away but don't have actually complete conclusion about this, but that's nothing but.",
                    "label": 0
                },
                {
                    "sent": "Xbox video one reason?",
                    "label": 0
                },
                {
                    "sent": "Variational approach doesn't look so natural somehow is, I think because.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discussion.",
                    "label": 0
                },
                {
                    "sent": "Can be sort of driving will naturally by looking at KLE loving.",
                    "label": 0
                },
                {
                    "sent": "Looks like kind of after fix that by changing the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Here to get us to the very.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree with you.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not that I mean this violation formulation doesn't.",
                    "label": 0
                },
                {
                    "sent": "He's not naturalizing the CTS, but in general, but.",
                    "label": 0
                },
                {
                    "sent": "Of those below 50.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they will tend towards the true GT model like their disaster is only under control to the conditions and we do agree that if you're optimizing the high prices.",
                    "label": 0
                },
                {
                    "sent": "You have to be.",
                    "label": 0
                },
                {
                    "sent": "You have to accept that it's going to do certain things together, highlight it and it yeah.",
                    "label": 0
                },
                {
                    "sent": "Giving people points and hyperactive, it has more flexibility to do some things which maybe you know which supposedly wouldn't wouldn't do so.",
                    "label": 0
                },
                {
                    "sent": "It seems like your method is definitely more robust.",
                    "label": 0
                },
                {
                    "sent": "But I think I mean The thing is that this variation is this kind of constraint.",
                    "label": 0
                },
                {
                    "sent": "Better to feed the full CP I mean.",
                    "label": 1
                },
                {
                    "sent": "The physical exam and you might actually work better.",
                    "label": 0
                },
                {
                    "sent": "I mean, you might have better prediction error becausw.",
                    "label": 0
                },
                {
                    "sent": "Scratch inflexible so.",
                    "label": 0
                },
                {
                    "sent": "Yeah I I am.",
                    "label": 0
                },
                {
                    "sent": "I find it very, very interesting.",
                    "label": 0
                },
                {
                    "sent": "Thank you go.",
                    "label": 0
                },
                {
                    "sent": "Competition.",
                    "label": 1
                },
                {
                    "sent": "Processes and it sort of seems to imply that in the best case will be doing like a full GP, right?",
                    "label": 0
                },
                {
                    "sent": "But we see in practice that actually simply are simply a different model in a way, right?",
                    "label": 0
                },
                {
                    "sent": "And this makes I mean, I needed to say this makes empirical comparisons of computationally efficient methods extremely difficult, and some of us are actually struggling still.",
                    "label": 0
                },
                {
                    "sent": "Whether we can get something like that out because you don't really know what you're trying to get, Christmas would be, or you just want to come over.",
                    "label": 0
                },
                {
                    "sent": "Nude lived in like having dimensional space.",
                    "label": 0
                },
                {
                    "sent": "How how is scale?",
                    "label": 0
                },
                {
                    "sent": "You're replacing, yeah.",
                    "label": 0
                },
                {
                    "sent": "Optimization with additional teaching point.",
                    "label": 0
                },
                {
                    "sent": "That's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I mean there is an answer about this, but this other investigation I mean The thing is that you can construct much better optimization algorithm because now you optimize over in some high dimensional space right?",
                    "label": 0
                },
                {
                    "sent": "So one way to get much better Alchemist to utilize the training points.",
                    "label": 0
                },
                {
                    "sent": "So you should make line services between training points.",
                    "label": 0
                },
                {
                    "sent": "So you might say OK, I will initialize it using points to some subset of data.",
                    "label": 0
                },
                {
                    "sent": "Then I will take another subset I will mate.",
                    "label": 0
                },
                {
                    "sent": "It's point from one subject with another point from other subject and then I will.",
                    "label": 0
                },
                {
                    "sent": "I will make an optimization across the lines that connect this.",
                    "label": 0
                },
                {
                    "sent": "So we're actually kind of.",
                    "label": 0
                },
                {
                    "sent": "Investigate another thing.",
                    "label": 0
                },
                {
                    "sent": "It will scale well with the dimension, but there is no results yet, so I don't know.",
                    "label": 0
                },
                {
                    "sent": "The other answer to that is the subset of data use of the inducing input Nicholas Pass.",
                    "label": 0
                },
                {
                    "sent": "Then really nice what he showed briefly at the end there allows you to one of the reasons Ed was proposing fits is because you couldn't optimize your active set from the training data and the hyperparameter at the same time without your objective bouncing around.",
                    "label": 0
                },
                {
                    "sent": "The other really nice property that you guys didn't emphasize so much is if you choose decides to restrict yourself your training data.",
                    "label": 0
                },
                {
                    "sent": "You can continuously optimize your objective function without worrying about bouncing around.",
                    "label": 0
                },
                {
                    "sent": "And in a high dimensional space, that's probably the best thing to do was like this here.",
                    "label": 0
                },
                {
                    "sent": "Dimensions I mean the choice in choosing using models from the training set, maybe something?",
                    "label": 0
                },
                {
                    "sent": "We don't have the answers here, but.",
                    "label": 0
                },
                {
                    "sent": "Talked about this when you know this refers to when you can incorporate in you data point, you can expand your.",
                    "label": 0
                },
                {
                    "sent": "Juicy.",
                    "label": 0
                },
                {
                    "sent": "It means that this point is when you.",
                    "label": 0
                },
                {
                    "sent": "Actual criterion using there is no when they have the.",
                    "label": 0
                },
                {
                    "sent": "Variational contains black beans and compare against other peoples.",
                    "label": 0
                },
                {
                    "sent": "Criteria for inclusion appointments and bringing some idea maybe.",
                    "label": 0
                },
                {
                    "sent": "The trace term is actually for the for the case of regression, that race term is the IBM selection criteria.",
                    "label": 1
                },
                {
                    "sent": "It's just the accuracy of the.",
                    "label": 0
                },
                {
                    "sent": "Data points so that does sort of validate in some sense the IBM regression criterion, 'cause you're actually trying to minimize this term only by the points you include, but you're ignoring your effect on this term, so it's very easy.",
                    "label": 0
                },
                {
                    "sent": "Select selection.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean.",
                    "label": 0
                },
                {
                    "sent": "Attack.",
                    "label": 0
                },
                {
                    "sent": "It's hacking.",
                    "label": 0
                },
                {
                    "sent": "Last line classification is that straightforward.",
                    "label": 0
                },
                {
                    "sent": "No, I mean.",
                    "label": 0
                },
                {
                    "sent": "Better ways of doing I mean like?",
                    "label": 0
                },
                {
                    "sent": "Matthias, eager, and Moreover they have done.",
                    "label": 0
                },
                {
                    "sent": "I mean, you have to do to introduce another approximation.",
                    "label": 0
                },
                {
                    "sent": "I guess, like I don't know Laplace, but this is not the way that I would like to do it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I want to do it in a way that I will be going to optimize.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just I mean.",
                    "label": 0
                },
                {
                    "sent": "The variation approximation to that effectively truncated Gaussian, so it just becomes the Gaussian approximations about under the cube P is not good.",
                    "label": 0
                },
                {
                    "sent": "If you take enough combined with the Gaussian variational, approximate it with the Gaussian, the variance of the approximation has to be very low, which is why EP is such a better idea because it peak use rather than kewpies.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        }
    }
}