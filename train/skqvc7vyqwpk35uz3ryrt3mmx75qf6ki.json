{
    "id": "skqvc7vyqwpk35uz3ryrt3mmx75qf6ki",
    "title": "IntervalRank - Isotonic Regression with Listwise and Pairwise Constraints",
    "info": {
        "author": [
            "Taesup Moon, Yahoo! Research Silicon Valley"
        ],
        "published": "Oct. 12, 2010",
        "recorded": "February 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/wsdm2010_moon_irir/",
    "segmentation": [
        [
            "OK hi everyone, my name is Tessa Moon an I'll present a yet another general learning to rank algorithm called the interval rank or isotonic regression with least twice and pairwise constraint.",
            "And I'm from Yahoo Labs and this is a joint work with Alex smaller each young and Zoe Zhang also from Yahoo.",
            "Yesterday in the keynote speech I saw a scheme with exactly the same name, but please keep in mind that this is a different scheme."
        ],
        [
            "So that all consists of three parts are briefly introduced, the background of the problem, and then I'll explain how we derive the algorithm interval rank, and then I'll conclude with the experimental."
        ],
        [
            "It's."
        ],
        [
            "So learning to rank is.",
            "A very popular approach in web search.",
            "Essentially it's a supervised machine learning framework for ranking and roughly consists of three categories.",
            "First we need a relevance labeled data, which I denote as zyan.",
            "Why I and XI represents the feature vector for query each query document pair and why I is the corresponding relevance label for that pair given by the human editors.",
            "So given we have N such example, the learning to rank algorithm defines a loss function which I do notice as L for that data and the ranking score of a ranking function F where ranking function is a function that Maps each feature vector to real value score, so that if you sort with respect to that score, you get a ranking with this data and loss function.",
            "Learning to rank algorithm trained a ranking function F we are optimizing this loss function and one of the common approach for the optimization is the functional gradient descent.",
            "Which iteratively optimizes this ranking function.",
            "So long is 3 different categories.",
            "Alot of focus has been on this loss function just like as the previous."
        ],
        [
            "Log an in fact various loss function has been proposed.",
            "The simplest case is a pointwise loss function where you focus on each training example individually and simple regression on the relevance target is the example."
        ],
        [
            "And the other class is the pairwise loss function where you look at the.",
            "Relative ordering between 2 two pairs and then try to be as consistent as possible to that preference.",
            "Pairs and these schemes are the examples for those loss functions."
        ],
        [
            "And the third category is the least weight loss function, where you treat the whole document list jointly and then try to optimize.",
            "And these are the different schemes that apply these least why loss functions?",
            "I omitted the exact citation for this schemes, but for detail, please refer to the paper.",
            "And given these."
        ],
        [
            "For loss functions, one can ask a question whether whether one approach dominate the other other approaches and the common wisdom is that maybe the least wise loss function is better than the pairwise loss function and the pairwise loss function is better than the pointwise loss function.",
            "However, we can also see that the least least wireless function can also have some caveats.",
            "So for example, let's say this is a score distribution of the training example.",
            "Before we are training the ranking function and we."
        ],
        [
            "We use the least wise loss function.",
            "We may end up having the score distribution something like this because we we care about the whole list and try to enter this weight loss function.",
            "Care about the whole list and try to make the order."
        ],
        [
            "Of the examples, but maybe it may not assign the similar scores.",
            "To the similarly relevant documents.",
            "So from this."
        ],
        [
            "Yes.",
            "We again ask a question of whether we can actually mix these different approaches an have some better ranking performance, and ideally what we want is we want to train a function that has scored distribution on the training example, something like this."
        ],
        [
            "Or in other words, we want to separate the scores, separate the documents with the different relevance is and we also want to with some significant score gap.",
            "And we also want to cluster the documents the score scores of the documents that has similar relevance.",
            "So this."
        ],
        [
            "Motivates our work an.",
            "We so we derive our algorithm interval rank."
        ],
        [
            "So our approach is.",
            "So we again define a loss function, but implicitly we are some optimization problem called isotonic regression.",
            "By throwing in these constraints that we want and then we try to reformulate this optimization problem to efficiently compute the functional gradient of that front of the ranking functions so that we can iterate the function of grading."
        ],
        [
            "He said.",
            "So.",
            "So we define our loss as a minimum total efforts to make scores satisfy the constraint.",
            "So what does this mean?",
            "So first simplicity, let's consider now we have only two different relevance.",
            "Grace the blue, blue Grace and the red grace.",
            "An we we may want to like and suppose the blue grade is more relevant than the red grace and then we want the score distribution.",
            "So we want to.",
            "Put the red, the blue examples higher than the red examples.",
            "All of all of those pairs with at least some score gap Delta, which is that which we can think of as a difference of the relevance.",
            "The numeric values of the relevance labels that are given by human errors, and we also want to cluster all the blue examples close to get close enough and red example close enough.",
            "So if you see these example, however, we see that these examples does not satisfy this constraint.",
            "So what we can do is we can.",
            "Add some some offsets or."
        ],
        [
            "Or some deltas like this so that we can move all these red example to this part and blue examples to this part so that, like all these examples, all these pairs now satisfy those constraints, the separation and the clustering.",
            "But there are not unique such deltas.",
            "We can have many deltas that leads to this.",
            "This satisfy this can."
        ],
        [
            "And this is another example.",
            "So still for this case it satisfies the constraint.",
            "Therefore, we define our loss function as."
        ],
        [
            "As the minimum total effort.",
            "So here we define the loss function As for this an example for.",
            "From now on we can think of this N as the number of documents for associated with a single query.",
            "We can because we can paralyze too many different queries as well.",
            "So for those loss function we define the loss as a minimum L2 norm square of a Delta where Delta is in RN that satisfies these constraints.",
            "So the first line.",
            "Is among all ordered pairs.",
            "We want those deltas to separate these examples by some gap Delta large Delta and for all the tight pairs we want to cluster those different examples together.",
            "So in fact this optimization problem is called the isotonic regression."
        ],
        [
            "An it was first proposed by in a Allerton paper in 2008 by Zheng ET al."
        ],
        [
            "So we can see that this formulation has both pairwise constraint among different relevance grade levels and it also has some least twice objectives so that we think about the whole list at the same time and try to minimize the total effort."
        ],
        [
            "And what they did is they try to just solve this problem and obtain the optimal solution Delta star and just use it as a functional gradient of this ranking functions F and do the iteration.",
            "But this approach has some."
        ],
        [
            "Roblems"
        ],
        [
            "So first thing is that they didn't have any formal proof whether the solution of this this problem is actually the functional gradient of this F, so they don't have it was just heuristic motivate."
        ],
        [
            "Chen and also if you look at this scheme, we can see that this is not practical because essentially this is a quadratic program.",
            "With N variables, an N square constraints, so it requires over and cube complexity to solve this problem, and we have to solve this optimization problem for every query in every iteration, so it was not practical.",
            "So basically we try to address these two issues."
        ],
        [
            "So first thing is, we show that the optimal solution, Delta star of that optimization problem, is indeed the functional gradient with respect to this ranking score F. We show this by the LaGrange multiplier analysis of the convex optimization and this is shown in lemma two of the paper."
        ],
        [
            "And to to speed up our scheme or reduce the complexity, we basically reduce the number of variables of the optimization and find some equivalent simpler problem.",
            "So recall that original formulation had the end variables, the number of documents, and then over N squared constraints.",
            "But we make two observations."
        ],
        [
            "First, we can see that the Delta step make these examples satisfy the constraint.",
            "We can have just Delta that satisfy that.",
            "Make the example satisfied with the constraint with equality.",
            "Those deltas are enough.",
            "So again, if you look at this example.",
            "So for example, if you look at these two pairs, we add D2 and D4, then those two pairs satisfy the constraint."
        ],
        [
            "But this D2 under four also makes them to satisfy constraints.",
            "So still the gap between those two examples is equal to Delta, the large Delta, so.",
            "Those deltas are enough, and from this observation we can."
        ],
        [
            "Actually see that if we come up with the intervals for each relevance grade.",
            "So such as like this.",
            "So this red interval is for the red grade scores and then blue interval for the blue grade scores."
        ],
        [
            "So if we whatever in."
        ],
        [
            "That is, if you come up with some interval that we can actually obtain this Delta the effort for to make this examples to satisfy those constraint."
        ],
        [
            "So for example this.",
            "For this example we just subtract.",
            "We need to subtract this D3 and move it to the boundary of this red interval.",
            "So that is that is."
        ],
        [
            "Lisa constraint, and for this example it's already in the interval, so we don't need to add anything."
        ],
        [
            "And for this example, we can move to this boundary."
        ],
        [
            "So from this observation we can actually see that finding the minimal effort Delta star is can be obtained from the optimal intervals elstar an you start, which is the lower bound and upper bound for each grade interval.",
            "So those optimal intervals that lead to that this minimum minimum effort Delta Star."
        ],
        [
            "So."
        ],
        [
            "Any equation that can be given something like this.",
            "So once you have UL star and you start and then you can compute the Delta store."
        ],
        [
            "Therefore we can refill me that formulate our loss function as this so the loss function can now be expressed.",
            "Like this in terms of LGNUG and then the constraints can also be transformed like this between those endpoint of those grade intervals.",
            "But all of a sudden, if you look at this now, then we have reduced the optimization problem that has constant number of variables.",
            "Basically just a number of different grades and then the constant number of constraints.",
            "So, so that's good, but we can see that this is now no longer a quadratic program, but it's still convex problem, so we can still solve this problem efficiently."
        ],
        [
            "By applying some techniques from the convex optimization.",
            "So first we use the log barrier method to to remove this inequality constraint by log barriers and then we use LB FS or conjugate gradient method to solve this.",
            "On constrained optimization fast and to apply these techniques we need to compute the objective and the gradient for each each this interval points and that can be done by simply sorting this scores and the score square where it requires this N log N complexity."
        ],
        [
            "So to summarize, basically we first need to find our scheme first, finds the relevance grades for each, each relevance grace that leads to the minimum effort.",
            "And once we find that we regress all the examples to this intervals and find Delta star and that's the functional gradient.",
            "So we do the functional gradient descent with using those values.",
            "And finally we can also add a pointwise regression loss.",
            "Something like this so that it can give some absolute score information, especially when there are not so many documents associated with the query, so that these intervals can now kind of centered around this absolute absolute scores that are given by.",
            "The editors."
        ],
        [
            "OK, so we did some experiment with this scheme on the date."
        ],
        [
            "So first experiment is again the standard letter 3.00.",
            "She met data which had 106 queries and 16,000 query document pairs and we again did the five fold cross validation by randomly splitting.",
            "These data to training, validation and test set and we use the functional gradient boosting trees as an example of the functional gradient descent.",
            "And we also added some slight variations by adding some slack variables in the in the constraints, because the labels given by the errors can be somewhat noisy, so we kind of relax this constraint by adding this slack variables, But again it does not affect the solving the optimization problem, and these are the parameters that."
        ],
        [
            "It used.",
            "So this is the end DCG result compared to all the baselines.",
            "So we can see that our scheme is specifically good at the top portion of the of the list."
        ],
        [
            "And this is also the precision at K result of our scheme.",
            "So at this point we don't have the systematic explanation why our scheme is especially stronger at the top portion of the list, but this is what we observe."
        ],
        [
            "In this data set, and then we also use the commercial search engine data.",
            "Where the training set has now 5000 queries and 340 query document pairs and test that has 900 queries and 32 K pairs and it also had five different relevance judgments by the human editors.",
            "We also use similarly as before functional gradient boosting trees an added some slack variables, and these are the parameters that we use.",
            "And we compared our scheme with this three different schemes, one using the pointwise loss function.",
            "The regress regression function and also using the pairwise G rank, which uses a pairwise loss function, at least Emily, which uses the least wise loss function.",
            "And we also compare with with or without this, adding regression term for these two schemes.",
            "And first of all, the running time of our scheme was was in the same range of this day."
        ],
        [
            "And schemes so thereby reducing the complexity of our scheme.",
            "Now our interval rank is in the same range of all other.",
            "Learning to rank schemes.",
            "Although we are solving this optimization problem at each iteration for each query and this is the NDC one result an yeah this this yellow line is the result with the additional regression term.",
            "So we can see it's it's above all other schemes."
        ],
        [
            "And this is the NBC 5 result.",
            "Which are where our scheme also?",
            "Stay always on top of other ski."
        ],
        [
            "So we get.",
            "We gain up to 1% over other methods or the 1% doesn't seem.",
            "Very big, but you know, in the web search data we we think that 1% gain is like significant gain that we get.",
            "So this concludes my talk.",
            "Thank you very much.",
            "Did you just compare this with something like 2 and severities support vector ordinal regression?",
            "So yeah, support vector Russian.",
            "I mean rank SVM.",
            "No, no.",
            "I meant support vector.",
            "Ordinal regression is the faster formulation by two and security.",
            "Several years back, huh?",
            "Yeah, we didn't compare with the ordinal regression scheme, so it seems like there are two possible places where you might win.",
            "One is that there are primarily dealing with the linear decision surface, whereas you can be with non linearity.",
            "The other seems like you have.",
            "They probably couldn't take care of Listwise losses, so it would be nice to a see if you're better and we somehow break that up into those two reasons.",
            "Yeah, OK, wanted to.",
            "Yeah, we haven't compared with those kids, but.",
            "Yeah, it's a good suggestion.",
            "Thank a quick comment.",
            "Actually it appears to me that the the list was a loss function you mentioned is not solely size, so it is the L2 norm of the Delta vector, which appears to me to be a pair wise one.",
            "What do you think about that so?",
            "Yeah, in fact, like Leastwise in the sense of, you know like optimizing N DCG, where you will the higher ranking position very obvious in the final result, yes, but I think we can also incorporate that that waiting thing to our loss function as well, but we haven't done it here.",
            "And yeah, I agree like we are kind of.",
            "Try to just minimize the norm, but still is spread across the whole list.",
            "Yeah, so another related question is very similar to that from soul.",
            "Man, you know when you compare your method with the baselines.",
            "As far as I know, most of the baselines on the latter data set using linear model, but you are using a gradient boosting tree, right?",
            "So the models are quite different.",
            "How do you compare with their experience?",
            "Performance is based on different basis.",
            "Yes, for for that or actually we we just look at the numbers that are given by the baseline.",
            "I mean baseline performance in the data.",
            "Yes yes yeah.",
            "So maybe the more fair comparison is I guess on this.",
            "Like even the larger data set with some other using the same same great interesting trees.",
            "Just like at the commercial search engine data.",
            "OK. Just sent this week I guess."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK hi everyone, my name is Tessa Moon an I'll present a yet another general learning to rank algorithm called the interval rank or isotonic regression with least twice and pairwise constraint.",
                    "label": 0
                },
                {
                    "sent": "And I'm from Yahoo Labs and this is a joint work with Alex smaller each young and Zoe Zhang also from Yahoo.",
                    "label": 1
                },
                {
                    "sent": "Yesterday in the keynote speech I saw a scheme with exactly the same name, but please keep in mind that this is a different scheme.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that all consists of three parts are briefly introduced, the background of the problem, and then I'll explain how we derive the algorithm interval rank, and then I'll conclude with the experimental.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So learning to rank is.",
                    "label": 1
                },
                {
                    "sent": "A very popular approach in web search.",
                    "label": 1
                },
                {
                    "sent": "Essentially it's a supervised machine learning framework for ranking and roughly consists of three categories.",
                    "label": 1
                },
                {
                    "sent": "First we need a relevance labeled data, which I denote as zyan.",
                    "label": 0
                },
                {
                    "sent": "Why I and XI represents the feature vector for query each query document pair and why I is the corresponding relevance label for that pair given by the human editors.",
                    "label": 0
                },
                {
                    "sent": "So given we have N such example, the learning to rank algorithm defines a loss function which I do notice as L for that data and the ranking score of a ranking function F where ranking function is a function that Maps each feature vector to real value score, so that if you sort with respect to that score, you get a ranking with this data and loss function.",
                    "label": 0
                },
                {
                    "sent": "Learning to rank algorithm trained a ranking function F we are optimizing this loss function and one of the common approach for the optimization is the functional gradient descent.",
                    "label": 1
                },
                {
                    "sent": "Which iteratively optimizes this ranking function.",
                    "label": 0
                },
                {
                    "sent": "So long is 3 different categories.",
                    "label": 0
                },
                {
                    "sent": "Alot of focus has been on this loss function just like as the previous.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Log an in fact various loss function has been proposed.",
                    "label": 0
                },
                {
                    "sent": "The simplest case is a pointwise loss function where you focus on each training example individually and simple regression on the relevance target is the example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other class is the pairwise loss function where you look at the.",
                    "label": 1
                },
                {
                    "sent": "Relative ordering between 2 two pairs and then try to be as consistent as possible to that preference.",
                    "label": 0
                },
                {
                    "sent": "Pairs and these schemes are the examples for those loss functions.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third category is the least weight loss function, where you treat the whole document list jointly and then try to optimize.",
                    "label": 1
                },
                {
                    "sent": "And these are the different schemes that apply these least why loss functions?",
                    "label": 0
                },
                {
                    "sent": "I omitted the exact citation for this schemes, but for detail, please refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "And given these.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For loss functions, one can ask a question whether whether one approach dominate the other other approaches and the common wisdom is that maybe the least wise loss function is better than the pairwise loss function and the pairwise loss function is better than the pointwise loss function.",
                    "label": 1
                },
                {
                    "sent": "However, we can also see that the least least wireless function can also have some caveats.",
                    "label": 1
                },
                {
                    "sent": "So for example, let's say this is a score distribution of the training example.",
                    "label": 0
                },
                {
                    "sent": "Before we are training the ranking function and we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use the least wise loss function.",
                    "label": 0
                },
                {
                    "sent": "We may end up having the score distribution something like this because we we care about the whole list and try to enter this weight loss function.",
                    "label": 0
                },
                {
                    "sent": "Care about the whole list and try to make the order.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the examples, but maybe it may not assign the similar scores.",
                    "label": 1
                },
                {
                    "sent": "To the similarly relevant documents.",
                    "label": 0
                },
                {
                    "sent": "So from this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We again ask a question of whether we can actually mix these different approaches an have some better ranking performance, and ideally what we want is we want to train a function that has scored distribution on the training example, something like this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or in other words, we want to separate the scores, separate the documents with the different relevance is and we also want to with some significant score gap.",
                    "label": 1
                },
                {
                    "sent": "And we also want to cluster the documents the score scores of the documents that has similar relevance.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivates our work an.",
                    "label": 0
                },
                {
                    "sent": "We so we derive our algorithm interval rank.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is.",
                    "label": 0
                },
                {
                    "sent": "So we again define a loss function, but implicitly we are some optimization problem called isotonic regression.",
                    "label": 1
                },
                {
                    "sent": "By throwing in these constraints that we want and then we try to reformulate this optimization problem to efficiently compute the functional gradient of that front of the ranking functions so that we can iterate the function of grading.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He said.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we define our loss as a minimum total efforts to make scores satisfy the constraint.",
                    "label": 1
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So first simplicity, let's consider now we have only two different relevance.",
                    "label": 0
                },
                {
                    "sent": "Grace the blue, blue Grace and the red grace.",
                    "label": 0
                },
                {
                    "sent": "An we we may want to like and suppose the blue grade is more relevant than the red grace and then we want the score distribution.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "Put the red, the blue examples higher than the red examples.",
                    "label": 0
                },
                {
                    "sent": "All of all of those pairs with at least some score gap Delta, which is that which we can think of as a difference of the relevance.",
                    "label": 0
                },
                {
                    "sent": "The numeric values of the relevance labels that are given by human errors, and we also want to cluster all the blue examples close to get close enough and red example close enough.",
                    "label": 0
                },
                {
                    "sent": "So if you see these example, however, we see that these examples does not satisfy this constraint.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can.",
                    "label": 0
                },
                {
                    "sent": "Add some some offsets or.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or some deltas like this so that we can move all these red example to this part and blue examples to this part so that, like all these examples, all these pairs now satisfy those constraints, the separation and the clustering.",
                    "label": 0
                },
                {
                    "sent": "But there are not unique such deltas.",
                    "label": 0
                },
                {
                    "sent": "We can have many deltas that leads to this.",
                    "label": 0
                },
                {
                    "sent": "This satisfy this can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is another example.",
                    "label": 0
                },
                {
                    "sent": "So still for this case it satisfies the constraint.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we define our loss function as.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As the minimum total effort.",
                    "label": 1
                },
                {
                    "sent": "So here we define the loss function As for this an example for.",
                    "label": 0
                },
                {
                    "sent": "From now on we can think of this N as the number of documents for associated with a single query.",
                    "label": 0
                },
                {
                    "sent": "We can because we can paralyze too many different queries as well.",
                    "label": 0
                },
                {
                    "sent": "So for those loss function we define the loss as a minimum L2 norm square of a Delta where Delta is in RN that satisfies these constraints.",
                    "label": 0
                },
                {
                    "sent": "So the first line.",
                    "label": 1
                },
                {
                    "sent": "Is among all ordered pairs.",
                    "label": 1
                },
                {
                    "sent": "We want those deltas to separate these examples by some gap Delta large Delta and for all the tight pairs we want to cluster those different examples together.",
                    "label": 0
                },
                {
                    "sent": "So in fact this optimization problem is called the isotonic regression.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An it was first proposed by in a Allerton paper in 2008 by Zheng ET al.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see that this formulation has both pairwise constraint among different relevance grade levels and it also has some least twice objectives so that we think about the whole list at the same time and try to minimize the total effort.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what they did is they try to just solve this problem and obtain the optimal solution Delta star and just use it as a functional gradient of this ranking functions F and do the iteration.",
                    "label": 0
                },
                {
                    "sent": "But this approach has some.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roblems",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first thing is that they didn't have any formal proof whether the solution of this this problem is actually the functional gradient of this F, so they don't have it was just heuristic motivate.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chen and also if you look at this scheme, we can see that this is not practical because essentially this is a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "With N variables, an N square constraints, so it requires over and cube complexity to solve this problem, and we have to solve this optimization problem for every query in every iteration, so it was not practical.",
                    "label": 0
                },
                {
                    "sent": "So basically we try to address these two issues.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first thing is, we show that the optimal solution, Delta star of that optimization problem, is indeed the functional gradient with respect to this ranking score F. We show this by the LaGrange multiplier analysis of the convex optimization and this is shown in lemma two of the paper.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to to speed up our scheme or reduce the complexity, we basically reduce the number of variables of the optimization and find some equivalent simpler problem.",
                    "label": 1
                },
                {
                    "sent": "So recall that original formulation had the end variables, the number of documents, and then over N squared constraints.",
                    "label": 0
                },
                {
                    "sent": "But we make two observations.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we can see that the Delta step make these examples satisfy the constraint.",
                    "label": 0
                },
                {
                    "sent": "We can have just Delta that satisfy that.",
                    "label": 0
                },
                {
                    "sent": "Make the example satisfied with the constraint with equality.",
                    "label": 0
                },
                {
                    "sent": "Those deltas are enough.",
                    "label": 0
                },
                {
                    "sent": "So again, if you look at this example.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at these two pairs, we add D2 and D4, then those two pairs satisfy the constraint.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this D2 under four also makes them to satisfy constraints.",
                    "label": 0
                },
                {
                    "sent": "So still the gap between those two examples is equal to Delta, the large Delta, so.",
                    "label": 0
                },
                {
                    "sent": "Those deltas are enough, and from this observation we can.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually see that if we come up with the intervals for each relevance grade.",
                    "label": 0
                },
                {
                    "sent": "So such as like this.",
                    "label": 0
                },
                {
                    "sent": "So this red interval is for the red grade scores and then blue interval for the blue grade scores.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we whatever in.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, if you come up with some interval that we can actually obtain this Delta the effort for to make this examples to satisfy those constraint.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example this.",
                    "label": 0
                },
                {
                    "sent": "For this example we just subtract.",
                    "label": 0
                },
                {
                    "sent": "We need to subtract this D3 and move it to the boundary of this red interval.",
                    "label": 0
                },
                {
                    "sent": "So that is that is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lisa constraint, and for this example it's already in the interval, so we don't need to add anything.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for this example, we can move to this boundary.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from this observation we can actually see that finding the minimal effort Delta star is can be obtained from the optimal intervals elstar an you start, which is the lower bound and upper bound for each grade interval.",
                    "label": 0
                },
                {
                    "sent": "So those optimal intervals that lead to that this minimum minimum effort Delta Star.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any equation that can be given something like this.",
                    "label": 0
                },
                {
                    "sent": "So once you have UL star and you start and then you can compute the Delta store.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore we can refill me that formulate our loss function as this so the loss function can now be expressed.",
                    "label": 0
                },
                {
                    "sent": "Like this in terms of LGNUG and then the constraints can also be transformed like this between those endpoint of those grade intervals.",
                    "label": 0
                },
                {
                    "sent": "But all of a sudden, if you look at this now, then we have reduced the optimization problem that has constant number of variables.",
                    "label": 0
                },
                {
                    "sent": "Basically just a number of different grades and then the constant number of constraints.",
                    "label": 0
                },
                {
                    "sent": "So, so that's good, but we can see that this is now no longer a quadratic program, but it's still convex problem, so we can still solve this problem efficiently.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By applying some techniques from the convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So first we use the log barrier method to to remove this inequality constraint by log barriers and then we use LB FS or conjugate gradient method to solve this.",
                    "label": 0
                },
                {
                    "sent": "On constrained optimization fast and to apply these techniques we need to compute the objective and the gradient for each each this interval points and that can be done by simply sorting this scores and the score square where it requires this N log N complexity.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, basically we first need to find our scheme first, finds the relevance grades for each, each relevance grace that leads to the minimum effort.",
                    "label": 1
                },
                {
                    "sent": "And once we find that we regress all the examples to this intervals and find Delta star and that's the functional gradient.",
                    "label": 1
                },
                {
                    "sent": "So we do the functional gradient descent with using those values.",
                    "label": 0
                },
                {
                    "sent": "And finally we can also add a pointwise regression loss.",
                    "label": 1
                },
                {
                    "sent": "Something like this so that it can give some absolute score information, especially when there are not so many documents associated with the query, so that these intervals can now kind of centered around this absolute absolute scores that are given by.",
                    "label": 0
                },
                {
                    "sent": "The editors.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we did some experiment with this scheme on the date.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first experiment is again the standard letter 3.00.",
                    "label": 0
                },
                {
                    "sent": "She met data which had 106 queries and 16,000 query document pairs and we again did the five fold cross validation by randomly splitting.",
                    "label": 1
                },
                {
                    "sent": "These data to training, validation and test set and we use the functional gradient boosting trees as an example of the functional gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And we also added some slight variations by adding some slack variables in the in the constraints, because the labels given by the errors can be somewhat noisy, so we kind of relax this constraint by adding this slack variables, But again it does not affect the solving the optimization problem, and these are the parameters that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It used.",
                    "label": 0
                },
                {
                    "sent": "So this is the end DCG result compared to all the baselines.",
                    "label": 0
                },
                {
                    "sent": "So we can see that our scheme is specifically good at the top portion of the of the list.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is also the precision at K result of our scheme.",
                    "label": 0
                },
                {
                    "sent": "So at this point we don't have the systematic explanation why our scheme is especially stronger at the top portion of the list, but this is what we observe.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this data set, and then we also use the commercial search engine data.",
                    "label": 1
                },
                {
                    "sent": "Where the training set has now 5000 queries and 340 query document pairs and test that has 900 queries and 32 K pairs and it also had five different relevance judgments by the human editors.",
                    "label": 0
                },
                {
                    "sent": "We also use similarly as before functional gradient boosting trees an added some slack variables, and these are the parameters that we use.",
                    "label": 1
                },
                {
                    "sent": "And we compared our scheme with this three different schemes, one using the pointwise loss function.",
                    "label": 1
                },
                {
                    "sent": "The regress regression function and also using the pairwise G rank, which uses a pairwise loss function, at least Emily, which uses the least wise loss function.",
                    "label": 0
                },
                {
                    "sent": "And we also compare with with or without this, adding regression term for these two schemes.",
                    "label": 0
                },
                {
                    "sent": "And first of all, the running time of our scheme was was in the same range of this day.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And schemes so thereby reducing the complexity of our scheme.",
                    "label": 0
                },
                {
                    "sent": "Now our interval rank is in the same range of all other.",
                    "label": 1
                },
                {
                    "sent": "Learning to rank schemes.",
                    "label": 0
                },
                {
                    "sent": "Although we are solving this optimization problem at each iteration for each query and this is the NDC one result an yeah this this yellow line is the result with the additional regression term.",
                    "label": 0
                },
                {
                    "sent": "So we can see it's it's above all other schemes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the NBC 5 result.",
                    "label": 0
                },
                {
                    "sent": "Which are where our scheme also?",
                    "label": 0
                },
                {
                    "sent": "Stay always on top of other ski.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we get.",
                    "label": 0
                },
                {
                    "sent": "We gain up to 1% over other methods or the 1% doesn't seem.",
                    "label": 1
                },
                {
                    "sent": "Very big, but you know, in the web search data we we think that 1% gain is like significant gain that we get.",
                    "label": 0
                },
                {
                    "sent": "So this concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Did you just compare this with something like 2 and severities support vector ordinal regression?",
                    "label": 0
                },
                {
                    "sent": "So yeah, support vector Russian.",
                    "label": 0
                },
                {
                    "sent": "I mean rank SVM.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "I meant support vector.",
                    "label": 0
                },
                {
                    "sent": "Ordinal regression is the faster formulation by two and security.",
                    "label": 0
                },
                {
                    "sent": "Several years back, huh?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we didn't compare with the ordinal regression scheme, so it seems like there are two possible places where you might win.",
                    "label": 0
                },
                {
                    "sent": "One is that there are primarily dealing with the linear decision surface, whereas you can be with non linearity.",
                    "label": 0
                },
                {
                    "sent": "The other seems like you have.",
                    "label": 0
                },
                {
                    "sent": "They probably couldn't take care of Listwise losses, so it would be nice to a see if you're better and we somehow break that up into those two reasons.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, wanted to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we haven't compared with those kids, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good suggestion.",
                    "label": 0
                },
                {
                    "sent": "Thank a quick comment.",
                    "label": 0
                },
                {
                    "sent": "Actually it appears to me that the the list was a loss function you mentioned is not solely size, so it is the L2 norm of the Delta vector, which appears to me to be a pair wise one.",
                    "label": 0
                },
                {
                    "sent": "What do you think about that so?",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact, like Leastwise in the sense of, you know like optimizing N DCG, where you will the higher ranking position very obvious in the final result, yes, but I think we can also incorporate that that waiting thing to our loss function as well, but we haven't done it here.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I agree like we are kind of.",
                    "label": 0
                },
                {
                    "sent": "Try to just minimize the norm, but still is spread across the whole list.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so another related question is very similar to that from soul.",
                    "label": 0
                },
                {
                    "sent": "Man, you know when you compare your method with the baselines.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, most of the baselines on the latter data set using linear model, but you are using a gradient boosting tree, right?",
                    "label": 0
                },
                {
                    "sent": "So the models are quite different.",
                    "label": 0
                },
                {
                    "sent": "How do you compare with their experience?",
                    "label": 0
                },
                {
                    "sent": "Performance is based on different basis.",
                    "label": 0
                },
                {
                    "sent": "Yes, for for that or actually we we just look at the numbers that are given by the baseline.",
                    "label": 1
                },
                {
                    "sent": "I mean baseline performance in the data.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So maybe the more fair comparison is I guess on this.",
                    "label": 0
                },
                {
                    "sent": "Like even the larger data set with some other using the same same great interesting trees.",
                    "label": 1
                },
                {
                    "sent": "Just like at the commercial search engine data.",
                    "label": 0
                },
                {
                    "sent": "OK. Just sent this week I guess.",
                    "label": 0
                }
            ]
        }
    }
}