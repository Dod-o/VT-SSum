{
    "id": "xow2phta7dfbuqdb35xpc6ng5qfal5k2",
    "title": "Geometric Tools for Graph Mining of Large Social and Information Networks",
    "info": {
        "author": [
            "Michael Mahoney, Department of Computer Science, Stanford University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining->Graph Mining",
            "Top->Computer Science->Spatial Data Structures"
        ]
    },
    "url": "http://videolectures.net/kdd2010_mahoney_gtgm/",
    "segmentation": [
        [
            "Basically, about a set of tools that maybe you can use to examine the structural properties of large social information graphs.",
            "And this actually grew out of a couple of particular applied problems that I'll mention in Internet advertising.",
            "But I think that the tools maybe are sort of much more general and part of the reason is that I think.",
            "If you want to know, sort of roughly with these sorts of networks, look like I mentioned in quite a bit more detail the types of graphs we're talking about, but a fairly wide range of things that might fit under the umbrella of social and information networks.",
            "They're actually pretty complicated things, as as you may know, if you work with them.",
            "I mean are generated.",
            "There tend to be fairly large, so there's a premium at on things that scale well algorithmically.",
            "They tend to be fairly sparse, so there's going to be a premium on sort of regularization, regularity, or statistical properties there oftentimes generated in complex or adversarial environments, which means that a lot of the models that you might want to use in other applications or not sort of particularly appropriate.",
            "And I think that for those reasons as well as others, I mean some of the more more straightforward techniques that have been used over the years, especially as some of these networks came to be.",
            "And in the 90s and early, you know, last 10 years tend to perform less well, and so the question is, maybe you know, how can we try and understand these graphs?",
            "So what I'd like to do is described in a sense, basically, and sometimes the tools that.",
            "That we had to invent in order to understand the structural properties of these graphs.",
            "So there's a lot of stuff in the slides, and I'll gloss over some of it, and then some will go into a bit more detail.",
            "These slides are actually on the web.",
            "I was giving this talk at ICML about a month ago.",
            "There's going to be a few typos that we picked up last time, and they may pick up this time.",
            "Then I'll correct those, but these these are actually on the web now.",
            "The uncorrected versions and you can download them, so I'll skip over some things, but you can look at those if you want, and if those are the particular things that are less interested in, that's OK too, so."
        ],
        [
            "There's lots and lots of networks out there.",
            "Technological and communication networks, power systems, and autonomous autonomous systems, and Rd networks.",
            "For instance, biological networks of various sorts.",
            "What I'll call social information graphs.",
            "So let's call it say, collaboration, networks, Friendship Networks, blog cross postings, advertiser bid phrase graphs.",
            "So this is actually the application of this stuff arose.",
            "Maybe financial economic network, so on so forth.",
            "So lots of sort of a popular way to think about.",
            "Data these days and just to give you an example of the types of networks."
        ],
        [
            "That you should have in the back of your mind.",
            "There was two there.",
            "As I mentioned those a couple of particular applications that gave rise to this line of work.",
            "We notice a number of fairly surprising and counterintuitive properties, and then if in fact those were the case, it couldn't have been that they were peculiar to the two advertiser bid phrase graphs were looking at, so we went back and looked at on the order of 70 more.",
            "So this is 10 of those 70.",
            "They aren't particularly large or small, but maybe a representative sample.",
            "The idea is that once you get beyond several 100 nodes or thousand nodes, things will change very very.",
            "Much relative to much smaller networks that are studied in the Fort Networks literature almost exclusively, once you get up to 100 million edges or whatever, depending on the albums you want to run, it gets to be hard to put it on a single machine.",
            "And if that's not true, now is true.",
            "Few years ago, it depends on the size of your memory, but you start to worry about memory management issues.",
            "So if you want to scale up an order of magnitude larger than that, you have started well managed memory management issue.",
            "So think about this is between couple thousand a couple million nodes, so something under 100 million edges so for.",
            "Instance something from live Journal, 4 million nodes, 40,000,000 edges.",
            "So that's typical of size and maybe a sparsity.",
            "So extremely extremely sparse.",
            "Epinions Flickr, delicious, a bunch of coauthorship networks in physics and DBL physics and computer science Citation networks blog crossposting think of these sort of something capturing the way information propagates.",
            "I'm going to draw loose distinction between.",
            "A social and information, but that's sort of a loose distinction.",
            "Snapshots of the web, bipartite affiliation graphs, office to papers or advertise, betrays a bunch of Internet graphs."
        ],
        [
            "So just to give you an example.",
            "Of how this arose, so you're at Yahoo.",
            "or Google, or Bing or whatever.",
            "Yeah, determined the search box.",
            "You get your spot.",
            "You get your algorithmic results and you get your your advertisements.",
            "So how do you determine the advertisement?",
            "So as you probably know, that's a very nontrivial thing.",
            "A lot of people spend a lot of time thinking about that.",
            "So in the context of constructing networks, how would you maybe construct a graph from this?",
            "Situation and what do you want to do with that graph so?"
        ],
        [
            "Just as an example, you might want to.",
            "I guess the color is not too good on this thing, but you might want to construct an advertiser keyword.",
            "Bipartite graphs on the left hand side, this time discretization of the universe of advertisers on the right hand side of keywords, and you want to maybe do KDD or do some sort of knowledge discovery data mining on this sort of graph?",
            "In order to what in order to provide new advertisements to maximize, click through rates or ROI or some objective function of interest.",
            "And so we're going to do this, you're going to quantify something this imperfect, but you can measure, and you're going to see how your algorithm performs.",
            "So one of the most basic things you can ask about a graph or network or data set more generally.",
            "So how does it cluster?",
            "You have to split into two or more pieces, and So what are clustering or community related problems that you might arise that might arise in this particular setting?",
            "So you might want to know something called marketplace depth broadening.",
            "Find new advertisers for particular query of submarket.",
            "You might want to do something like query recommender system suggested to advertisers.",
            "Query that have high probability of clicks.",
            "You might want to do, let's call it contextual query broadening.",
            "You might want to broaden the users query in some sense and there's a range of reasons why you might want to do this.",
            "So the two applications that gave Rise rise this line of work.",
            "One was basically something with query expansion.",
            "You want to not.",
            "You do not have the advisor bit under single word, but maybe bid on this word in words like this.",
            "Whatever that means, and so there's a range of ways to determine what that means.",
            "Model in natural language instead of economic and strategic issues in terms of the auction.",
            "So let's gloss that over and just ask in terms of the abstraction we have.",
            "What does that mean?",
            "And this gets back to the proverbial example that you're at the middle of a concept class.",
            "Maybe you can go to nearby queries and you'll get sort of a meaningful thing so you can bid not just on this one or that you thought it but nearby ones.",
            "Whereas if you're in the middle of two clusters like Jaguar, doing the expansion is going to be a little less robust because you don't know.",
            "Which of the two meanings of the user might be intending?",
            "So we wanted to broadening and so you want to plug natural clusters.",
            "Another thing might be you want to do some sort of bucket testing.",
            "Again, they sort of proverbial examples.",
            "You want to pull up Palo Alto Flowers or San Jose Flowers because you want to pull out the flower market?",
            "Do.",
            "Sort of behavioral targeting.",
            "So can you pull that out in a meaningful way?",
            "Can you pull out a cluster of nodes that in some sense is a micro market?",
            "In order to do whatever but in particular to bucket testing."
        ],
        [
            "So.",
            "The goal then is to find isolated markets.",
            "Are clusters saying this advertise a bid phrase graph with sufficient money or clicks sufficient size that it's worth your time and sufficient coherence that you can pull it out and think about it as an economic market and you can reason about it and bring intuition to bear and you can try and say something meaningful in terms of the results of the bucket test.",
            "One thing that's not usually asked is this even possible right?",
            "I mean both in the academic literature, writing a paper, and if you're trying to push something into production, if you go and tell your manager or the reviewer as well, we couldn't find it, they're going to say well go find something else.",
            "Alright, so there's this sort of a selection bias to getting positive results, and so the usual Mo will be defined.",
            "Some objective trying to pull something out if you pull something out, either write the paper pushing to production and if you fail to find something, define a slightly new objective look at a slightly different data set, that sort of thing.",
            "So what we want to do?",
            "One of the things we're going to see is some of the tools will develop.",
            "Will actually even be able to answer this question.",
            "Is this even possible?",
            "Namely, you know, is the hypothesis of the data, looks certain ways?",
            "Is that even true?",
            "Is that something we can even test?",
            "So what's one way that we might want to?",
            "Think about the data.",
            "So one way you might think about the data is that the data looks something like this.",
            "There's a gambling market.",
            "And there's a sports market, and maybe there's an intersection of the sports and gambling.",
            "Let's call it a sports market, some sort of hierarchical structure in the data.",
            "And.",
            "When we're drawing the graph like this, when we're putting it this sort of Samantha when putting this on the piece of paper, what we're saying is that you know there's a boundary here.",
            "It may be imperfect and idealization with some sort of boundary between the gambling market and the rest of the world at the sports market.",
            "In the rest of the world, that's pretty good.",
            "In the sense that we can pull that cluster out and think about these keywords as being more gambling like in these ones being less so.",
            "And if the graph happened to have certain properties, namely you could embed it on a piece of paper, then that would be true.",
            "But if the graph happened to be an expander or a tree, or something else like that, then it probably would not be true.",
            "And if the graph happened to be more realistic graph where it's messy and it's hard to determine which of those sort of Nelson models or reference states, it looks like, how will you even answer that question so when you draw this, you're thinking that you know you have some sort of nice clustering structure that the data might fit well in a low dimensional space, and then you can think about it and its wonders.",
            "You even if you don't aren't actually.",
            "Saying, I don't want the data to bed well in a low dimensional space or something like that.",
            "A lot of the data analysis tools that you might bring to bear on the problem sort of implicitly make that assumption.",
            "So for instance, if you're going to be doing a recursive decomposition.",
            "You split it in half two pieces and recurse with dynamic programming or any other range of things which you want.",
            "Is that when you split it in half it's 5050 or 6040 or something like that.",
            "You don't want to be pulling off 100 nodes out of the million node graph cousin.",
            "The recursion depth will be very big and you can split into two pieces.",
            "You have a left half and you have a right half and you have something in between stable left half of the gambling market in the right half and something in between.",
            "So a lot of tools that you might use will implicitly be assuming that the data looks something like this.",
            "So it's an empirical question, does the data actually look that way so that we?"
        ],
        [
            "You talking bout that?",
            "So there's a range of ways you know you can represent the graph, and we're going to be talking about a particularly simple idealization.",
            "We're going to be talking about the network effectively as a graph, as a bunch of nodes or nodes represent some sort of entities in a bunch of edges or edges represent some sort of interaction between the entities.",
            "Alright, so in particular you can talk about directed graph showing directed graphs.",
            "Today everything we say will be able to be handled by weighted graphs or non weighted graphs very gracefully.",
            "Directed this is a tricky thing that's an open problem and time evolution.",
            "Some of these structural results that we will be talking about will shed light on time evolution, but that's sort of a different in terms of modeling the data.",
            "That's a different thing, and we're not going to talk about that particular today, but some of the results, as I said, will be relevant for that.",
            "But in terms of ways you want to wait, the graph is no, no issues there.",
            "And the first thing to note is that graphs combinatorial and not obviously geometric things, right?",
            "I mean, if the graph happened to be constructed as a discretization of a nice place or geometric random graph or something.",
            "Then it you can may be interpreted as being geometric, but you know if I give you an arbitrary graph, it's not.",
            "Obviously there's any geometry there, so you might ask what are we even talking about.",
            "So the good thing is that you know you have.",
            "That's a powerful framework for thinking about a lot of algorithmic questions in terms of complexity.",
            "But the drawback is that you know if you're familiar with machine learning, geometry is really used in a very important way for doing learning and things like statistical inference.",
            "And when you're looking at the graph, you don't necessarily want to be making the proverbial sort of beer diapers connection, because that's a high frequency itemset, and that's probably already been found.",
            "What you want to do is do something like make a prediction as to whether this user in a given zip code at a given time of day and.",
            "Given gender will click on a given at if a given size and it has a certain number of bells and whistles, and at that level of granularity, that sort of things probably never even occurred in the database, never mind be be a frequent itemset, and so you want to be doing some sort of implicit statistical model or or explicit statistical modeling and so understanding these inference questions is going to be essential."
        ],
        [
            "So what we're thinking about?",
            "So again, a schematic illustration of a hierarchical graph is this, and this is basically the same graph that we saw two slides back.",
            "You know we have one cluster, another cluster, and they coupled together in some way.",
            "And you know, you might see this sort of figure in in papers.",
            "We have some clump here, and there's lots of cross talk, and this figure actually cut from an EPS file cut from somewhere, and the the benefit of doing a couple of cuttings is that a lot of the off diagonal stuff with the level of granularity that you see is sort of glossed over, and so it highlights the structure that I'm hoping that it is actually there.",
            "So when you see some structure like this with a lot of cross talk, is this evidence for this clusters there, or is that there's maybe clusters, but they're pretty tenuous.",
            "Yeah, I think this is an adjacency matrix and we set the ordering there in some way.",
            "You know I don't need to tell you 'cause I could have chosen one of 10 and we've gotten some similar clusters, yeah?",
            "No, think of this.",
            "I have, you know the discretization of the universe of.",
            "Advertisers and a discretization of the universe of queries.",
            "It's not obvious how to come up the right level of granularity, but I've done that and I put an edge.",
            "01 could be waited and so on, but 01 if advertiser I clicked on query or bid on query J.",
            "And it could be weighed, and it could be whether they won the auction or someone clicked or something like that.",
            "So there's the graph.",
            "And then I do something like that to present it in this format.",
            "So there's no difference between edges, Inter versus edges, intra and in fact the intervals intra is a distinction that I put on the graph after I after I've constructed it.",
            "Right, yeah, so the the advertiser is interested in this query 'cause they bid on it.",
            "Or if this was a click graph the user clicked on that page 'cause they're interested or something, yeah?"
        ],
        [
            "So you construct a graph this way, and you want to ask questions.",
            "So things people ask are usually fairly simple statistics, degree distributions, clustering coefficients, which is basically the probabilities of triangles close in the graph.",
            "There's been a lot a lot of work on that slightly more refined question is, are there natural clusters or communities and they have to define what you mean by clusters in communities and so on, but slightly more refined version of the of clustering coefficients.",
            "And that's sort of the question will be talking about you know, for instance, if you want to find isolated markets with sufficient coherence and size, and then you basically asking that question.",
            "How do the nails grow and involve?",
            "That's a question I want to ask.",
            "How do processes like decentralized search like diffusion information diffusion with cascades?",
            "How do things like that behave on the graphs and ultimately in a lot of cases, not necessarily in all cases, but in a lot of cases you want to do some sort of machine learning, classification, regression or ranking, and so you might want to pull out features, but in some cases you don't necessarily want to do you want to do some sort of analysis and understand qualitative properties of the graph?",
            "Because understanding qualitative properties of the graph will tell you whether you have roughly even the right model.",
            "And so it's going to be a tradeoff here that we're not going to talk about in detail, but in some cases the question is you want to have a slightly finer understanding the graph.",
            "In other cases, it's probably better to spend your time investing in a factor of 10 more data and cleaning it up.",
            "And depending on the Zack questions, you want to ask, you want to under the other of those.",
            "So I showed you what people want to think about the graphs and what they think about the graphs.",
            "This is actually with these networks look like."
        ],
        [
            "Alright, I'm just sort of a total mess when you try and embed it on the plane or on the piece of paper, or in three dimensions or anything, and the reason they look like a mess.",
            "We'll get to later is that at the end of the day, we're going to expand or like, especially at large scale, so the grass for expanders.",
            "Expanders are things that don't embed well in low dimensional spaces.",
            "Looking back to an expander, wooden expander is, but I mean I really think it's the case if you're doing data analysis, but certainly if you working on social and information graphs and you don't know what an expander is, you have at best one and probably 2 hands tide behind your back because they're very basic.",
            "Types of graphs that are very counter intuitive.",
            "You've never heard of them, but but social graphs are that way, so we'll get back to what that is.",
            "But The Dirty little secret here is that this is a publicly available graph, and we use publicly available software to visualize this.",
            "If you knew the state of the art in graphs that are out there, you probably would not be able to tell me what this thing is, right?",
            "But if you know the state of the art visualization algorithms, you probably would be able to tell me what visualization algorithm I use may because of these artifactual.",
            "Rings or something?",
            "So this visualization algorithm is in fact revealing more about the visualization algorithm.",
            "So this visualization is in fact revealing more about the visualization algorithm than about the graph being visualized.",
            "So that's sort of a disturbing situation if you're interested in visualizing graphs.",
            "On the other hand, it suggests that maybe you know, since these things are not particularly nice in some sense, that you know we can use the artifactual pre features of visualization, algorithms or algorithms more generally in order to extract insight, and so the tools I was referring to earlier are basically that we're going to be using worst case algorithms, not because we want to solve worst case problems.",
            "But because those will be the formalization of our ideas, but we'll be using the statistical and geometric properties implicit in those worst case algorithms in order to reveal insight in the graphs instead of an experimental sense will be experimentally probing the graphs and using the worst case the statistical properties of geometric properties implicit in those worst case algorithms in order to reveal some insight into the graph.",
            "So people do before we get to that, since this is what these things look like, they'll develop hierarchical models and core periphery models, small world models.",
            "We have some underlying geometry, and yet edges in some way.",
            "Heavy tailed models?",
            "So we tend to link to higher degree nodes so people develop these sorts of models and then try and think about the graphs and they usually fit various statistics.",
            "So what?"
        ],
        [
            "Different ways you can look at these graphs.",
            "So these are two.",
            "I mean, to oversimplify, a large body of work and then you sort of doing it at a high level.",
            "These are two very common paradigms to think about these lots of rest.",
            "And will be interested in with the implicit.",
            "Maybe geometric properties are in well intervention that for these statistical reasons.",
            "But you know, with these graphs with these two models, classes of models say about the local properties of what's going on the graph versus the global properties.",
            "Right?",
            "'cause if you want to make some claims about the San Jose flower market, it helps to know how that sits relative to the Palo Alto flower market or to the DC sandwich market or other things.",
            "You know how the local and global properties fit together so heavy tailed models or power law graphs this.",
            "These make you think about these assorted large size scales.",
            "There's extreme heterogeneity in local environments, and that's usually quantified by the degree of the of the nodes.",
            "There's extreme heterogeneity and indegree distribution, and the graphs are relatively unstructured.",
            "Otherwise, there's a bunch of generative mechanisms having with preferential attachment sort of models.",
            "To generate these sorts of things and the details of the nature of the instruction is depends on the detail of the model.",
            "But there's extreme heterogeneity in the local environments, relatively instructed otherwise, and this is as opposed to maybe, you know, local local clustering or structure that you want to think about in particular small size skills.",
            "Alot of small world models will start with some sort of.",
            "Pre specified geometry, 1 dimensional ring or a 2 dimensional lattice and then add edges randomly based on that and the idea there is that you want to get some sort of clustering locali.",
            "And you can have heterogeneity or not depending on the details of the particular model.",
            "Looking at some sort of clustering locally and then you add edges randomly in order to get some global property like the graph is searchable or that the graph is algorithmically searchable or whatever.",
            "And so this is a very different sort of local global connection going on here."
        ],
        [
            "And.",
            "That's two very different ways people think about analyzing networks.",
            "Now a very different stream of work and data analysis.",
            "As you probably know.",
            "Ticular sort of machine learning variety.",
            "Is to use it all loosely called geometric data analysis tools, and by this you know.",
            "I mean sort of matrix methods.",
            "Low rank methods, SVD, PCA, these sorts of things with some sort of geometric properties going on manifold methods are variants of low rank methods.",
            "If you're familiar with those are.",
            "And these geometric data analysis tools don't view the graph particularly, don't view the data, particularly graph.",
            "You might construct a graph, but you're really thinking of it much more smoothly and continuously, so you're viewing the data.",
            "You're modeling the data effectively, not as a graph, but as as a point cloud in RN and feature vectors.",
            "Have M data points.",
            "It's an M by N matrix, and these things are usually based on the singular value decomposition.",
            "Will talk about what that is a basis for some of the things we talk about aggressive later, but the singular value decomposition is a very basic structure result in vector spaces.",
            "Will get back to that an it's intimately related not only to algorithm attract ability but also to geometric properties and the geometry gives you a lot that's limiting certain ways, but it gives you a lot and in particular will give you stability, robustness, maybe capacity control basis for inference, things like that.",
            "And so even if you're working in Hilbert spaces and that sort of stuff, if you're familiar with that machine, learning this is at the end of the day, which is sitting on top of some sort of geometric thing like this.",
            "But this is very different than the story I described about the advertiser bitten phrase graphs.",
            "We have a graph, it's a very combinatorial thing.",
            "It's it's an expander that probably doesn't embed particularly well at all, and some low dimensions."
        ],
        [
            "And so a natural question might be, can these two approaches be combined in some way?",
            "And it's not obvious that they can.",
            "I mean, if you're thinking about a network, LinkedIn, you know the web, live Journal and advertising betrays.",
            "You have in a sense, it's just a single data point is a snapshot of what you saw.",
            "Yeah, Walmart transactions over the last month or year, it's just everything there is.",
            "You think of the databases that everything there is.",
            "It's not clear how to view that as a matrix.",
            "I mean you can write everything as a matrix, and so there's some sort of isomorphism there.",
            "But in terms of thinking about questions, you might want to ask and so on.",
            "It's it's not obvious how to let em or end the number of data points or features go to Infinity, 'cause it's a sort of a single snapshot, and so most work done in those areas don't particularly apply.",
            "So you know, do we going to view the data as a single data point.",
            "There's a bunch of feature vectors and you get there."
        ],
        [
            "Different results on these sorts of things so.",
            "We have here in a sense, it's a question of how to model data right data?",
            "Just click streams, their query logs or whatever they are, and so how are we going to model the data?",
            "And so we want to sort of draw a connection between maybe one in computer science where you typically think of the data is discrete, whether it's a graph or or discrete clickstream.",
            "In the discrete nature, is gives you basis for fast algorithms and thinking about fast algorithms.",
            "In particular where I said maybe statistics or in other areas where you can make domain specific assumptions.",
            "Well, let's say statistics broadly defined.",
            "You thinking with the data is a little bit more continuous in the goal isn't to think about the data is everything there is.",
            "But as a random instantiation of everything in the world and so you want to draw some sort."
        ],
        [
            "Inference, so they said in slightly more detail in computer science, so the data is really a record of everything that happened.",
            "And the goal is to process the data to find some sort of interesting patterns.",
            "And those patterns are oftentimes frequent item sets or something like that, and those are oftentimes computationally hard, at least in models of data access.",
            "Their appropriate for the size or scale of the data.",
            "And in statistics you think about you know that you have some sort of instant random instantiation underlying process and the data is 1.",
            "Example of that and the goal is to learn something about the world and the data.",
            "So holy because it's the only insight you have into the world.",
            "The rest is just talk and so the data is actually what your insight into what's actually going on.",
            "And you want to posit some model to make some info."
        ],
        [
            "So we want to combine these approaches and we said they're very different.",
            "There's certainly not incompatible, and I think if you look at just as an example ACM KDD meeting this meeting now versus five years ago versus 10 versus 15, I mean there's a real shift away from what you might want to call these data basic questions to having a more sophisticated understanding of statistical modeling questions.",
            "Um?",
            "And I think you're seeing the same thing on the other side, and insofar as sort of academic statistics isn't addressing a machine learning is is filling in the gap in early addressing.",
            "Some of those questions, so statistics and probabilistic ideas are central to a lot of work on developing randomized algorithms for matrix and other problems, and that's been a lot of interest in the last 10 years.",
            "Alot of intractable optimization problems on graphs or networks will yield to approximation when you make some sort of statistical assumptions about the network properties, so boosting is a computational his statistical procedure with a computational parameter also serves as a smoothing regularization parameter.",
            "And as we'll see lots of times, approximation algorithms will implicitly regularize implicitly smooth things out, and the reason why, please smooth things out is that, as you may know, there's a lot of work done in the theory of approximation algorithms, and roughly what they say is that if you have an intractable problem and you want to fast approximation algorithm, one virginal way to do that is to embed the original graph in some other space, and if that space has nice properties, then you compute something down here.",
            "There's a stretch factor, some sort of factor you lose in that embedding.",
            "You compute something in that nicer space.",
            "And then you make a statement about how it applies in the graph, and you lose that stretch factor.",
            "You lose that plus a little bit.",
            "And so you get an epsilon approximation or constant factor.",
            "Or log in approximation.",
            "So when you're putting in this nice place.",
            "If the data is properties that are sort of synergistic with that niceness, then good things happen.",
            "If not, then good things might not happen, and so the question might be understanding the properties of these places, the implicit statistical and geometric properties.",
            "That you are implicitly doing when you're running a fast algorithm.",
            "'cause as we'll see in a lot of cases, you have some objective and you compute something.",
            "Would you actually compute if you measure properties of what you actually computed there at least as much a result of the particular approximation algorithm approximation procedure, use as the objective is started with arguing about an objective without arguing about the procedure that you're going to use to compute it.",
            "Isn't."
        ],
        [
            "Isn't maybe some meaningful so and so at the end of the day, you know question that we want to be able to ask is can we use these tools to sort of poke at the data, improve the data in order to understand what the data look like, and you know what do you mean by look like an?",
            "I don't want to define exactly what I mean by look like and it's going to be a little bit more sort of qualitative objective than fit.",
            "This classification function fit this regression function.",
            "But the reason we want to know what the data look like is that that will sort of inform what may be the appropriate statistical models are or the appropriate way to think about the data.",
            "Is it the case that there's a gambling clusterin and sports cluster and there's an intersection?",
            "Or is that not even a useful way to think about the data?",
            "So in a sense, if you take a step back and look at 10,000 feet and yes, what is the data look like?",
            "Does it look like a hot dog?",
            "Is there a left half in the right half that you can cut?",
            "And if you're embedding the data loaded mental place, what you're saying is that it looks in some sense like a hot dog or pancakes, so if that's your graph?",
            "And you can embed it that way.",
            "What you're saying is a left half on the right half, and it looks sort of like this pancake or hotdog?",
            "Or is a data like a tree as they have some sort of hyperbolic structures?",
            "Or does it look tree like?",
            "Or maybe as a data something like, let's call it a point for lack of a better term, you know a complete graph or a sparse complete graph, namely an expander."
        ],
        [
            "Alright, so the goal here is to.",
            "Is to address some of these issues.",
            "So I want to cover algorithmic and statistical work on maybe identifying and exploiting what I'll call geometric structure in large graphs in large networks.",
            "I will address some of the theory questions.",
            "Some of the questions about addressing the theory practice gap.",
            "And some empirical observations that we and others have made.",
            "Understanding with these insights how they can translate to sort of very practical problems that the people worry about and some future directions.",
            "And.",
            "And I skip the thing.",
            "Well, the things to keep in mind, if you ever more machine running, bent, even infinite dimensions in Euclidean space is going to be too limiting.",
            "And that's because when you go to infinite amount of structured way and since you have to get enough data points to flesh out something.",
            "And scalability and robustness will be central.",
            "So we want to be able to get a million node graphs.",
            "But will I want to be able to compute something that's roughly meaningful?",
            "As opposed to just being totally an artifact of miscellaneous edges here and there that they're not so interesting."
        ],
        [
            "So we'll cover maybe popular algorithmic tools of the geometric flavor, SVD, and PCA.",
            "These sorts of things.",
            "Basically, to sort of give some intuitions the way people think about it, and hopefully that will be something that's even if you haven't worked on it will be.",
            "Something that you can make some points of contact because I think so.",
            "People usually intuitively think about these datasets.",
            "And then I want to talk about some graph algorithms and I'm going to use a certain clustering problem as a test case or Canonical example and the tools that apply more generally.",
            "But I use a certain clustering problem as a Canonical example to talk about approximation algorithms and their implicit properties, the geometric underpinnings and this will be a graph partitioning problem or clustering problem.",
            "This is of interest on the applied side, because if you're interested in finding clusters or communities, you've been doing some variant of this and on the on the theoretical side, because it's a very basic primitive that you can use to really understand what's going on.",
            "And then talk about some novel insights on graph structure.",
            "Does the graph look lower, high dimensional?",
            "What does that even mean when you're dealing with something this far so noisy and so on?",
            "So, um."
        ],
        [
            "I have 4 slides here in more detail.",
            "Why don't I not go through those in?"
        ],
        [
            "Detail now, but they'll be up on the web page and then I can get to the first piece."
        ],
        [
            "Here and let me pause to make sure that's what everyone's reasonably happy and things are reasonably clear.",
            "No.",
            "Yeah, so I don't know what the problem is.",
            "Expanders don't embed well in low dimensional spaces.",
            "I couldn't come up with a good pick."
        ],
        [
            "Yeah.",
            "Because I get a meaningful picture on the plane and you know, if you have a graph, I'll define experiment later, but it's something that's very, very well connected.",
            "As opposed to having a left half and right have that you can cut as opposed to being a tree where there's other properties.",
            "So think of a complete graph.",
            "And if you sort of squint at that, you might want to say.",
            "I mean there's there's no structure in that besides being a point.",
            "An expander sparse version of that, but if you think of it in a certain price, is exactly the same thing.",
            "Yeah, yeah, but it's also hot dog.",
            "I mean I'm I'm just saying look at 10,000 feet and sort of squint at the graph.",
            "Yeah, so that's what I mean by squinting here.",
            "I mean, the idea is, you know if you think about the data this way in the data really look like that, you'll get some answer.",
            "But maybe what's what's the right way to think about the data and and even though there's a little imprecise as is that if.",
            "Space.",
            "Good, so that's not what I mean that that that that that gets back to this question about an M features in dimension.",
            "So thanks for clarifying that.",
            "Yeah, so I I don't want to mean I don't mean that."
        ],
        [
            "OK, so the singular value decomposition.",
            "The definitions of following.",
            "So you're given an M by N matrix.",
            "We're not making any statistical assumptions here, you just write in any M by N matrix.",
            "You can write the following way.",
            "I can write it as U times Sigma times V transpose.",
            "So you as an orthogonal matrix.",
            "Think of that as a rotation and V is an orthogonal matrix.",
            "Think of it as a rotation and Sigma is a diagonal matrix.",
            "Alright, and the way to think about this is that a is a function going between M dimensional space and end dimensional space, and U&V are bases for these two spaces or spaces.",
            "And if you fix a basis here and you fix a basis here, all A is all the matrix is is a bunch of diagonal scaling factors.",
            "So that's a remarkable result, and in addition.",
            "All these sigmas are non negative and you can order them from largest to smallest.",
            "Some of which might be 0, and if you keep the first, that's the best rank one approximation of the matrix.",
            "If you keep the first 2, that's the best rank two approximation, the matrix.",
            "If you keep the 1st three, that's the best rank three approximation of the matrix.",
            "So that's actually a remarkably strong result if you think about more or less any other algebraic structure, that's false.",
            "OK, so that's actually a remarkable result.",
            "And.",
            "And so I think it's Diana Larry called this the Swiss Army knife of matrix decompositions of the Rolls Royce of numerical linear algebra, and the reason is, I mean arranger reasons.",
            "But you know, if you have this matrix, it's like a Swiss army knife.",
            "If you have this decomposition is like a Swiss army knife.",
            "You can do sort of anything in the world with it, but you never need to compute it in the same way you have a Swiss army knife with all these saws and Hammers and things.",
            "You're never going to the mall, but given it you have everything you need and so the question is which pieces of this do you want?",
            "Order the insights this yields you.",
            "If you need a basis for here, can you compute it some other way faster or vice versa?",
            "Alright, so."
        ],
        [
            "A bit of intuition is the following, so if you view the matrix as a function going from RN to RM back and forth, which is what it is, then the SVD is the basic structural result of vector spaces, and it's going to algorithmic.",
            "In statistical consequences mentions, but the basic structure result that says you go from this space to this space.",
            "And if you fix it, the one basis here in the other bases here is just a bunch of stretch factors.",
            "And being just a bunch of stretch factors, I mean you should think of this.",
            "The significant thing about the second or third or fourth or fifth singular component is that it's perpendicular to the top ones.",
            "You know the exact orthogonality is a very strong requirement."
        ],
        [
            "And it's going to manifest its footprint very strongly.",
            "So if you truncate the SVD at K items, you get the best rank K approximation, and this is the way it's oftentimes formalized that a sub K is the argument with spectral.",
            "Mohseni Unitarily invariant norm spectral intravenously."
        ],
        [
            "Overall Rank K matrices and the idea of what's going on here.",
            "When I said that you can order the sink, the sigmas from large to small is that you have some directions.",
            "Or the data stretched out and you have some directions where they're not.",
            "We're stretched out less, and so I asked, what's the direction?",
            "Of maximum variance.",
            "So I'm saying I'm going to maximizing some Frobenius norm here.",
            "For business numbers sum of squares basically, and this is this maximum stretch direction.",
            "So what you're saying is that when I write the data down and I want to know which is the maximum stretch direction, what's the maximum variance direction that's going to be the first singular vector?",
            "And then if I want to know, conditioned on ignoring that direction, so looking at the perpendicular directions, what's the maximum stretch direction that's going to be the second singular vector?",
            "And so on.",
            "Right?",
            "You know, maybe this one is slightly better in some sense, but you need to be exactly perpendicular to the first one, and so conditioned on that, that's the."
        ],
        [
            "The maximum stretch direction."
        ],
        [
            "So one use of the singular value decomposition of data analysis is you want to think about the data points as points in a vector space, and this could be points from wherever and you've put them in a vector space so they could be graphed.",
            "It could be trees, they could be strings that could be anything, and you're modeling them as a point.",
            "In this sense of the word.",
            "I have endpoints in dimensions.",
            "This will give you a matrix with M rows and columns.",
            "The Rose will correspond to points, and you may say that two objects are close if the angle between them is small, 'cause you put them on a sphere in normalized the variance away, and that's one thing people do."
        ],
        [
            "So for example, in late something called latent semantic indexing.",
            "If you know what this is, this is you want to think about term document data.",
            "Which isn't quite a social network, but it has some properties of that you know might be very large and might have similar sparsity properties and so on.",
            "In which the intuition you'd have is, well, this this K most important concepts in the corpus, and so I'll say, why don't I apply SVD to pull out K directions?",
            "Now, if you actually look at how much variance you explain by pulling out 10 directions, it's not so much.",
            "Now, that being said, LSI was a big advance in 15 or 18 years ago when I was developed because it was so much better than they sort of competing methods because the competing methods are very, very combinatorial and gave you a lot of descriptive freedom but overfit the data or had other problems, and so this video is sort of a nice sweet spot, and so that's sort of a common theme.",
            "Will see is that.",
            "A lot of these tools which you want to find is a sweet spot between descriptive flexibility and algorithm intractability.",
            "So the way to think about this is that you have a matrix and and you're taking advantage of the structural result about vector spaces.",
            "And the fact that the structures that means very constrained.",
            "So if you keep K directions.",
            "There's only so many places to hide your sins and hide your mistakes because you're taking advantage of this very strong structure.",
            "Result that as I said, aside from matrix, more less.",
            "Any other algebraic structure doesn't have the geometry gives you robustness and so on.",
            "And if you keep K or K + 1 where K is 50 or 100, you might get good results in terms of precision and recall, and that sort of things.",
            "So you could interpret these directions, but you are very thin."
        ],
        [
            "Yes, when you start interpreting these things will get back to him and one of the things to note about why SVD might or might not be successful in LSI or in social networks with applications is the following.",
            "So we already mentioned that often times a day to have a lot of heterogeneity, so there's some nodes that have very high degree, so the big variance in some notes that are very small in terms of the number of edges that they link to a number of other nodes.",
            "Are they linked to number of edges they have?",
            "So here's a theorem that particular assumption that there is a little strong, but you can vary it empirically.",
            "Results are very robust to varying the assumptions of theorem.",
            "I think from a helicopter Meteo that says basically the following, if you have a graph.",
            "In addition to make sense of a graph, and you have a heavy tail over degrees.",
            "So big big variance over degrees and limited power loss.",
            "A big variance over degrees and the graph is pretty noisy.",
            "Or unstructured to start to think about expanders in the limit of random graph, which is what they looked at, then you have a heavy tail over eigenvalues.",
            "And heavy tail over eigenvalues eigenvalues equals singular values.",
            "There's a bit of a difference, but for what I'm talking right now, think of them as the same.",
            "So eigenvalues and singular is very very late in the square of each other.",
            "If you have a heavy tail over degrees and you're pretty noisy in the limits of random graph, then you have heavy tail over eigenvalues and So what that means is that if you're in the term document context or social graph and you keep 10 singular directions with SVD, or can I get directions you might get pick a number 20% of the mass.",
            "20% of the information or variance whatever, and you say that's good.",
            "I've gotten 10 concepts.",
            "But if you ask yourself, well, in a little bit more refined, why?",
            "What did I in fact have?",
            "So I have some lead part of the information, but what if I want to get a little bit more?",
            "What if I want to get 90% or 20% of the information?",
            "So if you have a heavy tail over eigenvalues, what that means is that in order to get 20% of the mass or information you need to keep not 11, not 20, but something like 100 singular values or 100 eigenvalues.",
            "And to get 30% of the information needed to 1040% need 10,000, I mean a scale for something like a scale for each, and so now you're keeping a massive amount of.",
            "Massive number of directions because the exact orthogonality of your densifying things exactly where you want to not be densifying them because you're pushing, you're starting to get on the tail.",
            "Lots of things get pretty sparse, and so you're going to have problems because you'll be statistically overfitted.",
            "So you know that's maybe why latent factor type models?",
            "If you're familiar, Netflix, one of the models that they used in the mix was latent factor on bunch of the models that mixes latent factor type models and that gets out of course information.",
            "But fine grained information tends to be lossed."
        ],
        [
            "So.",
            "So you can have a general matrix.",
            "You can have a square matrix.",
            "You can have an SPS dia, symmetric positive semidefinite matrix or kernel.",
            "And in data analysis a structural properties of SVD are used most often in square, maybe adjacency matrix or kernels, but the basic result of those square matrix results or symmetric positive semidefinite results."
        ],
        [
            "Are using is a singular value decomposition the same thing that LSI is using?",
            "So algorithmic issues?",
            "So it's bigger is a lot of subtleties, exact computations take something like N cubed time, but typically you don't need to compute everything because it's it's a Swiss army knife.",
            "You know if you want the top K left singular vectors, it can be computed faster using various sorts of iterative methods.",
            "Oftentimes there's not a. I mean, there never is.",
            "I mean, it's a practical matter, but you know not much of a gap between the K for the K plus first singular value, so that's a fair question to ask, you know, do I really need the exact case?",
            "But maybe they I can add some mixing.",
            "Plus The Cave plus first and the latter is typically the case.",
            "You can get specialized numerical methods for large sparse matrices, and there's been a lot of work on the last 10 years in theoretical computer science and numerical linear algebra on randomized algorithms or epsilon approximation algorithms, and these are some types of interest because you can compute things faster and you only suffer epsilon error.",
            "But also these are interested because even if they don't compute things faster, they take roughly the same amount of time they often times are better, and they're better because they're worse, right?",
            "If I give you an answer that's not exactly 10 digits but is exactly 1 digit or two digits, namely an epsilon approximation.",
            "That may be better, even though it's a worse, you know, by a naive measure, discounting the exact position that may be better when you plug it to a downstream application.",
            "Basically because you haven't exactly fit the particular data at hand, which is much things out a little bit.",
            "So if I give you an approximation, think a small number of steps of the power method rather than exact answer, you oftentimes early stopping or things like that, or epsilon approximations in other ways will be more useful for downstream applications like classification and clustering, because you haven't sort of overfit to the particular problem at hand, so that's something that we'll get back to, and we'll see I mean, that's an example of it in a vector context, but that's sort of intuitively what's going on in graph approximation.",
            "Algorithms will get back to that."
        ],
        [
            "Principle components analysis and multidimensional scaling and other sorts of things as sort of basically using."
        ],
        [
            "SVD, so let me go, not get into the details of that.",
            "And to say that in terms of maybe statistical properties, you can always compute the best rank approximation if the data is nice and Gaussian, there's a natural statistical interpretation here.",
            "And more generally, you can think of it as some sort of model selection where the class of models you're working with is something that has some nice geometry.",
            "And the data may or may not look like that, right?",
            "If you're dealing with a term document matrix or social graph, it's very sparse and may not have that geometry.",
            "But your model selecting in that sense and putting yourself in a nice place.",
            "And there's a lot of connections, at least squares regression and PCA in terms of optimality.",
            "In terms of, you know, being the optimal.",
            "Being the optimal solution within a class of unbiased estimators and so on."
        ],
        [
            "So, relatedly, are geometric questions having to do with the SVD?",
            "And this video is it basically is is computing the lead axes of that hot dog or pancakes?",
            "You know the maximum variance directions if you have data that consist of sort of nice things like nice Gaussians and the issues not.",
            "There's nothing magical about a Gaussian.",
            "If you have anything that's pretty dense, where measures going to concentrate, and so the question here is is measure going to concentrate in some nice way?",
            "So can you think of the data is being asymptotically?",
            "Or maybe the data really not in the asymptotic regime and so you need to take into account those properties."
        ],
        [
            "So.",
            "If you think about all those sorts of algebraic structures like tensors or things, computing the rank of a tensor is intractable.",
            "Rank abstractions may not even exist.",
            "Lots of other hardness results, and so on.",
            "Non negative matrix factorization's.",
            "You know nothing is convex.",
            "The original work didn't even converge for Saddle Point and then people found variance of albums you converge the saddle points and much much weaker in terms of which you can say, so it's much weaker in terms of the algorithmic results, but much greater in terms of descriptive flexibility.",
            "And that's maybe not such a good thing.",
            "So we're going to be wanting to work with a fairly restricted class of things, so these geometric properties in approximation algorithms will do that for us, so vector spaces here very, very structured places, and that's something maybe that you can take advantage of."
        ],
        [
            "So there's a lot of work in kernel methods in machine learning, and I'm going to skip over some of these slides, except to say if you're familiar with kernel methods in machine learning, that's basically what we've been talking about, except."
        ],
        [
            "Something called kernels, which is basically symmetric positive semidefinite matrices."
        ],
        [
            "Um?",
            "Which can be used to encode similarity information, but so I'm going to skip over these, but these are up on the web page if you want.",
            "So interpreting the SVD.",
            "So what you want to say is the first singular direction means something.",
            "One concept class, the second singular direction means something.",
            "The third means something.",
            "And by the time you get to the 4th or 5th with the fifth direction means that mean the most significant property of the 5th, 6th or 7th direction is that it's perpendicular to all the previous ones.",
            "Right, because you're demanding, is active formality and the reason your instructor place and good things happen is that you're being very stringent about that, so that you at least more less.",
            "Any data set.",
            "I've looked at the most significant thing about the 10th singular direction is perpendicular to the top nine, so interpreting things you're in very thin ice.",
            "And the reason you're very thin ice is the following.",
            "If the date is nice and Gaussian, it's a hot dog like this.",
            "The first singular direction means something in terms of lead axis of variation.",
            "Second means something in terms of the second direction, but the data never looks like this, right?",
            "It may look like this and maybe a combination of two hot dogs.",
            "In which case the first direction points here.",
            "The second direction points here, and neither direction means anything in terms of the data, so it's maybe a toy example where this would arise.",
            "So say that the data consists of a bunch of sinusoids, and I have a couple of noisy sinusoids here.",
            "Right there and then a bunch of decaying exponentials.",
            "So the model for the data might be the sum.",
            "Oscillatory factor is something decaying, and the data some combination of these two, so the singular directions you get are these two things.",
            "You get that and that that both have sinusoidal and oscillating features, which is that direction and that direction, which is these two directions.",
            "But those don't correspond to any of the generating processes in the data which I've told you, sinusoids exponentials.",
            "So interpreting these directions in general, you're going to be pretty nice.",
            "The first one.",
            "Usually something in terms of data normalization, and it might be meaningful if you've done things carefully into designing on the data, but once you get beyond that gets much trickier, and basically it's for this reason.",
            "That being said, you know I wanted to mention one thing that people think about this particular relevance to."
        ],
        [
            "I have something called centrality.",
            "On the centrality of a vertex measures the relative importance of that vertex.",
            "In a graph.",
            "And so there's a bunch of different options.",
            "Degree centrality is maybe the number of links incident Isaiah Pon, or I guess I'm ignoring directed this year sopana node so something related to the degree is between the centrality which would be high for vertices that occur in many shortest paths.",
            "This closeness centrality eigenvector centrality.",
            "Lots of different ways to capture this, and the idea might be for the eigenvector centrality.",
            "This is going to have connections to high degree nodes.",
            "Because maybe those capture things that are important vertices are important, but if you look at the particular definition, it'll capture a bunch of other things.",
            "So in terms of the data consists of two clusters and a small number of links between them.",
            "Some of these notions of centrality will place these nodes being particularly important.",
            "Because there are a lot of cross, I'm sorry there.",
            "There are a lot of crosstalk between the two clusters.",
            "So you know if the data conforms with some generative processes that has intuition than the centrality measure might be appropriate.",
            "But you have to ask what does that particular thing compute?",
            "What if you have a graph as we'll see with the social graphs later, when the best clusters are extremely imbalanced.",
            "But would some measure of this B and does interpreting these singular directions correspond to something meaningful?"
        ],
        [
            "So one thing that I have a couple slides, I'm also going to gloss over, but at least want to mention is that even if you're not doing SVD or PCA or any of these things, these things are sort of under the hood and a lot of things that you might be doing in data more generally, but also in graphs will get too.",
            "So we mentioned latent semantic indexing and manifold based machine learning methods.",
            "There's alot of diffusion based methods, you pick a bunch of keywords and do a diffusion on the graph around that and try and pull out some sort of concept Cass.",
            "If you're doing diffusion, you're doing something very eigenvector related, because eigenvectors are very related to diffusions.",
            "If you think about the power method is the way to compute the top eigenvector.",
            "K means clustering spectral partitioning.",
            "Spectral partitioning is one way, one basically algorithm to cut up a graph.",
            "There's other algorithms to solve the same objective."
        ],
        [
            "K means what do we mean by this?",
            "So K means is a way to cluster a set of nodes and."
        ],
        [
            "You want to say what you want to say is that the data correspond to five clusters?",
            "I have some objective but I forgot to write that install.",
            "Update those in on the slide.",
            "There's a particular objective.",
            "It doesn't matter what it is, but the particular objective quantified the intuition that you know the data consists of five clusters like that and all the data points that sit around that.",
            "Now if you look at this particular objective which I didn't write down and you relax it not to be something discrete, but to be something continuous.",
            "Would you end up getting is the SVD solution?",
            "So that relaxation says that this objective.",
            "First cousin are in terms of relaxation, so it's very related to the SVD and that shouldn't be surprising is if you draw a picture like this.",
            "What I have here is a couple hundred data points in five clusters.",
            "So it should be very believable that we're doing something very very low dimensional here, right?",
            "We have hundreds of data points is sitting on the plane in five nights interpretable clusters?",
            "I mean, that's the gambling market.",
            "That's a sports market is a bit of overlap, and it's a fair question to say, does a graph actually look like this?",
            "So if you're doing K means clustering on a social graph, social networking information network?",
            "If the graph looks like this, you'll probably get something meaningful.",
            "If the graph looks like some of these other things, like a tree or expand or something.",
            "I mean, who knows what you're going to get and try and interpret.",
            "So there's a lot of things that."
        ],
        [
            "Um?",
            "Under the hood you're using something very similar to these Eigen, Eigen Max spectral partitioning.",
            "As we get back to later using something very similar.",
            "So the bottom line in terms of these things before moving, is that eigenvectors are very global entities.",
            "The exact orthogonality gives you very strong global properties.",
            "You need to be exactly orthogonal.",
            "Eigenvectors loaded by linear structure.",
            "You can generalize that to get nonlinear structure, but really it's linear in some other space.",
            "When you're saying it's linear there, you're assuming get enough data points that you can flesh out some.",
            "Your measure concentrates somewhere.",
            "If in some sense you more expander like or or, it's not the case.",
            "The measure of ever concentrates because so you generate the data in an adverse aerial way, and so there's some incentive for people to do something different than than that may.",
            "You may never hit that regime and.",
            "That being said, it would be nice to be able to use some of these geometric ideas, basically because these sorts of eigenvector methods identify some sort of sweet spot between descriptive flexibility in algorithm intractability, right?",
            "Everything's going to be in cubed as opposed to two to the N certain things.",
            "You're not going to find because of that, but they really give you sort of a sweet spot, and so can we.",
            "Can we take advantage of some of these ideas more generally?",
            "And so that's that will be."
        ],
        [
            "I'm talking about now.",
            "Unfortunately not immediately obvious.",
            "That we can see what I'm talking about.",
            "Small scale structuring as I said, think of what's going on in the advertising bit of phrase graph in the Palo Alto flower market.",
            "When you talk about larger scale, think of.",
            "You know a larger flower market or market in the in the same time in San Francisco Bay Area.",
            "That's why we talked about Palo Alto flower markets, but if it's from wherever you're from, I guess.",
            "So the DC flower market.",
            "Think of what's going on in in the Bay Area market more generally, or a market in the US more generally or something.",
            "So there's a much largest size systems, so it will turn out that the relationship between small scale structure and large scale structure is not reproduced.",
            "Even qualitatively.",
            "You'd like theory to provide any qualitative guidance being perfect, but give you a rough idea.",
            "It turns out it's not obvious.",
            "It's hard to justify and will talk about justifying this, but it's not even close, not even qualitatively reproduced by popular network models.",
            "Once you're better off just not doing anything 'cause they're not even qualitatively correct.",
            "And the reason this is important is that this relationship governs diffusion of information.",
            "If you want to understand viral marketing, it really matters whether the best cuts are 5050.",
            "Or 99.1.",
            "Or there's no good cuts at all.",
            "It also discovered.",
            "It governs things like decentralized search, dynamic properties, applicability of common data analysis and machine learning tools.",
            "So governs all sorts of things in pretty subtle ways, and we'll talk about that.",
            "And, relatedly, as I said, I mean there's just sort of a big disconnect between popular data analysis tools and a lot of machine learning, and a lot of network properties, so low dimensional geometric tools are common, but you know, really the network.",
            "It's hard to think about this is a set of feature vectors, it's just whatever it is.",
            "I mean, if you ran the Clock again, you'd get a very different web."
        ],
        [
            "Around the Clock for 99 you get a very different web so.",
            "With all this being said, let's switch now.",
            "To some graph algorithms.",
            "And what we want to do is to, in some sense of Dragon drawn, some of the intuitions that we've laid in terms of graph algorithms.",
            "And think about these sorts of results in that context.",
            "In order to understand the statistical properties and geometric properties involved in that, so that we can do better data analysis.",
            "On these networks in sort of a structured way.",
            "So I'm going to be talking about.",
            "A particular problem and the particular problem is that of graph partitioning.",
            "And so it's a fair question to ask why you think about graph partitioning.",
            "And if you're someone of a theoretical bent, a fair criticism is, you know you're think about graph partitioning, because that's the only thing you can prove theorems about.",
            "So that's not such a good reason, but it's one reason it's a fair one.",
            "If you have an applied bent, one reason you might want to be doing this is that it captures some notion of clustering or communities which you might think is a better reason, but there's a lot of other things that capture clustering in communities, and in fact might do a better job.",
            "So the reason we talk with graph partitioning innocence is because, again, it's at sort of a sweet spot between the theory into practice.",
            "It's an example of a problem that you can make very strong theoretical statements about.",
            "Ann, and you can really understand the inner workings of the algorithms.",
            "And yet at the same time it does a pretty good job capturing people's intuition as to what they mean by clusters and communities.",
            "Imperfect, and we get back to some of the problems with it, but you know it, it kept.",
            "It captures a pretty good intuition as to what they mean.",
            "So if we want to understand what the implicit properties are available in that sense, it's a good test case or good hydrogen Atom for understanding the method.",
            "Understanding the method of how using approximation algorithms experimental probes should work.",
            "Because we can draw on the theory that we know, but we can also try and map it to what practitioner would say as a natural cluster community or that sort of thing.",
            "So the spectral methods is flow based methods of multiresolution algorithms will be talking about some of their implicit properties of the scalability issues, and where these sorts of algorithms succeed and fail, and why they might succeed and fail.",
            "And once we have some of that, then we'll talk about.",
            "Maybe some novel insights that all of this gives us on.",
            "I think sort of very practical."
        ],
        [
            "Applications.",
            "So graph partitioning.",
            "Is.",
            "Sort of a general welcome meta problem.",
            "So it really refers to a family of.",
            "Optimization problems.",
            "Combinatorial optimization problems as opposed to continuous optimization problems.",
            "But a family of combinatorial optimization problems where we have a graph.",
            "And it's nicely color coding to partitions.",
            "'cause this is a partitionable graph, but we have a graph that is nodes and edges and we want to cut the sets of nodes into.",
            "Two pieces and you can imagine generalizations to three or K, but you want to cut the set of nodes into two pieces such that there's not much edge weight which is either number of edges of the graph is weighted.",
            "It's some of the weight, so there's not much edge weight between the.",
            "Two pieces and so we'll call that cut quality.",
            "It's a good cut if there's not much edge weight between the two pieces, but also would like both sides to be pretty large.",
            "We don't intuitively people find it a little dissatisfying and maybe the world is just as dissatisfying.",
            "I mean, it might be the case.",
            "That the world has certain bad properties, but people think it's a little bit intuitively dissatisfying.",
            "When you partition a graph and you pull off 100 nodes out of a million, so we want both edges to be pretty pretty large.",
            "So there's several standard formulas formulations of this.",
            "You can ask for graph by section.",
            "We impose a hard constraint 5050.",
            "You can do a soft version.",
            "I've paid a balanced by section.",
            "You know 7030 or 8020.",
            "You can use something called expansion or related objectives, where these last two examples of what is called Potion Cut objectives.",
            "Because you fold these two factors, these two criteria into a quotient and use a cut size divided by some volume notion.",
            "So the minimum of the size of A and the size of B.",
            "Or the product of the two sizes.",
            "I'll get back to the difference there.",
            "And if you have extreme heterogeneity over things like degrees, it might make sense to do cut size divided by the volume of a.",
            "Either the minimum volume of A and the volume of a compliment.",
            "Or other product.",
            "And so you should think of these two as being similar.",
            "These two different volume notions as being similar when there's not a lot of heterogeneity over degrees.",
            "And when some degrees are very, very high and some are very very small, at the latter, one might be better and the latter is something called conductance or normalized cuts.",
            "If you're familiar and computer vision, they call it normalized cuts.",
            "And the form is something called expansion.",
            "All right now, the difference between these two denominators.",
            "Is the following in their equivalents in a certain sense and the senses?",
            "All of these formulations are NP hard.",
            "But if I solve the problem for the first denominator.",
            "I have a factor of two approximation for the volume of the for the problem formalized with the other denominator.",
            "So if I take the denominator to be them in of the two volumes.",
            "Then have a factor of two approximation for the other problem being the product of the two volumes and vice versa.",
            "Now if I can solve the problem exactly, effective tools, pretty big.",
            "If I can solve the problem to epsilon equals .001 error, the factor to survey if the best I can do is solve it to a factor of 50 or log in or something like that.",
            "In fact it's pretty small, so in fact will see that that's the case.",
            "So I'm going to want to call these to the same now for any particular set of nodes.",
            "That's going to make a big difference.",
            "So I'm using the volume, the term volume to mean something vaguely related to size, and I guess I was a little precise because volume could be the cardinality of the number of nodes.",
            "Or it could mean this thing and calling volp which is volume, so that's the number of nodes, and that's something like the number of edges I'll define exactly what is in a little bit, but think of that is sort of a node based volume in edge base volume, and so if the graph is pretty regular, those should be the same because everyone has every notice 5 edges or whatever, but there's a big variability.",
            "It's going to be different, so there's a couple different notions of size or volume.",
            "And I want you to ignore the distinction between these two because.",
            "For any set of nodes, they're going to be the same.",
            "They one provides a slightly stronger balance towards good balance than the other, but for with respect we talking about later.",
            "That difference is very, very small, and if you want to do very fine things in certain parts of the graph, this difference might not might matter, and it's a fair question to say, you know I might have code that optimizes one or allegorize one and not the other, and I'm going to gloss over some of those things, But for what we talked about today, those are identical.",
            "So you shouldn't worry about the different notions of the denominator, that basically the same."
        ],
        [
            "We'll be talking about, so we're talking about graph partitioning.",
            "As I said, since it so graph partitioning will capture a qualitative notion of connectedness.",
            "Sometimes the data is processed to be connected, right?",
            "I mean, it's whatever clicks you saw today, so it's connected in that sense.",
            "But the question might be a sort of how it's connected.",
            "Will see that solving this problem will tell us sort of roughly, if you're looking at 10,000 feet and look down how is it connected with?",
            "What's the best split into two?",
            "It's a very well studied problem, both in theory and in practice practice, meaning implementing algorithms with practice.",
            "Also meaning doing data analysis.",
            "And lots of machine learning and data analysis applications are boiled down to this, so transductive learning or Community detection.",
            "Alot of things have been reduced to this sort of problem, so we're going to be doing is running an approximation algorithm and we want the output of the approximation algorithm want to think about the output of the approximation is not something we settle for.",
            "You know it's not like well, we can't solve the interactive problem, so we're going to settle for this.",
            "The output of the approximation algorithm will oftentimes be better than the exact solution because the data is very, very sparse and noisy, and the fact that we're going to filtering the approximation of filters us through a nice place will give us some regularity properties.",
            "We might actually be getting a better solution, so it's not really something we settle for.",
            "And then I guess I'll touch on the randomizer approximation algorithms give better answers in the exact solution applied sense.",
            "Um?",
            "And maybe the reason is that fast algorithms capture sort of a notion of qualitative existence, and in certain data analysis applications.",
            "So that's why I will be talking about it will get back to another reason a little bit later, but that's why we."
        ],
        [
            "Talking about it and this gets back to the figure I had before about it.",
            "Stand up at 10,000 feet in.",
            "You squint of the data and what is the data look like?",
            "So say that we want we have a graph.",
            "And we want to partition the graph into two sets of nodes are left half on the right, which is what this graph partitioning problems going to be doing.",
            "And say you know related thing is that we want to find the best fit of the adjacency matrix to be something like this.",
            "So I've put a bunch of columns here in a bunch of columns there, and you know, Alpha is some parameter that describes how well these columns talk to each other.",
            "And gamma is something describing how all these columns talk to each other, and beta is the cross talk.",
            "I haven't given a particular objective, but you can imagine a bunch of objectives that capture this.",
            "And so then you know the question is what is the data look like if you squint at it?",
            "So one thing the data might look like is if the data is low dimensional.",
            "If it's a hot dog or a pancake, then the data is going to look like something like this.",
            "There's going to be a left half and right half in the crock.",
            "Crosstalk is going to be smaller, left half and right half, and the crosstalk be smaller.",
            "And the reason is that the interface between the left and the right half is one dimension less than the left half and right half, so that's why it's going to be smaller, very different, sort of data structure.",
            "The graph might have if you squint at it.",
            "Is to give sort of naive example.",
            "If the graph is bipartite, bipartite means you have advertisers have bid phrases and no advertisers linked to each other, they always link to phrases and vice versa.",
            "So rectangular matrix may capture this.",
            "In which case the left half talks to the right half of the lighthouse talks the left half, but there's very little cross talk.",
            "If the data is an expander, then there's very little talked at all.",
            "You know, and a possibility is that the top left hand corner is large, meaning the left half talkswitch itself a lot.",
            "The bottom right small and the crosstalk isn't in between.",
            "In terms of Alpha betas and gammas, it's certainly a possibility.",
            "Might be hard to picture with this thing looks like.",
            "Will get back to that and so think for a little bit.",
            "You know what is?",
            "What would the graph look like if this is really what the graph looks like?",
            "So gay."
        ],
        [
            "Back to that, but maybe it's worth thinking about now and why are we going to worry about both criteria here?",
            "So some graphs, maybe space like graphs Rd networks which are like hot dogs and pancakes Rd networks?",
            "Random geometric graphs.",
            "Whatever finite element meshes so a lot of stuff that's done in scientific computation where where the data comes from something low dimensional when stars blowup there's only so many ways for information to go from here to here, and so that imposes strong regularity that you don't see in social information graphs typically.",
            "The cut quality and the cut balance work together.",
            "This synergistic in the sense that.",
            "If you take if you take a larger sets of nodes so the X axis here is the enclosed volume.",
            "You could take larger sets of nodes.",
            "Then in all these plots down will be good.",
            "I'll be showing lots of plots down, will be a good always.",
            "If you look at the Y axis, which is.",
            "The conductance of the expansion down is also good, so larger graphs, larger sets of nodes have a given graph and I asked what's the cut value or what's the conductance value or whatever down is good, and so they work together.",
            "I want to balance constraint.",
            "So I want to be out on the right in terms of.",
            "The size and so they set down as good.",
            "That's good because larger things are better.",
            "Right?",
            "And So what you see in a lot of space like graphs and it is that it goes down, and the intuition is that bigger circles are better than smaller circles, right?",
            "Bigger circles are better than smaller circles.",
            "And maybe circles are better than triangles, but you know they all go down.",
            "So we'll see in a lot of real world parallel graphs is not that they're expanders.",
            "Maybe you're up here and everything is bad, but that small things tend to have.",
            "They tend to be good small clusters, but the best ones that are larger and larger get worse and worse.",
            "So in a lot of social and information graphs, and this is a very very, very general property, I mean most any graph you look at will have this property that's very, very different than things that you might want to intuitively think about as being low dimensional.",
            "It'll get worse and worse.",
            "And that makes it awkward because it's not like you can just optimize objective.",
            "You'll get a whole profile, whole range of things at this tradeoff.",
            "Larger things that are worse, larger things still that are still worse."
        ],
        [
            "So the lay of the land in terms of graph partitioning.",
            "So we're going to be partitioning graphs.",
            "We want to cut them up.",
            "And in terms of the theory, but also the implementation, here's basically the lay of the land, and we'll go into a bit of detail about each of these.",
            "So there's a range of.",
            "There's a range of spectral methods, so spectral methods are going to be very related to SVD.",
            "An eigenvector methods that we're talking about.",
            "And here, spectral methods basically mean we compute an eigenvector of a certain matrix related to the graph.",
            "So we computed eigenvector computer single direction.",
            "We put all the nodes down on this direction.",
            "We cut somewhere.",
            "And in theory you cut with, you know, Rule A and practices will B or C or D Souza range of things, but they all sort of captured the idea that this to have graph and you should cut somewhere.",
            "So there's a rounding rule that says, where are you going to cut once you give in this direction, and that matters depending on the particular application you're thinking about, or whether you're trying to prove theorems, in which case you want something that's a little bit simpler.",
            "But there's also the question that I have a general direction.",
            "Maybe this is the wrong direction.",
            "Maybe this is the direction which case you're never going to get the right thing if you're pointing sort of in very much the wrong direction, so we're computing eigenvectors, no rounding.",
            "Local improvement.",
            "You know I have a set of.",
            "I have a input partition and I might want to tweak it and do a little bit better.",
            "So not surprisingly, these get trapped in local minima very easily, but they can be used to clean up cuts from other methods.",
            "I'll get back to an example that we've talked about sizes this smaller markets versus the larger markets.",
            "Can you view the graph?",
            "And it's very easy to view space like graphs, right?",
            "You know you look at the floor and you say here's the size scale of an inch.",
            "Years aside scalable foot, here's a size scale of a mile.",
            "He can coarse grain over that.",
            "Can you do something analogous in graphs?",
            "Not naturally space like so do some sort of multiresolution, so there's a lot of stuff done and space like graphs and multiresolution, and this flow based methods.",
            "So flow based methods at the end of the day have to do with Max flow and min cut.",
            "So Max flow and min cut Max.",
            "This is a very classical problem sitting on top of something called duality that will get back to you later.",
            "This is important for a lot of what we're going to be interested in.",
            "In particular if you remember a while back I said can we actually test the hypothesis that the data looked the way I presented it, and the way it tests the hypothesis to get a negative result as opposed to pulling something off the shelf and seeing what worked to say maybe the data does not look this way will be to use duality, which is going to be related to Max Flow, min cut ideas and so here you want to single commodity flow or a multi commodity flow.",
            "In order to see if the flow can reveal bottlenecks in the graph, and the hope is that if this is a good cut, then if you're trying to route flow through that, then the flow revealed that."
        ],
        [
            "So we'll talk about each of those methods.",
            "So start with spectral methods.",
            "So the work goes back to the to the 70s before you had high quality numerical code, high quality numerical code for these problems really probably came of age in the 80s.",
            "So very popular in in scientific computing and parallel computing.",
            "In the 80s, lots of machine learning work and I would type.",
            "They will fix it.",
            "A lot of machine learning work.",
            "Starting with the normalized cuts, if you're familiar with that paper.",
            "But a lot of work had been around, you know, before that, but in machine learning in the 2000s and not the two hundreds, I guess I have to fix that too.",
            "And the algorithm that is computed an eigenvector and you never need an exact eigenvector, just need a vector.",
            "That's who's really quotient is about what, right?",
            "So there's some robustness there.",
            "And then perform a rounding."
        ],
        [
            "And the theorem that you get and I should have.",
            "Maybe I don't have it, but the eigenvectors.",
            "Basically an eigenvector of the Laplacian in the Laplacian is a matrix that's going to be related to the graph.",
            "Think of the Laplacian as the adjacency matrix.",
            "But you might normalize it and and take into account the diagonal entries.",
            "And I think we will get, well, we'll get back to that, time permitting.",
            "But that's the matrix that we're actually going to communion vectors of, and the theorem that you get is that if Lambda is the 2nd eigenvalue of the Laplacian.",
            "Anfi is the conductance member, fires expansion, or fires.",
            "Conductances, two different denominators.",
            "And I told you to ignore the distinction.",
            "If fires your conductance.",
            "That's going to be intractable to compute.",
            "But now if computer and eigenvector and we know that eigenvectors are fast, it's a fair question.",
            "Said computing eigenvector of a billion node graph and will type for me will touch on that.",
            "But certainly up to millions and hundreds of millions.",
            "It's not such a problem.",
            "Lambdas and #5 and number and so how good is 5?",
            "This thing that we compute quickly?",
            "And so you can show that Lambda is less than five, is less than root Lambda, and is effective two in a route 8.",
            "Very sorry, Lambda is less than five, less than root Lambda.",
            "Right, so there's a couple constants, but the idea is that Lambda or Lambda over 2 is a lower bound and square root of Lambda is an upper bound.",
            "Alright, so there's nothing here about the number of nodes in the graph.",
            "So if you're familiar with the theory of algorithms, it's a little bit strange because you know it's not a log and approximation or root in.",
            "There's nothing about the number of nodes, but we get a quad factor that's quadratically good in terms of the structural parameter of the graph is the land will have a natural interpretation to the structural properties of the graph.",
            "Alright, and in fact there's other versions.",
            "This, let's call it a test version of this for any test vector, so malhas.",
            "Something let X be any vector that's perpendicular to the all ones vector, and that's very important for reasons that let me not get into then there's going to be a cut along X that satisfies this thing, which is related to this thing being greater than 5 squared, so that's squared as your same quadratic factor.",
            "So I don't need to be the exact eigenvector.",
            "I can be in approximate eigenvector or I can get bound in One Direction with any vector.",
            "So this result is a much more general and robust results.",
            "It's called Cheeger's inequality, 'cause Cheeger proved it in the continuous setting but along and Millman and Sinclair and others in the 80s got dis."
        ],
        [
            "Treat versions of this and the idea is that.",
            "If you compute this vector.",
            "In the adjacency matrix, looks something like this for Alpha and gamma are large and the crosstalk is pretty small.",
            "Then when you put the set of nodes on the vector, this eigenvector Q you put them there.",
            "Here's a bunch of the nodes.",
            "Here's another bunch of the nodes and you want to cut somewhere, and we're going to cut, so this is a toy example.",
            "It should be obvious where to cut, but the generalization is that it's something like this, so this will be up here and I'll go down a little bit and he should cut where it crosses zero or where there's a maximum change here, or there's a bunch of rules, but you'll cut somewhere like this, But the thing you should keep in the back of your mind is that this actually looks a lot like.",
            "The K means problem.",
            "Right?",
            "There's a left half and right half.",
            "There's two nodes and they both fit pretty well.",
            "So it's a fair question to say, what if this isn't satisfied?",
            "Or what if the cuts are not well balanced?",
            "What are you?"
        ],
        [
            "To get.",
            "So we'll get back to that, but it's a different way to look at.",
            "The question is to say, well, you know what if you know the graph doesn't look like this, how bad can things be?",
            "Can I come up with a graph that sort of looks as different than this is possible."
        ],
        [
            "Or, relatedly, is this quadratic factor?",
            "Maybe this isn't a weakness of the analysis, the theorems just aren't good enough, or maybe that quadratic factor really is there and it's an artifact of the spectral technique graphs exist that are that bad.",
            "So the answer to that."
        ],
        [
            "Oceans, yes, perhaps do exist that are that bad?",
            "So Gordon Miller constructed in node graphs with the spectral by section, cut his end of the 2/3 versus the optimal event of the 1/3 so quadratically bad.",
            "And the particular construction doesn't matter as much will be talking about.",
            "But the basic idea actually is going to be essential for we talking about.",
            "And the basic idea is we talked about for Spectrum is going to be related to diffusions, right?",
            "Because to compute the first eigenvector?",
            "Maybe not the best way numerically, but you know intuitively you can think of it as do with the power method.",
            "You take take some arbitrary vector random vector, hit it with the matrix several times and you'll converge to the best solution and long, so some other things to do.",
            "Variance of that.",
            "So there's something very much like a reinforcement of diffusion going on.",
            "So like a random spanning tree as opposed to minimum spanning tree.",
            "We don't take into account this reinforcement and so.",
            "The worst case graphs take advantage of.",
            "Let's call it long stringy things.",
            "Because diffusion is related to probability mass and takes a lot of effort to push probability mass down, a 1 dimensional line.",
            "Versus some other constructions that you have two pieces to talk to each other so you know if you.",
            "If you're diffusing on the plane or in three dimensional space on an expander, thing is going to be very different than if using online and so they have to call it cockroach grass, but you take cockroach graph that has very long legs.",
            "So long stringy things and then a bunch of other stuff.",
            "And the spectral method.",
            "The question is you want the eigenvector to point up and down because that will give you what you might want to think is the best cut.",
            "With the eigenvector method might cut that off, might cut the long stringy stuff off so you have a long enough string.",
            "You can confuse the vector quote confused in the sense of the spectral method, computes something which you want to do is find this combinatorial thing, that's something else.",
            "But in the spectrum method doesn't find it.",
            "So for a lot of graphs you know if you have a hot dog or something.",
            "The spectrum method will find it, but you can construct examples where the spectral method fails and the point isn't that I want to claim that this is what your social network looks like, but the point is that if you have the example we presented in the early slides, 4 million nodes in 40 million edges.",
            "Right, I mean, you can imagine that thing is pretty sparse and that you have a lot of stringy type things or a lot of things that are tenuously connected.",
            "We'll get back to an example of that in a little bit, but.",
            "Imagine the things that you know are not so well held together.",
            "So we're going to see that, and so social graphs have graphs, have structures that are analogous to this.",
            "Deep so deep is down.",
            "I said all the plots will be down will be good so deep refers to the value of the objective function and it's yeah it's good.",
            "So Spielman Tang, so the spectral partitioning works on nice graphs, nicer bounded degree, planar graphs, and well shaped meshes.",
            "So you think it should work, planar graphs and things that are low dimensional, they actually showed that it does work, but you can construct examples you know that might not have the low dimensional intuition, but I'm just giving you a graph.",
            "It in claiming came from a nice place and that kwadrat."
        ],
        [
            "Factor actually could be the case, so a complementary way to view spectral is not that I take an eigenvector operation.",
            "I do the power method along those.",
            "In computing eigenvector.",
            "You know what I told you is that the spectral methods in approximation algorithm will get back to other ones related to flow spectrums, approximation algorithm and I have this intractable objective and filtering this objective through a nice place.",
            "And then I return a set of nodes.",
            "And I'll do that through a different nice place with flow and then returned.",
            "And the question is, what are these nodes look like relative to the objective?",
            "So what are these nice places look like?",
            "So so for spectral.",
            "The Rayleigh quotient to characterize Lambda one.",
            "The second eigenvalue is this thing.",
            "It's a quadratic form on the top and a different quadratic form on the bottom.",
            "So if you've seen this, you know what this means.",
            "If you haven't, I don't want to go into details of what this means, except to say that it's a quadratic form that where it measures, how well mixing.",
            "Parts of the graph are so if you have a deep cut, a bottleneck, two complete graphs and a small number of edges between them, that's going to bottleneck to random walks, right?",
            "We start here.",
            "Move around right now it's going to a lot of effort to go through that thing.",
            "So this characterizes the mixing in this characterizes the variance.",
            "So you want to minimize mixing subject to variance constraint and relating bed.",
            "The graph related to that.",
            "What you're saying is I'm going to bed the graph and align this eigenvector and cut, but the line is defined by mixing this.",
            "Subject to variance constraint.",
            "Alright, but we need to require for reasons that I said I'm not going to X to be perpendicular to the all ones vector, so it turns out this is equivalent to this.",
            "These two are equivalent.",
            "So these look very different actually.",
            "The reason we want to be perpendicular to the all ones vector.",
            "If you're familiar something called the trivial eigenvector, that's the all ones vector.",
            "So you typically have an eigenvalue that's either zero or one.",
            "And that's going to be an interesting for certain reasons, and the variance is things that are perpendicular to that.",
            "So if you wrote down these two expressions and tried to relate them to each other, they wouldn't be at all equal.",
            "But if you're going to be perpendicular to that vector, then these will be.",
            "So what you have is the same thing on the top, the same variance condition, but something very very different on the bottom.",
            "And now you have a different quadratic form with a bunch of these.",
            "These are diagonal degrees.",
            "Now this has a very different interpretation.",
            "Not maximizing mixing subject to variance constraint on the line.",
            "What you're saying is I want to minimize or minimize.",
            "I want to minimize mixing subject to mixing, 'cause this is a quadratic form, but mixing now in a complete graph.",
            "So the sum there is I&J in the graph and the sum there is all inj.",
            "So minimizing mixing in your graph relative to mixing in the complete graph.",
            "So in a sense, what you're saying is I want to take a complete graph and we're saying a complete graph is a first cousin of an expander, so I want to take a complete graph.",
            "And I want to quote embed my my data graph in the complete graph and I want to know how well it fits and you can use duality ideas and we'll get back to lower bounds related to duality.",
            "So we're really thinking of filtering the graph through either a low dimensional place or a complete graph, and so certain parts of social networks will have low dimensional properties.",
            "Will see that at certain size scales and certain and these social information, Alex will have expanded like properties, other size, skills and so understanding the connections between those two will be important."
        ],
        [
            "So a lot of local improvement methods.",
            "And let me not get into the details of those, except to say that.",
            "Except to say that.",
            "A lot of 'em, sort of the things you intuitively think of.",
            "You know you have nodes in two sides, and the question is, can you move a node here?",
            "Can swap nodes around and sort of greedy way.",
            "So think of it as like a discrete version of coordinate descent or something.",
            "And these were methods that were developed early.",
            "And so they can get stuck in local minima very easily.",
            "But you can use them to refine cuts."
        ],
        [
            "Also.",
            "And one way that you can use local improvement methods is if you look at the graph at various size scales and the question is.",
            "What's relationship between those size skills?",
            "So for certain graphs?",
            "These low dimensional ones, maybe this one type relationship, and I've claimed that for the social information was very different and so we'll get back to that.",
            "But just to give you sort of visual example, so you want to partition this graph into two pieces.",
            "Maybe you want to cut off the land in the Sky or the land in the water.",
            "And so you'll do something called multiresolution partitioning.",
            "So you look at the graph at various size skills.",
            "So you take the graph and the graph is constructed from this by, you know some RGB.",
            "So similarity or something and you say, well, you know, rather than looking at the size scale, that's whatever you an inch on this screen.",
            "I want to look at scale to inches.",
            "4 inches 8 inches and you marked down here and then you get the courses possible.",
            "Cut you, cut it, you partition, and then you refine back, and at each refinement step you do some local improvement or something like that.",
            "So Chaco was maybe that's one of the first to do this.",
            "Medecins is very well known way to cut graphs.",
            "Uses variants of this rock lessons, a bunch of other things that do these sorts of techniques.",
            "Um?",
            "So if you actually pull down, quote will get back to using meds later for tearing apart the real."
        ],
        [
            "Yes, but they do something so like this.",
            "So maximum flow.",
            "So now let's switch from food from the local improvements and the spectrum to flow flow.",
            "Maybe something that people with a background in computer science or familiar more familiar with you have a directed graph G. You have a source and a sink.",
            "You have a capacity on each edge flow as a function such that all the flow in and flow of the same except at the source and sink.",
            "And the problem is to find the flow from the source to the sink, that's best.",
            "And the trick is that you also have capacity constraints.",
            "You can't just move it along all along the best way.",
            "So you need to find the things best subject to the capacity constraints.",
            "So this is a very very basic primitive.",
            "I mean, a lot of things will boil down to this.",
            "There's a lot of variance of this, and in particular you can have multiple sources and sinks.",
            "So that's going to be."
        ],
        [
            "Great that we're going to be interested, so single commodity flow has relationships to linear programming, Ford Focus, and is one of the original algorithms.",
            "Bunch of other methods.",
            "Most of the implementations now, I guess.",
            "Is a bunch of nice work by Goldberg and others that well, that and others are variants of what's called push relabel methods.",
            "You pushing flow around.",
            "And single commodity flow.",
            "There's something called the Max flow min cut theorem and Max flow.",
            "Min Cut says that if you want to maximize the flow subject to these capacity constraints.",
            "That's equal to the minimum cut in the graph and cut is expansion of conductance, so cuts the objective lunch today.",
            "So now I can tell you if I can solve this flow problem, then the maximum flow in the graph is equal to the minimum cut.",
            "Alright.",
            "One variant is that.",
            "And so that's a great result 'cause you can prove that amongst all of the combinatorially large number of cuts that you know that you have a lower bound.",
            "Do you want to search the mall?",
            "Yeah you can.",
            "You can get a fast certificate of a lower bound.",
            "If you have multicommodity flows meaning I have two or three or 10 or I can have all the way up to N. Choose two pairs.",
            "There's a couple different versions, but Max Flow is not equal to min cut.",
            "But the big breakthrough with Lightening Row back in the late 80s was that Max Flow is approximately equal to men caught up to a log K factor or K as a number of commodities.",
            "And the way this interfaces to graph partitioning is that we can look at all pairs multi commodity flow.",
            "So I'll then choose two pairs and so then you get the logarithm of N squared, which is going to log in."
        ],
        [
            "So single commodity flow, so one algorithm that you could have to solve for the best cut is the following.",
            "Look at all two to the end cuts.",
            "So I look at all to the end partitions in your graph.",
            "So the single commodity flow problem returned the best.",
            "So that's an algorithm that will work, right?",
            "It's not so fast, but that'll work.",
            "So the question is, can you do something faster?",
            "There is not fast enough to check all the partitions, so we can do something faster so we can look at the multi commodity flow version.",
            "We want our outflow between all pairs and choose two of them.",
            "And we want to do it all at once and then we want to find the edges that are and then and then cut the edges that are most congested because we think that the flow should reveal the cuts.",
            "So I said you get a log K factor.",
            "Log of N choose two is going to log in, so you're going to log in approximation.",
            "This is a very different type of approximation guarantee that we saw previously.",
            "Spectrolite spectral didn't say anything about and the number of nodes, but it didn't give you this quadratic factor.",
            "And the quadratic factors structural parameter of the graph that something that's very popular in a lot of areas of machine learning and scientific computing and so on.",
            "If you parameterized problems the way you know, we wouldn't throughout the computer science, and you want the algorithms that are fast.",
            "Depending on end, the number of nodes, but independent of any structural parameters of the graph.",
            "This is something that's going to make you much, much happier, 'cause it doesn't depend on structural paragraphs.",
            "It just depends on and the number of nodes.",
            "So you can detect solution if the idea is that can you detect solution if bottleneck forces those edges to be more congested than average?",
            "Now it's a question that you might ask before we saw that there are graphs that are quadratically bad.",
            "Are there graphs that are log in bed?",
            "Or is this a weakness in the analysis?",
            "So if you're going to be doing flow related algorithms, this is not a weakness in the analysis.",
            "There are graphs that are that bad.",
            "And those graphs are our old friends expanders.",
            "So expander graphs as I said, are graphs that don't have any good cuts.",
            "You every subset of nodes has a constant fraction of edges linking out.",
            "And that's not something you see if you have a low dimensional graph because bigger circles are better than smaller circles we said, and so there's a fixed.",
            "You know there's a lot of more stuff going on than at the surface, so expander graphs are graphs that you know have constant fraction of their edges linking out.",
            "And the simplest version is, I mean, think, think of this that it's pretty sparse.",
            "And so on average are most of your links, sort of nearby.",
            "You are far away.",
            "And so, in terms of the diameters in terms of the average diameter and so it turns out most of your links are pretty far away, so you're going to have and choose two pairs of nodes that you want to route commodity between, but most of the paths are going to be very long log in, and she's going to get an extra log in factor.",
            "So expander graphs really are that bad, and in particular it's not the case on expander graphs that you can detect the solution if the bottleneck for these graphs are more congested than average.",
            "So you get a global capacity constraint.",
            "Just because the diameters of the edges so you so the flow fails to reveal the best cuts in that sense.",
            "And again, this capacity constraint is very different than you see with spectral with spectral.",
            "If you have a complete grafana stringy thing, and you do one iteration of the power method and the second iteration of the power method, you're going to start to see that you're failing to push probability mass down the stringy things.",
            "So it's a very low.",
            "The notion of smoothing or regularization that you get with spectrum methods.",
            "Very local thing, but in certain senses that will pour it up.",
            "You'll see that locally, but also globally, but going from local to global is very graceful, mean here.",
            "Doing things locally is just fine.",
            "So you can think of this in terms of resistor circuits, in which case you can sort of short circuit the fact that you have a.",
            "A lollipop yeah, complete graph of the string thing and the flow will just go down that stringy piece very easily.",
            "But the capacity constraints comes not locally, but very globally on the expander graphs, and again, social networks will see maybe not surprising, especially the squint at them and look at them at large size.",
            "Skills are expanders.",
            "They have a lot of expanded like properties and."
        ],
        [
            "So.",
            "And so understanding the implicit properties here will be useful in sort of a complementary way to what it might be in spectral.",
            "So.",
            "These things you can view as integer programs or linear programs.",
            "And in the same way as I said, we could take spectral and write it some way and relax it.",
            "And and you gotta relax solution.",
            "You can write down.",
            "As an integer program, the multi commodity flow problem.",
            "You could also write it down as a linear program.",
            "And the linear program will be relaxation of the integer."
        ],
        [
            "And so the embedding of you of a flow.",
            "Again, we're sending these things from nice spaces.",
            "Is based on something called bourgain steering.",
            "This is every endpoint metric space will embed in L1 with distortion log in, so the flow algorithm not the way it actually implemented in code.",
            "But the basic idea is you could solve the LP on the previous page to get a distance function.",
            "Obtain the L1 embedding and this is going to be constructive.",
            "I'm not going to go into that, but deconstructive and then you do a rounding 'cause now I have a continuous thing you need around it back to a discrete thing, so here it's going to boil down to an embedding in a different embedding.",
            "Then we saw a spectral.",
            "Here you're embedding an L1 spectral so much more L2 type thing, and you're going to be using the ball game results.",
            "But it boils down to an embedding, and it turns out embedding this place expanders are the worst."
        ],
        [
            "So there's a number of implementation issues.",
            "Um?",
            "Maybe we should take 5 minutes off and just a minute or two and then come back.",
            "Um?",
            "So let me just have a couple more slides and then we'll take a bit of a break.",
            "Eigenvector code with Matlab relate pack depending on the details, sort of level of detail you want to get into the size of the graphs and so on.",
            "Often times take something like the number of nonzeros, because you're going to be doing something, it is very sparse.",
            "Menace is very nice, it's publicly available and you can use it very very fast in practice, but it doesn't sit on a stronger theory, but in fact some of the things will be talking about in a little bit will be using that.",
            "Single commodity flow takes something like N cubed time multi commodity, something like N squared.",
            "So then cubed end of the three halftime multipoint is going to be in squared.",
            "Those things going to expensive.",
            "Very large graphs but you can combine.",
            "Local versions of spectral and flow improve, and so on.",
            "To get something better, so maybe let me spend."
        ],
        [
            "5 minutes describing that will take a bit of a break, so the basic ideas hopefully have laid out, so I'm going to gloss over the spectral improve and in the flow, improving these sorts of things like that and just sort of mentioning quickly, but hopefully you have sort of the basic idea of how they work, and if you're interested I can describe it a little bit more detail how they work and give you pointers the local versions, but.",
            "You know, back to maybe why this is a good thing to think about.",
            "I mean so interesting, because what theory would say is that flow based methods are better than spectral and the reasons flow based method is better.",
            "Spectral will always give you login guarantee.",
            "Spectral might be good on expanders or loaded low dimensional things but basically unexpanded 'cause of quadratic of a constant is constant.",
            "Practice will say that.",
            "Something very different to practice, meaning machine learning.",
            "Scientific computing people are actually playing things, will, say, spectral methods are very fast.",
            "The robust, the robust, for reasons that we spent a fair bit of amount of time on.",
            "Feel the noise because of this local way that regularization is sort of implicitly implemented, and so there the method of choice.",
            "These people may not even know what flow methods are.",
            "If they do, they're not going to care, so this is, I think, sort of a fairly egregious maybe theory practice.",
            "Disconnect, right?",
            "You'd like the theory too.",
            "You know, if not if not.",
            "Solve practical problems immediately, provided these qualitative insight and so here we have a very, very simple question.",
            "Standard 10,000 feet you look down at the graph and you say, what does it look like?",
            "Is it a hot dog?",
            "Is it an expander?",
            "Is it something else and one group would say you know this is a better tool to use and this is how they look and the other group would say this is a better tool to use is how they look and their quality.",
            "I mean not even qualitatively in agreement.",
            "So now if your practitioner, you might say well, then I should just use spectral mean.",
            "Who cares what theory says?",
            "Because that's supposed to guide the practice.",
            "On the other hand, maybe a little less in KDD, but if you're a machine learning, you're very tired of spectral methods and you're going to think about spectrum.",
            "Method is coming from continuous places and you're going to think that if you go to infinite dimensions, that's going to give you a huge amount of freedom and solve all your problems.",
            "But these infinite dimensional vector spaces are still very structured versions of that, and not not intuitively, qualitatively like expanders.",
            "So having the idea of expanders and how expanders might behave and how.",
            "Two very different classes of approximation algorithms behave on these sorts of graphs.",
            "The question is, can we can we take these two very different tools?",
            "Very different types of graphs?",
            "Not that you know the social graphs that you would be encountering are low dimensional, or expand or anything like that exactly, can we use these to sort of playoff of each other and so in a sense, rather than being getting bad thing, this is actually a good thing.",
            "I mean, you know there's nothing better in the world than having sort of a big disconnect between what maybe theory and practice or whatever it says.",
            "And and having data to answer, there's nothing worse than having a disconnect like that and not having.",
            "Data to answer the question because then people to scream at each other.",
            "But I mean having a disconnect like that but having data to actually address some of these questions.",
            "I mean, that's a great thing, so that should be so pretty exciting.",
            "So this cut improvement algorithms, given a graph and an input cut, find a nearby cutter, certify that none exists, local algorithms and locally bias objectives.",
            "Oftentimes these will either be a local version of spectral or flow, and you want them to run in size depending on.",
            "Running time depending on the size of the output, can you combine spectral and flow in different ways, so you can certainly apply these ideas to other objective functions were not going to go into that.",
            "Cut improvement, you take a cut and you sort of want to improve it.",
            "You can do that with either spectral or flow is the short answer.",
            "Although actually doing it, the details will be quite a bit different because the local and global, local and global.",
            "Properties will be pretty different between these two.",
            "So."
        ],
        [
            "With respect to combining spectral and flow.",
            "There's a.",
            "There's a great result in terms of worst case analysis by or around Vazirani that in some sense says if you embed in yet a different place that is a little spectral, like in a little flow like.",
            "That the good things happen.",
            "You get it.",
            "You're better, worst case, approximation guarantee.",
            "In practice with this might mean is the following as sort of a data analysis tool.",
            "I do spectral so I glossed over."
        ],
        [
            "You know the local versions of spectral flow, but the idea is that you have an input cut and you want to use spectral or flow to say refine it or to ask can I get a little bit better cut in some sense, better objective function value being nicer or whatever, so say that I do the following.",
            "I take that."
        ],
        [
            "Graph I do spectral.",
            "This isn't the way it's usually described, but it's it's a dual view they usually describe.",
            "I do spectral, I get an eigenvector and I want to ask the question is this eigenvector?",
            "You know, quote the right direction.",
            "The direction that's along the worst case cut?",
            "Or did I compute something that's an artifact of my approximation algorithm is what I'm seeing.",
            "Is this direction dominated by the algorithm rather than the data?",
            "And so one way to answer that question is to do a local flow computation.",
            "So I didn't go into the details of the locality, but you can modify the objective functions to be locally biased.",
            "So if you can believe that you can do that in the in, the overheads, have the pointers that will the papers.",
            "If you believe that you can do that, then what you're saying is effectively can I use other approximation algorithm flow that fails and succeeds in very different places?",
            "To answer the question of whether this is the right direction?",
            "And as you can imagine, things are more complicated than yes or no.",
            "So the answer to this is a matching.",
            "So I start with the graphic computer vector and now I compute a weighted matching and I can add that to the graph and that's going to compute, give me a new graph G prime.",
            "I can do spectral on that.",
            "Again, I can ask the question is what I computed in artifact and you do that log in times and into some bells and whistles in the theory goes through?",
            "But you can also imagine that you want to do this in practice."
        ],
        [
            "So the original algorithm were around when he was, I think, end of the fourth rounder, the 5th.",
            "There's a bunch of work on these sort of.",
            "This is very much an online learning sort of algorithm idea.",
            "Couple those two together.",
            "The implement.",
            "We have implementations that work well on on on let's say for this particular thing you know 10s of thousands of nodes are smaller than the stuff will get too after the break, but reasonable size things.",
            "So."
        ],
        [
            "So with that maybe why don't we take a, you know, just a 5 minute break or something and then just to stretch legs and then I checked earlier and they said there's not a coffee break at the break I guess.",
            "So it's not like this 20 minutes off or something, so I'll start back in about 5 minutes on this Clock.",
            "And and.",
            "Will continue with the second half or second from the left."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, about a set of tools that maybe you can use to examine the structural properties of large social information graphs.",
                    "label": 1
                },
                {
                    "sent": "And this actually grew out of a couple of particular applied problems that I'll mention in Internet advertising.",
                    "label": 0
                },
                {
                    "sent": "But I think that the tools maybe are sort of much more general and part of the reason is that I think.",
                    "label": 0
                },
                {
                    "sent": "If you want to know, sort of roughly with these sorts of networks, look like I mentioned in quite a bit more detail the types of graphs we're talking about, but a fairly wide range of things that might fit under the umbrella of social and information networks.",
                    "label": 1
                },
                {
                    "sent": "They're actually pretty complicated things, as as you may know, if you work with them.",
                    "label": 0
                },
                {
                    "sent": "I mean are generated.",
                    "label": 0
                },
                {
                    "sent": "There tend to be fairly large, so there's a premium at on things that scale well algorithmically.",
                    "label": 0
                },
                {
                    "sent": "They tend to be fairly sparse, so there's going to be a premium on sort of regularization, regularity, or statistical properties there oftentimes generated in complex or adversarial environments, which means that a lot of the models that you might want to use in other applications or not sort of particularly appropriate.",
                    "label": 0
                },
                {
                    "sent": "And I think that for those reasons as well as others, I mean some of the more more straightforward techniques that have been used over the years, especially as some of these networks came to be.",
                    "label": 0
                },
                {
                    "sent": "And in the 90s and early, you know, last 10 years tend to perform less well, and so the question is, maybe you know, how can we try and understand these graphs?",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to do is described in a sense, basically, and sometimes the tools that.",
                    "label": 0
                },
                {
                    "sent": "That we had to invent in order to understand the structural properties of these graphs.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of stuff in the slides, and I'll gloss over some of it, and then some will go into a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "These slides are actually on the web.",
                    "label": 0
                },
                {
                    "sent": "I was giving this talk at ICML about a month ago.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a few typos that we picked up last time, and they may pick up this time.",
                    "label": 0
                },
                {
                    "sent": "Then I'll correct those, but these these are actually on the web now.",
                    "label": 0
                },
                {
                    "sent": "The uncorrected versions and you can download them, so I'll skip over some things, but you can look at those if you want, and if those are the particular things that are less interested in, that's OK too, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's lots and lots of networks out there.",
                    "label": 1
                },
                {
                    "sent": "Technological and communication networks, power systems, and autonomous autonomous systems, and Rd networks.",
                    "label": 1
                },
                {
                    "sent": "For instance, biological networks of various sorts.",
                    "label": 0
                },
                {
                    "sent": "What I'll call social information graphs.",
                    "label": 0
                },
                {
                    "sent": "So let's call it say, collaboration, networks, Friendship Networks, blog cross postings, advertiser bid phrase graphs.",
                    "label": 1
                },
                {
                    "sent": "So this is actually the application of this stuff arose.",
                    "label": 0
                },
                {
                    "sent": "Maybe financial economic network, so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So lots of sort of a popular way to think about.",
                    "label": 0
                },
                {
                    "sent": "Data these days and just to give you an example of the types of networks.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you should have in the back of your mind.",
                    "label": 0
                },
                {
                    "sent": "There was two there.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned those a couple of particular applications that gave rise to this line of work.",
                    "label": 0
                },
                {
                    "sent": "We notice a number of fairly surprising and counterintuitive properties, and then if in fact those were the case, it couldn't have been that they were peculiar to the two advertiser bid phrase graphs were looking at, so we went back and looked at on the order of 70 more.",
                    "label": 0
                },
                {
                    "sent": "So this is 10 of those 70.",
                    "label": 0
                },
                {
                    "sent": "They aren't particularly large or small, but maybe a representative sample.",
                    "label": 0
                },
                {
                    "sent": "The idea is that once you get beyond several 100 nodes or thousand nodes, things will change very very.",
                    "label": 0
                },
                {
                    "sent": "Much relative to much smaller networks that are studied in the Fort Networks literature almost exclusively, once you get up to 100 million edges or whatever, depending on the albums you want to run, it gets to be hard to put it on a single machine.",
                    "label": 0
                },
                {
                    "sent": "And if that's not true, now is true.",
                    "label": 0
                },
                {
                    "sent": "Few years ago, it depends on the size of your memory, but you start to worry about memory management issues.",
                    "label": 0
                },
                {
                    "sent": "So if you want to scale up an order of magnitude larger than that, you have started well managed memory management issue.",
                    "label": 0
                },
                {
                    "sent": "So think about this is between couple thousand a couple million nodes, so something under 100 million edges so for.",
                    "label": 0
                },
                {
                    "sent": "Instance something from live Journal, 4 million nodes, 40,000,000 edges.",
                    "label": 0
                },
                {
                    "sent": "So that's typical of size and maybe a sparsity.",
                    "label": 0
                },
                {
                    "sent": "So extremely extremely sparse.",
                    "label": 0
                },
                {
                    "sent": "Epinions Flickr, delicious, a bunch of coauthorship networks in physics and DBL physics and computer science Citation networks blog crossposting think of these sort of something capturing the way information propagates.",
                    "label": 0
                },
                {
                    "sent": "I'm going to draw loose distinction between.",
                    "label": 0
                },
                {
                    "sent": "A social and information, but that's sort of a loose distinction.",
                    "label": 1
                },
                {
                    "sent": "Snapshots of the web, bipartite affiliation graphs, office to papers or advertise, betrays a bunch of Internet graphs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you an example.",
                    "label": 0
                },
                {
                    "sent": "Of how this arose, so you're at Yahoo.",
                    "label": 0
                },
                {
                    "sent": "or Google, or Bing or whatever.",
                    "label": 0
                },
                {
                    "sent": "Yeah, determined the search box.",
                    "label": 0
                },
                {
                    "sent": "You get your spot.",
                    "label": 0
                },
                {
                    "sent": "You get your algorithmic results and you get your your advertisements.",
                    "label": 0
                },
                {
                    "sent": "So how do you determine the advertisement?",
                    "label": 0
                },
                {
                    "sent": "So as you probably know, that's a very nontrivial thing.",
                    "label": 0
                },
                {
                    "sent": "A lot of people spend a lot of time thinking about that.",
                    "label": 0
                },
                {
                    "sent": "So in the context of constructing networks, how would you maybe construct a graph from this?",
                    "label": 0
                },
                {
                    "sent": "Situation and what do you want to do with that graph so?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just as an example, you might want to.",
                    "label": 0
                },
                {
                    "sent": "I guess the color is not too good on this thing, but you might want to construct an advertiser keyword.",
                    "label": 0
                },
                {
                    "sent": "Bipartite graphs on the left hand side, this time discretization of the universe of advertisers on the right hand side of keywords, and you want to maybe do KDD or do some sort of knowledge discovery data mining on this sort of graph?",
                    "label": 0
                },
                {
                    "sent": "In order to what in order to provide new advertisements to maximize, click through rates or ROI or some objective function of interest.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to do this, you're going to quantify something this imperfect, but you can measure, and you're going to see how your algorithm performs.",
                    "label": 0
                },
                {
                    "sent": "So one of the most basic things you can ask about a graph or network or data set more generally.",
                    "label": 0
                },
                {
                    "sent": "So how does it cluster?",
                    "label": 0
                },
                {
                    "sent": "You have to split into two or more pieces, and So what are clustering or community related problems that you might arise that might arise in this particular setting?",
                    "label": 0
                },
                {
                    "sent": "So you might want to know something called marketplace depth broadening.",
                    "label": 1
                },
                {
                    "sent": "Find new advertisers for particular query of submarket.",
                    "label": 1
                },
                {
                    "sent": "You might want to do something like query recommender system suggested to advertisers.",
                    "label": 0
                },
                {
                    "sent": "Query that have high probability of clicks.",
                    "label": 1
                },
                {
                    "sent": "You might want to do, let's call it contextual query broadening.",
                    "label": 0
                },
                {
                    "sent": "You might want to broaden the users query in some sense and there's a range of reasons why you might want to do this.",
                    "label": 0
                },
                {
                    "sent": "So the two applications that gave Rise rise this line of work.",
                    "label": 0
                },
                {
                    "sent": "One was basically something with query expansion.",
                    "label": 0
                },
                {
                    "sent": "You want to not.",
                    "label": 0
                },
                {
                    "sent": "You do not have the advisor bit under single word, but maybe bid on this word in words like this.",
                    "label": 0
                },
                {
                    "sent": "Whatever that means, and so there's a range of ways to determine what that means.",
                    "label": 0
                },
                {
                    "sent": "Model in natural language instead of economic and strategic issues in terms of the auction.",
                    "label": 0
                },
                {
                    "sent": "So let's gloss that over and just ask in terms of the abstraction we have.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "And this gets back to the proverbial example that you're at the middle of a concept class.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can go to nearby queries and you'll get sort of a meaningful thing so you can bid not just on this one or that you thought it but nearby ones.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you're in the middle of two clusters like Jaguar, doing the expansion is going to be a little less robust because you don't know.",
                    "label": 0
                },
                {
                    "sent": "Which of the two meanings of the user might be intending?",
                    "label": 0
                },
                {
                    "sent": "So we wanted to broadening and so you want to plug natural clusters.",
                    "label": 0
                },
                {
                    "sent": "Another thing might be you want to do some sort of bucket testing.",
                    "label": 0
                },
                {
                    "sent": "Again, they sort of proverbial examples.",
                    "label": 0
                },
                {
                    "sent": "You want to pull up Palo Alto Flowers or San Jose Flowers because you want to pull out the flower market?",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "Sort of behavioral targeting.",
                    "label": 0
                },
                {
                    "sent": "So can you pull that out in a meaningful way?",
                    "label": 0
                },
                {
                    "sent": "Can you pull out a cluster of nodes that in some sense is a micro market?",
                    "label": 0
                },
                {
                    "sent": "In order to do whatever but in particular to bucket testing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The goal then is to find isolated markets.",
                    "label": 1
                },
                {
                    "sent": "Are clusters saying this advertise a bid phrase graph with sufficient money or clicks sufficient size that it's worth your time and sufficient coherence that you can pull it out and think about it as an economic market and you can reason about it and bring intuition to bear and you can try and say something meaningful in terms of the results of the bucket test.",
                    "label": 0
                },
                {
                    "sent": "One thing that's not usually asked is this even possible right?",
                    "label": 1
                },
                {
                    "sent": "I mean both in the academic literature, writing a paper, and if you're trying to push something into production, if you go and tell your manager or the reviewer as well, we couldn't find it, they're going to say well go find something else.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there's this sort of a selection bias to getting positive results, and so the usual Mo will be defined.",
                    "label": 0
                },
                {
                    "sent": "Some objective trying to pull something out if you pull something out, either write the paper pushing to production and if you fail to find something, define a slightly new objective look at a slightly different data set, that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do?",
                    "label": 0
                },
                {
                    "sent": "One of the things we're going to see is some of the tools will develop.",
                    "label": 0
                },
                {
                    "sent": "Will actually even be able to answer this question.",
                    "label": 1
                },
                {
                    "sent": "Is this even possible?",
                    "label": 0
                },
                {
                    "sent": "Namely, you know, is the hypothesis of the data, looks certain ways?",
                    "label": 0
                },
                {
                    "sent": "Is that even true?",
                    "label": 0
                },
                {
                    "sent": "Is that something we can even test?",
                    "label": 0
                },
                {
                    "sent": "So what's one way that we might want to?",
                    "label": 0
                },
                {
                    "sent": "Think about the data.",
                    "label": 0
                },
                {
                    "sent": "So one way you might think about the data is that the data looks something like this.",
                    "label": 0
                },
                {
                    "sent": "There's a gambling market.",
                    "label": 0
                },
                {
                    "sent": "And there's a sports market, and maybe there's an intersection of the sports and gambling.",
                    "label": 0
                },
                {
                    "sent": "Let's call it a sports market, some sort of hierarchical structure in the data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When we're drawing the graph like this, when we're putting it this sort of Samantha when putting this on the piece of paper, what we're saying is that you know there's a boundary here.",
                    "label": 0
                },
                {
                    "sent": "It may be imperfect and idealization with some sort of boundary between the gambling market and the rest of the world at the sports market.",
                    "label": 0
                },
                {
                    "sent": "In the rest of the world, that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "In the sense that we can pull that cluster out and think about these keywords as being more gambling like in these ones being less so.",
                    "label": 0
                },
                {
                    "sent": "And if the graph happened to have certain properties, namely you could embed it on a piece of paper, then that would be true.",
                    "label": 0
                },
                {
                    "sent": "But if the graph happened to be an expander or a tree, or something else like that, then it probably would not be true.",
                    "label": 0
                },
                {
                    "sent": "And if the graph happened to be more realistic graph where it's messy and it's hard to determine which of those sort of Nelson models or reference states, it looks like, how will you even answer that question so when you draw this, you're thinking that you know you have some sort of nice clustering structure that the data might fit well in a low dimensional space, and then you can think about it and its wonders.",
                    "label": 0
                },
                {
                    "sent": "You even if you don't aren't actually.",
                    "label": 0
                },
                {
                    "sent": "Saying, I don't want the data to bed well in a low dimensional space or something like that.",
                    "label": 0
                },
                {
                    "sent": "A lot of the data analysis tools that you might bring to bear on the problem sort of implicitly make that assumption.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you're going to be doing a recursive decomposition.",
                    "label": 0
                },
                {
                    "sent": "You split it in half two pieces and recurse with dynamic programming or any other range of things which you want.",
                    "label": 0
                },
                {
                    "sent": "Is that when you split it in half it's 5050 or 6040 or something like that.",
                    "label": 0
                },
                {
                    "sent": "You don't want to be pulling off 100 nodes out of the million node graph cousin.",
                    "label": 0
                },
                {
                    "sent": "The recursion depth will be very big and you can split into two pieces.",
                    "label": 0
                },
                {
                    "sent": "You have a left half and you have a right half and you have something in between stable left half of the gambling market in the right half and something in between.",
                    "label": 0
                },
                {
                    "sent": "So a lot of tools that you might use will implicitly be assuming that the data looks something like this.",
                    "label": 0
                },
                {
                    "sent": "So it's an empirical question, does the data actually look that way so that we?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You talking bout that?",
                    "label": 0
                },
                {
                    "sent": "So there's a range of ways you know you can represent the graph, and we're going to be talking about a particularly simple idealization.",
                    "label": 0
                },
                {
                    "sent": "We're going to be talking about the network effectively as a graph, as a bunch of nodes or nodes represent some sort of entities in a bunch of edges or edges represent some sort of interaction between the entities.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in particular you can talk about directed graph showing directed graphs.",
                    "label": 0
                },
                {
                    "sent": "Today everything we say will be able to be handled by weighted graphs or non weighted graphs very gracefully.",
                    "label": 0
                },
                {
                    "sent": "Directed this is a tricky thing that's an open problem and time evolution.",
                    "label": 0
                },
                {
                    "sent": "Some of these structural results that we will be talking about will shed light on time evolution, but that's sort of a different in terms of modeling the data.",
                    "label": 0
                },
                {
                    "sent": "That's a different thing, and we're not going to talk about that particular today, but some of the results, as I said, will be relevant for that.",
                    "label": 0
                },
                {
                    "sent": "But in terms of ways you want to wait, the graph is no, no issues there.",
                    "label": 0
                },
                {
                    "sent": "And the first thing to note is that graphs combinatorial and not obviously geometric things, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, if the graph happened to be constructed as a discretization of a nice place or geometric random graph or something.",
                    "label": 0
                },
                {
                    "sent": "Then it you can may be interpreted as being geometric, but you know if I give you an arbitrary graph, it's not.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's any geometry there, so you might ask what are we even talking about.",
                    "label": 0
                },
                {
                    "sent": "So the good thing is that you know you have.",
                    "label": 0
                },
                {
                    "sent": "That's a powerful framework for thinking about a lot of algorithmic questions in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "But the drawback is that you know if you're familiar with machine learning, geometry is really used in a very important way for doing learning and things like statistical inference.",
                    "label": 0
                },
                {
                    "sent": "And when you're looking at the graph, you don't necessarily want to be making the proverbial sort of beer diapers connection, because that's a high frequency itemset, and that's probably already been found.",
                    "label": 0
                },
                {
                    "sent": "What you want to do is do something like make a prediction as to whether this user in a given zip code at a given time of day and.",
                    "label": 0
                },
                {
                    "sent": "Given gender will click on a given at if a given size and it has a certain number of bells and whistles, and at that level of granularity, that sort of things probably never even occurred in the database, never mind be be a frequent itemset, and so you want to be doing some sort of implicit statistical model or or explicit statistical modeling and so understanding these inference questions is going to be essential.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're thinking about?",
                    "label": 0
                },
                {
                    "sent": "So again, a schematic illustration of a hierarchical graph is this, and this is basically the same graph that we saw two slides back.",
                    "label": 1
                },
                {
                    "sent": "You know we have one cluster, another cluster, and they coupled together in some way.",
                    "label": 0
                },
                {
                    "sent": "And you know, you might see this sort of figure in in papers.",
                    "label": 0
                },
                {
                    "sent": "We have some clump here, and there's lots of cross talk, and this figure actually cut from an EPS file cut from somewhere, and the the benefit of doing a couple of cuttings is that a lot of the off diagonal stuff with the level of granularity that you see is sort of glossed over, and so it highlights the structure that I'm hoping that it is actually there.",
                    "label": 0
                },
                {
                    "sent": "So when you see some structure like this with a lot of cross talk, is this evidence for this clusters there, or is that there's maybe clusters, but they're pretty tenuous.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think this is an adjacency matrix and we set the ordering there in some way.",
                    "label": 0
                },
                {
                    "sent": "You know I don't need to tell you 'cause I could have chosen one of 10 and we've gotten some similar clusters, yeah?",
                    "label": 0
                },
                {
                    "sent": "No, think of this.",
                    "label": 0
                },
                {
                    "sent": "I have, you know the discretization of the universe of.",
                    "label": 0
                },
                {
                    "sent": "Advertisers and a discretization of the universe of queries.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious how to come up the right level of granularity, but I've done that and I put an edge.",
                    "label": 0
                },
                {
                    "sent": "01 could be waited and so on, but 01 if advertiser I clicked on query or bid on query J.",
                    "label": 0
                },
                {
                    "sent": "And it could be weighed, and it could be whether they won the auction or someone clicked or something like that.",
                    "label": 0
                },
                {
                    "sent": "So there's the graph.",
                    "label": 0
                },
                {
                    "sent": "And then I do something like that to present it in this format.",
                    "label": 0
                },
                {
                    "sent": "So there's no difference between edges, Inter versus edges, intra and in fact the intervals intra is a distinction that I put on the graph after I after I've constructed it.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, so the the advertiser is interested in this query 'cause they bid on it.",
                    "label": 0
                },
                {
                    "sent": "Or if this was a click graph the user clicked on that page 'cause they're interested or something, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you construct a graph this way, and you want to ask questions.",
                    "label": 0
                },
                {
                    "sent": "So things people ask are usually fairly simple statistics, degree distributions, clustering coefficients, which is basically the probabilities of triangles close in the graph.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot a lot of work on that slightly more refined question is, are there natural clusters or communities and they have to define what you mean by clusters in communities and so on, but slightly more refined version of the of clustering coefficients.",
                    "label": 1
                },
                {
                    "sent": "And that's sort of the question will be talking about you know, for instance, if you want to find isolated markets with sufficient coherence and size, and then you basically asking that question.",
                    "label": 0
                },
                {
                    "sent": "How do the nails grow and involve?",
                    "label": 0
                },
                {
                    "sent": "That's a question I want to ask.",
                    "label": 1
                },
                {
                    "sent": "How do processes like decentralized search like diffusion information diffusion with cascades?",
                    "label": 0
                },
                {
                    "sent": "How do things like that behave on the graphs and ultimately in a lot of cases, not necessarily in all cases, but in a lot of cases you want to do some sort of machine learning, classification, regression or ranking, and so you might want to pull out features, but in some cases you don't necessarily want to do you want to do some sort of analysis and understand qualitative properties of the graph?",
                    "label": 1
                },
                {
                    "sent": "Because understanding qualitative properties of the graph will tell you whether you have roughly even the right model.",
                    "label": 0
                },
                {
                    "sent": "And so it's going to be a tradeoff here that we're not going to talk about in detail, but in some cases the question is you want to have a slightly finer understanding the graph.",
                    "label": 0
                },
                {
                    "sent": "In other cases, it's probably better to spend your time investing in a factor of 10 more data and cleaning it up.",
                    "label": 0
                },
                {
                    "sent": "And depending on the Zack questions, you want to ask, you want to under the other of those.",
                    "label": 0
                },
                {
                    "sent": "So I showed you what people want to think about the graphs and what they think about the graphs.",
                    "label": 0
                },
                {
                    "sent": "This is actually with these networks look like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, I'm just sort of a total mess when you try and embed it on the plane or on the piece of paper, or in three dimensions or anything, and the reason they look like a mess.",
                    "label": 0
                },
                {
                    "sent": "We'll get to later is that at the end of the day, we're going to expand or like, especially at large scale, so the grass for expanders.",
                    "label": 0
                },
                {
                    "sent": "Expanders are things that don't embed well in low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "Looking back to an expander, wooden expander is, but I mean I really think it's the case if you're doing data analysis, but certainly if you working on social and information graphs and you don't know what an expander is, you have at best one and probably 2 hands tide behind your back because they're very basic.",
                    "label": 0
                },
                {
                    "sent": "Types of graphs that are very counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "You've never heard of them, but but social graphs are that way, so we'll get back to what that is.",
                    "label": 0
                },
                {
                    "sent": "But The Dirty little secret here is that this is a publicly available graph, and we use publicly available software to visualize this.",
                    "label": 0
                },
                {
                    "sent": "If you knew the state of the art in graphs that are out there, you probably would not be able to tell me what this thing is, right?",
                    "label": 0
                },
                {
                    "sent": "But if you know the state of the art visualization algorithms, you probably would be able to tell me what visualization algorithm I use may because of these artifactual.",
                    "label": 0
                },
                {
                    "sent": "Rings or something?",
                    "label": 0
                },
                {
                    "sent": "So this visualization algorithm is in fact revealing more about the visualization algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this visualization is in fact revealing more about the visualization algorithm than about the graph being visualized.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a disturbing situation if you're interested in visualizing graphs.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it suggests that maybe you know, since these things are not particularly nice in some sense, that you know we can use the artifactual pre features of visualization, algorithms or algorithms more generally in order to extract insight, and so the tools I was referring to earlier are basically that we're going to be using worst case algorithms, not because we want to solve worst case problems.",
                    "label": 0
                },
                {
                    "sent": "But because those will be the formalization of our ideas, but we'll be using the statistical and geometric properties implicit in those worst case algorithms in order to reveal insight in the graphs instead of an experimental sense will be experimentally probing the graphs and using the worst case the statistical properties of geometric properties implicit in those worst case algorithms in order to reveal some insight into the graph.",
                    "label": 0
                },
                {
                    "sent": "So people do before we get to that, since this is what these things look like, they'll develop hierarchical models and core periphery models, small world models.",
                    "label": 1
                },
                {
                    "sent": "We have some underlying geometry, and yet edges in some way.",
                    "label": 0
                },
                {
                    "sent": "Heavy tailed models?",
                    "label": 0
                },
                {
                    "sent": "So we tend to link to higher degree nodes so people develop these sorts of models and then try and think about the graphs and they usually fit various statistics.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different ways you can look at these graphs.",
                    "label": 0
                },
                {
                    "sent": "So these are two.",
                    "label": 0
                },
                {
                    "sent": "I mean, to oversimplify, a large body of work and then you sort of doing it at a high level.",
                    "label": 0
                },
                {
                    "sent": "These are two very common paradigms to think about these lots of rest.",
                    "label": 0
                },
                {
                    "sent": "And will be interested in with the implicit.",
                    "label": 0
                },
                {
                    "sent": "Maybe geometric properties are in well intervention that for these statistical reasons.",
                    "label": 0
                },
                {
                    "sent": "But you know, with these graphs with these two models, classes of models say about the local properties of what's going on the graph versus the global properties.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "'cause if you want to make some claims about the San Jose flower market, it helps to know how that sits relative to the Palo Alto flower market or to the DC sandwich market or other things.",
                    "label": 0
                },
                {
                    "sent": "You know how the local and global properties fit together so heavy tailed models or power law graphs this.",
                    "label": 0
                },
                {
                    "sent": "These make you think about these assorted large size scales.",
                    "label": 0
                },
                {
                    "sent": "There's extreme heterogeneity in local environments, and that's usually quantified by the degree of the of the nodes.",
                    "label": 1
                },
                {
                    "sent": "There's extreme heterogeneity and indegree distribution, and the graphs are relatively unstructured.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, there's a bunch of generative mechanisms having with preferential attachment sort of models.",
                    "label": 0
                },
                {
                    "sent": "To generate these sorts of things and the details of the nature of the instruction is depends on the detail of the model.",
                    "label": 0
                },
                {
                    "sent": "But there's extreme heterogeneity in the local environments, relatively instructed otherwise, and this is as opposed to maybe, you know, local local clustering or structure that you want to think about in particular small size skills.",
                    "label": 1
                },
                {
                    "sent": "Alot of small world models will start with some sort of.",
                    "label": 0
                },
                {
                    "sent": "Pre specified geometry, 1 dimensional ring or a 2 dimensional lattice and then add edges randomly based on that and the idea there is that you want to get some sort of clustering locali.",
                    "label": 0
                },
                {
                    "sent": "And you can have heterogeneity or not depending on the details of the particular model.",
                    "label": 0
                },
                {
                    "sent": "Looking at some sort of clustering locally and then you add edges randomly in order to get some global property like the graph is searchable or that the graph is algorithmically searchable or whatever.",
                    "label": 0
                },
                {
                    "sent": "And so this is a very different sort of local global connection going on here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's two very different ways people think about analyzing networks.",
                    "label": 0
                },
                {
                    "sent": "Now a very different stream of work and data analysis.",
                    "label": 0
                },
                {
                    "sent": "As you probably know.",
                    "label": 0
                },
                {
                    "sent": "Ticular sort of machine learning variety.",
                    "label": 0
                },
                {
                    "sent": "Is to use it all loosely called geometric data analysis tools, and by this you know.",
                    "label": 1
                },
                {
                    "sent": "I mean sort of matrix methods.",
                    "label": 0
                },
                {
                    "sent": "Low rank methods, SVD, PCA, these sorts of things with some sort of geometric properties going on manifold methods are variants of low rank methods.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with those are.",
                    "label": 0
                },
                {
                    "sent": "And these geometric data analysis tools don't view the graph particularly, don't view the data, particularly graph.",
                    "label": 1
                },
                {
                    "sent": "You might construct a graph, but you're really thinking of it much more smoothly and continuously, so you're viewing the data.",
                    "label": 0
                },
                {
                    "sent": "You're modeling the data effectively, not as a graph, but as as a point cloud in RN and feature vectors.",
                    "label": 1
                },
                {
                    "sent": "Have M data points.",
                    "label": 0
                },
                {
                    "sent": "It's an M by N matrix, and these things are usually based on the singular value decomposition.",
                    "label": 1
                },
                {
                    "sent": "Will talk about what that is a basis for some of the things we talk about aggressive later, but the singular value decomposition is a very basic structure result in vector spaces.",
                    "label": 0
                },
                {
                    "sent": "Will get back to that an it's intimately related not only to algorithm attract ability but also to geometric properties and the geometry gives you a lot that's limiting certain ways, but it gives you a lot and in particular will give you stability, robustness, maybe capacity control basis for inference, things like that.",
                    "label": 0
                },
                {
                    "sent": "And so even if you're working in Hilbert spaces and that sort of stuff, if you're familiar with that machine, learning this is at the end of the day, which is sitting on top of some sort of geometric thing like this.",
                    "label": 0
                },
                {
                    "sent": "But this is very different than the story I described about the advertiser bitten phrase graphs.",
                    "label": 0
                },
                {
                    "sent": "We have a graph, it's a very combinatorial thing.",
                    "label": 0
                },
                {
                    "sent": "It's it's an expander that probably doesn't embed particularly well at all, and some low dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so a natural question might be, can these two approaches be combined in some way?",
                    "label": 1
                },
                {
                    "sent": "And it's not obvious that they can.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're thinking about a network, LinkedIn, you know the web, live Journal and advertising betrays.",
                    "label": 1
                },
                {
                    "sent": "You have in a sense, it's just a single data point is a snapshot of what you saw.",
                    "label": 0
                },
                {
                    "sent": "Yeah, Walmart transactions over the last month or year, it's just everything there is.",
                    "label": 1
                },
                {
                    "sent": "You think of the databases that everything there is.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how to view that as a matrix.",
                    "label": 0
                },
                {
                    "sent": "I mean you can write everything as a matrix, and so there's some sort of isomorphism there.",
                    "label": 0
                },
                {
                    "sent": "But in terms of thinking about questions, you might want to ask and so on.",
                    "label": 0
                },
                {
                    "sent": "It's it's not obvious how to let em or end the number of data points or features go to Infinity, 'cause it's a sort of a single snapshot, and so most work done in those areas don't particularly apply.",
                    "label": 1
                },
                {
                    "sent": "So you know, do we going to view the data as a single data point.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of feature vectors and you get there.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different results on these sorts of things so.",
                    "label": 0
                },
                {
                    "sent": "We have here in a sense, it's a question of how to model data right data?",
                    "label": 0
                },
                {
                    "sent": "Just click streams, their query logs or whatever they are, and so how are we going to model the data?",
                    "label": 0
                },
                {
                    "sent": "And so we want to sort of draw a connection between maybe one in computer science where you typically think of the data is discrete, whether it's a graph or or discrete clickstream.",
                    "label": 1
                },
                {
                    "sent": "In the discrete nature, is gives you basis for fast algorithms and thinking about fast algorithms.",
                    "label": 0
                },
                {
                    "sent": "In particular where I said maybe statistics or in other areas where you can make domain specific assumptions.",
                    "label": 1
                },
                {
                    "sent": "Well, let's say statistics broadly defined.",
                    "label": 1
                },
                {
                    "sent": "You thinking with the data is a little bit more continuous in the goal isn't to think about the data is everything there is.",
                    "label": 0
                },
                {
                    "sent": "But as a random instantiation of everything in the world and so you want to draw some sort.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inference, so they said in slightly more detail in computer science, so the data is really a record of everything that happened.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to process the data to find some sort of interesting patterns.",
                    "label": 1
                },
                {
                    "sent": "And those patterns are oftentimes frequent item sets or something like that, and those are oftentimes computationally hard, at least in models of data access.",
                    "label": 0
                },
                {
                    "sent": "Their appropriate for the size or scale of the data.",
                    "label": 1
                },
                {
                    "sent": "And in statistics you think about you know that you have some sort of instant random instantiation underlying process and the data is 1.",
                    "label": 0
                },
                {
                    "sent": "Example of that and the goal is to learn something about the world and the data.",
                    "label": 0
                },
                {
                    "sent": "So holy because it's the only insight you have into the world.",
                    "label": 0
                },
                {
                    "sent": "The rest is just talk and so the data is actually what your insight into what's actually going on.",
                    "label": 0
                },
                {
                    "sent": "And you want to posit some model to make some info.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to combine these approaches and we said they're very different.",
                    "label": 0
                },
                {
                    "sent": "There's certainly not incompatible, and I think if you look at just as an example ACM KDD meeting this meeting now versus five years ago versus 10 versus 15, I mean there's a real shift away from what you might want to call these data basic questions to having a more sophisticated understanding of statistical modeling questions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I think you're seeing the same thing on the other side, and insofar as sort of academic statistics isn't addressing a machine learning is is filling in the gap in early addressing.",
                    "label": 0
                },
                {
                    "sent": "Some of those questions, so statistics and probabilistic ideas are central to a lot of work on developing randomized algorithms for matrix and other problems, and that's been a lot of interest in the last 10 years.",
                    "label": 1
                },
                {
                    "sent": "Alot of intractable optimization problems on graphs or networks will yield to approximation when you make some sort of statistical assumptions about the network properties, so boosting is a computational his statistical procedure with a computational parameter also serves as a smoothing regularization parameter.",
                    "label": 1
                },
                {
                    "sent": "And as we'll see lots of times, approximation algorithms will implicitly regularize implicitly smooth things out, and the reason why, please smooth things out is that, as you may know, there's a lot of work done in the theory of approximation algorithms, and roughly what they say is that if you have an intractable problem and you want to fast approximation algorithm, one virginal way to do that is to embed the original graph in some other space, and if that space has nice properties, then you compute something down here.",
                    "label": 0
                },
                {
                    "sent": "There's a stretch factor, some sort of factor you lose in that embedding.",
                    "label": 0
                },
                {
                    "sent": "You compute something in that nicer space.",
                    "label": 0
                },
                {
                    "sent": "And then you make a statement about how it applies in the graph, and you lose that stretch factor.",
                    "label": 0
                },
                {
                    "sent": "You lose that plus a little bit.",
                    "label": 0
                },
                {
                    "sent": "And so you get an epsilon approximation or constant factor.",
                    "label": 0
                },
                {
                    "sent": "Or log in approximation.",
                    "label": 0
                },
                {
                    "sent": "So when you're putting in this nice place.",
                    "label": 0
                },
                {
                    "sent": "If the data is properties that are sort of synergistic with that niceness, then good things happen.",
                    "label": 0
                },
                {
                    "sent": "If not, then good things might not happen, and so the question might be understanding the properties of these places, the implicit statistical and geometric properties.",
                    "label": 0
                },
                {
                    "sent": "That you are implicitly doing when you're running a fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "'cause as we'll see in a lot of cases, you have some objective and you compute something.",
                    "label": 0
                },
                {
                    "sent": "Would you actually compute if you measure properties of what you actually computed there at least as much a result of the particular approximation algorithm approximation procedure, use as the objective is started with arguing about an objective without arguing about the procedure that you're going to use to compute it.",
                    "label": 0
                },
                {
                    "sent": "Isn't.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isn't maybe some meaningful so and so at the end of the day, you know question that we want to be able to ask is can we use these tools to sort of poke at the data, improve the data in order to understand what the data look like, and you know what do you mean by look like an?",
                    "label": 0
                },
                {
                    "sent": "I don't want to define exactly what I mean by look like and it's going to be a little bit more sort of qualitative objective than fit.",
                    "label": 0
                },
                {
                    "sent": "This classification function fit this regression function.",
                    "label": 0
                },
                {
                    "sent": "But the reason we want to know what the data look like is that that will sort of inform what may be the appropriate statistical models are or the appropriate way to think about the data.",
                    "label": 0
                },
                {
                    "sent": "Is it the case that there's a gambling clusterin and sports cluster and there's an intersection?",
                    "label": 0
                },
                {
                    "sent": "Or is that not even a useful way to think about the data?",
                    "label": 0
                },
                {
                    "sent": "So in a sense, if you take a step back and look at 10,000 feet and yes, what is the data look like?",
                    "label": 1
                },
                {
                    "sent": "Does it look like a hot dog?",
                    "label": 1
                },
                {
                    "sent": "Is there a left half in the right half that you can cut?",
                    "label": 0
                },
                {
                    "sent": "And if you're embedding the data loaded mental place, what you're saying is that it looks in some sense like a hot dog or pancakes, so if that's your graph?",
                    "label": 0
                },
                {
                    "sent": "And you can embed it that way.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is a left half on the right half, and it looks sort of like this pancake or hotdog?",
                    "label": 0
                },
                {
                    "sent": "Or is a data like a tree as they have some sort of hyperbolic structures?",
                    "label": 0
                },
                {
                    "sent": "Or does it look tree like?",
                    "label": 0
                },
                {
                    "sent": "Or maybe as a data something like, let's call it a point for lack of a better term, you know a complete graph or a sparse complete graph, namely an expander.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the goal here is to.",
                    "label": 0
                },
                {
                    "sent": "Is to address some of these issues.",
                    "label": 0
                },
                {
                    "sent": "So I want to cover algorithmic and statistical work on maybe identifying and exploiting what I'll call geometric structure in large graphs in large networks.",
                    "label": 1
                },
                {
                    "sent": "I will address some of the theory questions.",
                    "label": 0
                },
                {
                    "sent": "Some of the questions about addressing the theory practice gap.",
                    "label": 0
                },
                {
                    "sent": "And some empirical observations that we and others have made.",
                    "label": 0
                },
                {
                    "sent": "Understanding with these insights how they can translate to sort of very practical problems that the people worry about and some future directions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And I skip the thing.",
                    "label": 1
                },
                {
                    "sent": "Well, the things to keep in mind, if you ever more machine running, bent, even infinite dimensions in Euclidean space is going to be too limiting.",
                    "label": 0
                },
                {
                    "sent": "And that's because when you go to infinite amount of structured way and since you have to get enough data points to flesh out something.",
                    "label": 1
                },
                {
                    "sent": "And scalability and robustness will be central.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to get a million node graphs.",
                    "label": 0
                },
                {
                    "sent": "But will I want to be able to compute something that's roughly meaningful?",
                    "label": 0
                },
                {
                    "sent": "As opposed to just being totally an artifact of miscellaneous edges here and there that they're not so interesting.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll cover maybe popular algorithmic tools of the geometric flavor, SVD, and PCA.",
                    "label": 1
                },
                {
                    "sent": "These sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Basically, to sort of give some intuitions the way people think about it, and hopefully that will be something that's even if you haven't worked on it will be.",
                    "label": 0
                },
                {
                    "sent": "Something that you can make some points of contact because I think so.",
                    "label": 0
                },
                {
                    "sent": "People usually intuitively think about these datasets.",
                    "label": 0
                },
                {
                    "sent": "And then I want to talk about some graph algorithms and I'm going to use a certain clustering problem as a test case or Canonical example and the tools that apply more generally.",
                    "label": 0
                },
                {
                    "sent": "But I use a certain clustering problem as a Canonical example to talk about approximation algorithms and their implicit properties, the geometric underpinnings and this will be a graph partitioning problem or clustering problem.",
                    "label": 1
                },
                {
                    "sent": "This is of interest on the applied side, because if you're interested in finding clusters or communities, you've been doing some variant of this and on the on the theoretical side, because it's a very basic primitive that you can use to really understand what's going on.",
                    "label": 0
                },
                {
                    "sent": "And then talk about some novel insights on graph structure.",
                    "label": 0
                },
                {
                    "sent": "Does the graph look lower, high dimensional?",
                    "label": 0
                },
                {
                    "sent": "What does that even mean when you're dealing with something this far so noisy and so on?",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have 4 slides here in more detail.",
                    "label": 0
                },
                {
                    "sent": "Why don't I not go through those in?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detail now, but they'll be up on the web page and then I can get to the first piece.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here and let me pause to make sure that's what everyone's reasonably happy and things are reasonably clear.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I don't know what the problem is.",
                    "label": 0
                },
                {
                    "sent": "Expanders don't embed well in low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "I couldn't come up with a good pick.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Because I get a meaningful picture on the plane and you know, if you have a graph, I'll define experiment later, but it's something that's very, very well connected.",
                    "label": 0
                },
                {
                    "sent": "As opposed to having a left half and right have that you can cut as opposed to being a tree where there's other properties.",
                    "label": 0
                },
                {
                    "sent": "So think of a complete graph.",
                    "label": 0
                },
                {
                    "sent": "And if you sort of squint at that, you might want to say.",
                    "label": 1
                },
                {
                    "sent": "I mean there's there's no structure in that besides being a point.",
                    "label": 1
                },
                {
                    "sent": "An expander sparse version of that, but if you think of it in a certain price, is exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, but it's also hot dog.",
                    "label": 1
                },
                {
                    "sent": "I mean I'm I'm just saying look at 10,000 feet and sort of squint at the graph.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's what I mean by squinting here.",
                    "label": 0
                },
                {
                    "sent": "I mean, the idea is, you know if you think about the data this way in the data really look like that, you'll get some answer.",
                    "label": 1
                },
                {
                    "sent": "But maybe what's what's the right way to think about the data and and even though there's a little imprecise as is that if.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Good, so that's not what I mean that that that that that gets back to this question about an M features in dimension.",
                    "label": 0
                },
                {
                    "sent": "So thanks for clarifying that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I I don't want to mean I don't mean that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "The definitions of following.",
                    "label": 0
                },
                {
                    "sent": "So you're given an M by N matrix.",
                    "label": 0
                },
                {
                    "sent": "We're not making any statistical assumptions here, you just write in any M by N matrix.",
                    "label": 0
                },
                {
                    "sent": "You can write the following way.",
                    "label": 0
                },
                {
                    "sent": "I can write it as U times Sigma times V transpose.",
                    "label": 0
                },
                {
                    "sent": "So you as an orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Think of that as a rotation and V is an orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a rotation and Sigma is a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Alright, and the way to think about this is that a is a function going between M dimensional space and end dimensional space, and U&V are bases for these two spaces or spaces.",
                    "label": 0
                },
                {
                    "sent": "And if you fix a basis here and you fix a basis here, all A is all the matrix is is a bunch of diagonal scaling factors.",
                    "label": 0
                },
                {
                    "sent": "So that's a remarkable result, and in addition.",
                    "label": 0
                },
                {
                    "sent": "All these sigmas are non negative and you can order them from largest to smallest.",
                    "label": 0
                },
                {
                    "sent": "Some of which might be 0, and if you keep the first, that's the best rank one approximation of the matrix.",
                    "label": 0
                },
                {
                    "sent": "If you keep the first 2, that's the best rank two approximation, the matrix.",
                    "label": 0
                },
                {
                    "sent": "If you keep the 1st three, that's the best rank three approximation of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's actually a remarkably strong result if you think about more or less any other algebraic structure, that's false.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's actually a remarkable result.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so I think it's Diana Larry called this the Swiss Army knife of matrix decompositions of the Rolls Royce of numerical linear algebra, and the reason is, I mean arranger reasons.",
                    "label": 1
                },
                {
                    "sent": "But you know, if you have this matrix, it's like a Swiss army knife.",
                    "label": 0
                },
                {
                    "sent": "If you have this decomposition is like a Swiss army knife.",
                    "label": 0
                },
                {
                    "sent": "You can do sort of anything in the world with it, but you never need to compute it in the same way you have a Swiss army knife with all these saws and Hammers and things.",
                    "label": 0
                },
                {
                    "sent": "You're never going to the mall, but given it you have everything you need and so the question is which pieces of this do you want?",
                    "label": 0
                },
                {
                    "sent": "Order the insights this yields you.",
                    "label": 0
                },
                {
                    "sent": "If you need a basis for here, can you compute it some other way faster or vice versa?",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit of intuition is the following, so if you view the matrix as a function going from RN to RM back and forth, which is what it is, then the SVD is the basic structural result of vector spaces, and it's going to algorithmic.",
                    "label": 1
                },
                {
                    "sent": "In statistical consequences mentions, but the basic structure result that says you go from this space to this space.",
                    "label": 1
                },
                {
                    "sent": "And if you fix it, the one basis here in the other bases here is just a bunch of stretch factors.",
                    "label": 0
                },
                {
                    "sent": "And being just a bunch of stretch factors, I mean you should think of this.",
                    "label": 0
                },
                {
                    "sent": "The significant thing about the second or third or fourth or fifth singular component is that it's perpendicular to the top ones.",
                    "label": 0
                },
                {
                    "sent": "You know the exact orthogonality is a very strong requirement.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's going to manifest its footprint very strongly.",
                    "label": 0
                },
                {
                    "sent": "So if you truncate the SVD at K items, you get the best rank K approximation, and this is the way it's oftentimes formalized that a sub K is the argument with spectral.",
                    "label": 1
                },
                {
                    "sent": "Mohseni Unitarily invariant norm spectral intravenously.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall Rank K matrices and the idea of what's going on here.",
                    "label": 0
                },
                {
                    "sent": "When I said that you can order the sink, the sigmas from large to small is that you have some directions.",
                    "label": 0
                },
                {
                    "sent": "Or the data stretched out and you have some directions where they're not.",
                    "label": 1
                },
                {
                    "sent": "We're stretched out less, and so I asked, what's the direction?",
                    "label": 0
                },
                {
                    "sent": "Of maximum variance.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying I'm going to maximizing some Frobenius norm here.",
                    "label": 0
                },
                {
                    "sent": "For business numbers sum of squares basically, and this is this maximum stretch direction.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying is that when I write the data down and I want to know which is the maximum stretch direction, what's the maximum variance direction that's going to be the first singular vector?",
                    "label": 1
                },
                {
                    "sent": "And then if I want to know, conditioned on ignoring that direction, so looking at the perpendicular directions, what's the maximum stretch direction that's going to be the second singular vector?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "You know, maybe this one is slightly better in some sense, but you need to be exactly perpendicular to the first one, and so conditioned on that, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The maximum stretch direction.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one use of the singular value decomposition of data analysis is you want to think about the data points as points in a vector space, and this could be points from wherever and you've put them in a vector space so they could be graphed.",
                    "label": 1
                },
                {
                    "sent": "It could be trees, they could be strings that could be anything, and you're modeling them as a point.",
                    "label": 1
                },
                {
                    "sent": "In this sense of the word.",
                    "label": 0
                },
                {
                    "sent": "I have endpoints in dimensions.",
                    "label": 1
                },
                {
                    "sent": "This will give you a matrix with M rows and columns.",
                    "label": 0
                },
                {
                    "sent": "The Rose will correspond to points, and you may say that two objects are close if the angle between them is small, 'cause you put them on a sphere in normalized the variance away, and that's one thing people do.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, in late something called latent semantic indexing.",
                    "label": 1
                },
                {
                    "sent": "If you know what this is, this is you want to think about term document data.",
                    "label": 0
                },
                {
                    "sent": "Which isn't quite a social network, but it has some properties of that you know might be very large and might have similar sparsity properties and so on.",
                    "label": 0
                },
                {
                    "sent": "In which the intuition you'd have is, well, this this K most important concepts in the corpus, and so I'll say, why don't I apply SVD to pull out K directions?",
                    "label": 0
                },
                {
                    "sent": "Now, if you actually look at how much variance you explain by pulling out 10 directions, it's not so much.",
                    "label": 0
                },
                {
                    "sent": "Now, that being said, LSI was a big advance in 15 or 18 years ago when I was developed because it was so much better than they sort of competing methods because the competing methods are very, very combinatorial and gave you a lot of descriptive freedom but overfit the data or had other problems, and so this video is sort of a nice sweet spot, and so that's sort of a common theme.",
                    "label": 0
                },
                {
                    "sent": "Will see is that.",
                    "label": 0
                },
                {
                    "sent": "A lot of these tools which you want to find is a sweet spot between descriptive flexibility and algorithm intractability.",
                    "label": 0
                },
                {
                    "sent": "So the way to think about this is that you have a matrix and and you're taking advantage of the structural result about vector spaces.",
                    "label": 0
                },
                {
                    "sent": "And the fact that the structures that means very constrained.",
                    "label": 1
                },
                {
                    "sent": "So if you keep K directions.",
                    "label": 0
                },
                {
                    "sent": "There's only so many places to hide your sins and hide your mistakes because you're taking advantage of this very strong structure.",
                    "label": 0
                },
                {
                    "sent": "Result that as I said, aside from matrix, more less.",
                    "label": 0
                },
                {
                    "sent": "Any other algebraic structure doesn't have the geometry gives you robustness and so on.",
                    "label": 0
                },
                {
                    "sent": "And if you keep K or K + 1 where K is 50 or 100, you might get good results in terms of precision and recall, and that sort of things.",
                    "label": 1
                },
                {
                    "sent": "So you could interpret these directions, but you are very thin.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, when you start interpreting these things will get back to him and one of the things to note about why SVD might or might not be successful in LSI or in social networks with applications is the following.",
                    "label": 0
                },
                {
                    "sent": "So we already mentioned that often times a day to have a lot of heterogeneity, so there's some nodes that have very high degree, so the big variance in some notes that are very small in terms of the number of edges that they link to a number of other nodes.",
                    "label": 0
                },
                {
                    "sent": "Are they linked to number of edges they have?",
                    "label": 1
                },
                {
                    "sent": "So here's a theorem that particular assumption that there is a little strong, but you can vary it empirically.",
                    "label": 0
                },
                {
                    "sent": "Results are very robust to varying the assumptions of theorem.",
                    "label": 0
                },
                {
                    "sent": "I think from a helicopter Meteo that says basically the following, if you have a graph.",
                    "label": 0
                },
                {
                    "sent": "In addition to make sense of a graph, and you have a heavy tail over degrees.",
                    "label": 1
                },
                {
                    "sent": "So big big variance over degrees and limited power loss.",
                    "label": 0
                },
                {
                    "sent": "A big variance over degrees and the graph is pretty noisy.",
                    "label": 0
                },
                {
                    "sent": "Or unstructured to start to think about expanders in the limit of random graph, which is what they looked at, then you have a heavy tail over eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And heavy tail over eigenvalues eigenvalues equals singular values.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of a difference, but for what I'm talking right now, think of them as the same.",
                    "label": 0
                },
                {
                    "sent": "So eigenvalues and singular is very very late in the square of each other.",
                    "label": 0
                },
                {
                    "sent": "If you have a heavy tail over degrees and you're pretty noisy in the limits of random graph, then you have heavy tail over eigenvalues and So what that means is that if you're in the term document context or social graph and you keep 10 singular directions with SVD, or can I get directions you might get pick a number 20% of the mass.",
                    "label": 0
                },
                {
                    "sent": "20% of the information or variance whatever, and you say that's good.",
                    "label": 0
                },
                {
                    "sent": "I've gotten 10 concepts.",
                    "label": 0
                },
                {
                    "sent": "But if you ask yourself, well, in a little bit more refined, why?",
                    "label": 0
                },
                {
                    "sent": "What did I in fact have?",
                    "label": 0
                },
                {
                    "sent": "So I have some lead part of the information, but what if I want to get a little bit more?",
                    "label": 0
                },
                {
                    "sent": "What if I want to get 90% or 20% of the information?",
                    "label": 0
                },
                {
                    "sent": "So if you have a heavy tail over eigenvalues, what that means is that in order to get 20% of the mass or information you need to keep not 11, not 20, but something like 100 singular values or 100 eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And to get 30% of the information needed to 1040% need 10,000, I mean a scale for something like a scale for each, and so now you're keeping a massive amount of.",
                    "label": 0
                },
                {
                    "sent": "Massive number of directions because the exact orthogonality of your densifying things exactly where you want to not be densifying them because you're pushing, you're starting to get on the tail.",
                    "label": 0
                },
                {
                    "sent": "Lots of things get pretty sparse, and so you're going to have problems because you'll be statistically overfitted.",
                    "label": 0
                },
                {
                    "sent": "So you know that's maybe why latent factor type models?",
                    "label": 0
                },
                {
                    "sent": "If you're familiar, Netflix, one of the models that they used in the mix was latent factor on bunch of the models that mixes latent factor type models and that gets out of course information.",
                    "label": 0
                },
                {
                    "sent": "But fine grained information tends to be lossed.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you can have a general matrix.",
                    "label": 0
                },
                {
                    "sent": "You can have a square matrix.",
                    "label": 0
                },
                {
                    "sent": "You can have an SPS dia, symmetric positive semidefinite matrix or kernel.",
                    "label": 0
                },
                {
                    "sent": "And in data analysis a structural properties of SVD are used most often in square, maybe adjacency matrix or kernels, but the basic result of those square matrix results or symmetric positive semidefinite results.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are using is a singular value decomposition the same thing that LSI is using?",
                    "label": 0
                },
                {
                    "sent": "So algorithmic issues?",
                    "label": 0
                },
                {
                    "sent": "So it's bigger is a lot of subtleties, exact computations take something like N cubed time, but typically you don't need to compute everything because it's it's a Swiss army knife.",
                    "label": 0
                },
                {
                    "sent": "You know if you want the top K left singular vectors, it can be computed faster using various sorts of iterative methods.",
                    "label": 1
                },
                {
                    "sent": "Oftentimes there's not a. I mean, there never is.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a practical matter, but you know not much of a gap between the K for the K plus first singular value, so that's a fair question to ask, you know, do I really need the exact case?",
                    "label": 0
                },
                {
                    "sent": "But maybe they I can add some mixing.",
                    "label": 0
                },
                {
                    "sent": "Plus The Cave plus first and the latter is typically the case.",
                    "label": 0
                },
                {
                    "sent": "You can get specialized numerical methods for large sparse matrices, and there's been a lot of work on the last 10 years in theoretical computer science and numerical linear algebra on randomized algorithms or epsilon approximation algorithms, and these are some types of interest because you can compute things faster and you only suffer epsilon error.",
                    "label": 1
                },
                {
                    "sent": "But also these are interested because even if they don't compute things faster, they take roughly the same amount of time they often times are better, and they're better because they're worse, right?",
                    "label": 0
                },
                {
                    "sent": "If I give you an answer that's not exactly 10 digits but is exactly 1 digit or two digits, namely an epsilon approximation.",
                    "label": 0
                },
                {
                    "sent": "That may be better, even though it's a worse, you know, by a naive measure, discounting the exact position that may be better when you plug it to a downstream application.",
                    "label": 0
                },
                {
                    "sent": "Basically because you haven't exactly fit the particular data at hand, which is much things out a little bit.",
                    "label": 0
                },
                {
                    "sent": "So if I give you an approximation, think a small number of steps of the power method rather than exact answer, you oftentimes early stopping or things like that, or epsilon approximations in other ways will be more useful for downstream applications like classification and clustering, because you haven't sort of overfit to the particular problem at hand, so that's something that we'll get back to, and we'll see I mean, that's an example of it in a vector context, but that's sort of intuitively what's going on in graph approximation.",
                    "label": 0
                },
                {
                    "sent": "Algorithms will get back to that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Principle components analysis and multidimensional scaling and other sorts of things as sort of basically using.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SVD, so let me go, not get into the details of that.",
                    "label": 0
                },
                {
                    "sent": "And to say that in terms of maybe statistical properties, you can always compute the best rank approximation if the data is nice and Gaussian, there's a natural statistical interpretation here.",
                    "label": 1
                },
                {
                    "sent": "And more generally, you can think of it as some sort of model selection where the class of models you're working with is something that has some nice geometry.",
                    "label": 0
                },
                {
                    "sent": "And the data may or may not look like that, right?",
                    "label": 0
                },
                {
                    "sent": "If you're dealing with a term document matrix or social graph, it's very sparse and may not have that geometry.",
                    "label": 1
                },
                {
                    "sent": "But your model selecting in that sense and putting yourself in a nice place.",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of connections, at least squares regression and PCA in terms of optimality.",
                    "label": 0
                },
                {
                    "sent": "In terms of, you know, being the optimal.",
                    "label": 0
                },
                {
                    "sent": "Being the optimal solution within a class of unbiased estimators and so on.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, relatedly, are geometric questions having to do with the SVD?",
                    "label": 1
                },
                {
                    "sent": "And this video is it basically is is computing the lead axes of that hot dog or pancakes?",
                    "label": 0
                },
                {
                    "sent": "You know the maximum variance directions if you have data that consist of sort of nice things like nice Gaussians and the issues not.",
                    "label": 0
                },
                {
                    "sent": "There's nothing magical about a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you have anything that's pretty dense, where measures going to concentrate, and so the question here is is measure going to concentrate in some nice way?",
                    "label": 0
                },
                {
                    "sent": "So can you think of the data is being asymptotically?",
                    "label": 1
                },
                {
                    "sent": "Or maybe the data really not in the asymptotic regime and so you need to take into account those properties.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you think about all those sorts of algebraic structures like tensors or things, computing the rank of a tensor is intractable.",
                    "label": 1
                },
                {
                    "sent": "Rank abstractions may not even exist.",
                    "label": 0
                },
                {
                    "sent": "Lots of other hardness results, and so on.",
                    "label": 0
                },
                {
                    "sent": "Non negative matrix factorization's.",
                    "label": 0
                },
                {
                    "sent": "You know nothing is convex.",
                    "label": 0
                },
                {
                    "sent": "The original work didn't even converge for Saddle Point and then people found variance of albums you converge the saddle points and much much weaker in terms of which you can say, so it's much weaker in terms of the algorithmic results, but much greater in terms of descriptive flexibility.",
                    "label": 0
                },
                {
                    "sent": "And that's maybe not such a good thing.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be wanting to work with a fairly restricted class of things, so these geometric properties in approximation algorithms will do that for us, so vector spaces here very, very structured places, and that's something maybe that you can take advantage of.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a lot of work in kernel methods in machine learning, and I'm going to skip over some of these slides, except to say if you're familiar with kernel methods in machine learning, that's basically what we've been talking about, except.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something called kernels, which is basically symmetric positive semidefinite matrices.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which can be used to encode similarity information, but so I'm going to skip over these, but these are up on the web page if you want.",
                    "label": 0
                },
                {
                    "sent": "So interpreting the SVD.",
                    "label": 0
                },
                {
                    "sent": "So what you want to say is the first singular direction means something.",
                    "label": 0
                },
                {
                    "sent": "One concept class, the second singular direction means something.",
                    "label": 0
                },
                {
                    "sent": "The third means something.",
                    "label": 0
                },
                {
                    "sent": "And by the time you get to the 4th or 5th with the fifth direction means that mean the most significant property of the 5th, 6th or 7th direction is that it's perpendicular to all the previous ones.",
                    "label": 0
                },
                {
                    "sent": "Right, because you're demanding, is active formality and the reason your instructor place and good things happen is that you're being very stringent about that, so that you at least more less.",
                    "label": 0
                },
                {
                    "sent": "Any data set.",
                    "label": 0
                },
                {
                    "sent": "I've looked at the most significant thing about the 10th singular direction is perpendicular to the top nine, so interpreting things you're in very thin ice.",
                    "label": 0
                },
                {
                    "sent": "And the reason you're very thin ice is the following.",
                    "label": 0
                },
                {
                    "sent": "If the date is nice and Gaussian, it's a hot dog like this.",
                    "label": 1
                },
                {
                    "sent": "The first singular direction means something in terms of lead axis of variation.",
                    "label": 0
                },
                {
                    "sent": "Second means something in terms of the second direction, but the data never looks like this, right?",
                    "label": 0
                },
                {
                    "sent": "It may look like this and maybe a combination of two hot dogs.",
                    "label": 0
                },
                {
                    "sent": "In which case the first direction points here.",
                    "label": 0
                },
                {
                    "sent": "The second direction points here, and neither direction means anything in terms of the data, so it's maybe a toy example where this would arise.",
                    "label": 0
                },
                {
                    "sent": "So say that the data consists of a bunch of sinusoids, and I have a couple of noisy sinusoids here.",
                    "label": 0
                },
                {
                    "sent": "Right there and then a bunch of decaying exponentials.",
                    "label": 0
                },
                {
                    "sent": "So the model for the data might be the sum.",
                    "label": 0
                },
                {
                    "sent": "Oscillatory factor is something decaying, and the data some combination of these two, so the singular directions you get are these two things.",
                    "label": 1
                },
                {
                    "sent": "You get that and that that both have sinusoidal and oscillating features, which is that direction and that direction, which is these two directions.",
                    "label": 0
                },
                {
                    "sent": "But those don't correspond to any of the generating processes in the data which I've told you, sinusoids exponentials.",
                    "label": 1
                },
                {
                    "sent": "So interpreting these directions in general, you're going to be pretty nice.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                },
                {
                    "sent": "Usually something in terms of data normalization, and it might be meaningful if you've done things carefully into designing on the data, but once you get beyond that gets much trickier, and basically it's for this reason.",
                    "label": 0
                },
                {
                    "sent": "That being said, you know I wanted to mention one thing that people think about this particular relevance to.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have something called centrality.",
                    "label": 0
                },
                {
                    "sent": "On the centrality of a vertex measures the relative importance of that vertex.",
                    "label": 1
                },
                {
                    "sent": "In a graph.",
                    "label": 1
                },
                {
                    "sent": "And so there's a bunch of different options.",
                    "label": 1
                },
                {
                    "sent": "Degree centrality is maybe the number of links incident Isaiah Pon, or I guess I'm ignoring directed this year sopana node so something related to the degree is between the centrality which would be high for vertices that occur in many shortest paths.",
                    "label": 1
                },
                {
                    "sent": "This closeness centrality eigenvector centrality.",
                    "label": 0
                },
                {
                    "sent": "Lots of different ways to capture this, and the idea might be for the eigenvector centrality.",
                    "label": 0
                },
                {
                    "sent": "This is going to have connections to high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "Because maybe those capture things that are important vertices are important, but if you look at the particular definition, it'll capture a bunch of other things.",
                    "label": 0
                },
                {
                    "sent": "So in terms of the data consists of two clusters and a small number of links between them.",
                    "label": 0
                },
                {
                    "sent": "Some of these notions of centrality will place these nodes being particularly important.",
                    "label": 0
                },
                {
                    "sent": "Because there are a lot of cross, I'm sorry there.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of crosstalk between the two clusters.",
                    "label": 0
                },
                {
                    "sent": "So you know if the data conforms with some generative processes that has intuition than the centrality measure might be appropriate.",
                    "label": 0
                },
                {
                    "sent": "But you have to ask what does that particular thing compute?",
                    "label": 0
                },
                {
                    "sent": "What if you have a graph as we'll see with the social graphs later, when the best clusters are extremely imbalanced.",
                    "label": 0
                },
                {
                    "sent": "But would some measure of this B and does interpreting these singular directions correspond to something meaningful?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing that I have a couple slides, I'm also going to gloss over, but at least want to mention is that even if you're not doing SVD or PCA or any of these things, these things are sort of under the hood and a lot of things that you might be doing in data more generally, but also in graphs will get too.",
                    "label": 0
                },
                {
                    "sent": "So we mentioned latent semantic indexing and manifold based machine learning methods.",
                    "label": 1
                },
                {
                    "sent": "There's alot of diffusion based methods, you pick a bunch of keywords and do a diffusion on the graph around that and try and pull out some sort of concept Cass.",
                    "label": 0
                },
                {
                    "sent": "If you're doing diffusion, you're doing something very eigenvector related, because eigenvectors are very related to diffusions.",
                    "label": 0
                },
                {
                    "sent": "If you think about the power method is the way to compute the top eigenvector.",
                    "label": 1
                },
                {
                    "sent": "K means clustering spectral partitioning.",
                    "label": 0
                },
                {
                    "sent": "Spectral partitioning is one way, one basically algorithm to cut up a graph.",
                    "label": 0
                },
                {
                    "sent": "There's other algorithms to solve the same objective.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K means what do we mean by this?",
                    "label": 0
                },
                {
                    "sent": "So K means is a way to cluster a set of nodes and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to say what you want to say is that the data correspond to five clusters?",
                    "label": 1
                },
                {
                    "sent": "I have some objective but I forgot to write that install.",
                    "label": 0
                },
                {
                    "sent": "Update those in on the slide.",
                    "label": 0
                },
                {
                    "sent": "There's a particular objective.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what it is, but the particular objective quantified the intuition that you know the data consists of five clusters like that and all the data points that sit around that.",
                    "label": 1
                },
                {
                    "sent": "Now if you look at this particular objective which I didn't write down and you relax it not to be something discrete, but to be something continuous.",
                    "label": 1
                },
                {
                    "sent": "Would you end up getting is the SVD solution?",
                    "label": 0
                },
                {
                    "sent": "So that relaxation says that this objective.",
                    "label": 1
                },
                {
                    "sent": "First cousin are in terms of relaxation, so it's very related to the SVD and that shouldn't be surprising is if you draw a picture like this.",
                    "label": 0
                },
                {
                    "sent": "What I have here is a couple hundred data points in five clusters.",
                    "label": 0
                },
                {
                    "sent": "So it should be very believable that we're doing something very very low dimensional here, right?",
                    "label": 0
                },
                {
                    "sent": "We have hundreds of data points is sitting on the plane in five nights interpretable clusters?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the gambling market.",
                    "label": 0
                },
                {
                    "sent": "That's a sports market is a bit of overlap, and it's a fair question to say, does a graph actually look like this?",
                    "label": 0
                },
                {
                    "sent": "So if you're doing K means clustering on a social graph, social networking information network?",
                    "label": 0
                },
                {
                    "sent": "If the graph looks like this, you'll probably get something meaningful.",
                    "label": 0
                },
                {
                    "sent": "If the graph looks like some of these other things, like a tree or expand or something.",
                    "label": 0
                },
                {
                    "sent": "I mean, who knows what you're going to get and try and interpret.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of things that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Under the hood you're using something very similar to these Eigen, Eigen Max spectral partitioning.",
                    "label": 0
                },
                {
                    "sent": "As we get back to later using something very similar.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line in terms of these things before moving, is that eigenvectors are very global entities.",
                    "label": 1
                },
                {
                    "sent": "The exact orthogonality gives you very strong global properties.",
                    "label": 0
                },
                {
                    "sent": "You need to be exactly orthogonal.",
                    "label": 1
                },
                {
                    "sent": "Eigenvectors loaded by linear structure.",
                    "label": 0
                },
                {
                    "sent": "You can generalize that to get nonlinear structure, but really it's linear in some other space.",
                    "label": 0
                },
                {
                    "sent": "When you're saying it's linear there, you're assuming get enough data points that you can flesh out some.",
                    "label": 0
                },
                {
                    "sent": "Your measure concentrates somewhere.",
                    "label": 0
                },
                {
                    "sent": "If in some sense you more expander like or or, it's not the case.",
                    "label": 0
                },
                {
                    "sent": "The measure of ever concentrates because so you generate the data in an adverse aerial way, and so there's some incentive for people to do something different than than that may.",
                    "label": 0
                },
                {
                    "sent": "You may never hit that regime and.",
                    "label": 0
                },
                {
                    "sent": "That being said, it would be nice to be able to use some of these geometric ideas, basically because these sorts of eigenvector methods identify some sort of sweet spot between descriptive flexibility in algorithm intractability, right?",
                    "label": 1
                },
                {
                    "sent": "Everything's going to be in cubed as opposed to two to the N certain things.",
                    "label": 0
                },
                {
                    "sent": "You're not going to find because of that, but they really give you sort of a sweet spot, and so can we.",
                    "label": 0
                },
                {
                    "sent": "Can we take advantage of some of these ideas more generally?",
                    "label": 0
                },
                {
                    "sent": "And so that's that will be.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm talking about now.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately not immediately obvious.",
                    "label": 0
                },
                {
                    "sent": "That we can see what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "Small scale structuring as I said, think of what's going on in the advertising bit of phrase graph in the Palo Alto flower market.",
                    "label": 0
                },
                {
                    "sent": "When you talk about larger scale, think of.",
                    "label": 0
                },
                {
                    "sent": "You know a larger flower market or market in the in the same time in San Francisco Bay Area.",
                    "label": 0
                },
                {
                    "sent": "That's why we talked about Palo Alto flower markets, but if it's from wherever you're from, I guess.",
                    "label": 0
                },
                {
                    "sent": "So the DC flower market.",
                    "label": 0
                },
                {
                    "sent": "Think of what's going on in in the Bay Area market more generally, or a market in the US more generally or something.",
                    "label": 0
                },
                {
                    "sent": "So there's a much largest size systems, so it will turn out that the relationship between small scale structure and large scale structure is not reproduced.",
                    "label": 0
                },
                {
                    "sent": "Even qualitatively.",
                    "label": 0
                },
                {
                    "sent": "You'd like theory to provide any qualitative guidance being perfect, but give you a rough idea.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's not obvious.",
                    "label": 0
                },
                {
                    "sent": "It's hard to justify and will talk about justifying this, but it's not even close, not even qualitatively reproduced by popular network models.",
                    "label": 0
                },
                {
                    "sent": "Once you're better off just not doing anything 'cause they're not even qualitatively correct.",
                    "label": 0
                },
                {
                    "sent": "And the reason this is important is that this relationship governs diffusion of information.",
                    "label": 1
                },
                {
                    "sent": "If you want to understand viral marketing, it really matters whether the best cuts are 5050.",
                    "label": 0
                },
                {
                    "sent": "Or 99.1.",
                    "label": 0
                },
                {
                    "sent": "Or there's no good cuts at all.",
                    "label": 0
                },
                {
                    "sent": "It also discovered.",
                    "label": 0
                },
                {
                    "sent": "It governs things like decentralized search, dynamic properties, applicability of common data analysis and machine learning tools.",
                    "label": 1
                },
                {
                    "sent": "So governs all sorts of things in pretty subtle ways, and we'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "And, relatedly, as I said, I mean there's just sort of a big disconnect between popular data analysis tools and a lot of machine learning, and a lot of network properties, so low dimensional geometric tools are common, but you know, really the network.",
                    "label": 0
                },
                {
                    "sent": "It's hard to think about this is a set of feature vectors, it's just whatever it is.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you ran the Clock again, you'd get a very different web.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Around the Clock for 99 you get a very different web so.",
                    "label": 0
                },
                {
                    "sent": "With all this being said, let's switch now.",
                    "label": 0
                },
                {
                    "sent": "To some graph algorithms.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is to, in some sense of Dragon drawn, some of the intuitions that we've laid in terms of graph algorithms.",
                    "label": 1
                },
                {
                    "sent": "And think about these sorts of results in that context.",
                    "label": 0
                },
                {
                    "sent": "In order to understand the statistical properties and geometric properties involved in that, so that we can do better data analysis.",
                    "label": 0
                },
                {
                    "sent": "On these networks in sort of a structured way.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "A particular problem and the particular problem is that of graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "And so it's a fair question to ask why you think about graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "And if you're someone of a theoretical bent, a fair criticism is, you know you're think about graph partitioning, because that's the only thing you can prove theorems about.",
                    "label": 0
                },
                {
                    "sent": "So that's not such a good reason, but it's one reason it's a fair one.",
                    "label": 0
                },
                {
                    "sent": "If you have an applied bent, one reason you might want to be doing this is that it captures some notion of clustering or communities which you might think is a better reason, but there's a lot of other things that capture clustering in communities, and in fact might do a better job.",
                    "label": 0
                },
                {
                    "sent": "So the reason we talk with graph partitioning innocence is because, again, it's at sort of a sweet spot between the theory into practice.",
                    "label": 0
                },
                {
                    "sent": "It's an example of a problem that you can make very strong theoretical statements about.",
                    "label": 0
                },
                {
                    "sent": "Ann, and you can really understand the inner workings of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "And yet at the same time it does a pretty good job capturing people's intuition as to what they mean by clusters and communities.",
                    "label": 0
                },
                {
                    "sent": "Imperfect, and we get back to some of the problems with it, but you know it, it kept.",
                    "label": 0
                },
                {
                    "sent": "It captures a pretty good intuition as to what they mean.",
                    "label": 0
                },
                {
                    "sent": "So if we want to understand what the implicit properties are available in that sense, it's a good test case or good hydrogen Atom for understanding the method.",
                    "label": 0
                },
                {
                    "sent": "Understanding the method of how using approximation algorithms experimental probes should work.",
                    "label": 0
                },
                {
                    "sent": "Because we can draw on the theory that we know, but we can also try and map it to what practitioner would say as a natural cluster community or that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So the spectral methods is flow based methods of multiresolution algorithms will be talking about some of their implicit properties of the scalability issues, and where these sorts of algorithms succeed and fail, and why they might succeed and fail.",
                    "label": 1
                },
                {
                    "sent": "And once we have some of that, then we'll talk about.",
                    "label": 1
                },
                {
                    "sent": "Maybe some novel insights that all of this gives us on.",
                    "label": 0
                },
                {
                    "sent": "I think sort of very practical.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Applications.",
                    "label": 0
                },
                {
                    "sent": "So graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Sort of a general welcome meta problem.",
                    "label": 0
                },
                {
                    "sent": "So it really refers to a family of.",
                    "label": 0
                },
                {
                    "sent": "Optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Combinatorial optimization problems as opposed to continuous optimization problems.",
                    "label": 0
                },
                {
                    "sent": "But a family of combinatorial optimization problems where we have a graph.",
                    "label": 1
                },
                {
                    "sent": "And it's nicely color coding to partitions.",
                    "label": 0
                },
                {
                    "sent": "'cause this is a partitionable graph, but we have a graph that is nodes and edges and we want to cut the sets of nodes into.",
                    "label": 0
                },
                {
                    "sent": "Two pieces and you can imagine generalizations to three or K, but you want to cut the set of nodes into two pieces such that there's not much edge weight which is either number of edges of the graph is weighted.",
                    "label": 1
                },
                {
                    "sent": "It's some of the weight, so there's not much edge weight between the.",
                    "label": 0
                },
                {
                    "sent": "Two pieces and so we'll call that cut quality.",
                    "label": 0
                },
                {
                    "sent": "It's a good cut if there's not much edge weight between the two pieces, but also would like both sides to be pretty large.",
                    "label": 0
                },
                {
                    "sent": "We don't intuitively people find it a little dissatisfying and maybe the world is just as dissatisfying.",
                    "label": 0
                },
                {
                    "sent": "I mean, it might be the case.",
                    "label": 0
                },
                {
                    "sent": "That the world has certain bad properties, but people think it's a little bit intuitively dissatisfying.",
                    "label": 0
                },
                {
                    "sent": "When you partition a graph and you pull off 100 nodes out of a million, so we want both edges to be pretty pretty large.",
                    "label": 0
                },
                {
                    "sent": "So there's several standard formulas formulations of this.",
                    "label": 0
                },
                {
                    "sent": "You can ask for graph by section.",
                    "label": 0
                },
                {
                    "sent": "We impose a hard constraint 5050.",
                    "label": 0
                },
                {
                    "sent": "You can do a soft version.",
                    "label": 0
                },
                {
                    "sent": "I've paid a balanced by section.",
                    "label": 0
                },
                {
                    "sent": "You know 7030 or 8020.",
                    "label": 0
                },
                {
                    "sent": "You can use something called expansion or related objectives, where these last two examples of what is called Potion Cut objectives.",
                    "label": 0
                },
                {
                    "sent": "Because you fold these two factors, these two criteria into a quotient and use a cut size divided by some volume notion.",
                    "label": 0
                },
                {
                    "sent": "So the minimum of the size of A and the size of B.",
                    "label": 0
                },
                {
                    "sent": "Or the product of the two sizes.",
                    "label": 0
                },
                {
                    "sent": "I'll get back to the difference there.",
                    "label": 0
                },
                {
                    "sent": "And if you have extreme heterogeneity over things like degrees, it might make sense to do cut size divided by the volume of a.",
                    "label": 0
                },
                {
                    "sent": "Either the minimum volume of A and the volume of a compliment.",
                    "label": 0
                },
                {
                    "sent": "Or other product.",
                    "label": 0
                },
                {
                    "sent": "And so you should think of these two as being similar.",
                    "label": 0
                },
                {
                    "sent": "These two different volume notions as being similar when there's not a lot of heterogeneity over degrees.",
                    "label": 0
                },
                {
                    "sent": "And when some degrees are very, very high and some are very very small, at the latter, one might be better and the latter is something called conductance or normalized cuts.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar and computer vision, they call it normalized cuts.",
                    "label": 0
                },
                {
                    "sent": "And the form is something called expansion.",
                    "label": 0
                },
                {
                    "sent": "All right now, the difference between these two denominators.",
                    "label": 1
                },
                {
                    "sent": "Is the following in their equivalents in a certain sense and the senses?",
                    "label": 0
                },
                {
                    "sent": "All of these formulations are NP hard.",
                    "label": 0
                },
                {
                    "sent": "But if I solve the problem for the first denominator.",
                    "label": 0
                },
                {
                    "sent": "I have a factor of two approximation for the volume of the for the problem formalized with the other denominator.",
                    "label": 0
                },
                {
                    "sent": "So if I take the denominator to be them in of the two volumes.",
                    "label": 0
                },
                {
                    "sent": "Then have a factor of two approximation for the other problem being the product of the two volumes and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Now if I can solve the problem exactly, effective tools, pretty big.",
                    "label": 0
                },
                {
                    "sent": "If I can solve the problem to epsilon equals .001 error, the factor to survey if the best I can do is solve it to a factor of 50 or log in or something like that.",
                    "label": 0
                },
                {
                    "sent": "In fact it's pretty small, so in fact will see that that's the case.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to want to call these to the same now for any particular set of nodes.",
                    "label": 0
                },
                {
                    "sent": "That's going to make a big difference.",
                    "label": 0
                },
                {
                    "sent": "So I'm using the volume, the term volume to mean something vaguely related to size, and I guess I was a little precise because volume could be the cardinality of the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "Or it could mean this thing and calling volp which is volume, so that's the number of nodes, and that's something like the number of edges I'll define exactly what is in a little bit, but think of that is sort of a node based volume in edge base volume, and so if the graph is pretty regular, those should be the same because everyone has every notice 5 edges or whatever, but there's a big variability.",
                    "label": 0
                },
                {
                    "sent": "It's going to be different, so there's a couple different notions of size or volume.",
                    "label": 0
                },
                {
                    "sent": "And I want you to ignore the distinction between these two because.",
                    "label": 0
                },
                {
                    "sent": "For any set of nodes, they're going to be the same.",
                    "label": 0
                },
                {
                    "sent": "They one provides a slightly stronger balance towards good balance than the other, but for with respect we talking about later.",
                    "label": 0
                },
                {
                    "sent": "That difference is very, very small, and if you want to do very fine things in certain parts of the graph, this difference might not might matter, and it's a fair question to say, you know I might have code that optimizes one or allegorize one and not the other, and I'm going to gloss over some of those things, But for what we talked about today, those are identical.",
                    "label": 0
                },
                {
                    "sent": "So you shouldn't worry about the different notions of the denominator, that basically the same.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We'll be talking about, so we're talking about graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "As I said, since it so graph partitioning will capture a qualitative notion of connectedness.",
                    "label": 1
                },
                {
                    "sent": "Sometimes the data is processed to be connected, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's whatever clicks you saw today, so it's connected in that sense.",
                    "label": 0
                },
                {
                    "sent": "But the question might be a sort of how it's connected.",
                    "label": 0
                },
                {
                    "sent": "Will see that solving this problem will tell us sort of roughly, if you're looking at 10,000 feet and look down how is it connected with?",
                    "label": 0
                },
                {
                    "sent": "What's the best split into two?",
                    "label": 1
                },
                {
                    "sent": "It's a very well studied problem, both in theory and in practice practice, meaning implementing algorithms with practice.",
                    "label": 0
                },
                {
                    "sent": "Also meaning doing data analysis.",
                    "label": 1
                },
                {
                    "sent": "And lots of machine learning and data analysis applications are boiled down to this, so transductive learning or Community detection.",
                    "label": 0
                },
                {
                    "sent": "Alot of things have been reduced to this sort of problem, so we're going to be doing is running an approximation algorithm and we want the output of the approximation algorithm want to think about the output of the approximation is not something we settle for.",
                    "label": 1
                },
                {
                    "sent": "You know it's not like well, we can't solve the interactive problem, so we're going to settle for this.",
                    "label": 1
                },
                {
                    "sent": "The output of the approximation algorithm will oftentimes be better than the exact solution because the data is very, very sparse and noisy, and the fact that we're going to filtering the approximation of filters us through a nice place will give us some regularity properties.",
                    "label": 0
                },
                {
                    "sent": "We might actually be getting a better solution, so it's not really something we settle for.",
                    "label": 0
                },
                {
                    "sent": "And then I guess I'll touch on the randomizer approximation algorithms give better answers in the exact solution applied sense.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And maybe the reason is that fast algorithms capture sort of a notion of qualitative existence, and in certain data analysis applications.",
                    "label": 0
                },
                {
                    "sent": "So that's why I will be talking about it will get back to another reason a little bit later, but that's why we.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talking about it and this gets back to the figure I had before about it.",
                    "label": 0
                },
                {
                    "sent": "Stand up at 10,000 feet in.",
                    "label": 0
                },
                {
                    "sent": "You squint of the data and what is the data look like?",
                    "label": 1
                },
                {
                    "sent": "So say that we want we have a graph.",
                    "label": 0
                },
                {
                    "sent": "And we want to partition the graph into two sets of nodes are left half on the right, which is what this graph partitioning problems going to be doing.",
                    "label": 0
                },
                {
                    "sent": "And say you know related thing is that we want to find the best fit of the adjacency matrix to be something like this.",
                    "label": 1
                },
                {
                    "sent": "So I've put a bunch of columns here in a bunch of columns there, and you know, Alpha is some parameter that describes how well these columns talk to each other.",
                    "label": 0
                },
                {
                    "sent": "And gamma is something describing how all these columns talk to each other, and beta is the cross talk.",
                    "label": 0
                },
                {
                    "sent": "I haven't given a particular objective, but you can imagine a bunch of objectives that capture this.",
                    "label": 0
                },
                {
                    "sent": "And so then you know the question is what is the data look like if you squint at it?",
                    "label": 0
                },
                {
                    "sent": "So one thing the data might look like is if the data is low dimensional.",
                    "label": 0
                },
                {
                    "sent": "If it's a hot dog or a pancake, then the data is going to look like something like this.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a left half and right half in the crock.",
                    "label": 0
                },
                {
                    "sent": "Crosstalk is going to be smaller, left half and right half, and the crosstalk be smaller.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that the interface between the left and the right half is one dimension less than the left half and right half, so that's why it's going to be smaller, very different, sort of data structure.",
                    "label": 0
                },
                {
                    "sent": "The graph might have if you squint at it.",
                    "label": 0
                },
                {
                    "sent": "Is to give sort of naive example.",
                    "label": 0
                },
                {
                    "sent": "If the graph is bipartite, bipartite means you have advertisers have bid phrases and no advertisers linked to each other, they always link to phrases and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So rectangular matrix may capture this.",
                    "label": 0
                },
                {
                    "sent": "In which case the left half talks to the right half of the lighthouse talks the left half, but there's very little cross talk.",
                    "label": 0
                },
                {
                    "sent": "If the data is an expander, then there's very little talked at all.",
                    "label": 0
                },
                {
                    "sent": "You know, and a possibility is that the top left hand corner is large, meaning the left half talkswitch itself a lot.",
                    "label": 0
                },
                {
                    "sent": "The bottom right small and the crosstalk isn't in between.",
                    "label": 0
                },
                {
                    "sent": "In terms of Alpha betas and gammas, it's certainly a possibility.",
                    "label": 0
                },
                {
                    "sent": "Might be hard to picture with this thing looks like.",
                    "label": 0
                },
                {
                    "sent": "Will get back to that and so think for a little bit.",
                    "label": 0
                },
                {
                    "sent": "You know what is?",
                    "label": 0
                },
                {
                    "sent": "What would the graph look like if this is really what the graph looks like?",
                    "label": 0
                },
                {
                    "sent": "So gay.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back to that, but maybe it's worth thinking about now and why are we going to worry about both criteria here?",
                    "label": 1
                },
                {
                    "sent": "So some graphs, maybe space like graphs Rd networks which are like hot dogs and pancakes Rd networks?",
                    "label": 0
                },
                {
                    "sent": "Random geometric graphs.",
                    "label": 0
                },
                {
                    "sent": "Whatever finite element meshes so a lot of stuff that's done in scientific computation where where the data comes from something low dimensional when stars blowup there's only so many ways for information to go from here to here, and so that imposes strong regularity that you don't see in social information graphs typically.",
                    "label": 0
                },
                {
                    "sent": "The cut quality and the cut balance work together.",
                    "label": 1
                },
                {
                    "sent": "This synergistic in the sense that.",
                    "label": 0
                },
                {
                    "sent": "If you take if you take a larger sets of nodes so the X axis here is the enclosed volume.",
                    "label": 0
                },
                {
                    "sent": "You could take larger sets of nodes.",
                    "label": 0
                },
                {
                    "sent": "Then in all these plots down will be good.",
                    "label": 0
                },
                {
                    "sent": "I'll be showing lots of plots down, will be a good always.",
                    "label": 0
                },
                {
                    "sent": "If you look at the Y axis, which is.",
                    "label": 0
                },
                {
                    "sent": "The conductance of the expansion down is also good, so larger graphs, larger sets of nodes have a given graph and I asked what's the cut value or what's the conductance value or whatever down is good, and so they work together.",
                    "label": 0
                },
                {
                    "sent": "I want to balance constraint.",
                    "label": 0
                },
                {
                    "sent": "So I want to be out on the right in terms of.",
                    "label": 0
                },
                {
                    "sent": "The size and so they set down as good.",
                    "label": 0
                },
                {
                    "sent": "That's good because larger things are better.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And So what you see in a lot of space like graphs and it is that it goes down, and the intuition is that bigger circles are better than smaller circles, right?",
                    "label": 0
                },
                {
                    "sent": "Bigger circles are better than smaller circles.",
                    "label": 0
                },
                {
                    "sent": "And maybe circles are better than triangles, but you know they all go down.",
                    "label": 0
                },
                {
                    "sent": "So we'll see in a lot of real world parallel graphs is not that they're expanders.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're up here and everything is bad, but that small things tend to have.",
                    "label": 0
                },
                {
                    "sent": "They tend to be good small clusters, but the best ones that are larger and larger get worse and worse.",
                    "label": 0
                },
                {
                    "sent": "So in a lot of social and information graphs, and this is a very very, very general property, I mean most any graph you look at will have this property that's very, very different than things that you might want to intuitively think about as being low dimensional.",
                    "label": 0
                },
                {
                    "sent": "It'll get worse and worse.",
                    "label": 0
                },
                {
                    "sent": "And that makes it awkward because it's not like you can just optimize objective.",
                    "label": 0
                },
                {
                    "sent": "You'll get a whole profile, whole range of things at this tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Larger things that are worse, larger things still that are still worse.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the lay of the land in terms of graph partitioning.",
                    "label": 1
                },
                {
                    "sent": "So we're going to be partitioning graphs.",
                    "label": 0
                },
                {
                    "sent": "We want to cut them up.",
                    "label": 0
                },
                {
                    "sent": "And in terms of the theory, but also the implementation, here's basically the lay of the land, and we'll go into a bit of detail about each of these.",
                    "label": 0
                },
                {
                    "sent": "So there's a range of.",
                    "label": 0
                },
                {
                    "sent": "There's a range of spectral methods, so spectral methods are going to be very related to SVD.",
                    "label": 0
                },
                {
                    "sent": "An eigenvector methods that we're talking about.",
                    "label": 0
                },
                {
                    "sent": "And here, spectral methods basically mean we compute an eigenvector of a certain matrix related to the graph.",
                    "label": 0
                },
                {
                    "sent": "So we computed eigenvector computer single direction.",
                    "label": 0
                },
                {
                    "sent": "We put all the nodes down on this direction.",
                    "label": 0
                },
                {
                    "sent": "We cut somewhere.",
                    "label": 0
                },
                {
                    "sent": "And in theory you cut with, you know, Rule A and practices will B or C or D Souza range of things, but they all sort of captured the idea that this to have graph and you should cut somewhere.",
                    "label": 0
                },
                {
                    "sent": "So there's a rounding rule that says, where are you going to cut once you give in this direction, and that matters depending on the particular application you're thinking about, or whether you're trying to prove theorems, in which case you want something that's a little bit simpler.",
                    "label": 0
                },
                {
                    "sent": "But there's also the question that I have a general direction.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is the wrong direction.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is the direction which case you're never going to get the right thing if you're pointing sort of in very much the wrong direction, so we're computing eigenvectors, no rounding.",
                    "label": 0
                },
                {
                    "sent": "Local improvement.",
                    "label": 0
                },
                {
                    "sent": "You know I have a set of.",
                    "label": 0
                },
                {
                    "sent": "I have a input partition and I might want to tweak it and do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So not surprisingly, these get trapped in local minima very easily, but they can be used to clean up cuts from other methods.",
                    "label": 1
                },
                {
                    "sent": "I'll get back to an example that we've talked about sizes this smaller markets versus the larger markets.",
                    "label": 0
                },
                {
                    "sent": "Can you view the graph?",
                    "label": 0
                },
                {
                    "sent": "And it's very easy to view space like graphs, right?",
                    "label": 0
                },
                {
                    "sent": "You know you look at the floor and you say here's the size scale of an inch.",
                    "label": 0
                },
                {
                    "sent": "Years aside scalable foot, here's a size scale of a mile.",
                    "label": 0
                },
                {
                    "sent": "He can coarse grain over that.",
                    "label": 0
                },
                {
                    "sent": "Can you do something analogous in graphs?",
                    "label": 0
                },
                {
                    "sent": "Not naturally space like so do some sort of multiresolution, so there's a lot of stuff done and space like graphs and multiresolution, and this flow based methods.",
                    "label": 0
                },
                {
                    "sent": "So flow based methods at the end of the day have to do with Max flow and min cut.",
                    "label": 0
                },
                {
                    "sent": "So Max flow and min cut Max.",
                    "label": 0
                },
                {
                    "sent": "This is a very classical problem sitting on top of something called duality that will get back to you later.",
                    "label": 0
                },
                {
                    "sent": "This is important for a lot of what we're going to be interested in.",
                    "label": 0
                },
                {
                    "sent": "In particular if you remember a while back I said can we actually test the hypothesis that the data looked the way I presented it, and the way it tests the hypothesis to get a negative result as opposed to pulling something off the shelf and seeing what worked to say maybe the data does not look this way will be to use duality, which is going to be related to Max Flow, min cut ideas and so here you want to single commodity flow or a multi commodity flow.",
                    "label": 0
                },
                {
                    "sent": "In order to see if the flow can reveal bottlenecks in the graph, and the hope is that if this is a good cut, then if you're trying to route flow through that, then the flow revealed that.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll talk about each of those methods.",
                    "label": 0
                },
                {
                    "sent": "So start with spectral methods.",
                    "label": 1
                },
                {
                    "sent": "So the work goes back to the to the 70s before you had high quality numerical code, high quality numerical code for these problems really probably came of age in the 80s.",
                    "label": 0
                },
                {
                    "sent": "So very popular in in scientific computing and parallel computing.",
                    "label": 1
                },
                {
                    "sent": "In the 80s, lots of machine learning work and I would type.",
                    "label": 0
                },
                {
                    "sent": "They will fix it.",
                    "label": 0
                },
                {
                    "sent": "A lot of machine learning work.",
                    "label": 0
                },
                {
                    "sent": "Starting with the normalized cuts, if you're familiar with that paper.",
                    "label": 0
                },
                {
                    "sent": "But a lot of work had been around, you know, before that, but in machine learning in the 2000s and not the two hundreds, I guess I have to fix that too.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm that is computed an eigenvector and you never need an exact eigenvector, just need a vector.",
                    "label": 0
                },
                {
                    "sent": "That's who's really quotient is about what, right?",
                    "label": 0
                },
                {
                    "sent": "So there's some robustness there.",
                    "label": 0
                },
                {
                    "sent": "And then perform a rounding.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the theorem that you get and I should have.",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't have it, but the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Basically an eigenvector of the Laplacian in the Laplacian is a matrix that's going to be related to the graph.",
                    "label": 0
                },
                {
                    "sent": "Think of the Laplacian as the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "But you might normalize it and and take into account the diagonal entries.",
                    "label": 0
                },
                {
                    "sent": "And I think we will get, well, we'll get back to that, time permitting.",
                    "label": 0
                },
                {
                    "sent": "But that's the matrix that we're actually going to communion vectors of, and the theorem that you get is that if Lambda is the 2nd eigenvalue of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Anfi is the conductance member, fires expansion, or fires.",
                    "label": 1
                },
                {
                    "sent": "Conductances, two different denominators.",
                    "label": 0
                },
                {
                    "sent": "And I told you to ignore the distinction.",
                    "label": 0
                },
                {
                    "sent": "If fires your conductance.",
                    "label": 0
                },
                {
                    "sent": "That's going to be intractable to compute.",
                    "label": 0
                },
                {
                    "sent": "But now if computer and eigenvector and we know that eigenvectors are fast, it's a fair question.",
                    "label": 0
                },
                {
                    "sent": "Said computing eigenvector of a billion node graph and will type for me will touch on that.",
                    "label": 0
                },
                {
                    "sent": "But certainly up to millions and hundreds of millions.",
                    "label": 0
                },
                {
                    "sent": "It's not such a problem.",
                    "label": 0
                },
                {
                    "sent": "Lambdas and #5 and number and so how good is 5?",
                    "label": 0
                },
                {
                    "sent": "This thing that we compute quickly?",
                    "label": 0
                },
                {
                    "sent": "And so you can show that Lambda is less than five, is less than root Lambda, and is effective two in a route 8.",
                    "label": 0
                },
                {
                    "sent": "Very sorry, Lambda is less than five, less than root Lambda.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's a couple constants, but the idea is that Lambda or Lambda over 2 is a lower bound and square root of Lambda is an upper bound.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there's nothing here about the number of nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar with the theory of algorithms, it's a little bit strange because you know it's not a log and approximation or root in.",
                    "label": 0
                },
                {
                    "sent": "There's nothing about the number of nodes, but we get a quad factor that's quadratically good in terms of the structural parameter of the graph is the land will have a natural interpretation to the structural properties of the graph.",
                    "label": 0
                },
                {
                    "sent": "Alright, and in fact there's other versions.",
                    "label": 1
                },
                {
                    "sent": "This, let's call it a test version of this for any test vector, so malhas.",
                    "label": 0
                },
                {
                    "sent": "Something let X be any vector that's perpendicular to the all ones vector, and that's very important for reasons that let me not get into then there's going to be a cut along X that satisfies this thing, which is related to this thing being greater than 5 squared, so that's squared as your same quadratic factor.",
                    "label": 1
                },
                {
                    "sent": "So I don't need to be the exact eigenvector.",
                    "label": 1
                },
                {
                    "sent": "I can be in approximate eigenvector or I can get bound in One Direction with any vector.",
                    "label": 0
                },
                {
                    "sent": "So this result is a much more general and robust results.",
                    "label": 0
                },
                {
                    "sent": "It's called Cheeger's inequality, 'cause Cheeger proved it in the continuous setting but along and Millman and Sinclair and others in the 80s got dis.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Treat versions of this and the idea is that.",
                    "label": 0
                },
                {
                    "sent": "If you compute this vector.",
                    "label": 0
                },
                {
                    "sent": "In the adjacency matrix, looks something like this for Alpha and gamma are large and the crosstalk is pretty small.",
                    "label": 0
                },
                {
                    "sent": "Then when you put the set of nodes on the vector, this eigenvector Q you put them there.",
                    "label": 1
                },
                {
                    "sent": "Here's a bunch of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Here's another bunch of the nodes and you want to cut somewhere, and we're going to cut, so this is a toy example.",
                    "label": 0
                },
                {
                    "sent": "It should be obvious where to cut, but the generalization is that it's something like this, so this will be up here and I'll go down a little bit and he should cut where it crosses zero or where there's a maximum change here, or there's a bunch of rules, but you'll cut somewhere like this, But the thing you should keep in the back of your mind is that this actually looks a lot like.",
                    "label": 0
                },
                {
                    "sent": "The K means problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "There's a left half and right half.",
                    "label": 0
                },
                {
                    "sent": "There's two nodes and they both fit pretty well.",
                    "label": 0
                },
                {
                    "sent": "So it's a fair question to say, what if this isn't satisfied?",
                    "label": 0
                },
                {
                    "sent": "Or what if the cuts are not well balanced?",
                    "label": 1
                },
                {
                    "sent": "What are you?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get.",
                    "label": 0
                },
                {
                    "sent": "So we'll get back to that, but it's a different way to look at.",
                    "label": 0
                },
                {
                    "sent": "The question is to say, well, you know what if you know the graph doesn't look like this, how bad can things be?",
                    "label": 1
                },
                {
                    "sent": "Can I come up with a graph that sort of looks as different than this is possible.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or, relatedly, is this quadratic factor?",
                    "label": 0
                },
                {
                    "sent": "Maybe this isn't a weakness of the analysis, the theorems just aren't good enough, or maybe that quadratic factor really is there and it's an artifact of the spectral technique graphs exist that are that bad.",
                    "label": 0
                },
                {
                    "sent": "So the answer to that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oceans, yes, perhaps do exist that are that bad?",
                    "label": 0
                },
                {
                    "sent": "So Gordon Miller constructed in node graphs with the spectral by section, cut his end of the 2/3 versus the optimal event of the 1/3 so quadratically bad.",
                    "label": 0
                },
                {
                    "sent": "And the particular construction doesn't matter as much will be talking about.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea actually is going to be essential for we talking about.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is we talked about for Spectrum is going to be related to diffusions, right?",
                    "label": 0
                },
                {
                    "sent": "Because to compute the first eigenvector?",
                    "label": 0
                },
                {
                    "sent": "Maybe not the best way numerically, but you know intuitively you can think of it as do with the power method.",
                    "label": 0
                },
                {
                    "sent": "You take take some arbitrary vector random vector, hit it with the matrix several times and you'll converge to the best solution and long, so some other things to do.",
                    "label": 0
                },
                {
                    "sent": "Variance of that.",
                    "label": 0
                },
                {
                    "sent": "So there's something very much like a reinforcement of diffusion going on.",
                    "label": 0
                },
                {
                    "sent": "So like a random spanning tree as opposed to minimum spanning tree.",
                    "label": 0
                },
                {
                    "sent": "We don't take into account this reinforcement and so.",
                    "label": 0
                },
                {
                    "sent": "The worst case graphs take advantage of.",
                    "label": 1
                },
                {
                    "sent": "Let's call it long stringy things.",
                    "label": 0
                },
                {
                    "sent": "Because diffusion is related to probability mass and takes a lot of effort to push probability mass down, a 1 dimensional line.",
                    "label": 0
                },
                {
                    "sent": "Versus some other constructions that you have two pieces to talk to each other so you know if you.",
                    "label": 0
                },
                {
                    "sent": "If you're diffusing on the plane or in three dimensional space on an expander, thing is going to be very different than if using online and so they have to call it cockroach grass, but you take cockroach graph that has very long legs.",
                    "label": 0
                },
                {
                    "sent": "So long stringy things and then a bunch of other stuff.",
                    "label": 0
                },
                {
                    "sent": "And the spectral method.",
                    "label": 0
                },
                {
                    "sent": "The question is you want the eigenvector to point up and down because that will give you what you might want to think is the best cut.",
                    "label": 0
                },
                {
                    "sent": "With the eigenvector method might cut that off, might cut the long stringy stuff off so you have a long enough string.",
                    "label": 0
                },
                {
                    "sent": "You can confuse the vector quote confused in the sense of the spectral method, computes something which you want to do is find this combinatorial thing, that's something else.",
                    "label": 0
                },
                {
                    "sent": "But in the spectrum method doesn't find it.",
                    "label": 0
                },
                {
                    "sent": "So for a lot of graphs you know if you have a hot dog or something.",
                    "label": 0
                },
                {
                    "sent": "The spectrum method will find it, but you can construct examples where the spectral method fails and the point isn't that I want to claim that this is what your social network looks like, but the point is that if you have the example we presented in the early slides, 4 million nodes in 40 million edges.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean, you can imagine that thing is pretty sparse and that you have a lot of stringy type things or a lot of things that are tenuously connected.",
                    "label": 0
                },
                {
                    "sent": "We'll get back to an example of that in a little bit, but.",
                    "label": 0
                },
                {
                    "sent": "Imagine the things that you know are not so well held together.",
                    "label": 0
                },
                {
                    "sent": "So we're going to see that, and so social graphs have graphs, have structures that are analogous to this.",
                    "label": 0
                },
                {
                    "sent": "Deep so deep is down.",
                    "label": 0
                },
                {
                    "sent": "I said all the plots will be down will be good so deep refers to the value of the objective function and it's yeah it's good.",
                    "label": 0
                },
                {
                    "sent": "So Spielman Tang, so the spectral partitioning works on nice graphs, nicer bounded degree, planar graphs, and well shaped meshes.",
                    "label": 1
                },
                {
                    "sent": "So you think it should work, planar graphs and things that are low dimensional, they actually showed that it does work, but you can construct examples you know that might not have the low dimensional intuition, but I'm just giving you a graph.",
                    "label": 0
                },
                {
                    "sent": "It in claiming came from a nice place and that kwadrat.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Factor actually could be the case, so a complementary way to view spectral is not that I take an eigenvector operation.",
                    "label": 0
                },
                {
                    "sent": "I do the power method along those.",
                    "label": 0
                },
                {
                    "sent": "In computing eigenvector.",
                    "label": 0
                },
                {
                    "sent": "You know what I told you is that the spectral methods in approximation algorithm will get back to other ones related to flow spectrums, approximation algorithm and I have this intractable objective and filtering this objective through a nice place.",
                    "label": 0
                },
                {
                    "sent": "And then I return a set of nodes.",
                    "label": 0
                },
                {
                    "sent": "And I'll do that through a different nice place with flow and then returned.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what are these nodes look like relative to the objective?",
                    "label": 0
                },
                {
                    "sent": "So what are these nice places look like?",
                    "label": 0
                },
                {
                    "sent": "So so for spectral.",
                    "label": 0
                },
                {
                    "sent": "The Rayleigh quotient to characterize Lambda one.",
                    "label": 1
                },
                {
                    "sent": "The second eigenvalue is this thing.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic form on the top and a different quadratic form on the bottom.",
                    "label": 0
                },
                {
                    "sent": "So if you've seen this, you know what this means.",
                    "label": 0
                },
                {
                    "sent": "If you haven't, I don't want to go into details of what this means, except to say that it's a quadratic form that where it measures, how well mixing.",
                    "label": 0
                },
                {
                    "sent": "Parts of the graph are so if you have a deep cut, a bottleneck, two complete graphs and a small number of edges between them, that's going to bottleneck to random walks, right?",
                    "label": 0
                },
                {
                    "sent": "We start here.",
                    "label": 0
                },
                {
                    "sent": "Move around right now it's going to a lot of effort to go through that thing.",
                    "label": 0
                },
                {
                    "sent": "So this characterizes the mixing in this characterizes the variance.",
                    "label": 0
                },
                {
                    "sent": "So you want to minimize mixing subject to variance constraint and relating bed.",
                    "label": 1
                },
                {
                    "sent": "The graph related to that.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is I'm going to bed the graph and align this eigenvector and cut, but the line is defined by mixing this.",
                    "label": 0
                },
                {
                    "sent": "Subject to variance constraint.",
                    "label": 0
                },
                {
                    "sent": "Alright, but we need to require for reasons that I said I'm not going to X to be perpendicular to the all ones vector, so it turns out this is equivalent to this.",
                    "label": 0
                },
                {
                    "sent": "These two are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So these look very different actually.",
                    "label": 0
                },
                {
                    "sent": "The reason we want to be perpendicular to the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar something called the trivial eigenvector, that's the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "So you typically have an eigenvalue that's either zero or one.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be an interesting for certain reasons, and the variance is things that are perpendicular to that.",
                    "label": 0
                },
                {
                    "sent": "So if you wrote down these two expressions and tried to relate them to each other, they wouldn't be at all equal.",
                    "label": 0
                },
                {
                    "sent": "But if you're going to be perpendicular to that vector, then these will be.",
                    "label": 0
                },
                {
                    "sent": "So what you have is the same thing on the top, the same variance condition, but something very very different on the bottom.",
                    "label": 0
                },
                {
                    "sent": "And now you have a different quadratic form with a bunch of these.",
                    "label": 0
                },
                {
                    "sent": "These are diagonal degrees.",
                    "label": 0
                },
                {
                    "sent": "Now this has a very different interpretation.",
                    "label": 0
                },
                {
                    "sent": "Not maximizing mixing subject to variance constraint on the line.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is I want to minimize or minimize.",
                    "label": 0
                },
                {
                    "sent": "I want to minimize mixing subject to mixing, 'cause this is a quadratic form, but mixing now in a complete graph.",
                    "label": 1
                },
                {
                    "sent": "So the sum there is I&J in the graph and the sum there is all inj.",
                    "label": 0
                },
                {
                    "sent": "So minimizing mixing in your graph relative to mixing in the complete graph.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, what you're saying is I want to take a complete graph and we're saying a complete graph is a first cousin of an expander, so I want to take a complete graph.",
                    "label": 0
                },
                {
                    "sent": "And I want to quote embed my my data graph in the complete graph and I want to know how well it fits and you can use duality ideas and we'll get back to lower bounds related to duality.",
                    "label": 0
                },
                {
                    "sent": "So we're really thinking of filtering the graph through either a low dimensional place or a complete graph, and so certain parts of social networks will have low dimensional properties.",
                    "label": 0
                },
                {
                    "sent": "Will see that at certain size scales and certain and these social information, Alex will have expanded like properties, other size, skills and so understanding the connections between those two will be important.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a lot of local improvement methods.",
                    "label": 0
                },
                {
                    "sent": "And let me not get into the details of those, except to say that.",
                    "label": 0
                },
                {
                    "sent": "Except to say that.",
                    "label": 0
                },
                {
                    "sent": "A lot of 'em, sort of the things you intuitively think of.",
                    "label": 0
                },
                {
                    "sent": "You know you have nodes in two sides, and the question is, can you move a node here?",
                    "label": 0
                },
                {
                    "sent": "Can swap nodes around and sort of greedy way.",
                    "label": 0
                },
                {
                    "sent": "So think of it as like a discrete version of coordinate descent or something.",
                    "label": 0
                },
                {
                    "sent": "And these were methods that were developed early.",
                    "label": 0
                },
                {
                    "sent": "And so they can get stuck in local minima very easily.",
                    "label": 0
                },
                {
                    "sent": "But you can use them to refine cuts.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "And one way that you can use local improvement methods is if you look at the graph at various size scales and the question is.",
                    "label": 0
                },
                {
                    "sent": "What's relationship between those size skills?",
                    "label": 0
                },
                {
                    "sent": "So for certain graphs?",
                    "label": 0
                },
                {
                    "sent": "These low dimensional ones, maybe this one type relationship, and I've claimed that for the social information was very different and so we'll get back to that.",
                    "label": 0
                },
                {
                    "sent": "But just to give you sort of visual example, so you want to partition this graph into two pieces.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to cut off the land in the Sky or the land in the water.",
                    "label": 0
                },
                {
                    "sent": "And so you'll do something called multiresolution partitioning.",
                    "label": 0
                },
                {
                    "sent": "So you look at the graph at various size skills.",
                    "label": 0
                },
                {
                    "sent": "So you take the graph and the graph is constructed from this by, you know some RGB.",
                    "label": 0
                },
                {
                    "sent": "So similarity or something and you say, well, you know, rather than looking at the size scale, that's whatever you an inch on this screen.",
                    "label": 0
                },
                {
                    "sent": "I want to look at scale to inches.",
                    "label": 0
                },
                {
                    "sent": "4 inches 8 inches and you marked down here and then you get the courses possible.",
                    "label": 0
                },
                {
                    "sent": "Cut you, cut it, you partition, and then you refine back, and at each refinement step you do some local improvement or something like that.",
                    "label": 0
                },
                {
                    "sent": "So Chaco was maybe that's one of the first to do this.",
                    "label": 0
                },
                {
                    "sent": "Medecins is very well known way to cut graphs.",
                    "label": 0
                },
                {
                    "sent": "Uses variants of this rock lessons, a bunch of other things that do these sorts of techniques.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if you actually pull down, quote will get back to using meds later for tearing apart the real.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, but they do something so like this.",
                    "label": 0
                },
                {
                    "sent": "So maximum flow.",
                    "label": 0
                },
                {
                    "sent": "So now let's switch from food from the local improvements and the spectrum to flow flow.",
                    "label": 0
                },
                {
                    "sent": "Maybe something that people with a background in computer science or familiar more familiar with you have a directed graph G. You have a source and a sink.",
                    "label": 1
                },
                {
                    "sent": "You have a capacity on each edge flow as a function such that all the flow in and flow of the same except at the source and sink.",
                    "label": 0
                },
                {
                    "sent": "And the problem is to find the flow from the source to the sink, that's best.",
                    "label": 1
                },
                {
                    "sent": "And the trick is that you also have capacity constraints.",
                    "label": 0
                },
                {
                    "sent": "You can't just move it along all along the best way.",
                    "label": 0
                },
                {
                    "sent": "So you need to find the things best subject to the capacity constraints.",
                    "label": 0
                },
                {
                    "sent": "So this is a very very basic primitive.",
                    "label": 0
                },
                {
                    "sent": "I mean, a lot of things will boil down to this.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of variance of this, and in particular you can have multiple sources and sinks.",
                    "label": 1
                },
                {
                    "sent": "So that's going to be.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great that we're going to be interested, so single commodity flow has relationships to linear programming, Ford Focus, and is one of the original algorithms.",
                    "label": 1
                },
                {
                    "sent": "Bunch of other methods.",
                    "label": 0
                },
                {
                    "sent": "Most of the implementations now, I guess.",
                    "label": 0
                },
                {
                    "sent": "Is a bunch of nice work by Goldberg and others that well, that and others are variants of what's called push relabel methods.",
                    "label": 0
                },
                {
                    "sent": "You pushing flow around.",
                    "label": 0
                },
                {
                    "sent": "And single commodity flow.",
                    "label": 0
                },
                {
                    "sent": "There's something called the Max flow min cut theorem and Max flow.",
                    "label": 0
                },
                {
                    "sent": "Min Cut says that if you want to maximize the flow subject to these capacity constraints.",
                    "label": 0
                },
                {
                    "sent": "That's equal to the minimum cut in the graph and cut is expansion of conductance, so cuts the objective lunch today.",
                    "label": 1
                },
                {
                    "sent": "So now I can tell you if I can solve this flow problem, then the maximum flow in the graph is equal to the minimum cut.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "One variant is that.",
                    "label": 0
                },
                {
                    "sent": "And so that's a great result 'cause you can prove that amongst all of the combinatorially large number of cuts that you know that you have a lower bound.",
                    "label": 0
                },
                {
                    "sent": "Do you want to search the mall?",
                    "label": 0
                },
                {
                    "sent": "Yeah you can.",
                    "label": 0
                },
                {
                    "sent": "You can get a fast certificate of a lower bound.",
                    "label": 0
                },
                {
                    "sent": "If you have multicommodity flows meaning I have two or three or 10 or I can have all the way up to N. Choose two pairs.",
                    "label": 1
                },
                {
                    "sent": "There's a couple different versions, but Max Flow is not equal to min cut.",
                    "label": 0
                },
                {
                    "sent": "But the big breakthrough with Lightening Row back in the late 80s was that Max Flow is approximately equal to men caught up to a log K factor or K as a number of commodities.",
                    "label": 0
                },
                {
                    "sent": "And the way this interfaces to graph partitioning is that we can look at all pairs multi commodity flow.",
                    "label": 0
                },
                {
                    "sent": "So I'll then choose two pairs and so then you get the logarithm of N squared, which is going to log in.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So single commodity flow, so one algorithm that you could have to solve for the best cut is the following.",
                    "label": 0
                },
                {
                    "sent": "Look at all two to the end cuts.",
                    "label": 0
                },
                {
                    "sent": "So I look at all to the end partitions in your graph.",
                    "label": 0
                },
                {
                    "sent": "So the single commodity flow problem returned the best.",
                    "label": 1
                },
                {
                    "sent": "So that's an algorithm that will work, right?",
                    "label": 0
                },
                {
                    "sent": "It's not so fast, but that'll work.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can you do something faster?",
                    "label": 0
                },
                {
                    "sent": "There is not fast enough to check all the partitions, so we can do something faster so we can look at the multi commodity flow version.",
                    "label": 1
                },
                {
                    "sent": "We want our outflow between all pairs and choose two of them.",
                    "label": 0
                },
                {
                    "sent": "And we want to do it all at once and then we want to find the edges that are and then and then cut the edges that are most congested because we think that the flow should reveal the cuts.",
                    "label": 1
                },
                {
                    "sent": "So I said you get a log K factor.",
                    "label": 0
                },
                {
                    "sent": "Log of N choose two is going to log in, so you're going to log in approximation.",
                    "label": 0
                },
                {
                    "sent": "This is a very different type of approximation guarantee that we saw previously.",
                    "label": 0
                },
                {
                    "sent": "Spectrolite spectral didn't say anything about and the number of nodes, but it didn't give you this quadratic factor.",
                    "label": 0
                },
                {
                    "sent": "And the quadratic factors structural parameter of the graph that something that's very popular in a lot of areas of machine learning and scientific computing and so on.",
                    "label": 0
                },
                {
                    "sent": "If you parameterized problems the way you know, we wouldn't throughout the computer science, and you want the algorithms that are fast.",
                    "label": 0
                },
                {
                    "sent": "Depending on end, the number of nodes, but independent of any structural parameters of the graph.",
                    "label": 0
                },
                {
                    "sent": "This is something that's going to make you much, much happier, 'cause it doesn't depend on structural paragraphs.",
                    "label": 0
                },
                {
                    "sent": "It just depends on and the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "So you can detect solution if the idea is that can you detect solution if bottleneck forces those edges to be more congested than average?",
                    "label": 1
                },
                {
                    "sent": "Now it's a question that you might ask before we saw that there are graphs that are quadratically bad.",
                    "label": 0
                },
                {
                    "sent": "Are there graphs that are log in bed?",
                    "label": 0
                },
                {
                    "sent": "Or is this a weakness in the analysis?",
                    "label": 0
                },
                {
                    "sent": "So if you're going to be doing flow related algorithms, this is not a weakness in the analysis.",
                    "label": 0
                },
                {
                    "sent": "There are graphs that are that bad.",
                    "label": 0
                },
                {
                    "sent": "And those graphs are our old friends expanders.",
                    "label": 0
                },
                {
                    "sent": "So expander graphs as I said, are graphs that don't have any good cuts.",
                    "label": 0
                },
                {
                    "sent": "You every subset of nodes has a constant fraction of edges linking out.",
                    "label": 0
                },
                {
                    "sent": "And that's not something you see if you have a low dimensional graph because bigger circles are better than smaller circles we said, and so there's a fixed.",
                    "label": 0
                },
                {
                    "sent": "You know there's a lot of more stuff going on than at the surface, so expander graphs are graphs that you know have constant fraction of their edges linking out.",
                    "label": 0
                },
                {
                    "sent": "And the simplest version is, I mean, think, think of this that it's pretty sparse.",
                    "label": 0
                },
                {
                    "sent": "And so on average are most of your links, sort of nearby.",
                    "label": 0
                },
                {
                    "sent": "You are far away.",
                    "label": 0
                },
                {
                    "sent": "And so, in terms of the diameters in terms of the average diameter and so it turns out most of your links are pretty far away, so you're going to have and choose two pairs of nodes that you want to route commodity between, but most of the paths are going to be very long log in, and she's going to get an extra log in factor.",
                    "label": 0
                },
                {
                    "sent": "So expander graphs really are that bad, and in particular it's not the case on expander graphs that you can detect the solution if the bottleneck for these graphs are more congested than average.",
                    "label": 0
                },
                {
                    "sent": "So you get a global capacity constraint.",
                    "label": 0
                },
                {
                    "sent": "Just because the diameters of the edges so you so the flow fails to reveal the best cuts in that sense.",
                    "label": 0
                },
                {
                    "sent": "And again, this capacity constraint is very different than you see with spectral with spectral.",
                    "label": 0
                },
                {
                    "sent": "If you have a complete grafana stringy thing, and you do one iteration of the power method and the second iteration of the power method, you're going to start to see that you're failing to push probability mass down the stringy things.",
                    "label": 0
                },
                {
                    "sent": "So it's a very low.",
                    "label": 0
                },
                {
                    "sent": "The notion of smoothing or regularization that you get with spectrum methods.",
                    "label": 0
                },
                {
                    "sent": "Very local thing, but in certain senses that will pour it up.",
                    "label": 0
                },
                {
                    "sent": "You'll see that locally, but also globally, but going from local to global is very graceful, mean here.",
                    "label": 0
                },
                {
                    "sent": "Doing things locally is just fine.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this in terms of resistor circuits, in which case you can sort of short circuit the fact that you have a.",
                    "label": 0
                },
                {
                    "sent": "A lollipop yeah, complete graph of the string thing and the flow will just go down that stringy piece very easily.",
                    "label": 0
                },
                {
                    "sent": "But the capacity constraints comes not locally, but very globally on the expander graphs, and again, social networks will see maybe not surprising, especially the squint at them and look at them at large size.",
                    "label": 0
                },
                {
                    "sent": "Skills are expanders.",
                    "label": 0
                },
                {
                    "sent": "They have a lot of expanded like properties and.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And so understanding the implicit properties here will be useful in sort of a complementary way to what it might be in spectral.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These things you can view as integer programs or linear programs.",
                    "label": 0
                },
                {
                    "sent": "And in the same way as I said, we could take spectral and write it some way and relax it.",
                    "label": 0
                },
                {
                    "sent": "And and you gotta relax solution.",
                    "label": 0
                },
                {
                    "sent": "You can write down.",
                    "label": 0
                },
                {
                    "sent": "As an integer program, the multi commodity flow problem.",
                    "label": 1
                },
                {
                    "sent": "You could also write it down as a linear program.",
                    "label": 0
                },
                {
                    "sent": "And the linear program will be relaxation of the integer.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the embedding of you of a flow.",
                    "label": 0
                },
                {
                    "sent": "Again, we're sending these things from nice spaces.",
                    "label": 0
                },
                {
                    "sent": "Is based on something called bourgain steering.",
                    "label": 0
                },
                {
                    "sent": "This is every endpoint metric space will embed in L1 with distortion log in, so the flow algorithm not the way it actually implemented in code.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is you could solve the LP on the previous page to get a distance function.",
                    "label": 0
                },
                {
                    "sent": "Obtain the L1 embedding and this is going to be constructive.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into that, but deconstructive and then you do a rounding 'cause now I have a continuous thing you need around it back to a discrete thing, so here it's going to boil down to an embedding in a different embedding.",
                    "label": 0
                },
                {
                    "sent": "Then we saw a spectral.",
                    "label": 0
                },
                {
                    "sent": "Here you're embedding an L1 spectral so much more L2 type thing, and you're going to be using the ball game results.",
                    "label": 0
                },
                {
                    "sent": "But it boils down to an embedding, and it turns out embedding this place expanders are the worst.",
                    "label": 1
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a number of implementation issues.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Maybe we should take 5 minutes off and just a minute or two and then come back.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So let me just have a couple more slides and then we'll take a bit of a break.",
                    "label": 0
                },
                {
                    "sent": "Eigenvector code with Matlab relate pack depending on the details, sort of level of detail you want to get into the size of the graphs and so on.",
                    "label": 1
                },
                {
                    "sent": "Often times take something like the number of nonzeros, because you're going to be doing something, it is very sparse.",
                    "label": 0
                },
                {
                    "sent": "Menace is very nice, it's publicly available and you can use it very very fast in practice, but it doesn't sit on a stronger theory, but in fact some of the things will be talking about in a little bit will be using that.",
                    "label": 1
                },
                {
                    "sent": "Single commodity flow takes something like N cubed time multi commodity, something like N squared.",
                    "label": 0
                },
                {
                    "sent": "So then cubed end of the three halftime multipoint is going to be in squared.",
                    "label": 0
                },
                {
                    "sent": "Those things going to expensive.",
                    "label": 0
                },
                {
                    "sent": "Very large graphs but you can combine.",
                    "label": 0
                },
                {
                    "sent": "Local versions of spectral and flow improve, and so on.",
                    "label": 0
                },
                {
                    "sent": "To get something better, so maybe let me spend.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "5 minutes describing that will take a bit of a break, so the basic ideas hopefully have laid out, so I'm going to gloss over the spectral improve and in the flow, improving these sorts of things like that and just sort of mentioning quickly, but hopefully you have sort of the basic idea of how they work, and if you're interested I can describe it a little bit more detail how they work and give you pointers the local versions, but.",
                    "label": 0
                },
                {
                    "sent": "You know, back to maybe why this is a good thing to think about.",
                    "label": 1
                },
                {
                    "sent": "I mean so interesting, because what theory would say is that flow based methods are better than spectral and the reasons flow based method is better.",
                    "label": 1
                },
                {
                    "sent": "Spectral will always give you login guarantee.",
                    "label": 0
                },
                {
                    "sent": "Spectral might be good on expanders or loaded low dimensional things but basically unexpanded 'cause of quadratic of a constant is constant.",
                    "label": 1
                },
                {
                    "sent": "Practice will say that.",
                    "label": 0
                },
                {
                    "sent": "Something very different to practice, meaning machine learning.",
                    "label": 0
                },
                {
                    "sent": "Scientific computing people are actually playing things, will, say, spectral methods are very fast.",
                    "label": 0
                },
                {
                    "sent": "The robust, the robust, for reasons that we spent a fair bit of amount of time on.",
                    "label": 0
                },
                {
                    "sent": "Feel the noise because of this local way that regularization is sort of implicitly implemented, and so there the method of choice.",
                    "label": 0
                },
                {
                    "sent": "These people may not even know what flow methods are.",
                    "label": 0
                },
                {
                    "sent": "If they do, they're not going to care, so this is, I think, sort of a fairly egregious maybe theory practice.",
                    "label": 0
                },
                {
                    "sent": "Disconnect, right?",
                    "label": 0
                },
                {
                    "sent": "You'd like the theory too.",
                    "label": 0
                },
                {
                    "sent": "You know, if not if not.",
                    "label": 0
                },
                {
                    "sent": "Solve practical problems immediately, provided these qualitative insight and so here we have a very, very simple question.",
                    "label": 0
                },
                {
                    "sent": "Standard 10,000 feet you look down at the graph and you say, what does it look like?",
                    "label": 0
                },
                {
                    "sent": "Is it a hot dog?",
                    "label": 0
                },
                {
                    "sent": "Is it an expander?",
                    "label": 0
                },
                {
                    "sent": "Is it something else and one group would say you know this is a better tool to use and this is how they look and the other group would say this is a better tool to use is how they look and their quality.",
                    "label": 0
                },
                {
                    "sent": "I mean not even qualitatively in agreement.",
                    "label": 1
                },
                {
                    "sent": "So now if your practitioner, you might say well, then I should just use spectral mean.",
                    "label": 0
                },
                {
                    "sent": "Who cares what theory says?",
                    "label": 0
                },
                {
                    "sent": "Because that's supposed to guide the practice.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, maybe a little less in KDD, but if you're a machine learning, you're very tired of spectral methods and you're going to think about spectrum.",
                    "label": 0
                },
                {
                    "sent": "Method is coming from continuous places and you're going to think that if you go to infinite dimensions, that's going to give you a huge amount of freedom and solve all your problems.",
                    "label": 0
                },
                {
                    "sent": "But these infinite dimensional vector spaces are still very structured versions of that, and not not intuitively, qualitatively like expanders.",
                    "label": 0
                },
                {
                    "sent": "So having the idea of expanders and how expanders might behave and how.",
                    "label": 0
                },
                {
                    "sent": "Two very different classes of approximation algorithms behave on these sorts of graphs.",
                    "label": 0
                },
                {
                    "sent": "The question is, can we can we take these two very different tools?",
                    "label": 0
                },
                {
                    "sent": "Very different types of graphs?",
                    "label": 0
                },
                {
                    "sent": "Not that you know the social graphs that you would be encountering are low dimensional, or expand or anything like that exactly, can we use these to sort of playoff of each other and so in a sense, rather than being getting bad thing, this is actually a good thing.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know there's nothing better in the world than having sort of a big disconnect between what maybe theory and practice or whatever it says.",
                    "label": 0
                },
                {
                    "sent": "And and having data to answer, there's nothing worse than having a disconnect like that and not having.",
                    "label": 0
                },
                {
                    "sent": "Data to answer the question because then people to scream at each other.",
                    "label": 0
                },
                {
                    "sent": "But I mean having a disconnect like that but having data to actually address some of these questions.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's a great thing, so that should be so pretty exciting.",
                    "label": 0
                },
                {
                    "sent": "So this cut improvement algorithms, given a graph and an input cut, find a nearby cutter, certify that none exists, local algorithms and locally bias objectives.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes these will either be a local version of spectral or flow, and you want them to run in size depending on.",
                    "label": 0
                },
                {
                    "sent": "Running time depending on the size of the output, can you combine spectral and flow in different ways, so you can certainly apply these ideas to other objective functions were not going to go into that.",
                    "label": 0
                },
                {
                    "sent": "Cut improvement, you take a cut and you sort of want to improve it.",
                    "label": 0
                },
                {
                    "sent": "You can do that with either spectral or flow is the short answer.",
                    "label": 0
                },
                {
                    "sent": "Although actually doing it, the details will be quite a bit different because the local and global, local and global.",
                    "label": 0
                },
                {
                    "sent": "Properties will be pretty different between these two.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With respect to combining spectral and flow.",
                    "label": 1
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's a great result in terms of worst case analysis by or around Vazirani that in some sense says if you embed in yet a different place that is a little spectral, like in a little flow like.",
                    "label": 0
                },
                {
                    "sent": "That the good things happen.",
                    "label": 0
                },
                {
                    "sent": "You get it.",
                    "label": 0
                },
                {
                    "sent": "You're better, worst case, approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "In practice with this might mean is the following as sort of a data analysis tool.",
                    "label": 0
                },
                {
                    "sent": "I do spectral so I glossed over.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know the local versions of spectral flow, but the idea is that you have an input cut and you want to use spectral or flow to say refine it or to ask can I get a little bit better cut in some sense, better objective function value being nicer or whatever, so say that I do the following.",
                    "label": 0
                },
                {
                    "sent": "I take that.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph I do spectral.",
                    "label": 0
                },
                {
                    "sent": "This isn't the way it's usually described, but it's it's a dual view they usually describe.",
                    "label": 0
                },
                {
                    "sent": "I do spectral, I get an eigenvector and I want to ask the question is this eigenvector?",
                    "label": 0
                },
                {
                    "sent": "You know, quote the right direction.",
                    "label": 0
                },
                {
                    "sent": "The direction that's along the worst case cut?",
                    "label": 0
                },
                {
                    "sent": "Or did I compute something that's an artifact of my approximation algorithm is what I'm seeing.",
                    "label": 0
                },
                {
                    "sent": "Is this direction dominated by the algorithm rather than the data?",
                    "label": 0
                },
                {
                    "sent": "And so one way to answer that question is to do a local flow computation.",
                    "label": 0
                },
                {
                    "sent": "So I didn't go into the details of the locality, but you can modify the objective functions to be locally biased.",
                    "label": 0
                },
                {
                    "sent": "So if you can believe that you can do that in the in, the overheads, have the pointers that will the papers.",
                    "label": 0
                },
                {
                    "sent": "If you believe that you can do that, then what you're saying is effectively can I use other approximation algorithm flow that fails and succeeds in very different places?",
                    "label": 0
                },
                {
                    "sent": "To answer the question of whether this is the right direction?",
                    "label": 0
                },
                {
                    "sent": "And as you can imagine, things are more complicated than yes or no.",
                    "label": 0
                },
                {
                    "sent": "So the answer to this is a matching.",
                    "label": 0
                },
                {
                    "sent": "So I start with the graphic computer vector and now I compute a weighted matching and I can add that to the graph and that's going to compute, give me a new graph G prime.",
                    "label": 0
                },
                {
                    "sent": "I can do spectral on that.",
                    "label": 0
                },
                {
                    "sent": "Again, I can ask the question is what I computed in artifact and you do that log in times and into some bells and whistles in the theory goes through?",
                    "label": 0
                },
                {
                    "sent": "But you can also imagine that you want to do this in practice.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the original algorithm were around when he was, I think, end of the fourth rounder, the 5th.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of work on these sort of.",
                    "label": 0
                },
                {
                    "sent": "This is very much an online learning sort of algorithm idea.",
                    "label": 0
                },
                {
                    "sent": "Couple those two together.",
                    "label": 0
                },
                {
                    "sent": "The implement.",
                    "label": 0
                },
                {
                    "sent": "We have implementations that work well on on on let's say for this particular thing you know 10s of thousands of nodes are smaller than the stuff will get too after the break, but reasonable size things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that maybe why don't we take a, you know, just a 5 minute break or something and then just to stretch legs and then I checked earlier and they said there's not a coffee break at the break I guess.",
                    "label": 0
                },
                {
                    "sent": "So it's not like this 20 minutes off or something, so I'll start back in about 5 minutes on this Clock.",
                    "label": 0
                },
                {
                    "sent": "And and.",
                    "label": 0
                },
                {
                    "sent": "Will continue with the second half or second from the left.",
                    "label": 0
                }
            ]
        }
    }
}