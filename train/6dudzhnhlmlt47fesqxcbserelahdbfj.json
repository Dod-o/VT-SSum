{
    "id": "6dudzhnhlmlt47fesqxcbserelahdbfj",
    "title": "Basics of Computational Reinforcement Learning",
    "info": {
        "author": [
            "Michael Littman, Department of Computer Science, Rutgers, The State University of New Jersey"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Economics",
            "Top->Social Sciences->Psychology"
        ]
    },
    "url": "http://videolectures.net/rldm2015_littman_computational_reinforcement/",
    "segmentation": [
        [
            "Susan said this workshop is not a workshop tutorial.",
            "That was what I was told that I was giving, and in particular the organizer said it should be a tutorial for people who want to get up to speed on computational reinforcement learning, which is a phrase that I hadn't actually heard.",
            "But I think I think I know what they mean.",
            "So."
        ],
        [
            "So essentially, here's here's I'm going to split you all into four groups.",
            "There's people who haven't studied natural boom.",
            "So just hold this people who haven't studied natural reinforcement learning decision making people who have people who have studied artificial reinforcement learning, decision making, and people who haven't.",
            "So if you've haven't studied natural reinforcement learning decision making, but you have studied artificial stuff like you've ever heard me give a talk.",
            "For example, you should probably leave and go to Nathaniel.",
            "Feel free to come back for Davids talk, which I think is going to be really awesome.",
            "He actually sent me the slides in advance and it looked it looked really exciting.",
            "So you can come back, but you should just you should go.",
            "OK, maybe you're not in that category.",
            "Maybe in one of the other category, so you could be somebody who actually already knows the natural or LDM stuff, and so don't you don't need to hear Nathaniel's talk, so you should go socialize.",
            "There's all this nice place out there that the weather is beautiful, you know, come back for the focus talks which are going to be really cool, but again.",
            "You need to go.",
            "Alright, alright so or you could be in the category of you actually haven't studied either natural or artificial things and I'm not sure why you came.",
            "It's possible.",
            "I mean, I'm not, and I certainly don't mean to be discouraging to anybody who's in that category, but that would seem really like a gutsy thing to do, like I'm just going to go to the conference.",
            "I don't know what it stands for.",
            "Alright, and it could be that you have studied natural or LDM, but haven't studied are artificial LDM, and that's who I think I'm talking to, so you're welcome to stay.",
            "I don't think it's working.",
            "I can see some people who I know are in this box.",
            "So OK, so here.",
            "So one one last try to get rid of you and that is.",
            "There's going to be some people here who really are in this box, and if I know that, sorry that that really are in this box, right?",
            "But if I know there's people in this box, I'm going to kind of talk to you and that's going to be really hard for these people, and they're going to make them sad, and they're not going to learn the things that they need to learn.",
            "So you're really, you're hurting them.",
            "I can't get rid of you guys, alright.",
            "I mean truly Nathaniel Nathaniel said he would wait in the beginning.",
            "So that will be chance for people to come a little bit late.",
            "OK one I got one.",
            "Thanks, thanks for leaving the way.",
            "Which is now sad.",
            "'cause now it's sort of like, oh we got him but he was doing the right thing.",
            "We should be appreciative alright?"
        ],
        [
            "So that being said, I know this is going to be repeat for some of you guys, but this is what I'm going to try to get at I couple days ago I looked at the schedule and saw that this was a 3 hour talk.",
            "That's a long talk, so so I tried to put together, you know, material that would fill that time and maybe give you some interesting things to hear about and kind of get up to speed with some of the vocabulary and ideas that underlie.",
            "We're going to computational reinforcement learning that this is a reinforcement learning as known by computer scientists and possibly engineers.",
            "Possibly operations, research people, people who think of it as, as you know.",
            "Automatable math and not necessarily a model of something that's alive out in the real world.",
            "Alright, well done Sir.",
            "Alright alright.",
            "So now I have now there's no harm.",
            "Alright, moving on from that so great.",
            "Yeah, that's that's the other thing I can do is.",
            "I could just be really boring in the beginning and you're like, oh, it's that thing that I've heard 10,000 times and taught twice.",
            "You know, yeah, that's what it's going to be."
        ],
        [
            "Seriously, alright, so so here's gonna break it down.",
            "I'm going to 1st kind of talk about what I think computational RL is and kind of the mindset that underlies it.",
            "Then I'll get into some of the nitty gritty of planning and learning in Markov models.",
            "I'll shift over to a topic that probably shouldn't be in here, but I like it a lot, so I'm going to talk about it.",
            "Efficient reinforcement learning, and then some kind of a grab bag of other other topics that I think may be useful or particularly interesting, or some things that I think we need a lot of input on, and I want to make a case for why we should be studying them more.",
            "Alright, So what is compute?"
        ],
        [
            "National RL so just to set this up.",
            "Those of us who are in computational RL think of reinforcement learning is being a sub field of machine learning.",
            "Alright, so what's so?",
            "What's machine learning?",
            "Well machine learning I say to a first approximation, is this that we're trying to build programs, learners that take training sets and turn them into decision rules, and in particular.",
            "The way that we do this is, we think, about in this in the most commonly used setting in machine learning is supervised learning setting, and in that case the training data has this form of inputs get mapped to output.",
            "So for example, just to be concrete.",
            "So let's imagine that we are a web service company or something like that, and we're trying to.",
            "We're trying to make a program that's going to decide which version of a webpage to show to people who come and visit our site.",
            "So we need a way of mapping people to pages or people to styles of pages.",
            "People we can't really map actual people because we don't have them, but we have data about those people.",
            "So maybe we have some kind of feature ized representation of a web user.",
            "We have, for example, has this person visited the site before?",
            "How many times have they make any purchases on our site?",
            "Do we have any demographic information you know, male, female, US, not US, Canada, not Canada.",
            "Have they actually registered on our site?",
            "Is this is a person who actually has been here before and signed up?",
            "These are all sorts of facts that we could know and as a function of that, what we'd like to do is to choose which webpage decided that user.",
            "So we might want to show that user an advertisement of a certain kind.",
            "Or maybe the FAQ page, or maybe a login page or something like that.",
            "Or here's where you left off last time pages.",
            "Lots of choices and the right choice depends on which kind of user we're looking at.",
            "So in supervised learning, what we do is we collect in advance of big.",
            "Training set that says this kind of user should have been given this or user.",
            "Here's an example user or.",
            "Here's an actual user.",
            "This is what page we should show that user.",
            "Here's another one written as a feature vector.",
            "This is what we should show that one and so forth.",
            "We could have thousands and thousands millions upon millions of these potentially.",
            "We're going to feed it to our learner and the learner is going to spit out a decision rule and the goal of this decision rule is.",
            "To try to act like the training set.",
            "OK, so to try to do think map X is today's, so it's kind of like the way that the training set did it.",
            "However, that happened to be.",
            "You know ideally it should make the same kind of decisions as we saw in the training set, but but it may not.",
            "It may be that even has contradictory information in the training set, so it has to make some kind of choice, but ultimately it's trying to.",
            "Minimize error right?",
            "So minimize the mismatches between the inputs and the outputs.",
            "OK, so that's that's supervised.",
            "Learning super useful, super applied.",
            "It's all over the place.",
            "People doing all kinds of amazing things with it.",
            "But this is just one kind of machine learning.",
            "Here's another kind of machine learning.",
            "So one challenge of this is that you need this training set."
        ],
        [
            "Another kind of learning is unsupervised learning, so the idea and unsupervised learning is there.",
            "Examples don't don't tell us anything anymore, they're just examples like there's Joe and there's Fred and there Sarah, there's all these different people, and that's all we know and what it needs to do is come up with a decision rule that Maps people too well.",
            "What I mean, it doesn't really have an idea of what it should map them too, so it's just kind of cluster them together.",
            "It should just group together users that are similar to each other into similar classes.",
            "So one way this could work is again, we have features representations of the users.",
            "And we're going to output for each user.",
            "Is their class ABC&D with the goal that.",
            "Individuals users X is that are similar to each other, are going to get signed into the same cluster and things that are different people that are different are going to be in different clusters.",
            "If it was just the first would be really easy, right?",
            "So how do you make it so that any two people that are similar or in the same class assign them all a?",
            "Done, so that's really easy with what makes it harder is we also want to separate the ones that are not so similar.",
            "So we want to use these other classes as well.",
            "Alright.",
            "Challenges of unsupervised learning is that it'll do something, but it's not necessarily relevant to whatever it is you're trying to do, right?",
            "So my group people together by I don't know things that are not useful for deciding what web web page to show them in our shoe size.",
            "Maybe like something that just orthogonal or irrelevant to the task because it doesn't have any information about the task and its training set.",
            "Alright, so now.",
            "The draw."
        ],
        [
            "Reinforcement learning, so this is when this is when I get really excited, but but maybe we could all be kind of excited.",
            "So so in reinforcement learning, we have a different kind of experience.",
            "We have experienced that says, well, we saw this thing and we did this thing.",
            "And this is how good it was.",
            "We gotta score for that.",
            "Then we saw this other thing and did this other thing and we got a score for that too.",
            "So we had this whole big chain of experience.",
            "And Sue the thing that's new here is this how good it was.",
            "So we have some kind of evaluative feedback, some kind of numerical value.",
            "So it could be like how many things did they click on as a result of being shown that page.",
            "It could be how much time did they spend on that page?",
            "It could be.",
            "Did they ultimately make make a purchase later in that visit?",
            "But we get some kind of a measurable outcome.",
            "Something that we can we can think of as designers as being a positive outcome or a negative outcome, and our goal is to just construct a decision rule that has maximum expected value so it tends to produce the things that we said were good and it tends to not produce the things that we said were bad.",
            "Right, so this is a really powerful idea.",
            "It's so much cooler than the other two, right?",
            "So you don't have to tell it what to do.",
            "You just have to be able to score it.",
            "You can give it information unlike the unsupervised case where it's just in the wilderness, so this is really powerful, neat, but of course you know all is not.",
            "Sunshine and roses.",
            "This is actually very weak feedback.",
            "These kinds of scores are very weak.",
            "That doesn't really tell the learner what it should be doing in particular.",
            "In the vanilla form, we don't even know.",
            "Whether that's a good score or a bad score, right?",
            "Like I did this thing and I got a 7.",
            "OK, like I don't know what I should do with that information, right?",
            "So you can only start to make sense of it in context where you start to realize.",
            "Oh yeah, but you know this.",
            "Other time I got something really similar and did this other thing and I got much higher score so that 7 maybe is not so good 'cause it was seven out of 100.",
            "Alright, so that's that's kind of the essence of what we think of this as a computational problem."
        ],
        [
            "And just to kind of tie these threads together, this is this is a figure from I got to make a paper for nature.",
            "I'm very excited about that so I drew it figure I sent it to nature and then they came back with this which looks a lot like mine except the font is smaller.",
            "And this they did a much better job of lining up this circle part right here so, But anyway, so this is like a professionally rendered picture, so you should.",
            "We should all appreciate that.",
            "So this is kind of relating all these different kinds of feedback to each other that we get.",
            "Well, OK, so I didn't say this, but one of the really important axes for understanding the difference between these different kinds of machine learning are the kinds of feedback that are available.",
            "And I want to talk about three different dimensions of feedback.",
            "There's supervised or not supervised so or supervisors to the valuative so supervised.",
            "The learner is told what answer to give.",
            "You know when you see this, do that.",
            "And it's supposed to basically mimic it.",
            "So that's that's a good kind of feedback in that it actually is.",
            "There's a lot of supervision there for the learner.",
            "But it's you know, restrictive and you have to have that extra information.",
            "Their value to feedback is sort of weaker.",
            "It's less information for the learner, but may be easier to produce, maybe easier to produce something that kind of automatically scores.",
            "You know, I'll, I'll know it when I see it.",
            "I don't know what you should do, but I'll know it when I see it.",
            "So that's one distinction, sampled versus evaluative, and all the things that are.",
            "Sampled, oh, that's that's in this.",
            "Sorry no, that's exhausted.",
            "What did I say?",
            "Evaluative versus supervised is too many S is that's the problem.",
            "I don't know who came up with this figure, so.",
            "Alright, so evaluative versus supervised so supervised here is just you know supervised machine learning.",
            "Evaluative is all these kinds of related reinforcement learning kinds of things.",
            "Within evaluative feedback, we can make some other distinctions so one is sequential versus one shot so.",
            "If we were given sequential feedback, it means the the reward value that we got isn't necessarily relevant to the action that was just taken, or the decision that was just made.",
            "It may actually be you got high reward because you did something really clever.",
            "Five steps ago and you have to.",
            "Somehow the learner has to somehow connect those things up versus A1 shot thing which just says.",
            "Here's the input you give me the output.",
            "I'll tell you how good that one thing was, where each of them are sort of independent of the others.",
            "So the supervised learning is in this one shot zone.",
            "Yeah alright.",
            "But we do have something in the.",
            "One shot but evaluative zone.",
            "So the I will talk a little bit about bandits, bandits is.",
            "It is what we is.",
            "The name that we use for particular model of decision making, where you each shot you get to make a decision you put.",
            "We think of it is pulling an arm.",
            "We get to make some decision.",
            "We get some feedback for that.",
            "And then we repeat that over and over and over again.",
            "And so each decision that we make has an immediate payoff.",
            "So it's a one shot thing.",
            "But it's a value to feedback.",
            "We're not actually told what we should have done, which arm we should have pulled, which action we should have taken.",
            "Alright.",
            "So alright, the last thing I want to talk about is sampled versus exhaustive, so the idea of exhaustive is I'll show you all the X is that out.",
            "That could possibly matter.",
            "You don't generalize it all, just show you all of them exhaustively versus sampled, which is I'm just going to show you some XSS and you're going to fill in the blanks yourself.",
            "So supervised machine learning is usually in this well, it's sampled, but it's supervised zone, so in particular.",
            "If it were exhaustive and supervised, that would be really boring, right?",
            "So exhausted in supervised means.",
            "When I say when I say jump, you say how high when I say it's Tuesday, you say I'm thirsty, right?",
            "So I give you a list of what your responses should be for every possible input, and then later I give you one of those inputs and you should give the associated output.",
            "It sounds stupid, right?",
            "So there isn't a box for that, so supervised machine learning is more interesting than that, because it actually deals with this idea of kind of reading between the lines, generalizing between different examples.",
            "But bandit problems actually live in this exhaustive space, so you actually get to try all the different choices.",
            "What's difficult about it is you don't get the feedback.",
            "The valuations are very noisy, so you actually have to generalize.",
            "Not really generalize, but you have to use statistical methods to try to figure out well how good is this, really.",
            "Some things were good.",
            "Some things were not good.",
            "We have to kind of maybe average them together.",
            "Alright, so with that said, we got these two bubbles.",
            "Finally, this bubble tabular reinforcement learning is the idea that we're going to worry about the sequential aspect of things, so we're going to be acting in a world, and the actions that we take may not prove themselves relevant until a bit later.",
            "So we have to worry about that sequential aspect, but it's exhaustive.",
            "We actually do get to see every possible X every possible state in this case.",
            "And but it is evaluative, so we get to find out, was it good or bad and so things that fall into this or like playing Tic tac toe right where you can see every possible Tic Tac toe board.",
            "There's only a couple of 1000 of them and you can find out.",
            "Well, I've had this tic tac toe board and I took this action and then ultimately I won the game or I lost the game and so I had that evaluative feedback that I can put back to try to figure out what I should actually have done.",
            "So we'll talk about this.",
            "A bunch.",
            "Bandit.",
            "I'll mention briefly, contextual bandits, contextual bandits, or the idea where, well, it's it's like that first problem that I showed you where you have features about the web users and you want to find out what the right thing to do for that web user is.",
            "But I'm going to only give you samples.",
            "I'm going to give you some of the kinds of web users that are out there and you have to generalize from one to another, but the feedback is immediate.",
            "This there's no sequential aspect to it.",
            "And you know, in reinforcement learning kind of sits in the middle of this mess, right?",
            "Reinforcement learning is, well, it's not going to be supervised.",
            "It's not going to be exhaustive, and it's not going to be one shot.",
            "It's going to be sequential.",
            "Sampled and valuative, and yet even with even in the face of this extremely weak feedback, we have to try to make some good decisions."
        ],
        [
            "Yes.",
            "I think."
        ],
        [
            "That would make some sense, especially because this is the first time we're doing one of these tutorial things and I don't know if.",
            "I mean, I tried to do that set up in the beginning, but I don't know that work.",
            "So I don't know who I'm actually talking to or what you're actually concerned about.",
            "So yeah, so questions would be great.",
            "Do you want to ask one or do you want to just let people know that that's?",
            "It was hard for me to imagine a real life problem where I might get to see all the exs and then just figure out what to do.",
            "Sure so so.",
            "There's two reasons for studying this exhaustive case in the reinforcement learning setting.",
            "One is that the theory goes through really nicely and we can understand it, so it's kind of useful to study those to figure out how things could work in the best of all possible worlds.",
            "Turn problems into those kinds of problems by, for example, either unsupervised clustering or by hand clustering things together.",
            "So there's this big complex continuous space, but we're going to decide there's this kind of thing and this kind of thing in this kind of thing.",
            "An never revisit those decisions, and so that gives us an ability to actually evaluate things in an exhaustive way.",
            "Alright, so one of my plans here is that once I get through the kind of the beginning part.",
            "If there are areas I'm going to, I'm going to say some things about some things and not, say, some things about other things.",
            "I guess you probably won't tell me if I'm saying too much about something that you don't want to hear about, but you can't.",
            "You should probably tell me if I'm saying too little about something you do want to hear about."
        ],
        [
            "Alright, so so so why evaluative samples sequential?",
            "So this is this is a case that I really like, this is.",
            "I spend a lot of time watching little kids crawl because I feel like it's.",
            "It's a great example of a reinforcement learning problem.",
            "The full reinforcement learning problem.",
            "So this is kind of classic crawling.",
            "This is my nephew Reese getting from one part of the room to another and this is what you think of is crawling and so if I tell you yeah, crawling is this sort of really interesting computational problem.",
            "You might say not really, but wait.",
            "Different kids come up with different answers to the question of how am I going to get from point A to point B?",
            "So friend of mine from college, sorry.",
            "His his kid came up with that sort of I don't know bear crawl where he didn't put his knees on the ground.",
            "This is my my nephew Sidney.",
            "I don't know what she was thinking.",
            "Right, but she came with this sort of very weird, asymmetrical.",
            "You know soldiery crawl thing, my nephew Cameron, but the symmetry was important.",
            "So this is kind of like the butterfly stroke in swimming, except on linoleum.",
            "This is the brother of the one who did the bear crawl.",
            "He actually had had surgery very early on in his life, and it was difficult for him to use his hands in the standard crawling motion.",
            "He just came up with something else.",
            "He just, would, you know, scoot along on his bottom.",
            "Just paddling with his other hand.",
            "It's really, really good solution in the sense that he had one hand free to pick things up and manipulate them.",
            "The other hand is, you know, is busy and he doesn't have to pay for the damage to the clothes.",
            "So it's from his perspective.",
            "This is a really good answer.",
            "And this is Charles Isbell's daughter, Joanie.",
            "He'll be here later, but he's not here now to make me not play this, she came up with a solution that was maybe a local minimum.",
            "So she's trying to get to the ball.",
            "But Alas.",
            "When she was in the right place, she was in the wrong orientation again.",
            "It's it's a clever thing because kids do start off doing this rolling action, and so if you could just translate that into oh, I can use this to get around.",
            "That's that's a really good answer, except it has flaws.",
            "She's gotten past this, so we need not worry about her, but but OK.",
            "So what's my point in this, other than I can show you videos of people who I know and that is that that in each case here the feedback that they're getting?",
            "They're not.",
            "These kids are not watching other people crawl and saying, Oh yeah, that's a really good idea.",
            "I'll do this right.",
            "They're just getting a value to feedback.",
            "I'm trying to move over there and it either worked or didn't work.",
            "They're getting sampled feedback.",
            "They haven't lived in all possible situations.",
            "So as Susan pointed out, it's just rare that you get an exhaustive listing of everything that could ever happen, so they're clearly learning from special cases and generalizing to other cases.",
            "And it's definitely very sequential, right?",
            "So they're crawling from point A to point B and almost all these videos you can see it really well in jonis.",
            "They're all trying to get to something.",
            "There's an actual thing that they're trying to get to, and they know ultimately whether they got there or not, but there's a lot of lot of wiggling around that happens before that, and they have to somehow use the information of.",
            "I finally got to the ball or not too.",
            "Do credit assignment to all the different wiggles that happened before that right?",
            "So this is actually I want to claim a really hard and interesting problem.",
            "I don't think walking is quite as interesting because I think there's a lot of evolutionary structure that makes us all walk.",
            "Using at least a similar structure, I mean obviously some people more bouncy and some people you know there's lots of different kinds of walks that you can do, but they all involve you know left foot, right foot effort like the contact pattern is pretty much the same.",
            "But these were very, very different, like structurally very different things that they were coming up with.",
            "So I think it's I think it's a really interesting case to think about.",
            "And again, it felt very much like it's driven by some kind of motivation.",
            "I want to get to an object.",
            "How do I do that?",
            "How do I wiggle so that my dreams come true?"
        ],
        [
            "Alright, so here's a here's a robotic task that has some of the some of the features in common with that though.",
            "It's quite lame actually.",
            "When you get right down to it, so this is an AIBO robot that we programmed to try to figure out which way to turn in the lab so they could see the pink ball.",
            "Which I guess is kind of similar to the Joni case.",
            "She was also trying to get a wall, so the robot you know gets happy when it sees the ball, and so it's trying to figure out how to relate its camera input to which direction it should move to ultimately see the ball, and so it has all these features again.",
            "So it's evaluative in that we're not telling it which which way to turn left or right.",
            "We're just telling it, yeah, you saw the bar, or you haven't seen the ball yet.",
            "And it's it's sampled because the input that the robot is actually using here is a color histogram coming from its nose camera.",
            "So it's just looking out in the lab and it sees certain kinds of color patterns.",
            "And it probably never sees the same exact color pattern twice, so it has to be able to generalize between these different color patterns and its sequential in that the robot is in a position it takes an action, and there's no immediate feedback for that, necessarily.",
            "Another action, and then another action, and then maybe eventually it sees the ball.",
            "OK, now there's feedback, but it somehow was all set up by the actions that came before it.",
            "So again, not nearly as cool as.",
            "Little kids crawling, but again, has these three main features that we have to figure out how to deal with computationally.",
            "If we want to understand and create this kind of behavior.",
            "Hey good at the moment.",
            "Alright."
        ],
        [
            "Alright, so this is something that I call the reinforcement learning hypothesis, and again this comes from the computational perspective.",
            "The idea is that I'm interested in making smart critters like programs that do things that are intelligent looking or intelligent.",
            "Equivalent or something like that, so I'm going to say what motivates me is the idea that I believe intelligent behavior arises from the action of an individual seeking to maximize its received reward signals in a complex and changing world.",
            "So so essentially, the reinforcement learning hypothesis is you want to make AI or something like it.",
            "You have to do.",
            "You have to do reinforcement learning.",
            "Reinforcement learning is the way that complex agents in complex worlds can actually achieve their ends.",
            "Alright, so if you buy into that, you don't have to buy into it, but it'll probably be really boring for all of us if you don't, there's really 2 main things you have to you have to think about computationally to make this fly.",
            "One is you have to figure out where reward signals come from.",
            "That's actually kind of important and not very well studied.",
            "And then you have to develop algorithms that actually search the space of behaviors to maximize the reward functions.",
            "So just trying to make this a little more."
        ],
        [
            "Crete if you haven't seen this picture before, this comes from.",
            "Rich in Andy's book.",
            "And.",
            "This is kind of I don't know the reinforcement learning.",
            "API in a nutshell, it says that the world consists of an agent and an environment and they have the following conversation with each other.",
            "The agent chooses an action or makes a decision of some kind and it gets transmitted to the environment.",
            "The environment somehow processes that and spits out state what the what the new situation is.",
            "The new values is like what I was calling X before.",
            "And that's going to get transmitted to the agent along with some kind of evaluative feedback.",
            "So the environment is generating what state is going to come next, and the value of feedback.",
            "And the agent gets to choose an action and that gets fed back in and we just go around and around and around.",
            "This is in some sense a description of the system, but it also gives us a way of breaking it down into a computational problem or multiple computational problems."
        ],
        [
            "I want to say.",
            "If you're an agent builder, this is what the world looks like to you as an agent.",
            "So the agent is seeing state seeing what's the current situation, which way by facing what is or what?",
            "What information by getting from my nose camera.",
            "And the reward signal and it's trying to turn that into actions, and in particular.",
            "It's trying to figure out how to act to maximize its cumulative.",
            "Let's say discounted expected reward.",
            "So the amount of the total of the rewards that it gets averaged.",
            "And future discounted usually, and I feel like that's a question that Susan might want to ask me, but I don't want to answer it, which is why discounting so discounting says that we're going to get full value for award that we get now we're going to get like .9 of value for rewards.",
            "We get one step from now we get .9 squared value of things that happened, 2 steps from now, and it fades off like that.",
            "At a high level, this is a very reasonable thing, right?",
            "So like you know, give me the money now versus you know, give me the money.",
            "In two years, I'd really rather have it now.",
            "It sort of gets less useful to me as time goes on.",
            "Why does it have this exact form?",
            "Mostly I've heard people explain this mathematically, which I find not that satisfying.",
            "So at the end of the day, it's sort of because that's what we do, which is terrible answer, just terrible, and no one should invite me back to talk again because I said it, but.",
            "The fact the matter is this is the model that we're going to stick with for the purposes of this tutorial.",
            "There are people who are trying to question these assumptions and think about other kinds of.",
            "Reward metrics that you could try to optimize, but this is this is, I feel like the most straightforward one to understand and to connect to things and to work with, so we're just going to go with that.",
            "And so OK.",
            "So to the extent that the agent is trying to do is figure out how to map state to action so that reward is maximized.",
            "It's really an optimization problem of all possible ways of taking what I'm seeing and turning it into what I'm doing.",
            "Which one has the highest reward?",
            "So computationally it's very simple.",
            "It's not a simple problem, it's not.",
            "It's a very difficult problem, but it's a very simple to state problem, right?",
            "Like a possible behaviors, find the best one.",
            "There's not very many behaviors, then this is not hard at all.",
            "You just try them all."
        ],
        [
            "OK, but that's the world from the agent's perspective.",
            "I also want to highlight what the world looks like from the reward functions perspective, and Andy and riches figure in the book didn't didn't have this as a separate entity in the diagram, but I know in some of their papers they have actually called that out as a separate piece.",
            "So what's the reward function?",
            "The reward function is something that's taking.",
            "The actions that it's seeing from the agent.",
            "And turning them into evaluations, it's actually deciding was that good or not?",
            "How good was it?",
            "Is it 7 good?",
            "Was it 11 good?",
            "You know the reward function has to decide that.",
            "From the agent's perspective, the reward function is part of the environment, right?",
            "Something out there is just telling it what's good and bad, and it's slavishly trying to follow that.",
            "From the environment perspective, the actual physical world, if you will.",
            "The reward function is part of the agent, right?",
            "'cause the environment doesn't really care about anything.",
            "The environment just is.",
            "So the reward function is kind of this piece that you can think of is living inside the agent's head that is motivating the agent to do whatever it is that it does.",
            "And So what does the world look like to the reward function?",
            "It's trying to answer the question, how do I deliver reward so that the agent adopts desirable behavior?",
            "So in a sense, the reward function encapsulates some sort of desire about how should critters behave.",
            "And it should be chosen in such a way that actually maximizing that really does bring about the desired behavior.",
            "So from an economic standpoint, this is kind of mechanism design, so this is like how do you make an economy so that when things flow around, goodness happens.",
            "Right and so and again, for the most part, the.",
            "Computational reinforcement learning has focused on the agent design.",
            "We just take this as a given, but it really worth asking the question.",
            "Where does this come from and what are the implications about choosing one particular reward function as opposed to some other reward function?"
        ],
        [
            "Alright, so just one more word about reward before I start diving into algorithms and stuff.",
            "So rewards are kind of expressive as a way of telling creators what to do.",
            "I'm on the fence about this right now, actually, but I'm going to say that it's somewhat expressive.",
            "There's certainly a lot of possible numbers, so it's expressive in that sense you can.",
            "You can, by assigning a high reward to something you could encourage an agent to try to reach a goal state.",
            "So this is this is little soccer gridworld and the agent A is supposed to bring the ball into the goal, and so it can do anything at once.",
            "But if it brings the ball into the goal, it gets a plus one.",
            "So this is by virtue of putting this reward into the environment.",
            "We're actually defining the agents task.",
            "Go to the goal, put the.",
            "Put the ball in the goal.",
            "But we can also say things like avoid a failure state, so we can have.",
            "This is a cart pole, so this is a part that can ride back and forth and it's balancing this pole on it an we can for example put a negative one on both sides of this track here so that if it hits it or if the pole falls onto it then Ouch, but otherwise everything is cool.",
            "So by by having again zero awards everywhere except for in these bad states, it's actually saying to the agent.",
            "Again, you can do whatever you want, but don't drop the pole 'cause if you drop the pole, you're going.",
            "It's going to cost you.",
            "A lot of reinforcement learning problems that I've seen have both of these at once, like there's good things in the world and there's things you should avoid and so will put some positives and some negatives out there in the agent has to kind of wind its way through the environment.",
            "And then another way that rewards are often used as is a kind of shaping to actually make the learning problem easier and.",
            "There's debate about whether this is a good idea or not.",
            "But it's a good idea, so it's a necessary idea, and in particular, in this poll balance, in case we might say something like you know each step at which the pole is still balanced, I'm going to give you a + .1 for that like, not so much that it's the most important thing in the world.",
            "You, but enough that at least draws your attention to the fact that while it's balanced, that's good.",
            "Of course, ultimately it's the same, because if you drop the pole, you get the minus, and so that's bad, so you didn't.",
            "You didn't get the zero, so it's worse than the zeros.",
            "But sometimes these learning algorithms are really helped by this that they get something something more short-term that they can try to optimize instead of just.",
            "Empty vastness and then all of a sudden oh that was bad right?",
            "So little little little hints along the way.",
            "They can actually guide its decisions without having to have the whole big picture in its head all at once.",
            "OK, that's all I was going to.",
            "Yes, Susan.",
            "Problem, can you just say a little bit about?",
            "A setting in which you would want to have you want to shape the rewards in a real life problem?",
            "Yeah, absolutely so.",
            "I'm going to go with mobile Health because I don't really know about that, but you do and so you can correct me if I'm wrong so.",
            "If you're trying to get somebody to you know, do good things for their health and their longevity.",
            "You can give them zeros all the time, and then a -- 10 when they're dead.",
            "Right, but that turns out not to actually help them adjust their behavior much on the fly.",
            "So instead you might want to do, like yeah, I'm going to give you a plus one for jogging today.",
            "I'm going to be a -- 1 for getting 3 cookies and putting them on your desk because you're going to one at the end of each hour as a reward so so you can.",
            "You can actually sort of shape the behavior in a way that is again makes it easier to learn.",
            "It kind of makes things temporally more close in time, but hopefully doesn't actually change the the the correct behavior.",
            "Doesn't it still the right thing to do is you eat healthy exercise, do the things you're supposed to do.",
            "We're just giving you a little bit more information to make it easier to find that behavior.",
            "That and so we see this with robots and stuff all the time, and the soccer example is kind of a classic one.",
            "There was a time Once Upon a time where people actually had robots playing soccer and they decided to set it up as a reinforcement learning problem.",
            "Just like this, you know.",
            "Plus one when you score a goal or they didn't do this, but even better would be plus one for winning the game.",
            "Like is it good to score goals or not?",
            "Really, that's not my decision to make.",
            "Ultimately, I'm just going to tell you whether not you won the game at the end.",
            "If it's if you feel it helps you to have scored goals to achieve that, end good for you, but that's not part of the real reward function.",
            "And it's really hard, right?",
            "You have these robots just sort of wandering around, and then you know.",
            "And then all of a sudden the buzzer rings and they want and like nothing like they had no idea what had happened.",
            "Maybe one of them bump the ball into the goal that it's very hard to connect up all these different actions to the objective of the problem.",
            "So you start to throw in extra extra kinds of rewards.",
            "So you say, yeah, you know, it's good to have the ball.",
            "I'm going to give you a little plus for that, but not so good that you're going to, you know, like hog the ball.",
            "And this ends up being this very tricky kind of reward engineering thing that you have to do.",
            "Where again you give it, you give it, you know bonus for having the ball, but not so much that the agent is not going to be willing to kick it into the goal because like, but I'd like to have.",
            "Right, so so.",
            "Also stay away from the opponents little minus.",
            "For that you know.",
            "Don't be far from the goal.",
            "Don't be too close to the goal right?",
            "All these different hints could be turned into little extra rewards, not on a huge scale.",
            "Again, because that changes the nature of the problem but not on ignorable scale, because then it's actually not helping the agent at all."
        ],
        [
            "Alright, so now I want to dive into the agent side of the equation so so.",
            "How do we make agents actually make decisions to maximize reward?"
        ],
        [
            "So.",
            "The we have to to turn these into computational problems.",
            "We need to kind of formal problem specification, and there's actually a lot of ways to do this, but the field has pretty much committed itself to Markov decision processes and variants thereof.",
            "Which may not have been a great idea, but again, this is.",
            "This is a tutorial I'm just going to tell you what is we can worry about?",
            "What what could have been some other time?",
            "Over drinks at the end of the day, say alright so so this is this is this.",
            "I don't want to quit reinforcement learning with this model, but some people do and it is.",
            "Even if they don't, they often mean this when they're talking about reinforcement learning, and so the model goes like this.",
            "We're going to imagine that the environment part of the environment agent picture.",
            "The environment itself consists of two primary functions.",
            "The transition function T. Takes as input the current state in action and outputs a probability distribution over next states.",
            "So if I'm in, if I'm in this situation and I tried this action, there's some probability I'm going to fall in my face and there's some probability that I won't, and so you know the environment is the way it is.",
            "It specifies those probabilities.",
            "I might not know them.",
            "Which probably should keep me away from the edge, but but they they are specified there there in the in the environment model.",
            "The reward function is.",
            "The reward function is.",
            "It's the mapping from.",
            "If you're in some state and you take some action, here's what the evaluation for that is.",
            "Here's the points that you get for that.",
            "So like falling on the face probably going to be low scoring thing.",
            "That being said, probably would all remember this talk for very long time, so we have an upside to.",
            "But not in the short term.",
            "In the short term it would mostly be painful, alright, so so that's we think of the environment is consisting of these two functions, so they're embedded inside the environment, picture here.",
            "And what we're done trying to do as the agent is decide.",
            "Which actions do we take in which states?",
            "So we maximize our cumulative discount.",
            "That expected reward.",
            "So we often write this as a policy.",
            "This is \u03c0 star, which is mapping states to actions and the way that's going to do that is by taking argmax over this function.",
            "Q star.",
            "OK, so that's not helping what's Q star, so this is my attempt at.",
            "Trying to visualize that.",
            "So what we're imagining is that the critter in the environment is undergoing a series of transitions.",
            "It was in some state, it shows some action, it got some reward for that, and it ended up in some new state from which it could take another action and get another reward.",
            "And get transition to yet another state.",
            "Take another action and this chain goes on and on and on.",
            "And we want to capture information about the expected reward that whatever behavior it's going to take brings.",
            "So if the agent is acting optimally, it's acting in such a way that it maximizes his future discounted reward.",
            "Then what do we know about that?",
            "Well?",
            "Let's say let's say we're going to estimate actually something a little bit different, so the expected future discounted reward for starting in some State S taking an action A for one step and then behaving optimally thereafter.",
            "It's not exactly clear how we're going to know what that is, but it's nicely recursively defined in such a way that will be able to do it when we get there.",
            "But for now, just imagine that's what we're trying to optimize.",
            "No, sorry, that's what we're trying to evaluate or estimate.",
            "What's the total expected discounted reward for starting in some state and taking some action?",
            "For one step and then behaving optimally thereafter so we can write down what that's going to be, it's going to be whatever the immediate reward is for taking that action in that state plus the discounted value of the future and what's the future.",
            "What's the sum over all possible next states of the probability of reaching that next state as prime, given that we were in state S and took action A?",
            "This is the transition function.",
            "Times the value of the rest of the rest of the life after that.",
            "And how are we going to decide what to do there?",
            "Well, we're going to.",
            "We're going to some new state S prime.",
            "And if we actually have this Q star function.",
            "We should take whichever action first action leads to the hyest.",
            "Expected discounted reward from that state.",
            "So once we get dropped into S prime, we actually use this Q star function to decide what to do.",
            "So this is in a sense this is the optimal policy.",
            "This is the value of the optimal policy.",
            "This is the evaluation of that optimal policy, so there's again sort of recursively defined, but they have all the information that you need all in one little package.",
            "Alright.",
            "Any questions about that sort of a lot of the rest of the math leans on this pretty heavily, so if you're not super comfortable with this, then I probably should spend another moment on it.",
            "I mean, you could be wondering how would you possibly find Q star?",
            "I'll get to that, yes?",
            "Ah, good, so yes, so.",
            "Max first is you know you have a bunch of different options and give me the value of the highest scoring option.",
            "Argmax is which action actually gives me that highest scoring option.",
            "So this is just saying when I'm in some state, figure out which action brings me the highest future expected value from that state and return that action as the action I should take in that state.",
            "OK. Are ready.",
            "So."
        ],
        [
            "Let me just relate this briefly to the find the ball problem.",
            "This is you can think of this as being an MVP.",
            "In fact we did.",
            "That's how we did it.",
            "That's how we ended up building a reinforcement learning for this.",
            "Each different angle that the dog could be standing at is in the sense of different state.",
            "And then there's one of the States where connection is actually probably a collection of states where it actually can see the pink ball in its field of view.",
            "So we don't actually model it this way.",
            "We model it this way each.",
            "Each different angle that it could be is a different node in this graph.",
            "In this Markov decision process, the actions that can take turn left or turn right, or arrows in this graph.",
            "There's often probabilities annotated on these arrows.",
            "So for example, if turning left sometimes actually leaves you pretty much in the same place, but sometimes actually trans.",
            "Sometimes what you backwards sometimes actually turns left.",
            "That's all in the definition of the MDP.",
            "That whole probability distribution is in there.",
            "It's very rare, for example, to be in this state.",
            "You know facing this way, take the turn right action for one step and end up over here so that that transition is going to have probably 0 probability actually.",
            "And then some of the states we can actually annotate with their rewards, or actually all of them.",
            "We have to annotate with rewards so we could do it as zero reward everywhere, except for when you see the the ball and you get a plus one for that."
        ],
        [
            "Alright, so this is.",
            "Everything about solving the Bellman equation on one slide.",
            "I I made this slide.",
            "I was very proud of myself and then I called into question whether this was a good idea at all.",
            "'cause it's it's gets a little dense.",
            "But it's nice having it all in one place, so you can make connections between, so let me give this a try and then you can.",
            "You should ask me follow questions so.",
            "In a sense, planning an MVP decision making an MVP is solve the Bellman equation, figure out a Q star that satisfies this set of actually nonlinear constraints.",
            "'cause of the Max.",
            "And if I have that if I have a function that satisfies this equation, then I know anytime I'm going to state how good is it going to take this one action and then behave optimally.",
            "So I always take the best one.",
            "I always be optimal after that.",
            "OK solving Bellman equation we're done with the planning.",
            "We know what we need to do.",
            "So how do we do that?",
            "How do we solve equations?",
            "So there's there's really three main algorithms that talked about a lot valuation policy iteration in linear programming.",
            "There are some algorithms that are heuristics and blends of these in various ways, But these are kind of the three pillars on which most other things seem to be built.",
            "So the idea of.",
            "Value iteration is OK. We want to solve this equation.",
            "So what we're going to do is we're going to guess.",
            "Acute function.",
            "If we're right, then awesome, we're done.",
            "We're probably not going to be right, so we'll just guess it to be all zeros.",
            "Which works if all the rewards are heroes.",
            "But again, they're probably not, so we just discussed that as a starting place, and then we say, OK, we now have an estimate of Q star gas of actually fairly poor, guess probably of Q star.",
            "We want two star this Q star to actually equal this equation of the old Q star.",
            "So we actually treat it as an assignment statement.",
            "We actually say take our previous guests of the values.",
            "Compute for each for each state action pair, if I were to.",
            "Take that action from that state and then look at the distribution of states.",
            "I end up in and when I end up in them, use my estimate of the Q function that I had from my previous iteration of this algorithm, which again started off at zero really bad, but now it's a little bit better 'cause it's taking a little bit of the context into account.",
            "Then what we know is this algorithm.",
            "If we just keep iterating this, we keep reassigning the values based on this equation over and over and over again.",
            "In the limit it converges to the actual solution to the to the Bellman equation.",
            "So that's cool.",
            "This is the algorithm that a lot of people.",
            "Start with or or use because it's it really is very simple and it's it's a good workhorse 'cause you don't have to get a lot of things right.",
            "It just kind of does what it's supposed to do.",
            "But for me theoretical perspective, it's sort of awkward that it's OK, so it converges in the limit.",
            "In other words, Infinity from now you'll have the answer which.",
            "It's kind of the same as not having the answer ever.",
            "So there are other algorithms that we could apply, so policy iteration is is a variant of this idea.",
            "In some ways that actually converges in finite time.",
            "So the way policy iteration works is again, we're going to start off guessing a random queue function.",
            "Say all zeros doesn't matter, but let's just go with all zeros.",
            "Then we're going to say, let's use that Q function to build a policy.",
            "So what policy do I get if from every state I choose, whichever action gives me the highest estimated Q value in that state.",
            "Starting off at all zeros, it's just going to be some arbitrary policy, just completely untethered to reality, but it's something something concrete then what we do is we say, OK, that's a way to behave.",
            "How does that go?",
            "Let's actually evaluate AQ function specifically for that policy, so instead of Q star that we were looking at before is looking for the values of the optimal policy.",
            "I just want to know the values of the policy that we were just talking about.",
            "This is called policy evaluation, and this is actually a system of linear equations.",
            "This can be solved actually.",
            "Very reliably, very fast are our friends in numerical analysis.",
            "Do this for us and then we don't have to worry about Matlab, does this right?",
            "This is just done.",
            "So, and specifically what it's saying is well under this policy that we have.",
            "If we're in some state and we take some action, we're going to get the reward for that action plus the discounted expected value of the state that we end up in.",
            "Given that we take actions according to our policy \u03c0 T. Then when we land in state S prime, we're going to look at the Q value for the action that we're going to take from state S prime.",
            "Alright, so this evaluates policy \u03c0 T. And then we loop out again.",
            "So we now we have a new queue function.",
            "We can generate a new policy.",
            "But that is greedy with respect to that queue function that actually tries to maximize reward for that queue function.",
            "Then we can evaluate that.",
            "Then we can update our policy and we can evaluate that and we can up their policy.",
            "So again, fairly simple.",
            "But what's neat about this algorithm is each time.",
            "It doesn't evaluation.",
            "It either comes back with the same policy it had before.",
            "If it's optimal.",
            "Or it comes up with a policy that's better than the one that it had before.",
            "So because each time that it doesn't evaluation, it gets a better policy and there's only a finite number of policies.",
            "It's big, but this finite each state.",
            "There's some action that you choose there, so it's number of actions raised to the number of state, power, different combinations that you can have.",
            "Each time we do an iteration of policy iteration, at least one of those is history.",
            "So we're just ticking through them.",
            "Again, there's a lot of them.",
            "But eventually we hit the last one because there's a finite number of these things.",
            "So this is this is kind of a powerful thing to do.",
            "People don't do this very much.",
            "Mean people.",
            "When I say people, I mean people who write programs that solve them DPS.",
            "Don't use policy directions so much, but.",
            "But it's really quite.",
            "It's really quite lovely alright, and the last thing to mention is linear programming, so this is a neat idea that says what we're going to do is take our Markov decision process and convert it to a linear program, which is kind of an optimization problem that we know how to solve in polynomial time that we know how to actually solve efficiently.",
            "So if we can do that, if we can actually express the MDP perfectly as a as a linear program, then we hit it with a linear programming stick and the solution pops out and we're done.",
            "So this is, I think, done even less than policy iteration, but it is nice in terms of the guarantees that we get.",
            "So here's the way that this is done.",
            "What we're going to do is we're going to set variables that represent the value of the best action at each of the states S. And those are variables we don't know them at first, but we're just going to put them into this optimization problem as variables that the program is going to solve.",
            "We want to find the smallest sum of those variables such that for Allstate Action pairs the value at any state is bigger than or equal to.",
            "This which is the Q value at that state.",
            "According to the value function that we have.",
            "So what this is actually saying is.",
            "Make it so that the maximum action is the maximum action.",
            "It's above all the others, but it can't be too much above the others.",
            "It actually has to be the largest of them, so we're going to minimize the values such that their upper bounds.",
            "So the minimum upper bound is exactly the Max, so this gives you a way of expressing the notion of Max.",
            "As a system of linear inequality's an A linear evaluation function, and that's the definition of a linear program so.",
            "Again, we have a polynomial size linear program.",
            "We solve that and we get the solution to arbitration.",
            "So OK, so that was all.",
            "That was all on one side so.",
            "Did any of that land is that?",
            "Is that helpful?",
            "What's the?",
            "What's the takeaway?",
            "The takeaway is if you give me an MDP and it doesn't have 10s of millions of states.",
            "Then I can hit it with a number of possible algorithmic.",
            "Algorithms and and figure out the optimal way to behave the way that actually maximizes expected reward.",
            "Alright, that's The upshot."
        ],
        [
            "Alright, but that's not telling us about reinforcement learning in a sense, right?",
            "This is really telling us if someone gave me an MVP I could.",
            "I could figure out how to optimize reward, but the reinforcement learning problem is no.",
            "I just get to live in the world and I have to figure out from that how the world works.",
            "So there's a number of different flavors of algorithms that people studied.",
            "I'm going to focus on 2 right now, model free learning and model based learning.",
            "These seem to have had a lot of.",
            "Have shown that they have a lot of utility for explaining lots of different behaviors, even on the.",
            "Natural or LDM side, so it's it's good to kind of have a gut feeling about what this distinction is about.",
            "So let me start with model free learning.",
            "So the idea of model free learning.",
            "Is what the Asian experience is.",
            "Is this sequence of?",
            "I'm going to call them stars is if you haven't heard.",
            "People say that before.",
            "It probably sounds silly, but state action reward next state is SARS.",
            "It's not the respiratory SARS, it's the.",
            "If they are, we reinforcement learning so.",
            "So what?",
            "The agent is seeing is a whole sequence of those right, seeing a lot of state transitions, a lot of rewards go by.",
            "And what these model free algorithms do is they use that information to tweak an estimate of the Q function, so there's the Q function.",
            "It's out there, but we don't know it and we can't compute it 'cause we don't know transitions and rewards were just out in the world doing our thing, but each time we experienced one of these stars is we can use that to essentially improve our estimate of Q.",
            "We can use that estimate of the Q function.",
            "I drew it as like a cartoon EQ.",
            "'cause it's not the real Q, it's just kind of our best guess as it.",
            "It's like a caricature of the real Q.",
            "If we have that, if we have a guest of the of the best Q function, we can use that to act by by saying things like, well, I'm in a state.",
            "My cartoony Q function tells me this is the best action I should take in that state.",
            "Maybe I'll take that.",
            "And that gives us a way of generating actions.",
            "Alright, so.",
            "This experience that we get is being used to build up Q or actually revise Q directly.",
            "An algorithms like Q Learning and Sarsa, which I'll talk about are the most studied algorithms, kind of in this class.",
            "So sorry, no sorry this class is the most studied class I think of reinforcement learning algorithms and Q learning and Sarsa are good examples of algorithms in that class.",
            "So let me just let me contrast that with model based learning.",
            "So the idea in model based learning is we're actually going to take our sources of experience and use them to estimate are cartoony versions of the transition function in the reward function.",
            "So I've been in some state I tried in action.",
            "This is what I saw.",
            "Maybe that's how the world works.",
            "So I record that in my estimate of the transition reward function and now I can actually use any of those algorithms on the previous slide to turn those into an estimate of the Q function, and then I can use that to act.",
            "So both of these algorithms go through the queue function as kind of a critical bottleneck, but one tries to go there directly and one tries to go there via.",
            "The model.",
            "Alright, so.",
            "Part of me wants to say why this is important to people like on the biological side, but either either you know that already and I shouldn't tell you, or you don't know that.",
            "And then I'll probably tell you wrong and then you'll be wrong.",
            "That would hurt all of us so.",
            "I don't know any any does this distinction makes it so.",
            "Here's what people sometimes say they think of this kind of Q update type algorithms model free update algorithms as being kind of like habits, right?",
            "So it gives me a way of just acting without really thinking about it.",
            "When I'm going to state, I just look up the Q function and the Q function says this is the good thing to do, so I do it like, oh man, that's not even I was trying to.",
            "I was trying to drive to the store, not to my house.",
            "And they like in the this model based learning to more planful behavior that says, OK, I know what it is I'm trying to do.",
            "I know my reward function is.",
            "I understand how the world works.",
            "This would be a way that I would act that would give me the reward that I'm seeking.",
            "Alright, so this is more kind of.",
            "Habitual I forgot the other ones, called Peter came back.",
            "Maybe Peter notes.",
            "Goal directed thank you alright.",
            "Alright, so habitual goal directed.",
            "To a first approximation, I've seen people make other kinds of weird approximations of these.",
            "Also, you're talking about morals.",
            "It's like, well, there's morals that say you shouldn't do that action.",
            "You know that Q funds that Q values prohibited or you shouldn't take an action that would cause that to come about.",
            "And that's more like the model based kind of way of thinking about things.",
            "And philosophers have names for those two that I'm not going to say."
        ],
        [
            "Alright, but I do but I really do need to tell you what Q learning is 'cause.",
            "First of all, it's kind of awesome and second of all.",
            "Mostly it's kind of awesome, but second of all, but lots of people know about it, so if you don't know about it, you're going to, you know, not get to hang out at that party.",
            "So alright, so here's cooler.",
            "So Q learning actually is a family of algorithms.",
            "They all have one really important step in common, but they vary as to the how the other steps are built.",
            "So here the four steps and I used a slightly grayer color for the ones that are that have multiple ways of filling them in and then black for the really important right?",
            "So here's how cooling works.",
            "We have to initialize our Q function.",
            "We start off saying, here's what I think the values actually are going to be, which is probably going to be wrong.",
            "'cause if you knew that in advance you would need to learn so.",
            "So it's but the whether you guess high or low in the beginning turns out to be really important into the way the algorithm behaves.",
            "So OK, so pick a way to initialize your Q function then.",
            "We have to act, so at each step Q learning is in some state and it says.",
            "What should I do now?",
            "So a common thing to do would be use your estimated queue function to find the action that has the highest expected reward and take it.",
            "But you can't just do that.",
            "It turns out because first of all your Q function is garbage in the beginning, and so if you follow it.",
            "You might get lots of data that is actually not very helpful in figuring out how to optimize.",
            "Reward.",
            "The bad case here is typically.",
            "You believe that something out there, the Q function encodes the belief that something out there is really, really bad, so you don't want to go there.",
            "So the agent doesn't go there, so it doesn't learn that it wasn't actually that bad.",
            "So this is kind of a.",
            "You know very pessimistic view of the world.",
            "If you, if there's if there's negativeness out there and we and we believe it without testing it, then we might not actually be optimizing a reward in the long run.",
            "So right so it turns out to be important to sometimes explore.",
            "Actually sometimes try actions that the algorithm doesn't think are the best, but still maybe worth thinking about.",
            "Alright, so that's kind of the set up and now what happens?",
            "We get a SARS, so we were in some state.",
            "We chose some action.",
            "Then the environment gives us back a reward in the next state.",
            "This is what we do with that information.",
            "We take our cue function for the state action pair that we just left.",
            "That's the one that we're going to learn about.",
            "And we move it, we change its value alittle bit.",
            "This is a learning rate so just a tiny bit in the direction of what the immediate reward plus the discounted expected value of the next state.",
            "According to our cartoon Q function.",
            "Minus the Q value of the state that we just left.",
            "Right, so so this difference is actually really important part of this equation, so it's we're actually looking at the difference between how, how valuable we thought that state action pair was, and maybe how much how valuable we should think it is, because we just experienced an actual reward and a transition to a next state.",
            "And we can look up the value of that next date.",
            "So this is a guess as to the value of the state, action pair essay, and this is another guess.",
            "This is our current gas and the difference between them gives us an error signal, so if they are exactly the same, the error zero.",
            "So we don't have to change.",
            "Our guests at all, 'cause we're perfect.",
            "But if.",
            "It looks like so if this is too small, for example, so we gotta get a small value out of out of this and we had guest a really big value here.",
            "This difference is going to be negative, and that's going to cause this Q value to get pushed down.",
            "So it looked like it was too big.",
            "Then the learning rule says make it smaller if it looks like it's too small then learning rule says make it bigger.",
            "So.",
            "It makes sense.",
            "Um?",
            "And then we have to decide what to do with this learning rate.",
            "So this is this is a little multiplier that says we want to move a little bit in that direction.",
            "How a little bit do you want to move?",
            "It turns out you have to change this overtime.",
            "Well, it turns out you can change this overtime and maybe you should if you want to get certain kinds of guarantees.",
            "So this is what we know about this algorithm.",
            "We know that there are choices for Step 1, two, and four.",
            "That actually guarantee that our estimate of the Q function converges to the actual solution to the Bellman equation.",
            "In the limit.",
            "So.",
            "There's a number of proofs of this.",
            "This is kind of the first one that was out there and appreciated.",
            "This is kind of awesome right?",
            "So that you may not appreciate.",
            "You may not think that it's awesome at all.",
            "You may be right, but like to me, there's an awesomeness here.",
            "Which is?",
            "This is essentially one line of code, and this one line of code solves a linear program.",
            "It solves the bellman equation.",
            "Given enough time experience, what not, but, but this is a really powerful thing and so this is almost the kind of thing that you can't not implement, so there's a lot of papers where people have.",
            "Very unreasonably used.",
            "Q Learning because they couldn't not do it.",
            "It was just.",
            "It's just so easy to do, you have to do it.",
            "Compelled so this is this is really an.",
            "It really does kind of work.",
            "What you find often is that it takes a very long time for you to get close to the Q values.",
            "The optimal Q values and so that the learning algorithm can actually spend a lot of time just kind of.",
            "Wandering around doing things that don't look all that useful, but all this is starting to add up.",
            "It's updating the Q values and ultimately it's making them equal to the solution to the medication.",
            "So couple of things to note about this.",
            "Another one is that learning here is off policy, which is to say it's learning about the values of behaving optimally, even though it's not necessarily behaving optimally.",
            "And that's because right here in the update it does this sort of hypothetical.",
            "Well, I'm going to imagine that when I get to the state as prime, I'll take the best action when it actually gets there.",
            "It might not do that, right?",
            "It might take an exploratory action, something random, but it's going to do its updates, assuming that that's the that's what it's doing, and so ultimately these values go to the value of the optimal policy, even if that's not the policy that the agent is following.",
            "And then the other thing I want to point out is the connection to temporal difference learning so.",
            "This is a really important concept.",
            "It comes up a lot in in temporal prediction and in the context of Q.",
            "Learning the temporal difference part is this temporal difference that we have our estimate of the Q value at one point in time and we have our estimate of the value at the next point in time and we relate them to each other and for it to work.",
            "In MPs we have to jiggle with this one a little bit because we might have gotten a little bit of reward and that caused him to be different.",
            "But it's this.",
            "It's this difference between our predictions that's driving the learning.",
            "OK. Great, I think I need some water.",
            "Actually, it's been an hour so I can have a look."
        ],
        [
            "Alright, I don't have too much to say about Sarsa, but if you've heard this phrase, you probably want at least know what it is, and it's not so hard to say what it is.",
            "So sources just like you learning.",
            "Except we replaced this update.",
            "Here we had a Max over actions that we're using.",
            "Instead, we're going to use some action, a prime, which is the action that we actually took in state S prime.",
            "And so for that to work, we actually have to keep track in terms of experience, not just state action reward next date.",
            "But we also have to know what the next action we choose is.",
            "Hence the name, Sarsa State Action Reward state action.",
            "And so this is actually learning on policy.",
            "It's actually learning.",
            "The value of the policy that it's actually following.",
            "If that happens to be the optimal policy, it will learn the optimal value function, but it might not be.",
            "It could be something else.",
            "It's just going to learn the value of that policy.",
            "One of the things that's nice about this is it interacts really well with other TD methods like TD Lambda, which is a family of algorithms for temporal prediction.",
            "That interpolates between immediate one step updates, which is what we have here and sort of more eventual like I did a whole sequence of things, and here's how it turned out.",
            "In the end I can use that to update my prediction as well.",
            "TD Lambda gives us a way of blending those two things together.",
            "And.",
            "The way that this connects with that is, if you think of essay as being like a thing, we're going essay.",
            "We get reward, and then we're in a new essay, so it's sort of like a Markov process or or transition process from state action pairs, two state action pairs, and so we can make predictions about if you're into action pair.",
            "What's that eventual reward going to lead to?"
        ],
        [
            "Alright, so that's that's two model free algorithms.",
            "Now I'm going to talk a little bit about model based learning.",
            "So.",
            "This is way more complicated looking than I should have made it so.",
            "So all I'm saying on this slide is.",
            "The way that we can do model based learning is we can say, well we were in some state.",
            "We took some action, we got some reward.",
            "We got some next state.",
            "Hey, that's kind of a little sample of information that we can use to estimate the rewards and transitions.",
            "So the reward we can actually move it a little bit closer to.",
            "Kind of a temporal differences kind of thing I guess, not temporal, just different, see.",
            "So that's what this is actually trying to compute is the average reward that we got when we were in this state and took this action.",
            "And these lines are just trying to do the same thing with transition probabilities, so we're just essentially counting when we were in this state and took this action.",
            "How many times do we end up in this next date this next date this next date, this next date, and average them out so we have a guess as to what the probability distribution looks like.",
            "So this gives us a way to actually capture.",
            "The MVP by just running around and experiencing it.",
            "And so one thing that we know is that if we actually visit Allstate, action pairs infinitely often, so we just really plow through that MVP over and over and over and over and over and over and over, then and our learning rates decay in the appropriate way.",
            "Then our estimate of the Q function that we get by solving.",
            "The MDP for our cartoony transition and reward function goes to the optimal Q function.",
            "So why is that interest?"
        ],
        [
            "Eating part of it is because the following is true.",
            "This is a really useful fact about MVP's and that is if I have a reward function Anna transition function, an approximation of each of those, so some reward function that's kind of close to the reward function and a guess of the transition function.",
            "That's kind of close to the transition function.",
            "Then if we solve the Bellman equation for our cartoony are isn't even get a cartoony Q.",
            "That's also going to be kind of close to the real Q function, so there's a kind of smoothness in this space if you change the parameters of an MVP.",
            "A little bit, it doesn't change the Q values that much.",
            "It contains the optimal policy a lot, but it's not going to change the values very much, and so you're going to have a really close estimate of those of those values.",
            "And in fact it's more than just that.",
            "It's close.",
            "We can actually bound how far off we would be if we were, you know, as a function of how far off are reward estimate is how far off are transition estimate is, what our discount factor looks like, how many States and actions we have we can bound.",
            "How far acting according to QQ is to actually getting optimal reward in that MVP?",
            "So we can say the reward that we get is going to be within this of the actual optimal, or we might not be perfect, but we're not far from perfect.",
            "So that's a really valuable result.",
            "And that that you know sort of justifies this idea that we can use a model based approach which is never in finite time going to.",
            "Have the actual true MVP learned, but that's OK because it will get close and close is going to be close enough.",
            "It it it's you, you discover how powerful this property is when you start working with Markov models that don't have this property, so that you can't actually learn anything because you learn for a finite amount of time and you have an estimate and it's off infinitesimally.",
            "But that gives you completely the wrong answer, so that happens for certain kinds of criteria, but not for these kind of MVP.",
            "Discounted reward criteria.",
            "So it's again, it's a, it's a.",
            "It's a nice property."
        ],
        [
            "Alright, so I want to briefly mention.",
            "This notion of generalized Oh yes, so so basically that means that the all this is continuous, right?",
            "But you could still have local minima, right?",
            "So what do you?"
        ],
        [
            "By continuous and well, what I mean is that there are no huge jumps in the in these functions are yes, right?",
            "So you have that kind of continuity.",
            "That's right there, but they're not know if we're learning the the reward in the transition function by actually visiting enough of the MDP, then solving this is not going to lead to a local minimum.",
            "It will actually approximate the real Q star, which is unique, right?",
            "So given an MVP there is one Q star for that MVP.",
            "At least in the discounted setting, gamma is less than one.",
            "OK, then there's one answer and we get.",
            "We get really close to it.",
            "Yeah.",
            "So if you if you don't know how far away your reward function is from the actual reward function, how can you estimate the distance of your value?",
            "Yeah, OK, so that's so there's.",
            "There's two points to make, I think about that one is.",
            "You can't, right?",
            "So this is.",
            "These bounds are not necessarily bounds that the agent knows while acting.",
            "These are bounds that just hold true of the agents behavior, right?",
            "So some external watcher of the agent can say, oh, you've been going this long and I can see how far your estimates are, and that's going to tell me how far your actions are going to be in terms of how much reward you're going to get, But the agent itself, yeah, might not know that at all.",
            "It might be very difficult to tell that unless additional assumptions are made on, say, the range of the possible rewards or.",
            "Mostly that.",
            "Yeah.",
            "OK, cool."
        ],
        [
            "Alright, so I wanted to mention this probably briefly.",
            "So.",
            "There's all this nice mathematical machinery that shows that you can do Q learning type things in MDP's, but MVP's are not.",
            "The only thing in the universe.",
            "So.",
            "It turns out that there's a lot of things that really closely related MVP's, and it would be a pain to have to do the math over and again, so Once Upon a time I worked with Chubb separate Roddy who could do that math and I asked him, could you just do this math once in this general way?",
            "And then I never have to do it, I'll just use your results over and over again with, you know tweaks.",
            "He was game for that, so we did that and so here's the form that we came up with, so it turns out that if you can think of your decision process as being kind of like an MVP, you're in a state you take an action.",
            "You get some reward.",
            "You get transitioned to some next state according to probabilities, and then when you get to that next state you have a lot of different choices about what action to choose next.",
            "So in the MVP setting we choose the maximum action, but you could choose the minimum action and it's like sort of a cost MDP.",
            "Or maybe you could choose like a safe action in some sense you know and be.",
            "Paranoid MDP or something like that like there's all sorts of different ways that you can choose what's a reasonable action to take from the state that you end up in, so let me just generalize across all of them and say that we've got some operator times I that we're going to apply to the RQ values that is a non expansive summary of the values of the actions in the state that we land in.",
            "For it to be non expansive, that means that when we summarize the value of the state that we land in this S prime for two different Q functions, that the difference between them is less than or equal to.",
            "The actual difference is the maximum difference between those two functions.",
            "So it can't get wider apart by doing this summary.",
            "So it turns out a lot of functions have this very nice property, so min and Max and mean and median and even some things that don't start with M actually satisfy this non expansion property.",
            "And because they do, that means if we choose actions according to any of these summary operators, we get two very nice things for free.",
            "One is value, iteration is going to converge, and the other one is that Q learning is going to converge.",
            "And so this now holds over a really wide range of possible algorithms, and we again ways of picking the action so that we just, you know, with one theorem we can just, you know, rule them all.",
            "Alright, so let me yes.",
            "I want to give an example but yeah good.",
            "So so it yeah, that's fair so.",
            "We change what we mean by optimal, which sounds like cheating.",
            "Now that I said out loud, it's just the solution to the Bellman equation where the Bellman equation uses that operator, whatever that operator is, and then the the Q learning also uses that operator in this spot, and so this ends up it ends up converging to the solution to that yeah, it would be really weird if it actually converge to the optimal thing even though we're trying to minimize.",
            "Yeah, that would be strange.",
            "Alright, so."
        ],
        [
            "I was mostly interested in this because I was I was really interested in multi agent decision-making.",
            "So how does an agent decide what to do to maximize its reward?",
            "Given that there's other agents in the world also trying to maximize reward?",
            "So, so we're talking about sequential sequential environments, and we've expanded the environment so that each state.",
            "Two agents are going to get to make a decision.",
            "This is action for me and there's action for my opponent or.",
            "I don't know friend whatever the other the other critter in my world with me.",
            "I have a reward function and the other guy has a reward function and the transitions depend on our joint decisions.",
            "Alright, so this is another kind of model is very similar to Markov decision processes, but it's not just me trying to maximize reward anymore, there's this other stuff going on in there.",
            "So let me let me give you a concrete example.",
            "So this is this is a little gridworld.",
            "If you were at our ODM last time, then you've seen this already.",
            "But this is a little little game that we played around with.",
            "We're actually in the process of getting this hooked up with and getting people to play it.",
            "Now 'cause we think it's more interesting than it.",
            "Themes.",
            "So, alright, so here's the game.",
            "The game is that that were one of these critters, say the redcritter.",
            "Mario and we get to go North, South East, West or stay put.",
            "And the other player, the green player gets to do the same thing at the same time, and then we move if we tried to move into the same place, then a coin gets flipped in.",
            "One of us makes it, the other one gets bounced back.",
            "If the red guy makes it to the red goal.",
            "He gets 100 points if the green guy makes it to the green goal, he gets 100 points, but the game ends at that point.",
            "So if I get me, the red guy gets to the red gold and green hasn't gotten there yet.",
            "Green gets nothing.",
            "So it's a game that sort of could be cooperative.",
            "We can both win, but it could also be competitive.",
            "I could win and you could lose, or vice versa.",
            "Alright, so that sort of clear that we go.",
            "So let me let me actually simulate this so.",
            "Let's say they chose to go to that center square on that first move because it was on their shortest paths to their respective goals.",
            "But Luigi got there in Mario didn't.",
            "So now they now they get to choose actions.",
            "And Luigi got to the goal, and now the game ends so I didn't get any points.",
            "In fact, it costs me a little bit to do all those moves.",
            "So that makes life feel worthwhile anyway, but this is this is a game in which we'd like to kind of make some decisions.",
            "We've actually figured out what's a reasonable way to behave in this environment.",
            "This environment.",
            "This particular grid is really irritating because.",
            "There isn't the right answer.",
            "It it depends what I the best thing for me to do depends what you're going to do.",
            "And of course the best thing for you to do depends on what I'm going to do, and so you can't just kind of a priority to solve this out.",
            "This is why we think it's going to be fun to do with people 'cause we think they're going to come up with some way to coordinate and then kind of stick with that.",
            "But it's not preordained.",
            "It's not something in the game.",
            "It actually comes out of the interaction between the players."
        ],
        [
            "So if we wanted to do decision making in this kind of environment, we can set up a kind of a Bellman equation that says if you know my Q value.",
            "Given that we're in state S and we choose this joint action, I choose a, you choose B. I'm going to get some immediate reward for that, and then it's going to get added to the reward I get after we transition to whatever new state we're going to go to, and then when we get there, actions are going to be chosen somehow.",
            "We have to pick that somehow eventually and based on that action that was picked, that's going to give us the respective Q value.",
            "So so how do we do that?",
            "How do we decide what's going to happen in the new state?",
            "So yeah, so this is my attempt to kind of making that visual.",
            "It sort of depends on what you think who you think you're playing with.",
            "What are the assumptions in the other?",
            "The other player there and so let me show you how different kinds of assumptions lead to different operators in that slot and can lead to learning algorithms that converge or don't converge."
        ],
        [
            "So, so we might think that we're playing against some kind of fixed agent that just if this has some way of behaving whenever it's in a state, it has some probability of choosing some action, and we might not know what it is at first.",
            "But we can play for awhile and we can learn that.",
            "In that particular case, actually, what we need to learn is what's the probability that my opponent will choose a given action given a state if that opponent really is just some kind of automaton just going to behave a certain way no matter what, then our summary operator is.",
            "Once we get to some new state S prime, the actions that we're going to choose our well, my opponent is going to choose according to the probability function that defines its behavior, and I'm going to choose to maximize my reward given that.",
            "OK, we can plug this in as our summary operator and this is kind of nice 'cause it is a non expansion so we get value duration working.",
            "We get Q learning working.",
            "And so that gives it.",
            "But it's a kind of a strong assumption to assume that the other player is stationary, right?",
            "The other players has a way of choosing probabilities and I cannot impact that.",
            "Yes.",
            "No, this is yeah, this is this is game theory.",
            "Yeah, this is very very in the space of game theory, but it connects up with this decision-making.",
            "Yeah, it's.",
            "It's game theory."
        ],
        [
            "Why do we need new names for everything is a reasonable question.",
            "I want to say that I'm not solely responsible for misnaming's between different fields.",
            "But I but I, but I hear you, yes.",
            "OK, so that being said, we do something Max game three people use that.",
            "Alright, so here's here's another kind of assumption you can make about.",
            "The opponent.",
            "Could be that the opponents.",
            "What the opponent is doing in this environment is trying to be as helpful as possible to me, so when we get to a new state S prime, how are the actions going to be picked?",
            "Well, I'm going to pick my action to be the best action that I can pick, given that my opponent is going to choose an action.",
            "Which is the best action that my opponent can pick for me and then and then we behave according that sort of imagining.",
            "The other player is a friend.",
            "This gives us a summary operator.",
            "This is also a non expansion.",
            "It's very easy to compute 'cause it really is like just just an MVP.",
            "At this point we're just kind of together choosing an action.",
            "It doesn't necessarily lead to very realistic behavior, yeah?",
            "South Tennessee.",
            "Yeah.",
            "Alright, so another assumption we can make is that the other player is going to do whatever possible to make my life miserable.",
            "And so in this case, it's sort of like some right when we get to a state.",
            "What we're imagining is going to happen is I'm going to take the best action I can, given that my opponent is going to try to take the worst action.",
            "For me.",
            "Alright, so this is a very pessimistic assumption.",
            "Again, the good news, at least mathematically, is that this gives us a summary operator that is in an expansion, so we can use this in a Q learning setting and actually get the values to workout.",
            "It's somewhat easy to compute.",
            "You basically have to solve a linear program each time you do this this update this X here, but but again it's it's nice because it isn't an expansion and things kind of work.",
            "So why is this sort of a problem?"
        ],
        [
            "So again, the good news is.",
            "R. They're not expansions.",
            "Q Learningworks valuation works.",
            "The bad news is that it's sort of a weird set of assumptions to make, so if I assume that the opponent is a friend, this is sort of how imagining things are going to go in this grid.",
            "So this is a grid where.",
            "Princess Peach is trying to get to the pink square Mario, trying to get to the Red Square.",
            "There's walls here, so you might not be able to see so well, so I can't approach from the side.",
            "I have to approach it from the mouth of the.",
            "Square.",
            "Alright, so this is what Mario is imagining.",
            "The friendly agent doing.",
            "That worked out great for us.",
            "If we make that assumption and we actually behave that way but the opponent doesn't behave the way that we were.",
            "Assuming this doesn't work at all, right?",
            "So this ends up being a really bad idea.",
            "On the other hand, so we could say, well, that's because we were too optimistic.",
            "Let's just be pessimistic.",
            "So this is what pessimistic does a reset it back to the beginning.",
            "This is the optimal behavior Now, because Mario is assuming that.",
            "Bowser is trying to hurt Mario, which in this case it can do by just staying in front of the goal.",
            "So the best thing that Mario can do in response to that is just don't move.",
            "Don't waste energy.",
            "So this is this is this is what they do forever now.",
            "Which again might not have been necessary because it could mean that the opponent was making a similar assumption about Mario and now they're both just stuck.",
            "So that's kind of a bummer.",
            "Alright, so the."
        ],
        [
            "He I just want to show one more version of this game.",
            "This is this is sort of what Q learning actually does.",
            "If you put it in this game with the assumption that the I think it's the assumption that the other player is on atomic.",
            "An automaton, right that is just behaving according to some fixed probabilities, which it's not, 'cause they're both actually Q learners, but we ignore that fact and we run it anyway.",
            "Papademetriou has this phrase to use an algorithm against hope, like you know that the algorithm not supposed to work but use it anyway, and it actually does something good.",
            "So this is, this is what it finds.",
            "So Luigi goes into Mario's goal.",
            "Mario passes by and then jumps into the far corner while Luigi gets goes down to start heading for Luigi's goal.",
            "And now.",
            "They are synchronized.",
            "So it's sort of neat thing where why did Mario do that?",
            "It turns out it did that because anything else that it would have done would have left Luigi just sitting in the goal, trying to protect it.",
            "'cause Luigi has no.",
            "Incentive to move out of the goal.",
            "If Mario can get to Mario's goal before Luigi gets to Luigi's goal, so we have to wait until Mario is far enough away from Mario's goal to start moving and it just does that.",
            "So.",
            "I don't know what my point is other than just need.",
            "Again, I don't know what people would do in this.",
            "We did we put people through a similar kind of game and.",
            "People are weird.",
            "That was the high order bit alright."
        ],
        [
            "We're starting to experiment now with using ideas.",
            "This sort of cognitive hierarchy ideas, that sort of say I'm going to imagine that you're the kind of person that imagines that I'm the kind of person that imagines that your random right, and so we go back and forth a number of times until we figure out, yeah, this would be a sensible way for me to behave.",
            "Given those assumptions, we think that it actually works out really well in a bunch of these games, and maybe is a bit more people like 'cause people definitely do this back and forth reflection thing.",
            "But we're in the process of working on that now.",
            "I could I kind of rushed it because I realized that there was a lot on the slide that I didn't want to say so.",
            "So the the cognitive hierarchy idea says.",
            "The Level 0 behavior is, let's say, random.",
            "Level 1 behavior is optimized against level 0 behavior.",
            "Level 2 behavior is optimized against level one behavior and so forth.",
            "Back and forth like that and so.",
            "I think in some of these great games it actually does something sort of sensible, especially if you're optimizing not against just the previous level, but some distribution over the previous levels where the agent will actually.",
            "Sort of make offers like kind of move in such a way that the other agent has a chance to move in such a way that they can both kind of get to their goals, but.",
            "Yeah, but we were not sure, so that's why I didn't want to say that.",
            "Alright, sorry.",
            "Uh.",
            "Anyways, stay tuned.",
            "I'll have results on this soon."
        ],
        [
            "Alright, so The upshot for me of this sort of games pieces.",
            "First of all, we can reuse this this technology for learning in this setting where you're actually learning with multiple agents so that schools good technology if you can reuse it in different ways.",
            "The other part that I think is really interesting is that the reinforcement learning decision making actually is taking place in the context of a culture, right?",
            "So if you're acting actually acting with other agents, you need to have some sense of how those other agents are going to behave, and there's different possible answers.",
            "But we all kind of locked into this system together, and so our learners have to do that as well."
        ],
        [
            "Alright, so we are at the halfway mark.",
            "So do we stretch and stuff?",
            "Let's stretch and stuff."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Susan said this workshop is not a workshop tutorial.",
                    "label": 0
                },
                {
                    "sent": "That was what I was told that I was giving, and in particular the organizer said it should be a tutorial for people who want to get up to speed on computational reinforcement learning, which is a phrase that I hadn't actually heard.",
                    "label": 1
                },
                {
                    "sent": "But I think I think I know what they mean.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So essentially, here's here's I'm going to split you all into four groups.",
                    "label": 0
                },
                {
                    "sent": "There's people who haven't studied natural boom.",
                    "label": 0
                },
                {
                    "sent": "So just hold this people who haven't studied natural reinforcement learning decision making people who have people who have studied artificial reinforcement learning, decision making, and people who haven't.",
                    "label": 0
                },
                {
                    "sent": "So if you've haven't studied natural reinforcement learning decision making, but you have studied artificial stuff like you've ever heard me give a talk.",
                    "label": 0
                },
                {
                    "sent": "For example, you should probably leave and go to Nathaniel.",
                    "label": 0
                },
                {
                    "sent": "Feel free to come back for Davids talk, which I think is going to be really awesome.",
                    "label": 1
                },
                {
                    "sent": "He actually sent me the slides in advance and it looked it looked really exciting.",
                    "label": 0
                },
                {
                    "sent": "So you can come back, but you should just you should go.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe you're not in that category.",
                    "label": 0
                },
                {
                    "sent": "Maybe in one of the other category, so you could be somebody who actually already knows the natural or LDM stuff, and so don't you don't need to hear Nathaniel's talk, so you should go socialize.",
                    "label": 0
                },
                {
                    "sent": "There's all this nice place out there that the weather is beautiful, you know, come back for the focus talks which are going to be really cool, but again.",
                    "label": 0
                },
                {
                    "sent": "You need to go.",
                    "label": 0
                },
                {
                    "sent": "Alright, alright so or you could be in the category of you actually haven't studied either natural or artificial things and I'm not sure why you came.",
                    "label": 0
                },
                {
                    "sent": "It's possible.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not, and I certainly don't mean to be discouraging to anybody who's in that category, but that would seem really like a gutsy thing to do, like I'm just going to go to the conference.",
                    "label": 0
                },
                {
                    "sent": "I don't know what it stands for.",
                    "label": 0
                },
                {
                    "sent": "Alright, and it could be that you have studied natural or LDM, but haven't studied are artificial LDM, and that's who I think I'm talking to, so you're welcome to stay.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's working.",
                    "label": 0
                },
                {
                    "sent": "I can see some people who I know are in this box.",
                    "label": 0
                },
                {
                    "sent": "So OK, so here.",
                    "label": 0
                },
                {
                    "sent": "So one one last try to get rid of you and that is.",
                    "label": 0
                },
                {
                    "sent": "There's going to be some people here who really are in this box, and if I know that, sorry that that really are in this box, right?",
                    "label": 0
                },
                {
                    "sent": "But if I know there's people in this box, I'm going to kind of talk to you and that's going to be really hard for these people, and they're going to make them sad, and they're not going to learn the things that they need to learn.",
                    "label": 0
                },
                {
                    "sent": "So you're really, you're hurting them.",
                    "label": 0
                },
                {
                    "sent": "I can't get rid of you guys, alright.",
                    "label": 0
                },
                {
                    "sent": "I mean truly Nathaniel Nathaniel said he would wait in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So that will be chance for people to come a little bit late.",
                    "label": 0
                },
                {
                    "sent": "OK one I got one.",
                    "label": 0
                },
                {
                    "sent": "Thanks, thanks for leaving the way.",
                    "label": 0
                },
                {
                    "sent": "Which is now sad.",
                    "label": 0
                },
                {
                    "sent": "'cause now it's sort of like, oh we got him but he was doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "We should be appreciative alright?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that being said, I know this is going to be repeat for some of you guys, but this is what I'm going to try to get at I couple days ago I looked at the schedule and saw that this was a 3 hour talk.",
                    "label": 0
                },
                {
                    "sent": "That's a long talk, so so I tried to put together, you know, material that would fill that time and maybe give you some interesting things to hear about and kind of get up to speed with some of the vocabulary and ideas that underlie.",
                    "label": 0
                },
                {
                    "sent": "We're going to computational reinforcement learning that this is a reinforcement learning as known by computer scientists and possibly engineers.",
                    "label": 1
                },
                {
                    "sent": "Possibly operations, research people, people who think of it as, as you know.",
                    "label": 0
                },
                {
                    "sent": "Automatable math and not necessarily a model of something that's alive out in the real world.",
                    "label": 0
                },
                {
                    "sent": "Alright, well done Sir.",
                    "label": 0
                },
                {
                    "sent": "Alright alright.",
                    "label": 0
                },
                {
                    "sent": "So now I have now there's no harm.",
                    "label": 0
                },
                {
                    "sent": "Alright, moving on from that so great.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's the other thing I can do is.",
                    "label": 0
                },
                {
                    "sent": "I could just be really boring in the beginning and you're like, oh, it's that thing that I've heard 10,000 times and taught twice.",
                    "label": 0
                },
                {
                    "sent": "You know, yeah, that's what it's going to be.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seriously, alright, so so here's gonna break it down.",
                    "label": 0
                },
                {
                    "sent": "I'm going to 1st kind of talk about what I think computational RL is and kind of the mindset that underlies it.",
                    "label": 0
                },
                {
                    "sent": "Then I'll get into some of the nitty gritty of planning and learning in Markov models.",
                    "label": 1
                },
                {
                    "sent": "I'll shift over to a topic that probably shouldn't be in here, but I like it a lot, so I'm going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "Efficient reinforcement learning, and then some kind of a grab bag of other other topics that I think may be useful or particularly interesting, or some things that I think we need a lot of input on, and I want to make a case for why we should be studying them more.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what is compute?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "National RL so just to set this up.",
                    "label": 0
                },
                {
                    "sent": "Those of us who are in computational RL think of reinforcement learning is being a sub field of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Alright, so what's so?",
                    "label": 0
                },
                {
                    "sent": "What's machine learning?",
                    "label": 0
                },
                {
                    "sent": "Well machine learning I say to a first approximation, is this that we're trying to build programs, learners that take training sets and turn them into decision rules, and in particular.",
                    "label": 0
                },
                {
                    "sent": "The way that we do this is, we think, about in this in the most commonly used setting in machine learning is supervised learning setting, and in that case the training data has this form of inputs get mapped to output.",
                    "label": 0
                },
                {
                    "sent": "So for example, just to be concrete.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine that we are a web service company or something like that, and we're trying to.",
                    "label": 0
                },
                {
                    "sent": "We're trying to make a program that's going to decide which version of a webpage to show to people who come and visit our site.",
                    "label": 0
                },
                {
                    "sent": "So we need a way of mapping people to pages or people to styles of pages.",
                    "label": 0
                },
                {
                    "sent": "People we can't really map actual people because we don't have them, but we have data about those people.",
                    "label": 0
                },
                {
                    "sent": "So maybe we have some kind of feature ized representation of a web user.",
                    "label": 1
                },
                {
                    "sent": "We have, for example, has this person visited the site before?",
                    "label": 0
                },
                {
                    "sent": "How many times have they make any purchases on our site?",
                    "label": 0
                },
                {
                    "sent": "Do we have any demographic information you know, male, female, US, not US, Canada, not Canada.",
                    "label": 0
                },
                {
                    "sent": "Have they actually registered on our site?",
                    "label": 0
                },
                {
                    "sent": "Is this is a person who actually has been here before and signed up?",
                    "label": 0
                },
                {
                    "sent": "These are all sorts of facts that we could know and as a function of that, what we'd like to do is to choose which webpage decided that user.",
                    "label": 0
                },
                {
                    "sent": "So we might want to show that user an advertisement of a certain kind.",
                    "label": 1
                },
                {
                    "sent": "Or maybe the FAQ page, or maybe a login page or something like that.",
                    "label": 0
                },
                {
                    "sent": "Or here's where you left off last time pages.",
                    "label": 0
                },
                {
                    "sent": "Lots of choices and the right choice depends on which kind of user we're looking at.",
                    "label": 0
                },
                {
                    "sent": "So in supervised learning, what we do is we collect in advance of big.",
                    "label": 0
                },
                {
                    "sent": "Training set that says this kind of user should have been given this or user.",
                    "label": 0
                },
                {
                    "sent": "Here's an example user or.",
                    "label": 0
                },
                {
                    "sent": "Here's an actual user.",
                    "label": 0
                },
                {
                    "sent": "This is what page we should show that user.",
                    "label": 0
                },
                {
                    "sent": "Here's another one written as a feature vector.",
                    "label": 0
                },
                {
                    "sent": "This is what we should show that one and so forth.",
                    "label": 0
                },
                {
                    "sent": "We could have thousands and thousands millions upon millions of these potentially.",
                    "label": 0
                },
                {
                    "sent": "We're going to feed it to our learner and the learner is going to spit out a decision rule and the goal of this decision rule is.",
                    "label": 0
                },
                {
                    "sent": "To try to act like the training set.",
                    "label": 1
                },
                {
                    "sent": "OK, so to try to do think map X is today's, so it's kind of like the way that the training set did it.",
                    "label": 0
                },
                {
                    "sent": "However, that happened to be.",
                    "label": 0
                },
                {
                    "sent": "You know ideally it should make the same kind of decisions as we saw in the training set, but but it may not.",
                    "label": 0
                },
                {
                    "sent": "It may be that even has contradictory information in the training set, so it has to make some kind of choice, but ultimately it's trying to.",
                    "label": 0
                },
                {
                    "sent": "Minimize error right?",
                    "label": 0
                },
                {
                    "sent": "So minimize the mismatches between the inputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's supervised.",
                    "label": 0
                },
                {
                    "sent": "Learning super useful, super applied.",
                    "label": 0
                },
                {
                    "sent": "It's all over the place.",
                    "label": 0
                },
                {
                    "sent": "People doing all kinds of amazing things with it.",
                    "label": 0
                },
                {
                    "sent": "But this is just one kind of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Here's another kind of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So one challenge of this is that you need this training set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another kind of learning is unsupervised learning, so the idea and unsupervised learning is there.",
                    "label": 0
                },
                {
                    "sent": "Examples don't don't tell us anything anymore, they're just examples like there's Joe and there's Fred and there Sarah, there's all these different people, and that's all we know and what it needs to do is come up with a decision rule that Maps people too well.",
                    "label": 1
                },
                {
                    "sent": "What I mean, it doesn't really have an idea of what it should map them too, so it's just kind of cluster them together.",
                    "label": 0
                },
                {
                    "sent": "It should just group together users that are similar to each other into similar classes.",
                    "label": 0
                },
                {
                    "sent": "So one way this could work is again, we have features representations of the users.",
                    "label": 0
                },
                {
                    "sent": "And we're going to output for each user.",
                    "label": 0
                },
                {
                    "sent": "Is their class ABC&D with the goal that.",
                    "label": 0
                },
                {
                    "sent": "Individuals users X is that are similar to each other, are going to get signed into the same cluster and things that are different people that are different are going to be in different clusters.",
                    "label": 0
                },
                {
                    "sent": "If it was just the first would be really easy, right?",
                    "label": 0
                },
                {
                    "sent": "So how do you make it so that any two people that are similar or in the same class assign them all a?",
                    "label": 1
                },
                {
                    "sent": "Done, so that's really easy with what makes it harder is we also want to separate the ones that are not so similar.",
                    "label": 0
                },
                {
                    "sent": "So we want to use these other classes as well.",
                    "label": 1
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Challenges of unsupervised learning is that it'll do something, but it's not necessarily relevant to whatever it is you're trying to do, right?",
                    "label": 0
                },
                {
                    "sent": "So my group people together by I don't know things that are not useful for deciding what web web page to show them in our shoe size.",
                    "label": 0
                },
                {
                    "sent": "Maybe like something that just orthogonal or irrelevant to the task because it doesn't have any information about the task and its training set.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now.",
                    "label": 0
                },
                {
                    "sent": "The draw.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reinforcement learning, so this is when this is when I get really excited, but but maybe we could all be kind of excited.",
                    "label": 0
                },
                {
                    "sent": "So so in reinforcement learning, we have a different kind of experience.",
                    "label": 0
                },
                {
                    "sent": "We have experienced that says, well, we saw this thing and we did this thing.",
                    "label": 0
                },
                {
                    "sent": "And this is how good it was.",
                    "label": 0
                },
                {
                    "sent": "We gotta score for that.",
                    "label": 1
                },
                {
                    "sent": "Then we saw this other thing and did this other thing and we got a score for that too.",
                    "label": 0
                },
                {
                    "sent": "So we had this whole big chain of experience.",
                    "label": 0
                },
                {
                    "sent": "And Sue the thing that's new here is this how good it was.",
                    "label": 1
                },
                {
                    "sent": "So we have some kind of evaluative feedback, some kind of numerical value.",
                    "label": 0
                },
                {
                    "sent": "So it could be like how many things did they click on as a result of being shown that page.",
                    "label": 0
                },
                {
                    "sent": "It could be how much time did they spend on that page?",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "Did they ultimately make make a purchase later in that visit?",
                    "label": 0
                },
                {
                    "sent": "But we get some kind of a measurable outcome.",
                    "label": 0
                },
                {
                    "sent": "Something that we can we can think of as designers as being a positive outcome or a negative outcome, and our goal is to just construct a decision rule that has maximum expected value so it tends to produce the things that we said were good and it tends to not produce the things that we said were bad.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is a really powerful idea.",
                    "label": 0
                },
                {
                    "sent": "It's so much cooler than the other two, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't have to tell it what to do.",
                    "label": 0
                },
                {
                    "sent": "You just have to be able to score it.",
                    "label": 0
                },
                {
                    "sent": "You can give it information unlike the unsupervised case where it's just in the wilderness, so this is really powerful, neat, but of course you know all is not.",
                    "label": 1
                },
                {
                    "sent": "Sunshine and roses.",
                    "label": 0
                },
                {
                    "sent": "This is actually very weak feedback.",
                    "label": 0
                },
                {
                    "sent": "These kinds of scores are very weak.",
                    "label": 0
                },
                {
                    "sent": "That doesn't really tell the learner what it should be doing in particular.",
                    "label": 0
                },
                {
                    "sent": "In the vanilla form, we don't even know.",
                    "label": 0
                },
                {
                    "sent": "Whether that's a good score or a bad score, right?",
                    "label": 0
                },
                {
                    "sent": "Like I did this thing and I got a 7.",
                    "label": 0
                },
                {
                    "sent": "OK, like I don't know what I should do with that information, right?",
                    "label": 0
                },
                {
                    "sent": "So you can only start to make sense of it in context where you start to realize.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, but you know this.",
                    "label": 0
                },
                {
                    "sent": "Other time I got something really similar and did this other thing and I got much higher score so that 7 maybe is not so good 'cause it was seven out of 100.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's that's kind of the essence of what we think of this as a computational problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just to kind of tie these threads together, this is this is a figure from I got to make a paper for nature.",
                    "label": 0
                },
                {
                    "sent": "I'm very excited about that so I drew it figure I sent it to nature and then they came back with this which looks a lot like mine except the font is smaller.",
                    "label": 0
                },
                {
                    "sent": "And this they did a much better job of lining up this circle part right here so, But anyway, so this is like a professionally rendered picture, so you should.",
                    "label": 0
                },
                {
                    "sent": "We should all appreciate that.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of relating all these different kinds of feedback to each other that we get.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so I didn't say this, but one of the really important axes for understanding the difference between these different kinds of machine learning are the kinds of feedback that are available.",
                    "label": 0
                },
                {
                    "sent": "And I want to talk about three different dimensions of feedback.",
                    "label": 0
                },
                {
                    "sent": "There's supervised or not supervised so or supervisors to the valuative so supervised.",
                    "label": 0
                },
                {
                    "sent": "The learner is told what answer to give.",
                    "label": 0
                },
                {
                    "sent": "You know when you see this, do that.",
                    "label": 0
                },
                {
                    "sent": "And it's supposed to basically mimic it.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a good kind of feedback in that it actually is.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of supervision there for the learner.",
                    "label": 0
                },
                {
                    "sent": "But it's you know, restrictive and you have to have that extra information.",
                    "label": 0
                },
                {
                    "sent": "Their value to feedback is sort of weaker.",
                    "label": 0
                },
                {
                    "sent": "It's less information for the learner, but may be easier to produce, maybe easier to produce something that kind of automatically scores.",
                    "label": 0
                },
                {
                    "sent": "You know, I'll, I'll know it when I see it.",
                    "label": 0
                },
                {
                    "sent": "I don't know what you should do, but I'll know it when I see it.",
                    "label": 0
                },
                {
                    "sent": "So that's one distinction, sampled versus evaluative, and all the things that are.",
                    "label": 0
                },
                {
                    "sent": "Sampled, oh, that's that's in this.",
                    "label": 0
                },
                {
                    "sent": "Sorry no, that's exhausted.",
                    "label": 0
                },
                {
                    "sent": "What did I say?",
                    "label": 0
                },
                {
                    "sent": "Evaluative versus supervised is too many S is that's the problem.",
                    "label": 0
                },
                {
                    "sent": "I don't know who came up with this figure, so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so evaluative versus supervised so supervised here is just you know supervised machine learning.",
                    "label": 0
                },
                {
                    "sent": "Evaluative is all these kinds of related reinforcement learning kinds of things.",
                    "label": 0
                },
                {
                    "sent": "Within evaluative feedback, we can make some other distinctions so one is sequential versus one shot so.",
                    "label": 0
                },
                {
                    "sent": "If we were given sequential feedback, it means the the reward value that we got isn't necessarily relevant to the action that was just taken, or the decision that was just made.",
                    "label": 0
                },
                {
                    "sent": "It may actually be you got high reward because you did something really clever.",
                    "label": 0
                },
                {
                    "sent": "Five steps ago and you have to.",
                    "label": 0
                },
                {
                    "sent": "Somehow the learner has to somehow connect those things up versus A1 shot thing which just says.",
                    "label": 0
                },
                {
                    "sent": "Here's the input you give me the output.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you how good that one thing was, where each of them are sort of independent of the others.",
                    "label": 0
                },
                {
                    "sent": "So the supervised learning is in this one shot zone.",
                    "label": 0
                },
                {
                    "sent": "Yeah alright.",
                    "label": 0
                },
                {
                    "sent": "But we do have something in the.",
                    "label": 0
                },
                {
                    "sent": "One shot but evaluative zone.",
                    "label": 0
                },
                {
                    "sent": "So the I will talk a little bit about bandits, bandits is.",
                    "label": 0
                },
                {
                    "sent": "It is what we is.",
                    "label": 0
                },
                {
                    "sent": "The name that we use for particular model of decision making, where you each shot you get to make a decision you put.",
                    "label": 0
                },
                {
                    "sent": "We think of it is pulling an arm.",
                    "label": 0
                },
                {
                    "sent": "We get to make some decision.",
                    "label": 0
                },
                {
                    "sent": "We get some feedback for that.",
                    "label": 0
                },
                {
                    "sent": "And then we repeat that over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "And so each decision that we make has an immediate payoff.",
                    "label": 0
                },
                {
                    "sent": "So it's a one shot thing.",
                    "label": 0
                },
                {
                    "sent": "But it's a value to feedback.",
                    "label": 0
                },
                {
                    "sent": "We're not actually told what we should have done, which arm we should have pulled, which action we should have taken.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So alright, the last thing I want to talk about is sampled versus exhaustive, so the idea of exhaustive is I'll show you all the X is that out.",
                    "label": 0
                },
                {
                    "sent": "That could possibly matter.",
                    "label": 0
                },
                {
                    "sent": "You don't generalize it all, just show you all of them exhaustively versus sampled, which is I'm just going to show you some XSS and you're going to fill in the blanks yourself.",
                    "label": 0
                },
                {
                    "sent": "So supervised machine learning is usually in this well, it's sampled, but it's supervised zone, so in particular.",
                    "label": 0
                },
                {
                    "sent": "If it were exhaustive and supervised, that would be really boring, right?",
                    "label": 0
                },
                {
                    "sent": "So exhausted in supervised means.",
                    "label": 0
                },
                {
                    "sent": "When I say when I say jump, you say how high when I say it's Tuesday, you say I'm thirsty, right?",
                    "label": 0
                },
                {
                    "sent": "So I give you a list of what your responses should be for every possible input, and then later I give you one of those inputs and you should give the associated output.",
                    "label": 0
                },
                {
                    "sent": "It sounds stupid, right?",
                    "label": 0
                },
                {
                    "sent": "So there isn't a box for that, so supervised machine learning is more interesting than that, because it actually deals with this idea of kind of reading between the lines, generalizing between different examples.",
                    "label": 0
                },
                {
                    "sent": "But bandit problems actually live in this exhaustive space, so you actually get to try all the different choices.",
                    "label": 0
                },
                {
                    "sent": "What's difficult about it is you don't get the feedback.",
                    "label": 0
                },
                {
                    "sent": "The valuations are very noisy, so you actually have to generalize.",
                    "label": 0
                },
                {
                    "sent": "Not really generalize, but you have to use statistical methods to try to figure out well how good is this, really.",
                    "label": 0
                },
                {
                    "sent": "Some things were good.",
                    "label": 0
                },
                {
                    "sent": "Some things were not good.",
                    "label": 0
                },
                {
                    "sent": "We have to kind of maybe average them together.",
                    "label": 0
                },
                {
                    "sent": "Alright, so with that said, we got these two bubbles.",
                    "label": 0
                },
                {
                    "sent": "Finally, this bubble tabular reinforcement learning is the idea that we're going to worry about the sequential aspect of things, so we're going to be acting in a world, and the actions that we take may not prove themselves relevant until a bit later.",
                    "label": 0
                },
                {
                    "sent": "So we have to worry about that sequential aspect, but it's exhaustive.",
                    "label": 0
                },
                {
                    "sent": "We actually do get to see every possible X every possible state in this case.",
                    "label": 1
                },
                {
                    "sent": "And but it is evaluative, so we get to find out, was it good or bad and so things that fall into this or like playing Tic tac toe right where you can see every possible Tic Tac toe board.",
                    "label": 0
                },
                {
                    "sent": "There's only a couple of 1000 of them and you can find out.",
                    "label": 0
                },
                {
                    "sent": "Well, I've had this tic tac toe board and I took this action and then ultimately I won the game or I lost the game and so I had that evaluative feedback that I can put back to try to figure out what I should actually have done.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk about this.",
                    "label": 0
                },
                {
                    "sent": "A bunch.",
                    "label": 0
                },
                {
                    "sent": "Bandit.",
                    "label": 0
                },
                {
                    "sent": "I'll mention briefly, contextual bandits, contextual bandits, or the idea where, well, it's it's like that first problem that I showed you where you have features about the web users and you want to find out what the right thing to do for that web user is.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to only give you samples.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you some of the kinds of web users that are out there and you have to generalize from one to another, but the feedback is immediate.",
                    "label": 0
                },
                {
                    "sent": "This there's no sequential aspect to it.",
                    "label": 0
                },
                {
                    "sent": "And you know, in reinforcement learning kind of sits in the middle of this mess, right?",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is, well, it's not going to be supervised.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be exhaustive, and it's not going to be one shot.",
                    "label": 0
                },
                {
                    "sent": "It's going to be sequential.",
                    "label": 0
                },
                {
                    "sent": "Sampled and valuative, and yet even with even in the face of this extremely weak feedback, we have to try to make some good decisions.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would make some sense, especially because this is the first time we're doing one of these tutorial things and I don't know if.",
                    "label": 0
                },
                {
                    "sent": "I mean, I tried to do that set up in the beginning, but I don't know that work.",
                    "label": 0
                },
                {
                    "sent": "So I don't know who I'm actually talking to or what you're actually concerned about.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so questions would be great.",
                    "label": 0
                },
                {
                    "sent": "Do you want to ask one or do you want to just let people know that that's?",
                    "label": 0
                },
                {
                    "sent": "It was hard for me to imagine a real life problem where I might get to see all the exs and then just figure out what to do.",
                    "label": 0
                },
                {
                    "sent": "Sure so so.",
                    "label": 0
                },
                {
                    "sent": "There's two reasons for studying this exhaustive case in the reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "One is that the theory goes through really nicely and we can understand it, so it's kind of useful to study those to figure out how things could work in the best of all possible worlds.",
                    "label": 0
                },
                {
                    "sent": "Turn problems into those kinds of problems by, for example, either unsupervised clustering or by hand clustering things together.",
                    "label": 0
                },
                {
                    "sent": "So there's this big complex continuous space, but we're going to decide there's this kind of thing and this kind of thing in this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "An never revisit those decisions, and so that gives us an ability to actually evaluate things in an exhaustive way.",
                    "label": 0
                },
                {
                    "sent": "Alright, so one of my plans here is that once I get through the kind of the beginning part.",
                    "label": 0
                },
                {
                    "sent": "If there are areas I'm going to, I'm going to say some things about some things and not, say, some things about other things.",
                    "label": 0
                },
                {
                    "sent": "I guess you probably won't tell me if I'm saying too much about something that you don't want to hear about, but you can't.",
                    "label": 0
                },
                {
                    "sent": "You should probably tell me if I'm saying too little about something you do want to hear about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so so so why evaluative samples sequential?",
                    "label": 0
                },
                {
                    "sent": "So this is this is a case that I really like, this is.",
                    "label": 0
                },
                {
                    "sent": "I spend a lot of time watching little kids crawl because I feel like it's.",
                    "label": 0
                },
                {
                    "sent": "It's a great example of a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "The full reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of classic crawling.",
                    "label": 0
                },
                {
                    "sent": "This is my nephew Reese getting from one part of the room to another and this is what you think of is crawling and so if I tell you yeah, crawling is this sort of really interesting computational problem.",
                    "label": 0
                },
                {
                    "sent": "You might say not really, but wait.",
                    "label": 0
                },
                {
                    "sent": "Different kids come up with different answers to the question of how am I going to get from point A to point B?",
                    "label": 0
                },
                {
                    "sent": "So friend of mine from college, sorry.",
                    "label": 0
                },
                {
                    "sent": "His his kid came up with that sort of I don't know bear crawl where he didn't put his knees on the ground.",
                    "label": 0
                },
                {
                    "sent": "This is my my nephew Sidney.",
                    "label": 0
                },
                {
                    "sent": "I don't know what she was thinking.",
                    "label": 0
                },
                {
                    "sent": "Right, but she came with this sort of very weird, asymmetrical.",
                    "label": 0
                },
                {
                    "sent": "You know soldiery crawl thing, my nephew Cameron, but the symmetry was important.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of like the butterfly stroke in swimming, except on linoleum.",
                    "label": 0
                },
                {
                    "sent": "This is the brother of the one who did the bear crawl.",
                    "label": 0
                },
                {
                    "sent": "He actually had had surgery very early on in his life, and it was difficult for him to use his hands in the standard crawling motion.",
                    "label": 0
                },
                {
                    "sent": "He just came up with something else.",
                    "label": 0
                },
                {
                    "sent": "He just, would, you know, scoot along on his bottom.",
                    "label": 0
                },
                {
                    "sent": "Just paddling with his other hand.",
                    "label": 0
                },
                {
                    "sent": "It's really, really good solution in the sense that he had one hand free to pick things up and manipulate them.",
                    "label": 0
                },
                {
                    "sent": "The other hand is, you know, is busy and he doesn't have to pay for the damage to the clothes.",
                    "label": 1
                },
                {
                    "sent": "So it's from his perspective.",
                    "label": 0
                },
                {
                    "sent": "This is a really good answer.",
                    "label": 0
                },
                {
                    "sent": "And this is Charles Isbell's daughter, Joanie.",
                    "label": 0
                },
                {
                    "sent": "He'll be here later, but he's not here now to make me not play this, she came up with a solution that was maybe a local minimum.",
                    "label": 0
                },
                {
                    "sent": "So she's trying to get to the ball.",
                    "label": 0
                },
                {
                    "sent": "But Alas.",
                    "label": 0
                },
                {
                    "sent": "When she was in the right place, she was in the wrong orientation again.",
                    "label": 0
                },
                {
                    "sent": "It's it's a clever thing because kids do start off doing this rolling action, and so if you could just translate that into oh, I can use this to get around.",
                    "label": 0
                },
                {
                    "sent": "That's that's a really good answer, except it has flaws.",
                    "label": 0
                },
                {
                    "sent": "She's gotten past this, so we need not worry about her, but but OK.",
                    "label": 0
                },
                {
                    "sent": "So what's my point in this, other than I can show you videos of people who I know and that is that that in each case here the feedback that they're getting?",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "These kids are not watching other people crawl and saying, Oh yeah, that's a really good idea.",
                    "label": 0
                },
                {
                    "sent": "I'll do this right.",
                    "label": 0
                },
                {
                    "sent": "They're just getting a value to feedback.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to move over there and it either worked or didn't work.",
                    "label": 0
                },
                {
                    "sent": "They're getting sampled feedback.",
                    "label": 0
                },
                {
                    "sent": "They haven't lived in all possible situations.",
                    "label": 0
                },
                {
                    "sent": "So as Susan pointed out, it's just rare that you get an exhaustive listing of everything that could ever happen, so they're clearly learning from special cases and generalizing to other cases.",
                    "label": 0
                },
                {
                    "sent": "And it's definitely very sequential, right?",
                    "label": 0
                },
                {
                    "sent": "So they're crawling from point A to point B and almost all these videos you can see it really well in jonis.",
                    "label": 0
                },
                {
                    "sent": "They're all trying to get to something.",
                    "label": 0
                },
                {
                    "sent": "There's an actual thing that they're trying to get to, and they know ultimately whether they got there or not, but there's a lot of lot of wiggling around that happens before that, and they have to somehow use the information of.",
                    "label": 0
                },
                {
                    "sent": "I finally got to the ball or not too.",
                    "label": 0
                },
                {
                    "sent": "Do credit assignment to all the different wiggles that happened before that right?",
                    "label": 0
                },
                {
                    "sent": "So this is actually I want to claim a really hard and interesting problem.",
                    "label": 0
                },
                {
                    "sent": "I don't think walking is quite as interesting because I think there's a lot of evolutionary structure that makes us all walk.",
                    "label": 0
                },
                {
                    "sent": "Using at least a similar structure, I mean obviously some people more bouncy and some people you know there's lots of different kinds of walks that you can do, but they all involve you know left foot, right foot effort like the contact pattern is pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "But these were very, very different, like structurally very different things that they were coming up with.",
                    "label": 0
                },
                {
                    "sent": "So I think it's I think it's a really interesting case to think about.",
                    "label": 0
                },
                {
                    "sent": "And again, it felt very much like it's driven by some kind of motivation.",
                    "label": 0
                },
                {
                    "sent": "I want to get to an object.",
                    "label": 0
                },
                {
                    "sent": "How do I do that?",
                    "label": 0
                },
                {
                    "sent": "How do I wiggle so that my dreams come true?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here's a here's a robotic task that has some of the some of the features in common with that though.",
                    "label": 0
                },
                {
                    "sent": "It's quite lame actually.",
                    "label": 0
                },
                {
                    "sent": "When you get right down to it, so this is an AIBO robot that we programmed to try to figure out which way to turn in the lab so they could see the pink ball.",
                    "label": 0
                },
                {
                    "sent": "Which I guess is kind of similar to the Joni case.",
                    "label": 0
                },
                {
                    "sent": "She was also trying to get a wall, so the robot you know gets happy when it sees the ball, and so it's trying to figure out how to relate its camera input to which direction it should move to ultimately see the ball, and so it has all these features again.",
                    "label": 0
                },
                {
                    "sent": "So it's evaluative in that we're not telling it which which way to turn left or right.",
                    "label": 1
                },
                {
                    "sent": "We're just telling it, yeah, you saw the bar, or you haven't seen the ball yet.",
                    "label": 0
                },
                {
                    "sent": "And it's it's sampled because the input that the robot is actually using here is a color histogram coming from its nose camera.",
                    "label": 0
                },
                {
                    "sent": "So it's just looking out in the lab and it sees certain kinds of color patterns.",
                    "label": 0
                },
                {
                    "sent": "And it probably never sees the same exact color pattern twice, so it has to be able to generalize between these different color patterns and its sequential in that the robot is in a position it takes an action, and there's no immediate feedback for that, necessarily.",
                    "label": 0
                },
                {
                    "sent": "Another action, and then another action, and then maybe eventually it sees the ball.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's feedback, but it somehow was all set up by the actions that came before it.",
                    "label": 0
                },
                {
                    "sent": "So again, not nearly as cool as.",
                    "label": 0
                },
                {
                    "sent": "Little kids crawling, but again, has these three main features that we have to figure out how to deal with computationally.",
                    "label": 0
                },
                {
                    "sent": "If we want to understand and create this kind of behavior.",
                    "label": 0
                },
                {
                    "sent": "Hey good at the moment.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is something that I call the reinforcement learning hypothesis, and again this comes from the computational perspective.",
                    "label": 0
                },
                {
                    "sent": "The idea is that I'm interested in making smart critters like programs that do things that are intelligent looking or intelligent.",
                    "label": 0
                },
                {
                    "sent": "Equivalent or something like that, so I'm going to say what motivates me is the idea that I believe intelligent behavior arises from the action of an individual seeking to maximize its received reward signals in a complex and changing world.",
                    "label": 1
                },
                {
                    "sent": "So so essentially, the reinforcement learning hypothesis is you want to make AI or something like it.",
                    "label": 0
                },
                {
                    "sent": "You have to do.",
                    "label": 0
                },
                {
                    "sent": "You have to do reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is the way that complex agents in complex worlds can actually achieve their ends.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if you buy into that, you don't have to buy into it, but it'll probably be really boring for all of us if you don't, there's really 2 main things you have to you have to think about computationally to make this fly.",
                    "label": 1
                },
                {
                    "sent": "One is you have to figure out where reward signals come from.",
                    "label": 0
                },
                {
                    "sent": "That's actually kind of important and not very well studied.",
                    "label": 1
                },
                {
                    "sent": "And then you have to develop algorithms that actually search the space of behaviors to maximize the reward functions.",
                    "label": 0
                },
                {
                    "sent": "So just trying to make this a little more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crete if you haven't seen this picture before, this comes from.",
                    "label": 0
                },
                {
                    "sent": "Rich in Andy's book.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is kind of I don't know the reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "API in a nutshell, it says that the world consists of an agent and an environment and they have the following conversation with each other.",
                    "label": 0
                },
                {
                    "sent": "The agent chooses an action or makes a decision of some kind and it gets transmitted to the environment.",
                    "label": 0
                },
                {
                    "sent": "The environment somehow processes that and spits out state what the what the new situation is.",
                    "label": 0
                },
                {
                    "sent": "The new values is like what I was calling X before.",
                    "label": 0
                },
                {
                    "sent": "And that's going to get transmitted to the agent along with some kind of evaluative feedback.",
                    "label": 0
                },
                {
                    "sent": "So the environment is generating what state is going to come next, and the value of feedback.",
                    "label": 0
                },
                {
                    "sent": "And the agent gets to choose an action and that gets fed back in and we just go around and around and around.",
                    "label": 0
                },
                {
                    "sent": "This is in some sense a description of the system, but it also gives us a way of breaking it down into a computational problem or multiple computational problems.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to say.",
                    "label": 0
                },
                {
                    "sent": "If you're an agent builder, this is what the world looks like to you as an agent.",
                    "label": 1
                },
                {
                    "sent": "So the agent is seeing state seeing what's the current situation, which way by facing what is or what?",
                    "label": 0
                },
                {
                    "sent": "What information by getting from my nose camera.",
                    "label": 0
                },
                {
                    "sent": "And the reward signal and it's trying to turn that into actions, and in particular.",
                    "label": 0
                },
                {
                    "sent": "It's trying to figure out how to act to maximize its cumulative.",
                    "label": 1
                },
                {
                    "sent": "Let's say discounted expected reward.",
                    "label": 1
                },
                {
                    "sent": "So the amount of the total of the rewards that it gets averaged.",
                    "label": 0
                },
                {
                    "sent": "And future discounted usually, and I feel like that's a question that Susan might want to ask me, but I don't want to answer it, which is why discounting so discounting says that we're going to get full value for award that we get now we're going to get like .9 of value for rewards.",
                    "label": 0
                },
                {
                    "sent": "We get one step from now we get .9 squared value of things that happened, 2 steps from now, and it fades off like that.",
                    "label": 0
                },
                {
                    "sent": "At a high level, this is a very reasonable thing, right?",
                    "label": 0
                },
                {
                    "sent": "So like you know, give me the money now versus you know, give me the money.",
                    "label": 0
                },
                {
                    "sent": "In two years, I'd really rather have it now.",
                    "label": 0
                },
                {
                    "sent": "It sort of gets less useful to me as time goes on.",
                    "label": 0
                },
                {
                    "sent": "Why does it have this exact form?",
                    "label": 0
                },
                {
                    "sent": "Mostly I've heard people explain this mathematically, which I find not that satisfying.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, it's sort of because that's what we do, which is terrible answer, just terrible, and no one should invite me back to talk again because I said it, but.",
                    "label": 0
                },
                {
                    "sent": "The fact the matter is this is the model that we're going to stick with for the purposes of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "There are people who are trying to question these assumptions and think about other kinds of.",
                    "label": 0
                },
                {
                    "sent": "Reward metrics that you could try to optimize, but this is this is, I feel like the most straightforward one to understand and to connect to things and to work with, so we're just going to go with that.",
                    "label": 0
                },
                {
                    "sent": "And so OK.",
                    "label": 0
                },
                {
                    "sent": "So to the extent that the agent is trying to do is figure out how to map state to action so that reward is maximized.",
                    "label": 0
                },
                {
                    "sent": "It's really an optimization problem of all possible ways of taking what I'm seeing and turning it into what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "Which one has the highest reward?",
                    "label": 0
                },
                {
                    "sent": "So computationally it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It's not a simple problem, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's a very difficult problem, but it's a very simple to state problem, right?",
                    "label": 0
                },
                {
                    "sent": "Like a possible behaviors, find the best one.",
                    "label": 0
                },
                {
                    "sent": "There's not very many behaviors, then this is not hard at all.",
                    "label": 0
                },
                {
                    "sent": "You just try them all.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but that's the world from the agent's perspective.",
                    "label": 0
                },
                {
                    "sent": "I also want to highlight what the world looks like from the reward functions perspective, and Andy and riches figure in the book didn't didn't have this as a separate entity in the diagram, but I know in some of their papers they have actually called that out as a separate piece.",
                    "label": 0
                },
                {
                    "sent": "So what's the reward function?",
                    "label": 0
                },
                {
                    "sent": "The reward function is something that's taking.",
                    "label": 0
                },
                {
                    "sent": "The actions that it's seeing from the agent.",
                    "label": 0
                },
                {
                    "sent": "And turning them into evaluations, it's actually deciding was that good or not?",
                    "label": 0
                },
                {
                    "sent": "How good was it?",
                    "label": 0
                },
                {
                    "sent": "Is it 7 good?",
                    "label": 0
                },
                {
                    "sent": "Was it 11 good?",
                    "label": 0
                },
                {
                    "sent": "You know the reward function has to decide that.",
                    "label": 0
                },
                {
                    "sent": "From the agent's perspective, the reward function is part of the environment, right?",
                    "label": 0
                },
                {
                    "sent": "Something out there is just telling it what's good and bad, and it's slavishly trying to follow that.",
                    "label": 0
                },
                {
                    "sent": "From the environment perspective, the actual physical world, if you will.",
                    "label": 0
                },
                {
                    "sent": "The reward function is part of the agent, right?",
                    "label": 0
                },
                {
                    "sent": "'cause the environment doesn't really care about anything.",
                    "label": 0
                },
                {
                    "sent": "The environment just is.",
                    "label": 0
                },
                {
                    "sent": "So the reward function is kind of this piece that you can think of is living inside the agent's head that is motivating the agent to do whatever it is that it does.",
                    "label": 0
                },
                {
                    "sent": "And So what does the world look like to the reward function?",
                    "label": 0
                },
                {
                    "sent": "It's trying to answer the question, how do I deliver reward so that the agent adopts desirable behavior?",
                    "label": 1
                },
                {
                    "sent": "So in a sense, the reward function encapsulates some sort of desire about how should critters behave.",
                    "label": 0
                },
                {
                    "sent": "And it should be chosen in such a way that actually maximizing that really does bring about the desired behavior.",
                    "label": 0
                },
                {
                    "sent": "So from an economic standpoint, this is kind of mechanism design, so this is like how do you make an economy so that when things flow around, goodness happens.",
                    "label": 0
                },
                {
                    "sent": "Right and so and again, for the most part, the.",
                    "label": 0
                },
                {
                    "sent": "Computational reinforcement learning has focused on the agent design.",
                    "label": 0
                },
                {
                    "sent": "We just take this as a given, but it really worth asking the question.",
                    "label": 0
                },
                {
                    "sent": "Where does this come from and what are the implications about choosing one particular reward function as opposed to some other reward function?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so just one more word about reward before I start diving into algorithms and stuff.",
                    "label": 0
                },
                {
                    "sent": "So rewards are kind of expressive as a way of telling creators what to do.",
                    "label": 0
                },
                {
                    "sent": "I'm on the fence about this right now, actually, but I'm going to say that it's somewhat expressive.",
                    "label": 0
                },
                {
                    "sent": "There's certainly a lot of possible numbers, so it's expressive in that sense you can.",
                    "label": 0
                },
                {
                    "sent": "You can, by assigning a high reward to something you could encourage an agent to try to reach a goal state.",
                    "label": 1
                },
                {
                    "sent": "So this is this is little soccer gridworld and the agent A is supposed to bring the ball into the goal, and so it can do anything at once.",
                    "label": 0
                },
                {
                    "sent": "But if it brings the ball into the goal, it gets a plus one.",
                    "label": 0
                },
                {
                    "sent": "So this is by virtue of putting this reward into the environment.",
                    "label": 0
                },
                {
                    "sent": "We're actually defining the agents task.",
                    "label": 0
                },
                {
                    "sent": "Go to the goal, put the.",
                    "label": 0
                },
                {
                    "sent": "Put the ball in the goal.",
                    "label": 0
                },
                {
                    "sent": "But we can also say things like avoid a failure state, so we can have.",
                    "label": 1
                },
                {
                    "sent": "This is a cart pole, so this is a part that can ride back and forth and it's balancing this pole on it an we can for example put a negative one on both sides of this track here so that if it hits it or if the pole falls onto it then Ouch, but otherwise everything is cool.",
                    "label": 0
                },
                {
                    "sent": "So by by having again zero awards everywhere except for in these bad states, it's actually saying to the agent.",
                    "label": 0
                },
                {
                    "sent": "Again, you can do whatever you want, but don't drop the pole 'cause if you drop the pole, you're going.",
                    "label": 0
                },
                {
                    "sent": "It's going to cost you.",
                    "label": 0
                },
                {
                    "sent": "A lot of reinforcement learning problems that I've seen have both of these at once, like there's good things in the world and there's things you should avoid and so will put some positives and some negatives out there in the agent has to kind of wind its way through the environment.",
                    "label": 0
                },
                {
                    "sent": "And then another way that rewards are often used as is a kind of shaping to actually make the learning problem easier and.",
                    "label": 0
                },
                {
                    "sent": "There's debate about whether this is a good idea or not.",
                    "label": 0
                },
                {
                    "sent": "But it's a good idea, so it's a necessary idea, and in particular, in this poll balance, in case we might say something like you know each step at which the pole is still balanced, I'm going to give you a + .1 for that like, not so much that it's the most important thing in the world.",
                    "label": 0
                },
                {
                    "sent": "You, but enough that at least draws your attention to the fact that while it's balanced, that's good.",
                    "label": 0
                },
                {
                    "sent": "Of course, ultimately it's the same, because if you drop the pole, you get the minus, and so that's bad, so you didn't.",
                    "label": 0
                },
                {
                    "sent": "You didn't get the zero, so it's worse than the zeros.",
                    "label": 0
                },
                {
                    "sent": "But sometimes these learning algorithms are really helped by this that they get something something more short-term that they can try to optimize instead of just.",
                    "label": 0
                },
                {
                    "sent": "Empty vastness and then all of a sudden oh that was bad right?",
                    "label": 0
                },
                {
                    "sent": "So little little little hints along the way.",
                    "label": 0
                },
                {
                    "sent": "They can actually guide its decisions without having to have the whole big picture in its head all at once.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all I was going to.",
                    "label": 0
                },
                {
                    "sent": "Yes, Susan.",
                    "label": 0
                },
                {
                    "sent": "Problem, can you just say a little bit about?",
                    "label": 0
                },
                {
                    "sent": "A setting in which you would want to have you want to shape the rewards in a real life problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go with mobile Health because I don't really know about that, but you do and so you can correct me if I'm wrong so.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to get somebody to you know, do good things for their health and their longevity.",
                    "label": 0
                },
                {
                    "sent": "You can give them zeros all the time, and then a -- 10 when they're dead.",
                    "label": 0
                },
                {
                    "sent": "Right, but that turns out not to actually help them adjust their behavior much on the fly.",
                    "label": 0
                },
                {
                    "sent": "So instead you might want to do, like yeah, I'm going to give you a plus one for jogging today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be a -- 1 for getting 3 cookies and putting them on your desk because you're going to one at the end of each hour as a reward so so you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually sort of shape the behavior in a way that is again makes it easier to learn.",
                    "label": 0
                },
                {
                    "sent": "It kind of makes things temporally more close in time, but hopefully doesn't actually change the the the correct behavior.",
                    "label": 0
                },
                {
                    "sent": "Doesn't it still the right thing to do is you eat healthy exercise, do the things you're supposed to do.",
                    "label": 0
                },
                {
                    "sent": "We're just giving you a little bit more information to make it easier to find that behavior.",
                    "label": 0
                },
                {
                    "sent": "That and so we see this with robots and stuff all the time, and the soccer example is kind of a classic one.",
                    "label": 0
                },
                {
                    "sent": "There was a time Once Upon a time where people actually had robots playing soccer and they decided to set it up as a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "Just like this, you know.",
                    "label": 0
                },
                {
                    "sent": "Plus one when you score a goal or they didn't do this, but even better would be plus one for winning the game.",
                    "label": 0
                },
                {
                    "sent": "Like is it good to score goals or not?",
                    "label": 0
                },
                {
                    "sent": "Really, that's not my decision to make.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, I'm just going to tell you whether not you won the game at the end.",
                    "label": 0
                },
                {
                    "sent": "If it's if you feel it helps you to have scored goals to achieve that, end good for you, but that's not part of the real reward function.",
                    "label": 0
                },
                {
                    "sent": "And it's really hard, right?",
                    "label": 0
                },
                {
                    "sent": "You have these robots just sort of wandering around, and then you know.",
                    "label": 0
                },
                {
                    "sent": "And then all of a sudden the buzzer rings and they want and like nothing like they had no idea what had happened.",
                    "label": 0
                },
                {
                    "sent": "Maybe one of them bump the ball into the goal that it's very hard to connect up all these different actions to the objective of the problem.",
                    "label": 0
                },
                {
                    "sent": "So you start to throw in extra extra kinds of rewards.",
                    "label": 0
                },
                {
                    "sent": "So you say, yeah, you know, it's good to have the ball.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you a little plus for that, but not so good that you're going to, you know, like hog the ball.",
                    "label": 0
                },
                {
                    "sent": "And this ends up being this very tricky kind of reward engineering thing that you have to do.",
                    "label": 0
                },
                {
                    "sent": "Where again you give it, you give it, you know bonus for having the ball, but not so much that the agent is not going to be willing to kick it into the goal because like, but I'd like to have.",
                    "label": 0
                },
                {
                    "sent": "Right, so so.",
                    "label": 0
                },
                {
                    "sent": "Also stay away from the opponents little minus.",
                    "label": 0
                },
                {
                    "sent": "For that you know.",
                    "label": 0
                },
                {
                    "sent": "Don't be far from the goal.",
                    "label": 0
                },
                {
                    "sent": "Don't be too close to the goal right?",
                    "label": 0
                },
                {
                    "sent": "All these different hints could be turned into little extra rewards, not on a huge scale.",
                    "label": 0
                },
                {
                    "sent": "Again, because that changes the nature of the problem but not on ignorable scale, because then it's actually not helping the agent at all.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now I want to dive into the agent side of the equation so so.",
                    "label": 0
                },
                {
                    "sent": "How do we make agents actually make decisions to maximize reward?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The we have to to turn these into computational problems.",
                    "label": 0
                },
                {
                    "sent": "We need to kind of formal problem specification, and there's actually a lot of ways to do this, but the field has pretty much committed itself to Markov decision processes and variants thereof.",
                    "label": 1
                },
                {
                    "sent": "Which may not have been a great idea, but again, this is.",
                    "label": 0
                },
                {
                    "sent": "This is a tutorial I'm just going to tell you what is we can worry about?",
                    "label": 0
                },
                {
                    "sent": "What what could have been some other time?",
                    "label": 0
                },
                {
                    "sent": "Over drinks at the end of the day, say alright so so this is this is this.",
                    "label": 0
                },
                {
                    "sent": "I don't want to quit reinforcement learning with this model, but some people do and it is.",
                    "label": 0
                },
                {
                    "sent": "Even if they don't, they often mean this when they're talking about reinforcement learning, and so the model goes like this.",
                    "label": 0
                },
                {
                    "sent": "We're going to imagine that the environment part of the environment agent picture.",
                    "label": 0
                },
                {
                    "sent": "The environment itself consists of two primary functions.",
                    "label": 0
                },
                {
                    "sent": "The transition function T. Takes as input the current state in action and outputs a probability distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "So if I'm in, if I'm in this situation and I tried this action, there's some probability I'm going to fall in my face and there's some probability that I won't, and so you know the environment is the way it is.",
                    "label": 0
                },
                {
                    "sent": "It specifies those probabilities.",
                    "label": 0
                },
                {
                    "sent": "I might not know them.",
                    "label": 0
                },
                {
                    "sent": "Which probably should keep me away from the edge, but but they they are specified there there in the in the environment model.",
                    "label": 0
                },
                {
                    "sent": "The reward function is.",
                    "label": 0
                },
                {
                    "sent": "The reward function is.",
                    "label": 0
                },
                {
                    "sent": "It's the mapping from.",
                    "label": 0
                },
                {
                    "sent": "If you're in some state and you take some action, here's what the evaluation for that is.",
                    "label": 0
                },
                {
                    "sent": "Here's the points that you get for that.",
                    "label": 0
                },
                {
                    "sent": "So like falling on the face probably going to be low scoring thing.",
                    "label": 0
                },
                {
                    "sent": "That being said, probably would all remember this talk for very long time, so we have an upside to.",
                    "label": 0
                },
                {
                    "sent": "But not in the short term.",
                    "label": 0
                },
                {
                    "sent": "In the short term it would mostly be painful, alright, so so that's we think of the environment is consisting of these two functions, so they're embedded inside the environment, picture here.",
                    "label": 0
                },
                {
                    "sent": "And what we're done trying to do as the agent is decide.",
                    "label": 0
                },
                {
                    "sent": "Which actions do we take in which states?",
                    "label": 0
                },
                {
                    "sent": "So we maximize our cumulative discount.",
                    "label": 0
                },
                {
                    "sent": "That expected reward.",
                    "label": 0
                },
                {
                    "sent": "So we often write this as a policy.",
                    "label": 0
                },
                {
                    "sent": "This is \u03c0 star, which is mapping states to actions and the way that's going to do that is by taking argmax over this function.",
                    "label": 0
                },
                {
                    "sent": "Q star.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's not helping what's Q star, so this is my attempt at.",
                    "label": 0
                },
                {
                    "sent": "Trying to visualize that.",
                    "label": 0
                },
                {
                    "sent": "So what we're imagining is that the critter in the environment is undergoing a series of transitions.",
                    "label": 0
                },
                {
                    "sent": "It was in some state, it shows some action, it got some reward for that, and it ended up in some new state from which it could take another action and get another reward.",
                    "label": 0
                },
                {
                    "sent": "And get transition to yet another state.",
                    "label": 0
                },
                {
                    "sent": "Take another action and this chain goes on and on and on.",
                    "label": 0
                },
                {
                    "sent": "And we want to capture information about the expected reward that whatever behavior it's going to take brings.",
                    "label": 0
                },
                {
                    "sent": "So if the agent is acting optimally, it's acting in such a way that it maximizes his future discounted reward.",
                    "label": 0
                },
                {
                    "sent": "Then what do we know about that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "Let's say let's say we're going to estimate actually something a little bit different, so the expected future discounted reward for starting in some State S taking an action A for one step and then behaving optimally thereafter.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly clear how we're going to know what that is, but it's nicely recursively defined in such a way that will be able to do it when we get there.",
                    "label": 0
                },
                {
                    "sent": "But for now, just imagine that's what we're trying to optimize.",
                    "label": 0
                },
                {
                    "sent": "No, sorry, that's what we're trying to evaluate or estimate.",
                    "label": 0
                },
                {
                    "sent": "What's the total expected discounted reward for starting in some state and taking some action?",
                    "label": 0
                },
                {
                    "sent": "For one step and then behaving optimally thereafter so we can write down what that's going to be, it's going to be whatever the immediate reward is for taking that action in that state plus the discounted value of the future and what's the future.",
                    "label": 0
                },
                {
                    "sent": "What's the sum over all possible next states of the probability of reaching that next state as prime, given that we were in state S and took action A?",
                    "label": 0
                },
                {
                    "sent": "This is the transition function.",
                    "label": 1
                },
                {
                    "sent": "Times the value of the rest of the rest of the life after that.",
                    "label": 0
                },
                {
                    "sent": "And how are we going to decide what to do there?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to some new state S prime.",
                    "label": 0
                },
                {
                    "sent": "And if we actually have this Q star function.",
                    "label": 0
                },
                {
                    "sent": "We should take whichever action first action leads to the hyest.",
                    "label": 0
                },
                {
                    "sent": "Expected discounted reward from that state.",
                    "label": 0
                },
                {
                    "sent": "So once we get dropped into S prime, we actually use this Q star function to decide what to do.",
                    "label": 0
                },
                {
                    "sent": "So this is in a sense this is the optimal policy.",
                    "label": 1
                },
                {
                    "sent": "This is the value of the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "This is the evaluation of that optimal policy, so there's again sort of recursively defined, but they have all the information that you need all in one little package.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Any questions about that sort of a lot of the rest of the math leans on this pretty heavily, so if you're not super comfortable with this, then I probably should spend another moment on it.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could be wondering how would you possibly find Q star?",
                    "label": 0
                },
                {
                    "sent": "I'll get to that, yes?",
                    "label": 0
                },
                {
                    "sent": "Ah, good, so yes, so.",
                    "label": 0
                },
                {
                    "sent": "Max first is you know you have a bunch of different options and give me the value of the highest scoring option.",
                    "label": 0
                },
                {
                    "sent": "Argmax is which action actually gives me that highest scoring option.",
                    "label": 0
                },
                {
                    "sent": "So this is just saying when I'm in some state, figure out which action brings me the highest future expected value from that state and return that action as the action I should take in that state.",
                    "label": 0
                },
                {
                    "sent": "OK. Are ready.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just relate this briefly to the find the ball problem.",
                    "label": 1
                },
                {
                    "sent": "This is you can think of this as being an MVP.",
                    "label": 0
                },
                {
                    "sent": "In fact we did.",
                    "label": 0
                },
                {
                    "sent": "That's how we did it.",
                    "label": 0
                },
                {
                    "sent": "That's how we ended up building a reinforcement learning for this.",
                    "label": 0
                },
                {
                    "sent": "Each different angle that the dog could be standing at is in the sense of different state.",
                    "label": 0
                },
                {
                    "sent": "And then there's one of the States where connection is actually probably a collection of states where it actually can see the pink ball in its field of view.",
                    "label": 0
                },
                {
                    "sent": "So we don't actually model it this way.",
                    "label": 0
                },
                {
                    "sent": "We model it this way each.",
                    "label": 0
                },
                {
                    "sent": "Each different angle that it could be is a different node in this graph.",
                    "label": 0
                },
                {
                    "sent": "In this Markov decision process, the actions that can take turn left or turn right, or arrows in this graph.",
                    "label": 0
                },
                {
                    "sent": "There's often probabilities annotated on these arrows.",
                    "label": 0
                },
                {
                    "sent": "So for example, if turning left sometimes actually leaves you pretty much in the same place, but sometimes actually trans.",
                    "label": 0
                },
                {
                    "sent": "Sometimes what you backwards sometimes actually turns left.",
                    "label": 0
                },
                {
                    "sent": "That's all in the definition of the MDP.",
                    "label": 0
                },
                {
                    "sent": "That whole probability distribution is in there.",
                    "label": 0
                },
                {
                    "sent": "It's very rare, for example, to be in this state.",
                    "label": 0
                },
                {
                    "sent": "You know facing this way, take the turn right action for one step and end up over here so that that transition is going to have probably 0 probability actually.",
                    "label": 0
                },
                {
                    "sent": "And then some of the states we can actually annotate with their rewards, or actually all of them.",
                    "label": 0
                },
                {
                    "sent": "We have to annotate with rewards so we could do it as zero reward everywhere, except for when you see the the ball and you get a plus one for that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is.",
                    "label": 0
                },
                {
                    "sent": "Everything about solving the Bellman equation on one slide.",
                    "label": 0
                },
                {
                    "sent": "I I made this slide.",
                    "label": 0
                },
                {
                    "sent": "I was very proud of myself and then I called into question whether this was a good idea at all.",
                    "label": 0
                },
                {
                    "sent": "'cause it's it's gets a little dense.",
                    "label": 0
                },
                {
                    "sent": "But it's nice having it all in one place, so you can make connections between, so let me give this a try and then you can.",
                    "label": 0
                },
                {
                    "sent": "You should ask me follow questions so.",
                    "label": 0
                },
                {
                    "sent": "In a sense, planning an MVP decision making an MVP is solve the Bellman equation, figure out a Q star that satisfies this set of actually nonlinear constraints.",
                    "label": 0
                },
                {
                    "sent": "'cause of the Max.",
                    "label": 0
                },
                {
                    "sent": "And if I have that if I have a function that satisfies this equation, then I know anytime I'm going to state how good is it going to take this one action and then behave optimally.",
                    "label": 0
                },
                {
                    "sent": "So I always take the best one.",
                    "label": 0
                },
                {
                    "sent": "I always be optimal after that.",
                    "label": 0
                },
                {
                    "sent": "OK solving Bellman equation we're done with the planning.",
                    "label": 0
                },
                {
                    "sent": "We know what we need to do.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "How do we solve equations?",
                    "label": 0
                },
                {
                    "sent": "So there's there's really three main algorithms that talked about a lot valuation policy iteration in linear programming.",
                    "label": 0
                },
                {
                    "sent": "There are some algorithms that are heuristics and blends of these in various ways, But these are kind of the three pillars on which most other things seem to be built.",
                    "label": 0
                },
                {
                    "sent": "So the idea of.",
                    "label": 0
                },
                {
                    "sent": "Value iteration is OK. We want to solve this equation.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to guess.",
                    "label": 0
                },
                {
                    "sent": "Acute function.",
                    "label": 0
                },
                {
                    "sent": "If we're right, then awesome, we're done.",
                    "label": 0
                },
                {
                    "sent": "We're probably not going to be right, so we'll just guess it to be all zeros.",
                    "label": 0
                },
                {
                    "sent": "Which works if all the rewards are heroes.",
                    "label": 0
                },
                {
                    "sent": "But again, they're probably not, so we just discussed that as a starting place, and then we say, OK, we now have an estimate of Q star gas of actually fairly poor, guess probably of Q star.",
                    "label": 0
                },
                {
                    "sent": "We want two star this Q star to actually equal this equation of the old Q star.",
                    "label": 0
                },
                {
                    "sent": "So we actually treat it as an assignment statement.",
                    "label": 0
                },
                {
                    "sent": "We actually say take our previous guests of the values.",
                    "label": 0
                },
                {
                    "sent": "Compute for each for each state action pair, if I were to.",
                    "label": 0
                },
                {
                    "sent": "Take that action from that state and then look at the distribution of states.",
                    "label": 0
                },
                {
                    "sent": "I end up in and when I end up in them, use my estimate of the Q function that I had from my previous iteration of this algorithm, which again started off at zero really bad, but now it's a little bit better 'cause it's taking a little bit of the context into account.",
                    "label": 0
                },
                {
                    "sent": "Then what we know is this algorithm.",
                    "label": 0
                },
                {
                    "sent": "If we just keep iterating this, we keep reassigning the values based on this equation over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "In the limit it converges to the actual solution to the to the Bellman equation.",
                    "label": 1
                },
                {
                    "sent": "So that's cool.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm that a lot of people.",
                    "label": 0
                },
                {
                    "sent": "Start with or or use because it's it really is very simple and it's it's a good workhorse 'cause you don't have to get a lot of things right.",
                    "label": 0
                },
                {
                    "sent": "It just kind of does what it's supposed to do.",
                    "label": 0
                },
                {
                    "sent": "But for me theoretical perspective, it's sort of awkward that it's OK, so it converges in the limit.",
                    "label": 0
                },
                {
                    "sent": "In other words, Infinity from now you'll have the answer which.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the same as not having the answer ever.",
                    "label": 0
                },
                {
                    "sent": "So there are other algorithms that we could apply, so policy iteration is is a variant of this idea.",
                    "label": 0
                },
                {
                    "sent": "In some ways that actually converges in finite time.",
                    "label": 0
                },
                {
                    "sent": "So the way policy iteration works is again, we're going to start off guessing a random queue function.",
                    "label": 0
                },
                {
                    "sent": "Say all zeros doesn't matter, but let's just go with all zeros.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to say, let's use that Q function to build a policy.",
                    "label": 0
                },
                {
                    "sent": "So what policy do I get if from every state I choose, whichever action gives me the highest estimated Q value in that state.",
                    "label": 0
                },
                {
                    "sent": "Starting off at all zeros, it's just going to be some arbitrary policy, just completely untethered to reality, but it's something something concrete then what we do is we say, OK, that's a way to behave.",
                    "label": 0
                },
                {
                    "sent": "How does that go?",
                    "label": 0
                },
                {
                    "sent": "Let's actually evaluate AQ function specifically for that policy, so instead of Q star that we were looking at before is looking for the values of the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "I just want to know the values of the policy that we were just talking about.",
                    "label": 0
                },
                {
                    "sent": "This is called policy evaluation, and this is actually a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "This can be solved actually.",
                    "label": 0
                },
                {
                    "sent": "Very reliably, very fast are our friends in numerical analysis.",
                    "label": 0
                },
                {
                    "sent": "Do this for us and then we don't have to worry about Matlab, does this right?",
                    "label": 0
                },
                {
                    "sent": "This is just done.",
                    "label": 0
                },
                {
                    "sent": "So, and specifically what it's saying is well under this policy that we have.",
                    "label": 0
                },
                {
                    "sent": "If we're in some state and we take some action, we're going to get the reward for that action plus the discounted expected value of the state that we end up in.",
                    "label": 0
                },
                {
                    "sent": "Given that we take actions according to our policy \u03c0 T. Then when we land in state S prime, we're going to look at the Q value for the action that we're going to take from state S prime.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this evaluates policy \u03c0 T. And then we loop out again.",
                    "label": 0
                },
                {
                    "sent": "So we now we have a new queue function.",
                    "label": 0
                },
                {
                    "sent": "We can generate a new policy.",
                    "label": 0
                },
                {
                    "sent": "But that is greedy with respect to that queue function that actually tries to maximize reward for that queue function.",
                    "label": 0
                },
                {
                    "sent": "Then we can evaluate that.",
                    "label": 0
                },
                {
                    "sent": "Then we can update our policy and we can evaluate that and we can up their policy.",
                    "label": 0
                },
                {
                    "sent": "So again, fairly simple.",
                    "label": 0
                },
                {
                    "sent": "But what's neat about this algorithm is each time.",
                    "label": 0
                },
                {
                    "sent": "It doesn't evaluation.",
                    "label": 0
                },
                {
                    "sent": "It either comes back with the same policy it had before.",
                    "label": 0
                },
                {
                    "sent": "If it's optimal.",
                    "label": 0
                },
                {
                    "sent": "Or it comes up with a policy that's better than the one that it had before.",
                    "label": 0
                },
                {
                    "sent": "So because each time that it doesn't evaluation, it gets a better policy and there's only a finite number of policies.",
                    "label": 0
                },
                {
                    "sent": "It's big, but this finite each state.",
                    "label": 0
                },
                {
                    "sent": "There's some action that you choose there, so it's number of actions raised to the number of state, power, different combinations that you can have.",
                    "label": 0
                },
                {
                    "sent": "Each time we do an iteration of policy iteration, at least one of those is history.",
                    "label": 0
                },
                {
                    "sent": "So we're just ticking through them.",
                    "label": 0
                },
                {
                    "sent": "Again, there's a lot of them.",
                    "label": 0
                },
                {
                    "sent": "But eventually we hit the last one because there's a finite number of these things.",
                    "label": 0
                },
                {
                    "sent": "So this is this is kind of a powerful thing to do.",
                    "label": 0
                },
                {
                    "sent": "People don't do this very much.",
                    "label": 0
                },
                {
                    "sent": "Mean people.",
                    "label": 0
                },
                {
                    "sent": "When I say people, I mean people who write programs that solve them DPS.",
                    "label": 0
                },
                {
                    "sent": "Don't use policy directions so much, but.",
                    "label": 0
                },
                {
                    "sent": "But it's really quite.",
                    "label": 0
                },
                {
                    "sent": "It's really quite lovely alright, and the last thing to mention is linear programming, so this is a neat idea that says what we're going to do is take our Markov decision process and convert it to a linear program, which is kind of an optimization problem that we know how to solve in polynomial time that we know how to actually solve efficiently.",
                    "label": 0
                },
                {
                    "sent": "So if we can do that, if we can actually express the MDP perfectly as a as a linear program, then we hit it with a linear programming stick and the solution pops out and we're done.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think, done even less than policy iteration, but it is nice in terms of the guarantees that we get.",
                    "label": 0
                },
                {
                    "sent": "So here's the way that this is done.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to set variables that represent the value of the best action at each of the states S. And those are variables we don't know them at first, but we're just going to put them into this optimization problem as variables that the program is going to solve.",
                    "label": 0
                },
                {
                    "sent": "We want to find the smallest sum of those variables such that for Allstate Action pairs the value at any state is bigger than or equal to.",
                    "label": 0
                },
                {
                    "sent": "This which is the Q value at that state.",
                    "label": 0
                },
                {
                    "sent": "According to the value function that we have.",
                    "label": 0
                },
                {
                    "sent": "So what this is actually saying is.",
                    "label": 0
                },
                {
                    "sent": "Make it so that the maximum action is the maximum action.",
                    "label": 0
                },
                {
                    "sent": "It's above all the others, but it can't be too much above the others.",
                    "label": 0
                },
                {
                    "sent": "It actually has to be the largest of them, so we're going to minimize the values such that their upper bounds.",
                    "label": 0
                },
                {
                    "sent": "So the minimum upper bound is exactly the Max, so this gives you a way of expressing the notion of Max.",
                    "label": 0
                },
                {
                    "sent": "As a system of linear inequality's an A linear evaluation function, and that's the definition of a linear program so.",
                    "label": 0
                },
                {
                    "sent": "Again, we have a polynomial size linear program.",
                    "label": 0
                },
                {
                    "sent": "We solve that and we get the solution to arbitration.",
                    "label": 0
                },
                {
                    "sent": "So OK, so that was all.",
                    "label": 0
                },
                {
                    "sent": "That was all on one side so.",
                    "label": 0
                },
                {
                    "sent": "Did any of that land is that?",
                    "label": 0
                },
                {
                    "sent": "Is that helpful?",
                    "label": 0
                },
                {
                    "sent": "What's the?",
                    "label": 0
                },
                {
                    "sent": "What's the takeaway?",
                    "label": 0
                },
                {
                    "sent": "The takeaway is if you give me an MDP and it doesn't have 10s of millions of states.",
                    "label": 0
                },
                {
                    "sent": "Then I can hit it with a number of possible algorithmic.",
                    "label": 0
                },
                {
                    "sent": "Algorithms and and figure out the optimal way to behave the way that actually maximizes expected reward.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's The upshot.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, but that's not telling us about reinforcement learning in a sense, right?",
                    "label": 0
                },
                {
                    "sent": "This is really telling us if someone gave me an MVP I could.",
                    "label": 0
                },
                {
                    "sent": "I could figure out how to optimize reward, but the reinforcement learning problem is no.",
                    "label": 0
                },
                {
                    "sent": "I just get to live in the world and I have to figure out from that how the world works.",
                    "label": 0
                },
                {
                    "sent": "So there's a number of different flavors of algorithms that people studied.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus on 2 right now, model free learning and model based learning.",
                    "label": 0
                },
                {
                    "sent": "These seem to have had a lot of.",
                    "label": 0
                },
                {
                    "sent": "Have shown that they have a lot of utility for explaining lots of different behaviors, even on the.",
                    "label": 0
                },
                {
                    "sent": "Natural or LDM side, so it's it's good to kind of have a gut feeling about what this distinction is about.",
                    "label": 0
                },
                {
                    "sent": "So let me start with model free learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea of model free learning.",
                    "label": 0
                },
                {
                    "sent": "Is what the Asian experience is.",
                    "label": 0
                },
                {
                    "sent": "Is this sequence of?",
                    "label": 0
                },
                {
                    "sent": "I'm going to call them stars is if you haven't heard.",
                    "label": 0
                },
                {
                    "sent": "People say that before.",
                    "label": 0
                },
                {
                    "sent": "It probably sounds silly, but state action reward next state is SARS.",
                    "label": 0
                },
                {
                    "sent": "It's not the respiratory SARS, it's the.",
                    "label": 0
                },
                {
                    "sent": "If they are, we reinforcement learning so.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "The agent is seeing is a whole sequence of those right, seeing a lot of state transitions, a lot of rewards go by.",
                    "label": 0
                },
                {
                    "sent": "And what these model free algorithms do is they use that information to tweak an estimate of the Q function, so there's the Q function.",
                    "label": 0
                },
                {
                    "sent": "It's out there, but we don't know it and we can't compute it 'cause we don't know transitions and rewards were just out in the world doing our thing, but each time we experienced one of these stars is we can use that to essentially improve our estimate of Q.",
                    "label": 0
                },
                {
                    "sent": "We can use that estimate of the Q function.",
                    "label": 0
                },
                {
                    "sent": "I drew it as like a cartoon EQ.",
                    "label": 0
                },
                {
                    "sent": "'cause it's not the real Q, it's just kind of our best guess as it.",
                    "label": 0
                },
                {
                    "sent": "It's like a caricature of the real Q.",
                    "label": 0
                },
                {
                    "sent": "If we have that, if we have a guest of the of the best Q function, we can use that to act by by saying things like, well, I'm in a state.",
                    "label": 0
                },
                {
                    "sent": "My cartoony Q function tells me this is the best action I should take in that state.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll take that.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a way of generating actions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "This experience that we get is being used to build up Q or actually revise Q directly.",
                    "label": 1
                },
                {
                    "sent": "An algorithms like Q Learning and Sarsa, which I'll talk about are the most studied algorithms, kind of in this class.",
                    "label": 0
                },
                {
                    "sent": "So sorry, no sorry this class is the most studied class I think of reinforcement learning algorithms and Q learning and Sarsa are good examples of algorithms in that class.",
                    "label": 0
                },
                {
                    "sent": "So let me just let me contrast that with model based learning.",
                    "label": 0
                },
                {
                    "sent": "So the idea in model based learning is we're actually going to take our sources of experience and use them to estimate are cartoony versions of the transition function in the reward function.",
                    "label": 0
                },
                {
                    "sent": "So I've been in some state I tried in action.",
                    "label": 0
                },
                {
                    "sent": "This is what I saw.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's how the world works.",
                    "label": 0
                },
                {
                    "sent": "So I record that in my estimate of the transition reward function and now I can actually use any of those algorithms on the previous slide to turn those into an estimate of the Q function, and then I can use that to act.",
                    "label": 0
                },
                {
                    "sent": "So both of these algorithms go through the queue function as kind of a critical bottleneck, but one tries to go there directly and one tries to go there via.",
                    "label": 0
                },
                {
                    "sent": "The model.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Part of me wants to say why this is important to people like on the biological side, but either either you know that already and I shouldn't tell you, or you don't know that.",
                    "label": 0
                },
                {
                    "sent": "And then I'll probably tell you wrong and then you'll be wrong.",
                    "label": 0
                },
                {
                    "sent": "That would hurt all of us so.",
                    "label": 0
                },
                {
                    "sent": "I don't know any any does this distinction makes it so.",
                    "label": 0
                },
                {
                    "sent": "Here's what people sometimes say they think of this kind of Q update type algorithms model free update algorithms as being kind of like habits, right?",
                    "label": 0
                },
                {
                    "sent": "So it gives me a way of just acting without really thinking about it.",
                    "label": 0
                },
                {
                    "sent": "When I'm going to state, I just look up the Q function and the Q function says this is the good thing to do, so I do it like, oh man, that's not even I was trying to.",
                    "label": 0
                },
                {
                    "sent": "I was trying to drive to the store, not to my house.",
                    "label": 0
                },
                {
                    "sent": "And they like in the this model based learning to more planful behavior that says, OK, I know what it is I'm trying to do.",
                    "label": 0
                },
                {
                    "sent": "I know my reward function is.",
                    "label": 0
                },
                {
                    "sent": "I understand how the world works.",
                    "label": 0
                },
                {
                    "sent": "This would be a way that I would act that would give me the reward that I'm seeking.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is more kind of.",
                    "label": 0
                },
                {
                    "sent": "Habitual I forgot the other ones, called Peter came back.",
                    "label": 0
                },
                {
                    "sent": "Maybe Peter notes.",
                    "label": 0
                },
                {
                    "sent": "Goal directed thank you alright.",
                    "label": 0
                },
                {
                    "sent": "Alright, so habitual goal directed.",
                    "label": 0
                },
                {
                    "sent": "To a first approximation, I've seen people make other kinds of weird approximations of these.",
                    "label": 0
                },
                {
                    "sent": "Also, you're talking about morals.",
                    "label": 0
                },
                {
                    "sent": "It's like, well, there's morals that say you shouldn't do that action.",
                    "label": 0
                },
                {
                    "sent": "You know that Q funds that Q values prohibited or you shouldn't take an action that would cause that to come about.",
                    "label": 0
                },
                {
                    "sent": "And that's more like the model based kind of way of thinking about things.",
                    "label": 0
                },
                {
                    "sent": "And philosophers have names for those two that I'm not going to say.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, but I do but I really do need to tell you what Q learning is 'cause.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's kind of awesome and second of all.",
                    "label": 0
                },
                {
                    "sent": "Mostly it's kind of awesome, but second of all, but lots of people know about it, so if you don't know about it, you're going to, you know, not get to hang out at that party.",
                    "label": 0
                },
                {
                    "sent": "So alright, so here's cooler.",
                    "label": 0
                },
                {
                    "sent": "So Q learning actually is a family of algorithms.",
                    "label": 0
                },
                {
                    "sent": "They all have one really important step in common, but they vary as to the how the other steps are built.",
                    "label": 0
                },
                {
                    "sent": "So here the four steps and I used a slightly grayer color for the ones that are that have multiple ways of filling them in and then black for the really important right?",
                    "label": 0
                },
                {
                    "sent": "So here's how cooling works.",
                    "label": 0
                },
                {
                    "sent": "We have to initialize our Q function.",
                    "label": 0
                },
                {
                    "sent": "We start off saying, here's what I think the values actually are going to be, which is probably going to be wrong.",
                    "label": 0
                },
                {
                    "sent": "'cause if you knew that in advance you would need to learn so.",
                    "label": 0
                },
                {
                    "sent": "So it's but the whether you guess high or low in the beginning turns out to be really important into the way the algorithm behaves.",
                    "label": 0
                },
                {
                    "sent": "So OK, so pick a way to initialize your Q function then.",
                    "label": 0
                },
                {
                    "sent": "We have to act, so at each step Q learning is in some state and it says.",
                    "label": 0
                },
                {
                    "sent": "What should I do now?",
                    "label": 0
                },
                {
                    "sent": "So a common thing to do would be use your estimated queue function to find the action that has the highest expected reward and take it.",
                    "label": 0
                },
                {
                    "sent": "But you can't just do that.",
                    "label": 0
                },
                {
                    "sent": "It turns out because first of all your Q function is garbage in the beginning, and so if you follow it.",
                    "label": 0
                },
                {
                    "sent": "You might get lots of data that is actually not very helpful in figuring out how to optimize.",
                    "label": 0
                },
                {
                    "sent": "Reward.",
                    "label": 0
                },
                {
                    "sent": "The bad case here is typically.",
                    "label": 0
                },
                {
                    "sent": "You believe that something out there, the Q function encodes the belief that something out there is really, really bad, so you don't want to go there.",
                    "label": 0
                },
                {
                    "sent": "So the agent doesn't go there, so it doesn't learn that it wasn't actually that bad.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a.",
                    "label": 0
                },
                {
                    "sent": "You know very pessimistic view of the world.",
                    "label": 0
                },
                {
                    "sent": "If you, if there's if there's negativeness out there and we and we believe it without testing it, then we might not actually be optimizing a reward in the long run.",
                    "label": 0
                },
                {
                    "sent": "So right so it turns out to be important to sometimes explore.",
                    "label": 0
                },
                {
                    "sent": "Actually sometimes try actions that the algorithm doesn't think are the best, but still maybe worth thinking about.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's kind of the set up and now what happens?",
                    "label": 0
                },
                {
                    "sent": "We get a SARS, so we were in some state.",
                    "label": 0
                },
                {
                    "sent": "We chose some action.",
                    "label": 0
                },
                {
                    "sent": "Then the environment gives us back a reward in the next state.",
                    "label": 1
                },
                {
                    "sent": "This is what we do with that information.",
                    "label": 0
                },
                {
                    "sent": "We take our cue function for the state action pair that we just left.",
                    "label": 0
                },
                {
                    "sent": "That's the one that we're going to learn about.",
                    "label": 0
                },
                {
                    "sent": "And we move it, we change its value alittle bit.",
                    "label": 0
                },
                {
                    "sent": "This is a learning rate so just a tiny bit in the direction of what the immediate reward plus the discounted expected value of the next state.",
                    "label": 0
                },
                {
                    "sent": "According to our cartoon Q function.",
                    "label": 0
                },
                {
                    "sent": "Minus the Q value of the state that we just left.",
                    "label": 0
                },
                {
                    "sent": "Right, so so this difference is actually really important part of this equation, so it's we're actually looking at the difference between how, how valuable we thought that state action pair was, and maybe how much how valuable we should think it is, because we just experienced an actual reward and a transition to a next state.",
                    "label": 0
                },
                {
                    "sent": "And we can look up the value of that next date.",
                    "label": 0
                },
                {
                    "sent": "So this is a guess as to the value of the state, action pair essay, and this is another guess.",
                    "label": 0
                },
                {
                    "sent": "This is our current gas and the difference between them gives us an error signal, so if they are exactly the same, the error zero.",
                    "label": 0
                },
                {
                    "sent": "So we don't have to change.",
                    "label": 0
                },
                {
                    "sent": "Our guests at all, 'cause we're perfect.",
                    "label": 0
                },
                {
                    "sent": "But if.",
                    "label": 0
                },
                {
                    "sent": "It looks like so if this is too small, for example, so we gotta get a small value out of out of this and we had guest a really big value here.",
                    "label": 0
                },
                {
                    "sent": "This difference is going to be negative, and that's going to cause this Q value to get pushed down.",
                    "label": 0
                },
                {
                    "sent": "So it looked like it was too big.",
                    "label": 0
                },
                {
                    "sent": "Then the learning rule says make it smaller if it looks like it's too small then learning rule says make it bigger.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It makes sense.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then we have to decide what to do with this learning rate.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a little multiplier that says we want to move a little bit in that direction.",
                    "label": 0
                },
                {
                    "sent": "How a little bit do you want to move?",
                    "label": 0
                },
                {
                    "sent": "It turns out you have to change this overtime.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out you can change this overtime and maybe you should if you want to get certain kinds of guarantees.",
                    "label": 0
                },
                {
                    "sent": "So this is what we know about this algorithm.",
                    "label": 1
                },
                {
                    "sent": "We know that there are choices for Step 1, two, and four.",
                    "label": 0
                },
                {
                    "sent": "That actually guarantee that our estimate of the Q function converges to the actual solution to the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "In the limit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's a number of proofs of this.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the first one that was out there and appreciated.",
                    "label": 0
                },
                {
                    "sent": "This is kind of awesome right?",
                    "label": 0
                },
                {
                    "sent": "So that you may not appreciate.",
                    "label": 0
                },
                {
                    "sent": "You may not think that it's awesome at all.",
                    "label": 0
                },
                {
                    "sent": "You may be right, but like to me, there's an awesomeness here.",
                    "label": 0
                },
                {
                    "sent": "Which is?",
                    "label": 0
                },
                {
                    "sent": "This is essentially one line of code, and this one line of code solves a linear program.",
                    "label": 0
                },
                {
                    "sent": "It solves the bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Given enough time experience, what not, but, but this is a really powerful thing and so this is almost the kind of thing that you can't not implement, so there's a lot of papers where people have.",
                    "label": 0
                },
                {
                    "sent": "Very unreasonably used.",
                    "label": 0
                },
                {
                    "sent": "Q Learning because they couldn't not do it.",
                    "label": 0
                },
                {
                    "sent": "It was just.",
                    "label": 0
                },
                {
                    "sent": "It's just so easy to do, you have to do it.",
                    "label": 0
                },
                {
                    "sent": "Compelled so this is this is really an.",
                    "label": 0
                },
                {
                    "sent": "It really does kind of work.",
                    "label": 0
                },
                {
                    "sent": "What you find often is that it takes a very long time for you to get close to the Q values.",
                    "label": 0
                },
                {
                    "sent": "The optimal Q values and so that the learning algorithm can actually spend a lot of time just kind of.",
                    "label": 0
                },
                {
                    "sent": "Wandering around doing things that don't look all that useful, but all this is starting to add up.",
                    "label": 0
                },
                {
                    "sent": "It's updating the Q values and ultimately it's making them equal to the solution to the medication.",
                    "label": 1
                },
                {
                    "sent": "So couple of things to note about this.",
                    "label": 0
                },
                {
                    "sent": "Another one is that learning here is off policy, which is to say it's learning about the values of behaving optimally, even though it's not necessarily behaving optimally.",
                    "label": 0
                },
                {
                    "sent": "And that's because right here in the update it does this sort of hypothetical.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to imagine that when I get to the state as prime, I'll take the best action when it actually gets there.",
                    "label": 0
                },
                {
                    "sent": "It might not do that, right?",
                    "label": 1
                },
                {
                    "sent": "It might take an exploratory action, something random, but it's going to do its updates, assuming that that's the that's what it's doing, and so ultimately these values go to the value of the optimal policy, even if that's not the policy that the agent is following.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing I want to point out is the connection to temporal difference learning so.",
                    "label": 0
                },
                {
                    "sent": "This is a really important concept.",
                    "label": 0
                },
                {
                    "sent": "It comes up a lot in in temporal prediction and in the context of Q.",
                    "label": 0
                },
                {
                    "sent": "Learning the temporal difference part is this temporal difference that we have our estimate of the Q value at one point in time and we have our estimate of the value at the next point in time and we relate them to each other and for it to work.",
                    "label": 0
                },
                {
                    "sent": "In MPs we have to jiggle with this one a little bit because we might have gotten a little bit of reward and that caused him to be different.",
                    "label": 0
                },
                {
                    "sent": "But it's this.",
                    "label": 0
                },
                {
                    "sent": "It's this difference between our predictions that's driving the learning.",
                    "label": 0
                },
                {
                    "sent": "OK. Great, I think I need some water.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's been an hour so I can have a look.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, I don't have too much to say about Sarsa, but if you've heard this phrase, you probably want at least know what it is, and it's not so hard to say what it is.",
                    "label": 0
                },
                {
                    "sent": "So sources just like you learning.",
                    "label": 0
                },
                {
                    "sent": "Except we replaced this update.",
                    "label": 0
                },
                {
                    "sent": "Here we had a Max over actions that we're using.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're going to use some action, a prime, which is the action that we actually took in state S prime.",
                    "label": 0
                },
                {
                    "sent": "And so for that to work, we actually have to keep track in terms of experience, not just state action reward next date.",
                    "label": 0
                },
                {
                    "sent": "But we also have to know what the next action we choose is.",
                    "label": 0
                },
                {
                    "sent": "Hence the name, Sarsa State Action Reward state action.",
                    "label": 0
                },
                {
                    "sent": "And so this is actually learning on policy.",
                    "label": 1
                },
                {
                    "sent": "It's actually learning.",
                    "label": 0
                },
                {
                    "sent": "The value of the policy that it's actually following.",
                    "label": 0
                },
                {
                    "sent": "If that happens to be the optimal policy, it will learn the optimal value function, but it might not be.",
                    "label": 0
                },
                {
                    "sent": "It could be something else.",
                    "label": 0
                },
                {
                    "sent": "It's just going to learn the value of that policy.",
                    "label": 0
                },
                {
                    "sent": "One of the things that's nice about this is it interacts really well with other TD methods like TD Lambda, which is a family of algorithms for temporal prediction.",
                    "label": 1
                },
                {
                    "sent": "That interpolates between immediate one step updates, which is what we have here and sort of more eventual like I did a whole sequence of things, and here's how it turned out.",
                    "label": 0
                },
                {
                    "sent": "In the end I can use that to update my prediction as well.",
                    "label": 0
                },
                {
                    "sent": "TD Lambda gives us a way of blending those two things together.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The way that this connects with that is, if you think of essay as being like a thing, we're going essay.",
                    "label": 0
                },
                {
                    "sent": "We get reward, and then we're in a new essay, so it's sort of like a Markov process or or transition process from state action pairs, two state action pairs, and so we can make predictions about if you're into action pair.",
                    "label": 0
                },
                {
                    "sent": "What's that eventual reward going to lead to?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's that's two model free algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk a little bit about model based learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is way more complicated looking than I should have made it so.",
                    "label": 0
                },
                {
                    "sent": "So all I'm saying on this slide is.",
                    "label": 0
                },
                {
                    "sent": "The way that we can do model based learning is we can say, well we were in some state.",
                    "label": 0
                },
                {
                    "sent": "We took some action, we got some reward.",
                    "label": 0
                },
                {
                    "sent": "We got some next state.",
                    "label": 0
                },
                {
                    "sent": "Hey, that's kind of a little sample of information that we can use to estimate the rewards and transitions.",
                    "label": 0
                },
                {
                    "sent": "So the reward we can actually move it a little bit closer to.",
                    "label": 0
                },
                {
                    "sent": "Kind of a temporal differences kind of thing I guess, not temporal, just different, see.",
                    "label": 0
                },
                {
                    "sent": "So that's what this is actually trying to compute is the average reward that we got when we were in this state and took this action.",
                    "label": 0
                },
                {
                    "sent": "And these lines are just trying to do the same thing with transition probabilities, so we're just essentially counting when we were in this state and took this action.",
                    "label": 0
                },
                {
                    "sent": "How many times do we end up in this next date this next date this next date, this next date, and average them out so we have a guess as to what the probability distribution looks like.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a way to actually capture.",
                    "label": 0
                },
                {
                    "sent": "The MVP by just running around and experiencing it.",
                    "label": 0
                },
                {
                    "sent": "And so one thing that we know is that if we actually visit Allstate, action pairs infinitely often, so we just really plow through that MVP over and over and over and over and over and over and over, then and our learning rates decay in the appropriate way.",
                    "label": 0
                },
                {
                    "sent": "Then our estimate of the Q function that we get by solving.",
                    "label": 0
                },
                {
                    "sent": "The MDP for our cartoony transition and reward function goes to the optimal Q function.",
                    "label": 0
                },
                {
                    "sent": "So why is that interest?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eating part of it is because the following is true.",
                    "label": 0
                },
                {
                    "sent": "This is a really useful fact about MVP's and that is if I have a reward function Anna transition function, an approximation of each of those, so some reward function that's kind of close to the reward function and a guess of the transition function.",
                    "label": 0
                },
                {
                    "sent": "That's kind of close to the transition function.",
                    "label": 0
                },
                {
                    "sent": "Then if we solve the Bellman equation for our cartoony are isn't even get a cartoony Q.",
                    "label": 0
                },
                {
                    "sent": "That's also going to be kind of close to the real Q function, so there's a kind of smoothness in this space if you change the parameters of an MVP.",
                    "label": 0
                },
                {
                    "sent": "A little bit, it doesn't change the Q values that much.",
                    "label": 0
                },
                {
                    "sent": "It contains the optimal policy a lot, but it's not going to change the values very much, and so you're going to have a really close estimate of those of those values.",
                    "label": 0
                },
                {
                    "sent": "And in fact it's more than just that.",
                    "label": 0
                },
                {
                    "sent": "It's close.",
                    "label": 0
                },
                {
                    "sent": "We can actually bound how far off we would be if we were, you know, as a function of how far off are reward estimate is how far off are transition estimate is, what our discount factor looks like, how many States and actions we have we can bound.",
                    "label": 0
                },
                {
                    "sent": "How far acting according to QQ is to actually getting optimal reward in that MVP?",
                    "label": 0
                },
                {
                    "sent": "So we can say the reward that we get is going to be within this of the actual optimal, or we might not be perfect, but we're not far from perfect.",
                    "label": 0
                },
                {
                    "sent": "So that's a really valuable result.",
                    "label": 0
                },
                {
                    "sent": "And that that you know sort of justifies this idea that we can use a model based approach which is never in finite time going to.",
                    "label": 0
                },
                {
                    "sent": "Have the actual true MVP learned, but that's OK because it will get close and close is going to be close enough.",
                    "label": 0
                },
                {
                    "sent": "It it it's you, you discover how powerful this property is when you start working with Markov models that don't have this property, so that you can't actually learn anything because you learn for a finite amount of time and you have an estimate and it's off infinitesimally.",
                    "label": 0
                },
                {
                    "sent": "But that gives you completely the wrong answer, so that happens for certain kinds of criteria, but not for these kind of MVP.",
                    "label": 0
                },
                {
                    "sent": "Discounted reward criteria.",
                    "label": 0
                },
                {
                    "sent": "So it's again, it's a, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a nice property.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I want to briefly mention.",
                    "label": 0
                },
                {
                    "sent": "This notion of generalized Oh yes, so so basically that means that the all this is continuous, right?",
                    "label": 0
                },
                {
                    "sent": "But you could still have local minima, right?",
                    "label": 0
                },
                {
                    "sent": "So what do you?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By continuous and well, what I mean is that there are no huge jumps in the in these functions are yes, right?",
                    "label": 0
                },
                {
                    "sent": "So you have that kind of continuity.",
                    "label": 0
                },
                {
                    "sent": "That's right there, but they're not know if we're learning the the reward in the transition function by actually visiting enough of the MDP, then solving this is not going to lead to a local minimum.",
                    "label": 1
                },
                {
                    "sent": "It will actually approximate the real Q star, which is unique, right?",
                    "label": 0
                },
                {
                    "sent": "So given an MVP there is one Q star for that MVP.",
                    "label": 0
                },
                {
                    "sent": "At least in the discounted setting, gamma is less than one.",
                    "label": 0
                },
                {
                    "sent": "OK, then there's one answer and we get.",
                    "label": 0
                },
                {
                    "sent": "We get really close to it.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you if you don't know how far away your reward function is from the actual reward function, how can you estimate the distance of your value?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that's so there's.",
                    "label": 0
                },
                {
                    "sent": "There's two points to make, I think about that one is.",
                    "label": 0
                },
                {
                    "sent": "You can't, right?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "These bounds are not necessarily bounds that the agent knows while acting.",
                    "label": 0
                },
                {
                    "sent": "These are bounds that just hold true of the agents behavior, right?",
                    "label": 1
                },
                {
                    "sent": "So some external watcher of the agent can say, oh, you've been going this long and I can see how far your estimates are, and that's going to tell me how far your actions are going to be in terms of how much reward you're going to get, But the agent itself, yeah, might not know that at all.",
                    "label": 0
                },
                {
                    "sent": "It might be very difficult to tell that unless additional assumptions are made on, say, the range of the possible rewards or.",
                    "label": 0
                },
                {
                    "sent": "Mostly that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, cool.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I wanted to mention this probably briefly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's all this nice mathematical machinery that shows that you can do Q learning type things in MDP's, but MVP's are not.",
                    "label": 0
                },
                {
                    "sent": "The only thing in the universe.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns out that there's a lot of things that really closely related MVP's, and it would be a pain to have to do the math over and again, so Once Upon a time I worked with Chubb separate Roddy who could do that math and I asked him, could you just do this math once in this general way?",
                    "label": 0
                },
                {
                    "sent": "And then I never have to do it, I'll just use your results over and over again with, you know tweaks.",
                    "label": 0
                },
                {
                    "sent": "He was game for that, so we did that and so here's the form that we came up with, so it turns out that if you can think of your decision process as being kind of like an MVP, you're in a state you take an action.",
                    "label": 0
                },
                {
                    "sent": "You get some reward.",
                    "label": 0
                },
                {
                    "sent": "You get transitioned to some next state according to probabilities, and then when you get to that next state you have a lot of different choices about what action to choose next.",
                    "label": 0
                },
                {
                    "sent": "So in the MVP setting we choose the maximum action, but you could choose the minimum action and it's like sort of a cost MDP.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you could choose like a safe action in some sense you know and be.",
                    "label": 0
                },
                {
                    "sent": "Paranoid MDP or something like that like there's all sorts of different ways that you can choose what's a reasonable action to take from the state that you end up in, so let me just generalize across all of them and say that we've got some operator times I that we're going to apply to the RQ values that is a non expansive summary of the values of the actions in the state that we land in.",
                    "label": 0
                },
                {
                    "sent": "For it to be non expansive, that means that when we summarize the value of the state that we land in this S prime for two different Q functions, that the difference between them is less than or equal to.",
                    "label": 0
                },
                {
                    "sent": "The actual difference is the maximum difference between those two functions.",
                    "label": 0
                },
                {
                    "sent": "So it can't get wider apart by doing this summary.",
                    "label": 0
                },
                {
                    "sent": "So it turns out a lot of functions have this very nice property, so min and Max and mean and median and even some things that don't start with M actually satisfy this non expansion property.",
                    "label": 0
                },
                {
                    "sent": "And because they do, that means if we choose actions according to any of these summary operators, we get two very nice things for free.",
                    "label": 0
                },
                {
                    "sent": "One is value, iteration is going to converge, and the other one is that Q learning is going to converge.",
                    "label": 0
                },
                {
                    "sent": "And so this now holds over a really wide range of possible algorithms, and we again ways of picking the action so that we just, you know, with one theorem we can just, you know, rule them all.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me yes.",
                    "label": 0
                },
                {
                    "sent": "I want to give an example but yeah good.",
                    "label": 0
                },
                {
                    "sent": "So so it yeah, that's fair so.",
                    "label": 0
                },
                {
                    "sent": "We change what we mean by optimal, which sounds like cheating.",
                    "label": 0
                },
                {
                    "sent": "Now that I said out loud, it's just the solution to the Bellman equation where the Bellman equation uses that operator, whatever that operator is, and then the the Q learning also uses that operator in this spot, and so this ends up it ends up converging to the solution to that yeah, it would be really weird if it actually converge to the optimal thing even though we're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be strange.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was mostly interested in this because I was I was really interested in multi agent decision-making.",
                    "label": 0
                },
                {
                    "sent": "So how does an agent decide what to do to maximize its reward?",
                    "label": 0
                },
                {
                    "sent": "Given that there's other agents in the world also trying to maximize reward?",
                    "label": 1
                },
                {
                    "sent": "So, so we're talking about sequential sequential environments, and we've expanded the environment so that each state.",
                    "label": 0
                },
                {
                    "sent": "Two agents are going to get to make a decision.",
                    "label": 0
                },
                {
                    "sent": "This is action for me and there's action for my opponent or.",
                    "label": 0
                },
                {
                    "sent": "I don't know friend whatever the other the other critter in my world with me.",
                    "label": 0
                },
                {
                    "sent": "I have a reward function and the other guy has a reward function and the transitions depend on our joint decisions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is another kind of model is very similar to Markov decision processes, but it's not just me trying to maximize reward anymore, there's this other stuff going on in there.",
                    "label": 0
                },
                {
                    "sent": "So let me let me give you a concrete example.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a little gridworld.",
                    "label": 0
                },
                {
                    "sent": "If you were at our ODM last time, then you've seen this already.",
                    "label": 0
                },
                {
                    "sent": "But this is a little little game that we played around with.",
                    "label": 0
                },
                {
                    "sent": "We're actually in the process of getting this hooked up with and getting people to play it.",
                    "label": 0
                },
                {
                    "sent": "Now 'cause we think it's more interesting than it.",
                    "label": 0
                },
                {
                    "sent": "Themes.",
                    "label": 0
                },
                {
                    "sent": "So, alright, so here's the game.",
                    "label": 0
                },
                {
                    "sent": "The game is that that were one of these critters, say the redcritter.",
                    "label": 0
                },
                {
                    "sent": "Mario and we get to go North, South East, West or stay put.",
                    "label": 0
                },
                {
                    "sent": "And the other player, the green player gets to do the same thing at the same time, and then we move if we tried to move into the same place, then a coin gets flipped in.",
                    "label": 0
                },
                {
                    "sent": "One of us makes it, the other one gets bounced back.",
                    "label": 0
                },
                {
                    "sent": "If the red guy makes it to the red goal.",
                    "label": 0
                },
                {
                    "sent": "He gets 100 points if the green guy makes it to the green goal, he gets 100 points, but the game ends at that point.",
                    "label": 0
                },
                {
                    "sent": "So if I get me, the red guy gets to the red gold and green hasn't gotten there yet.",
                    "label": 0
                },
                {
                    "sent": "Green gets nothing.",
                    "label": 0
                },
                {
                    "sent": "So it's a game that sort of could be cooperative.",
                    "label": 0
                },
                {
                    "sent": "We can both win, but it could also be competitive.",
                    "label": 0
                },
                {
                    "sent": "I could win and you could lose, or vice versa.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that sort of clear that we go.",
                    "label": 0
                },
                {
                    "sent": "So let me let me actually simulate this so.",
                    "label": 0
                },
                {
                    "sent": "Let's say they chose to go to that center square on that first move because it was on their shortest paths to their respective goals.",
                    "label": 0
                },
                {
                    "sent": "But Luigi got there in Mario didn't.",
                    "label": 0
                },
                {
                    "sent": "So now they now they get to choose actions.",
                    "label": 0
                },
                {
                    "sent": "And Luigi got to the goal, and now the game ends so I didn't get any points.",
                    "label": 0
                },
                {
                    "sent": "In fact, it costs me a little bit to do all those moves.",
                    "label": 0
                },
                {
                    "sent": "So that makes life feel worthwhile anyway, but this is this is a game in which we'd like to kind of make some decisions.",
                    "label": 0
                },
                {
                    "sent": "We've actually figured out what's a reasonable way to behave in this environment.",
                    "label": 0
                },
                {
                    "sent": "This environment.",
                    "label": 0
                },
                {
                    "sent": "This particular grid is really irritating because.",
                    "label": 0
                },
                {
                    "sent": "There isn't the right answer.",
                    "label": 0
                },
                {
                    "sent": "It it depends what I the best thing for me to do depends what you're going to do.",
                    "label": 0
                },
                {
                    "sent": "And of course the best thing for you to do depends on what I'm going to do, and so you can't just kind of a priority to solve this out.",
                    "label": 0
                },
                {
                    "sent": "This is why we think it's going to be fun to do with people 'cause we think they're going to come up with some way to coordinate and then kind of stick with that.",
                    "label": 0
                },
                {
                    "sent": "But it's not preordained.",
                    "label": 0
                },
                {
                    "sent": "It's not something in the game.",
                    "label": 0
                },
                {
                    "sent": "It actually comes out of the interaction between the players.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we wanted to do decision making in this kind of environment, we can set up a kind of a Bellman equation that says if you know my Q value.",
                    "label": 1
                },
                {
                    "sent": "Given that we're in state S and we choose this joint action, I choose a, you choose B. I'm going to get some immediate reward for that, and then it's going to get added to the reward I get after we transition to whatever new state we're going to go to, and then when we get there, actions are going to be chosen somehow.",
                    "label": 0
                },
                {
                    "sent": "We have to pick that somehow eventually and based on that action that was picked, that's going to give us the respective Q value.",
                    "label": 0
                },
                {
                    "sent": "So so how do we do that?",
                    "label": 0
                },
                {
                    "sent": "How do we decide what's going to happen in the new state?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this is my attempt to kind of making that visual.",
                    "label": 0
                },
                {
                    "sent": "It sort of depends on what you think who you think you're playing with.",
                    "label": 0
                },
                {
                    "sent": "What are the assumptions in the other?",
                    "label": 0
                },
                {
                    "sent": "The other player there and so let me show you how different kinds of assumptions lead to different operators in that slot and can lead to learning algorithms that converge or don't converge.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so we might think that we're playing against some kind of fixed agent that just if this has some way of behaving whenever it's in a state, it has some probability of choosing some action, and we might not know what it is at first.",
                    "label": 0
                },
                {
                    "sent": "But we can play for awhile and we can learn that.",
                    "label": 0
                },
                {
                    "sent": "In that particular case, actually, what we need to learn is what's the probability that my opponent will choose a given action given a state if that opponent really is just some kind of automaton just going to behave a certain way no matter what, then our summary operator is.",
                    "label": 0
                },
                {
                    "sent": "Once we get to some new state S prime, the actions that we're going to choose our well, my opponent is going to choose according to the probability function that defines its behavior, and I'm going to choose to maximize my reward given that.",
                    "label": 0
                },
                {
                    "sent": "OK, we can plug this in as our summary operator and this is kind of nice 'cause it is a non expansion so we get value duration working.",
                    "label": 0
                },
                {
                    "sent": "We get Q learning working.",
                    "label": 0
                },
                {
                    "sent": "And so that gives it.",
                    "label": 0
                },
                {
                    "sent": "But it's a kind of a strong assumption to assume that the other player is stationary, right?",
                    "label": 0
                },
                {
                    "sent": "The other players has a way of choosing probabilities and I cannot impact that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No, this is yeah, this is this is game theory.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is very very in the space of game theory, but it connects up with this decision-making.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's.",
                    "label": 0
                },
                {
                    "sent": "It's game theory.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why do we need new names for everything is a reasonable question.",
                    "label": 0
                },
                {
                    "sent": "I want to say that I'm not solely responsible for misnaming's between different fields.",
                    "label": 0
                },
                {
                    "sent": "But I but I, but I hear you, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that being said, we do something Max game three people use that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's here's another kind of assumption you can make about.",
                    "label": 0
                },
                {
                    "sent": "The opponent.",
                    "label": 0
                },
                {
                    "sent": "Could be that the opponents.",
                    "label": 0
                },
                {
                    "sent": "What the opponent is doing in this environment is trying to be as helpful as possible to me, so when we get to a new state S prime, how are the actions going to be picked?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to pick my action to be the best action that I can pick, given that my opponent is going to choose an action.",
                    "label": 0
                },
                {
                    "sent": "Which is the best action that my opponent can pick for me and then and then we behave according that sort of imagining.",
                    "label": 0
                },
                {
                    "sent": "The other player is a friend.",
                    "label": 1
                },
                {
                    "sent": "This gives us a summary operator.",
                    "label": 0
                },
                {
                    "sent": "This is also a non expansion.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to compute 'cause it really is like just just an MVP.",
                    "label": 1
                },
                {
                    "sent": "At this point we're just kind of together choosing an action.",
                    "label": 0
                },
                {
                    "sent": "It doesn't necessarily lead to very realistic behavior, yeah?",
                    "label": 0
                },
                {
                    "sent": "South Tennessee.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, so another assumption we can make is that the other player is going to do whatever possible to make my life miserable.",
                    "label": 0
                },
                {
                    "sent": "And so in this case, it's sort of like some right when we get to a state.",
                    "label": 0
                },
                {
                    "sent": "What we're imagining is going to happen is I'm going to take the best action I can, given that my opponent is going to try to take the worst action.",
                    "label": 0
                },
                {
                    "sent": "For me.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is a very pessimistic assumption.",
                    "label": 0
                },
                {
                    "sent": "Again, the good news, at least mathematically, is that this gives us a summary operator that is in an expansion, so we can use this in a Q learning setting and actually get the values to workout.",
                    "label": 0
                },
                {
                    "sent": "It's somewhat easy to compute.",
                    "label": 1
                },
                {
                    "sent": "You basically have to solve a linear program each time you do this this update this X here, but but again it's it's nice because it isn't an expansion and things kind of work.",
                    "label": 0
                },
                {
                    "sent": "So why is this sort of a problem?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the good news is.",
                    "label": 1
                },
                {
                    "sent": "R. They're not expansions.",
                    "label": 0
                },
                {
                    "sent": "Q Learningworks valuation works.",
                    "label": 0
                },
                {
                    "sent": "The bad news is that it's sort of a weird set of assumptions to make, so if I assume that the opponent is a friend, this is sort of how imagining things are going to go in this grid.",
                    "label": 0
                },
                {
                    "sent": "So this is a grid where.",
                    "label": 0
                },
                {
                    "sent": "Princess Peach is trying to get to the pink square Mario, trying to get to the Red Square.",
                    "label": 0
                },
                {
                    "sent": "There's walls here, so you might not be able to see so well, so I can't approach from the side.",
                    "label": 0
                },
                {
                    "sent": "I have to approach it from the mouth of the.",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is what Mario is imagining.",
                    "label": 0
                },
                {
                    "sent": "The friendly agent doing.",
                    "label": 0
                },
                {
                    "sent": "That worked out great for us.",
                    "label": 0
                },
                {
                    "sent": "If we make that assumption and we actually behave that way but the opponent doesn't behave the way that we were.",
                    "label": 0
                },
                {
                    "sent": "Assuming this doesn't work at all, right?",
                    "label": 0
                },
                {
                    "sent": "So this ends up being a really bad idea.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, so we could say, well, that's because we were too optimistic.",
                    "label": 0
                },
                {
                    "sent": "Let's just be pessimistic.",
                    "label": 0
                },
                {
                    "sent": "So this is what pessimistic does a reset it back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "This is the optimal behavior Now, because Mario is assuming that.",
                    "label": 0
                },
                {
                    "sent": "Bowser is trying to hurt Mario, which in this case it can do by just staying in front of the goal.",
                    "label": 0
                },
                {
                    "sent": "So the best thing that Mario can do in response to that is just don't move.",
                    "label": 0
                },
                {
                    "sent": "Don't waste energy.",
                    "label": 0
                },
                {
                    "sent": "So this is this is this is what they do forever now.",
                    "label": 0
                },
                {
                    "sent": "Which again might not have been necessary because it could mean that the opponent was making a similar assumption about Mario and now they're both just stuck.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a bummer.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He I just want to show one more version of this game.",
                    "label": 0
                },
                {
                    "sent": "This is this is sort of what Q learning actually does.",
                    "label": 0
                },
                {
                    "sent": "If you put it in this game with the assumption that the I think it's the assumption that the other player is on atomic.",
                    "label": 0
                },
                {
                    "sent": "An automaton, right that is just behaving according to some fixed probabilities, which it's not, 'cause they're both actually Q learners, but we ignore that fact and we run it anyway.",
                    "label": 0
                },
                {
                    "sent": "Papademetriou has this phrase to use an algorithm against hope, like you know that the algorithm not supposed to work but use it anyway, and it actually does something good.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what it finds.",
                    "label": 0
                },
                {
                    "sent": "So Luigi goes into Mario's goal.",
                    "label": 0
                },
                {
                    "sent": "Mario passes by and then jumps into the far corner while Luigi gets goes down to start heading for Luigi's goal.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "They are synchronized.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of neat thing where why did Mario do that?",
                    "label": 0
                },
                {
                    "sent": "It turns out it did that because anything else that it would have done would have left Luigi just sitting in the goal, trying to protect it.",
                    "label": 0
                },
                {
                    "sent": "'cause Luigi has no.",
                    "label": 0
                },
                {
                    "sent": "Incentive to move out of the goal.",
                    "label": 0
                },
                {
                    "sent": "If Mario can get to Mario's goal before Luigi gets to Luigi's goal, so we have to wait until Mario is far enough away from Mario's goal to start moving and it just does that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I don't know what my point is other than just need.",
                    "label": 0
                },
                {
                    "sent": "Again, I don't know what people would do in this.",
                    "label": 0
                },
                {
                    "sent": "We did we put people through a similar kind of game and.",
                    "label": 0
                },
                {
                    "sent": "People are weird.",
                    "label": 0
                },
                {
                    "sent": "That was the high order bit alright.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're starting to experiment now with using ideas.",
                    "label": 0
                },
                {
                    "sent": "This sort of cognitive hierarchy ideas, that sort of say I'm going to imagine that you're the kind of person that imagines that I'm the kind of person that imagines that your random right, and so we go back and forth a number of times until we figure out, yeah, this would be a sensible way for me to behave.",
                    "label": 0
                },
                {
                    "sent": "Given those assumptions, we think that it actually works out really well in a bunch of these games, and maybe is a bit more people like 'cause people definitely do this back and forth reflection thing.",
                    "label": 0
                },
                {
                    "sent": "But we're in the process of working on that now.",
                    "label": 0
                },
                {
                    "sent": "I could I kind of rushed it because I realized that there was a lot on the slide that I didn't want to say so.",
                    "label": 0
                },
                {
                    "sent": "So the the cognitive hierarchy idea says.",
                    "label": 0
                },
                {
                    "sent": "The Level 0 behavior is, let's say, random.",
                    "label": 0
                },
                {
                    "sent": "Level 1 behavior is optimized against level 0 behavior.",
                    "label": 0
                },
                {
                    "sent": "Level 2 behavior is optimized against level one behavior and so forth.",
                    "label": 0
                },
                {
                    "sent": "Back and forth like that and so.",
                    "label": 0
                },
                {
                    "sent": "I think in some of these great games it actually does something sort of sensible, especially if you're optimizing not against just the previous level, but some distribution over the previous levels where the agent will actually.",
                    "label": 0
                },
                {
                    "sent": "Sort of make offers like kind of move in such a way that the other agent has a chance to move in such a way that they can both kind of get to their goals, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but we were not sure, so that's why I didn't want to say that.",
                    "label": 0
                },
                {
                    "sent": "Alright, sorry.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Anyways, stay tuned.",
                    "label": 0
                },
                {
                    "sent": "I'll have results on this soon.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so The upshot for me of this sort of games pieces.",
                    "label": 0
                },
                {
                    "sent": "First of all, we can reuse this this technology for learning in this setting where you're actually learning with multiple agents so that schools good technology if you can reuse it in different ways.",
                    "label": 0
                },
                {
                    "sent": "The other part that I think is really interesting is that the reinforcement learning decision making actually is taking place in the context of a culture, right?",
                    "label": 1
                },
                {
                    "sent": "So if you're acting actually acting with other agents, you need to have some sense of how those other agents are going to behave, and there's different possible answers.",
                    "label": 0
                },
                {
                    "sent": "But we all kind of locked into this system together, and so our learners have to do that as well.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we are at the halfway mark.",
                    "label": 0
                },
                {
                    "sent": "So do we stretch and stuff?",
                    "label": 0
                },
                {
                    "sent": "Let's stretch and stuff.",
                    "label": 0
                }
            ]
        }
    }
}