{
    "id": "ojlujie5wyfxntigy4frqb5fx23lgt3o",
    "title": "Robustness properties of support vector machines and related methods",
    "info": {
        "author": [
            "Andreas Christmann, Department of Mathematics, University of Bayreuth"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mcslw04_christmann_rpsvm/",
    "segmentation": [
        [
            "Talk a little bit about methods from 2 disciplines from machine learning, convex immunization and from statistics robustness.",
            "And we are trying to get these things together.",
            "My talk is has the following questions.",
            "First I start with some notions of our convex optimization, although I know that probably during the last two days you have so much about comics memorization.",
            "I need some notations.",
            "Then I will go to.",
            "Robustness properties of these methods and also to methods that can be useful to use these methods to really huge datasets.",
            "For example, I have a data set from 15 German insurance companies, so with classical algorithms it was impossible to compute these methods for this data set.",
            "OK, so let me start with convex."
        ],
        [
            "Civilization, so I will assume that our risk factors or axis are living in some SpaceX in the D dimensional Euclidean space and our responses say why are living in some space which are a subset of the Euclidean space, both closed or open and empty.",
            "Of course, in the classification case I will assume that the response are minus one or plus one, so no event or event.",
            "And in the Russian case via.",
            "Usually are a big subset of R. There is assume that we have a data set of end points.",
            "XIIIXI coming from the dimension space by I from Y. OK.",
            "The only the only assumption made by these machine learning methods is that the pair XIYI is coming IID from some totally unknown distribution probability distribution P. So P is just an element of all probability distributions on X * Y.",
            "No assumption is made on densities, notions, and assumptions made on, say, normality, skewness, symmetry or something like that.",
            "In contrast to what Professor Davis was talking about before, I will need a loss function to measure how far away my predictions say.",
            "F of XIF is an unknown function, or if you have an intercept term of F of X, I + B is away from our observed response.",
            "Why I so the loss function is denoted by error.",
            "And my main my main two goals are estimating the unknown function F, which is of course if a map of T of our empirical distribution PN and perhaps prediction.",
            "So if I. I have a new vector X knew.",
            "What is my prediction of F head of F of X new OK System azatian based on kernels.",
            "Need of course kernels.",
            "What is the kernel?",
            "A kernel is a map."
        ],
        [
            "X * X two R, and we assume that there is some Hilbert space H and the feature map 5 mapping from X2 H such that the kernel which measures how far away our risk vector X and our risk vector X prime R can also be written down as the scalar product in the Hilbert space of 5X and 5X.",
            "Crime.",
            "OK. Of course, the notion of reproducing kernel, the reproducing kernel Hilbert space, which is a senchal for this talk, is that.",
            "Necessary that you specify the Hilbert space other reproducing kernel Hilbert space itself.",
            "If you specify a kernel, then the reproducing kernel Hilbert space is unique and the other way around.",
            "If you have a reproducing kernel space, then there is a unique reproducing kernel, so in practice people specify a kernel.",
            "And then there is some reproducing kernel Hilbert space.",
            "But for mathematics you need this Hilbert space.",
            "OK, sometimes I need notions of continuous bounded or universal kernels.",
            "Universal kernels are continuous kernels on some compact metric space such that the corresponding Hilbert space is dense in C of X or the continuous functions.",
            "And one classical.",
            "Universal Credit is the RBF kernel is the caution RBF kernel that is mainly the density of a multivariate normal distribution, so X of some quantity times the squared Euclidean distance between X&X prime.",
            "OK, now what is connect Systemization?",
            "Let us denote the risk by just the."
        ],
        [
            "The expectation of the losses.",
            "OK, if you do not know FF available space, one idea would be well minimize that.",
            "But of course you did not make any assumptions on the distribution P, so you cannot solve that.",
            "Well, what is the regular risk?",
            "You just add some penalising quantity Lambda times, squared norm of F in the Hilbert space.",
            "Lambda is some fixed non negative quantity and by PN I denote the empirical measure of an datapoints botnick proposed to estimate F the unknown function by minimizing the regularised empirical risk.",
            "I.",
            "Uh.",
            "We replace.",
            "The right in the regularised risk, the unknown distribution P by the empirical distribution.",
            "PN.",
            "OK, and then we minimize with respect to all functions that Hilbert space.",
            "This approach estimating F has a lot of nice properties.",
            "Why?",
            "Well, first of all, marketing proposed to use a convex function to avoid NP hard problems and that there is a solution and so on and so on.",
            "He proposed to use a regularising quantity Lambda times squared norm of F to minimize the risk of overfitting.",
            "Otherwise, it can happen that the function is just interpolating everything or we is wiggling around.",
            "And the third thing is he proposes not to consider all functions or all measurable functions.",
            "This space is perhaps too big.",
            "He proposed to use Hilbert space, which is still very flexible if you use a universal kernel is dense in C of X, but still it is mathematical.",
            "More convenient to work with that.",
            "People have shown that the solution of that.",
            "Problem has the following representation is just a linear combination of our kernel valuations, F of X XI multiplied by some unknown.",
            "Quantities Alpha I and if Alpha I is unequal to 0 then X is called a support vector because otherwise XI does not contribute to the solution.",
            "If I is 0 then XI did not contribute to the solution.",
            "I will talk more or less on the theoretical problem that is.",
            "Here we have the empirical distribution PN.",
            "The regular theoretical risk would be.",
            "This.",
            "Problem.",
            "It's identical to the regular central risk up to the difference that here we have the empirical distribution and here we have the true distribution.",
            "So let us consider a map T which Maps from the set of all probability distributions on X * Y two Filbert space reproducing kernel Hilbert space H such that T of P is just that solution.",
            "And currently, of course, you can also do everything if you have.",
            "And intercept, but I will mainly focus on the problem without an interceptor.",
            "Well, what are classic?",
            "Can't you just put it into F?",
            "Um?",
            "At plus needs such as another function.",
            "Yeah, in principle yes.",
            "But.",
            "Quantity B sometimes make trouble becausw.",
            "If you only have F, then the solution usually exists as unique, but if you have F&B then can happen that this is not no longer true.",
            "If I usually these two things are separated.",
            "OK, just to give you a short impr."
        ],
        [
            "Action about convex functions used for classification.",
            "Currently, sick regression uses this smooth function Ada boost that decreases exponentially fast support vector machine proposed by Vapnik uses this very simple loss function.",
            "These squares and so on and so on.",
            "Here are some examples for use in the Russian case map mix.",
            "Epson support vector regression uses this loss function, so similar to a V shape.",
            "And there is some connection to the third string method that was described by Professor Davis in the talk before.",
            "But you can also use these queries or smooth version of that OK Huawei robustness of convexity memorization.",
            "Well the only assump."
        ],
        [
            "Made by these people from machine learning is that repair xiy I of random variables are IID from some totally unknown distribution P. Of course, that does not mean that the conditional distribution of Y given X is IID integration case.",
            "Of course, that is usually not the case, but although that simple.",
            "Assumption seems to be not a hard assumption.",
            "I think it is hard assumption why well, independent, of course.",
            "Why should all samples be independent?",
            "But I will not talk about that, but why should all?",
            "All pairs xiy I.",
            "Come from the same distribution.",
            "What happens if, say, you have a distribution P and a distribution Q such that in some matrix the distance between P&Q is small?",
            "And then what happens to your solution T of P -- C FQ is that small or big?",
            "Or to be more complete, per perhaps.",
            "You have a data set of N data points, so your solution is T of PN.",
            "If you would have an infinite sample size, your solution would be T of P. How far away are we solutions?",
            "Is it a smooth transition or not?",
            "What about imprecise data?",
            "What about outliers and what is the impact of the loss functions?",
            "A kernel on your method?",
            "I will try to give a few results to these questions.",
            "Well, I think the.",
            "The key idea behind robustness can be shown by these plots.",
            "The question is if all data points."
        ],
        [
            "Frumpy when you're going to estimate this T of P, but it can happen that you are not able to observe data from P, but only from a distribution.",
            "Say nearby P, say 1 minus epsilon.",
            "Of the data points are coming from P, but a small portion of the data points my come from might be generated from some other distribution, pietila which of course is also unknown.",
            "In our case, of course, two of P is just our unknown function FP Lambda.",
            "If you have a parametric problem, or if you use a linear kernel when you have that kind of picture, so you think that there is some true distribution P. Because you are in a parametric way model, you might think that the true distribution is element of the blue curve.",
            "If you do that in a robust way, you need a neighborhood around that, so perhaps that data point should illustrate that.",
            "Mixture distribution.",
            "1 minus epsilon P plus epsilon pietila.",
            "OK, but if you use a nonparametric?",
            "Goshen RBF kernel.",
            "You have to define neighborhoods not only for distributions belong to that parametric model, but for all distributions because you do not limit yourself to just a parametric model.",
            "So you have to define neighborhoods for examples across their own neighborhood which is also enabled in the topological sense.",
            "For all distributions P and you ask yourself what happens.",
            "If your data are generated, say by that distribution instead of that distribution, but this matter is there a solution quite different or is very similar?",
            "OK, what are usual robustness concepts Hunter proposed to influence function there is a cat over but if."
        ],
        [
            "And a good method should have abounded inference function.",
            "That is the idea of the influence function.",
            "What is the definition of the influence function?",
            "The influence function given some point set?",
            "Alright, some point that given the map T and the distribution P is directional derivative.",
            "So you evaluate T at the mixture distribution and here you do not consider all distributions Petula but only a direct distribution.",
            "There's asset minus.",
            "What you expect under the true model to have P divided by epsilon and then take the limit epsilon to 0.",
            "So it measures the small infinitesimally small amount of mixture.",
            "Sensitivity curve proposed by W 2 two key essay, say final sample, analogon of the inference function that you can write that in that way it just measures the amount of.",
            "Or the impact of one data point in that on your estimate.",
            "Box biased proposed by PJ Huber.",
            "What happens to the bias?",
            "If your distribution is in some epsilon neighborhood around P. The maximum bias.",
            "And finally.",
            "Define."
        ],
        [
            ", breakdown point proposed by Donoho and Tuba.",
            "It matches the smallest contamination when the estimators, so when the estimators worthless so.",
            "Assume that you have a data set S of N data points.",
            "The final sample breakdown point of an estimate as defined in that way.",
            "So it is the minimal quotient M / N or the middle portion of data points.",
            "If you like to say that sets the bias is finite and the bias of course is defined as the following quantity, you compute your quantity for the original data set you compute.",
            "The data set for modified datasets and take the norm and then this is premium and the supremum is taking over all possible samples that can be obtained by replacing any any M of the original data points by arbitrary values in X * 5.",
            "OK, if you are."
        ],
        [
            "Familiar with the.",
            "Robust.",
            "Robustness concepts in parametric models.",
            "You know, M estimators, and here started comparison between MSE majors and kernel based methods from machine learning, so M estimators do not have our organization.",
            "Kernel based estimators does have here the parameter space is finite dimensional.",
            "Here usually the parameter space is infinite dimensional, especially if you use the caution RBF kernel.",
            "We are estimating here usually in parametric parametric regression.",
            "For example linear function.",
            "Here is a more complex function and what is very essential I think is.",
            "The inverse function of an estimator in linear regression, for example, is for fixed point that where you plug in the quantum contamination mess.",
            "A fixed vector in Rd.",
            "It is only a function if you vary that.",
            "For each fixed that where you plug in the direct distribution, the influence function is just a vector.",
            "But here you have for each fixed set.",
            "The case that the inference function itself is a function in the Hilbert space for each fixed set.",
            "So that makes things much more complicated, and you need some methods from function analysis to prove things here, and I just like to show one example.",
            "One of the result."
        ],
        [
            "Here.",
            "For the classification case, assume that the loss function is convex.",
            "But the as a second derivative is positive and continuous and that you have a continuous kernel.",
            "So these assumptions are not hard.",
            "Assume that.",
            "I love the domain.",
            "Weather risk factors are living in is compact or that X is an open or bounded subspace of Rd.",
            "But then we need that the kernel is bounded.",
            "Then we can show that the influence function, the influence function of these Conexus minimization methods saitiev P is our estimate.",
            "FP Lambda exists for all points that and for all probability distributions P. And the inference function has this formula.",
            "It's not so essential that you read the.",
            "The formula itself.",
            "What is interesting is that is a function exactly where you plug in the point contamination mess that.",
            "OK, and all these quantities only depend on that by that quantity.",
            "So I said in the beginning that a good, robust method should have abounded influence function.",
            "How can you bound that?",
            "Well, just look at all these formula and see can we bound that.",
            "Yeah, and you only have to use a loss function such as the first derivative is bounded, then the first effectors bounded.",
            "That measures.",
            "There there is a jewel and use a kernel which is bounded and the feature map is bounded.",
            "Say for example the RBF kernel.",
            "So if you use a loss function which is has a first derivative which is bounded and bounded kernel, then you get a method that is robust in that sense that has a bounded inference function.",
            "Similar thought we also have for the case F + B and it's interesting to compare this influence function with the mallows M estimation linear regression models which has a very similar structure amatrix here upside function down waiting residuals invite space or in the residual space and here weights.",
            "For down grading out as in X space that is also here, of course, that is in the Hilbert space, and that is only in ID.",
            "OK, just to give you an example."
        ],
        [
            "A picture of that quantity which depends on that.",
            "Four corners expression with the RBF kernel.",
            "So it is a function for all points that.",
            "So I just.",
            "Defined that as the .2 -- 2 and I choose a model where the probability for one is high in 98%.",
            "9880 nine 89% sorry so if we observe response which is plus one so a proper observation so that quantity is almost almost identical to zero.",
            "You have to be very careful to take this really small array.",
            "But if you have another improbable observation, say advice equal to minus one, you see that there is only bounded.",
            "And the local impact bound it becausw the first derivative of the loss function of KLR is bounded and the local impact due to the RBF kernel.",
            "OK, we got some other results for the classification.",
            "For example the difference quotient."
        ],
        [
            "The definition of the inference function is uniformly bounded.",
            "Special cases are support vector, machine, kernel regression, Adaboost, least squares and so on.",
            "Also, we can bound the norm of the sensitivity curve and the Max bias and in a uniform way for Russian things are a little bit more complicated.",
            "But well, again we can show that the solution of the regularize theoretical risk exists is unique, and it's consistent under some tail assumptions on Pi will come.",
            "Check on that assumptions in a few minutes.",
            "These assumptions are not technical assumptions.",
            "They are coming from the problem itself.",
            "For the inference function existence that inference function is bounded if again the first derivative of the loss function is bounded, the kernel is bounded.",
            "And we were able to see that there is some interesting relationship between universal consistency and there is consistency that the function that the method is able to learn.",
            "Every function which is measurable and robustness.",
            "And this interesting becausw quite some people are working in understanding stability.",
            "We just have some relationship to two key sensitivity curves in that area and general generalization properties.",
            "So I think our results.",
            "I've also some relationship to this.",
            "Papers OK, just an example.",
            "Whether all that theory works at all.",
            "Some data points.",
            "Epson support vector duration."
        ],
        [
            "This includes.",
            "Performed quite quite good one hour prior is somewhere in the Sky by 1000 I think.",
            "Cause the first derivative of the loss function or the first derivative at least up to one point, is bounded, but that is not true for the square support vector operation.",
            "You can clearly see that that Outlier has a big impact, not only in one point, but also in a region.",
            "OK, why?"
        ],
        [
            "Saying that the Russian case is a little bit more complicated than the classification case.",
            "I think support vector machine contact systemization for the regression case for the regression case.",
            "Most of the classification code for the aggression case are, say non robust post problems.",
            "What do I mean with that?",
            "Say SP Lambda?",
            "Our estimate is defined as the optimal function F in our Hilbert space which minimizes.",
            "The risk the expectation is suspect through P of the loss function plus some regularization quantity.",
            "OK, some people have considered what happens if we replace the square here in the norm by some other quantity, but there is not so important I think from my point of view, let us assume that we are in the regression case and that's the support of the conditional distribution of Y given X an.",
            "That the support of the residuals say y -- F of X is are both an unbounded, otherwise we have not really.",
            "Robustness problem.",
            "So then the story is as follows.",
            "For all epsilon positive there is a distribution Q.",
            "A probability distribution on X * y such that the sum metric.",
            "The distance between P&Q is small.",
            "But The LP risk of our estimate.",
            "But the true solution is of course more negative that's clear and bounded for P. But the same risk.",
            "But evaluated not for P but for Q, although the difference is quite small.",
            "Infinite in finite.",
            "Sorry is in finite.",
            "You just have to.",
            "I say consider a mixture distribution with some Koshy part or something like that.",
            "So I think there is some inherent instability or non robust.",
            "This author context innovation problem and this is not due to the.",
            "Causation part, but due to the expectation part.",
            "And this is also true for some other models in the annals paper of written by James and Carter, there are many more examples for something similar.",
            "So what can we do?",
            "So let us consider what happens if we have an unbounded convex loss function then.",
            "That cannot circumvent that problem for all P for some P yes, but not for all P, why not?",
            "Well, you have to specify the loss function in advance, but then it can happen.",
            "It happens that there is a some bad distribution.",
            "Nearby P, but not identical to P for that.",
            "This happens here, it's finite, and here it's in finite the other way around.",
            "If you have a bounded but non convex function then you have American problems, multiple solutions and so on.",
            "And usually people from machine learning will not accept that.",
            "So I think there is a need for robust alternative for the convex optimization problem and I have no solutions yet for that problem.",
            "I think it is worth to think about whether we should replace the expectation because the expectation makes the trouble by something more robust, say a robust location estimate or robust scale estimate for the distribution of the residuals.",
            "Something like that.",
            "I have no results for that, but I think.",
            "You cannot overcome that problem if you still use the expectation.",
            "OK. No, I switched to robust learning from it."
        ],
        [
            "That is something related to robustness, but also related to.",
            "Huge datasets and how we can compute these methods for really huge datasets.",
            "The problem is if you have.",
            "These methods from convex minimalization you usually cannot source that in the primal space usually use.",
            "Approach to transfer that to the dual program and you have to solve dual program.",
            "But the dual program.",
            "Has the problem that if that you should store a matrix which is quadratic.",
            "But N * N. So if you have 1000 data points, you need a computer which can store a matrix 1000 * 1000.",
            "OK, no problem, but what happens if you have a data set with 1 million 1 million time times 1,000,000 * 8 bytes is quite quite big and.",
            "Um?",
            "The.",
            "Currently available best algorithms to do that usually slow down enormously.",
            "If you have really big datasets, especially if the dual program is not quadratic but really only convex.",
            "Just one example, I used Michael R. Implemented by grouping but proposed by Kathy at all 2002 and 2004.",
            "So for classification.",
            "I simulated data according to this simple model with eight extension variables and some interaction terms.",
            "If you have a data set with only two thousand data points.",
            "It can be done on my PC in around 4 seconds and the cage which is used is small.",
            "If you're 10,000, well, you need already 1 1/2 minute.",
            "If you have 100,000 observations, it is not 10 minutes or something like that is around 10 hours.",
            "Because it is no longer possible to store everything in in the ram of your computer.",
            "But at the end of my talk I will talk about an application I'm involved in from insurance and well, there are 4.6 million customers and so you can think that there's a real problem if I would run that.",
            "It will take several months.",
            "I do not know the exact time because I stopped after two months.",
            "I conjecture it will take on my PC at least between six months and a year.",
            "And of course there is a real problem.",
            "These methods have find theoretical properties, but if we are going to use them in big datasets, we cannot compute that.",
            "So I like to propose something very very simple.",
            "And we will see what are these simple proposal has proboscis properties and whether it can help.",
            "Is a situation where we are no longer able to use the original algorithms.",
            "'cause the data set is too big so.",
            "First thing, we partition the whole data set by random into B destroyed subsets.",
            "I called them."
        ],
        [
            "I've then model each byte by a robust estimate.",
            "And call that estimate, say T of T and B&B is the sample size, which should be approximately equal to N / B and then aggregation and prediction step based on the median.",
            "Because I like to do that in a robust way.",
            "But you can also do that in a.",
            "The classical way by using the mean, say, compute the median of all your estimates or component wise or by multivariate M estimation and so on.",
            "And I'm especially interested also in confidence intervals, people from machine learning usually.",
            "A good in in giving you predictions F head of X.",
            "This decision I'm also interested in getting precision into this for my predictions.",
            "So by just using very old methods from other statistics, if you use the median, you can have a distribution free confidence interval for for the quantity you are interested in for the median just by.",
            "Computing order statistics.",
            "And of course there are connections with the median so hectic, blockwise subsampling, begging, and learning ensembles from bytes, but that is not robust.",
            "OK, from a computational point of view."
        ],
        [
            "It is assumed that we have CCP use my laptop only has one CPU, but in my office I have a machine with at least two CPUs and perhaps you are better situations.",
            "You have CCP, you say.",
            "Say if the original algorithm is of order G1 of MD Anderson, number of observations and D is a number of risk factors you have, then our view with the bytes has.",
            "That kind of order for the computation time, so it's much less same for memory space and for the hard disk doesn't work for the application.",
            "Yes, for the data set with around 4.6 million customers, I use B equal to 17 cause I like to have some nice confidence intervals for.",
            "And the median.",
            "With only two CPUs and I do no longer need half a year, only one week, so one week is still not good.",
            "But if you contrast that with half a year, it's at least some progress.",
            "OK, what can be said?"
        ],
        [
            "About our before kernel methods are will be in principle can also be used to many other methods of course, but let us assume that we have a kernel method, so and that be the number of bytes fixed, then the RB estimator based on the mean is itself a kernel estimator that is not at all clear.",
            "But it turns out that even this is a solution has a really nice.",
            "Solution, so the solution of the RB estimator based on a kernel estimator can be written down as well.",
            "Just a weighted version of the original estimators.",
            "And even you can just count the number of support vectors.",
            "So is quite quite simple from that point of view.",
            "You can also derive bounds for the number of support vectors so."
        ],
        [
            "The number of weights not equal to 0.",
            "The message is with probability tending to one the fraction of support vectors of the RB estimators essentially greater.",
            "Then the average of the base risk for the bytes.",
            "SLP is base risk OK now?"
        ],
        [
            "These methods she seemed to work for huge datasets that can we use with several CPUs and so on.",
            "But what about statistical properties?",
            "What is about the error with consistency?",
            "There's assume that P is fixed, but we have a loss function which is convex.",
            "Of course, convex risk minimization.",
            "The loss function is fixed and convex so that we have universal kernel.",
            "For example the RBF kernel that is the most often used by people in applied science.",
            "In applications and that user kernel estimator, which is good service that it is Elvis consistent, it's able to learn.",
            "Your method is able to learn what happens for the RB version of that estimator.",
            "It's it turns out that our B estimator is also able to learn.",
            "In that sense it's errors consistent, that is.",
            "The LP risk of your estimator.",
            "Converges to the base risk.",
            "In probability, if N increases to Infinity.",
            "But is the base risk is the best you can hope.",
            "The best just in a second.",
            "It is just the.",
            "The information with respect to all measurable functions, not just only two functions in the Hilbert space to all measurable functions.",
            "Yes, you have a question.",
            "Keep the box on his fixed an increase in the elect the number of bytes code Infinity, because I mean otherwise.",
            "You essentially so well.",
            "Let's pretend that each byte now is by self goes to Infinity, so.",
            "I mean, it's kind of it's kind of cold vius if the underlying learner on estimates on your flight is consistent in the average will be consistent.",
            "But like if you well, by technical constraints, have to keep Blacks or speak.",
            "I have not yet the full proof, but I think it should work if the number of bytes is increasing but not too fast.",
            "Say something like log of North.",
            "If the number of pilots is increasing too fast, then I think I will be well perhaps come into trouble cause, well, just give me a very silly counterexample.",
            "If B, the number of bytes is equal to N, then all subsamples.",
            "Are consisting of just one data point.",
            "This cannot work.",
            "This cannot work if one if you have only one data point, you cannot estimate a function, it cannot work.",
            "So I think it is.",
            "At least one partial result that if these six or.",
            "Probably be is increasing, but not too fast.",
            "That should work.",
            "But I have not no solution for the general case, just pushing all the all the owners of the proof back into, well, the estimator on the bike.",
            "So yes, if you have something that's consistent on the bike, the bite size increases as in number of surveys provide thing.",
            "Obviously you get an average over a whole bunch of consistent estimator, so you gotta have something useful.",
            "Just wondering how this compares.",
            "For instance, two methods like online learning where you get the order of one over square root in but right version.",
            "I have no comparisons made up to now.",
            "For me it's not so clear that.",
            "That works in that case for if you say other robust estimates F, not just the mean, say for the median or a robust estimate or an estimator, I have no.",
            "Proof that is really this error is consistent, so there is still work if B is fixed, but sorry I cannot answer your question in OK for the proof.",
            "There's really short.",
            "Of course, if you take the difference between the IP risk of your estimate, the RB estimate and the base risk, there's no negative of course, trust.",
            "Lock that in because the loss function is convex, and because that is a linear combination convex combination, then you can.",
            "A right that in in that way.",
            "And again, just switch."
        ],
        [
            "Integration and summation, and cause all these quantities are average consistent.",
            "Then you immediately have that.",
            "But if you just replace here we mean by the median or an estimator or something else, it is no longer so easy to see that that works.",
            "OK consistency well."
        ],
        [
            "If you use our be based on the median, and if you know that your estimate converges in probability, or almost sure, then same turns out for the RB estimator.",
            "And here are also some simple properties of Arab based on mean, but well, I'm working in the area of robot statistic."
        ],
        [
            "Therefore, I'm also interested in the inference function.",
            "What turns out the influence function of the RB estimator?",
            "Is it if you have a robust estimate, say, currently sick regression?",
            "And if you use that for the RB method, is the inference function still bounded?",
            "And what is it?",
            "Yeah, it turns out it's the same.",
            "So if you use a robust method.",
            "Then it turns out that the RB estimator again is robust in that sense.",
            "OK, the final sample breakdown point.",
            "Consider."
        ],
        [
            "That you have B bytes and at the final sample regular part of your original estimator is denoted by epsilon.",
            "NB star of TB.",
            "OK, the breakdown point of that subsample and then also final sample breakdown point of the estimated by the aggregation step by epsilon B star of new head.",
            "Then you can write down the final sample breakdown point of your estimator.",
            "And that is at least as high as just the product of the final sample breakdown points of your original method and the method you use for the aggregation step.",
            "So if you use mean.",
            "The final sample rate on point is 0, so the final sample breakdown point is not zero, but is still robust.",
            "And that's why you bought if B converges to Infinity, the breakdown point converges to zero and that is 1 argument against what you said.",
            "If B is increasing then you lose robustness in that way.",
            "If you take the median for example, that quantity is approximately 1/2 and then sorry is different.",
            "OK, just five minutes or so for the application I have."
        ],
        [
            "Project with an echo.",
            "Watch now in Bristol.",
            "Formally in Traverse City of Essen on risky differentiation.",
            "High dimensional data structures in our SFP I got a data set from the official.",
            "This is all from 15 German insurance companies.",
            "The data set is approximately 9 gigabyte as a compressed file.",
            "Data from around 4.6 million customers and many many expansion variables.",
            "What are the statistical objectives?",
            "Well, the people from the insurance companies are mainly interested in what they call the pure premium.",
            "The expected claim amount given the vector of risk factors X, but they have also a secondary response."
        ],
        [
            "Variable that is the probability of having at least one claim within one year.",
            "OK, there are many complex dependencies."
        ],
        [
            "Structures in the data set with you reticle information.",
            "There's Berlin, that is the rule area.",
            "We have normal relationships even if you aggregate.",
            "Honestly, we have outliers and so on.",
            "We have here an interesting bump of people around 45 years and so on.",
            "OK, I think it's."
        ],
        [
            "Interesting to use these convex systemization methods, not directly because they will not work if you use just SVM or something like that.",
            "It will not work.",
            "Why not cause only 5% of all the customers had a claim at all in one year, so it's not useful and the people of the insurance companies know that data set very well, so they are.",
            "Interested in getting you information from the data?",
            "Yes, you have question.",
            "Actually, you, naturally speaking two different samples citation.",
            "You should end up with a series of.",
            "More than one.",
            "Um?",
            "That is 1 model I'm talking about.",
            "That is the first response variable they're interested in, but just the expected claim amount.",
            "That is, the probability of having at least one claim.",
            "And so on.",
            "So I propose to use a rough class variable saying whether there is there was no accident.",
            "A small accent, say up to 2000 euro, and so on, and say a big claim amount, about 50,000 euro and so on, and I use just full of total probability to write that down, and I use kernel risk regression to estimate probabilities.",
            "I cannot use support vector machine.",
            "Cause button tivari proof last year that support vector machine cannot estimate all conditional probabilities and that's on support vector regression or something like that for the Russian terms.",
            "And of course I made some splitting into training validation test data, so let's just."
        ],
        [
            "It came out overall for males and females, so young males have a higher risk.",
            "Then young females around, say 30 years they.",
            "Perform more or less equal, but more interesting is if we consider all these quantities, that is, the probability of having at least one claim the probability of having a small claim given at least one claim.",
            "Or what is the expected claim amount given.",
            "You know that there was a small claim and risk factor X and so on.",
            "Just well, I use many variables at the same time, but just to."
        ],
        [
            "Give a few pictures.",
            "Here.",
            "I stratified the results by age of the main user anpa genda Blues for males and red is for females, and you can clearly see say from that.",
            "But young females have a higher probability of having a small claim, say up to 2000 euro.",
            "But young men have a higher probability, producing more expensive claims.",
            "So and the interaction term changes.",
            "Here the red curve is above.",
            "Here's the blue curve is above and was quite interesting.",
            "The people from the insurance company T knew that these curves are conjecture, at least at these curves were decreasing with age of the main user.",
            "But it was new to them that this curve increases, so the probability of having a small claim.",
            "Given that you have at least one claim increases with H. But was quite new to the people from the insurance companies.",
            "Yes, I think you have question I mean.",
            "The 5th.",
            "Applicable to something like one side of aggression?",
            "I mean what you're really after is the distribute the conditional distribution of the claims, and I mean what you're doing is essentially modeling a very heavy tailed distribution by just quantizing it into dreams mean.",
            "When you want to estimate that directly, no, I would not use quantized becausw.",
            "People from the insurance company.",
            "Have some interesting quantize, but you can argue as long as you like.",
            "They are in the end not interested in quantized.",
            "They have to pay the amount of money, say sum of values and the sum is related to the mean, not to quantiles.",
            "So it is from a statistical point of view, perhaps more realistic to model quantiles, but the insurance companies.",
            "Have to pay for the claims and therefore they are interested in the mean becausw of the relationship to the summer.",
            "And Alaska.",
            "Figure."
        ],
        [
            "Usually methods from convex risk minimization only able to give good predictions.",
            "But as a statistician I'm interested in also in having nonparametric confidence intervals and you can clearly see, although I did not model that this simple approaches.",
            "Able to produce as symmetric confidence intervals and that which increases with with the prediction, and this is quite natural, cause the distribution is highly skewed of course.",
            "So for example here.",
            "Or or here.",
            "The median.",
            "The Blue + is not in the sense center of the confidence intervals.",
            "And the width of the confidence intervals increases.",
            "That is quite natural, yes.",
            "Actually claims or.",
            "I mean the figures that you're closing here actually claims versus predicted claims.",
            "That doesn't make sense becausw.",
            "Yes, but it's nevertheless it does not make sense becausw 95% of all people have no claims at all, so the value is exactly equal to 0.",
            "There's only makes sense if you compare that to groups of people, otherwise the residuals are still big becausw 0 is the observation or say something really huge.",
            "But nevertheless the expectation is around baseline off 300 euro.",
            "So you have to compare that in a.",
            "In a more clever way than just computing residuals.",
            "Yes.",
            "This decision is close on this side.",
            "I believe ultimately interested in segmentation of the market.",
            "It's actually.",
            "So what type of models of chemistry be used?",
            "Because they wanted segmentation from rabbit prediction.",
            "OK here I only used a starting point.",
            "Linear kernels, although I know that usually linear kernels are not the best one and then I use caution RBF kernels.",
            "I did not use polynomial kernels, kernels, becausw from my theoretical work with English Sign.",
            "What we know that polynomial kernels are unbounded kernels and therefore they do not offer robustness.",
            "But I like robustness.",
            "And people from the insurance company while I convince them that they should also be interesting robustus becausw, it should not happen that the whole tariff structure should be destroyed by just one extreme value in one one group.",
            "It should be a little bit stable, and of course.",
            "The extreme point should be in, but the way it should be should be bounded should not be unbounded.",
            "So does it all.",
            "Sorry.",
            "Yeah.",
            "Online algorithm for solving.",
            "Yeah, but.",
            "If you use a linear kernel, then the corresponding Hilbert space is not big enough, usually too.",
            "Model high dimension unknown relationships between variables.",
            "You can mainly only use overfit affine hyperplanes.",
            "At the start at the start, just to compare whether the RBF kernel is better.",
            "And of course it was better.",
            "I knew that from.",
            "Just like that.",
            "That's just exploratory data analysis.",
            "If you like, adjust the scales of each of the graph of each of the various independently.",
            "Or I mean, what did you do for that normalization?",
            "Because I mean this is actually wonderful situation where you can do a lot of modeling and probably gain big time by doing good modeling.",
            "Yeah, of course I do time reasons I was not talking about exploratory data analysis.",
            "I use that enterprise miner from data mining.",
            "I use TriNet.",
            "Uh, for for sophisticated kind of making trees alot of trees here I only restricted attention to context memorization, so you only took.",
            "One week sentences no no no.",
            "Uh.",
            "I I.",
            "Due to the size of the data set, I was not able to make say grid search or pattern search or cross validation with the whole data set.",
            "I'm just happy to compute it.",
            "For a few combinations, but what I did is of course I sample, say 5% of the data and then I tried to find good combinations of hyperparameters and then I made some arrangements how these hyperparameters might be modified to work also for the big data set.",
            "Of course I know that it's not the best we can do, but the computation task is a problem.",
            "And what we also did by simulations we try to compare different methods for the determination of the hyper parameters.",
            "We use fine grids, fine grid search, MLM searches, the simplex method, patent search by Mama and Bennett.",
            "We use random search and I think one other method I've forgotten the name now and not for the big data set.",
            "But for many Bond benchmark datasets an for.",
            "Subsamples of the big data set I well I have not.",
            "A big supercomputer, so I cannot do.",
            "That with with the whole data set.",
            "So time consuming.",
            "The main problem is KR.",
            "That is really time consuming even with account if at all proposal.",
            "Factorization sound similar low dimensional subspace approximations?",
            "I mean, that's quite standard, but I have not done that.",
            "I I read the paper by by Barton Tivari and it was quite convincing to me that support vector machines and so on are not able to compute all the conditional probabilities, especially in that situation where the probability.",
            "Of having say a big claim, about 50,000 euro is only around oh .07% oh .07%, and therefore this is independent of whether user is user kernel adjust, logistic regression or whatever other risk minimization procedure that you provided mean.",
            "This is talking about the specific function space, or an approximation thereof, that you might use.",
            "No, I did not mean this is essentially would also would allow you to run rather than having to breakdown things like.",
            "You can get the linear time method in the number of observations.",
            "All you have to do is out of four storage, so there's work by Winston Shine broken fine.",
            "There's work by Brighton coworkers.",
            "Put down that actually works quite nicely.",
            "OK, perhaps we can.",
            "I could discuss a little bit later on.",
            "I just saw a few programs, perhaps the best programs, but perhaps you can give me some advice, but I was not able to really get software for doing something like that for huge datasets, for there are many nice programs for working in that area, say for 2000, but not for millions.",
            "At least that is my impression.",
            "OK.",
            "Sorry I did not do that.",
            "Yes.",
            "Thank you.",
            "Thanks.",
            "So, have you actually found anything in your approach where you could show that you've done better than what you do?",
            "The trick using?",
            "Enterprise miner?",
            "Yeah, enterprise miner.",
            "Wow.",
            "I even got some money from the.",
            "EFG that is similar to NSF in the US?"
        ],
        [
            "So they gave me money to to buy a license of the software package.",
            "With minor, but for that project data enterprise, my dad is not out to be really successful, I use."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk a little bit about methods from 2 disciplines from machine learning, convex immunization and from statistics robustness.",
                    "label": 0
                },
                {
                    "sent": "And we are trying to get these things together.",
                    "label": 0
                },
                {
                    "sent": "My talk is has the following questions.",
                    "label": 0
                },
                {
                    "sent": "First I start with some notions of our convex optimization, although I know that probably during the last two days you have so much about comics memorization.",
                    "label": 0
                },
                {
                    "sent": "I need some notations.",
                    "label": 0
                },
                {
                    "sent": "Then I will go to.",
                    "label": 0
                },
                {
                    "sent": "Robustness properties of these methods and also to methods that can be useful to use these methods to really huge datasets.",
                    "label": 1
                },
                {
                    "sent": "For example, I have a data set from 15 German insurance companies, so with classical algorithms it was impossible to compute these methods for this data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with convex.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Civilization, so I will assume that our risk factors or axis are living in some SpaceX in the D dimensional Euclidean space and our responses say why are living in some space which are a subset of the Euclidean space, both closed or open and empty.",
                    "label": 1
                },
                {
                    "sent": "Of course, in the classification case I will assume that the response are minus one or plus one, so no event or event.",
                    "label": 0
                },
                {
                    "sent": "And in the Russian case via.",
                    "label": 0
                },
                {
                    "sent": "Usually are a big subset of R. There is assume that we have a data set of end points.",
                    "label": 0
                },
                {
                    "sent": "XIIIXI coming from the dimension space by I from Y. OK.",
                    "label": 0
                },
                {
                    "sent": "The only the only assumption made by these machine learning methods is that the pair XIYI is coming IID from some totally unknown distribution probability distribution P. So P is just an element of all probability distributions on X * Y.",
                    "label": 0
                },
                {
                    "sent": "No assumption is made on densities, notions, and assumptions made on, say, normality, skewness, symmetry or something like that.",
                    "label": 0
                },
                {
                    "sent": "In contrast to what Professor Davis was talking about before, I will need a loss function to measure how far away my predictions say.",
                    "label": 0
                },
                {
                    "sent": "F of XIF is an unknown function, or if you have an intercept term of F of X, I + B is away from our observed response.",
                    "label": 1
                },
                {
                    "sent": "Why I so the loss function is denoted by error.",
                    "label": 0
                },
                {
                    "sent": "And my main my main two goals are estimating the unknown function F, which is of course if a map of T of our empirical distribution PN and perhaps prediction.",
                    "label": 0
                },
                {
                    "sent": "So if I. I have a new vector X knew.",
                    "label": 0
                },
                {
                    "sent": "What is my prediction of F head of F of X new OK System azatian based on kernels.",
                    "label": 0
                },
                {
                    "sent": "Need of course kernels.",
                    "label": 0
                },
                {
                    "sent": "What is the kernel?",
                    "label": 0
                },
                {
                    "sent": "A kernel is a map.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "X * X two R, and we assume that there is some Hilbert space H and the feature map 5 mapping from X2 H such that the kernel which measures how far away our risk vector X and our risk vector X prime R can also be written down as the scalar product in the Hilbert space of 5X and 5X.",
                    "label": 1
                },
                {
                    "sent": "Crime.",
                    "label": 0
                },
                {
                    "sent": "OK. Of course, the notion of reproducing kernel, the reproducing kernel Hilbert space, which is a senchal for this talk, is that.",
                    "label": 0
                },
                {
                    "sent": "Necessary that you specify the Hilbert space other reproducing kernel Hilbert space itself.",
                    "label": 0
                },
                {
                    "sent": "If you specify a kernel, then the reproducing kernel Hilbert space is unique and the other way around.",
                    "label": 0
                },
                {
                    "sent": "If you have a reproducing kernel space, then there is a unique reproducing kernel, so in practice people specify a kernel.",
                    "label": 0
                },
                {
                    "sent": "And then there is some reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "But for mathematics you need this Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, sometimes I need notions of continuous bounded or universal kernels.",
                    "label": 0
                },
                {
                    "sent": "Universal kernels are continuous kernels on some compact metric space such that the corresponding Hilbert space is dense in C of X or the continuous functions.",
                    "label": 0
                },
                {
                    "sent": "And one classical.",
                    "label": 0
                },
                {
                    "sent": "Universal Credit is the RBF kernel is the caution RBF kernel that is mainly the density of a multivariate normal distribution, so X of some quantity times the squared Euclidean distance between X&X prime.",
                    "label": 0
                },
                {
                    "sent": "OK, now what is connect Systemization?",
                    "label": 0
                },
                {
                    "sent": "Let us denote the risk by just the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The expectation of the losses.",
                    "label": 0
                },
                {
                    "sent": "OK, if you do not know FF available space, one idea would be well minimize that.",
                    "label": 0
                },
                {
                    "sent": "But of course you did not make any assumptions on the distribution P, so you cannot solve that.",
                    "label": 0
                },
                {
                    "sent": "Well, what is the regular risk?",
                    "label": 0
                },
                {
                    "sent": "You just add some penalising quantity Lambda times, squared norm of F in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "Lambda is some fixed non negative quantity and by PN I denote the empirical measure of an datapoints botnick proposed to estimate F the unknown function by minimizing the regularised empirical risk.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "We replace.",
                    "label": 0
                },
                {
                    "sent": "The right in the regularised risk, the unknown distribution P by the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "PN.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we minimize with respect to all functions that Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "This approach estimating F has a lot of nice properties.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, marketing proposed to use a convex function to avoid NP hard problems and that there is a solution and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "He proposed to use a regularising quantity Lambda times squared norm of F to minimize the risk of overfitting.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, it can happen that the function is just interpolating everything or we is wiggling around.",
                    "label": 0
                },
                {
                    "sent": "And the third thing is he proposes not to consider all functions or all measurable functions.",
                    "label": 0
                },
                {
                    "sent": "This space is perhaps too big.",
                    "label": 0
                },
                {
                    "sent": "He proposed to use Hilbert space, which is still very flexible if you use a universal kernel is dense in C of X, but still it is mathematical.",
                    "label": 0
                },
                {
                    "sent": "More convenient to work with that.",
                    "label": 0
                },
                {
                    "sent": "People have shown that the solution of that.",
                    "label": 0
                },
                {
                    "sent": "Problem has the following representation is just a linear combination of our kernel valuations, F of X XI multiplied by some unknown.",
                    "label": 0
                },
                {
                    "sent": "Quantities Alpha I and if Alpha I is unequal to 0 then X is called a support vector because otherwise XI does not contribute to the solution.",
                    "label": 0
                },
                {
                    "sent": "If I is 0 then XI did not contribute to the solution.",
                    "label": 0
                },
                {
                    "sent": "I will talk more or less on the theoretical problem that is.",
                    "label": 0
                },
                {
                    "sent": "Here we have the empirical distribution PN.",
                    "label": 0
                },
                {
                    "sent": "The regular theoretical risk would be.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "It's identical to the regular central risk up to the difference that here we have the empirical distribution and here we have the true distribution.",
                    "label": 0
                },
                {
                    "sent": "So let us consider a map T which Maps from the set of all probability distributions on X * Y two Filbert space reproducing kernel Hilbert space H such that T of P is just that solution.",
                    "label": 0
                },
                {
                    "sent": "And currently, of course, you can also do everything if you have.",
                    "label": 0
                },
                {
                    "sent": "And intercept, but I will mainly focus on the problem without an interceptor.",
                    "label": 0
                },
                {
                    "sent": "Well, what are classic?",
                    "label": 0
                },
                {
                    "sent": "Can't you just put it into F?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "At plus needs such as another function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in principle yes.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "Quantity B sometimes make trouble becausw.",
                    "label": 0
                },
                {
                    "sent": "If you only have F, then the solution usually exists as unique, but if you have F&B then can happen that this is not no longer true.",
                    "label": 0
                },
                {
                    "sent": "If I usually these two things are separated.",
                    "label": 0
                },
                {
                    "sent": "OK, just to give you a short impr.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action about convex functions used for classification.",
                    "label": 0
                },
                {
                    "sent": "Currently, sick regression uses this smooth function Ada boost that decreases exponentially fast support vector machine proposed by Vapnik uses this very simple loss function.",
                    "label": 0
                },
                {
                    "sent": "These squares and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "Here are some examples for use in the Russian case map mix.",
                    "label": 0
                },
                {
                    "sent": "Epson support vector regression uses this loss function, so similar to a V shape.",
                    "label": 0
                },
                {
                    "sent": "And there is some connection to the third string method that was described by Professor Davis in the talk before.",
                    "label": 0
                },
                {
                    "sent": "But you can also use these queries or smooth version of that OK Huawei robustness of convexity memorization.",
                    "label": 0
                },
                {
                    "sent": "Well the only assump.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Made by these people from machine learning is that repair xiy I of random variables are IID from some totally unknown distribution P. Of course, that does not mean that the conditional distribution of Y given X is IID integration case.",
                    "label": 0
                },
                {
                    "sent": "Of course, that is usually not the case, but although that simple.",
                    "label": 0
                },
                {
                    "sent": "Assumption seems to be not a hard assumption.",
                    "label": 0
                },
                {
                    "sent": "I think it is hard assumption why well, independent, of course.",
                    "label": 0
                },
                {
                    "sent": "Why should all samples be independent?",
                    "label": 0
                },
                {
                    "sent": "But I will not talk about that, but why should all?",
                    "label": 0
                },
                {
                    "sent": "All pairs xiy I.",
                    "label": 0
                },
                {
                    "sent": "Come from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "What happens if, say, you have a distribution P and a distribution Q such that in some matrix the distance between P&Q is small?",
                    "label": 0
                },
                {
                    "sent": "And then what happens to your solution T of P -- C FQ is that small or big?",
                    "label": 0
                },
                {
                    "sent": "Or to be more complete, per perhaps.",
                    "label": 0
                },
                {
                    "sent": "You have a data set of N data points, so your solution is T of PN.",
                    "label": 0
                },
                {
                    "sent": "If you would have an infinite sample size, your solution would be T of P. How far away are we solutions?",
                    "label": 0
                },
                {
                    "sent": "Is it a smooth transition or not?",
                    "label": 0
                },
                {
                    "sent": "What about imprecise data?",
                    "label": 0
                },
                {
                    "sent": "What about outliers and what is the impact of the loss functions?",
                    "label": 0
                },
                {
                    "sent": "A kernel on your method?",
                    "label": 0
                },
                {
                    "sent": "I will try to give a few results to these questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I think the.",
                    "label": 0
                },
                {
                    "sent": "The key idea behind robustness can be shown by these plots.",
                    "label": 0
                },
                {
                    "sent": "The question is if all data points.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Frumpy when you're going to estimate this T of P, but it can happen that you are not able to observe data from P, but only from a distribution.",
                    "label": 0
                },
                {
                    "sent": "Say nearby P, say 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "Of the data points are coming from P, but a small portion of the data points my come from might be generated from some other distribution, pietila which of course is also unknown.",
                    "label": 0
                },
                {
                    "sent": "In our case, of course, two of P is just our unknown function FP Lambda.",
                    "label": 0
                },
                {
                    "sent": "If you have a parametric problem, or if you use a linear kernel when you have that kind of picture, so you think that there is some true distribution P. Because you are in a parametric way model, you might think that the true distribution is element of the blue curve.",
                    "label": 0
                },
                {
                    "sent": "If you do that in a robust way, you need a neighborhood around that, so perhaps that data point should illustrate that.",
                    "label": 0
                },
                {
                    "sent": "Mixture distribution.",
                    "label": 0
                },
                {
                    "sent": "1 minus epsilon P plus epsilon pietila.",
                    "label": 0
                },
                {
                    "sent": "OK, but if you use a nonparametric?",
                    "label": 0
                },
                {
                    "sent": "Goshen RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "You have to define neighborhoods not only for distributions belong to that parametric model, but for all distributions because you do not limit yourself to just a parametric model.",
                    "label": 0
                },
                {
                    "sent": "So you have to define neighborhoods for examples across their own neighborhood which is also enabled in the topological sense.",
                    "label": 0
                },
                {
                    "sent": "For all distributions P and you ask yourself what happens.",
                    "label": 1
                },
                {
                    "sent": "If your data are generated, say by that distribution instead of that distribution, but this matter is there a solution quite different or is very similar?",
                    "label": 0
                },
                {
                    "sent": "OK, what are usual robustness concepts Hunter proposed to influence function there is a cat over but if.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a good method should have abounded inference function.",
                    "label": 0
                },
                {
                    "sent": "That is the idea of the influence function.",
                    "label": 0
                },
                {
                    "sent": "What is the definition of the influence function?",
                    "label": 0
                },
                {
                    "sent": "The influence function given some point set?",
                    "label": 0
                },
                {
                    "sent": "Alright, some point that given the map T and the distribution P is directional derivative.",
                    "label": 0
                },
                {
                    "sent": "So you evaluate T at the mixture distribution and here you do not consider all distributions Petula but only a direct distribution.",
                    "label": 0
                },
                {
                    "sent": "There's asset minus.",
                    "label": 0
                },
                {
                    "sent": "What you expect under the true model to have P divided by epsilon and then take the limit epsilon to 0.",
                    "label": 0
                },
                {
                    "sent": "So it measures the small infinitesimally small amount of mixture.",
                    "label": 0
                },
                {
                    "sent": "Sensitivity curve proposed by W 2 two key essay, say final sample, analogon of the inference function that you can write that in that way it just measures the amount of.",
                    "label": 0
                },
                {
                    "sent": "Or the impact of one data point in that on your estimate.",
                    "label": 0
                },
                {
                    "sent": "Box biased proposed by PJ Huber.",
                    "label": 0
                },
                {
                    "sent": "What happens to the bias?",
                    "label": 0
                },
                {
                    "sent": "If your distribution is in some epsilon neighborhood around P. The maximum bias.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "Define.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ", breakdown point proposed by Donoho and Tuba.",
                    "label": 0
                },
                {
                    "sent": "It matches the smallest contamination when the estimators, so when the estimators worthless so.",
                    "label": 0
                },
                {
                    "sent": "Assume that you have a data set S of N data points.",
                    "label": 0
                },
                {
                    "sent": "The final sample breakdown point of an estimate as defined in that way.",
                    "label": 0
                },
                {
                    "sent": "So it is the minimal quotient M / N or the middle portion of data points.",
                    "label": 0
                },
                {
                    "sent": "If you like to say that sets the bias is finite and the bias of course is defined as the following quantity, you compute your quantity for the original data set you compute.",
                    "label": 0
                },
                {
                    "sent": "The data set for modified datasets and take the norm and then this is premium and the supremum is taking over all possible samples that can be obtained by replacing any any M of the original data points by arbitrary values in X * 5.",
                    "label": 0
                },
                {
                    "sent": "OK, if you are.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Familiar with the.",
                    "label": 0
                },
                {
                    "sent": "Robust.",
                    "label": 0
                },
                {
                    "sent": "Robustness concepts in parametric models.",
                    "label": 0
                },
                {
                    "sent": "You know, M estimators, and here started comparison between MSE majors and kernel based methods from machine learning, so M estimators do not have our organization.",
                    "label": 0
                },
                {
                    "sent": "Kernel based estimators does have here the parameter space is finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "Here usually the parameter space is infinite dimensional, especially if you use the caution RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "We are estimating here usually in parametric parametric regression.",
                    "label": 0
                },
                {
                    "sent": "For example linear function.",
                    "label": 0
                },
                {
                    "sent": "Here is a more complex function and what is very essential I think is.",
                    "label": 0
                },
                {
                    "sent": "The inverse function of an estimator in linear regression, for example, is for fixed point that where you plug in the quantum contamination mess.",
                    "label": 0
                },
                {
                    "sent": "A fixed vector in Rd.",
                    "label": 1
                },
                {
                    "sent": "It is only a function if you vary that.",
                    "label": 0
                },
                {
                    "sent": "For each fixed that where you plug in the direct distribution, the influence function is just a vector.",
                    "label": 0
                },
                {
                    "sent": "But here you have for each fixed set.",
                    "label": 0
                },
                {
                    "sent": "The case that the inference function itself is a function in the Hilbert space for each fixed set.",
                    "label": 0
                },
                {
                    "sent": "So that makes things much more complicated, and you need some methods from function analysis to prove things here, and I just like to show one example.",
                    "label": 0
                },
                {
                    "sent": "One of the result.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "For the classification case, assume that the loss function is convex.",
                    "label": 0
                },
                {
                    "sent": "But the as a second derivative is positive and continuous and that you have a continuous kernel.",
                    "label": 0
                },
                {
                    "sent": "So these assumptions are not hard.",
                    "label": 0
                },
                {
                    "sent": "Assume that.",
                    "label": 0
                },
                {
                    "sent": "I love the domain.",
                    "label": 0
                },
                {
                    "sent": "Weather risk factors are living in is compact or that X is an open or bounded subspace of Rd.",
                    "label": 0
                },
                {
                    "sent": "But then we need that the kernel is bounded.",
                    "label": 0
                },
                {
                    "sent": "Then we can show that the influence function, the influence function of these Conexus minimization methods saitiev P is our estimate.",
                    "label": 0
                },
                {
                    "sent": "FP Lambda exists for all points that and for all probability distributions P. And the inference function has this formula.",
                    "label": 0
                },
                {
                    "sent": "It's not so essential that you read the.",
                    "label": 0
                },
                {
                    "sent": "The formula itself.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is that is a function exactly where you plug in the point contamination mess that.",
                    "label": 0
                },
                {
                    "sent": "OK, and all these quantities only depend on that by that quantity.",
                    "label": 0
                },
                {
                    "sent": "So I said in the beginning that a good, robust method should have abounded influence function.",
                    "label": 0
                },
                {
                    "sent": "How can you bound that?",
                    "label": 0
                },
                {
                    "sent": "Well, just look at all these formula and see can we bound that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and you only have to use a loss function such as the first derivative is bounded, then the first effectors bounded.",
                    "label": 0
                },
                {
                    "sent": "That measures.",
                    "label": 0
                },
                {
                    "sent": "There there is a jewel and use a kernel which is bounded and the feature map is bounded.",
                    "label": 0
                },
                {
                    "sent": "Say for example the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "So if you use a loss function which is has a first derivative which is bounded and bounded kernel, then you get a method that is robust in that sense that has a bounded inference function.",
                    "label": 0
                },
                {
                    "sent": "Similar thought we also have for the case F + B and it's interesting to compare this influence function with the mallows M estimation linear regression models which has a very similar structure amatrix here upside function down waiting residuals invite space or in the residual space and here weights.",
                    "label": 1
                },
                {
                    "sent": "For down grading out as in X space that is also here, of course, that is in the Hilbert space, and that is only in ID.",
                    "label": 0
                },
                {
                    "sent": "OK, just to give you an example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A picture of that quantity which depends on that.",
                    "label": 0
                },
                {
                    "sent": "Four corners expression with the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "So it is a function for all points that.",
                    "label": 0
                },
                {
                    "sent": "So I just.",
                    "label": 0
                },
                {
                    "sent": "Defined that as the .2 -- 2 and I choose a model where the probability for one is high in 98%.",
                    "label": 0
                },
                {
                    "sent": "9880 nine 89% sorry so if we observe response which is plus one so a proper observation so that quantity is almost almost identical to zero.",
                    "label": 0
                },
                {
                    "sent": "You have to be very careful to take this really small array.",
                    "label": 0
                },
                {
                    "sent": "But if you have another improbable observation, say advice equal to minus one, you see that there is only bounded.",
                    "label": 0
                },
                {
                    "sent": "And the local impact bound it becausw the first derivative of the loss function of KLR is bounded and the local impact due to the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, we got some other results for the classification.",
                    "label": 0
                },
                {
                    "sent": "For example the difference quotient.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The definition of the inference function is uniformly bounded.",
                    "label": 1
                },
                {
                    "sent": "Special cases are support vector, machine, kernel regression, Adaboost, least squares and so on.",
                    "label": 0
                },
                {
                    "sent": "Also, we can bound the norm of the sensitivity curve and the Max bias and in a uniform way for Russian things are a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "But well, again we can show that the solution of the regularize theoretical risk exists is unique, and it's consistent under some tail assumptions on Pi will come.",
                    "label": 0
                },
                {
                    "sent": "Check on that assumptions in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "These assumptions are not technical assumptions.",
                    "label": 0
                },
                {
                    "sent": "They are coming from the problem itself.",
                    "label": 0
                },
                {
                    "sent": "For the inference function existence that inference function is bounded if again the first derivative of the loss function is bounded, the kernel is bounded.",
                    "label": 0
                },
                {
                    "sent": "And we were able to see that there is some interesting relationship between universal consistency and there is consistency that the function that the method is able to learn.",
                    "label": 0
                },
                {
                    "sent": "Every function which is measurable and robustness.",
                    "label": 0
                },
                {
                    "sent": "And this interesting becausw quite some people are working in understanding stability.",
                    "label": 0
                },
                {
                    "sent": "We just have some relationship to two key sensitivity curves in that area and general generalization properties.",
                    "label": 0
                },
                {
                    "sent": "So I think our results.",
                    "label": 0
                },
                {
                    "sent": "I've also some relationship to this.",
                    "label": 0
                },
                {
                    "sent": "Papers OK, just an example.",
                    "label": 0
                },
                {
                    "sent": "Whether all that theory works at all.",
                    "label": 0
                },
                {
                    "sent": "Some data points.",
                    "label": 0
                },
                {
                    "sent": "Epson support vector duration.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This includes.",
                    "label": 0
                },
                {
                    "sent": "Performed quite quite good one hour prior is somewhere in the Sky by 1000 I think.",
                    "label": 0
                },
                {
                    "sent": "Cause the first derivative of the loss function or the first derivative at least up to one point, is bounded, but that is not true for the square support vector operation.",
                    "label": 0
                },
                {
                    "sent": "You can clearly see that that Outlier has a big impact, not only in one point, but also in a region.",
                    "label": 0
                },
                {
                    "sent": "OK, why?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saying that the Russian case is a little bit more complicated than the classification case.",
                    "label": 0
                },
                {
                    "sent": "I think support vector machine contact systemization for the regression case for the regression case.",
                    "label": 0
                },
                {
                    "sent": "Most of the classification code for the aggression case are, say non robust post problems.",
                    "label": 1
                },
                {
                    "sent": "What do I mean with that?",
                    "label": 0
                },
                {
                    "sent": "Say SP Lambda?",
                    "label": 0
                },
                {
                    "sent": "Our estimate is defined as the optimal function F in our Hilbert space which minimizes.",
                    "label": 1
                },
                {
                    "sent": "The risk the expectation is suspect through P of the loss function plus some regularization quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, some people have considered what happens if we replace the square here in the norm by some other quantity, but there is not so important I think from my point of view, let us assume that we are in the regression case and that's the support of the conditional distribution of Y given X an.",
                    "label": 0
                },
                {
                    "sent": "That the support of the residuals say y -- F of X is are both an unbounded, otherwise we have not really.",
                    "label": 0
                },
                {
                    "sent": "Robustness problem.",
                    "label": 0
                },
                {
                    "sent": "So then the story is as follows.",
                    "label": 0
                },
                {
                    "sent": "For all epsilon positive there is a distribution Q.",
                    "label": 0
                },
                {
                    "sent": "A probability distribution on X * y such that the sum metric.",
                    "label": 0
                },
                {
                    "sent": "The distance between P&Q is small.",
                    "label": 0
                },
                {
                    "sent": "But The LP risk of our estimate.",
                    "label": 0
                },
                {
                    "sent": "But the true solution is of course more negative that's clear and bounded for P. But the same risk.",
                    "label": 0
                },
                {
                    "sent": "But evaluated not for P but for Q, although the difference is quite small.",
                    "label": 0
                },
                {
                    "sent": "Infinite in finite.",
                    "label": 0
                },
                {
                    "sent": "Sorry is in finite.",
                    "label": 0
                },
                {
                    "sent": "You just have to.",
                    "label": 0
                },
                {
                    "sent": "I say consider a mixture distribution with some Koshy part or something like that.",
                    "label": 0
                },
                {
                    "sent": "So I think there is some inherent instability or non robust.",
                    "label": 0
                },
                {
                    "sent": "This author context innovation problem and this is not due to the.",
                    "label": 0
                },
                {
                    "sent": "Causation part, but due to the expectation part.",
                    "label": 1
                },
                {
                    "sent": "And this is also true for some other models in the annals paper of written by James and Carter, there are many more examples for something similar.",
                    "label": 1
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "So let us consider what happens if we have an unbounded convex loss function then.",
                    "label": 0
                },
                {
                    "sent": "That cannot circumvent that problem for all P for some P yes, but not for all P, why not?",
                    "label": 1
                },
                {
                    "sent": "Well, you have to specify the loss function in advance, but then it can happen.",
                    "label": 0
                },
                {
                    "sent": "It happens that there is a some bad distribution.",
                    "label": 0
                },
                {
                    "sent": "Nearby P, but not identical to P for that.",
                    "label": 0
                },
                {
                    "sent": "This happens here, it's finite, and here it's in finite the other way around.",
                    "label": 1
                },
                {
                    "sent": "If you have a bounded but non convex function then you have American problems, multiple solutions and so on.",
                    "label": 0
                },
                {
                    "sent": "And usually people from machine learning will not accept that.",
                    "label": 1
                },
                {
                    "sent": "So I think there is a need for robust alternative for the convex optimization problem and I have no solutions yet for that problem.",
                    "label": 0
                },
                {
                    "sent": "I think it is worth to think about whether we should replace the expectation because the expectation makes the trouble by something more robust, say a robust location estimate or robust scale estimate for the distribution of the residuals.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "I have no results for that, but I think.",
                    "label": 0
                },
                {
                    "sent": "You cannot overcome that problem if you still use the expectation.",
                    "label": 0
                },
                {
                    "sent": "OK. No, I switched to robust learning from it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is something related to robustness, but also related to.",
                    "label": 0
                },
                {
                    "sent": "Huge datasets and how we can compute these methods for really huge datasets.",
                    "label": 0
                },
                {
                    "sent": "The problem is if you have.",
                    "label": 0
                },
                {
                    "sent": "These methods from convex minimalization you usually cannot source that in the primal space usually use.",
                    "label": 0
                },
                {
                    "sent": "Approach to transfer that to the dual program and you have to solve dual program.",
                    "label": 0
                },
                {
                    "sent": "But the dual program.",
                    "label": 0
                },
                {
                    "sent": "Has the problem that if that you should store a matrix which is quadratic.",
                    "label": 0
                },
                {
                    "sent": "But N * N. So if you have 1000 data points, you need a computer which can store a matrix 1000 * 1000.",
                    "label": 0
                },
                {
                    "sent": "OK, no problem, but what happens if you have a data set with 1 million 1 million time times 1,000,000 * 8 bytes is quite quite big and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Currently available best algorithms to do that usually slow down enormously.",
                    "label": 0
                },
                {
                    "sent": "If you have really big datasets, especially if the dual program is not quadratic but really only convex.",
                    "label": 0
                },
                {
                    "sent": "Just one example, I used Michael R. Implemented by grouping but proposed by Kathy at all 2002 and 2004.",
                    "label": 0
                },
                {
                    "sent": "So for classification.",
                    "label": 0
                },
                {
                    "sent": "I simulated data according to this simple model with eight extension variables and some interaction terms.",
                    "label": 0
                },
                {
                    "sent": "If you have a data set with only two thousand data points.",
                    "label": 0
                },
                {
                    "sent": "It can be done on my PC in around 4 seconds and the cage which is used is small.",
                    "label": 0
                },
                {
                    "sent": "If you're 10,000, well, you need already 1 1/2 minute.",
                    "label": 0
                },
                {
                    "sent": "If you have 100,000 observations, it is not 10 minutes or something like that is around 10 hours.",
                    "label": 0
                },
                {
                    "sent": "Because it is no longer possible to store everything in in the ram of your computer.",
                    "label": 0
                },
                {
                    "sent": "But at the end of my talk I will talk about an application I'm involved in from insurance and well, there are 4.6 million customers and so you can think that there's a real problem if I would run that.",
                    "label": 0
                },
                {
                    "sent": "It will take several months.",
                    "label": 0
                },
                {
                    "sent": "I do not know the exact time because I stopped after two months.",
                    "label": 0
                },
                {
                    "sent": "I conjecture it will take on my PC at least between six months and a year.",
                    "label": 0
                },
                {
                    "sent": "And of course there is a real problem.",
                    "label": 0
                },
                {
                    "sent": "These methods have find theoretical properties, but if we are going to use them in big datasets, we cannot compute that.",
                    "label": 0
                },
                {
                    "sent": "So I like to propose something very very simple.",
                    "label": 0
                },
                {
                    "sent": "And we will see what are these simple proposal has proboscis properties and whether it can help.",
                    "label": 0
                },
                {
                    "sent": "Is a situation where we are no longer able to use the original algorithms.",
                    "label": 0
                },
                {
                    "sent": "'cause the data set is too big so.",
                    "label": 0
                },
                {
                    "sent": "First thing, we partition the whole data set by random into B destroyed subsets.",
                    "label": 0
                },
                {
                    "sent": "I called them.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've then model each byte by a robust estimate.",
                    "label": 0
                },
                {
                    "sent": "And call that estimate, say T of T and B&B is the sample size, which should be approximately equal to N / B and then aggregation and prediction step based on the median.",
                    "label": 1
                },
                {
                    "sent": "Because I like to do that in a robust way.",
                    "label": 0
                },
                {
                    "sent": "But you can also do that in a.",
                    "label": 0
                },
                {
                    "sent": "The classical way by using the mean, say, compute the median of all your estimates or component wise or by multivariate M estimation and so on.",
                    "label": 1
                },
                {
                    "sent": "And I'm especially interested also in confidence intervals, people from machine learning usually.",
                    "label": 0
                },
                {
                    "sent": "A good in in giving you predictions F head of X.",
                    "label": 0
                },
                {
                    "sent": "This decision I'm also interested in getting precision into this for my predictions.",
                    "label": 0
                },
                {
                    "sent": "So by just using very old methods from other statistics, if you use the median, you can have a distribution free confidence interval for for the quantity you are interested in for the median just by.",
                    "label": 0
                },
                {
                    "sent": "Computing order statistics.",
                    "label": 1
                },
                {
                    "sent": "And of course there are connections with the median so hectic, blockwise subsampling, begging, and learning ensembles from bytes, but that is not robust.",
                    "label": 0
                },
                {
                    "sent": "OK, from a computational point of view.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is assumed that we have CCP use my laptop only has one CPU, but in my office I have a machine with at least two CPUs and perhaps you are better situations.",
                    "label": 0
                },
                {
                    "sent": "You have CCP, you say.",
                    "label": 0
                },
                {
                    "sent": "Say if the original algorithm is of order G1 of MD Anderson, number of observations and D is a number of risk factors you have, then our view with the bytes has.",
                    "label": 0
                },
                {
                    "sent": "That kind of order for the computation time, so it's much less same for memory space and for the hard disk doesn't work for the application.",
                    "label": 1
                },
                {
                    "sent": "Yes, for the data set with around 4.6 million customers, I use B equal to 17 cause I like to have some nice confidence intervals for.",
                    "label": 0
                },
                {
                    "sent": "And the median.",
                    "label": 0
                },
                {
                    "sent": "With only two CPUs and I do no longer need half a year, only one week, so one week is still not good.",
                    "label": 0
                },
                {
                    "sent": "But if you contrast that with half a year, it's at least some progress.",
                    "label": 0
                },
                {
                    "sent": "OK, what can be said?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About our before kernel methods are will be in principle can also be used to many other methods of course, but let us assume that we have a kernel method, so and that be the number of bytes fixed, then the RB estimator based on the mean is itself a kernel estimator that is not at all clear.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that even this is a solution has a really nice.",
                    "label": 0
                },
                {
                    "sent": "Solution, so the solution of the RB estimator based on a kernel estimator can be written down as well.",
                    "label": 0
                },
                {
                    "sent": "Just a weighted version of the original estimators.",
                    "label": 0
                },
                {
                    "sent": "And even you can just count the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "So is quite quite simple from that point of view.",
                    "label": 0
                },
                {
                    "sent": "You can also derive bounds for the number of support vectors so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The number of weights not equal to 0.",
                    "label": 1
                },
                {
                    "sent": "The message is with probability tending to one the fraction of support vectors of the RB estimators essentially greater.",
                    "label": 1
                },
                {
                    "sent": "Then the average of the base risk for the bytes.",
                    "label": 0
                },
                {
                    "sent": "SLP is base risk OK now?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These methods she seemed to work for huge datasets that can we use with several CPUs and so on.",
                    "label": 0
                },
                {
                    "sent": "But what about statistical properties?",
                    "label": 0
                },
                {
                    "sent": "What is about the error with consistency?",
                    "label": 0
                },
                {
                    "sent": "There's assume that P is fixed, but we have a loss function which is convex.",
                    "label": 0
                },
                {
                    "sent": "Of course, convex risk minimization.",
                    "label": 0
                },
                {
                    "sent": "The loss function is fixed and convex so that we have universal kernel.",
                    "label": 1
                },
                {
                    "sent": "For example the RBF kernel that is the most often used by people in applied science.",
                    "label": 0
                },
                {
                    "sent": "In applications and that user kernel estimator, which is good service that it is Elvis consistent, it's able to learn.",
                    "label": 0
                },
                {
                    "sent": "Your method is able to learn what happens for the RB version of that estimator.",
                    "label": 0
                },
                {
                    "sent": "It's it turns out that our B estimator is also able to learn.",
                    "label": 1
                },
                {
                    "sent": "In that sense it's errors consistent, that is.",
                    "label": 0
                },
                {
                    "sent": "The LP risk of your estimator.",
                    "label": 1
                },
                {
                    "sent": "Converges to the base risk.",
                    "label": 0
                },
                {
                    "sent": "In probability, if N increases to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But is the base risk is the best you can hope.",
                    "label": 0
                },
                {
                    "sent": "The best just in a second.",
                    "label": 0
                },
                {
                    "sent": "It is just the.",
                    "label": 0
                },
                {
                    "sent": "The information with respect to all measurable functions, not just only two functions in the Hilbert space to all measurable functions.",
                    "label": 0
                },
                {
                    "sent": "Yes, you have a question.",
                    "label": 0
                },
                {
                    "sent": "Keep the box on his fixed an increase in the elect the number of bytes code Infinity, because I mean otherwise.",
                    "label": 0
                },
                {
                    "sent": "You essentially so well.",
                    "label": 0
                },
                {
                    "sent": "Let's pretend that each byte now is by self goes to Infinity, so.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's kind of it's kind of cold vius if the underlying learner on estimates on your flight is consistent in the average will be consistent.",
                    "label": 0
                },
                {
                    "sent": "But like if you well, by technical constraints, have to keep Blacks or speak.",
                    "label": 0
                },
                {
                    "sent": "I have not yet the full proof, but I think it should work if the number of bytes is increasing but not too fast.",
                    "label": 0
                },
                {
                    "sent": "Say something like log of North.",
                    "label": 0
                },
                {
                    "sent": "If the number of pilots is increasing too fast, then I think I will be well perhaps come into trouble cause, well, just give me a very silly counterexample.",
                    "label": 0
                },
                {
                    "sent": "If B, the number of bytes is equal to N, then all subsamples.",
                    "label": 0
                },
                {
                    "sent": "Are consisting of just one data point.",
                    "label": 0
                },
                {
                    "sent": "This cannot work.",
                    "label": 0
                },
                {
                    "sent": "This cannot work if one if you have only one data point, you cannot estimate a function, it cannot work.",
                    "label": 0
                },
                {
                    "sent": "So I think it is.",
                    "label": 0
                },
                {
                    "sent": "At least one partial result that if these six or.",
                    "label": 0
                },
                {
                    "sent": "Probably be is increasing, but not too fast.",
                    "label": 0
                },
                {
                    "sent": "That should work.",
                    "label": 0
                },
                {
                    "sent": "But I have not no solution for the general case, just pushing all the all the owners of the proof back into, well, the estimator on the bike.",
                    "label": 0
                },
                {
                    "sent": "So yes, if you have something that's consistent on the bike, the bite size increases as in number of surveys provide thing.",
                    "label": 0
                },
                {
                    "sent": "Obviously you get an average over a whole bunch of consistent estimator, so you gotta have something useful.",
                    "label": 0
                },
                {
                    "sent": "Just wondering how this compares.",
                    "label": 0
                },
                {
                    "sent": "For instance, two methods like online learning where you get the order of one over square root in but right version.",
                    "label": 0
                },
                {
                    "sent": "I have no comparisons made up to now.",
                    "label": 0
                },
                {
                    "sent": "For me it's not so clear that.",
                    "label": 0
                },
                {
                    "sent": "That works in that case for if you say other robust estimates F, not just the mean, say for the median or a robust estimate or an estimator, I have no.",
                    "label": 0
                },
                {
                    "sent": "Proof that is really this error is consistent, so there is still work if B is fixed, but sorry I cannot answer your question in OK for the proof.",
                    "label": 0
                },
                {
                    "sent": "There's really short.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you take the difference between the IP risk of your estimate, the RB estimate and the base risk, there's no negative of course, trust.",
                    "label": 0
                },
                {
                    "sent": "Lock that in because the loss function is convex, and because that is a linear combination convex combination, then you can.",
                    "label": 0
                },
                {
                    "sent": "A right that in in that way.",
                    "label": 0
                },
                {
                    "sent": "And again, just switch.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Integration and summation, and cause all these quantities are average consistent.",
                    "label": 0
                },
                {
                    "sent": "Then you immediately have that.",
                    "label": 0
                },
                {
                    "sent": "But if you just replace here we mean by the median or an estimator or something else, it is no longer so easy to see that that works.",
                    "label": 0
                },
                {
                    "sent": "OK consistency well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you use our be based on the median, and if you know that your estimate converges in probability, or almost sure, then same turns out for the RB estimator.",
                    "label": 0
                },
                {
                    "sent": "And here are also some simple properties of Arab based on mean, but well, I'm working in the area of robot statistic.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, I'm also interested in the inference function.",
                    "label": 0
                },
                {
                    "sent": "What turns out the influence function of the RB estimator?",
                    "label": 1
                },
                {
                    "sent": "Is it if you have a robust estimate, say, currently sick regression?",
                    "label": 1
                },
                {
                    "sent": "And if you use that for the RB method, is the inference function still bounded?",
                    "label": 0
                },
                {
                    "sent": "And what is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it turns out it's the same.",
                    "label": 0
                },
                {
                    "sent": "So if you use a robust method.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that the RB estimator again is robust in that sense.",
                    "label": 0
                },
                {
                    "sent": "OK, the final sample breakdown point.",
                    "label": 0
                },
                {
                    "sent": "Consider.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you have B bytes and at the final sample regular part of your original estimator is denoted by epsilon.",
                    "label": 0
                },
                {
                    "sent": "NB star of TB.",
                    "label": 0
                },
                {
                    "sent": "OK, the breakdown point of that subsample and then also final sample breakdown point of the estimated by the aggregation step by epsilon B star of new head.",
                    "label": 1
                },
                {
                    "sent": "Then you can write down the final sample breakdown point of your estimator.",
                    "label": 0
                },
                {
                    "sent": "And that is at least as high as just the product of the final sample breakdown points of your original method and the method you use for the aggregation step.",
                    "label": 0
                },
                {
                    "sent": "So if you use mean.",
                    "label": 0
                },
                {
                    "sent": "The final sample rate on point is 0, so the final sample breakdown point is not zero, but is still robust.",
                    "label": 0
                },
                {
                    "sent": "And that's why you bought if B converges to Infinity, the breakdown point converges to zero and that is 1 argument against what you said.",
                    "label": 1
                },
                {
                    "sent": "If B is increasing then you lose robustness in that way.",
                    "label": 0
                },
                {
                    "sent": "If you take the median for example, that quantity is approximately 1/2 and then sorry is different.",
                    "label": 0
                },
                {
                    "sent": "OK, just five minutes or so for the application I have.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Project with an echo.",
                    "label": 0
                },
                {
                    "sent": "Watch now in Bristol.",
                    "label": 0
                },
                {
                    "sent": "Formally in Traverse City of Essen on risky differentiation.",
                    "label": 0
                },
                {
                    "sent": "High dimensional data structures in our SFP I got a data set from the official.",
                    "label": 1
                },
                {
                    "sent": "This is all from 15 German insurance companies.",
                    "label": 0
                },
                {
                    "sent": "The data set is approximately 9 gigabyte as a compressed file.",
                    "label": 0
                },
                {
                    "sent": "Data from around 4.6 million customers and many many expansion variables.",
                    "label": 0
                },
                {
                    "sent": "What are the statistical objectives?",
                    "label": 0
                },
                {
                    "sent": "Well, the people from the insurance companies are mainly interested in what they call the pure premium.",
                    "label": 0
                },
                {
                    "sent": "The expected claim amount given the vector of risk factors X, but they have also a secondary response.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variable that is the probability of having at least one claim within one year.",
                    "label": 0
                },
                {
                    "sent": "OK, there are many complex dependencies.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structures in the data set with you reticle information.",
                    "label": 0
                },
                {
                    "sent": "There's Berlin, that is the rule area.",
                    "label": 0
                },
                {
                    "sent": "We have normal relationships even if you aggregate.",
                    "label": 0
                },
                {
                    "sent": "Honestly, we have outliers and so on.",
                    "label": 0
                },
                {
                    "sent": "We have here an interesting bump of people around 45 years and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, I think it's.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting to use these convex systemization methods, not directly because they will not work if you use just SVM or something like that.",
                    "label": 0
                },
                {
                    "sent": "It will not work.",
                    "label": 0
                },
                {
                    "sent": "Why not cause only 5% of all the customers had a claim at all in one year, so it's not useful and the people of the insurance companies know that data set very well, so they are.",
                    "label": 0
                },
                {
                    "sent": "Interested in getting you information from the data?",
                    "label": 0
                },
                {
                    "sent": "Yes, you have question.",
                    "label": 0
                },
                {
                    "sent": "Actually, you, naturally speaking two different samples citation.",
                    "label": 0
                },
                {
                    "sent": "You should end up with a series of.",
                    "label": 0
                },
                {
                    "sent": "More than one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That is 1 model I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "That is the first response variable they're interested in, but just the expected claim amount.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of having at least one claim.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So I propose to use a rough class variable saying whether there is there was no accident.",
                    "label": 0
                },
                {
                    "sent": "A small accent, say up to 2000 euro, and so on, and say a big claim amount, about 50,000 euro and so on, and I use just full of total probability to write that down, and I use kernel risk regression to estimate probabilities.",
                    "label": 0
                },
                {
                    "sent": "I cannot use support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Cause button tivari proof last year that support vector machine cannot estimate all conditional probabilities and that's on support vector regression or something like that for the Russian terms.",
                    "label": 0
                },
                {
                    "sent": "And of course I made some splitting into training validation test data, so let's just.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It came out overall for males and females, so young males have a higher risk.",
                    "label": 0
                },
                {
                    "sent": "Then young females around, say 30 years they.",
                    "label": 0
                },
                {
                    "sent": "Perform more or less equal, but more interesting is if we consider all these quantities, that is, the probability of having at least one claim the probability of having a small claim given at least one claim.",
                    "label": 0
                },
                {
                    "sent": "Or what is the expected claim amount given.",
                    "label": 0
                },
                {
                    "sent": "You know that there was a small claim and risk factor X and so on.",
                    "label": 0
                },
                {
                    "sent": "Just well, I use many variables at the same time, but just to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give a few pictures.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "I stratified the results by age of the main user anpa genda Blues for males and red is for females, and you can clearly see say from that.",
                    "label": 1
                },
                {
                    "sent": "But young females have a higher probability of having a small claim, say up to 2000 euro.",
                    "label": 0
                },
                {
                    "sent": "But young men have a higher probability, producing more expensive claims.",
                    "label": 0
                },
                {
                    "sent": "So and the interaction term changes.",
                    "label": 0
                },
                {
                    "sent": "Here the red curve is above.",
                    "label": 0
                },
                {
                    "sent": "Here's the blue curve is above and was quite interesting.",
                    "label": 0
                },
                {
                    "sent": "The people from the insurance company T knew that these curves are conjecture, at least at these curves were decreasing with age of the main user.",
                    "label": 1
                },
                {
                    "sent": "But it was new to them that this curve increases, so the probability of having a small claim.",
                    "label": 0
                },
                {
                    "sent": "Given that you have at least one claim increases with H. But was quite new to the people from the insurance companies.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think you have question I mean.",
                    "label": 0
                },
                {
                    "sent": "The 5th.",
                    "label": 0
                },
                {
                    "sent": "Applicable to something like one side of aggression?",
                    "label": 0
                },
                {
                    "sent": "I mean what you're really after is the distribute the conditional distribution of the claims, and I mean what you're doing is essentially modeling a very heavy tailed distribution by just quantizing it into dreams mean.",
                    "label": 0
                },
                {
                    "sent": "When you want to estimate that directly, no, I would not use quantized becausw.",
                    "label": 0
                },
                {
                    "sent": "People from the insurance company.",
                    "label": 0
                },
                {
                    "sent": "Have some interesting quantize, but you can argue as long as you like.",
                    "label": 0
                },
                {
                    "sent": "They are in the end not interested in quantized.",
                    "label": 0
                },
                {
                    "sent": "They have to pay the amount of money, say sum of values and the sum is related to the mean, not to quantiles.",
                    "label": 0
                },
                {
                    "sent": "So it is from a statistical point of view, perhaps more realistic to model quantiles, but the insurance companies.",
                    "label": 0
                },
                {
                    "sent": "Have to pay for the claims and therefore they are interested in the mean becausw of the relationship to the summer.",
                    "label": 0
                },
                {
                    "sent": "And Alaska.",
                    "label": 0
                },
                {
                    "sent": "Figure.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Usually methods from convex risk minimization only able to give good predictions.",
                    "label": 1
                },
                {
                    "sent": "But as a statistician I'm interested in also in having nonparametric confidence intervals and you can clearly see, although I did not model that this simple approaches.",
                    "label": 0
                },
                {
                    "sent": "Able to produce as symmetric confidence intervals and that which increases with with the prediction, and this is quite natural, cause the distribution is highly skewed of course.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "Or or here.",
                    "label": 0
                },
                {
                    "sent": "The median.",
                    "label": 0
                },
                {
                    "sent": "The Blue + is not in the sense center of the confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "And the width of the confidence intervals increases.",
                    "label": 0
                },
                {
                    "sent": "That is quite natural, yes.",
                    "label": 0
                },
                {
                    "sent": "Actually claims or.",
                    "label": 0
                },
                {
                    "sent": "I mean the figures that you're closing here actually claims versus predicted claims.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make sense becausw.",
                    "label": 0
                },
                {
                    "sent": "Yes, but it's nevertheless it does not make sense becausw 95% of all people have no claims at all, so the value is exactly equal to 0.",
                    "label": 0
                },
                {
                    "sent": "There's only makes sense if you compare that to groups of people, otherwise the residuals are still big becausw 0 is the observation or say something really huge.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless the expectation is around baseline off 300 euro.",
                    "label": 0
                },
                {
                    "sent": "So you have to compare that in a.",
                    "label": 0
                },
                {
                    "sent": "In a more clever way than just computing residuals.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This decision is close on this side.",
                    "label": 0
                },
                {
                    "sent": "I believe ultimately interested in segmentation of the market.",
                    "label": 0
                },
                {
                    "sent": "It's actually.",
                    "label": 0
                },
                {
                    "sent": "So what type of models of chemistry be used?",
                    "label": 0
                },
                {
                    "sent": "Because they wanted segmentation from rabbit prediction.",
                    "label": 0
                },
                {
                    "sent": "OK here I only used a starting point.",
                    "label": 0
                },
                {
                    "sent": "Linear kernels, although I know that usually linear kernels are not the best one and then I use caution RBF kernels.",
                    "label": 0
                },
                {
                    "sent": "I did not use polynomial kernels, kernels, becausw from my theoretical work with English Sign.",
                    "label": 0
                },
                {
                    "sent": "What we know that polynomial kernels are unbounded kernels and therefore they do not offer robustness.",
                    "label": 0
                },
                {
                    "sent": "But I like robustness.",
                    "label": 0
                },
                {
                    "sent": "And people from the insurance company while I convince them that they should also be interesting robustus becausw, it should not happen that the whole tariff structure should be destroyed by just one extreme value in one one group.",
                    "label": 0
                },
                {
                    "sent": "It should be a little bit stable, and of course.",
                    "label": 0
                },
                {
                    "sent": "The extreme point should be in, but the way it should be should be bounded should not be unbounded.",
                    "label": 0
                },
                {
                    "sent": "So does it all.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Online algorithm for solving.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "If you use a linear kernel, then the corresponding Hilbert space is not big enough, usually too.",
                    "label": 0
                },
                {
                    "sent": "Model high dimension unknown relationships between variables.",
                    "label": 0
                },
                {
                    "sent": "You can mainly only use overfit affine hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "At the start at the start, just to compare whether the RBF kernel is better.",
                    "label": 0
                },
                {
                    "sent": "And of course it was better.",
                    "label": 0
                },
                {
                    "sent": "I knew that from.",
                    "label": 0
                },
                {
                    "sent": "Just like that.",
                    "label": 0
                },
                {
                    "sent": "That's just exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "If you like, adjust the scales of each of the graph of each of the various independently.",
                    "label": 0
                },
                {
                    "sent": "Or I mean, what did you do for that normalization?",
                    "label": 0
                },
                {
                    "sent": "Because I mean this is actually wonderful situation where you can do a lot of modeling and probably gain big time by doing good modeling.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course I do time reasons I was not talking about exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "I use that enterprise miner from data mining.",
                    "label": 0
                },
                {
                    "sent": "I use TriNet.",
                    "label": 0
                },
                {
                    "sent": "Uh, for for sophisticated kind of making trees alot of trees here I only restricted attention to context memorization, so you only took.",
                    "label": 0
                },
                {
                    "sent": "One week sentences no no no.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "I I.",
                    "label": 0
                },
                {
                    "sent": "Due to the size of the data set, I was not able to make say grid search or pattern search or cross validation with the whole data set.",
                    "label": 0
                },
                {
                    "sent": "I'm just happy to compute it.",
                    "label": 0
                },
                {
                    "sent": "For a few combinations, but what I did is of course I sample, say 5% of the data and then I tried to find good combinations of hyperparameters and then I made some arrangements how these hyperparameters might be modified to work also for the big data set.",
                    "label": 0
                },
                {
                    "sent": "Of course I know that it's not the best we can do, but the computation task is a problem.",
                    "label": 0
                },
                {
                    "sent": "And what we also did by simulations we try to compare different methods for the determination of the hyper parameters.",
                    "label": 0
                },
                {
                    "sent": "We use fine grids, fine grid search, MLM searches, the simplex method, patent search by Mama and Bennett.",
                    "label": 0
                },
                {
                    "sent": "We use random search and I think one other method I've forgotten the name now and not for the big data set.",
                    "label": 0
                },
                {
                    "sent": "But for many Bond benchmark datasets an for.",
                    "label": 0
                },
                {
                    "sent": "Subsamples of the big data set I well I have not.",
                    "label": 0
                },
                {
                    "sent": "A big supercomputer, so I cannot do.",
                    "label": 0
                },
                {
                    "sent": "That with with the whole data set.",
                    "label": 0
                },
                {
                    "sent": "So time consuming.",
                    "label": 0
                },
                {
                    "sent": "The main problem is KR.",
                    "label": 0
                },
                {
                    "sent": "That is really time consuming even with account if at all proposal.",
                    "label": 0
                },
                {
                    "sent": "Factorization sound similar low dimensional subspace approximations?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's quite standard, but I have not done that.",
                    "label": 0
                },
                {
                    "sent": "I I read the paper by by Barton Tivari and it was quite convincing to me that support vector machines and so on are not able to compute all the conditional probabilities, especially in that situation where the probability.",
                    "label": 1
                },
                {
                    "sent": "Of having say a big claim, about 50,000 euro is only around oh .07% oh .07%, and therefore this is independent of whether user is user kernel adjust, logistic regression or whatever other risk minimization procedure that you provided mean.",
                    "label": 0
                },
                {
                    "sent": "This is talking about the specific function space, or an approximation thereof, that you might use.",
                    "label": 0
                },
                {
                    "sent": "No, I did not mean this is essentially would also would allow you to run rather than having to breakdown things like.",
                    "label": 0
                },
                {
                    "sent": "You can get the linear time method in the number of observations.",
                    "label": 0
                },
                {
                    "sent": "All you have to do is out of four storage, so there's work by Winston Shine broken fine.",
                    "label": 0
                },
                {
                    "sent": "There's work by Brighton coworkers.",
                    "label": 0
                },
                {
                    "sent": "Put down that actually works quite nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, perhaps we can.",
                    "label": 0
                },
                {
                    "sent": "I could discuss a little bit later on.",
                    "label": 0
                },
                {
                    "sent": "I just saw a few programs, perhaps the best programs, but perhaps you can give me some advice, but I was not able to really get software for doing something like that for huge datasets, for there are many nice programs for working in that area, say for 2000, but not for millions.",
                    "label": 0
                },
                {
                    "sent": "At least that is my impression.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry I did not do that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So, have you actually found anything in your approach where you could show that you've done better than what you do?",
                    "label": 0
                },
                {
                    "sent": "The trick using?",
                    "label": 0
                },
                {
                    "sent": "Enterprise miner?",
                    "label": 0
                },
                {
                    "sent": "Yeah, enterprise miner.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "I even got some money from the.",
                    "label": 0
                },
                {
                    "sent": "EFG that is similar to NSF in the US?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they gave me money to to buy a license of the software package.",
                    "label": 0
                },
                {
                    "sent": "With minor, but for that project data enterprise, my dad is not out to be really successful, I use.",
                    "label": 0
                }
            ]
        }
    }
}