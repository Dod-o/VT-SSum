{
    "id": "6jp6s6bl5xti6rtly7yqcmtwlrv7o2zr",
    "title": "Information Theoretic Regularization for Semi-Supervised Boosting",
    "info": {
        "author": [
            "Lei Zheng, University of Texas at Austin"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_zheng_itrssb/",
    "segmentation": [
        [
            "So the author basically.",
            "No.",
            "Oh yeah, wow.",
            "OK, so so so."
        ],
        [
            "Many of the first after they could not get off get this coming back to preach states so they couldn't come from Europe so so I'm keeping them.",
            "Just talk.",
            "I'm Christine from universe, Texas.",
            "So really, it's about the information theoretic regularization, semi supervised boosting so fairly quickly follow the scripts.",
            "So essentially give an introduction."
        ],
        [
            "So, boosting as you know, is boosting supervised learning methods and usually I is one of the most well known work is Ada Boost.",
            "And the other thing is topics introduces supervised learning classification.",
            "So where we can give feature vectors and labels and generalize to the new unseen labels and also the unsupervised learning given data point and clustering and so on and so forth.",
            "So the semi supervised learning nowadays become very popular because the most data set for large data set you cannot really labeled data.",
            "So given a set of labeled party, basically we wish to predict those unlabeled.",
            "So these are datasets which have no labels.",
            "OK, so there's many different approaches.",
            "Let me see.",
            "Size."
        ],
        [
            "So semi supervised learning methods.",
            "There are some many, many methods.",
            "So EM as a late 90s by people at I think CMU and UCM.",
            "Basically fixed data points and using generative model and gradually code training and bootstrapping things.",
            "And gradually now is in this paper that use a mutual information entropy to regularization.",
            "And most recently I think graph based methods become really popular.",
            "I think the most.",
            "The last awhile to two years, so many different approaches."
        ],
        [
            "In Semi supervised learning and this one is the paper used this way so so this is really semi supervised boosting.",
            "Essentially there are two methods here.",
            "Use this one is entropy regularization and the other is mutual information regularization so."
        ],
        [
            "Little bit faster.",
            "Save the time and."
        ],
        [
            "Um, let me see.",
            "So typically there are two methods are.",
            "Why is a general approach is in this case?",
            "Is maximum entropy approach in the other approach, this paper takes about.",
            "It's basically there was straightforward greedy function optimization.",
            "Instead of using more vigorous kind of how to say probabilistic approach.",
            "So so here are the."
        ],
        [
            "The formulas."
        ],
        [
            "First term is labeled data and these are so in boosting we have a lot of fairly simple classifiers or discriminate functions.",
            "So basically you have all the coefficient combined together and so this is the labeled data.",
            "This is unlabeled data and essentially claim is that this enabled data.",
            "Putting here helps the classification, otherwise is straightforward straightforward kind of, I mean transductive kind of our semi supervised methods so.",
            "OK."
        ],
        [
            "I'm not very familiar with traffic, so I'm going very fast and unless you have questions, so basically I just are taking gradient to increasing gradient descent ascent and taking gradient and running all this program."
        ],
        [
            "Which is this is numbers these parameters and.",
            "Are there some constraints here?",
            "Basically, it's basically straight forward gradient descent optimization so."
        ],
        [
            "Um?"
        ],
        [
            "So that so that's the posting part in this."
        ],
        [
            "Is the how to say the?",
            "Regularization path so that basically normalize the log linear models that you have two terms so edge is a discriminant function.",
            "That why is the sign so?",
            "So basically we minimize.",
            "This is logistic loss is just minimize.",
            "This function is maximized and so we have entropy regularization.",
            "We have mutual information regularization, so two different approaches."
        ],
        [
            "So then that's the previous multi asses two class, but you can easily generate to multiclass multiclass normalized log log linear model so.",
            "So.",
            "OK, so."
        ],
        [
            "OK, so."
        ],
        [
            "Very fast, so these are there some statistics experiment on the synthetic data.",
            "This is a comparison with the earlier Ada boost logic.",
            "Boost assembled large boost, so these are some earlier earlier method and this this 29 described the newer method proposed in this paper.",
            "So you can see that this is the testing error.",
            "You can see that as a boosting as the labeled percentage of just actually single labels, so 55.",
            "Labeled data 10 labeled 10 data points are labeled, and so as you see that as the number of labeled data increases, typically typically performance, you need really improve somehow.",
            "This kind of stabilized."
        ],
        [
            "So this is the.",
            "This is for the experiment, one which is.",
            "I'm not sure what we exactly is the.",
            "But at a point, but essentially you can see that this is a relatively large.",
            "Much better from this is iterations.",
            "In the optimization.",
            "You can see this is comparison, so to say to boost this logic pushed his is this one and just switch our entropy and distance mission information and so on and so forth.",
            "OK, so the label is here.",
            "The labeled data set is labeled.",
            "Is set to five.",
            "I'm not sure what is exactly the."
        ],
        [
            "Stressors are set, so this issue is that different parameters because there's always a parameter you have to set it.",
            "So this is a government set .2 and discover set to .1.",
            "Thanks for entropy regularization.",
            "So there's a total loss.",
            "You can see that as.",
            "As far as the parameter increases here you can see the total loss increase here.",
            "I.",
            "So the way so this says that when regularization parameters along the active, is very large, the loss function become nonconvex, whereas when it's very small, if count is still convex, some are nonconvex.",
            "It says the total loss function.",
            "Become convex unless you're so.",
            "So these are the."
        ],
        [
            "On the benchmark data, this is relatively UCR.",
            "Linear repository and also so this says 15 percent, 15% labeled data is labeled and 85% of our neighbors.",
            "And this is some of the comparisons.",
            "So ballot if for datasets pimaan for their hearts.",
            "As you can see that generally speaking, this is.",
            "This is the classification loss are classification errors.",
            "So you can see general there's 2 newer method does slightly better than previous one.",
            "Logically most.",
            "Actually they should compare it to a graph.",
            "A star transaction vessel, I believe that's better than this.",
            "It's just personal finance, personal comment, and these are there are."
        ],
        [
            "Dataset valigi is very popular data set.",
            "You can see that as the number of labeled data in increases, the well the error.",
            "I think this is across our.",
            "Across.",
            "Division error typically go should go down as because the number of labeled data."
        ],
        [
            "I increases and so there's some comparisons, so I think stop here and not go."
        ],
        [
            "Too many.",
            "Basically this is a summarization.",
            "This is the paper producer two.",
            "Our method.",
            "Essentially new method semi supervised boosting.",
            "Why is why is mutual information based on the other is maximum entropy.",
            "So the functions are combined.",
            "Loss functions are non convex.",
            "But this paper basically does standard greedy optimization and the results seem to be fairly good, at least compared with the earlier methods.",
            "Semi supervised boosted so.",
            "So they are working on the form analysis of this paper."
        ],
        [
            "Thank you.",
            "Any comments on maybe?",
            "Yes.",
            "Depend on your, typically those parameters.",
            "In many cases, I'm not very sure here.",
            "Typically with use cross validation tool well to get best results.",
            "And the number of features we use and we have no don't know the comma is just a parameter before these terms.",
            "Let me see."
        ],
        [
            "Um?",
            "This is the earlier 1, is this is it?",
            "This is the.",
            "So this is labeled part.",
            "This is enabled.",
            "How this contributes?",
            "Yes, yeah.",
            "Yeah, so so we have a one graph this shows."
        ],
        [
            "That are 4, is closed one, so the unlabeled part become more significant.",
            "You can see the loss.",
            "It's actually well.",
            "This is loss function, so this is relatively small, smaller on the gamma .2 point, 2.2 for the unlabeled part.",
            "Yeah, if you tell us how.",
            "This should be selected in a new application that involves, let's say, more features are working actively.",
            "I don't know.",
            "I think this is you can see that this is a.",
            "This shows that basically these two terms the size are reasonable.",
            "So you said to one it wouldn't get.",
            "Some results were terribly wrong, I think.",
            "But typically well, people to compare compare with earlier results.",
            "This is determined by cross validation.",
            "Nothing questions.",
            "Any other experiments done on large scale data sets?",
            "So for this one, I think I think that is EG is also part of UCI data set.",
            "Each is actually time data set.",
            "It didn't really say I kind of forgot today idea, the due date and said yeah, they actually only runs three datasets.",
            "Yes, they probably could have run more, but I think for this one probably is faster because typically graph based methods you are you do a graph on the square of the number of datasets so it's much larger than I think.",
            "But I believe the result is better.",
            "I believe you're using our pricing based methodology's.",
            "Let the OK thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the author basically.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, wow.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many of the first after they could not get off get this coming back to preach states so they couldn't come from Europe so so I'm keeping them.",
                    "label": 0
                },
                {
                    "sent": "Just talk.",
                    "label": 0
                },
                {
                    "sent": "I'm Christine from universe, Texas.",
                    "label": 0
                },
                {
                    "sent": "So really, it's about the information theoretic regularization, semi supervised boosting so fairly quickly follow the scripts.",
                    "label": 1
                },
                {
                    "sent": "So essentially give an introduction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, boosting as you know, is boosting supervised learning methods and usually I is one of the most well known work is Ada Boost.",
                    "label": 1
                },
                {
                    "sent": "And the other thing is topics introduces supervised learning classification.",
                    "label": 0
                },
                {
                    "sent": "So where we can give feature vectors and labels and generalize to the new unseen labels and also the unsupervised learning given data point and clustering and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So the semi supervised learning nowadays become very popular because the most data set for large data set you cannot really labeled data.",
                    "label": 1
                },
                {
                    "sent": "So given a set of labeled party, basically we wish to predict those unlabeled.",
                    "label": 0
                },
                {
                    "sent": "So these are datasets which have no labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's many different approaches.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "Size.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So semi supervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "There are some many, many methods.",
                    "label": 0
                },
                {
                    "sent": "So EM as a late 90s by people at I think CMU and UCM.",
                    "label": 0
                },
                {
                    "sent": "Basically fixed data points and using generative model and gradually code training and bootstrapping things.",
                    "label": 1
                },
                {
                    "sent": "And gradually now is in this paper that use a mutual information entropy to regularization.",
                    "label": 0
                },
                {
                    "sent": "And most recently I think graph based methods become really popular.",
                    "label": 0
                },
                {
                    "sent": "I think the most.",
                    "label": 0
                },
                {
                    "sent": "The last awhile to two years, so many different approaches.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In Semi supervised learning and this one is the paper used this way so so this is really semi supervised boosting.",
                    "label": 0
                },
                {
                    "sent": "Essentially there are two methods here.",
                    "label": 0
                },
                {
                    "sent": "Use this one is entropy regularization and the other is mutual information regularization so.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit faster.",
                    "label": 0
                },
                {
                    "sent": "Save the time and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, let me see.",
                    "label": 0
                },
                {
                    "sent": "So typically there are two methods are.",
                    "label": 0
                },
                {
                    "sent": "Why is a general approach is in this case?",
                    "label": 0
                },
                {
                    "sent": "Is maximum entropy approach in the other approach, this paper takes about.",
                    "label": 1
                },
                {
                    "sent": "It's basically there was straightforward greedy function optimization.",
                    "label": 1
                },
                {
                    "sent": "Instead of using more vigorous kind of how to say probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "So so here are the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The formulas.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First term is labeled data and these are so in boosting we have a lot of fairly simple classifiers or discriminate functions.",
                    "label": 0
                },
                {
                    "sent": "So basically you have all the coefficient combined together and so this is the labeled data.",
                    "label": 0
                },
                {
                    "sent": "This is unlabeled data and essentially claim is that this enabled data.",
                    "label": 0
                },
                {
                    "sent": "Putting here helps the classification, otherwise is straightforward straightforward kind of, I mean transductive kind of our semi supervised methods so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not very familiar with traffic, so I'm going very fast and unless you have questions, so basically I just are taking gradient to increasing gradient descent ascent and taking gradient and running all this program.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is this is numbers these parameters and.",
                    "label": 0
                },
                {
                    "sent": "Are there some constraints here?",
                    "label": 0
                },
                {
                    "sent": "Basically, it's basically straight forward gradient descent optimization so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that so that's the posting part in this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the how to say the?",
                    "label": 0
                },
                {
                    "sent": "Regularization path so that basically normalize the log linear models that you have two terms so edge is a discriminant function.",
                    "label": 0
                },
                {
                    "sent": "That why is the sign so?",
                    "label": 0
                },
                {
                    "sent": "So basically we minimize.",
                    "label": 0
                },
                {
                    "sent": "This is logistic loss is just minimize.",
                    "label": 1
                },
                {
                    "sent": "This function is maximized and so we have entropy regularization.",
                    "label": 0
                },
                {
                    "sent": "We have mutual information regularization, so two different approaches.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then that's the previous multi asses two class, but you can easily generate to multiclass multiclass normalized log log linear model so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very fast, so these are there some statistics experiment on the synthetic data.",
                    "label": 1
                },
                {
                    "sent": "This is a comparison with the earlier Ada boost logic.",
                    "label": 0
                },
                {
                    "sent": "Boost assembled large boost, so these are some earlier earlier method and this this 29 described the newer method proposed in this paper.",
                    "label": 0
                },
                {
                    "sent": "So you can see that this is the testing error.",
                    "label": 0
                },
                {
                    "sent": "You can see that as a boosting as the labeled percentage of just actually single labels, so 55.",
                    "label": 0
                },
                {
                    "sent": "Labeled data 10 labeled 10 data points are labeled, and so as you see that as the number of labeled data increases, typically typically performance, you need really improve somehow.",
                    "label": 0
                },
                {
                    "sent": "This kind of stabilized.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is for the experiment, one which is.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what we exactly is the.",
                    "label": 0
                },
                {
                    "sent": "But at a point, but essentially you can see that this is a relatively large.",
                    "label": 0
                },
                {
                    "sent": "Much better from this is iterations.",
                    "label": 0
                },
                {
                    "sent": "In the optimization.",
                    "label": 0
                },
                {
                    "sent": "You can see this is comparison, so to say to boost this logic pushed his is this one and just switch our entropy and distance mission information and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so the label is here.",
                    "label": 0
                },
                {
                    "sent": "The labeled data set is labeled.",
                    "label": 0
                },
                {
                    "sent": "Is set to five.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what is exactly the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stressors are set, so this issue is that different parameters because there's always a parameter you have to set it.",
                    "label": 0
                },
                {
                    "sent": "So this is a government set .2 and discover set to .1.",
                    "label": 0
                },
                {
                    "sent": "Thanks for entropy regularization.",
                    "label": 0
                },
                {
                    "sent": "So there's a total loss.",
                    "label": 1
                },
                {
                    "sent": "You can see that as.",
                    "label": 0
                },
                {
                    "sent": "As far as the parameter increases here you can see the total loss increase here.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So the way so this says that when regularization parameters along the active, is very large, the loss function become nonconvex, whereas when it's very small, if count is still convex, some are nonconvex.",
                    "label": 1
                },
                {
                    "sent": "It says the total loss function.",
                    "label": 1
                },
                {
                    "sent": "Become convex unless you're so.",
                    "label": 0
                },
                {
                    "sent": "So these are the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the benchmark data, this is relatively UCR.",
                    "label": 1
                },
                {
                    "sent": "Linear repository and also so this says 15 percent, 15% labeled data is labeled and 85% of our neighbors.",
                    "label": 1
                },
                {
                    "sent": "And this is some of the comparisons.",
                    "label": 0
                },
                {
                    "sent": "So ballot if for datasets pimaan for their hearts.",
                    "label": 0
                },
                {
                    "sent": "As you can see that generally speaking, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the classification loss are classification errors.",
                    "label": 0
                },
                {
                    "sent": "So you can see general there's 2 newer method does slightly better than previous one.",
                    "label": 0
                },
                {
                    "sent": "Logically most.",
                    "label": 0
                },
                {
                    "sent": "Actually they should compare it to a graph.",
                    "label": 0
                },
                {
                    "sent": "A star transaction vessel, I believe that's better than this.",
                    "label": 0
                },
                {
                    "sent": "It's just personal finance, personal comment, and these are there are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dataset valigi is very popular data set.",
                    "label": 0
                },
                {
                    "sent": "You can see that as the number of labeled data in increases, the well the error.",
                    "label": 0
                },
                {
                    "sent": "I think this is across our.",
                    "label": 0
                },
                {
                    "sent": "Across.",
                    "label": 0
                },
                {
                    "sent": "Division error typically go should go down as because the number of labeled data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I increases and so there's some comparisons, so I think stop here and not go.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Too many.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a summarization.",
                    "label": 0
                },
                {
                    "sent": "This is the paper producer two.",
                    "label": 0
                },
                {
                    "sent": "Our method.",
                    "label": 0
                },
                {
                    "sent": "Essentially new method semi supervised boosting.",
                    "label": 1
                },
                {
                    "sent": "Why is why is mutual information based on the other is maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "So the functions are combined.",
                    "label": 0
                },
                {
                    "sent": "Loss functions are non convex.",
                    "label": 1
                },
                {
                    "sent": "But this paper basically does standard greedy optimization and the results seem to be fairly good, at least compared with the earlier methods.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised boosted so.",
                    "label": 1
                },
                {
                    "sent": "So they are working on the form analysis of this paper.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Any comments on maybe?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Depend on your, typically those parameters.",
                    "label": 0
                },
                {
                    "sent": "In many cases, I'm not very sure here.",
                    "label": 0
                },
                {
                    "sent": "Typically with use cross validation tool well to get best results.",
                    "label": 0
                },
                {
                    "sent": "And the number of features we use and we have no don't know the comma is just a parameter before these terms.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is the earlier 1, is this is it?",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "So this is labeled part.",
                    "label": 0
                },
                {
                    "sent": "This is enabled.",
                    "label": 0
                },
                {
                    "sent": "How this contributes?",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so we have a one graph this shows.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That are 4, is closed one, so the unlabeled part become more significant.",
                    "label": 0
                },
                {
                    "sent": "You can see the loss.",
                    "label": 1
                },
                {
                    "sent": "It's actually well.",
                    "label": 1
                },
                {
                    "sent": "This is loss function, so this is relatively small, smaller on the gamma .2 point, 2.2 for the unlabeled part.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you tell us how.",
                    "label": 0
                },
                {
                    "sent": "This should be selected in a new application that involves, let's say, more features are working actively.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think this is you can see that this is a.",
                    "label": 0
                },
                {
                    "sent": "This shows that basically these two terms the size are reasonable.",
                    "label": 0
                },
                {
                    "sent": "So you said to one it wouldn't get.",
                    "label": 0
                },
                {
                    "sent": "Some results were terribly wrong, I think.",
                    "label": 0
                },
                {
                    "sent": "But typically well, people to compare compare with earlier results.",
                    "label": 0
                },
                {
                    "sent": "This is determined by cross validation.",
                    "label": 0
                },
                {
                    "sent": "Nothing questions.",
                    "label": 0
                },
                {
                    "sent": "Any other experiments done on large scale data sets?",
                    "label": 0
                },
                {
                    "sent": "So for this one, I think I think that is EG is also part of UCI data set.",
                    "label": 0
                },
                {
                    "sent": "Each is actually time data set.",
                    "label": 0
                },
                {
                    "sent": "It didn't really say I kind of forgot today idea, the due date and said yeah, they actually only runs three datasets.",
                    "label": 0
                },
                {
                    "sent": "Yes, they probably could have run more, but I think for this one probably is faster because typically graph based methods you are you do a graph on the square of the number of datasets so it's much larger than I think.",
                    "label": 0
                },
                {
                    "sent": "But I believe the result is better.",
                    "label": 0
                },
                {
                    "sent": "I believe you're using our pricing based methodology's.",
                    "label": 0
                },
                {
                    "sent": "Let the OK thank you.",
                    "label": 0
                }
            ]
        }
    }
}