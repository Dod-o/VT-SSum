{
    "id": "nlqc6of2475gqy5uq74dukdtgmhmqgmq",
    "title": "Regularization Paths and Coordinate Descent",
    "info": {
        "author": [
            "Trevor Hastie, Stanford University"
        ],
        "published": "Sept. 26, 2008",
        "recorded": "August 2008",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd08_hastie_rpcd/",
    "segmentation": [
        [
            "Welcome, this is a slightly different setting from the usual general session.",
            "In that sense, you actually have table.",
            "This is the setup that we can manage to get the day moving because this is where we'll have our lunch as well.",
            "So we have it set up this way.",
            "Hopefully you can utilize the table for taking notes.",
            "Welcome again, we have an exciting day of sessions, presentations and activities ahead of us.",
            "Hope you all had a good rest last night with that will get going for today, maybe one more minute.",
            "People still trickling in.",
            "OK. Hello everybody, it gives me great pleasure to be introducing Trevor Hastie to be giving us a talk today on fast regularization paths via coordinate descent.",
            "Keyboard is well known in this community for his yellow book on elements of statistical learning.",
            "For many of us who have gotten into data mining through the non statistical path, this has been a very valuable book.",
            "It has given us the much needed foundations for understanding many of the data mining operators that we just take for granted, hence the idea of this talk.",
            "We think that it's.",
            "Going to be really valuable for us to understand data mining from one of the stalwarts in the field of statistics.",
            "He has another book, this one is for statisticians on generalized additive models, which he has coauthored with Tipsy Ronnie.",
            "He has been working in statistics for 30 years and of course he has made a diverse range of contributions in many different areas.",
            "So if I have to just use 2 words to summarize most of what he has done, it's nonparametric regression and classification.",
            "Currently he is applying.",
            "All his tools for solving various problems in the bio and medicine and genomics kind of industry.",
            "On the computing side, in fact, many of the libraries which are there in the famous statistical software packages like S Plus and R have been due to his influence and contribution.",
            "But he confesses that his favorite language is in fact fortron, not the statistical languages.",
            "He has been with the Department of Statistics in Stanford for almost 19 years.",
            "First five years as a PhD student and then 2012 years as a free floating faculty member and for the past two years he has been on a sentence.",
            "Apparently he's the chair of the Department and he's really waiting for that term to end in another one year.",
            "And like many great statisticians, he has spent like 9.",
            "Very productive and fun years in Bell Labs in its hey days so he's very fond of those that period two he grew up in South Africa in fact and he enjoys surfing and he continues to live with that passion, although sometimes the detour goes into the web hold.",
            "So this he is very well known in the field and he has given many talks.",
            "In fact this is his 24th keynote talk in over the last eight years.",
            "He is fellow of.",
            "The the American Statistical Foundation and the Institute of Mathematical Statistics, and I thought it was a big deal, but then he tells me in the morning today that he was a fellow of the Royal Statistical Society even before he got his PhD.",
            "So without further delay, let me welcome Trevor to give us his talk.",
            "Thank you thinking.",
            "Am I alive?"
        ],
        [
            "Can everyone hear me?",
            "Well, thank you very much.",
            "First of all, healing for organizing the conference and sonita for a very nice introduction.",
            "Thank you very much.",
            "She asked me for some background dirt and I gave her quite a lot.",
            "She spared me some of the worst, so thank you, so it's very nice to be here.",
            "I know a lot of you are in the computer science field and it must really.",
            "George is here.",
            "Talk on coordinate descent.",
            "Becausw, I think I wouldn't.",
            "The sense being dismissed as a technique for optimization.",
            "30 Among my engineering friends.",
            "But undaunted, we we plunged ahead, and I hope our convince you today.",
            "At least in this application.",
            "So this is joint work with my like 2 coauthors Jerry Friedman and Rob Tibshirani from Stanford.",
            "Yeah they are.",
            "Enjoying one of their favorite pastimes.",
            "Jerry's with a glass of Cabernet in his hand.",
            "Actually, to be fair to Rob, that's my student Myong Park.",
            "Graduating, and I wasn't able to be there.",
            "So he he went in, and."
        ],
        [
            "Rated R for me.",
            "So.",
            "This is this talks about linear models.",
            "And using linear models in data mining probably 10 years ago, we wouldn't give such a talk because linear models were somewhat out of phase fashion.",
            "We were doing much more aggressive modeling.",
            "But something's changed.",
            "Datasets have grown wide.",
            "Which means we have many more features and samples in a lot of applications.",
            "And so the linear model is a regained favor in in the dataminers toolbox.",
            "We have so many variables, often the linear model and in some applications is or.",
            "We can.",
            "We can manage to do an an before we start overfitting, so you have some examples.",
            "So document classification is A is a prime example.",
            "We often use a bag of words model.",
            "Where we take the collection of say words in English language and mark them as presents a or absent in a document and that can easily lead to say, 20,000 features for each document and we might have something like 5000 documents.",
            "And so, in statistics jargon, we'd say P is bigger than N. We always use P for the number of variables and N for the number of samples.",
            "So for example, the spam classification example is is is 1 email classification as spam or not spam?",
            "It's a document.",
            "We look at the set of words at least some of the early spam filters would do that and classify document as spam or not.",
            "In fact.",
            "Little embarrassingly when I was first invited by Sunita to be the keynote speaker in this in this conference.",
            "The message went straight into my spam bucket.",
            "And so I didn't see it.",
            "And then she sent me a reminder 2 weeks later did I want to actually give this talk or not, and I had to tell her that I hadn't seen the message.",
            "And sure enough, I found it in the in the spam bucket.",
            "So my I guess my spam filter is still work in progress.",
            "Image deblurring or classification.",
            "So images notoriously have a large number of features if you use the pixels as as features.",
            "And the number of samples can be relatively small.",
            "An area where where I worked a lot is in genomics and so for example in microarray studies.",
            "You can easily have 40,000 features which maybe represent versions of jeans and, and you might only have as few as 100 samples.",
            "And more recently, in Genome Wide Association studies where we measure snips single nucleotide polymorphism's and they can easily be 500,000.",
            "Lately even a million snips measured along the genome, and often they do these in for specific diseases.",
            "They trying to discover causes of specific diseases, and you might have, say, 2000 case control samples.",
            "So in all these cases we got many, many more features than we have observations and the linear model is very useful.",
            "And we use them such as linear regression, logistic regression and similar models.",
            "And.",
            "As we all know from basic statistics, you can't fit a linear model if P is bigger than in, and unless you do some kind of regularization.",
            "So you either have to work with a subset of the variables or constrain the code."
        ],
        [
            "Actions of the variables.",
            "When you do the fit.",
            "So the next slide is is a bit of crass advertising.",
            "This is the second edition of our elements of statistical learning we've been saying.",
            "I've been saying since 2004, then it's going to be coming out at the end of the year.",
            "Will finally it is coming out at the end of the year.",
            "It's done and we have.",
            "There's a it's going to have about 200 extra pages and there will be extra chapters on wide data, which is the topic of the talk today on random forests, graphical models and ensemble methods.",
            "And a lot of other new material scattered throughout the book on path algorithms, which is also the topic of today.",
            "Kernel methods and more.",
            "So that should be out in December and.",
            "For those who interested tips, Ronnie and I teach a two day course twice a year, and all the topics, especially the new topics in the book, are covered in in the course of the next time we teach.",
            "It is in October in Boston, and if you look on either his web page or mine."
        ],
        [
            "Find details of the course.",
            "So I guess the topic today starts with the lassoo, which is regular L1 regularization for linear regression introduced by tips, Ronnie in 1995.",
            "And we see we have a linear model, just a linear regression model, the X the why is the response, the excise or the predictors one through P or the features?",
            "And we fit the model by least squares subject to an L1 constraint on the coefficients.",
            "So there's the regularization we bound the sum of the absolute values of the coefficients to be smaller than some number T. Now.",
            "If the balance if she's big enough.",
            "And bigger than, say, the L1 norm of the unrestricted least squares coefficients.",
            "It will be unregularized, but if P is bigger than N we wouldn't be able to compute that fit.",
            "They'd be infinitely many, so by tightening the bound we constrain the fit and give an leads to a unique solution.",
            "And similar to older technique in statistics called Ridge regression, where we use the quadratic penalty for controlling the coefficients.",
            "And what does these bounds do is pull the coefficients towards 0 and that restricts them.",
            "It also reduces the variance if you pull them all the way to 0.",
            "Of course their variance is zero and but you've gotten all fit.",
            "So what's the big deal?",
            "Is the difference between L1 and L2?",
            "Well, the main thing is that the L1 norm said some of the coefficients to 0, and that's very useful because not only did we regularize the fit, we get rid of a lot of the variables and we left with presumably the more important variables.",
            "And this little picture here, which is in our book as well, sort of shows you why that happens.",
            "The L1 ball is a diamond shaped ball, whereas the L2 ball is a spherical ball and the contours of this least squares criterion which we see there there's ellipsoidal contours.",
            "We moved out on the contours until we hit the constraint ball and with the L1 ball is a good chance we'll hit it on the corner or in a sharp edge where, which means some of the coefficients are set to 0.",
            "So for simply for that reason."
        ],
        [
            "L1 regularization has become very popular.",
            "So let me give you a brief history of L1 regularization.",
            "It really started and in statistics at least, it's probably been around for a very long time, but in statistics it became first prominent with wavelet soft thresholding.",
            "So with wavelet basis.",
            "Usually orthonormal basis.",
            "Donna Hearn Johnston found you could do wavelet selection by using L1 regularization, and you'd set a lot of wavelet coefficients to zero and some would be shrunk and I'll show you the software sold in operation in in a moment.",
            "And then, but those that was for an orthonormal basis and then tip serani introduced to Lesu, which was the same idea for general sets of predictors.",
            "Non orthogonal predictors in 1995.",
            "The same idea was used in basis pursuit by 10 Donna and Saunders in 1996, again in a wavelet context.",
            "But now when you have dictionaries of different wavelet bases and so you lose orthogonality across the basis for getting a compact representation of a signal.",
            "Since then, a lot of activity is taking place with their suits being extended to many linear model settings.",
            "For example, survival models tips around in 1997, logistic regression models, and so on.",
            "And more recently, there's a whole new field involved in in.",
            "In the signal processing community called compressed sensing.",
            "The name is due to Donna her 2004 and also Candace and Tower 2005.",
            "And the idea is you can perform near exact recovery of sparse signals in very high dimensions.",
            "In many cases.",
            "In this context, you think of the L1 as a good surrogate for L0 regularization, where you're trying to find a very compact basis for representing the signal.",
            "That's a NP complete problem.",
            "Use L1 regularization and they have all kinds of nice theorems that show that even though using L1 instead of L0, you actually can recover the the optimal basis with with very very high probability.",
            "So this is becoming very hot."
        ],
        [
            "Area.",
            "OK, so here's a picture.",
            "Of the less what we call the coefficient profile.",
            "So it's again the last few problem at the bottom of the screen I've put the criterion here in a slightly different form that we had before.",
            "Instead of having a bound on the coefficient, we call this the Lagrangian form.",
            "We have the residual sum of squares plus Lambda times the L1 norm of the coefficient vector.",
            "So beta year is a vector of coefficients, and we put an L1 norm and on those and we optimize it.",
            "Nation of residual sum of squares plus Lambda times abound as you increase Lambda.",
            "You'll force the code.",
            "The solution will have the coefficients forced towards 0.",
            "So what happens?",
            "So this is a small example.",
            "There's eight predictors and we start off with Lambda very big, so all the coefficients are zero, and that's at the left part of the plot.",
            "Here this is the zero line here, and each of these curves is a coefficient.",
            "As we relax Lambda.",
            "And so as we relax Lambda, the coefficients grow away from zero, and in this case there's more N is bigger than P. So unrestricted least squares fit is given on the right, and we see the path of the coefficients.",
            "So we call that a regularization path.",
            "And it's very useful to have such a path, because at the end of the day you need to pick the parameter Lambda or the bounty on for the coefficients, and so having the whole path let's gives you a means for picking a sensible value.",
            "And awful often we'll use cross validation or some measure of prediction error to pick the value, but it's useful to have the whole path.",
            "So in the case of the lawsuit.",
            "Solving this problem just naively, it's a convex optimization problem, requires quadratic programming, and you can you know it's a.",
            "It's a fairly easy quadratic program, but you can solve the problem that way and that's originally it was solved.",
            "Anne."
        ],
        [
            "In in 2001, Efron and coauthors I was one of them.",
            "But everyone really discovered that you could solve this problem in much more efficiently, and the reason was is that the."
        ],
        [
            "Efficient paths, the profiles are all piecewise linear.",
            "So if you look carefully at this picture, you'll see you can see the piecewise linear parts and each of the vertical lines indicate the breakpoints as it."
        ],
        [
            "Changes.",
            "And that led to the what's known as the laws algorithm for solving this.",
            "This convict this whole path of of solutions.",
            "And to summarize, you can compute the entire path in the same computations as a single least squares fit.",
            "So that's very efficient.",
            "Essentially, the only there's a computation needed for each of the steps.",
            "And so that led to huge speedup in computing the last two profile.",
            "So that was called the law's algorithm.",
            "The names rather obscured sense for least angle regression.",
            "And as I say, is due to effort.",
            "And that's what happened in that.",
            "So that was in 2001, and that in turn led to a huge flood of path algorithms for all kinds of different regularization problems.",
            "No, the idea that you could get the entire solution path efficiently was very attractive, and because in any problem where you got regularization, you need to compute the solution at many points and see if you can compute the whole path efficiently.",
            "That was a very attractive thing.",
            "So there's I've listed some of the examples.",
            "There's lots more.",
            "This is a biased list as you'll see my name is is an names of my students, so in a lot of these.",
            "But the first one, the group class who this is for the case where.",
            "We variables naturally come in groups, will talk about that a little bit later, one in learning 2006, a developer path algorithm.",
            "For support vector machine, the cost parameter in the support vector machine is a tuning parameter regularization parameter, and it turns out there's a piecewise linear path for the solutions of the support vector machine.",
            "As you vary the cost parameter.",
            "And that was work with some of my students in tips, Ronnie.",
            "2004 will talk about the elastic net later on in the talk quantile regression, lianzhou, logistic regression and generalized linear models.",
            "My student Myong park.",
            "And more recently, the dancing selector.",
            "That's the work of Candace and tell it's a different approach to the last.",
            "Use the criterion slightly different, but the solutions very similar.",
            "This is in the context of signal processing and sparse representation, and another student of mine.",
            "Gareth James and and and and a postdoc Rodchenko developed a path album for that.",
            "So these old path algorithms and this has become a little mini craze in statistics to develop path algorithms for for for the next problem you can come, you know that you can.",
            "The problem is not many of them don't enjoy the piecewise linearity of the laws algorithm that made the laws algorithm special.",
            "You could compute the path very efficiently for a lot of these.",
            "At some of them do have piecewise linearity like the support vector machine and the group class, who has something like.",
            "Well, actually take that back.",
            "It doesn't, but some of the others do.",
            "But for many of them they don't, and so approximations who need an algorithm so much slower.",
            "For example, for generalized linear models, that algorithms, there's no piecewise linearity, and so for logistic regression, for example, the paths are piecewise smooth and you have to do a lot of computations, and so the computer.",
            "So the algorithm sees up for large problems.",
            "So there's a need for efficient algorithms.",
            "For law."
        ],
        [
            "Other problems?",
            "And so this brings us to the topic of today's coordinate descent.",
            "The idea is you solve the less you problem by coordinate descent, which means optimize each parameter separately, holding the others fixed.",
            "Now, this doesn't seem like it's going to lead to a foster algorithm.",
            "Right for solving systems of linear equations.",
            "This is the Gauss Seidel algorithm.",
            "In the context of lesu, it's something similar, but you optimize in one coordinate at the time.",
            "So why is this in the lead to an efficient algorithm?",
            "So I'm going to convince you of this today.",
            "And what we do.",
            "It turns out that all the updates are trivial, and that's one of the reasons it's it's very efficient.",
            "And what we do is we do this on a grid of Lambda values.",
            "That's a regularization parameter, so we don't get the exact path, but we get it on a finer grid as we choose starting from some Lambda Max down to Lambda Min, and we use warm starts and it turns out that's very efficient.",
            "What's really attractive is you can do this with a variety of loss functions and additive penalties, and you get this the same kind of efficiency.",
            "So coordinate descent.",
            "Achieves dramatic speedups over all competitors by factors of 10, sometimes 100, and more.",
            "And so the next few slides I'm going to show you.",
            "I'm going to demonstrate some of these."
        ],
        [
            "The speedup results.",
            "So use that same coefficient profile for the for the little example I had and we have the large profile or the lines and the coordinate descent profile superimposed with dots.",
            "And so you see there's 100 dots there, so we're approximating the path as accurately as you need too, and we can even make the the mesh fine if we like without much loss in time.",
            "So I'm going to show you some simulations and some and some performance results in real data."
        ],
        [
            "And then I'll tell you about the algorithm, the details after it, so the competitors are Lars algorithm as implemented in the R package.",
            "For squared error loss.",
            "GLM net, that's the package that implements our coordinate descent, so that's a four train based package which is as a front end in R. And it does squared error and logistic regression losses both for two and multiple class logistic regression.",
            "There's a recent competitor was L1 log Reg.",
            "That's a little too logistic regression package by colleagues in electrical engineering at Stanford, Steve Boyd and two of his students.",
            "Cohen, Kim.",
            "And that came out a year ago using state of the art interior point methods for convex optimization, and this is for two class logistic regression.",
            "And then.",
            "BBB R&B Mr That's stands for Bayesian binomial or multinomial regression.",
            "This is a package by Gangchen, Lewis and Madigan.",
            "Now surprisingly, they also use look coordinate descent even though it's Bayesian they actually go for the posterior mode using the Laplace prior and that amounts to a lasso penalized logistic regression.",
            "And and so they use coordinate descent as well.",
            "And and this is an example where the Devils in the details 'cause we have, you'll see we have a dramatic speedup."
        ],
        [
            "Over them as well.",
            "So let me show you the results.",
            "So yes, linear regression so squared error loss dense features.",
            "In other words, ends bigger than.",
            "Oh, not forgetten bigger than peed.",
            "We going to distinguish between sparse features and dense features, so you'll see that if the features are sparse, such as the bag of words model, the features are sparse in lots of zeros we can achieve even better efficiencies.",
            "So dense means not sparse.",
            "So we have two cases.",
            "The one is N is bigger than P505 thousand training 100 predictors.",
            "And here we have the times along the top here.",
            "You see the correlation.",
            "Average correlation amongst the features.",
            "So with coordinate descent, you'd expect if the features are very correlated, would take much longer because they're all competing for the same coefficients.",
            "A very coefficients are very correlated, and you'd think it would take a long time to settle down.",
            "So we we in the simulations we we control the correlation as well, didn't make a big difference.",
            "Jail inmate is is this new package.",
            "Here's the piecewise linear Lars algorithm is implemented in R, and in this case the speedup.",
            "It's faster GLM Nets faster, but not a huge amount faster.",
            "Switch things around so N is 100 PES, five 50,000 now 50,000 variables.",
            "Anne.",
            "It takes the limit longer, but still faster than the piecewise linear laws.",
            "We chose.",
            "The P = 100 or N = 100 On purpose becausw.",
            "For one property of this L1 penalty is for the linear model.",
            "Is that if if P is bigger than N, so 50,000 yen N is 100?",
            "No model has more than 100 nonzero coefficients, so that's a property of the L1 penalty.",
            "So even though you got 50,000 variables at no time, as you relax, the parameter is.",
            "Are there more than 100 non zero coefficients?",
            "Because if you think about it with a linear model, 100 coefficients allows you to have a saturated footwear.",
            "You can model perfectly, and so as you move down the regularization path, when you get to the least least regularised model, what you have is the exact fit of the data with the smallest L1 norm amongst all the infinite fits.",
            "That would give you exact fit of the data.",
            "So you have zero residual sum of squares, but smallest L1 norm and at no time are there more than 100 non zero coefficients.",
            "And the reason we chose 100 was becausw.",
            "And we did 100 steps of our piecewise linear path, and that's because the law's algorithm is essentially takes 100 steps to."
        ],
        [
            "Get to the solution 'cause it's of those piecewise linear steps to make the comparisons fair.",
            "Use logistic regression, also dense features.",
            "So this is where there's no piecewise linear exact algorithm, and GLM net is at the top.",
            "L1 log net.",
            "That's the convex optimization algorithm is below.",
            "We've got a factor of about 10 speed up on this on these examples.",
            "Both with.",
            "In bigger than P and with P bigger than N, yeah the fact up affect the speed up effect is a bit a bit higher maybe."
        ],
        [
            "20 times faster.",
            "Now we go to sparse features.",
            "And so we've got.",
            "Two cases here in is 10,000 Pisa 100.",
            "Here we we compare with BBR as well.",
            "This is logistic regression.",
            "So this is the the other coordinate descent algorithm.",
            "And delaminates Foster yeah, that's not a huge factor faster than than BBR, but but slightly bigger factor of 10 then an L1 log net.",
            "And likewise, when you switch NNP around PES 10,000 and he's 100 were faster as well.",
            "This night this is 95% zeros in the in the X matrix.",
            "That's our sparsity's.",
            "In order to compare with BBR BBR does automatic cross validation, so that's why in these runs this is the total time for 10 fold cross validation over a grid of a."
        ],
        [
            "100 values of Lambda.",
            "That was to make the comparisons fair.",
            "And so now for some real datasets.",
            "First of all, two dense datasets.",
            "The first one is a microarray classification example.",
            "It's a well known data set there, 14 classes, 144 observations, 16,000 genes.",
            "Um?",
            "Let's see and this is the time taken for tenfold cross validation.",
            "So in this first row?",
            "Yeah, that's going to be a.",
            "This is 14 classes, so that's going to be a multinomial or multiclass logistic regression model, 'cause you got 14 classes starting to compute the conditional probability of each of the classes given given X.",
            "But there's 16,000 variables, so you're going to have 16,000 * 14 coefficients in the model.",
            "It took Delaminate 2.5 minutes.",
            "And surprisingly it took BBR or BMR in this case 2.1 hours.",
            "So that was a factor of 60.",
            "Oh one draw.",
            "Greg doesn't do multiclass logistic regression.",
            "There's another data set.",
            "The leukemia data set.",
            "We did quite a bit faster than both and again, surprisingly faster than than then BBR.",
            "It's a two class problem.",
            "And then for sparse datasets.",
            "There's an Internet ad data set.",
            "It's just focused on the newsgroup.",
            "The newsgroup data set was a very big for US data.",
            "Set 11,000 training observations, three quarter of 1,000,000 features.",
            "But the data is very sparse.",
            "Report this sparsity.",
            "No, I don't, but it's it's.",
            "It's sparse data and yeah, so this is a two class classification problem.",
            "We compute the solution on 100 values of Lambda and.",
            "Jillan it took 2 minutes to complete.",
            "And L1 drug law, Greg took 3.5 hours and we couldn't get BBR to complete on this problem so.",
            "That's that's a huge factor, and my colleague Steve Boyd.",
            "Launched when he saw the results because their 3.5 hours was a bragging point in the in their paper.",
            "So we were pretty pleased with this.",
            "A lot of lot of the performance results are due to just careful tuning of the algorithm.",
            "Careful.",
            "Evaluation of when it's converged and what regions you've achieved."
        ],
        [
            "Urgent.",
            "OK, so.",
            "There's a history to L1 regularization for the lawsuit.",
            "So first in 1997, after the lawsuit paper at University of Toronto, Tip Shawnees student Wenjiang Fu develops what he called the shooting algorithm for the Lassoo.",
            "This was a coordinate descent algorithm.",
            "Tips are needed."
        ],
        [
            "I fully appreciate it and so never pursued it.",
            "In 2002, Ingrid Yodobashi is the famous wavelet person, gave gives a talk at Stanford, describes a one at a time algorithm for the lesu.",
            "I implemented it, made an error and and robbed Ronnie, and I conclude that the man."
        ],
        [
            "It doesn't work.",
            "2006 Friedman is external examiner at pH D. Oral of Anita vendor Queen, the University of Leiden, who uses coordinates ascent for the Elastic Net, which I'll tell you about in a moment.",
            "And he came back Friedman and myself and Rob revisit the problem and that got us to where we are today.",
            "Others have two, so there's there's papers by Krishna Puram and Hartemink in 2005 that used coordinate descent.",
            "Again, as I mentioned, Lincoln, Lewis and Madigan 2007 Woon Lang 2008 and Maya vendor Gear and Billman in 2008.",
            "That's where the group lawsuit.",
            "So the idea of using coordinate descent has been around for awhile and.",
            "But it's and lots of people are cutting onto it as well now."
        ],
        [
            "So.",
            "Let me show you how simple it is for the.",
            "For the less you, there's the residual sum of squares plus the penalty.",
            "We can a cycle over each of the coordinates one at a time until convergence, so you can see from the criterion if we're going to just optimize one parameter and hold all the others fixed.",
            "What we do is we computer partial residual with subtract all those terms from why?",
            "Who's baited we not optimizing right?",
            "And that would become the response, and now we have a simple univariate regression problem.",
            "And we if we assume that the variables are standardized, it turns out that the solution is characterized by the following.",
            "You just compute that univariant least squares coefficient, which is given by this expression here, 'cause each of the X is are standardized have unit.",
            "To have unit variance.",
            "And then use you update the coefficient by soft thresholding, which means you compute this coefficient and then soft thresholding means you subtract the amount Lambda Lambda the value of the penalty.",
            "Let's suppose it's positive you subtract Lambda from the coefficient, and if you hit zero, you set it to 0, otherwise you've shrunk it down by the amount lamb you've translated it by Lambda, so that's off thresholding, and that's all the the coordinate update is.",
            "It's a simple inner product to compute the new coefficient, and then the software shoulder.",
            "And I should tell you that for the lesu, especially in the beginning of the path, alot of the coefficients are zero and stay 0.",
            "So you compute the coefficient, you quickly see you not going to change it, you leave it at zero and nothing changes.",
            "So you move on to the next step."
        ],
        [
            "So that's one of the reasons why coordinate descent is really fast.",
            "But there's lots of other tricks that that that make these implementations very fast.",
            "So the first is that.",
            "Many of the coefficient, as I said, many of the coefficients tesero we need to compute this.",
            "This coefficient.",
            "Which is this operation over here?",
            "Well, that operation can be simplified into this expression.",
            "Over here, we're now instead of partial residuals.",
            "These are the overall residuals from the model, and this is the current value of the coefficient.",
            "So you just need to compute inner product between the variables.",
            "Each of the variables and the current residuals.",
            "And then you get that that coefficient very efficiently.",
            "So one of the one of the tricks is we call covariance updates, and this is for the end bigger than P case.",
            "You can cash in a product so when we compute this inner product over here, I've rewritten it here.",
            "That's an inner product between XJ&Y.",
            "I've written these X Rays are vector right of N vector and wise and in vector.",
            "So that's an inner product between XJ&Y minus.",
            "Some of the inner products between XJ and other variables in the active set active set been variables whose coefficients are non 0.",
            "Times the coefficients.",
            "Now you only need to complete these things once.",
            "So once you've computed each of these inner product, you store them away.",
            "It's the inner product between all the variables in the active set and all the other variables.",
            "And if the active sits quite small, you can do that.",
            "You can store them away and cash these things and so.",
            "The the the coordinate updates become much more efficient.",
            "Right, if these things are cashed.",
            "With sparse updates.",
            "So if the X is are sparse, that matrix is say 95% zeros.",
            "In a product so very easy to compute right, you only have to go over the non zero values.",
            "So you can really exploit exploit sparsity when you use coordinate descent.",
            "Whereas if you do not a big linear regression model in the X is sparse.",
            "You have to go too much more work to exploit sparsity.",
            "But for coordinate descent it's very natural.",
            "And then there's other tricks like active set convergence, so you know for any given value of Lambda, we cycling around, updating the coefficients.",
            "Well, in principle we need to go over all P variables all the time to see they don't change, but we've gotten a currently active set of nonzero coefficients, which is usually much, much smaller than the full set.",
            "And what we do is we actually converge on them and then check to see if anything in the non active set wants to come into the model.",
            "Often they don't, and then we done.",
            "So we initialized.",
            "We can quickly identify the smallest Lambda for which all the coefficients are zero.",
            "So we start right at the left end part of the path where all the coefficients are zero and that point you."
        ],
        [
            "In Trivoli find out and then we move from there.",
            "And now warm starts are natural.",
            "Two.",
            "We computing over greater values of Lambda and so that was a segue into into this.",
            "We start at the value of Lambda.",
            "We order coefficients are zero.",
            "The smallest value for which that's true that you can find out easily, and then we increment Lambda.",
            "We decrement Lambda small amounts down towards Lambda Min and so.",
            "What happens is that the active set grows from zero.",
            "That grows in fairly slowly, and so we have warm starts as we move from one Lambda to the next, and that adds to the efficiency.",
            "And finally we use the FFT to help the computation that stands for Friedman plus 410 plus tricks.",
            "Jerry Friedman's been a avid Fortran programmer for the last 30 something 40 years, and it's fun to work with him and he knows lots of tricks for for for avoiding unnecessary computations.",
            "I say no, no sloppy flops.",
            "You know you have to fight like hell to get him to to put a test for convergence inside one of the inner loops.",
            "You know, because that's a."
        ],
        [
            "Necessary computations.",
            "So for logistic models.",
            "I'm not going to go into details here we have.",
            "So now you gotta extra layer of nonlinearity, which basically amounts to an outer loop.",
            "We call it the Newton updates.",
            "So we essentially doing that using the Newton algorithm for fitting the logistic regression model.",
            "But each in a loop of the Newton algorithm is a weighted squared error loss problem, and so we just use the same algorithm for the inner loop and then update the outer loop and you can find details in up in our paper.",
            "From the multinomial multi class at similar we use a symmetric formulation, normally for the multinomial.",
            "If you've got K classes, you'd fit K -- 1 linear models because the probabilities sum to one.",
            "But because of the regularization we don't need to bother with that at awkward asymmetry, and so we have a symmetric model, and again you can find."
        ],
        [
            "Details in the paper.",
            "Another novelty in in what we do is we use the elastic net penalty, so this was introduced by a student of mine resort who's now at the University of Minnesota.",
            "It's a compromise penalty between the L2 and the L1 penalty.",
            "And so here we see it over here.",
            "This is just the penalty.",
            "So it's, um, Dover the P variables and what we have is a.",
            "It's a combination of this L2 and there's the L1 for each coefficient, and we have some additional parameter Alpha which controls a mix.",
            "And the half year is just to get a nice clean representation when we try and differentiate.",
            "So what the what the elastic net penalty does is.",
            "For the less you you get older you know you get at most.",
            "The last two sets a lot of coefficients to 0.",
            "The less you doesn't do too well, if you got very correlated predictors or features.",
            "So if you have a lot of features, are very correlated less, you cannot distinguish too well between the variables in their coefficients, and it tends to get a bit wild in situations like that the quadratic penalty.",
            "Causes the coefficients to behave very well.",
            "It makes some sort of snap towards each other and or be equal and share the coefficients.",
            "And so the elastic net.",
            "What it does is it was designed for microarray studies where you have groups of genes that are very color correlated.",
            "Genes in pathways.",
            "What it tends to do, select a whole group together and make their coefficients if they're very correlated, similar to each other.",
            "So you have this mix of L1 and L2.",
            "And often we will have this elastic net with the Ridge.",
            "The quadratic penalty we all have Alpha very close to one, so you have a little bit of quadratic, but mostly L1.",
            "And what it does is it gives you a sparse solution, but the coefficients are much better behaved, so it's a very handy penalty, and it turns out that the coordinate update for for this penalty is equally similar as simple, so it's a soft threshold in, just like in less you.",
            "That's the top term in the top divided by."
        ],
        [
            "Scaling factor.",
            "OK, so the last technique so it's equally.",
            "Simple, so this gives you an idea of the effect of that elastic net penalty.",
            "So we've got 3 examples yet in a.",
            "In a microarray problem with three 3 1/2 thousand genes and 72 training observations.",
            "So on the left we've got the less you path or part of the less you path we just go up to the first 10 steps basically.",
            "Here's the elastic Net path and what you see is there's more nonzero coefficients, but of course they shrunk more to 0 because there's more of them, but it still sparse, and if you go all the way and have pure quadratic, then all the coefficients, all three, another 1000 or 9 zero, but they shrunk heavily down towards 0, so this is typically not a very useful solution.",
            "This is a much more useful solution, and Beth tends to be better behaved in this solution.",
            "And incidentally, the support vector machine uses as many of you know, uses a quadratic penalty on the coefficients.",
            "All the coefficients are non 0, so it's."
        ],
        [
            "Coefficient profile looks something like this.",
            "Here's an example multiclass classification using a whole lot of different methods.",
            "It's it's that, oh, it's at example we saw earlier that took two minutes to to run.",
            "14 classes and we see that actually and with cross validation and everything in elastic net penalized multinomial model.",
            "Perform the best.",
            "These are small sample results and so you really have.",
            "I mean, the question is whether there's no not really a significant difference between these test error results because of the small test set of size 54.",
            "Nevertheless, people, including us still make these comparisons and."
        ],
        [
            "And chose 384 genes out of 16,000.",
            "So.",
            "How we doing for time?",
            "I want to do some time for questions.",
            "So if I take 5 more minutes, is that OK, yeah?",
            "So to.",
            "A brief summary.",
            "For coordinate descent to work.",
            "What you need is you need a loss function which is over here a differentiable loss function.",
            "And.",
            "And the convicts penalty.",
            "So the penalties convicts an additive.",
            "Then then you can.",
            "Then you can prove that coordinate descent converges to the solution, so both R&P each of the PJS are convex.",
            "And R is differentiable.",
            "It doesn't have to be differentiable, but in our case the cases we think of its differentiable encoding of descent converges to the unique solution to this problem.",
            "Anne.",
            "Often each coordinates step is trivial, such As for the lesu.",
            "And decreasing Lambda slowly means not much cycling is needed, so you can get the whole path.",
            "And.",
            "And as I said earlier, coordinate moves can exploit passive sparsity.",
            "Now, this additivity, yet these even though I've got it written as if the penalties are, there's a penalty for each coefficient.",
            "You can have groups of coefficients, so it can be blockwise additive."
        ],
        [
            "And you can still exploit coordinate descent.",
            "So I'm going to just briefly tell you about some other applications where we've had success with coordinate descent and others have.",
            "Most recently for us is undirected graphical models, so learning the dependent structure of of of so-called Gaussian graphical models via the less who in fact we've done it for non Gaussian as well for discrete graphical models.",
            "But I'll talk about the.",
            "The Gaussian case now and so.",
            "For those who work in graphical models, the standard approach for modeling sparsity is to work with the inverse covariance matrix, which captures the dependence structure.",
            "So wherever you've got zeros in the inverse covariance matrix means that the car partial correlation between that.",
            "Between those two variables is zero, and so you would not have a link in the in the graphical model.",
            "And so.",
            "Using maximum likelihood on the parameter on the inverse covariance matrix with an L1 penalty.",
            "On the on the inverse covariance there it is over here, so this is this is a P by P covariance matrix and we put an L1 penalty on all the all the parameters in the covariance matrix and this is just the partially maximized log likelihood for the Gaussian model.",
            "And it turns out that if you do blockwise coordinate descent.",
            "So.",
            "You've got a P by P covariance matrix you solving for all.",
            "A whole column of coefficients at a time.",
            "Turns out that solving that amounts to a kind of lesu regularization problem, so it's like a less you regression problem and you can exploit coordinate descent.",
            "So there's too many details to to go into it here, but again, we can do that very efficiently, and we can compute the whole path as we vary Lambda.",
            "So for example, we can solve moderately sparse graphs with 1000 nodes in under a minute.",
            "And I'll give you an example.",
            "Now this is a small problem.",
            "11 proteins measured on 7000."
        ],
        [
            "Training observations.",
            "And so the other graphical models you see the lambdas written above the plot when Lambda zero, Lambda zero over the it's an and.",
            "Yeah, it's a unregularized fit, so there's an edge between every protein.",
            "And then as you ramp up Lambda, the graph gets sparse sparser and sparser, so we can compute."
        ],
        [
            "The whole path of solutions very efficiently.",
            "The group last year.",
            "That's one in Linnan Mayan vendor gear.",
            "Two different groups and others have worked on it too.",
            "So each in that case, each of the terms the penalties refer to sets of parameters.",
            "And you don't have a 01 norm you over some of L2 norms.",
            "And so this term over here is actually the L2 norm of a vector of coefficients.",
            "So the variables are arranged in groups and each group has a set of linear coefficients and we we penalize the vector of coefficients for each group with an L2 norm.",
            "Notice it's not L2 squared, it's just L2.",
            "And what that does is a tends to select the whole group of variables or leave it out.",
            "And if they in they all non 0.",
            "And that's useful.",
            "For example, if you fit in linear models with categorical predictors with multiple levels and you represent each of those by dummy variable sense, so you have a group of coefficients.",
            "So you want that whole categorical variable in or out of the model, with all of its coefficients.",
            "And coordinate the sensor."
        ],
        [
            "Useful for solving that problem.",
            "And something we worked on a few years ago.",
            "CJ stands for comparative genome hybridization modeling and and we we introduced well tips Ronnie and colleagues introduced something called the Fuse Desu.",
            "So there in this particular example, the coefficients actually form a time series, so they are arranged not at time Series A series in genome order, and so there's enabling this notion to coefficients, and so we've got a penalty that both shrinks the coefficients to zero and and also constrains neighboring coefficients to be the same.",
            "So you're trying to encourage a piecewise constant food.",
            "And here's the solution that we get out, and again we used code and descent methods to solve that problem."
        ],
        [
            "And there's a paper on websites on that.",
            "So finish now and so in summary, then L1 regularization variance has become a powerful tool with the advent of wide data in compressed sensing, often exact sparse signal recovery is possible with L1 methods and coordinate descent in our view is the fastest known algorithm for solving these problems along a path of values for the.",
            "For the tuning parameter and so thank you very much and I'll be happy to take questions.",
            "Is a microphone you got?",
            "So let's take night.",
            "She introduced the quadratic penalty and which is a generalization of the lasso.",
            "So does that make sense to keep general generalize it into order of three and four and keep up K?",
            "That makes sense.",
            "I missed the first part of your questions or is so elastic and add general generalize dollar Sue, or by introducing the quadratic penalties, so does that make sense to keep generalized it in two out of three another fall, so OK?",
            "I I don't know which is key.",
            "The elastic net just gives you a compromise between the L1 and L2 regularization.",
            "So I'm not sure where you get in order K or you thinking of elves.",
            "Are you thinking of LK penalties?",
            "Were you using Adobe's so that that does not make sense?",
            "No, not really, because I see the OK penalties are somewhat intractable, not only easy ones or L1 and L2.",
            "OK, 4K bigger than two become nonlinear in an hard an for less than one or non convex and so those become hard as well.",
            "I go to some literature to all around this, thanks.",
            "There's a microphone coming.",
            "At the beginning, you mentioned that you can find the whole regularization path in a short time, but I think you have to provide a algorithm to find all the critical lambdas, isn't it?",
            "Because the whole the whole regularization path need all the lambdas.",
            "Well, we don't find all the lambdas in algorithm.",
            "We find the grid of lambdas right and we can we can refine the grid if you like.",
            "It turns out that if we use 100 values of Lambda and then we change and your 200 values of Lambda, it doesn't take twice as long.",
            "It takes a very short amount more because you got the warm starts or more effective.",
            "And so when you move you know half a step instead of a full step to the next grade value.",
            "Much less changes, and so you get there much more quickly.",
            "So your comparison with loss every IS in time.",
            "I think you have to set up some stopping criteria about your algorithm, isn't it?",
            "Well, what we did there is we had a orgasm computer 100 steps becausw laws took approximately 100 steps.",
            "That's where the break points in the piecewise linear work.",
            "So just to make it more compatible, OK, thank you.",
            "So.",
            "So so you said that one and two are the only easy cases and you ruled out anything less than one and anything bigger than two.",
            "But you didn't mention between one and two.",
            "Oh, those are hard too.",
            "Oh, another easy cases Infinity L Infinity is easy case.",
            "But yeah, now be between one and two is hard as well and why.",
            "Oh, but just because you got you know you get some nonlinearity in the penalty when you try and differentiate it.",
            "You find this term comes in the denominator and you know you have to iterate it.",
            "It's ugly.",
            "I have a question regarding the comparison of lasso regression versus Ridge regression.",
            "Certainly there's no question about the interpretability advantages, but have there been any rigorous comparisons of the out of sample accuracy of one versus the other?",
            "Yeah, there have been comparisons.",
            "Yeah, so Ridge regression is, you know, in a Bayesian sense, ideal when you have a Gaussian prior on the coefficients.",
            "So you can prove in that case that Ridge is that the optimal thing to do.",
            "So that's a situation where all the coefficients are non zero and perhaps a lot of them are small and centered around 0, but all are non zero.",
            "Then the optimal thing to do is Ridge regression.",
            "But what what happens in the case like that is, if P is very large, you have to regularize quite a lot, and even though it's the optimal thing to do, Ridge does poorly.",
            "As that anything else does even more poorly.",
            "So you kind of lost in a situation like that.",
            "If you've got lots of variables and relatively few training samples.",
            "OK, so nothing does very well but rich does the best.",
            "And so the only situations where you really well often these are more realistic real life situations or when you had lots of variables, but most of them are useless and there's some small subset that are useful.",
            "And then L1 regularization tends to do will do much better than than rich.",
            "Thank you.",
            "So so hey, so do you find the right non zero coefficients?",
            "So I guess biologists are very much interested in finding the right genes, the right influence factors and so on.",
            "Do you find the right non zero coefficients always?",
            "No, of course we never know, but that's that.",
            "Bears on the work of Donahue and Dan, Dan and Candace and tell where they they have some really interesting theorems now that show that if if you take your X matrix and if the pairwise correlations between the columns of X.",
            "Are bounded so.",
            "Nope, Nope.",
            "Two variables are two correlated.",
            "Then they can actually prove that L1 selection will find the L0 optimal solution, which means the best subset solution, the L1, which is a convex criterion, will actually find them, so they have results like that that prove that you actually do find with high probability these solutions.",
            "Now of course these bounds on the on the pairwise correlations are in realistic in practice, but.",
            "You know in in the areas where I worked in microarray studies, there are lots of correlations right?",
            "And so there is always some confusion as to whether you found the optimal.",
            "That's why we write we like this.",
            "This elastic net penalty because you sort of hedge your bets and you introduce all the correlated guys come in at the same time.",
            "So you say this group seems to be important and then you have to figure out.",
            "Later, you know which ones.",
            "Hi.",
            "A couple of questions.",
            "So in the rules in the Internet we have enough data, have a lot of data.",
            "So what is the perspective for this in the indicates where you have enough data.",
            "The second question is how do you answer that?",
            "I didn't even get the first question, so can you just say it again?",
            "So so you have this loss or method and all this regression method you use.",
            "Different algorithm is seems you say you have a lot of advantages in the wide data.",
            "So when we have enough data.",
            "Any is a much bigger than P. What is the advantage for that?",
            "So now you gotta big in and Big piece.",
            "Yeah, well and you kind of stuck because.",
            "You know you're still going to get an efficient algorithm, but it's going to be much slower if if the data is sparse.",
            "If you have sparsity, then that'll help, right?",
            "But it will still be.",
            "It'll still be an efficient algorithm.",
            "Efficiency comes from using the last two penalty 'cause you got a lot of zeros in your solution.",
            "So as long as you're interested in solutions with is, you know at points along the path where there's a lot of non zero coefficient, there's a lot of zero coefficients that less who is going to be an efficient way.",
            "I mean sorry, the coordinate descent is going to be efficient way to get to that.",
            "OK then that leads to the second question is when you have different you evaluate a different algorithm with especially emphasize our speeds.",
            "What about performance and how you evaluate the performance of different algorithm.",
            "Well, what do you mean by performance?",
            "Peacemaker is a is A1 model give you better performance or better now they giving you the same solution.",
            "They all elders.",
            "Comparisons here were all algorithms for fitting the same model.",
            "So there is no difference in the models, but if you apply that model to a holdout data set and what would be the difference will be will be well.",
            "What difference you'll see?",
            "You'll see a much.",
            "You'll get your solution much faster if you use GLM net, but other than that you're getting the same solution, so there's no difference in the solution.",
            "Our models of.",
            "I mean, we we provide an R package called GLM net which is free so anyone can get it and so then you have you have it's sort of friendly environment for for working with the model and working with your data.",
            "So for those who views Ariel you can just go in and use the package and maybe I'll convert some people to use in R in.",
            "In the audience, I guess what I mean is the performance is more predictive power.",
            "Like for example if you use a logistic regression then you need to for the.",
            "For the industry industry you need to make your model work in the future in terms of productivity.",
            "Let's you know there's lots of different models.",
            "This is assuming you want to use the logistic regression model.",
            "Alot of people like logistic regression models in these kinds of problems because you know when P is very big, it's pretty much all you can.",
            "You can work with.",
            "And they tend to perform well.",
            "I've my sources tell me that in the document classification problem, the logistic regression is the most popular one of the most popular models.",
            "Right now, regularize logistic regression?",
            "How does how does the algorithm perform when you have a mix of binary and scalar data?",
            "Oh yeah, there's no problem.",
            "You know you can mix.",
            "By me and quantitative data categorical variables, it works fine.",
            "Selection and then since I know.",
            "These features are relevant.",
            "Then I use I apply L2 Norm Noel theology.",
            "No, first apply L1 known to get a subset of features and then I just use those.",
            "This subset of features and then apply Ridge regression to make prediction.",
            "Since I do care the predictive accuracy, does it make sense?",
            "Well, I would.",
            "What actually I think makes makes more sense.",
            "Is yes user 01 to do the selection?",
            "But typically the complaint in is that you've now got the selected the solution as selected.",
            "Maybe a good set of variables, but it's regularize them too much, and so there's some bias.",
            "So what people do is then take the selected variables and fit them unregularized.",
            "To unbias him so you use the old one to do the feature selection, and then you fit the model with less regularization's.",
            "There's a whole host of of of proposals for doing that in a clever way, so for example, just run the last two twice.",
            "Unless you first time using cross validation and pick the point where it stops in, that'll tend to stop early because it's it doesn't want to let in all the noise variables.",
            "But now you you can find yourself just to the active set and rear.",
            "Unless you are now, it will go further with the cross validation and fit those coefficients with less regularization without having to worry about noise and these other proposals like that for doing that.",
            "So adaptive lasuen, so could you talk?",
            "I am over here.",
            "Could you talk just a little bit again about why it's good to have the entire regularization path and not just the.",
            "Like the best fit?",
            "Yeah, well so the reason you wanted is because these these models any of these regularization models are defined up to a tuning parameter.",
            "And that's not very good 'cause you don't really have a way of guessing a good value for the tuning Prem.",
            "So you need to have some other means for selecting the tuning parameter so we use cross validation or some measure of prediction error to select the tuning parameter.",
            "Or maybe some subjective stuff as well about how sparse we want the solution to be, but but in order to do that efficiently you need to be able to compute a lot of solution values for different values of Lambda and so that's why it's nice to be able to get the whole path.",
            "And curiously, for I didn't mention this in the talk, but if you want to, if you want to say for logistic regression model with a particular value of Lambda, let's say a non not too sparse solution.",
            "So Lambda quite small, so quite a lot of non zero coefficients.",
            "It turns out if you just ran coordinate descent on that problem right there with that value of Lambda would take more time to converge than if you did the whole path starting from zero.",
            "So the whole pool starting from zero gives you like a interior path.",
            "Root towards the solution you want and it does it faster than just computing the single solution.",
            "You mention in the beginning that regularization is introduced especially for the case when P is greater than one, right?",
            "So I have been working with the other regularization algorithm like LDA, but my in my case I just use cross validation to choose Lambda, but my observation is that when he is much larger than the Lambda is usually very small, but when P is a compara bulto earn, the Lambda is usually.",
            "Large, so I'm not sure if because there are some recent work that prove that eldia and regression are essentially equivalent under very mild conditions, so I'm not sure if you have any similar observation in the regression case.",
            "If yes, do you have any interpretation of this phenomenon?",
            "Yeah, well.",
            "That's that.",
            "Sounds like it's going the wrong way right?",
            "So so when you got P very large, you do.",
            "Yes, you do less regularization and when you when PES closer to N, yes, that's fine.",
            "I haven't seen that.",
            "I do know with classification problems if if the if the data allow you to get perfect separation.",
            "So even when P is much bigger than N, so this is in the case where support vector machines work very well that you don't really even know P is much bigger than in you end up doing very little regularization.",
            "Because you trying to find the optimal separating hyperplane, and often a very small amount of regularization's, all you need just to control the solution, but it gives you the optimal separator.",
            "But I don't have insight as to why when you add P closer to the end that you'd pick a more regularization.",
            "Could be that in those problems the amount of overlap was different.",
            "You know?",
            "Which plays a big role in in where you pick the tuning parameters.",
            "So just say yes or no question.",
            "So your problem is convex, so can coordinate.",
            "Dissent can guarantee to find the global solution theoretically yes.",
            "Thanks.",
            "Both.",
            "Thanks to all for this exciting talk and for answering all these questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome, this is a slightly different setting from the usual general session.",
                    "label": 0
                },
                {
                    "sent": "In that sense, you actually have table.",
                    "label": 0
                },
                {
                    "sent": "This is the setup that we can manage to get the day moving because this is where we'll have our lunch as well.",
                    "label": 0
                },
                {
                    "sent": "So we have it set up this way.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can utilize the table for taking notes.",
                    "label": 0
                },
                {
                    "sent": "Welcome again, we have an exciting day of sessions, presentations and activities ahead of us.",
                    "label": 0
                },
                {
                    "sent": "Hope you all had a good rest last night with that will get going for today, maybe one more minute.",
                    "label": 0
                },
                {
                    "sent": "People still trickling in.",
                    "label": 0
                },
                {
                    "sent": "OK. Hello everybody, it gives me great pleasure to be introducing Trevor Hastie to be giving us a talk today on fast regularization paths via coordinate descent.",
                    "label": 1
                },
                {
                    "sent": "Keyboard is well known in this community for his yellow book on elements of statistical learning.",
                    "label": 0
                },
                {
                    "sent": "For many of us who have gotten into data mining through the non statistical path, this has been a very valuable book.",
                    "label": 0
                },
                {
                    "sent": "It has given us the much needed foundations for understanding many of the data mining operators that we just take for granted, hence the idea of this talk.",
                    "label": 0
                },
                {
                    "sent": "We think that it's.",
                    "label": 0
                },
                {
                    "sent": "Going to be really valuable for us to understand data mining from one of the stalwarts in the field of statistics.",
                    "label": 0
                },
                {
                    "sent": "He has another book, this one is for statisticians on generalized additive models, which he has coauthored with Tipsy Ronnie.",
                    "label": 0
                },
                {
                    "sent": "He has been working in statistics for 30 years and of course he has made a diverse range of contributions in many different areas.",
                    "label": 0
                },
                {
                    "sent": "So if I have to just use 2 words to summarize most of what he has done, it's nonparametric regression and classification.",
                    "label": 0
                },
                {
                    "sent": "Currently he is applying.",
                    "label": 0
                },
                {
                    "sent": "All his tools for solving various problems in the bio and medicine and genomics kind of industry.",
                    "label": 0
                },
                {
                    "sent": "On the computing side, in fact, many of the libraries which are there in the famous statistical software packages like S Plus and R have been due to his influence and contribution.",
                    "label": 0
                },
                {
                    "sent": "But he confesses that his favorite language is in fact fortron, not the statistical languages.",
                    "label": 0
                },
                {
                    "sent": "He has been with the Department of Statistics in Stanford for almost 19 years.",
                    "label": 0
                },
                {
                    "sent": "First five years as a PhD student and then 2012 years as a free floating faculty member and for the past two years he has been on a sentence.",
                    "label": 0
                },
                {
                    "sent": "Apparently he's the chair of the Department and he's really waiting for that term to end in another one year.",
                    "label": 0
                },
                {
                    "sent": "And like many great statisticians, he has spent like 9.",
                    "label": 0
                },
                {
                    "sent": "Very productive and fun years in Bell Labs in its hey days so he's very fond of those that period two he grew up in South Africa in fact and he enjoys surfing and he continues to live with that passion, although sometimes the detour goes into the web hold.",
                    "label": 0
                },
                {
                    "sent": "So this he is very well known in the field and he has given many talks.",
                    "label": 0
                },
                {
                    "sent": "In fact this is his 24th keynote talk in over the last eight years.",
                    "label": 0
                },
                {
                    "sent": "He is fellow of.",
                    "label": 0
                },
                {
                    "sent": "The the American Statistical Foundation and the Institute of Mathematical Statistics, and I thought it was a big deal, but then he tells me in the morning today that he was a fellow of the Royal Statistical Society even before he got his PhD.",
                    "label": 0
                },
                {
                    "sent": "So without further delay, let me welcome Trevor to give us his talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you thinking.",
                    "label": 0
                },
                {
                    "sent": "Am I alive?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can everyone hear me?",
                    "label": 0
                },
                {
                    "sent": "Well, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "First of all, healing for organizing the conference and sonita for a very nice introduction.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "She asked me for some background dirt and I gave her quite a lot.",
                    "label": 0
                },
                {
                    "sent": "She spared me some of the worst, so thank you, so it's very nice to be here.",
                    "label": 0
                },
                {
                    "sent": "I know a lot of you are in the computer science field and it must really.",
                    "label": 0
                },
                {
                    "sent": "George is here.",
                    "label": 0
                },
                {
                    "sent": "Talk on coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "Becausw, I think I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "The sense being dismissed as a technique for optimization.",
                    "label": 0
                },
                {
                    "sent": "30 Among my engineering friends.",
                    "label": 0
                },
                {
                    "sent": "But undaunted, we we plunged ahead, and I hope our convince you today.",
                    "label": 0
                },
                {
                    "sent": "At least in this application.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with my like 2 coauthors Jerry Friedman and Rob Tibshirani from Stanford.",
                    "label": 0
                },
                {
                    "sent": "Yeah they are.",
                    "label": 0
                },
                {
                    "sent": "Enjoying one of their favorite pastimes.",
                    "label": 0
                },
                {
                    "sent": "Jerry's with a glass of Cabernet in his hand.",
                    "label": 0
                },
                {
                    "sent": "Actually, to be fair to Rob, that's my student Myong Park.",
                    "label": 0
                },
                {
                    "sent": "Graduating, and I wasn't able to be there.",
                    "label": 0
                },
                {
                    "sent": "So he he went in, and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rated R for me.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this talks about linear models.",
                    "label": 0
                },
                {
                    "sent": "And using linear models in data mining probably 10 years ago, we wouldn't give such a talk because linear models were somewhat out of phase fashion.",
                    "label": 0
                },
                {
                    "sent": "We were doing much more aggressive modeling.",
                    "label": 0
                },
                {
                    "sent": "But something's changed.",
                    "label": 0
                },
                {
                    "sent": "Datasets have grown wide.",
                    "label": 0
                },
                {
                    "sent": "Which means we have many more features and samples in a lot of applications.",
                    "label": 0
                },
                {
                    "sent": "And so the linear model is a regained favor in in the dataminers toolbox.",
                    "label": 0
                },
                {
                    "sent": "We have so many variables, often the linear model and in some applications is or.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can manage to do an an before we start overfitting, so you have some examples.",
                    "label": 0
                },
                {
                    "sent": "So document classification is A is a prime example.",
                    "label": 0
                },
                {
                    "sent": "We often use a bag of words model.",
                    "label": 0
                },
                {
                    "sent": "Where we take the collection of say words in English language and mark them as presents a or absent in a document and that can easily lead to say, 20,000 features for each document and we might have something like 5000 documents.",
                    "label": 0
                },
                {
                    "sent": "And so, in statistics jargon, we'd say P is bigger than N. We always use P for the number of variables and N for the number of samples.",
                    "label": 0
                },
                {
                    "sent": "So for example, the spam classification example is is is 1 email classification as spam or not spam?",
                    "label": 0
                },
                {
                    "sent": "It's a document.",
                    "label": 0
                },
                {
                    "sent": "We look at the set of words at least some of the early spam filters would do that and classify document as spam or not.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "Little embarrassingly when I was first invited by Sunita to be the keynote speaker in this in this conference.",
                    "label": 0
                },
                {
                    "sent": "The message went straight into my spam bucket.",
                    "label": 0
                },
                {
                    "sent": "And so I didn't see it.",
                    "label": 0
                },
                {
                    "sent": "And then she sent me a reminder 2 weeks later did I want to actually give this talk or not, and I had to tell her that I hadn't seen the message.",
                    "label": 0
                },
                {
                    "sent": "And sure enough, I found it in the in the spam bucket.",
                    "label": 0
                },
                {
                    "sent": "So my I guess my spam filter is still work in progress.",
                    "label": 0
                },
                {
                    "sent": "Image deblurring or classification.",
                    "label": 0
                },
                {
                    "sent": "So images notoriously have a large number of features if you use the pixels as as features.",
                    "label": 0
                },
                {
                    "sent": "And the number of samples can be relatively small.",
                    "label": 0
                },
                {
                    "sent": "An area where where I worked a lot is in genomics and so for example in microarray studies.",
                    "label": 0
                },
                {
                    "sent": "You can easily have 40,000 features which maybe represent versions of jeans and, and you might only have as few as 100 samples.",
                    "label": 0
                },
                {
                    "sent": "And more recently, in Genome Wide Association studies where we measure snips single nucleotide polymorphism's and they can easily be 500,000.",
                    "label": 0
                },
                {
                    "sent": "Lately even a million snips measured along the genome, and often they do these in for specific diseases.",
                    "label": 0
                },
                {
                    "sent": "They trying to discover causes of specific diseases, and you might have, say, 2000 case control samples.",
                    "label": 0
                },
                {
                    "sent": "So in all these cases we got many, many more features than we have observations and the linear model is very useful.",
                    "label": 0
                },
                {
                    "sent": "And we use them such as linear regression, logistic regression and similar models.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As we all know from basic statistics, you can't fit a linear model if P is bigger than in, and unless you do some kind of regularization.",
                    "label": 0
                },
                {
                    "sent": "So you either have to work with a subset of the variables or constrain the code.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions of the variables.",
                    "label": 0
                },
                {
                    "sent": "When you do the fit.",
                    "label": 0
                },
                {
                    "sent": "So the next slide is is a bit of crass advertising.",
                    "label": 0
                },
                {
                    "sent": "This is the second edition of our elements of statistical learning we've been saying.",
                    "label": 0
                },
                {
                    "sent": "I've been saying since 2004, then it's going to be coming out at the end of the year.",
                    "label": 0
                },
                {
                    "sent": "Will finally it is coming out at the end of the year.",
                    "label": 0
                },
                {
                    "sent": "It's done and we have.",
                    "label": 0
                },
                {
                    "sent": "There's a it's going to have about 200 extra pages and there will be extra chapters on wide data, which is the topic of the talk today on random forests, graphical models and ensemble methods.",
                    "label": 0
                },
                {
                    "sent": "And a lot of other new material scattered throughout the book on path algorithms, which is also the topic of today.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods and more.",
                    "label": 0
                },
                {
                    "sent": "So that should be out in December and.",
                    "label": 0
                },
                {
                    "sent": "For those who interested tips, Ronnie and I teach a two day course twice a year, and all the topics, especially the new topics in the book, are covered in in the course of the next time we teach.",
                    "label": 0
                },
                {
                    "sent": "It is in October in Boston, and if you look on either his web page or mine.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find details of the course.",
                    "label": 0
                },
                {
                    "sent": "So I guess the topic today starts with the lassoo, which is regular L1 regularization for linear regression introduced by tips, Ronnie in 1995.",
                    "label": 0
                },
                {
                    "sent": "And we see we have a linear model, just a linear regression model, the X the why is the response, the excise or the predictors one through P or the features?",
                    "label": 0
                },
                {
                    "sent": "And we fit the model by least squares subject to an L1 constraint on the coefficients.",
                    "label": 0
                },
                {
                    "sent": "So there's the regularization we bound the sum of the absolute values of the coefficients to be smaller than some number T. Now.",
                    "label": 0
                },
                {
                    "sent": "If the balance if she's big enough.",
                    "label": 0
                },
                {
                    "sent": "And bigger than, say, the L1 norm of the unrestricted least squares coefficients.",
                    "label": 0
                },
                {
                    "sent": "It will be unregularized, but if P is bigger than N we wouldn't be able to compute that fit.",
                    "label": 0
                },
                {
                    "sent": "They'd be infinitely many, so by tightening the bound we constrain the fit and give an leads to a unique solution.",
                    "label": 0
                },
                {
                    "sent": "And similar to older technique in statistics called Ridge regression, where we use the quadratic penalty for controlling the coefficients.",
                    "label": 0
                },
                {
                    "sent": "And what does these bounds do is pull the coefficients towards 0 and that restricts them.",
                    "label": 0
                },
                {
                    "sent": "It also reduces the variance if you pull them all the way to 0.",
                    "label": 0
                },
                {
                    "sent": "Of course their variance is zero and but you've gotten all fit.",
                    "label": 0
                },
                {
                    "sent": "So what's the big deal?",
                    "label": 0
                },
                {
                    "sent": "Is the difference between L1 and L2?",
                    "label": 0
                },
                {
                    "sent": "Well, the main thing is that the L1 norm said some of the coefficients to 0, and that's very useful because not only did we regularize the fit, we get rid of a lot of the variables and we left with presumably the more important variables.",
                    "label": 0
                },
                {
                    "sent": "And this little picture here, which is in our book as well, sort of shows you why that happens.",
                    "label": 0
                },
                {
                    "sent": "The L1 ball is a diamond shaped ball, whereas the L2 ball is a spherical ball and the contours of this least squares criterion which we see there there's ellipsoidal contours.",
                    "label": 0
                },
                {
                    "sent": "We moved out on the contours until we hit the constraint ball and with the L1 ball is a good chance we'll hit it on the corner or in a sharp edge where, which means some of the coefficients are set to 0.",
                    "label": 0
                },
                {
                    "sent": "So for simply for that reason.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "L1 regularization has become very popular.",
                    "label": 0
                },
                {
                    "sent": "So let me give you a brief history of L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "It really started and in statistics at least, it's probably been around for a very long time, but in statistics it became first prominent with wavelet soft thresholding.",
                    "label": 0
                },
                {
                    "sent": "So with wavelet basis.",
                    "label": 0
                },
                {
                    "sent": "Usually orthonormal basis.",
                    "label": 0
                },
                {
                    "sent": "Donna Hearn Johnston found you could do wavelet selection by using L1 regularization, and you'd set a lot of wavelet coefficients to zero and some would be shrunk and I'll show you the software sold in operation in in a moment.",
                    "label": 0
                },
                {
                    "sent": "And then, but those that was for an orthonormal basis and then tip serani introduced to Lesu, which was the same idea for general sets of predictors.",
                    "label": 0
                },
                {
                    "sent": "Non orthogonal predictors in 1995.",
                    "label": 0
                },
                {
                    "sent": "The same idea was used in basis pursuit by 10 Donna and Saunders in 1996, again in a wavelet context.",
                    "label": 0
                },
                {
                    "sent": "But now when you have dictionaries of different wavelet bases and so you lose orthogonality across the basis for getting a compact representation of a signal.",
                    "label": 0
                },
                {
                    "sent": "Since then, a lot of activity is taking place with their suits being extended to many linear model settings.",
                    "label": 0
                },
                {
                    "sent": "For example, survival models tips around in 1997, logistic regression models, and so on.",
                    "label": 0
                },
                {
                    "sent": "And more recently, there's a whole new field involved in in.",
                    "label": 0
                },
                {
                    "sent": "In the signal processing community called compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "The name is due to Donna her 2004 and also Candace and Tower 2005.",
                    "label": 0
                },
                {
                    "sent": "And the idea is you can perform near exact recovery of sparse signals in very high dimensions.",
                    "label": 0
                },
                {
                    "sent": "In many cases.",
                    "label": 0
                },
                {
                    "sent": "In this context, you think of the L1 as a good surrogate for L0 regularization, where you're trying to find a very compact basis for representing the signal.",
                    "label": 0
                },
                {
                    "sent": "That's a NP complete problem.",
                    "label": 0
                },
                {
                    "sent": "Use L1 regularization and they have all kinds of nice theorems that show that even though using L1 instead of L0, you actually can recover the the optimal basis with with very very high probability.",
                    "label": 0
                },
                {
                    "sent": "So this is becoming very hot.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Area.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a picture.",
                    "label": 0
                },
                {
                    "sent": "Of the less what we call the coefficient profile.",
                    "label": 0
                },
                {
                    "sent": "So it's again the last few problem at the bottom of the screen I've put the criterion here in a slightly different form that we had before.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a bound on the coefficient, we call this the Lagrangian form.",
                    "label": 0
                },
                {
                    "sent": "We have the residual sum of squares plus Lambda times the L1 norm of the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "So beta year is a vector of coefficients, and we put an L1 norm and on those and we optimize it.",
                    "label": 0
                },
                {
                    "sent": "Nation of residual sum of squares plus Lambda times abound as you increase Lambda.",
                    "label": 0
                },
                {
                    "sent": "You'll force the code.",
                    "label": 0
                },
                {
                    "sent": "The solution will have the coefficients forced towards 0.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "So this is a small example.",
                    "label": 0
                },
                {
                    "sent": "There's eight predictors and we start off with Lambda very big, so all the coefficients are zero, and that's at the left part of the plot.",
                    "label": 0
                },
                {
                    "sent": "Here this is the zero line here, and each of these curves is a coefficient.",
                    "label": 0
                },
                {
                    "sent": "As we relax Lambda.",
                    "label": 0
                },
                {
                    "sent": "And so as we relax Lambda, the coefficients grow away from zero, and in this case there's more N is bigger than P. So unrestricted least squares fit is given on the right, and we see the path of the coefficients.",
                    "label": 0
                },
                {
                    "sent": "So we call that a regularization path.",
                    "label": 0
                },
                {
                    "sent": "And it's very useful to have such a path, because at the end of the day you need to pick the parameter Lambda or the bounty on for the coefficients, and so having the whole path let's gives you a means for picking a sensible value.",
                    "label": 0
                },
                {
                    "sent": "And awful often we'll use cross validation or some measure of prediction error to pick the value, but it's useful to have the whole path.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "Solving this problem just naively, it's a convex optimization problem, requires quadratic programming, and you can you know it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly easy quadratic program, but you can solve the problem that way and that's originally it was solved.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In in 2001, Efron and coauthors I was one of them.",
                    "label": 0
                },
                {
                    "sent": "But everyone really discovered that you could solve this problem in much more efficiently, and the reason was is that the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficient paths, the profiles are all piecewise linear.",
                    "label": 0
                },
                {
                    "sent": "So if you look carefully at this picture, you'll see you can see the piecewise linear parts and each of the vertical lines indicate the breakpoints as it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Changes.",
                    "label": 0
                },
                {
                    "sent": "And that led to the what's known as the laws algorithm for solving this.",
                    "label": 0
                },
                {
                    "sent": "This convict this whole path of of solutions.",
                    "label": 0
                },
                {
                    "sent": "And to summarize, you can compute the entire path in the same computations as a single least squares fit.",
                    "label": 0
                },
                {
                    "sent": "So that's very efficient.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the only there's a computation needed for each of the steps.",
                    "label": 0
                },
                {
                    "sent": "And so that led to huge speedup in computing the last two profile.",
                    "label": 0
                },
                {
                    "sent": "So that was called the law's algorithm.",
                    "label": 0
                },
                {
                    "sent": "The names rather obscured sense for least angle regression.",
                    "label": 0
                },
                {
                    "sent": "And as I say, is due to effort.",
                    "label": 0
                },
                {
                    "sent": "And that's what happened in that.",
                    "label": 0
                },
                {
                    "sent": "So that was in 2001, and that in turn led to a huge flood of path algorithms for all kinds of different regularization problems.",
                    "label": 0
                },
                {
                    "sent": "No, the idea that you could get the entire solution path efficiently was very attractive, and because in any problem where you got regularization, you need to compute the solution at many points and see if you can compute the whole path efficiently.",
                    "label": 0
                },
                {
                    "sent": "That was a very attractive thing.",
                    "label": 0
                },
                {
                    "sent": "So there's I've listed some of the examples.",
                    "label": 0
                },
                {
                    "sent": "There's lots more.",
                    "label": 0
                },
                {
                    "sent": "This is a biased list as you'll see my name is is an names of my students, so in a lot of these.",
                    "label": 0
                },
                {
                    "sent": "But the first one, the group class who this is for the case where.",
                    "label": 0
                },
                {
                    "sent": "We variables naturally come in groups, will talk about that a little bit later, one in learning 2006, a developer path algorithm.",
                    "label": 0
                },
                {
                    "sent": "For support vector machine, the cost parameter in the support vector machine is a tuning parameter regularization parameter, and it turns out there's a piecewise linear path for the solutions of the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "As you vary the cost parameter.",
                    "label": 0
                },
                {
                    "sent": "And that was work with some of my students in tips, Ronnie.",
                    "label": 0
                },
                {
                    "sent": "2004 will talk about the elastic net later on in the talk quantile regression, lianzhou, logistic regression and generalized linear models.",
                    "label": 0
                },
                {
                    "sent": "My student Myong park.",
                    "label": 0
                },
                {
                    "sent": "And more recently, the dancing selector.",
                    "label": 0
                },
                {
                    "sent": "That's the work of Candace and tell it's a different approach to the last.",
                    "label": 0
                },
                {
                    "sent": "Use the criterion slightly different, but the solutions very similar.",
                    "label": 0
                },
                {
                    "sent": "This is in the context of signal processing and sparse representation, and another student of mine.",
                    "label": 0
                },
                {
                    "sent": "Gareth James and and and and a postdoc Rodchenko developed a path album for that.",
                    "label": 0
                },
                {
                    "sent": "So these old path algorithms and this has become a little mini craze in statistics to develop path algorithms for for for the next problem you can come, you know that you can.",
                    "label": 0
                },
                {
                    "sent": "The problem is not many of them don't enjoy the piecewise linearity of the laws algorithm that made the laws algorithm special.",
                    "label": 0
                },
                {
                    "sent": "You could compute the path very efficiently for a lot of these.",
                    "label": 0
                },
                {
                    "sent": "At some of them do have piecewise linearity like the support vector machine and the group class, who has something like.",
                    "label": 0
                },
                {
                    "sent": "Well, actually take that back.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, but some of the others do.",
                    "label": 0
                },
                {
                    "sent": "But for many of them they don't, and so approximations who need an algorithm so much slower.",
                    "label": 0
                },
                {
                    "sent": "For example, for generalized linear models, that algorithms, there's no piecewise linearity, and so for logistic regression, for example, the paths are piecewise smooth and you have to do a lot of computations, and so the computer.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm sees up for large problems.",
                    "label": 0
                },
                {
                    "sent": "So there's a need for efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "For law.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other problems?",
                    "label": 0
                },
                {
                    "sent": "And so this brings us to the topic of today's coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "The idea is you solve the less you problem by coordinate descent, which means optimize each parameter separately, holding the others fixed.",
                    "label": 0
                },
                {
                    "sent": "Now, this doesn't seem like it's going to lead to a foster algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right for solving systems of linear equations.",
                    "label": 0
                },
                {
                    "sent": "This is the Gauss Seidel algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the context of lesu, it's something similar, but you optimize in one coordinate at the time.",
                    "label": 0
                },
                {
                    "sent": "So why is this in the lead to an efficient algorithm?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to convince you of this today.",
                    "label": 0
                },
                {
                    "sent": "And what we do.",
                    "label": 0
                },
                {
                    "sent": "It turns out that all the updates are trivial, and that's one of the reasons it's it's very efficient.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we do this on a grid of Lambda values.",
                    "label": 0
                },
                {
                    "sent": "That's a regularization parameter, so we don't get the exact path, but we get it on a finer grid as we choose starting from some Lambda Max down to Lambda Min, and we use warm starts and it turns out that's very efficient.",
                    "label": 0
                },
                {
                    "sent": "What's really attractive is you can do this with a variety of loss functions and additive penalties, and you get this the same kind of efficiency.",
                    "label": 0
                },
                {
                    "sent": "So coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "Achieves dramatic speedups over all competitors by factors of 10, sometimes 100, and more.",
                    "label": 0
                },
                {
                    "sent": "And so the next few slides I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "I'm going to demonstrate some of these.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The speedup results.",
                    "label": 0
                },
                {
                    "sent": "So use that same coefficient profile for the for the little example I had and we have the large profile or the lines and the coordinate descent profile superimposed with dots.",
                    "label": 0
                },
                {
                    "sent": "And so you see there's 100 dots there, so we're approximating the path as accurately as you need too, and we can even make the the mesh fine if we like without much loss in time.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show you some simulations and some and some performance results in real data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'll tell you about the algorithm, the details after it, so the competitors are Lars algorithm as implemented in the R package.",
                    "label": 0
                },
                {
                    "sent": "For squared error loss.",
                    "label": 0
                },
                {
                    "sent": "GLM net, that's the package that implements our coordinate descent, so that's a four train based package which is as a front end in R. And it does squared error and logistic regression losses both for two and multiple class logistic regression.",
                    "label": 0
                },
                {
                    "sent": "There's a recent competitor was L1 log Reg.",
                    "label": 0
                },
                {
                    "sent": "That's a little too logistic regression package by colleagues in electrical engineering at Stanford, Steve Boyd and two of his students.",
                    "label": 0
                },
                {
                    "sent": "Cohen, Kim.",
                    "label": 0
                },
                {
                    "sent": "And that came out a year ago using state of the art interior point methods for convex optimization, and this is for two class logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "BBB R&B Mr That's stands for Bayesian binomial or multinomial regression.",
                    "label": 0
                },
                {
                    "sent": "This is a package by Gangchen, Lewis and Madigan.",
                    "label": 0
                },
                {
                    "sent": "Now surprisingly, they also use look coordinate descent even though it's Bayesian they actually go for the posterior mode using the Laplace prior and that amounts to a lasso penalized logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And and so they use coordinate descent as well.",
                    "label": 0
                },
                {
                    "sent": "And and this is an example where the Devils in the details 'cause we have, you'll see we have a dramatic speedup.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over them as well.",
                    "label": 0
                },
                {
                    "sent": "So let me show you the results.",
                    "label": 0
                },
                {
                    "sent": "So yes, linear regression so squared error loss dense features.",
                    "label": 0
                },
                {
                    "sent": "In other words, ends bigger than.",
                    "label": 0
                },
                {
                    "sent": "Oh, not forgetten bigger than peed.",
                    "label": 0
                },
                {
                    "sent": "We going to distinguish between sparse features and dense features, so you'll see that if the features are sparse, such as the bag of words model, the features are sparse in lots of zeros we can achieve even better efficiencies.",
                    "label": 0
                },
                {
                    "sent": "So dense means not sparse.",
                    "label": 0
                },
                {
                    "sent": "So we have two cases.",
                    "label": 0
                },
                {
                    "sent": "The one is N is bigger than P505 thousand training 100 predictors.",
                    "label": 0
                },
                {
                    "sent": "And here we have the times along the top here.",
                    "label": 0
                },
                {
                    "sent": "You see the correlation.",
                    "label": 0
                },
                {
                    "sent": "Average correlation amongst the features.",
                    "label": 0
                },
                {
                    "sent": "So with coordinate descent, you'd expect if the features are very correlated, would take much longer because they're all competing for the same coefficients.",
                    "label": 0
                },
                {
                    "sent": "A very coefficients are very correlated, and you'd think it would take a long time to settle down.",
                    "label": 0
                },
                {
                    "sent": "So we we in the simulations we we control the correlation as well, didn't make a big difference.",
                    "label": 0
                },
                {
                    "sent": "Jail inmate is is this new package.",
                    "label": 0
                },
                {
                    "sent": "Here's the piecewise linear Lars algorithm is implemented in R, and in this case the speedup.",
                    "label": 0
                },
                {
                    "sent": "It's faster GLM Nets faster, but not a huge amount faster.",
                    "label": 0
                },
                {
                    "sent": "Switch things around so N is 100 PES, five 50,000 now 50,000 variables.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It takes the limit longer, but still faster than the piecewise linear laws.",
                    "label": 0
                },
                {
                    "sent": "We chose.",
                    "label": 0
                },
                {
                    "sent": "The P = 100 or N = 100 On purpose becausw.",
                    "label": 0
                },
                {
                    "sent": "For one property of this L1 penalty is for the linear model.",
                    "label": 0
                },
                {
                    "sent": "Is that if if P is bigger than N, so 50,000 yen N is 100?",
                    "label": 0
                },
                {
                    "sent": "No model has more than 100 nonzero coefficients, so that's a property of the L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "So even though you got 50,000 variables at no time, as you relax, the parameter is.",
                    "label": 0
                },
                {
                    "sent": "Are there more than 100 non zero coefficients?",
                    "label": 0
                },
                {
                    "sent": "Because if you think about it with a linear model, 100 coefficients allows you to have a saturated footwear.",
                    "label": 0
                },
                {
                    "sent": "You can model perfectly, and so as you move down the regularization path, when you get to the least least regularised model, what you have is the exact fit of the data with the smallest L1 norm amongst all the infinite fits.",
                    "label": 0
                },
                {
                    "sent": "That would give you exact fit of the data.",
                    "label": 0
                },
                {
                    "sent": "So you have zero residual sum of squares, but smallest L1 norm and at no time are there more than 100 non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "And the reason we chose 100 was becausw.",
                    "label": 0
                },
                {
                    "sent": "And we did 100 steps of our piecewise linear path, and that's because the law's algorithm is essentially takes 100 steps to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get to the solution 'cause it's of those piecewise linear steps to make the comparisons fair.",
                    "label": 0
                },
                {
                    "sent": "Use logistic regression, also dense features.",
                    "label": 0
                },
                {
                    "sent": "So this is where there's no piecewise linear exact algorithm, and GLM net is at the top.",
                    "label": 0
                },
                {
                    "sent": "L1 log net.",
                    "label": 0
                },
                {
                    "sent": "That's the convex optimization algorithm is below.",
                    "label": 0
                },
                {
                    "sent": "We've got a factor of about 10 speed up on this on these examples.",
                    "label": 0
                },
                {
                    "sent": "Both with.",
                    "label": 0
                },
                {
                    "sent": "In bigger than P and with P bigger than N, yeah the fact up affect the speed up effect is a bit a bit higher maybe.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20 times faster.",
                    "label": 0
                },
                {
                    "sent": "Now we go to sparse features.",
                    "label": 0
                },
                {
                    "sent": "And so we've got.",
                    "label": 0
                },
                {
                    "sent": "Two cases here in is 10,000 Pisa 100.",
                    "label": 0
                },
                {
                    "sent": "Here we we compare with BBR as well.",
                    "label": 0
                },
                {
                    "sent": "This is logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So this is the the other coordinate descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "And delaminates Foster yeah, that's not a huge factor faster than than BBR, but but slightly bigger factor of 10 then an L1 log net.",
                    "label": 0
                },
                {
                    "sent": "And likewise, when you switch NNP around PES 10,000 and he's 100 were faster as well.",
                    "label": 0
                },
                {
                    "sent": "This night this is 95% zeros in the in the X matrix.",
                    "label": 0
                },
                {
                    "sent": "That's our sparsity's.",
                    "label": 0
                },
                {
                    "sent": "In order to compare with BBR BBR does automatic cross validation, so that's why in these runs this is the total time for 10 fold cross validation over a grid of a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "100 values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "That was to make the comparisons fair.",
                    "label": 0
                },
                {
                    "sent": "And so now for some real datasets.",
                    "label": 0
                },
                {
                    "sent": "First of all, two dense datasets.",
                    "label": 0
                },
                {
                    "sent": "The first one is a microarray classification example.",
                    "label": 0
                },
                {
                    "sent": "It's a well known data set there, 14 classes, 144 observations, 16,000 genes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's see and this is the time taken for tenfold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So in this first row?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's going to be a.",
                    "label": 0
                },
                {
                    "sent": "This is 14 classes, so that's going to be a multinomial or multiclass logistic regression model, 'cause you got 14 classes starting to compute the conditional probability of each of the classes given given X.",
                    "label": 0
                },
                {
                    "sent": "But there's 16,000 variables, so you're going to have 16,000 * 14 coefficients in the model.",
                    "label": 0
                },
                {
                    "sent": "It took Delaminate 2.5 minutes.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly it took BBR or BMR in this case 2.1 hours.",
                    "label": 0
                },
                {
                    "sent": "So that was a factor of 60.",
                    "label": 0
                },
                {
                    "sent": "Oh one draw.",
                    "label": 0
                },
                {
                    "sent": "Greg doesn't do multiclass logistic regression.",
                    "label": 0
                },
                {
                    "sent": "There's another data set.",
                    "label": 0
                },
                {
                    "sent": "The leukemia data set.",
                    "label": 0
                },
                {
                    "sent": "We did quite a bit faster than both and again, surprisingly faster than than then BBR.",
                    "label": 0
                },
                {
                    "sent": "It's a two class problem.",
                    "label": 0
                },
                {
                    "sent": "And then for sparse datasets.",
                    "label": 0
                },
                {
                    "sent": "There's an Internet ad data set.",
                    "label": 0
                },
                {
                    "sent": "It's just focused on the newsgroup.",
                    "label": 0
                },
                {
                    "sent": "The newsgroup data set was a very big for US data.",
                    "label": 0
                },
                {
                    "sent": "Set 11,000 training observations, three quarter of 1,000,000 features.",
                    "label": 0
                },
                {
                    "sent": "But the data is very sparse.",
                    "label": 0
                },
                {
                    "sent": "Report this sparsity.",
                    "label": 0
                },
                {
                    "sent": "No, I don't, but it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's sparse data and yeah, so this is a two class classification problem.",
                    "label": 0
                },
                {
                    "sent": "We compute the solution on 100 values of Lambda and.",
                    "label": 0
                },
                {
                    "sent": "Jillan it took 2 minutes to complete.",
                    "label": 0
                },
                {
                    "sent": "And L1 drug law, Greg took 3.5 hours and we couldn't get BBR to complete on this problem so.",
                    "label": 0
                },
                {
                    "sent": "That's that's a huge factor, and my colleague Steve Boyd.",
                    "label": 0
                },
                {
                    "sent": "Launched when he saw the results because their 3.5 hours was a bragging point in the in their paper.",
                    "label": 0
                },
                {
                    "sent": "So we were pretty pleased with this.",
                    "label": 0
                },
                {
                    "sent": "A lot of lot of the performance results are due to just careful tuning of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Careful.",
                    "label": 0
                },
                {
                    "sent": "Evaluation of when it's converged and what regions you've achieved.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Urgent.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's a history to L1 regularization for the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "So first in 1997, after the lawsuit paper at University of Toronto, Tip Shawnees student Wenjiang Fu develops what he called the shooting algorithm for the Lassoo.",
                    "label": 1
                },
                {
                    "sent": "This was a coordinate descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "Tips are needed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I fully appreciate it and so never pursued it.",
                    "label": 0
                },
                {
                    "sent": "In 2002, Ingrid Yodobashi is the famous wavelet person, gave gives a talk at Stanford, describes a one at a time algorithm for the lesu.",
                    "label": 1
                },
                {
                    "sent": "I implemented it, made an error and and robbed Ronnie, and I conclude that the man.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "2006 Friedman is external examiner at pH D. Oral of Anita vendor Queen, the University of Leiden, who uses coordinates ascent for the Elastic Net, which I'll tell you about in a moment.",
                    "label": 0
                },
                {
                    "sent": "And he came back Friedman and myself and Rob revisit the problem and that got us to where we are today.",
                    "label": 0
                },
                {
                    "sent": "Others have two, so there's there's papers by Krishna Puram and Hartemink in 2005 that used coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "Again, as I mentioned, Lincoln, Lewis and Madigan 2007 Woon Lang 2008 and Maya vendor Gear and Billman in 2008.",
                    "label": 0
                },
                {
                    "sent": "That's where the group lawsuit.",
                    "label": 0
                },
                {
                    "sent": "So the idea of using coordinate descent has been around for awhile and.",
                    "label": 0
                },
                {
                    "sent": "But it's and lots of people are cutting onto it as well now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me show you how simple it is for the.",
                    "label": 0
                },
                {
                    "sent": "For the less you, there's the residual sum of squares plus the penalty.",
                    "label": 0
                },
                {
                    "sent": "We can a cycle over each of the coordinates one at a time until convergence, so you can see from the criterion if we're going to just optimize one parameter and hold all the others fixed.",
                    "label": 1
                },
                {
                    "sent": "What we do is we computer partial residual with subtract all those terms from why?",
                    "label": 0
                },
                {
                    "sent": "Who's baited we not optimizing right?",
                    "label": 0
                },
                {
                    "sent": "And that would become the response, and now we have a simple univariate regression problem.",
                    "label": 0
                },
                {
                    "sent": "And we if we assume that the variables are standardized, it turns out that the solution is characterized by the following.",
                    "label": 0
                },
                {
                    "sent": "You just compute that univariant least squares coefficient, which is given by this expression here, 'cause each of the X is are standardized have unit.",
                    "label": 0
                },
                {
                    "sent": "To have unit variance.",
                    "label": 0
                },
                {
                    "sent": "And then use you update the coefficient by soft thresholding, which means you compute this coefficient and then soft thresholding means you subtract the amount Lambda Lambda the value of the penalty.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose it's positive you subtract Lambda from the coefficient, and if you hit zero, you set it to 0, otherwise you've shrunk it down by the amount lamb you've translated it by Lambda, so that's off thresholding, and that's all the the coordinate update is.",
                    "label": 0
                },
                {
                    "sent": "It's a simple inner product to compute the new coefficient, and then the software shoulder.",
                    "label": 0
                },
                {
                    "sent": "And I should tell you that for the lesu, especially in the beginning of the path, alot of the coefficients are zero and stay 0.",
                    "label": 1
                },
                {
                    "sent": "So you compute the coefficient, you quickly see you not going to change it, you leave it at zero and nothing changes.",
                    "label": 0
                },
                {
                    "sent": "So you move on to the next step.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's one of the reasons why coordinate descent is really fast.",
                    "label": 0
                },
                {
                    "sent": "But there's lots of other tricks that that that make these implementations very fast.",
                    "label": 0
                },
                {
                    "sent": "So the first is that.",
                    "label": 0
                },
                {
                    "sent": "Many of the coefficient, as I said, many of the coefficients tesero we need to compute this.",
                    "label": 0
                },
                {
                    "sent": "This coefficient.",
                    "label": 0
                },
                {
                    "sent": "Which is this operation over here?",
                    "label": 0
                },
                {
                    "sent": "Well, that operation can be simplified into this expression.",
                    "label": 0
                },
                {
                    "sent": "Over here, we're now instead of partial residuals.",
                    "label": 0
                },
                {
                    "sent": "These are the overall residuals from the model, and this is the current value of the coefficient.",
                    "label": 1
                },
                {
                    "sent": "So you just need to compute inner product between the variables.",
                    "label": 0
                },
                {
                    "sent": "Each of the variables and the current residuals.",
                    "label": 0
                },
                {
                    "sent": "And then you get that that coefficient very efficiently.",
                    "label": 0
                },
                {
                    "sent": "So one of the one of the tricks is we call covariance updates, and this is for the end bigger than P case.",
                    "label": 0
                },
                {
                    "sent": "You can cash in a product so when we compute this inner product over here, I've rewritten it here.",
                    "label": 0
                },
                {
                    "sent": "That's an inner product between XJ&Y.",
                    "label": 0
                },
                {
                    "sent": "I've written these X Rays are vector right of N vector and wise and in vector.",
                    "label": 0
                },
                {
                    "sent": "So that's an inner product between XJ&Y minus.",
                    "label": 0
                },
                {
                    "sent": "Some of the inner products between XJ and other variables in the active set active set been variables whose coefficients are non 0.",
                    "label": 0
                },
                {
                    "sent": "Times the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Now you only need to complete these things once.",
                    "label": 0
                },
                {
                    "sent": "So once you've computed each of these inner product, you store them away.",
                    "label": 0
                },
                {
                    "sent": "It's the inner product between all the variables in the active set and all the other variables.",
                    "label": 0
                },
                {
                    "sent": "And if the active sits quite small, you can do that.",
                    "label": 0
                },
                {
                    "sent": "You can store them away and cash these things and so.",
                    "label": 0
                },
                {
                    "sent": "The the the coordinate updates become much more efficient.",
                    "label": 0
                },
                {
                    "sent": "Right, if these things are cashed.",
                    "label": 0
                },
                {
                    "sent": "With sparse updates.",
                    "label": 0
                },
                {
                    "sent": "So if the X is are sparse, that matrix is say 95% zeros.",
                    "label": 0
                },
                {
                    "sent": "In a product so very easy to compute right, you only have to go over the non zero values.",
                    "label": 0
                },
                {
                    "sent": "So you can really exploit exploit sparsity when you use coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you do not a big linear regression model in the X is sparse.",
                    "label": 0
                },
                {
                    "sent": "You have to go too much more work to exploit sparsity.",
                    "label": 0
                },
                {
                    "sent": "But for coordinate descent it's very natural.",
                    "label": 0
                },
                {
                    "sent": "And then there's other tricks like active set convergence, so you know for any given value of Lambda, we cycling around, updating the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Well, in principle we need to go over all P variables all the time to see they don't change, but we've gotten a currently active set of nonzero coefficients, which is usually much, much smaller than the full set.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we actually converge on them and then check to see if anything in the non active set wants to come into the model.",
                    "label": 0
                },
                {
                    "sent": "Often they don't, and then we done.",
                    "label": 0
                },
                {
                    "sent": "So we initialized.",
                    "label": 0
                },
                {
                    "sent": "We can quickly identify the smallest Lambda for which all the coefficients are zero.",
                    "label": 1
                },
                {
                    "sent": "So we start right at the left end part of the path where all the coefficients are zero and that point you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In Trivoli find out and then we move from there.",
                    "label": 0
                },
                {
                    "sent": "And now warm starts are natural.",
                    "label": 0
                },
                {
                    "sent": "Two.",
                    "label": 0
                },
                {
                    "sent": "We computing over greater values of Lambda and so that was a segue into into this.",
                    "label": 0
                },
                {
                    "sent": "We start at the value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "We order coefficients are zero.",
                    "label": 0
                },
                {
                    "sent": "The smallest value for which that's true that you can find out easily, and then we increment Lambda.",
                    "label": 0
                },
                {
                    "sent": "We decrement Lambda small amounts down towards Lambda Min and so.",
                    "label": 0
                },
                {
                    "sent": "What happens is that the active set grows from zero.",
                    "label": 0
                },
                {
                    "sent": "That grows in fairly slowly, and so we have warm starts as we move from one Lambda to the next, and that adds to the efficiency.",
                    "label": 1
                },
                {
                    "sent": "And finally we use the FFT to help the computation that stands for Friedman plus 410 plus tricks.",
                    "label": 0
                },
                {
                    "sent": "Jerry Friedman's been a avid Fortran programmer for the last 30 something 40 years, and it's fun to work with him and he knows lots of tricks for for for avoiding unnecessary computations.",
                    "label": 0
                },
                {
                    "sent": "I say no, no sloppy flops.",
                    "label": 0
                },
                {
                    "sent": "You know you have to fight like hell to get him to to put a test for convergence inside one of the inner loops.",
                    "label": 0
                },
                {
                    "sent": "You know, because that's a.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Necessary computations.",
                    "label": 0
                },
                {
                    "sent": "So for logistic models.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into details here we have.",
                    "label": 0
                },
                {
                    "sent": "So now you gotta extra layer of nonlinearity, which basically amounts to an outer loop.",
                    "label": 0
                },
                {
                    "sent": "We call it the Newton updates.",
                    "label": 0
                },
                {
                    "sent": "So we essentially doing that using the Newton algorithm for fitting the logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "But each in a loop of the Newton algorithm is a weighted squared error loss problem, and so we just use the same algorithm for the inner loop and then update the outer loop and you can find details in up in our paper.",
                    "label": 0
                },
                {
                    "sent": "From the multinomial multi class at similar we use a symmetric formulation, normally for the multinomial.",
                    "label": 0
                },
                {
                    "sent": "If you've got K classes, you'd fit K -- 1 linear models because the probabilities sum to one.",
                    "label": 0
                },
                {
                    "sent": "But because of the regularization we don't need to bother with that at awkward asymmetry, and so we have a symmetric model, and again you can find.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Details in the paper.",
                    "label": 0
                },
                {
                    "sent": "Another novelty in in what we do is we use the elastic net penalty, so this was introduced by a student of mine resort who's now at the University of Minnesota.",
                    "label": 0
                },
                {
                    "sent": "It's a compromise penalty between the L2 and the L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "And so here we see it over here.",
                    "label": 0
                },
                {
                    "sent": "This is just the penalty.",
                    "label": 0
                },
                {
                    "sent": "So it's, um, Dover the P variables and what we have is a.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of this L2 and there's the L1 for each coefficient, and we have some additional parameter Alpha which controls a mix.",
                    "label": 0
                },
                {
                    "sent": "And the half year is just to get a nice clean representation when we try and differentiate.",
                    "label": 0
                },
                {
                    "sent": "So what the what the elastic net penalty does is.",
                    "label": 0
                },
                {
                    "sent": "For the less you you get older you know you get at most.",
                    "label": 0
                },
                {
                    "sent": "The last two sets a lot of coefficients to 0.",
                    "label": 0
                },
                {
                    "sent": "The less you doesn't do too well, if you got very correlated predictors or features.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lot of features, are very correlated less, you cannot distinguish too well between the variables in their coefficients, and it tends to get a bit wild in situations like that the quadratic penalty.",
                    "label": 0
                },
                {
                    "sent": "Causes the coefficients to behave very well.",
                    "label": 0
                },
                {
                    "sent": "It makes some sort of snap towards each other and or be equal and share the coefficients.",
                    "label": 0
                },
                {
                    "sent": "And so the elastic net.",
                    "label": 0
                },
                {
                    "sent": "What it does is it was designed for microarray studies where you have groups of genes that are very color correlated.",
                    "label": 0
                },
                {
                    "sent": "Genes in pathways.",
                    "label": 0
                },
                {
                    "sent": "What it tends to do, select a whole group together and make their coefficients if they're very correlated, similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So you have this mix of L1 and L2.",
                    "label": 0
                },
                {
                    "sent": "And often we will have this elastic net with the Ridge.",
                    "label": 0
                },
                {
                    "sent": "The quadratic penalty we all have Alpha very close to one, so you have a little bit of quadratic, but mostly L1.",
                    "label": 0
                },
                {
                    "sent": "And what it does is it gives you a sparse solution, but the coefficients are much better behaved, so it's a very handy penalty, and it turns out that the coordinate update for for this penalty is equally similar as simple, so it's a soft threshold in, just like in less you.",
                    "label": 0
                },
                {
                    "sent": "That's the top term in the top divided by.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scaling factor.",
                    "label": 0
                },
                {
                    "sent": "OK, so the last technique so it's equally.",
                    "label": 0
                },
                {
                    "sent": "Simple, so this gives you an idea of the effect of that elastic net penalty.",
                    "label": 0
                },
                {
                    "sent": "So we've got 3 examples yet in a.",
                    "label": 0
                },
                {
                    "sent": "In a microarray problem with three 3 1/2 thousand genes and 72 training observations.",
                    "label": 0
                },
                {
                    "sent": "So on the left we've got the less you path or part of the less you path we just go up to the first 10 steps basically.",
                    "label": 0
                },
                {
                    "sent": "Here's the elastic Net path and what you see is there's more nonzero coefficients, but of course they shrunk more to 0 because there's more of them, but it still sparse, and if you go all the way and have pure quadratic, then all the coefficients, all three, another 1000 or 9 zero, but they shrunk heavily down towards 0, so this is typically not a very useful solution.",
                    "label": 0
                },
                {
                    "sent": "This is a much more useful solution, and Beth tends to be better behaved in this solution.",
                    "label": 0
                },
                {
                    "sent": "And incidentally, the support vector machine uses as many of you know, uses a quadratic penalty on the coefficients.",
                    "label": 0
                },
                {
                    "sent": "All the coefficients are non 0, so it's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coefficient profile looks something like this.",
                    "label": 0
                },
                {
                    "sent": "Here's an example multiclass classification using a whole lot of different methods.",
                    "label": 0
                },
                {
                    "sent": "It's it's that, oh, it's at example we saw earlier that took two minutes to to run.",
                    "label": 0
                },
                {
                    "sent": "14 classes and we see that actually and with cross validation and everything in elastic net penalized multinomial model.",
                    "label": 0
                },
                {
                    "sent": "Perform the best.",
                    "label": 0
                },
                {
                    "sent": "These are small sample results and so you really have.",
                    "label": 0
                },
                {
                    "sent": "I mean, the question is whether there's no not really a significant difference between these test error results because of the small test set of size 54.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, people, including us still make these comparisons and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And chose 384 genes out of 16,000.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How we doing for time?",
                    "label": 0
                },
                {
                    "sent": "I want to do some time for questions.",
                    "label": 0
                },
                {
                    "sent": "So if I take 5 more minutes, is that OK, yeah?",
                    "label": 0
                },
                {
                    "sent": "So to.",
                    "label": 0
                },
                {
                    "sent": "A brief summary.",
                    "label": 0
                },
                {
                    "sent": "For coordinate descent to work.",
                    "label": 0
                },
                {
                    "sent": "What you need is you need a loss function which is over here a differentiable loss function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the convicts penalty.",
                    "label": 0
                },
                {
                    "sent": "So the penalties convicts an additive.",
                    "label": 0
                },
                {
                    "sent": "Then then you can.",
                    "label": 0
                },
                {
                    "sent": "Then you can prove that coordinate descent converges to the solution, so both R&P each of the PJS are convex.",
                    "label": 0
                },
                {
                    "sent": "And R is differentiable.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be differentiable, but in our case the cases we think of its differentiable encoding of descent converges to the unique solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Often each coordinates step is trivial, such As for the lesu.",
                    "label": 0
                },
                {
                    "sent": "And decreasing Lambda slowly means not much cycling is needed, so you can get the whole path.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And as I said earlier, coordinate moves can exploit passive sparsity.",
                    "label": 0
                },
                {
                    "sent": "Now, this additivity, yet these even though I've got it written as if the penalties are, there's a penalty for each coefficient.",
                    "label": 0
                },
                {
                    "sent": "You can have groups of coefficients, so it can be blockwise additive.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can still exploit coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to just briefly tell you about some other applications where we've had success with coordinate descent and others have.",
                    "label": 0
                },
                {
                    "sent": "Most recently for us is undirected graphical models, so learning the dependent structure of of of so-called Gaussian graphical models via the less who in fact we've done it for non Gaussian as well for discrete graphical models.",
                    "label": 0
                },
                {
                    "sent": "But I'll talk about the.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian case now and so.",
                    "label": 0
                },
                {
                    "sent": "For those who work in graphical models, the standard approach for modeling sparsity is to work with the inverse covariance matrix, which captures the dependence structure.",
                    "label": 0
                },
                {
                    "sent": "So wherever you've got zeros in the inverse covariance matrix means that the car partial correlation between that.",
                    "label": 0
                },
                {
                    "sent": "Between those two variables is zero, and so you would not have a link in the in the graphical model.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Using maximum likelihood on the parameter on the inverse covariance matrix with an L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "On the on the inverse covariance there it is over here, so this is this is a P by P covariance matrix and we put an L1 penalty on all the all the parameters in the covariance matrix and this is just the partially maximized log likelihood for the Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you do blockwise coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You've got a P by P covariance matrix you solving for all.",
                    "label": 0
                },
                {
                    "sent": "A whole column of coefficients at a time.",
                    "label": 0
                },
                {
                    "sent": "Turns out that solving that amounts to a kind of lesu regularization problem, so it's like a less you regression problem and you can exploit coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So there's too many details to to go into it here, but again, we can do that very efficiently, and we can compute the whole path as we vary Lambda.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can solve moderately sparse graphs with 1000 nodes in under a minute.",
                    "label": 0
                },
                {
                    "sent": "And I'll give you an example.",
                    "label": 0
                },
                {
                    "sent": "Now this is a small problem.",
                    "label": 0
                },
                {
                    "sent": "11 proteins measured on 7000.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training observations.",
                    "label": 0
                },
                {
                    "sent": "And so the other graphical models you see the lambdas written above the plot when Lambda zero, Lambda zero over the it's an and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a unregularized fit, so there's an edge between every protein.",
                    "label": 0
                },
                {
                    "sent": "And then as you ramp up Lambda, the graph gets sparse sparser and sparser, so we can compute.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole path of solutions very efficiently.",
                    "label": 0
                },
                {
                    "sent": "The group last year.",
                    "label": 0
                },
                {
                    "sent": "That's one in Linnan Mayan vendor gear.",
                    "label": 0
                },
                {
                    "sent": "Two different groups and others have worked on it too.",
                    "label": 0
                },
                {
                    "sent": "So each in that case, each of the terms the penalties refer to sets of parameters.",
                    "label": 0
                },
                {
                    "sent": "And you don't have a 01 norm you over some of L2 norms.",
                    "label": 0
                },
                {
                    "sent": "And so this term over here is actually the L2 norm of a vector of coefficients.",
                    "label": 0
                },
                {
                    "sent": "So the variables are arranged in groups and each group has a set of linear coefficients and we we penalize the vector of coefficients for each group with an L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Notice it's not L2 squared, it's just L2.",
                    "label": 0
                },
                {
                    "sent": "And what that does is a tends to select the whole group of variables or leave it out.",
                    "label": 0
                },
                {
                    "sent": "And if they in they all non 0.",
                    "label": 0
                },
                {
                    "sent": "And that's useful.",
                    "label": 0
                },
                {
                    "sent": "For example, if you fit in linear models with categorical predictors with multiple levels and you represent each of those by dummy variable sense, so you have a group of coefficients.",
                    "label": 0
                },
                {
                    "sent": "So you want that whole categorical variable in or out of the model, with all of its coefficients.",
                    "label": 0
                },
                {
                    "sent": "And coordinate the sensor.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useful for solving that problem.",
                    "label": 0
                },
                {
                    "sent": "And something we worked on a few years ago.",
                    "label": 0
                },
                {
                    "sent": "CJ stands for comparative genome hybridization modeling and and we we introduced well tips Ronnie and colleagues introduced something called the Fuse Desu.",
                    "label": 0
                },
                {
                    "sent": "So there in this particular example, the coefficients actually form a time series, so they are arranged not at time Series A series in genome order, and so there's enabling this notion to coefficients, and so we've got a penalty that both shrinks the coefficients to zero and and also constrains neighboring coefficients to be the same.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to encourage a piecewise constant food.",
                    "label": 0
                },
                {
                    "sent": "And here's the solution that we get out, and again we used code and descent methods to solve that problem.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a paper on websites on that.",
                    "label": 0
                },
                {
                    "sent": "So finish now and so in summary, then L1 regularization variance has become a powerful tool with the advent of wide data in compressed sensing, often exact sparse signal recovery is possible with L1 methods and coordinate descent in our view is the fastest known algorithm for solving these problems along a path of values for the.",
                    "label": 0
                },
                {
                    "sent": "For the tuning parameter and so thank you very much and I'll be happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "Is a microphone you got?",
                    "label": 0
                },
                {
                    "sent": "So let's take night.",
                    "label": 0
                },
                {
                    "sent": "She introduced the quadratic penalty and which is a generalization of the lasso.",
                    "label": 1
                },
                {
                    "sent": "So does that make sense to keep general generalize it into order of three and four and keep up K?",
                    "label": 0
                },
                {
                    "sent": "That makes sense.",
                    "label": 0
                },
                {
                    "sent": "I missed the first part of your questions or is so elastic and add general generalize dollar Sue, or by introducing the quadratic penalties, so does that make sense to keep generalized it in two out of three another fall, so OK?",
                    "label": 0
                },
                {
                    "sent": "I I don't know which is key.",
                    "label": 0
                },
                {
                    "sent": "The elastic net just gives you a compromise between the L1 and L2 regularization.",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure where you get in order K or you thinking of elves.",
                    "label": 0
                },
                {
                    "sent": "Are you thinking of LK penalties?",
                    "label": 0
                },
                {
                    "sent": "Were you using Adobe's so that that does not make sense?",
                    "label": 0
                },
                {
                    "sent": "No, not really, because I see the OK penalties are somewhat intractable, not only easy ones or L1 and L2.",
                    "label": 0
                },
                {
                    "sent": "OK, 4K bigger than two become nonlinear in an hard an for less than one or non convex and so those become hard as well.",
                    "label": 0
                },
                {
                    "sent": "I go to some literature to all around this, thanks.",
                    "label": 0
                },
                {
                    "sent": "There's a microphone coming.",
                    "label": 0
                },
                {
                    "sent": "At the beginning, you mentioned that you can find the whole regularization path in a short time, but I think you have to provide a algorithm to find all the critical lambdas, isn't it?",
                    "label": 0
                },
                {
                    "sent": "Because the whole the whole regularization path need all the lambdas.",
                    "label": 0
                },
                {
                    "sent": "Well, we don't find all the lambdas in algorithm.",
                    "label": 0
                },
                {
                    "sent": "We find the grid of lambdas right and we can we can refine the grid if you like.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if we use 100 values of Lambda and then we change and your 200 values of Lambda, it doesn't take twice as long.",
                    "label": 0
                },
                {
                    "sent": "It takes a very short amount more because you got the warm starts or more effective.",
                    "label": 0
                },
                {
                    "sent": "And so when you move you know half a step instead of a full step to the next grade value.",
                    "label": 0
                },
                {
                    "sent": "Much less changes, and so you get there much more quickly.",
                    "label": 0
                },
                {
                    "sent": "So your comparison with loss every IS in time.",
                    "label": 0
                },
                {
                    "sent": "I think you have to set up some stopping criteria about your algorithm, isn't it?",
                    "label": 0
                },
                {
                    "sent": "Well, what we did there is we had a orgasm computer 100 steps becausw laws took approximately 100 steps.",
                    "label": 0
                },
                {
                    "sent": "That's where the break points in the piecewise linear work.",
                    "label": 0
                },
                {
                    "sent": "So just to make it more compatible, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So so you said that one and two are the only easy cases and you ruled out anything less than one and anything bigger than two.",
                    "label": 0
                },
                {
                    "sent": "But you didn't mention between one and two.",
                    "label": 0
                },
                {
                    "sent": "Oh, those are hard too.",
                    "label": 0
                },
                {
                    "sent": "Oh, another easy cases Infinity L Infinity is easy case.",
                    "label": 0
                },
                {
                    "sent": "But yeah, now be between one and two is hard as well and why.",
                    "label": 0
                },
                {
                    "sent": "Oh, but just because you got you know you get some nonlinearity in the penalty when you try and differentiate it.",
                    "label": 0
                },
                {
                    "sent": "You find this term comes in the denominator and you know you have to iterate it.",
                    "label": 0
                },
                {
                    "sent": "It's ugly.",
                    "label": 0
                },
                {
                    "sent": "I have a question regarding the comparison of lasso regression versus Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "Certainly there's no question about the interpretability advantages, but have there been any rigorous comparisons of the out of sample accuracy of one versus the other?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there have been comparisons.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so Ridge regression is, you know, in a Bayesian sense, ideal when you have a Gaussian prior on the coefficients.",
                    "label": 0
                },
                {
                    "sent": "So you can prove in that case that Ridge is that the optimal thing to do.",
                    "label": 0
                },
                {
                    "sent": "So that's a situation where all the coefficients are non zero and perhaps a lot of them are small and centered around 0, but all are non zero.",
                    "label": 0
                },
                {
                    "sent": "Then the optimal thing to do is Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "But what what happens in the case like that is, if P is very large, you have to regularize quite a lot, and even though it's the optimal thing to do, Ridge does poorly.",
                    "label": 0
                },
                {
                    "sent": "As that anything else does even more poorly.",
                    "label": 0
                },
                {
                    "sent": "So you kind of lost in a situation like that.",
                    "label": 0
                },
                {
                    "sent": "If you've got lots of variables and relatively few training samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so nothing does very well but rich does the best.",
                    "label": 0
                },
                {
                    "sent": "And so the only situations where you really well often these are more realistic real life situations or when you had lots of variables, but most of them are useless and there's some small subset that are useful.",
                    "label": 0
                },
                {
                    "sent": "And then L1 regularization tends to do will do much better than than rich.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So so hey, so do you find the right non zero coefficients?",
                    "label": 0
                },
                {
                    "sent": "So I guess biologists are very much interested in finding the right genes, the right influence factors and so on.",
                    "label": 0
                },
                {
                    "sent": "Do you find the right non zero coefficients always?",
                    "label": 0
                },
                {
                    "sent": "No, of course we never know, but that's that.",
                    "label": 0
                },
                {
                    "sent": "Bears on the work of Donahue and Dan, Dan and Candace and tell where they they have some really interesting theorems now that show that if if you take your X matrix and if the pairwise correlations between the columns of X.",
                    "label": 0
                },
                {
                    "sent": "Are bounded so.",
                    "label": 0
                },
                {
                    "sent": "Nope, Nope.",
                    "label": 0
                },
                {
                    "sent": "Two variables are two correlated.",
                    "label": 0
                },
                {
                    "sent": "Then they can actually prove that L1 selection will find the L0 optimal solution, which means the best subset solution, the L1, which is a convex criterion, will actually find them, so they have results like that that prove that you actually do find with high probability these solutions.",
                    "label": 0
                },
                {
                    "sent": "Now of course these bounds on the on the pairwise correlations are in realistic in practice, but.",
                    "label": 0
                },
                {
                    "sent": "You know in in the areas where I worked in microarray studies, there are lots of correlations right?",
                    "label": 0
                },
                {
                    "sent": "And so there is always some confusion as to whether you found the optimal.",
                    "label": 0
                },
                {
                    "sent": "That's why we write we like this.",
                    "label": 0
                },
                {
                    "sent": "This elastic net penalty because you sort of hedge your bets and you introduce all the correlated guys come in at the same time.",
                    "label": 0
                },
                {
                    "sent": "So you say this group seems to be important and then you have to figure out.",
                    "label": 0
                },
                {
                    "sent": "Later, you know which ones.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "A couple of questions.",
                    "label": 0
                },
                {
                    "sent": "So in the rules in the Internet we have enough data, have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "So what is the perspective for this in the indicates where you have enough data.",
                    "label": 0
                },
                {
                    "sent": "The second question is how do you answer that?",
                    "label": 0
                },
                {
                    "sent": "I didn't even get the first question, so can you just say it again?",
                    "label": 0
                },
                {
                    "sent": "So so you have this loss or method and all this regression method you use.",
                    "label": 0
                },
                {
                    "sent": "Different algorithm is seems you say you have a lot of advantages in the wide data.",
                    "label": 1
                },
                {
                    "sent": "So when we have enough data.",
                    "label": 0
                },
                {
                    "sent": "Any is a much bigger than P. What is the advantage for that?",
                    "label": 0
                },
                {
                    "sent": "So now you gotta big in and Big piece.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well and you kind of stuck because.",
                    "label": 0
                },
                {
                    "sent": "You know you're still going to get an efficient algorithm, but it's going to be much slower if if the data is sparse.",
                    "label": 0
                },
                {
                    "sent": "If you have sparsity, then that'll help, right?",
                    "label": 0
                },
                {
                    "sent": "But it will still be.",
                    "label": 0
                },
                {
                    "sent": "It'll still be an efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "Efficiency comes from using the last two penalty 'cause you got a lot of zeros in your solution.",
                    "label": 1
                },
                {
                    "sent": "So as long as you're interested in solutions with is, you know at points along the path where there's a lot of non zero coefficient, there's a lot of zero coefficients that less who is going to be an efficient way.",
                    "label": 0
                },
                {
                    "sent": "I mean sorry, the coordinate descent is going to be efficient way to get to that.",
                    "label": 0
                },
                {
                    "sent": "OK then that leads to the second question is when you have different you evaluate a different algorithm with especially emphasize our speeds.",
                    "label": 0
                },
                {
                    "sent": "What about performance and how you evaluate the performance of different algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, what do you mean by performance?",
                    "label": 0
                },
                {
                    "sent": "Peacemaker is a is A1 model give you better performance or better now they giving you the same solution.",
                    "label": 0
                },
                {
                    "sent": "They all elders.",
                    "label": 0
                },
                {
                    "sent": "Comparisons here were all algorithms for fitting the same model.",
                    "label": 0
                },
                {
                    "sent": "So there is no difference in the models, but if you apply that model to a holdout data set and what would be the difference will be will be well.",
                    "label": 0
                },
                {
                    "sent": "What difference you'll see?",
                    "label": 0
                },
                {
                    "sent": "You'll see a much.",
                    "label": 0
                },
                {
                    "sent": "You'll get your solution much faster if you use GLM net, but other than that you're getting the same solution, so there's no difference in the solution.",
                    "label": 0
                },
                {
                    "sent": "Our models of.",
                    "label": 0
                },
                {
                    "sent": "I mean, we we provide an R package called GLM net which is free so anyone can get it and so then you have you have it's sort of friendly environment for for working with the model and working with your data.",
                    "label": 0
                },
                {
                    "sent": "So for those who views Ariel you can just go in and use the package and maybe I'll convert some people to use in R in.",
                    "label": 0
                },
                {
                    "sent": "In the audience, I guess what I mean is the performance is more predictive power.",
                    "label": 0
                },
                {
                    "sent": "Like for example if you use a logistic regression then you need to for the.",
                    "label": 0
                },
                {
                    "sent": "For the industry industry you need to make your model work in the future in terms of productivity.",
                    "label": 1
                },
                {
                    "sent": "Let's you know there's lots of different models.",
                    "label": 0
                },
                {
                    "sent": "This is assuming you want to use the logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "Alot of people like logistic regression models in these kinds of problems because you know when P is very big, it's pretty much all you can.",
                    "label": 0
                },
                {
                    "sent": "You can work with.",
                    "label": 0
                },
                {
                    "sent": "And they tend to perform well.",
                    "label": 0
                },
                {
                    "sent": "I've my sources tell me that in the document classification problem, the logistic regression is the most popular one of the most popular models.",
                    "label": 0
                },
                {
                    "sent": "Right now, regularize logistic regression?",
                    "label": 0
                },
                {
                    "sent": "How does how does the algorithm perform when you have a mix of binary and scalar data?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, there's no problem.",
                    "label": 0
                },
                {
                    "sent": "You know you can mix.",
                    "label": 0
                },
                {
                    "sent": "By me and quantitative data categorical variables, it works fine.",
                    "label": 0
                },
                {
                    "sent": "Selection and then since I know.",
                    "label": 0
                },
                {
                    "sent": "These features are relevant.",
                    "label": 0
                },
                {
                    "sent": "Then I use I apply L2 Norm Noel theology.",
                    "label": 0
                },
                {
                    "sent": "No, first apply L1 known to get a subset of features and then I just use those.",
                    "label": 0
                },
                {
                    "sent": "This subset of features and then apply Ridge regression to make prediction.",
                    "label": 0
                },
                {
                    "sent": "Since I do care the predictive accuracy, does it make sense?",
                    "label": 0
                },
                {
                    "sent": "Well, I would.",
                    "label": 0
                },
                {
                    "sent": "What actually I think makes makes more sense.",
                    "label": 0
                },
                {
                    "sent": "Is yes user 01 to do the selection?",
                    "label": 0
                },
                {
                    "sent": "But typically the complaint in is that you've now got the selected the solution as selected.",
                    "label": 0
                },
                {
                    "sent": "Maybe a good set of variables, but it's regularize them too much, and so there's some bias.",
                    "label": 0
                },
                {
                    "sent": "So what people do is then take the selected variables and fit them unregularized.",
                    "label": 0
                },
                {
                    "sent": "To unbias him so you use the old one to do the feature selection, and then you fit the model with less regularization's.",
                    "label": 0
                },
                {
                    "sent": "There's a whole host of of of proposals for doing that in a clever way, so for example, just run the last two twice.",
                    "label": 0
                },
                {
                    "sent": "Unless you first time using cross validation and pick the point where it stops in, that'll tend to stop early because it's it doesn't want to let in all the noise variables.",
                    "label": 0
                },
                {
                    "sent": "But now you you can find yourself just to the active set and rear.",
                    "label": 0
                },
                {
                    "sent": "Unless you are now, it will go further with the cross validation and fit those coefficients with less regularization without having to worry about noise and these other proposals like that for doing that.",
                    "label": 0
                },
                {
                    "sent": "So adaptive lasuen, so could you talk?",
                    "label": 0
                },
                {
                    "sent": "I am over here.",
                    "label": 0
                },
                {
                    "sent": "Could you talk just a little bit again about why it's good to have the entire regularization path and not just the.",
                    "label": 0
                },
                {
                    "sent": "Like the best fit?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well so the reason you wanted is because these these models any of these regularization models are defined up to a tuning parameter.",
                    "label": 0
                },
                {
                    "sent": "And that's not very good 'cause you don't really have a way of guessing a good value for the tuning Prem.",
                    "label": 0
                },
                {
                    "sent": "So you need to have some other means for selecting the tuning parameter so we use cross validation or some measure of prediction error to select the tuning parameter.",
                    "label": 0
                },
                {
                    "sent": "Or maybe some subjective stuff as well about how sparse we want the solution to be, but but in order to do that efficiently you need to be able to compute a lot of solution values for different values of Lambda and so that's why it's nice to be able to get the whole path.",
                    "label": 0
                },
                {
                    "sent": "And curiously, for I didn't mention this in the talk, but if you want to, if you want to say for logistic regression model with a particular value of Lambda, let's say a non not too sparse solution.",
                    "label": 0
                },
                {
                    "sent": "So Lambda quite small, so quite a lot of non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you just ran coordinate descent on that problem right there with that value of Lambda would take more time to converge than if you did the whole path starting from zero.",
                    "label": 0
                },
                {
                    "sent": "So the whole pool starting from zero gives you like a interior path.",
                    "label": 0
                },
                {
                    "sent": "Root towards the solution you want and it does it faster than just computing the single solution.",
                    "label": 0
                },
                {
                    "sent": "You mention in the beginning that regularization is introduced especially for the case when P is greater than one, right?",
                    "label": 0
                },
                {
                    "sent": "So I have been working with the other regularization algorithm like LDA, but my in my case I just use cross validation to choose Lambda, but my observation is that when he is much larger than the Lambda is usually very small, but when P is a compara bulto earn, the Lambda is usually.",
                    "label": 0
                },
                {
                    "sent": "Large, so I'm not sure if because there are some recent work that prove that eldia and regression are essentially equivalent under very mild conditions, so I'm not sure if you have any similar observation in the regression case.",
                    "label": 0
                },
                {
                    "sent": "If yes, do you have any interpretation of this phenomenon?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well.",
                    "label": 0
                },
                {
                    "sent": "That's that.",
                    "label": 0
                },
                {
                    "sent": "Sounds like it's going the wrong way right?",
                    "label": 0
                },
                {
                    "sent": "So so when you got P very large, you do.",
                    "label": 0
                },
                {
                    "sent": "Yes, you do less regularization and when you when PES closer to N, yes, that's fine.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen that.",
                    "label": 0
                },
                {
                    "sent": "I do know with classification problems if if the if the data allow you to get perfect separation.",
                    "label": 0
                },
                {
                    "sent": "So even when P is much bigger than N, so this is in the case where support vector machines work very well that you don't really even know P is much bigger than in you end up doing very little regularization.",
                    "label": 0
                },
                {
                    "sent": "Because you trying to find the optimal separating hyperplane, and often a very small amount of regularization's, all you need just to control the solution, but it gives you the optimal separator.",
                    "label": 0
                },
                {
                    "sent": "But I don't have insight as to why when you add P closer to the end that you'd pick a more regularization.",
                    "label": 0
                },
                {
                    "sent": "Could be that in those problems the amount of overlap was different.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Which plays a big role in in where you pick the tuning parameters.",
                    "label": 0
                },
                {
                    "sent": "So just say yes or no question.",
                    "label": 0
                },
                {
                    "sent": "So your problem is convex, so can coordinate.",
                    "label": 0
                },
                {
                    "sent": "Dissent can guarantee to find the global solution theoretically yes.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "Thanks to all for this exciting talk and for answering all these questions.",
                    "label": 0
                }
            ]
        }
    }
}