{
    "id": "ath4gqs2esez5eqcgphcpie3b7oojbpc",
    "title": "Data-Dependent Geometries and Structures: Analyses and Algorithms for Machine Learning",
    "info": {
        "author": [
            "Mark Herbster, Department of Computer Science, University College London"
        ],
        "published": "April 25, 2012",
        "recorded": "March 2012",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics->Geometry",
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/workshops2012_herbster_data_dependent/",
    "segmentation": [
        [
            "OK hi, I'm mark harpster.",
            "I'd like to talk about a joint collaboration between three sites at UCL.",
            "We had myself Guy Lever a postdoc with John Shaw Taylor and then at Insubria we had Claudio Gentili and a postdoc with with Claudio Fabio Vitali and then also we had Nella Kristiani at Bristol OK and so the.",
            "Idea of this research project was to look into data dependent geometry's and structures.",
            "So what do I"
        ],
        [
            "Mean by a data dependent geometry.",
            "So let's think about perhaps a standard paradigm in supervised learning.",
            "Using for example support vector machine.",
            "So the support vector machine we have we have a data set sampled from some space with a given geometry.",
            "The distances between the particular points is independent of the sample, but why do I say that we think about support vector machine?",
            "We have some kernel.",
            "That kernel induces a metric and whatever point that we happened, whatever points we happen to sample, we take two points.",
            "The distance between them is independent of the sample once.",
            "The Colonel is chosen OK."
        ],
        [
            "The dependent paradigm OK here.",
            "That is a data set sampled from a space with an unknown geometry.",
            "And hence that the distances between the particular points are going to be dependent upon the sample.",
            "How do we do this?",
            "That's the question.",
            "There's of course going to be a lot of ways to do this, and many different ways, so we're going to need some assumptions.",
            "We're going to need some setup, but will just kind of perhaps explore one or two approaches.",
            "OK."
        ],
        [
            "OK, so so why might we want such a data dependent geometry?",
            "Let me try to give you a couple of motivating examples.",
            "First example, suppose we have some data set of news stories.",
            "For instance, the Bristol data set.",
            "Now imagine well, maybe here.",
            "Imagine we want to do some sort of classification in the news stories.",
            "Maybe we're going to use some nearest neighbors function to do some sort of classification.",
            "So maybe what we do is we have the news stories.",
            "Maybe we introduce some distance between news stories by using some bag of words based metric.",
            "So for example here.",
            "Maybe you have one news story whose tile is research and development and Fusion increased by 60%.",
            "Last quarter were also given.",
            "The text of this article."
        ],
        [
            "Now suppose we also have another news story.",
            "Saint Pierre Berge Gazeteer major layoffs expected in the tourism sector.",
            "We see these news stories.",
            "Maybe we look at the bag of words, Colonel.",
            "They may be quite far apart.",
            "In distance, both intuitively to us an maybe by looking at our bag of words based metric are different."
        ],
        [
            "But now if we see a third news story, something about some super tanker foundering on the Florida coast largest oil spill the Millennium, then maybe some connection between the top two starts to be ever."
        ],
        [
            "And I knowing three, we might say, ah, there's a major oil spill.",
            "People are concerned about natural gas.",
            "Is this led to at least an interest in stories about alternate energy sources an there's now some connection between these two stories and some somehow.",
            "Intuitively.",
            "These two stories are perhaps more connected than maybe the first bag of words.",
            "Kernel metric might indicate an somehow we want to take into account this third story in the distance.",
            "So knowing three suggests the distance between one and two should be reduced."
        ],
        [
            "So now I'm going to give today.",
            "Now I'm going to give the standard example which I kept putting in the talk and taking out the talks.",
            "I may hopefully many of you have probably seen this, but one standard way of doing this sort of learning this is the classic Two moons example and this is just another kind of visual illustration of what I'm saying.",
            "Many of you in this room have perhaps seen it, but imagine we have two data points labeled into two classes and we have an we want to predict.",
            "The label of this point.",
            "We look at we say well, the nearest neighbor or the optimal hyperplane.",
            "Really, we should probably predict A plus for this point.",
            "But then if we."
        ],
        [
            "Go ahead and we have a bunch of unlabeled data and we see this distribution of data.",
            "We might think.",
            "Well, maybe these are two separate classes, classes, maybe labels, respect class distributions.",
            "Maybe I should really label this as a minus so so the idea is that unlabeled data can help us.",
            "And this this example is often used as a motivation for a graph based semi supervised learning and."
        ],
        [
            "Instruction, which is one of the things we're going to study.",
            "So data dependent geometry is potentially a huge topic.",
            "We had to choose some things to do, so let's take a look at what are some of the topics that we ended up studying.",
            "So we did a number of work.",
            "I would broadly say about graph based, semi supervised learning or transduction.",
            "We studied graph Laplacian based methods.",
            "We considered the SSL extension with data dependent kernels.",
            "We did a lot of work.",
            "Should probably put this first real concern of this project and a general problem with many of these data dependent methods is that they're slow.",
            "They take lots of time for the data set so.",
            "I think for most of the papers, a key.",
            "Thing that we're always after approximations or some ways to make algorithms fast, so certainly here.",
            "Insubria we considered artwork the tree approximation.",
            "This gives us something quite fast.",
            "Also follow up work at UCL.",
            "We like to prove theoretical guarantees on our method, so for this work we gave some mistake bounds.",
            "Once you have this notion that so maybe I should say a few words about the graph based semi supervised learning.",
            "So here are the ideas."
        ],
        [
            "Here we have some distribution of data points an we a graph based method is we take the data points.",
            "We build some graph where edges are were points are likely to link to nearby points and then we use that graph to make predictions.",
            "So the data points are the vertices of the graph, so we're always predicting vertices which correspond to data points where one of these vertices might be like a news story in the."
        ],
        [
            "Previous example OK. Another closely related thing is, instead of thinking of the data points, is being vertices another place for these graph based algorithms come up is in social networks popular thing in web Sciences to classify links IE are these two people friends or not?",
            "Friends in this biological network?",
            "Do these two chemicals positively reinforce or inhibit?",
            "The reaction, so we might want to classify edges between points rather than vertices an.",
            "We also looked at fast algorithms, another approach.",
            "We considered it is here.",
            "In some sense our data set is given, but now perhaps slightly deeper approach would be to actually try to look at the structure of the distribution generating the data rather than just the sample OK?"
        ],
        [
            "So to accomplish this?",
            "Our resource allocation is false.",
            "The basic idea is we would fund RA.",
            "So at UCL.",
            "We've funded NRA guy Lever for five months an insubria we funded a.",
            "The postdoc Fabio Vitali for nine months and then so we can have some discussion between sites.",
            "We also have some funds that we use for travel OK."
        ],
        [
            "So far we're hoping for slight extension on the project because we haven't managed to spend all the money at the Insubria site yet, but so far we have essentially 6 publications, either in submission or in various stages.",
            "So these first 2 publications are from the insubria sites, second to our publications, which I'm involved in at UCL.",
            "Also, there's some collaboration with Insubria here and then.",
            "These are at UCL.",
            "OK."
        ],
        [
            "So let's jump right into what some of the research was in a bit more detail.",
            "Siri add insubria What's discussed there two papers, so we considered one this problem of vertex classification weighted graphs.",
            "This you might use in graph based transduction or part of a semi supervised method.",
            "And then also they considered a link classification on unweighted graphs.",
            "So in addition to experiments in fast algorithms motivating the authors at insubria.",
            "But this idea that that we want to produce guarantees for our algorithms and these guarantees should depend on some sort of meaningful natural complexity measure for the problem.",
            "I'm not going to have too much time to go into details of these complexity measures, but for.",
            "Common with UCL.",
            "This idea that things that you might want to label on a graph.",
            "You want some sort of smoothness defined relative the graph you want, perhaps labels which are similar to be near one another and for link classification basically well, I'll describe that complexity measure when I get to link classification.",
            "So in terms of the two measures.",
            "And Subia gave guaranteed gives guarantees on their algorithms optimality guarantees.",
            "They show that chasu that's optimal in some strong sense and.",
            "So give another connection to the optimal algorithm for link classification.",
            "The algorithms are all I at least per prediction in the worst case, I believe linear insubria's here, so they can correct me if I make a mistake and they've done experiments and so there's experiments on these in the overall goal is we're just trying to minimize miss."
        ],
        [
            "Classification mistakes OK?",
            "So vertex classification, the shasu algorithm so.",
            "Understanding the broader motivation, if we're thinking about graph based semi supervised learning, we have these problems.",
            "Which methods which are very popular like label propagation on the full graph, popularized by Belkin, New Yogi Zoo and Ghahramani many such methods.",
            "Based on the full graph, the problem is these methods based on a full graph are often cubic in in time complexity.",
            "In the worst case, and also in the sense if you're after an optimal algorithm in terms of proving guarantees, there's definitely some subtleties involved in the full case, so they're looking at approximating the full graph of the tree.",
            "Or perhaps you have a tree to begin with, so these could be hyperlinked web pages, social Networks, Co authoring.",
            "Works biological networks.",
            "These are intrinsic to the data or maybe they built.",
            "A tree from the data.",
            "So the learning problem which which they considered.",
            "Is we have a tree which is modeling our data points.",
            "These are perhaps our news stories and maybe you know the edges on this tree is weighted.",
            "2 news stories may have more words in common than other news stories and.",
            "The only thing when they actually goes to predictions to consider the graph topology that they consider binary labeling.",
            "Although their results can extend to.",
            "Two broader labeling classes.",
            "And so.",
            "Basically what they're aiming for is is is if 2 two points are near one another on the tree I if there's an edge they should have the same label, and particularly if we have a weight on the edge, then there should be more likely to have a small label.",
            "So the idea is the bias is we want things with a small weighted cut.",
            "OK, so.",
            "I.",
            "So this shows you algorithm accepts as input some way to tree if if you give it a graph they have nice results that showing that if you have a spanning tree of the original graph, you can still prove some nice properties.",
            "OK, so how does Shasu works so we can understand the way Shasu works in essentially three steps.",
            "We're going to partition the tree into components satisfying some properties.",
            "We're going to use a minimum cut to estimate the labels on the.",
            "Component on the borders of these components, and then if there's some.",
            "If you know Min cut has this problem that you know you consider the minimum cut with respect to labels, there may be more than one minimum cuts.",
            "There may be more than one minimum cut, so we have to somehow choose in that case.",
            "And So what we do if there's more than one minimum cut, we use the nearest neighbor, OK?"
        ],
        [
            "OK, so insubria an anvil on proves that that the accuracy of shasu is optimal up to log factors in this tree model they have a simple algorithm for implementing it.",
            "We can find minimum cuts using a sum product algorithm on a tree in linear time.",
            "So the worst case time complexity prediction for them was linear in the number of vertices.",
            "Then also they're proving their bouncing guarantees in the online setting, but if you're interested in batch protocol then it's just linear in the the complete set and this is a huge win over these methods that take cubic in time OK."
        ],
        [
            "So.",
            "Shasu they've done some initial experiments so far.",
            "Initial experiments, I believe, considered quite large text.",
            "I believe in this web spam detection task.",
            "This is some 800,000 vertices in the tree considered character recognition text.",
            "'cause organization in bioinformatics.",
            "So as competitors they chose the following three algorithms.",
            "They chose label propagation, which is an algorithm which uses the full graph for information and this is.",
            "This algorithm is very popular.",
            "This is, I'd say when the most common type of graph based SSL algorithms, but it can be quite slow, especially in comparison to shasu.",
            "Considered a quite simple kind of perhaps baseline test algorithm where you're doing an online majority vote based on the labels of adjacent know, so you somehow have some local approximation.",
            "So here we have the full graph.",
            "Here we have a local approximation.",
            "Now WTH the previous algorithm by Insubria and Milan, which further approximates the tree by embedding that tree into a line.",
            "So that's even somehow a simpler approximation, so we have these.",
            "Two algorithms which are somehow simplifications of the tree, and if we view the tree, is a simplification of the graph we have labeled prop, but it turns out that shizu does quite well in this setup, so they use a variety of methods of generating spanning trees.",
            "For for both jasun WTA, they looked at different training set sizes and the performance of Shasu outperform WT in OMV which is a good thing we wanted to to outperform these simpler approximations and even though it's much faster.",
            "Didn't always outperform label propagation, but for cases when the training set size is small and we use an aggregation of spanning trees, we still have a fast algorithm shasu.",
            "Performed label Prop an.",
            "You might say that when you do have a tree in some sense what shasu is doing is.",
            "Actually, I don't want to make that claim."
        ],
        [
            "I'll be careful rather than make conjectures on the spot.",
            "OK, so.",
            "After Vertex classification, well, let's classify the edges.",
            "The links of a vertex.",
            "OK, so.",
            "And please correct me if I don't completely understand every point here, but here we have a link classification task.",
            "So what do I mean?",
            "So usual in semi supervised learning links always represent the idea that the two things are similar, that they should be in the same class.",
            "Now we're broadening the model here.",
            "These black edges represent the fact that two edges that let's the usual thing that we expect.",
            "Labels here to be similar.",
            "These are similar class, but we also have this idea that things might be dissimilar like these two people dislike one another.",
            "We distrust one another in some network are we have a biological network and we have some inhibitory reaction.",
            "OK, so now we're given some prototype graph where we have the edges.",
            "You know, perhaps we can tell from some experiments that communication is going on between different vertices in the graph, but we don't know whether these.",
            "These communications represent positive effects or negative effects, and the aim is is we want to eventually predict.",
            "Whether the edges represent positive or negative affects so here the learner, so they.",
            "They discuss this problem without an active learning protocol, but the results are perhaps a bit more exciting in the active learning setup, so we're going to focus on the active learning setup, so the setup for active learning they used is the following nature says, oh, you're going to get to choose perhaps K edges that you can request labels from a set of edges in this graph and feed it to the system.",
            "Say, please give me the labels on these edges.",
            "Are these positive edges or these negative edges?",
            "So once the learner gets these positive or negative edges, the learner is going to predict the labels of all the remaining edges.",
            "OK.",
            "So."
        ],
        [
            "What sort of results do they have?",
            "OK, so well, maybe going so they're going to prove guarantees in terms of complexity measure based on correlation clustering.",
            "Not going to do this in detail, but the rough idea.",
            "Imagine that we have some edges which are positively labeled and negatively labeled.",
            "Now imagine we want to extend that labeling to rest of the graph.",
            "What would be the best way to extend that labeling to rest of the graph?",
            "Somehow we'd like to extend the labeling in such a way so that we have a minimum number of disagreements.",
            "IE, we don't want three negative edges in a cycle, for instance, that would would give us a disagreement.",
            "So so the optimal thing to do would be to extend it, minimizing the disagreements.",
            "This is unfortunately an MP hard problem, but still they're going to get bounds in terms of not doing much worse than that MP hard problem, so they devised, so the algorithms they devised.",
            "Is optimal to factor rho, where rho is the ratio of the test 2.",
            "Training set size.",
            "On any labeled graph.",
            "And we can't allow the training set.",
            "Sites would be smaller than role of times the training tests.",
            "Fat size the algorithm is relatively fast.",
            "Rosa sensually, no more than linear in the worst case.",
            "And.",
            "So still looking forward to some some experiments on this, but you know they've already explored the use of randomization against adverse aerial label assignments and they hope to do some experiments on real networks OK. Well, just interrupting with questions, let me move on to some of the UCL contributions."
        ],
        [
            "OK, I split the USL contributions into two parts.",
            "OK so from UCL.",
            "Two papers to look at it.",
            "Triangle resist inequality for peer assistance and efficient prediction for tree Markov random fields industry model.",
            "One thing that's nice about this second paper work, and this is the first time that we're really getting some nice collaboration going between the two sites.",
            "Claudio's post Doc Fabio's visited UCL 3 times and we plan to hope to have him visit some more to continue research on this problem.",
            "OK, so.",
            "We're going to try to understand geometry through resistance metric in the 1st paper, and we're going to look at fast online algorithms for labeling a graph in the 2nd paper.",
            "So the 1st paper.",
            "Well, well, we'll discuss this when I get to the paper, but will say that P resistance is a way of generalizing the effective resistance of a network, Laplacian.",
            "Min cut methods are popular for graph based semi supervised learning.",
            "When you have P resistance, it includes and generalizes both.",
            "This paper is just about proving an inequality an an this inequality will give us a simple insight into K center clustering."
        ],
        [
            "P resistance OK.",
            "So.",
            "What is P resistance?",
            "What is effective resistance have to do with graph based data dependent semi supervised geometry at all?",
            "Well, the basic motivation is the following.",
            "If we think back to the slide with the two moons and we build a graph on the thing of the graph gives us a series of metrics which we could potentially use to label points, perhaps using nearest neighbors are more sophisticatedly.",
            "These metrics might be part of an inner product or a Norman used in some interpolation or regularization scheme.",
            "One simple metric on a graph would be.",
            "Well, once you have a graph, maybe the right way to think of things being near one another is through Geode esic distance.",
            "So you know like with the news stories example, once we have that third news story, those points one and two are now closer than geodesic distance.",
            "But we have many connections between two points on a graph.",
            "Maybe we don't want to just count the minimum number of distance of pass, but somehow we want to take into account the number of edge disjoint.",
            "Paths OK so kind of getting behind these graph based methods like label propagation somewhere behind there sitting the notion of effective resistance.",
            "I'm not going to quite make the formal connection."
        ],
        [
            "Pleat Lee, but let's just discuss P resistance alone, so the effective resistance when P is 2 I I have this network of resistors.",
            "I know what the resistance is between point A&B.",
            "What we could certainly just solve algebraically using Kirchoff's laws, but otherwise we get it from this variational thing of minimizing the power such that the voltage difference is fixed, so that defines the effective resistance for us.",
            "Now if we replace this, P = 2.",
            "At a generalization of effective resistance, we get something that acts like pipes when P is equals one, we get something that acts like geodesic distances.",
            "When P goes to Infinity."
        ],
        [
            "So P resistance trades off geodesic distance and connectivity."
        ],
        [
            "Simple facts about P resistance or the following were familiar with the law of resistors in parallel.",
            "This is unchanged.",
            "We know for normal electrical circuits we have resistors in series.",
            "That resistance is just some.",
            "However, if we replace this with P, it turns out that resistance is some in terms of 1 / P -- 1 norm."
        ],
        [
            "OK, so this paper is largely what's going to perhaps seem we just want to prove a small inequality.",
            "Well known case that electrical network.",
            "We have this following the usual triangle inequality.",
            "The effective resistance between A&C between A and B&B and C must be larger than the effective resistance between A&C.",
            "Just this is the normal triangle equality from metric.",
            "This this itself is is actually got interesting because it's it's quite mysterious.",
            "This was well known when I started this, but if you think about it.",
            "The electrical resistance if you went back a few slides and you looked at how how is defined, it's defined from this quadratic form, and so you think about, you know like instances support vector machine.",
            "Somebody gives you a kernel.",
            "The kernel defines a metric, but somehow you have a metric is.",
            "Is the square root of.",
            "If you're expressing things in terms of thing, what's interesting about effective resistance?",
            "Is yes.",
            "Effective resistance falls out of a positive semi definite quadratic form norm, but rather than being the square root of it, it's just the quadratic form itself.",
            "So not only is it a metric.",
            "Normally it's also the square of.",
            "It is also a metric when restricted to points on the graph, which is quite an interesting property, so this is 1 motivation why we want to explore this further and see what happens for other values of P."
        ],
        [
            "So at P = 1 with a little bit of searching actually found.",
            "Yeah there is a result that somehow with pipes it starts to make sense where resistance is one over inverse capacity, that the.",
            "That the the triangle inequality is now stronger such that the.",
            "The the edges is no larger than the than the maximum of the inverse capacity or resistance."
        ],
        [
            "So somehow we want to interpolate between these two things, so we prove the following generalized triangle inequality.",
            "So for P between one and two.",
            "This inequality is always stronger than this inequality, so we have a metric when P is larger than two, the P resistance no longer acts like a metric, but we still have a weaker inequality.",
            "How?"
        ],
        [
            "Well, let's look at one, perhaps insight that this brings us.",
            "Suppose so we're familiar with using doing spectral clustering.",
            "So one thing we can do is we can another way of using a graph and doing clustering is.",
            "We can use the distances induced by P resistance and we can do K center clustering.",
            "So it turns out that if we do K center clustering, I want to cover the graph in balls.",
            "In in K balls with the smallest possible radius, then we can do that optimally for general metric up to a factor of two.",
            "But since we have a stronger inequality for peer assistance, we can get an improved factor for smaller P, and so in fact we equals one.",
            "We get the optimal clustering with just far this first."
        ],
        [
            "Algorithm OK. More directly applied paper looked at you says the following is back to the type of work that Insubria was doing on shizu for prediction on weighted graphs.",
            "We now well we're interested in proving bounds in last one.",
            "So we just want to perhaps look at this question algorithmically and look at other techniques that we might want to predict a labeled tree.",
            "So here we have a Markov random field on a tree and we're interested in online prediction.",
            "So here we're going to predict so at any point in time, time goes on infinitely, might predict the label at at some vertex in the tree.",
            "We might then update it given vertex by associating label, we might delete a label at a vertex, but the point is, we always want to do these predict steps fast.",
            "We always want to do these updates steps fast somehow.",
            "And so we make out, oh, I know how to do this fast from Markov random field.",
            "I'm familiar with belief propagation.",
            "This is a well solved problem.",
            "On a tree, it's linear.",
            "We can use belief propagation.",
            "I can calculate all the marginals of exact inference in linear time, but in this online model linear is too slow for us, IE.",
            "Peer pressure.",
            "Wanna predict here?",
            "Make a prediction.",
            "Get a point.",
            "Predict here going to prediction make it point predict here predict here particularly so for the imagine going back and forth.",
            "Each of these steps is taking linear time.",
            "This is quite expensive.",
            "We would like to we don't want to pay linear time prediction.",
            "We'd like to pay something much smaller on a line.",
            "We can reduce this to something logarithmically.",
            "Found out after starting this that Perl also has a way of doing this online in general graphs, so I'm not going to review pearls results here, but we improve on Pearl in a number of cases, But this is a technical discussion, but the basic idea to improve on belief exact inference on a tree Markov random field is to build a tree on the tree."
        ],
        [
            "How my doing time wise?",
            "OK. We have a tree and ideas and we're going to somehow build a tree on a tree and so.",
            "Isomorphic to the tree on the tree is a hierarchical covering of the tree.",
            "Let's do this fast.",
            "So here's our tree.",
            "Imagine we split this tree somehow.",
            "We want to evenly divide the tree as much as possible.",
            "So we split the tree so we have a cover.",
            "These are disjoint, except they share one vertex.",
            "Then each of these we then go ahead and split the green points.",
            "We have these components and then we keep going till we have edges and these are our base things.",
            "At the bottom and so the question is, how do we build such a hierarchical cover of minimum height?",
            "This is an example of hierarchical cover, so if we had a line, it's perhaps intuitive to us that we can do something binary and gives us a minimum height.",
            "It's no more than log rhythmic.",
            "Then we have a tree and we want to build a tree on the tree.",
            "It's not quite so obvious how what's the minimum height of that tree on a tree we can build, but this is what we give an algorithm for."
        ],
        [
            "So it turns out this tree which we can build on a tree, has to we can prove that it has to be at least log the height of the tree, but it's no more than log the cardinality of the vertices or the height of the tree.",
            "So on a line graph it's reduced to log in.",
            "And and on on a star graph it's unchanged and there's a nice geometric characterization of this in terms of if the diameter is the minimum bedding into path into a graph, then this is the minimum embedding of of the graph into a binary tree, and you take the log of F. So once we have this tree, we can use this variant to belief.",
            "Propagation will call D propagation and now all our prediction upstate steps take.",
            "Oky, which in the worst cases is logged an online belief propagation.",
            "You're always going to take linear time for your prediction or update steps and you kind of initialization is order T and the NIPS workshop paper unfortunately, use dynamic programming.",
            "Find disable T cubed algorithm.",
            "But now we can do this in linear time to fight."
        ],
        [
            "The train OK, let me speak about the second part of activities at UCL.",
            "So here we're going to look at learning with data dependent hypothesis classes.",
            "Theoretical and practical advances two papers, one paper data dependent, curls near linear time.",
            "Here I'm going to rush a little bit since I'm a little bit short on time, but here.",
            "This paper does something nice that that none of the research before has yet considered all the other research.",
            "Somehow we always reduce the problem down to a graph or a tree.",
            "This is an perhaps do transduction, but many times you want to work, you have some larger hypothesis space an you want to your method to apply to unseen data which you haven't used to build your graph or tree.",
            "So the previous is perhaps transduction and this is pure semi supervised learning.",
            "So we're going to look at a way of doing this but.",
            "Doing it efficiently.",
            "And then the 2nd paper we're going to go back to the idea of bounds depending on the data distribution and we're going to give a tighter PAC Bayes bounds through looking at a distribution dependent prior."
        ],
        [
            "OK.",
            "So we want to somehow mix a graph with a continuous domain.",
            "But we want our method to be fast and we don't want it to be complicated, so.",
            "For our original for graph, we have some some regularizer associated with the graph, and then we also are going to have a smooth kernel and we want to combine the two.",
            "Now if we."
        ],
        [
            "Imagine we have a kernel on a graph and we have.",
            "A kernel, for instance a Gaussian kernel.",
            "Now a simple thing that you can do would be well to combine two kernels.",
            "You can use just use a convex combination.",
            "You can just sum the two colonels and that gives you a new kernel, but you know some very nice advantages prior to us since Wony but had been done previously rather than summing two kernels, something kind of happening, the dual domain you can think of summing the inner product's.",
            "Directly use some in the primal domain rather than summing in the dual domain sindone E at all.",
            "They gave a closed form for computing this when this was a positive semi definite.",
            "This is Colonel so you can find this kernel, but it has cubic complexity so the idea of this work is that somehow we want to.",
            "Find this kernel, but not take cubic complexity.",
            "And so the idea is essentially the following.",
            "Given that set of points, unlabeled points, that we might use to define our graph, we're going to have some subset of points, some subset of points which might be chosen through some careful methods, but largely in the experiments we chose things randomly.",
            "These are the landmark points, so we're going to approximate the original graph with a smaller graph.",
            "Then with this landmark set of points.",
            "We're going to go ahead and use the technique of sandoni at all to build this kernel and then.",
            "To compute things fast, we're going to use a method for solving linear systems quickly from a Spielman and Tang.",
            "I'm kind of going quickly here, but.",
            "Feel free to ask."
        ],
        [
            "Myself, rich on offline for more of the details so.",
            "What's the benefit so?",
            "We get a lot of the benefit of using a huge graph.",
            "We don't have short circuits, but we can choose a smaller landmark set according to the amount of computational resource we have to work with.",
            "So here, so we've gone ahead and done some experiments on large scale SSL here.",
            "Here we can see the roads is also the experiment.",
            "So here's our method over a range of training set sizes.",
            "We did quite well.",
            "Now we can't compare this directly to the Laplacian SVM because with 64,000 points we just couldn't solve and get this kernel in any period of time, so designed essentially a budget version of this Laplacian SVM.",
            "Which improves on on the straight Gaussian kernel, but still a bit weaker than using our method for Tracy.",
            "Conservative performance OK.",
            "This graph.",
            "Is a little bit about future research here.",
            "Perhaps some of you are familiar with the complex project, but here.",
            "Here is using the same type of kernel which is a mixture of Gaussian annul, apasan, approximated and here we have some maze problem in reinforcement learning.",
            "These white points represent landmark points.",
            "This is a value function and the idea is that somehow the value function corresponds to.",
            "Due to the kernel being fixed at some point and then looking at the value as the kernel changes so the Gaussian kernel gives us the smoothness.",
            "Here the Laplacian kind of allows us to propagate or the edge.",
            "So rather than having kind of straight.",
            "Concentric, beautiful radial basic contours.",
            "Here the Laplacian this shape to this.",
            "So this is a potential application of these type of kernels to reinforcement learning.",
            "Gang this point OK?",
            "And they work in a Journal version of this.",
            "OK I."
        ],
        [
            "So.",
            "I stopped to forget about this, but I'll let China so back to learning data distribution.",
            "John, I'm not a PKB Asian so I will.",
            "I will probably mangle this a little bit, but the key thing here is is.",
            "Is to eliminate the prior in fact phase bound.",
            "So normally you for instance would have a relative entropy term on the right hand side.",
            "That would include the prior.",
            "Now we've eliminated this.",
            "Well, maybe John can help me out if I make any mistakes instead, our complexity is basically measured through this term gamma, which is a mixture which we used to sample from over our hypothesis class.",
            "That same thing, but with the true risk, yes, but you don't know what it is, but you can still workout the kernel divergences between that distribution, so you have a prior distribution that you don't know what it is, but yes.",
            "And you get this sort of divide.",
            "You know.",
            "Unlike most bands, it's 1 / M to the 2/3, or in many cases yes.",
            "So that's the idea.",
            "But the gamma is a little bit of an unknown factor here, so yeah, it is quite clear that you get OK.",
            "Thank you for clarifying."
        ],
        [
            "Application of this is we can consider regular regularization with.",
            "A kernel method and we can have different bandwidths of the kernel and we plug in kernel method.",
            "We go ahead and get the following bound.",
            "Fair enough, OK?"
        ],
        [
            "Future directions unfortunately had trouble starting the project on time it insubria, so we've requested if possible.",
            "If we could complete the project in September 2012 to use Fabio's money to that point, we plan to extend results on fast online prediction for tree MRF's.",
            "Extend the mall a great deal.",
            "I've been told by in Syria that we can expect some experiments on the Bristol data set, which we've had some trouble getting ahold of in the future.",
            "I mean, I'm quite happy with the research all over, and I think there's a lot of directions for extensions.",
            "Personally, I'm very interested in developing a collaboration with Tubingen.",
            "There's already kind of four papers in this P resistance Area, 2 from UCL, 2 from Tubingen.",
            "We were trying to plan a visit so that we can have a PhD student from Tubingen visit UCL awhile.",
            "Maybe Insubria is also interested in this research, so we'd like to kind of build on this momentum here already.",
            "Some directions, computational issues.",
            "Distance is hard to compute efficiently.",
            "What we want to understand performance over a Fuller range of P. Then I briefly discussed the reinforcement learning application where if you went back to that previous picture, if you change the value of P and you have this perhaps maze example.",
            "Well, if you have an arrow board or perhaps a geode ethic, path distance works better.",
            "So we want a larger value of P OK?",
            "Thank you, that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK hi, I'm mark harpster.",
                    "label": 0
                },
                {
                    "sent": "I'd like to talk about a joint collaboration between three sites at UCL.",
                    "label": 0
                },
                {
                    "sent": "We had myself Guy Lever a postdoc with John Shaw Taylor and then at Insubria we had Claudio Gentili and a postdoc with with Claudio Fabio Vitali and then also we had Nella Kristiani at Bristol OK and so the.",
                    "label": 0
                },
                {
                    "sent": "Idea of this research project was to look into data dependent geometry's and structures.",
                    "label": 0
                },
                {
                    "sent": "So what do I",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mean by a data dependent geometry.",
                    "label": 0
                },
                {
                    "sent": "So let's think about perhaps a standard paradigm in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Using for example support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So the support vector machine we have we have a data set sampled from some space with a given geometry.",
                    "label": 1
                },
                {
                    "sent": "The distances between the particular points is independent of the sample, but why do I say that we think about support vector machine?",
                    "label": 1
                },
                {
                    "sent": "We have some kernel.",
                    "label": 1
                },
                {
                    "sent": "That kernel induces a metric and whatever point that we happened, whatever points we happen to sample, we take two points.",
                    "label": 0
                },
                {
                    "sent": "The distance between them is independent of the sample once.",
                    "label": 0
                },
                {
                    "sent": "The Colonel is chosen OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The dependent paradigm OK here.",
                    "label": 0
                },
                {
                    "sent": "That is a data set sampled from a space with an unknown geometry.",
                    "label": 1
                },
                {
                    "sent": "And hence that the distances between the particular points are going to be dependent upon the sample.",
                    "label": 0
                },
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "There's of course going to be a lot of ways to do this, and many different ways, so we're going to need some assumptions.",
                    "label": 0
                },
                {
                    "sent": "We're going to need some setup, but will just kind of perhaps explore one or two approaches.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so why might we want such a data dependent geometry?",
                    "label": 0
                },
                {
                    "sent": "Let me try to give you a couple of motivating examples.",
                    "label": 0
                },
                {
                    "sent": "First example, suppose we have some data set of news stories.",
                    "label": 0
                },
                {
                    "sent": "For instance, the Bristol data set.",
                    "label": 0
                },
                {
                    "sent": "Now imagine well, maybe here.",
                    "label": 0
                },
                {
                    "sent": "Imagine we want to do some sort of classification in the news stories.",
                    "label": 0
                },
                {
                    "sent": "Maybe we're going to use some nearest neighbors function to do some sort of classification.",
                    "label": 0
                },
                {
                    "sent": "So maybe what we do is we have the news stories.",
                    "label": 0
                },
                {
                    "sent": "Maybe we introduce some distance between news stories by using some bag of words based metric.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have one news story whose tile is research and development and Fusion increased by 60%.",
                    "label": 1
                },
                {
                    "sent": "Last quarter were also given.",
                    "label": 0
                },
                {
                    "sent": "The text of this article.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now suppose we also have another news story.",
                    "label": 0
                },
                {
                    "sent": "Saint Pierre Berge Gazeteer major layoffs expected in the tourism sector.",
                    "label": 1
                },
                {
                    "sent": "We see these news stories.",
                    "label": 0
                },
                {
                    "sent": "Maybe we look at the bag of words, Colonel.",
                    "label": 0
                },
                {
                    "sent": "They may be quite far apart.",
                    "label": 0
                },
                {
                    "sent": "In distance, both intuitively to us an maybe by looking at our bag of words based metric are different.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now if we see a third news story, something about some super tanker foundering on the Florida coast largest oil spill the Millennium, then maybe some connection between the top two starts to be ever.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I knowing three, we might say, ah, there's a major oil spill.",
                    "label": 0
                },
                {
                    "sent": "People are concerned about natural gas.",
                    "label": 0
                },
                {
                    "sent": "Is this led to at least an interest in stories about alternate energy sources an there's now some connection between these two stories and some somehow.",
                    "label": 0
                },
                {
                    "sent": "Intuitively.",
                    "label": 0
                },
                {
                    "sent": "These two stories are perhaps more connected than maybe the first bag of words.",
                    "label": 0
                },
                {
                    "sent": "Kernel metric might indicate an somehow we want to take into account this third story in the distance.",
                    "label": 0
                },
                {
                    "sent": "So knowing three suggests the distance between one and two should be reduced.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to give today.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to give the standard example which I kept putting in the talk and taking out the talks.",
                    "label": 0
                },
                {
                    "sent": "I may hopefully many of you have probably seen this, but one standard way of doing this sort of learning this is the classic Two moons example and this is just another kind of visual illustration of what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "Many of you in this room have perhaps seen it, but imagine we have two data points labeled into two classes and we have an we want to predict.",
                    "label": 0
                },
                {
                    "sent": "The label of this point.",
                    "label": 0
                },
                {
                    "sent": "We look at we say well, the nearest neighbor or the optimal hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Really, we should probably predict A plus for this point.",
                    "label": 0
                },
                {
                    "sent": "But then if we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go ahead and we have a bunch of unlabeled data and we see this distribution of data.",
                    "label": 0
                },
                {
                    "sent": "We might think.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe these are two separate classes, classes, maybe labels, respect class distributions.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should really label this as a minus so so the idea is that unlabeled data can help us.",
                    "label": 0
                },
                {
                    "sent": "And this this example is often used as a motivation for a graph based semi supervised learning and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instruction, which is one of the things we're going to study.",
                    "label": 0
                },
                {
                    "sent": "So data dependent geometry is potentially a huge topic.",
                    "label": 0
                },
                {
                    "sent": "We had to choose some things to do, so let's take a look at what are some of the topics that we ended up studying.",
                    "label": 0
                },
                {
                    "sent": "So we did a number of work.",
                    "label": 0
                },
                {
                    "sent": "I would broadly say about graph based, semi supervised learning or transduction.",
                    "label": 0
                },
                {
                    "sent": "We studied graph Laplacian based methods.",
                    "label": 0
                },
                {
                    "sent": "We considered the SSL extension with data dependent kernels.",
                    "label": 1
                },
                {
                    "sent": "We did a lot of work.",
                    "label": 0
                },
                {
                    "sent": "Should probably put this first real concern of this project and a general problem with many of these data dependent methods is that they're slow.",
                    "label": 0
                },
                {
                    "sent": "They take lots of time for the data set so.",
                    "label": 0
                },
                {
                    "sent": "I think for most of the papers, a key.",
                    "label": 0
                },
                {
                    "sent": "Thing that we're always after approximations or some ways to make algorithms fast, so certainly here.",
                    "label": 0
                },
                {
                    "sent": "Insubria we considered artwork the tree approximation.",
                    "label": 0
                },
                {
                    "sent": "This gives us something quite fast.",
                    "label": 0
                },
                {
                    "sent": "Also follow up work at UCL.",
                    "label": 1
                },
                {
                    "sent": "We like to prove theoretical guarantees on our method, so for this work we gave some mistake bounds.",
                    "label": 0
                },
                {
                    "sent": "Once you have this notion that so maybe I should say a few words about the graph based semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So here are the ideas.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have some distribution of data points an we a graph based method is we take the data points.",
                    "label": 0
                },
                {
                    "sent": "We build some graph where edges are were points are likely to link to nearby points and then we use that graph to make predictions.",
                    "label": 0
                },
                {
                    "sent": "So the data points are the vertices of the graph, so we're always predicting vertices which correspond to data points where one of these vertices might be like a news story in the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous example OK. Another closely related thing is, instead of thinking of the data points, is being vertices another place for these graph based algorithms come up is in social networks popular thing in web Sciences to classify links IE are these two people friends or not?",
                    "label": 0
                },
                {
                    "sent": "Friends in this biological network?",
                    "label": 0
                },
                {
                    "sent": "Do these two chemicals positively reinforce or inhibit?",
                    "label": 0
                },
                {
                    "sent": "The reaction, so we might want to classify edges between points rather than vertices an.",
                    "label": 0
                },
                {
                    "sent": "We also looked at fast algorithms, another approach.",
                    "label": 1
                },
                {
                    "sent": "We considered it is here.",
                    "label": 0
                },
                {
                    "sent": "In some sense our data set is given, but now perhaps slightly deeper approach would be to actually try to look at the structure of the distribution generating the data rather than just the sample OK?",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to accomplish this?",
                    "label": 0
                },
                {
                    "sent": "Our resource allocation is false.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is we would fund RA.",
                    "label": 0
                },
                {
                    "sent": "So at UCL.",
                    "label": 0
                },
                {
                    "sent": "We've funded NRA guy Lever for five months an insubria we funded a.",
                    "label": 1
                },
                {
                    "sent": "The postdoc Fabio Vitali for nine months and then so we can have some discussion between sites.",
                    "label": 0
                },
                {
                    "sent": "We also have some funds that we use for travel OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far we're hoping for slight extension on the project because we haven't managed to spend all the money at the Insubria site yet, but so far we have essentially 6 publications, either in submission or in various stages.",
                    "label": 0
                },
                {
                    "sent": "So these first 2 publications are from the insubria sites, second to our publications, which I'm involved in at UCL.",
                    "label": 0
                },
                {
                    "sent": "Also, there's some collaboration with Insubria here and then.",
                    "label": 0
                },
                {
                    "sent": "These are at UCL.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's jump right into what some of the research was in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "Siri add insubria What's discussed there two papers, so we considered one this problem of vertex classification weighted graphs.",
                    "label": 1
                },
                {
                    "sent": "This you might use in graph based transduction or part of a semi supervised method.",
                    "label": 0
                },
                {
                    "sent": "And then also they considered a link classification on unweighted graphs.",
                    "label": 1
                },
                {
                    "sent": "So in addition to experiments in fast algorithms motivating the authors at insubria.",
                    "label": 0
                },
                {
                    "sent": "But this idea that that we want to produce guarantees for our algorithms and these guarantees should depend on some sort of meaningful natural complexity measure for the problem.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to have too much time to go into details of these complexity measures, but for.",
                    "label": 0
                },
                {
                    "sent": "Common with UCL.",
                    "label": 0
                },
                {
                    "sent": "This idea that things that you might want to label on a graph.",
                    "label": 0
                },
                {
                    "sent": "You want some sort of smoothness defined relative the graph you want, perhaps labels which are similar to be near one another and for link classification basically well, I'll describe that complexity measure when I get to link classification.",
                    "label": 0
                },
                {
                    "sent": "So in terms of the two measures.",
                    "label": 0
                },
                {
                    "sent": "And Subia gave guaranteed gives guarantees on their algorithms optimality guarantees.",
                    "label": 1
                },
                {
                    "sent": "They show that chasu that's optimal in some strong sense and.",
                    "label": 0
                },
                {
                    "sent": "So give another connection to the optimal algorithm for link classification.",
                    "label": 0
                },
                {
                    "sent": "The algorithms are all I at least per prediction in the worst case, I believe linear insubria's here, so they can correct me if I make a mistake and they've done experiments and so there's experiments on these in the overall goal is we're just trying to minimize miss.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification mistakes OK?",
                    "label": 0
                },
                {
                    "sent": "So vertex classification, the shasu algorithm so.",
                    "label": 1
                },
                {
                    "sent": "Understanding the broader motivation, if we're thinking about graph based semi supervised learning, we have these problems.",
                    "label": 0
                },
                {
                    "sent": "Which methods which are very popular like label propagation on the full graph, popularized by Belkin, New Yogi Zoo and Ghahramani many such methods.",
                    "label": 0
                },
                {
                    "sent": "Based on the full graph, the problem is these methods based on a full graph are often cubic in in time complexity.",
                    "label": 0
                },
                {
                    "sent": "In the worst case, and also in the sense if you're after an optimal algorithm in terms of proving guarantees, there's definitely some subtleties involved in the full case, so they're looking at approximating the full graph of the tree.",
                    "label": 0
                },
                {
                    "sent": "Or perhaps you have a tree to begin with, so these could be hyperlinked web pages, social Networks, Co authoring.",
                    "label": 0
                },
                {
                    "sent": "Works biological networks.",
                    "label": 0
                },
                {
                    "sent": "These are intrinsic to the data or maybe they built.",
                    "label": 0
                },
                {
                    "sent": "A tree from the data.",
                    "label": 1
                },
                {
                    "sent": "So the learning problem which which they considered.",
                    "label": 0
                },
                {
                    "sent": "Is we have a tree which is modeling our data points.",
                    "label": 0
                },
                {
                    "sent": "These are perhaps our news stories and maybe you know the edges on this tree is weighted.",
                    "label": 0
                },
                {
                    "sent": "2 news stories may have more words in common than other news stories and.",
                    "label": 1
                },
                {
                    "sent": "The only thing when they actually goes to predictions to consider the graph topology that they consider binary labeling.",
                    "label": 0
                },
                {
                    "sent": "Although their results can extend to.",
                    "label": 0
                },
                {
                    "sent": "Two broader labeling classes.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Basically what they're aiming for is is is if 2 two points are near one another on the tree I if there's an edge they should have the same label, and particularly if we have a weight on the edge, then there should be more likely to have a small label.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the bias is we want things with a small weighted cut.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this shows you algorithm accepts as input some way to tree if if you give it a graph they have nice results that showing that if you have a spanning tree of the original graph, you can still prove some nice properties.",
                    "label": 1
                },
                {
                    "sent": "OK, so how does Shasu works so we can understand the way Shasu works in essentially three steps.",
                    "label": 1
                },
                {
                    "sent": "We're going to partition the tree into components satisfying some properties.",
                    "label": 0
                },
                {
                    "sent": "We're going to use a minimum cut to estimate the labels on the.",
                    "label": 0
                },
                {
                    "sent": "Component on the borders of these components, and then if there's some.",
                    "label": 0
                },
                {
                    "sent": "If you know Min cut has this problem that you know you consider the minimum cut with respect to labels, there may be more than one minimum cuts.",
                    "label": 0
                },
                {
                    "sent": "There may be more than one minimum cut, so we have to somehow choose in that case.",
                    "label": 0
                },
                {
                    "sent": "And So what we do if there's more than one minimum cut, we use the nearest neighbor, OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so insubria an anvil on proves that that the accuracy of shasu is optimal up to log factors in this tree model they have a simple algorithm for implementing it.",
                    "label": 1
                },
                {
                    "sent": "We can find minimum cuts using a sum product algorithm on a tree in linear time.",
                    "label": 1
                },
                {
                    "sent": "So the worst case time complexity prediction for them was linear in the number of vertices.",
                    "label": 0
                },
                {
                    "sent": "Then also they're proving their bouncing guarantees in the online setting, but if you're interested in batch protocol then it's just linear in the the complete set and this is a huge win over these methods that take cubic in time OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Shasu they've done some initial experiments so far.",
                    "label": 0
                },
                {
                    "sent": "Initial experiments, I believe, considered quite large text.",
                    "label": 0
                },
                {
                    "sent": "I believe in this web spam detection task.",
                    "label": 1
                },
                {
                    "sent": "This is some 800,000 vertices in the tree considered character recognition text.",
                    "label": 1
                },
                {
                    "sent": "'cause organization in bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "So as competitors they chose the following three algorithms.",
                    "label": 0
                },
                {
                    "sent": "They chose label propagation, which is an algorithm which uses the full graph for information and this is.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is very popular.",
                    "label": 0
                },
                {
                    "sent": "This is, I'd say when the most common type of graph based SSL algorithms, but it can be quite slow, especially in comparison to shasu.",
                    "label": 0
                },
                {
                    "sent": "Considered a quite simple kind of perhaps baseline test algorithm where you're doing an online majority vote based on the labels of adjacent know, so you somehow have some local approximation.",
                    "label": 0
                },
                {
                    "sent": "So here we have the full graph.",
                    "label": 0
                },
                {
                    "sent": "Here we have a local approximation.",
                    "label": 0
                },
                {
                    "sent": "Now WTH the previous algorithm by Insubria and Milan, which further approximates the tree by embedding that tree into a line.",
                    "label": 0
                },
                {
                    "sent": "So that's even somehow a simpler approximation, so we have these.",
                    "label": 0
                },
                {
                    "sent": "Two algorithms which are somehow simplifications of the tree, and if we view the tree, is a simplification of the graph we have labeled prop, but it turns out that shizu does quite well in this setup, so they use a variety of methods of generating spanning trees.",
                    "label": 0
                },
                {
                    "sent": "For for both jasun WTA, they looked at different training set sizes and the performance of Shasu outperform WT in OMV which is a good thing we wanted to to outperform these simpler approximations and even though it's much faster.",
                    "label": 0
                },
                {
                    "sent": "Didn't always outperform label propagation, but for cases when the training set size is small and we use an aggregation of spanning trees, we still have a fast algorithm shasu.",
                    "label": 1
                },
                {
                    "sent": "Performed label Prop an.",
                    "label": 0
                },
                {
                    "sent": "You might say that when you do have a tree in some sense what shasu is doing is.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't want to make that claim.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll be careful rather than make conjectures on the spot.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "After Vertex classification, well, let's classify the edges.",
                    "label": 1
                },
                {
                    "sent": "The links of a vertex.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And please correct me if I don't completely understand every point here, but here we have a link classification task.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean?",
                    "label": 0
                },
                {
                    "sent": "So usual in semi supervised learning links always represent the idea that the two things are similar, that they should be in the same class.",
                    "label": 0
                },
                {
                    "sent": "Now we're broadening the model here.",
                    "label": 0
                },
                {
                    "sent": "These black edges represent the fact that two edges that let's the usual thing that we expect.",
                    "label": 0
                },
                {
                    "sent": "Labels here to be similar.",
                    "label": 0
                },
                {
                    "sent": "These are similar class, but we also have this idea that things might be dissimilar like these two people dislike one another.",
                    "label": 0
                },
                {
                    "sent": "We distrust one another in some network are we have a biological network and we have some inhibitory reaction.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're given some prototype graph where we have the edges.",
                    "label": 0
                },
                {
                    "sent": "You know, perhaps we can tell from some experiments that communication is going on between different vertices in the graph, but we don't know whether these.",
                    "label": 0
                },
                {
                    "sent": "These communications represent positive effects or negative effects, and the aim is is we want to eventually predict.",
                    "label": 0
                },
                {
                    "sent": "Whether the edges represent positive or negative affects so here the learner, so they.",
                    "label": 0
                },
                {
                    "sent": "They discuss this problem without an active learning protocol, but the results are perhaps a bit more exciting in the active learning setup, so we're going to focus on the active learning setup, so the setup for active learning they used is the following nature says, oh, you're going to get to choose perhaps K edges that you can request labels from a set of edges in this graph and feed it to the system.",
                    "label": 1
                },
                {
                    "sent": "Say, please give me the labels on these edges.",
                    "label": 0
                },
                {
                    "sent": "Are these positive edges or these negative edges?",
                    "label": 0
                },
                {
                    "sent": "So once the learner gets these positive or negative edges, the learner is going to predict the labels of all the remaining edges.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What sort of results do they have?",
                    "label": 0
                },
                {
                    "sent": "OK, so well, maybe going so they're going to prove guarantees in terms of complexity measure based on correlation clustering.",
                    "label": 1
                },
                {
                    "sent": "Not going to do this in detail, but the rough idea.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we have some edges which are positively labeled and negatively labeled.",
                    "label": 0
                },
                {
                    "sent": "Now imagine we want to extend that labeling to rest of the graph.",
                    "label": 0
                },
                {
                    "sent": "What would be the best way to extend that labeling to rest of the graph?",
                    "label": 0
                },
                {
                    "sent": "Somehow we'd like to extend the labeling in such a way so that we have a minimum number of disagreements.",
                    "label": 0
                },
                {
                    "sent": "IE, we don't want three negative edges in a cycle, for instance, that would would give us a disagreement.",
                    "label": 0
                },
                {
                    "sent": "So so the optimal thing to do would be to extend it, minimizing the disagreements.",
                    "label": 0
                },
                {
                    "sent": "This is unfortunately an MP hard problem, but still they're going to get bounds in terms of not doing much worse than that MP hard problem, so they devised, so the algorithms they devised.",
                    "label": 1
                },
                {
                    "sent": "Is optimal to factor rho, where rho is the ratio of the test 2.",
                    "label": 0
                },
                {
                    "sent": "Training set size.",
                    "label": 0
                },
                {
                    "sent": "On any labeled graph.",
                    "label": 0
                },
                {
                    "sent": "And we can't allow the training set.",
                    "label": 0
                },
                {
                    "sent": "Sites would be smaller than role of times the training tests.",
                    "label": 1
                },
                {
                    "sent": "Fat size the algorithm is relatively fast.",
                    "label": 0
                },
                {
                    "sent": "Rosa sensually, no more than linear in the worst case.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So still looking forward to some some experiments on this, but you know they've already explored the use of randomization against adverse aerial label assignments and they hope to do some experiments on real networks OK. Well, just interrupting with questions, let me move on to some of the UCL contributions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I split the USL contributions into two parts.",
                    "label": 0
                },
                {
                    "sent": "OK so from UCL.",
                    "label": 0
                },
                {
                    "sent": "Two papers to look at it.",
                    "label": 0
                },
                {
                    "sent": "Triangle resist inequality for peer assistance and efficient prediction for tree Markov random fields industry model.",
                    "label": 1
                },
                {
                    "sent": "One thing that's nice about this second paper work, and this is the first time that we're really getting some nice collaboration going between the two sites.",
                    "label": 0
                },
                {
                    "sent": "Claudio's post Doc Fabio's visited UCL 3 times and we plan to hope to have him visit some more to continue research on this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "We're going to try to understand geometry through resistance metric in the 1st paper, and we're going to look at fast online algorithms for labeling a graph in the 2nd paper.",
                    "label": 1
                },
                {
                    "sent": "So the 1st paper.",
                    "label": 0
                },
                {
                    "sent": "Well, well, we'll discuss this when I get to the paper, but will say that P resistance is a way of generalizing the effective resistance of a network, Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Min cut methods are popular for graph based semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "When you have P resistance, it includes and generalizes both.",
                    "label": 0
                },
                {
                    "sent": "This paper is just about proving an inequality an an this inequality will give us a simple insight into K center clustering.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "P resistance OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What is P resistance?",
                    "label": 0
                },
                {
                    "sent": "What is effective resistance have to do with graph based data dependent semi supervised geometry at all?",
                    "label": 0
                },
                {
                    "sent": "Well, the basic motivation is the following.",
                    "label": 0
                },
                {
                    "sent": "If we think back to the slide with the two moons and we build a graph on the thing of the graph gives us a series of metrics which we could potentially use to label points, perhaps using nearest neighbors are more sophisticatedly.",
                    "label": 0
                },
                {
                    "sent": "These metrics might be part of an inner product or a Norman used in some interpolation or regularization scheme.",
                    "label": 0
                },
                {
                    "sent": "One simple metric on a graph would be.",
                    "label": 0
                },
                {
                    "sent": "Well, once you have a graph, maybe the right way to think of things being near one another is through Geode esic distance.",
                    "label": 0
                },
                {
                    "sent": "So you know like with the news stories example, once we have that third news story, those points one and two are now closer than geodesic distance.",
                    "label": 0
                },
                {
                    "sent": "But we have many connections between two points on a graph.",
                    "label": 0
                },
                {
                    "sent": "Maybe we don't want to just count the minimum number of distance of pass, but somehow we want to take into account the number of edge disjoint.",
                    "label": 0
                },
                {
                    "sent": "Paths OK so kind of getting behind these graph based methods like label propagation somewhere behind there sitting the notion of effective resistance.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to quite make the formal connection.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pleat Lee, but let's just discuss P resistance alone, so the effective resistance when P is 2 I I have this network of resistors.",
                    "label": 1
                },
                {
                    "sent": "I know what the resistance is between point A&B.",
                    "label": 0
                },
                {
                    "sent": "What we could certainly just solve algebraically using Kirchoff's laws, but otherwise we get it from this variational thing of minimizing the power such that the voltage difference is fixed, so that defines the effective resistance for us.",
                    "label": 0
                },
                {
                    "sent": "Now if we replace this, P = 2.",
                    "label": 0
                },
                {
                    "sent": "At a generalization of effective resistance, we get something that acts like pipes when P is equals one, we get something that acts like geodesic distances.",
                    "label": 0
                },
                {
                    "sent": "When P goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So P resistance trades off geodesic distance and connectivity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple facts about P resistance or the following were familiar with the law of resistors in parallel.",
                    "label": 1
                },
                {
                    "sent": "This is unchanged.",
                    "label": 1
                },
                {
                    "sent": "We know for normal electrical circuits we have resistors in series.",
                    "label": 0
                },
                {
                    "sent": "That resistance is just some.",
                    "label": 0
                },
                {
                    "sent": "However, if we replace this with P, it turns out that resistance is some in terms of 1 / P -- 1 norm.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this paper is largely what's going to perhaps seem we just want to prove a small inequality.",
                    "label": 0
                },
                {
                    "sent": "Well known case that electrical network.",
                    "label": 0
                },
                {
                    "sent": "We have this following the usual triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "The effective resistance between A&C between A and B&B and C must be larger than the effective resistance between A&C.",
                    "label": 0
                },
                {
                    "sent": "Just this is the normal triangle equality from metric.",
                    "label": 0
                },
                {
                    "sent": "This this itself is is actually got interesting because it's it's quite mysterious.",
                    "label": 0
                },
                {
                    "sent": "This was well known when I started this, but if you think about it.",
                    "label": 0
                },
                {
                    "sent": "The electrical resistance if you went back a few slides and you looked at how how is defined, it's defined from this quadratic form, and so you think about, you know like instances support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Somebody gives you a kernel.",
                    "label": 0
                },
                {
                    "sent": "The kernel defines a metric, but somehow you have a metric is.",
                    "label": 0
                },
                {
                    "sent": "Is the square root of.",
                    "label": 0
                },
                {
                    "sent": "If you're expressing things in terms of thing, what's interesting about effective resistance?",
                    "label": 0
                },
                {
                    "sent": "Is yes.",
                    "label": 0
                },
                {
                    "sent": "Effective resistance falls out of a positive semi definite quadratic form norm, but rather than being the square root of it, it's just the quadratic form itself.",
                    "label": 0
                },
                {
                    "sent": "So not only is it a metric.",
                    "label": 0
                },
                {
                    "sent": "Normally it's also the square of.",
                    "label": 0
                },
                {
                    "sent": "It is also a metric when restricted to points on the graph, which is quite an interesting property, so this is 1 motivation why we want to explore this further and see what happens for other values of P.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at P = 1 with a little bit of searching actually found.",
                    "label": 0
                },
                {
                    "sent": "Yeah there is a result that somehow with pipes it starts to make sense where resistance is one over inverse capacity, that the.",
                    "label": 0
                },
                {
                    "sent": "That the the triangle inequality is now stronger such that the.",
                    "label": 0
                },
                {
                    "sent": "The the edges is no larger than the than the maximum of the inverse capacity or resistance.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So somehow we want to interpolate between these two things, so we prove the following generalized triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "So for P between one and two.",
                    "label": 0
                },
                {
                    "sent": "This inequality is always stronger than this inequality, so we have a metric when P is larger than two, the P resistance no longer acts like a metric, but we still have a weaker inequality.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's look at one, perhaps insight that this brings us.",
                    "label": 0
                },
                {
                    "sent": "Suppose so we're familiar with using doing spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So one thing we can do is we can another way of using a graph and doing clustering is.",
                    "label": 0
                },
                {
                    "sent": "We can use the distances induced by P resistance and we can do K center clustering.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that if we do K center clustering, I want to cover the graph in balls.",
                    "label": 0
                },
                {
                    "sent": "In in K balls with the smallest possible radius, then we can do that optimally for general metric up to a factor of two.",
                    "label": 0
                },
                {
                    "sent": "But since we have a stronger inequality for peer assistance, we can get an improved factor for smaller P, and so in fact we equals one.",
                    "label": 0
                },
                {
                    "sent": "We get the optimal clustering with just far this first.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm OK. More directly applied paper looked at you says the following is back to the type of work that Insubria was doing on shizu for prediction on weighted graphs.",
                    "label": 0
                },
                {
                    "sent": "We now well we're interested in proving bounds in last one.",
                    "label": 0
                },
                {
                    "sent": "So we just want to perhaps look at this question algorithmically and look at other techniques that we might want to predict a labeled tree.",
                    "label": 0
                },
                {
                    "sent": "So here we have a Markov random field on a tree and we're interested in online prediction.",
                    "label": 0
                },
                {
                    "sent": "So here we're going to predict so at any point in time, time goes on infinitely, might predict the label at at some vertex in the tree.",
                    "label": 0
                },
                {
                    "sent": "We might then update it given vertex by associating label, we might delete a label at a vertex, but the point is, we always want to do these predict steps fast.",
                    "label": 1
                },
                {
                    "sent": "We always want to do these updates steps fast somehow.",
                    "label": 0
                },
                {
                    "sent": "And so we make out, oh, I know how to do this fast from Markov random field.",
                    "label": 0
                },
                {
                    "sent": "I'm familiar with belief propagation.",
                    "label": 0
                },
                {
                    "sent": "This is a well solved problem.",
                    "label": 1
                },
                {
                    "sent": "On a tree, it's linear.",
                    "label": 0
                },
                {
                    "sent": "We can use belief propagation.",
                    "label": 0
                },
                {
                    "sent": "I can calculate all the marginals of exact inference in linear time, but in this online model linear is too slow for us, IE.",
                    "label": 0
                },
                {
                    "sent": "Peer pressure.",
                    "label": 0
                },
                {
                    "sent": "Wanna predict here?",
                    "label": 0
                },
                {
                    "sent": "Make a prediction.",
                    "label": 0
                },
                {
                    "sent": "Get a point.",
                    "label": 0
                },
                {
                    "sent": "Predict here going to prediction make it point predict here predict here particularly so for the imagine going back and forth.",
                    "label": 0
                },
                {
                    "sent": "Each of these steps is taking linear time.",
                    "label": 0
                },
                {
                    "sent": "This is quite expensive.",
                    "label": 0
                },
                {
                    "sent": "We would like to we don't want to pay linear time prediction.",
                    "label": 0
                },
                {
                    "sent": "We'd like to pay something much smaller on a line.",
                    "label": 0
                },
                {
                    "sent": "We can reduce this to something logarithmically.",
                    "label": 0
                },
                {
                    "sent": "Found out after starting this that Perl also has a way of doing this online in general graphs, so I'm not going to review pearls results here, but we improve on Pearl in a number of cases, But this is a technical discussion, but the basic idea to improve on belief exact inference on a tree Markov random field is to build a tree on the tree.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How my doing time wise?",
                    "label": 0
                },
                {
                    "sent": "OK. We have a tree and ideas and we're going to somehow build a tree on a tree and so.",
                    "label": 0
                },
                {
                    "sent": "Isomorphic to the tree on the tree is a hierarchical covering of the tree.",
                    "label": 0
                },
                {
                    "sent": "Let's do this fast.",
                    "label": 0
                },
                {
                    "sent": "So here's our tree.",
                    "label": 0
                },
                {
                    "sent": "Imagine we split this tree somehow.",
                    "label": 0
                },
                {
                    "sent": "We want to evenly divide the tree as much as possible.",
                    "label": 0
                },
                {
                    "sent": "So we split the tree so we have a cover.",
                    "label": 0
                },
                {
                    "sent": "These are disjoint, except they share one vertex.",
                    "label": 0
                },
                {
                    "sent": "Then each of these we then go ahead and split the green points.",
                    "label": 0
                },
                {
                    "sent": "We have these components and then we keep going till we have edges and these are our base things.",
                    "label": 0
                },
                {
                    "sent": "At the bottom and so the question is, how do we build such a hierarchical cover of minimum height?",
                    "label": 0
                },
                {
                    "sent": "This is an example of hierarchical cover, so if we had a line, it's perhaps intuitive to us that we can do something binary and gives us a minimum height.",
                    "label": 0
                },
                {
                    "sent": "It's no more than log rhythmic.",
                    "label": 0
                },
                {
                    "sent": "Then we have a tree and we want to build a tree on the tree.",
                    "label": 0
                },
                {
                    "sent": "It's not quite so obvious how what's the minimum height of that tree on a tree we can build, but this is what we give an algorithm for.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out this tree which we can build on a tree, has to we can prove that it has to be at least log the height of the tree, but it's no more than log the cardinality of the vertices or the height of the tree.",
                    "label": 0
                },
                {
                    "sent": "So on a line graph it's reduced to log in.",
                    "label": 0
                },
                {
                    "sent": "And and on on a star graph it's unchanged and there's a nice geometric characterization of this in terms of if the diameter is the minimum bedding into path into a graph, then this is the minimum embedding of of the graph into a binary tree, and you take the log of F. So once we have this tree, we can use this variant to belief.",
                    "label": 0
                },
                {
                    "sent": "Propagation will call D propagation and now all our prediction upstate steps take.",
                    "label": 0
                },
                {
                    "sent": "Oky, which in the worst cases is logged an online belief propagation.",
                    "label": 1
                },
                {
                    "sent": "You're always going to take linear time for your prediction or update steps and you kind of initialization is order T and the NIPS workshop paper unfortunately, use dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "Find disable T cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "But now we can do this in linear time to fight.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The train OK, let me speak about the second part of activities at UCL.",
                    "label": 0
                },
                {
                    "sent": "So here we're going to look at learning with data dependent hypothesis classes.",
                    "label": 1
                },
                {
                    "sent": "Theoretical and practical advances two papers, one paper data dependent, curls near linear time.",
                    "label": 0
                },
                {
                    "sent": "Here I'm going to rush a little bit since I'm a little bit short on time, but here.",
                    "label": 0
                },
                {
                    "sent": "This paper does something nice that that none of the research before has yet considered all the other research.",
                    "label": 0
                },
                {
                    "sent": "Somehow we always reduce the problem down to a graph or a tree.",
                    "label": 0
                },
                {
                    "sent": "This is an perhaps do transduction, but many times you want to work, you have some larger hypothesis space an you want to your method to apply to unseen data which you haven't used to build your graph or tree.",
                    "label": 0
                },
                {
                    "sent": "So the previous is perhaps transduction and this is pure semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at a way of doing this but.",
                    "label": 0
                },
                {
                    "sent": "Doing it efficiently.",
                    "label": 0
                },
                {
                    "sent": "And then the 2nd paper we're going to go back to the idea of bounds depending on the data distribution and we're going to give a tighter PAC Bayes bounds through looking at a distribution dependent prior.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we want to somehow mix a graph with a continuous domain.",
                    "label": 0
                },
                {
                    "sent": "But we want our method to be fast and we don't want it to be complicated, so.",
                    "label": 0
                },
                {
                    "sent": "For our original for graph, we have some some regularizer associated with the graph, and then we also are going to have a smooth kernel and we want to combine the two.",
                    "label": 0
                },
                {
                    "sent": "Now if we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine we have a kernel on a graph and we have.",
                    "label": 0
                },
                {
                    "sent": "A kernel, for instance a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Now a simple thing that you can do would be well to combine two kernels.",
                    "label": 0
                },
                {
                    "sent": "You can use just use a convex combination.",
                    "label": 0
                },
                {
                    "sent": "You can just sum the two colonels and that gives you a new kernel, but you know some very nice advantages prior to us since Wony but had been done previously rather than summing two kernels, something kind of happening, the dual domain you can think of summing the inner product's.",
                    "label": 0
                },
                {
                    "sent": "Directly use some in the primal domain rather than summing in the dual domain sindone E at all.",
                    "label": 0
                },
                {
                    "sent": "They gave a closed form for computing this when this was a positive semi definite.",
                    "label": 0
                },
                {
                    "sent": "This is Colonel so you can find this kernel, but it has cubic complexity so the idea of this work is that somehow we want to.",
                    "label": 0
                },
                {
                    "sent": "Find this kernel, but not take cubic complexity.",
                    "label": 0
                },
                {
                    "sent": "And so the idea is essentially the following.",
                    "label": 0
                },
                {
                    "sent": "Given that set of points, unlabeled points, that we might use to define our graph, we're going to have some subset of points, some subset of points which might be chosen through some careful methods, but largely in the experiments we chose things randomly.",
                    "label": 0
                },
                {
                    "sent": "These are the landmark points, so we're going to approximate the original graph with a smaller graph.",
                    "label": 0
                },
                {
                    "sent": "Then with this landmark set of points.",
                    "label": 0
                },
                {
                    "sent": "We're going to go ahead and use the technique of sandoni at all to build this kernel and then.",
                    "label": 0
                },
                {
                    "sent": "To compute things fast, we're going to use a method for solving linear systems quickly from a Spielman and Tang.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of going quickly here, but.",
                    "label": 0
                },
                {
                    "sent": "Feel free to ask.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Myself, rich on offline for more of the details so.",
                    "label": 0
                },
                {
                    "sent": "What's the benefit so?",
                    "label": 0
                },
                {
                    "sent": "We get a lot of the benefit of using a huge graph.",
                    "label": 1
                },
                {
                    "sent": "We don't have short circuits, but we can choose a smaller landmark set according to the amount of computational resource we have to work with.",
                    "label": 0
                },
                {
                    "sent": "So here, so we've gone ahead and done some experiments on large scale SSL here.",
                    "label": 0
                },
                {
                    "sent": "Here we can see the roads is also the experiment.",
                    "label": 0
                },
                {
                    "sent": "So here's our method over a range of training set sizes.",
                    "label": 0
                },
                {
                    "sent": "We did quite well.",
                    "label": 0
                },
                {
                    "sent": "Now we can't compare this directly to the Laplacian SVM because with 64,000 points we just couldn't solve and get this kernel in any period of time, so designed essentially a budget version of this Laplacian SVM.",
                    "label": 0
                },
                {
                    "sent": "Which improves on on the straight Gaussian kernel, but still a bit weaker than using our method for Tracy.",
                    "label": 0
                },
                {
                    "sent": "Conservative performance OK.",
                    "label": 0
                },
                {
                    "sent": "This graph.",
                    "label": 0
                },
                {
                    "sent": "Is a little bit about future research here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps some of you are familiar with the complex project, but here.",
                    "label": 0
                },
                {
                    "sent": "Here is using the same type of kernel which is a mixture of Gaussian annul, apasan, approximated and here we have some maze problem in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "These white points represent landmark points.",
                    "label": 0
                },
                {
                    "sent": "This is a value function and the idea is that somehow the value function corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Due to the kernel being fixed at some point and then looking at the value as the kernel changes so the Gaussian kernel gives us the smoothness.",
                    "label": 0
                },
                {
                    "sent": "Here the Laplacian kind of allows us to propagate or the edge.",
                    "label": 0
                },
                {
                    "sent": "So rather than having kind of straight.",
                    "label": 0
                },
                {
                    "sent": "Concentric, beautiful radial basic contours.",
                    "label": 0
                },
                {
                    "sent": "Here the Laplacian this shape to this.",
                    "label": 0
                },
                {
                    "sent": "So this is a potential application of these type of kernels to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Gang this point OK?",
                    "label": 1
                },
                {
                    "sent": "And they work in a Journal version of this.",
                    "label": 0
                },
                {
                    "sent": "OK I.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I stopped to forget about this, but I'll let China so back to learning data distribution.",
                    "label": 0
                },
                {
                    "sent": "John, I'm not a PKB Asian so I will.",
                    "label": 0
                },
                {
                    "sent": "I will probably mangle this a little bit, but the key thing here is is.",
                    "label": 0
                },
                {
                    "sent": "Is to eliminate the prior in fact phase bound.",
                    "label": 1
                },
                {
                    "sent": "So normally you for instance would have a relative entropy term on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "That would include the prior.",
                    "label": 0
                },
                {
                    "sent": "Now we've eliminated this.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe John can help me out if I make any mistakes instead, our complexity is basically measured through this term gamma, which is a mixture which we used to sample from over our hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "That same thing, but with the true risk, yes, but you don't know what it is, but you can still workout the kernel divergences between that distribution, so you have a prior distribution that you don't know what it is, but yes.",
                    "label": 0
                },
                {
                    "sent": "And you get this sort of divide.",
                    "label": 1
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Unlike most bands, it's 1 / M to the 2/3, or in many cases yes.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "But the gamma is a little bit of an unknown factor here, so yeah, it is quite clear that you get OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you for clarifying.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application of this is we can consider regular regularization with.",
                    "label": 0
                },
                {
                    "sent": "A kernel method and we can have different bandwidths of the kernel and we plug in kernel method.",
                    "label": 0
                },
                {
                    "sent": "We go ahead and get the following bound.",
                    "label": 0
                },
                {
                    "sent": "Fair enough, OK?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Future directions unfortunately had trouble starting the project on time it insubria, so we've requested if possible.",
                    "label": 0
                },
                {
                    "sent": "If we could complete the project in September 2012 to use Fabio's money to that point, we plan to extend results on fast online prediction for tree MRF's.",
                    "label": 1
                },
                {
                    "sent": "Extend the mall a great deal.",
                    "label": 0
                },
                {
                    "sent": "I've been told by in Syria that we can expect some experiments on the Bristol data set, which we've had some trouble getting ahold of in the future.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm quite happy with the research all over, and I think there's a lot of directions for extensions.",
                    "label": 0
                },
                {
                    "sent": "Personally, I'm very interested in developing a collaboration with Tubingen.",
                    "label": 0
                },
                {
                    "sent": "There's already kind of four papers in this P resistance Area, 2 from UCL, 2 from Tubingen.",
                    "label": 0
                },
                {
                    "sent": "We were trying to plan a visit so that we can have a PhD student from Tubingen visit UCL awhile.",
                    "label": 0
                },
                {
                    "sent": "Maybe Insubria is also interested in this research, so we'd like to kind of build on this momentum here already.",
                    "label": 0
                },
                {
                    "sent": "Some directions, computational issues.",
                    "label": 0
                },
                {
                    "sent": "Distance is hard to compute efficiently.",
                    "label": 0
                },
                {
                    "sent": "What we want to understand performance over a Fuller range of P. Then I briefly discussed the reinforcement learning application where if you went back to that previous picture, if you change the value of P and you have this perhaps maze example.",
                    "label": 0
                },
                {
                    "sent": "Well, if you have an arrow board or perhaps a geode ethic, path distance works better.",
                    "label": 0
                },
                {
                    "sent": "So we want a larger value of P OK?",
                    "label": 0
                },
                {
                    "sent": "Thank you, that's it.",
                    "label": 0
                }
            ]
        }
    }
}