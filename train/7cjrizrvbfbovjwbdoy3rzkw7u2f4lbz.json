{
    "id": "7cjrizrvbfbovjwbdoy3rzkw7u2f4lbz",
    "title": "Summarizing News Articles using Question-and-Answer Pairs via Learning",
    "info": {
        "author": [
            "Xiaxia Wang, State Key Laboratory for Novel Software Technology, Nanjing University"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_wang_news_articles/",
    "segmentation": [
        [
            "OK hi everyone, thanks for coming on today.",
            "I'm going to talk about our work, summarizing news articles using question and answer pairs by learning approaches.",
            "So my name is George and this is joint work with com."
        ],
        [
            "And where from Google Research at New York City?",
            "OK, so our main motivation stems from the launch of the new version of Google News in 2018, which introduced a FAQ feature.",
            "So just to give you a simple example here, sorry.",
            "Yeah, so here is a new story about the recent Southern California take five and below the top coverage you can actually find the most frequently asked questions by Google users.",
            "So for example, here are the users are interested in is Santa Rosa a large city in California is in danger of the file and also users are interested in what is the cause of this California file.",
            "So on the research side, we were what we would like to ask is, can we structurally summarize news articles using these question an answer in pairs.",
            "So one benefit of doing this is to avoid the bias in news articles because we are aggregating over a large number of anonymized user queries and the second benefit is we want to capture the most important aspects of the new story that Google users are most interested in."
        ],
        [
            "OK, so new summarization has been a popular research topic for decades, but not much work has been done for new summarization using question answers.",
            "So high level there are two fundamental approaches, so the first one is just to mining is just to mine the questions from user query logs and this is what most existing literature is interested in and the second approach is to learn the questions from news article contents.",
            "So the second approach is especially important for newly published articles.",
            "As well as long tail articles, because for those articles the user query logs is easy, are either not available or the data is super sparse.",
            "So our paper actually focuses on the second approach learning approach, and we're basically using the weak supervision over the data from the first approach, mining the by many the query logs."
        ],
        [
            "OK, so here is our line of today's talk.",
            "So first I will give an overview of our entire system and then I will talk about the mining approach to gather the weak supervision data and then after that I will talk about our learning approach for auto generation of the structure summary.",
            "And finally I will talk about the experiments.",
            "OK, so here is an overview."
        ],
        [
            "Two of the entire system, so you can clearly see that our system mainly consists of two parts, so the first part is this mining part and the second one is this learning part.",
            "So for the mining part we start from news documents as well as their associated question queries and we group them together and get all the question clusters and then for each question cluster we extract the answer snippets from news documents and after gathering this data we use them.",
            "As the weeks vision data and further into the learning system and in the learning system, we start from document content only without using any of the questions queries and we build a model to predict attention Maps by learning from the answer snippet and after that we have the predicted attended document and then we use our.",
            "We build our natural question generation model on the sentences with its attendant positions.",
            "As our answer position and we are able to generate a bunch of questions.",
            "And further, we have a separate module that's called question summarizers that can further summarize all the questions and combining everything together.",
            "We have the generated structure summary, so I will talk about this in more details in the following slides."
        ],
        [
            "OK, so let me start with the mining approach.",
            "So the first step of our mining approaches by extracting question queries from anonymized query logs.",
            "So note that from the user logs actually most queries are referring to single entities, so those are not super interesting because they don't.",
            "They're not super helpful in producing the structure summary.",
            "So what we are interested in our is actually the question queries which are more specific and more useful in capturing the important aspects.",
            "In a new story.",
            "And in the second stage we do question clustering to group semantically identical but syntactically different queries altogether, and the approach we're using is just hierarchical agglomerative clustering and similarity between 2 questions queries is captured by the averaging over the pre trained word embeddings and we are using awaiting of inverted word frequency.",
            "So just to give you a quick example here.",
            "So suppose we have a new story which is about Starbucks closed for anti bias training.",
            "And we gather the question queries from user logs and after group or the question queries together we are able to obtain 3 three different clusters.",
            "So the first cluster is asking about what time is Starbucks closed and the second cluster is asking about why is Starbucks closed and the last one is asking about what is anti bias training."
        ],
        [
            "So after we got all those questions query clusters what we do here is are we extract the answer snippets from news documents.",
            "So the model we are applying here in this state of the art model called QA net and the good thing about this is because we have the question clusters.",
            "So we can actually use utilized.",
            "The question paraphrases are within each question cluster and we do this by identifying the answer with the highest aggregated confidence score over the entire cluster.",
            "So again, just to give you one example.",
            "So using our previous example.",
            "So for each of the questions cluster we are able to extract the correct answer snippet from the news article.",
            "So for the first question cluster ask about what time is Starbucks closed, we are able to extract this Tuesday afternoon from the news document."
        ],
        [
            "So as a last part we also have a question summarization module where the goal is to improve the readability of the question clusters and remove the redundancy as much as possible.",
            "So the way we're doing this is super simple.",
            "We just find the top non question query that's closest to all the questions queries in our query cluster.",
            "So again, using the previous example for the first question, cluster the top non question query, is Starbucks training closing time which summarizes this question cluster pretty well.",
            "So this approach actually works very well in practice."
        ],
        [
            "OK, so let's everything for the mining approach.",
            "It seems like the mining approach already works very well for all the popular news articles, so you might want to ask, why do we need that learning approach?",
            "So we need to think like does the mining approach actually works for you for newly published articles?",
            "As well as long tail articles, because for those articles the user query log is actually either not available or the data is very sparse, so you don't have any data mined from.",
            "So the key question we want to answer in this paper is can we automatically generate the structure summary from an article content only without using its query logs?",
            "OK, so now let me describe our entire learning system.",
            "So again using the previous new story as an example, we start from documents only."
        ],
        [
            "We predict it's attention Maps, which basically means we predict which snippet in those articles should be highlighted.",
            "So, like in this case, the Tuesday afternoon or anti bias training should be highlighted and then we have a question generation system that generates the corresponding questions.",
            "Based on each of the highlighted snippets.",
            "And then we also have the question summarization module that produced the question summarization over each of these clusters.",
            "And the combining the questions summer Question summary, as well as the highlighted snippet from the document, we have what we called the structure summary of this entire news article."
        ],
        [
            "So how do we exactly do this?",
            "How do we first go from the documents to is highlighted?",
            "Places to it's highlighted snippets?",
            "So this is an overview of our learning system.",
            "So basically we start by tokenizing documents into different context words.",
            "We took the pre trained word embeddings as well as one hot representation of the POF stacks.",
            "We build our bidirectional STM layer or on top of the word embeddings and we also have a self attention layer that can better contextualize the hidden representations.",
            "So after that we have the augmented context which is basically each hidden state.",
            "Comes paginated with the self attended context.",
            "After that we have a projection layer which is basically two layer feed forward network and finally we have the prediction layer which predicts the probability of each word whether it should be highlighted.",
            "And finally we have a weighted cross entropy loss to balance between the positive and negative classes.",
            "And for this entire learning system we are using the document with their answer snippets from the mining approach as our training data."
        ],
        [
            "OK, so for the second phase we want to go from the highlighted document snippets to their corresponding generated questions.",
            "So how do we do this?",
            "So on high level this is actually a sequence to sequence model.",
            "So the 1st three layers are actually exactly the same as before.",
            "The only difference here is the only difference here is we use other positions, so there's a binary indicator of whether each word is belongs to the answer snippet or not.",
            "Do not belong to the answers in it.",
            "And after that we have the encoder layer, which is again a bidirectional RSTM layer, and for the decoder layer we're using an attention based decoder with the copy mechanism.",
            "I only have 5 minutes left, so I'm not going into details.",
            "If you have any questions, feel free to ask me.",
            "So for this system we're using the question answer pairs from the mining approach as our training data.",
            "So finally, for the questions summarizing module, so you."
        ],
        [
            "I see that this model is exactly the same as before, so the only difference here is we change the input output a little bit different, so the input here now is all the questions and the output is the question summary, and again we are using the question cluster as well as the question summary from the mining approach as our training data for this."
        ],
        [
            "Yep.",
            "OK, so for experiments we made it experiments on two datasets.",
            "The first one is the famous academic year data set.",
            "The squad data set and the second one is a large scale news data set which we collected from the Google News corpus.",
            "So you can see that there the two days that have very different characteristics in terms of like the news data set is much more diverse because it has a much lower number of average QA pairs per article.",
            "So these so the squad data set has questions generated by humans.",
            "And the newsletters that contain questions that are actually asked by the by the users.",
            "So these are questions user actually interested in."
        ],
        [
            "So first to evaluate our document attention map prediction, we can see that our model with the self attended context provides the best, gives the best AUC score over the news data set.",
            "We did a bunch of ablation experiments as well to see which feature is most important."
        ],
        [
            "And for the second phase, the natural question generation model, our model, the sequence to sequence model plus the copying mechanism.",
            "The binary indicator for the answer position as well as a few Aztec features give the best performance in terms of Bleu score and rose scores."
        ],
        [
            "And here are some example generated questions on the news data.",
            "So for example like.",
            "Let me just highlight the second case, the second case.",
            "The question is actually more open ended because most of the sentence has been highlighted.",
            "It's talking about some cycle and the model actually can capture that and is able to generate a question saying what happened.",
            "What has happened to this cycle?",
            "So this is very good."
        ],
        [
            "So finally to evaluate the question summarizer, we can see that our sequence to sequence model plus both the copy mechanism and the PS text features gives the best score in terms of Bleu scores with different number of N grams as well as the road score."
        ],
        [
            "And here are some example generated summaries.",
            "So."
        ],
        [
            "So in conclusion, in this paper we propose to summarize news articles in a structured way using Question answer pairs.",
            "So we first provide we first propose an unsupervised mining approach to automatically produce structure summary from articles and their query logs, and the 2nd we propose a learning approach using weak supervision data from the mining approach.",
            "So we are able to generate structure summary from article content only and we also and this is especially important.",
            "For newly published articles as well as along their articles.",
            "So that's pretty much for today's talk.",
            "Thanks for listening.",
            "Are there any questions?",
            "Have you tried to drive Contacts or any other use cases and publicly announces it'll give you anticipate their other use cases with plugs?",
            "I do have an example of the use cases.",
            "Yeah, so for instance, classifying electronic labels.",
            "In terms of the context.",
            "Yep, so our system is actually pretty big, so I believe each of the components can be used in different user cases.",
            "So for example, like the important snippet prediction can actually be used to like highlight which content from news article is important.",
            "Like you can use it in many other scenarios as well.",
            "And the question generation system actually can be used like if you just want to know what what kind of questions can be generated.",
            "Can be generated from news article.",
            "You can use that as well.",
            "Yes, so I was wondering.",
            "Or to find out.",
            "So what specific document embedding are you talking about?",
            "I think our model here.",
            "So it's actually just a slightly different way of representing the document embeddings, because we are using the word embeddings, we're using the bidirectional STM to capture the contextualization of different hidden states.",
            "So in some sense, this is just a different representation of the documenting body.",
            "Oh custard Patch out.",
            "That's a good question, yeah?",
            "Um?",
            "Yeah, we haven't experimented with those spot.",
            "That's a possible direction.",
            "Yes, we can use like better document embeddings.",
            "There are bunch of different ways to do that.",
            "Yes, we can do that as well, yeah?",
            "In this world, I think your focus.",
            "Text document.",
            "Additional data sources, like public knowledge graph.",
            "That's interesting, um.",
            "Going large graph is pretty big, I think for certain subset of the Google Knowledge Graph.",
            "Maybe you can apply some similar techniques as well, like if you are only focused on like specific entity attribute type of triples.",
            "You can predict questions like what attribute does this entity have?",
            "Something like that?",
            "Yes, you can maybe apply this technique on those type of questions as well.",
            "I had the same question.",
            "The question was how to apply this technique to acquiring these larger.",
            "And the second one is your results.",
            "I fly to high.",
            "But do you think that you could integrate some semantic features in our training models?",
            "Coming from this large map or for you is cut.",
            "So to answer your first question, I think this app this technique can still be applied are we only need to figure out a different representation of the input field?",
            "Current input is just a text document, so you have all the LCM TM layers.",
            "So to better represent Knowledge graph, maybe you need a slightly different representation of the input.",
            "But I think that I can still be applied to answer your second question.",
            "What kind of semantic features are you talking about?",
            "Duration from even more complex units.",
            "Oh, I see what you mean.",
            "Yes, yes, I think we can augment each part of the component in the system like violin large graph.",
            "Not yes.",
            "Yes you definitely can do this.",
            "So thank you so much.",
            "Get caught."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK hi everyone, thanks for coming on today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about our work, summarizing news articles using question and answer pairs by learning approaches.",
                    "label": 1
                },
                {
                    "sent": "So my name is George and this is joint work with com.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And where from Google Research at New York City?",
                    "label": 0
                },
                {
                    "sent": "OK, so our main motivation stems from the launch of the new version of Google News in 2018, which introduced a FAQ feature.",
                    "label": 1
                },
                {
                    "sent": "So just to give you a simple example here, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here is a new story about the recent Southern California take five and below the top coverage you can actually find the most frequently asked questions by Google users.",
                    "label": 0
                },
                {
                    "sent": "So for example, here are the users are interested in is Santa Rosa a large city in California is in danger of the file and also users are interested in what is the cause of this California file.",
                    "label": 0
                },
                {
                    "sent": "So on the research side, we were what we would like to ask is, can we structurally summarize news articles using these question an answer in pairs.",
                    "label": 0
                },
                {
                    "sent": "So one benefit of doing this is to avoid the bias in news articles because we are aggregating over a large number of anonymized user queries and the second benefit is we want to capture the most important aspects of the new story that Google users are most interested in.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so new summarization has been a popular research topic for decades, but not much work has been done for new summarization using question answers.",
                    "label": 1
                },
                {
                    "sent": "So high level there are two fundamental approaches, so the first one is just to mining is just to mine the questions from user query logs and this is what most existing literature is interested in and the second approach is to learn the questions from news article contents.",
                    "label": 1
                },
                {
                    "sent": "So the second approach is especially important for newly published articles.",
                    "label": 0
                },
                {
                    "sent": "As well as long tail articles, because for those articles the user query logs is easy, are either not available or the data is super sparse.",
                    "label": 0
                },
                {
                    "sent": "So our paper actually focuses on the second approach learning approach, and we're basically using the weak supervision over the data from the first approach, mining the by many the query logs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is our line of today's talk.",
                    "label": 0
                },
                {
                    "sent": "So first I will give an overview of our entire system and then I will talk about the mining approach to gather the weak supervision data and then after that I will talk about our learning approach for auto generation of the structure summary.",
                    "label": 1
                },
                {
                    "sent": "And finally I will talk about the experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is an overview.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two of the entire system, so you can clearly see that our system mainly consists of two parts, so the first part is this mining part and the second one is this learning part.",
                    "label": 0
                },
                {
                    "sent": "So for the mining part we start from news documents as well as their associated question queries and we group them together and get all the question clusters and then for each question cluster we extract the answer snippets from news documents and after gathering this data we use them.",
                    "label": 0
                },
                {
                    "sent": "As the weeks vision data and further into the learning system and in the learning system, we start from document content only without using any of the questions queries and we build a model to predict attention Maps by learning from the answer snippet and after that we have the predicted attended document and then we use our.",
                    "label": 0
                },
                {
                    "sent": "We build our natural question generation model on the sentences with its attendant positions.",
                    "label": 0
                },
                {
                    "sent": "As our answer position and we are able to generate a bunch of questions.",
                    "label": 0
                },
                {
                    "sent": "And further, we have a separate module that's called question summarizers that can further summarize all the questions and combining everything together.",
                    "label": 0
                },
                {
                    "sent": "We have the generated structure summary, so I will talk about this in more details in the following slides.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me start with the mining approach.",
                    "label": 0
                },
                {
                    "sent": "So the first step of our mining approaches by extracting question queries from anonymized query logs.",
                    "label": 1
                },
                {
                    "sent": "So note that from the user logs actually most queries are referring to single entities, so those are not super interesting because they don't.",
                    "label": 0
                },
                {
                    "sent": "They're not super helpful in producing the structure summary.",
                    "label": 0
                },
                {
                    "sent": "So what we are interested in our is actually the question queries which are more specific and more useful in capturing the important aspects.",
                    "label": 1
                },
                {
                    "sent": "In a new story.",
                    "label": 0
                },
                {
                    "sent": "And in the second stage we do question clustering to group semantically identical but syntactically different queries altogether, and the approach we're using is just hierarchical agglomerative clustering and similarity between 2 questions queries is captured by the averaging over the pre trained word embeddings and we are using awaiting of inverted word frequency.",
                    "label": 1
                },
                {
                    "sent": "So just to give you a quick example here.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have a new story which is about Starbucks closed for anti bias training.",
                    "label": 0
                },
                {
                    "sent": "And we gather the question queries from user logs and after group or the question queries together we are able to obtain 3 three different clusters.",
                    "label": 0
                },
                {
                    "sent": "So the first cluster is asking about what time is Starbucks closed and the second cluster is asking about why is Starbucks closed and the last one is asking about what is anti bias training.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after we got all those questions query clusters what we do here is are we extract the answer snippets from news documents.",
                    "label": 0
                },
                {
                    "sent": "So the model we are applying here in this state of the art model called QA net and the good thing about this is because we have the question clusters.",
                    "label": 0
                },
                {
                    "sent": "So we can actually use utilized.",
                    "label": 0
                },
                {
                    "sent": "The question paraphrases are within each question cluster and we do this by identifying the answer with the highest aggregated confidence score over the entire cluster.",
                    "label": 1
                },
                {
                    "sent": "So again, just to give you one example.",
                    "label": 0
                },
                {
                    "sent": "So using our previous example.",
                    "label": 0
                },
                {
                    "sent": "So for each of the questions cluster we are able to extract the correct answer snippet from the news article.",
                    "label": 0
                },
                {
                    "sent": "So for the first question cluster ask about what time is Starbucks closed, we are able to extract this Tuesday afternoon from the news document.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a last part we also have a question summarization module where the goal is to improve the readability of the question clusters and remove the redundancy as much as possible.",
                    "label": 1
                },
                {
                    "sent": "So the way we're doing this is super simple.",
                    "label": 1
                },
                {
                    "sent": "We just find the top non question query that's closest to all the questions queries in our query cluster.",
                    "label": 0
                },
                {
                    "sent": "So again, using the previous example for the first question, cluster the top non question query, is Starbucks training closing time which summarizes this question cluster pretty well.",
                    "label": 1
                },
                {
                    "sent": "So this approach actually works very well in practice.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's everything for the mining approach.",
                    "label": 0
                },
                {
                    "sent": "It seems like the mining approach already works very well for all the popular news articles, so you might want to ask, why do we need that learning approach?",
                    "label": 0
                },
                {
                    "sent": "So we need to think like does the mining approach actually works for you for newly published articles?",
                    "label": 0
                },
                {
                    "sent": "As well as long tail articles, because for those articles the user query log is actually either not available or the data is very sparse, so you don't have any data mined from.",
                    "label": 0
                },
                {
                    "sent": "So the key question we want to answer in this paper is can we automatically generate the structure summary from an article content only without using its query logs?",
                    "label": 1
                },
                {
                    "sent": "OK, so now let me describe our entire learning system.",
                    "label": 0
                },
                {
                    "sent": "So again using the previous new story as an example, we start from documents only.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We predict it's attention Maps, which basically means we predict which snippet in those articles should be highlighted.",
                    "label": 1
                },
                {
                    "sent": "So, like in this case, the Tuesday afternoon or anti bias training should be highlighted and then we have a question generation system that generates the corresponding questions.",
                    "label": 0
                },
                {
                    "sent": "Based on each of the highlighted snippets.",
                    "label": 0
                },
                {
                    "sent": "And then we also have the question summarization module that produced the question summarization over each of these clusters.",
                    "label": 0
                },
                {
                    "sent": "And the combining the questions summer Question summary, as well as the highlighted snippet from the document, we have what we called the structure summary of this entire news article.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we exactly do this?",
                    "label": 0
                },
                {
                    "sent": "How do we first go from the documents to is highlighted?",
                    "label": 0
                },
                {
                    "sent": "Places to it's highlighted snippets?",
                    "label": 0
                },
                {
                    "sent": "So this is an overview of our learning system.",
                    "label": 0
                },
                {
                    "sent": "So basically we start by tokenizing documents into different context words.",
                    "label": 0
                },
                {
                    "sent": "We took the pre trained word embeddings as well as one hot representation of the POF stacks.",
                    "label": 0
                },
                {
                    "sent": "We build our bidirectional STM layer or on top of the word embeddings and we also have a self attention layer that can better contextualize the hidden representations.",
                    "label": 0
                },
                {
                    "sent": "So after that we have the augmented context which is basically each hidden state.",
                    "label": 0
                },
                {
                    "sent": "Comes paginated with the self attended context.",
                    "label": 0
                },
                {
                    "sent": "After that we have a projection layer which is basically two layer feed forward network and finally we have the prediction layer which predicts the probability of each word whether it should be highlighted.",
                    "label": 0
                },
                {
                    "sent": "And finally we have a weighted cross entropy loss to balance between the positive and negative classes.",
                    "label": 0
                },
                {
                    "sent": "And for this entire learning system we are using the document with their answer snippets from the mining approach as our training data.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for the second phase we want to go from the highlighted document snippets to their corresponding generated questions.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "So on high level this is actually a sequence to sequence model.",
                    "label": 0
                },
                {
                    "sent": "So the 1st three layers are actually exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "The only difference here is the only difference here is we use other positions, so there's a binary indicator of whether each word is belongs to the answer snippet or not.",
                    "label": 0
                },
                {
                    "sent": "Do not belong to the answers in it.",
                    "label": 0
                },
                {
                    "sent": "And after that we have the encoder layer, which is again a bidirectional RSTM layer, and for the decoder layer we're using an attention based decoder with the copy mechanism.",
                    "label": 0
                },
                {
                    "sent": "I only have 5 minutes left, so I'm not going into details.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions, feel free to ask me.",
                    "label": 0
                },
                {
                    "sent": "So for this system we're using the question answer pairs from the mining approach as our training data.",
                    "label": 1
                },
                {
                    "sent": "So finally, for the questions summarizing module, so you.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I see that this model is exactly the same as before, so the only difference here is we change the input output a little bit different, so the input here now is all the questions and the output is the question summary, and again we are using the question cluster as well as the question summary from the mining approach as our training data for this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "OK, so for experiments we made it experiments on two datasets.",
                    "label": 0
                },
                {
                    "sent": "The first one is the famous academic year data set.",
                    "label": 0
                },
                {
                    "sent": "The squad data set and the second one is a large scale news data set which we collected from the Google News corpus.",
                    "label": 0
                },
                {
                    "sent": "So you can see that there the two days that have very different characteristics in terms of like the news data set is much more diverse because it has a much lower number of average QA pairs per article.",
                    "label": 0
                },
                {
                    "sent": "So these so the squad data set has questions generated by humans.",
                    "label": 0
                },
                {
                    "sent": "And the newsletters that contain questions that are actually asked by the by the users.",
                    "label": 0
                },
                {
                    "sent": "So these are questions user actually interested in.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first to evaluate our document attention map prediction, we can see that our model with the self attended context provides the best, gives the best AUC score over the news data set.",
                    "label": 0
                },
                {
                    "sent": "We did a bunch of ablation experiments as well to see which feature is most important.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the second phase, the natural question generation model, our model, the sequence to sequence model plus the copying mechanism.",
                    "label": 0
                },
                {
                    "sent": "The binary indicator for the answer position as well as a few Aztec features give the best performance in terms of Bleu score and rose scores.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are some example generated questions on the news data.",
                    "label": 1
                },
                {
                    "sent": "So for example like.",
                    "label": 0
                },
                {
                    "sent": "Let me just highlight the second case, the second case.",
                    "label": 0
                },
                {
                    "sent": "The question is actually more open ended because most of the sentence has been highlighted.",
                    "label": 0
                },
                {
                    "sent": "It's talking about some cycle and the model actually can capture that and is able to generate a question saying what happened.",
                    "label": 0
                },
                {
                    "sent": "What has happened to this cycle?",
                    "label": 0
                },
                {
                    "sent": "So this is very good.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally to evaluate the question summarizer, we can see that our sequence to sequence model plus both the copy mechanism and the PS text features gives the best score in terms of Bleu scores with different number of N grams as well as the road score.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some example generated summaries.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, in this paper we propose to summarize news articles in a structured way using Question answer pairs.",
                    "label": 1
                },
                {
                    "sent": "So we first provide we first propose an unsupervised mining approach to automatically produce structure summary from articles and their query logs, and the 2nd we propose a learning approach using weak supervision data from the mining approach.",
                    "label": 1
                },
                {
                    "sent": "So we are able to generate structure summary from article content only and we also and this is especially important.",
                    "label": 0
                },
                {
                    "sent": "For newly published articles as well as along their articles.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty much for today's talk.",
                    "label": 0
                },
                {
                    "sent": "Thanks for listening.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Have you tried to drive Contacts or any other use cases and publicly announces it'll give you anticipate their other use cases with plugs?",
                    "label": 0
                },
                {
                    "sent": "I do have an example of the use cases.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for instance, classifying electronic labels.",
                    "label": 0
                },
                {
                    "sent": "In terms of the context.",
                    "label": 0
                },
                {
                    "sent": "Yep, so our system is actually pretty big, so I believe each of the components can be used in different user cases.",
                    "label": 0
                },
                {
                    "sent": "So for example, like the important snippet prediction can actually be used to like highlight which content from news article is important.",
                    "label": 0
                },
                {
                    "sent": "Like you can use it in many other scenarios as well.",
                    "label": 0
                },
                {
                    "sent": "And the question generation system actually can be used like if you just want to know what what kind of questions can be generated.",
                    "label": 0
                },
                {
                    "sent": "Can be generated from news article.",
                    "label": 0
                },
                {
                    "sent": "You can use that as well.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I was wondering.",
                    "label": 0
                },
                {
                    "sent": "Or to find out.",
                    "label": 0
                },
                {
                    "sent": "So what specific document embedding are you talking about?",
                    "label": 0
                },
                {
                    "sent": "I think our model here.",
                    "label": 0
                },
                {
                    "sent": "So it's actually just a slightly different way of representing the document embeddings, because we are using the word embeddings, we're using the bidirectional STM to capture the contextualization of different hidden states.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, this is just a different representation of the documenting body.",
                    "label": 0
                },
                {
                    "sent": "Oh custard Patch out.",
                    "label": 0
                },
                {
                    "sent": "That's a good question, yeah?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we haven't experimented with those spot.",
                    "label": 0
                },
                {
                    "sent": "That's a possible direction.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can use like better document embeddings.",
                    "label": 0
                },
                {
                    "sent": "There are bunch of different ways to do that.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can do that as well, yeah?",
                    "label": 0
                },
                {
                    "sent": "In this world, I think your focus.",
                    "label": 0
                },
                {
                    "sent": "Text document.",
                    "label": 0
                },
                {
                    "sent": "Additional data sources, like public knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "That's interesting, um.",
                    "label": 0
                },
                {
                    "sent": "Going large graph is pretty big, I think for certain subset of the Google Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can apply some similar techniques as well, like if you are only focused on like specific entity attribute type of triples.",
                    "label": 0
                },
                {
                    "sent": "You can predict questions like what attribute does this entity have?",
                    "label": 0
                },
                {
                    "sent": "Something like that?",
                    "label": 0
                },
                {
                    "sent": "Yes, you can maybe apply this technique on those type of questions as well.",
                    "label": 0
                },
                {
                    "sent": "I had the same question.",
                    "label": 0
                },
                {
                    "sent": "The question was how to apply this technique to acquiring these larger.",
                    "label": 0
                },
                {
                    "sent": "And the second one is your results.",
                    "label": 0
                },
                {
                    "sent": "I fly to high.",
                    "label": 0
                },
                {
                    "sent": "But do you think that you could integrate some semantic features in our training models?",
                    "label": 0
                },
                {
                    "sent": "Coming from this large map or for you is cut.",
                    "label": 0
                },
                {
                    "sent": "So to answer your first question, I think this app this technique can still be applied are we only need to figure out a different representation of the input field?",
                    "label": 0
                },
                {
                    "sent": "Current input is just a text document, so you have all the LCM TM layers.",
                    "label": 0
                },
                {
                    "sent": "So to better represent Knowledge graph, maybe you need a slightly different representation of the input.",
                    "label": 0
                },
                {
                    "sent": "But I think that I can still be applied to answer your second question.",
                    "label": 0
                },
                {
                    "sent": "What kind of semantic features are you talking about?",
                    "label": 0
                },
                {
                    "sent": "Duration from even more complex units.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see what you mean.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I think we can augment each part of the component in the system like violin large graph.",
                    "label": 0
                },
                {
                    "sent": "Not yes.",
                    "label": 0
                },
                {
                    "sent": "Yes you definitely can do this.",
                    "label": 0
                },
                {
                    "sent": "So thank you so much.",
                    "label": 0
                },
                {
                    "sent": "Get caught.",
                    "label": 0
                }
            ]
        }
    }
}