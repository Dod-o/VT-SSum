{
    "id": "li6wpbio3q6s4j6rb4oqxxs5ozdhrr55",
    "title": "Bayesian Probabilistic Models for Image Retrieval",
    "info": {
        "author": [
            "Vassilios Stathopoulos, Department of Statistical Science, University College London"
        ],
        "published": "Nov. 11, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/wapa2011_stathopoulos_bayesian/",
    "segmentation": [
        [
            "OK, so yeah I spent as Tom said my name is basilia.",
            "Yeah, I'm from the Department of Statistical Science of UCL.",
            "This is work actually I was doing during my PhD.",
            "Part of the work I was doing for my PhD and is with my supervisor in German, Josh.",
            "From the Information Retrieval group in the University of Glasgow.",
            "So I will be talking about Bayesian probabilistic models in the sense that we apply.",
            "Bayesian inference for probabilistic models for image retrieval, and by immeasurable I mean ad hoc image retrieval, where the problem is we have a static collection of images and then the user supplies query match and we return a rank list of the static image collection.",
            "To chat to other ranking more relevant images."
        ],
        [
            "The top.",
            "So that's a brief outline.",
            "I will start with some background on bag of terms, image retrieval models, probabilistic models for information retrieval in general, and I will try to give the motivation for what we're doing in this work.",
            "Then I will go into the more technical section, which is how we apply Bayesian inference methods for probabilistic models for information retrieval, and in particular image retrieval models.",
            "And then I will show some experiments on a small collection image collection, which is has been extensively used in the field.",
            "And then I will close with some points for discussion and."
        ],
        [
            "Future work so bug of terms.",
            "Image retrieval is considered.",
            "Now the state of the art for image retrieval travel systems and the key to their success was that they rely on representation, which is exactly the same as the one used for text retrieval for text documents.",
            "So you convert the images into representation which where its image is represented by as a bug as a set an ordered set of parts.",
            "And these parts can be part of objects.",
            "So for example, if it's a face, it might be the nose.",
            "There's the the mouth and so on.",
            "Now, once you have this representation, then we can apply all the standard methods from information retrieval for waiting and ranking images and that provides us as an abstraction layer between the image domain and the information retrieval method.",
            "So basically we can use out of the box retrieval system such as Lucene or Terrier or if you're familiar with this kind of retrieval systems, an index images instead of text.",
            "So the bug of terms representation relies basically on three stages, which I explain briefly in the next slides.",
            "It's region detection features, description and code block generation quantization."
        ],
        [
            "So the region detection algorithm basically is responsible for segmenting images into the parts into the components of the objects in the image.",
            "The basic property of region detection algorithm should be repeatability and what I mean by the repeatability is that when we take the same scene, take a picture of the same scene and different lighting conditions or slight different perspective of the camera like rotation or tilt or something like that, the same.",
            "Regions have to be.",
            "Detected and that simplifies the retrieval algorithm because then we can match.",
            "So for example, in the face context, if we always extract regions from the nose, the mouth and the years it's easier to detect them later on.",
            "So for this the most there are many ways to do that.",
            "The most successful and popular in the literature are the interest point detectors, where basically they rely on some notion of curvature an.",
            "By curvature we mean edges.",
            "So basically they detect regions around.",
            "Corners or blobs and features like that and you result in elliptical regions such as those in the second image.",
            "Now the problem with the interest point is that they rely on edges and for that, for example, regions such as the Sky here, where they have a homogeneous appearance, they are not represented.",
            "Usually they don't generate any interest point, so we can capture them at all.",
            "So another approach is to use image segmentation.",
            "Algorithms like watershed or normalized cuts or just clustering of the pixels in terms of their color, and that tends to cluster the image, segment it into homogeneous regions.",
            "The problem is that with this approach is that we can capture this kind of regions, but it's very sensitive to illumination changes like lighting conditions and perspective changes, which will result in different regions every time so.",
            "Kind of balancing between is the first method which is just using a regular grid to segment the image into blocks.",
            "Now this doesn't capture all the properties that we need, like, especially repeatability, but there are certain tricks we can do for that, like for example, we can start with a regular grid of 8 by 8 pixels and then have several groups of different resolution, so we can capture different scales like objects appearing in different distances from the camera.",
            "And actually in these two papers aside there, the do an extensive study of different sampling strategies for region detection, and they found that actually regular grades of different resolution are one of the most successful methods for object recognition and image retrieval in general."
        ],
        [
            "So the second stage just once we have extracted the small regions, is to describe them to extract some features out of them, for which we're going to index later.",
            "So the most famous is the scale invariant feature transform, where basically from a small region we extract we have the edges and we extract the histogram of edge orientations.",
            "So basically just a well it's for histograms, waited actually for four subblocks of the.",
            "Inside the region and.",
            "Results in a feature vector of 128 dimensions which counting how many times we found an edge with a particular direction.",
            "Again, the problem with this approach is that we can capture color because we're based on edges.",
            "There are extensions we can add, like the average color of the of the region in the feature vector.",
            "But still as we rely on edges.",
            "Problems with homogeneous regions such as skies or C or whatever, which doesn't have a lot of edges that can be described accurately.",
            "So another approach which has been recently used in object recognition was to use the discrete cosine transform.",
            "I think Ben said had used that in one dimensional domain, so in this example basically you represent an image region as a weighted sum of.",
            "Orthogonal basis functions now in this in the right hand right hand side image you can see like these are the basis functions for an 8 by 8 region and for each of these basis functions you have a real number and if you sum them altogether, weighted by these real numbers, you end up with an image.",
            "This particular example is just an image with the letter A in a white background.",
            "So it doesn't really show, but that's the representation you have now.",
            "The good thing with discrete cosine transform is that you can exploit the compression properties they have, it has, so you can basically represent each region with the feature vector, which is maybe about 70 dimensions, maybe less depending on the compression you choose to use.",
            "Compared to the density to the SIFT features, it's a big."
        ],
        [
            "The reduction, so once you have the.",
            "Regions and you have extracted features from the regions you have to create the visual vocabulary with.",
            "That is the visual terms of parts that you are going to use to describe the image, and this is usually done by applying K means to cluster all the features extracted from the images.",
            "Now once we obtain the clusters, the cluster means we treat them as words in vocabulary of text and so we assigned to each cluster mean.",
            "A unique identifier.",
            "Now, once we have that, we can just write everything into a text file and treat an image as text where words are the identifiers for the cluster means.",
            "And yeah, of course we have to quantize its image by finding its image region, associating its image region to its closest visual term cluster in the vocabulary now.",
            "Once you have this representation, the we can apply any information retrieval, waiting and ranking scheme.",
            "So for example, Tom yesterday mentioned the vector space model with TF IDF weighting.",
            "So in this setting we have a D which is an image or a document is a vector of elements Dita DT where each element DT lower case.",
            "It's the weight or the importance of the term.",
            "Of the theater that appears in this image now the TF IDF just associated to this element the number of times term T appears to D that's denoted with N lowercase DD and is multiplied by the idea of the inverse document frequency in the collection.",
            "So we down weight terms which appear very frequently in the whole collection and using this representation.",
            "Then as a measure of similarity, you can use the inner product.",
            "Which is just the sum over the common terms that appear in both the document, the image and the query match.",
            "If you normalize document vectors to have a unit length and this results into cosine similarity.",
            "So that's pretty standard.",
            "Now the problem that comes with that is that the vector space model doesn't give us any guidance on how to select the weight.",
            "Like here we use the TF IDF weighting and we don't have any insight on how to choose these weights, especially the idea formula is something that has been developed in the 70s.",
            "It appeared firstly in the Journal of the American Association of Information Systems.",
            "It's very old.",
            "Generally they were using that in library science for indexing books."
        ],
        [
            "So.",
            "A way to have a formal method for developing, waiting and ranking functions is to resort to the probabilistic modeling.",
            "So in information retrieval there are two popular frameworks.",
            "One is the probabilistic relevance framework.",
            "Well, I give a reference from 2009, but actually this goes back to 80s.",
            "The first instance of this, which basically the information retrieval problem, is handled as a classification problem.",
            "So you have.",
            "Collection and you have to classify given a query the whole collection into relevant and relevant but you are in a peculiar situation where you don't have any training set to train.",
            "So you have to make a lot of assumptions.",
            "Now the second approach, which is the one I'm using here is the language modeling approach which I will explain in the next."
        ],
        [
            "Slides, so the language modeling approach you assume that there is a generative process from where the documents are generated so generated, so there is a stochastic process and which is governed by parameters, which here we will denote with filter.",
            "And each document is a random sample from this process, and so because it's document has its own process, the parameters Theta are indexed with a small D. The simplest model is to assume a multinomial distribution.",
            "Basically, the parameters Theta turn out to be just the probability of each term in the document.",
            "Write the distribution.",
            "I don't think it's necessary for here, just included for the sake of completeness.",
            "Now, once we have the model, we can resort to.",
            "Statistical literature and estimate the parameters so one.",
            "Of the simplest way to estimate the parameters to take the maximum likelihood like estimate the parameters such as the likelihood, which is a faster manipulation.",
            "Is maximized and that leads to simple counting.",
            "Basically for this particular model, so the parameters are set to the number of times the term appears.",
            "The document divided by the total terms in the document in the image.",
            "For the particular case.",
            "However, this can lead to overfitting because you can imagine that you have a large set of terms and a document can contain all the terms, so it will have terms with zero frequency and the probability of a term.",
            "With a zero frequency, the maximum likelihood estimate will be 0 and cause in the first equation the likelihood you take the product over this, then any document with the term of zero frequency will have zero probability.",
            "You will ever retrieve it.",
            "A solution to that is to resort to smoothing, so add something some pseudo count at the term frequencies.",
            "The theoretical implication of smoothing is actually.",
            "Assuming a prior distribution of the over the model parameters, so we can express pretty much like, I don't have the reference here, but there is a nice paper that shows how you can express all the smoothing methods that have been proposed in the information retrieval literature in terms of.",
            "A small thing using a different distribution.",
            "So basically you assume that the parameters have a prior distribution, which is a direct which is parameterized by some other parameters which we call hyperparameters Alpha.",
            "Now once we have the prior and the likelihood which is the first equation, we can estimate the maximum posteriori.",
            "Point estimate for the parameters, which is actually.",
            "The set of parameters that maximizes the joint likelihood.",
            "In this case, it turns out to be very easy to do that, because it's just counting again, but you also add this hyperparameters Alpha from the digital distribution, and you discount minus one, so basically.",
            "The most popular smoothing methods from IR can be recovered from this.",
            "If you set these Alpha T to the average term frequency in the whole collection, so."
        ],
        [
            "When you use this for a ranking documents, you have to evaluate for the query match the query document or the query vector.",
            "The likelihood under the model you have estimated with the parameters.",
            "Right now the right hand side is the log of this an we have excluded all the terms that log 'cause it doesn't change.",
            "The ranking is rank equivalent, so you can use it because it simplifies the calculation and also if you exclude terms which don't depend on the document, that just depends on the query.",
            "Get exactly the same ranking so you can simplify it further.",
            "So the interesting part is that the first sum is only depended on.",
            "Terms where they exist like their frequency is non zero in the document and in the query which means that this allows for an efficient implementation.",
            "You don't have to iterate through all the terms in the vocabulary for this query and you can see that the ranking function takes the form of a term the first time, which is in the log.",
            "That is the term frequency.",
            "The number of terms that number of times the terms appears in the document divided by this hyperparameters Alpha T which if you have set them to the.",
            "Average time frequency of the term in the collection results into something similar to IDF ranking function, so you can see you can recover the TF IDF by resorting to completely probabilistic framework.",
            "Now the implementation for this, as Tom said yesterday, is just inverted index data structure, which is basically has table of all the terms."
        ],
        [
            "Now.",
            "For image retrieval.",
            "We don't have like the bag of terms assumes that if you have quantized or limits features into terms discrete terms, the problem with quantization is that leads to quantization errors, and you can see especially the K means you can view it as a special case of the EM algorithm for.",
            "Components of equal mixing weights and I just dropped covariance matrices, so a better way to handle that is to do directly to estimate directly the density of the features in its image, right?",
            "And so another popular model is to handle directly like we don't do the quantization stage anymore, we just do the region detection and feature extraction and I will have its image as a collection of vectors in Rd.",
            "Uh, now 40.",
            "Too much we have again, a generative model which is governed by the parameter Citadis, which we take it to be a mixture of Gaussian distributions and the parameters set that they're actually the mixing coefficients.",
            "Pi KUK and covariance matrices Sigma K. That's actually a method that has been used also for audio retrieval and music.",
            "It's pretty general.",
            "Depends on what kind of extraction you use for the features.",
            "Now again, you can estimate the parameter spike, a Neuquen Sigma K with the EM algorithm for the mixture models, and once you have a query image you can rank the images in the collection by just using the log of the query likelihood under the estimated model parameters.",
            "With this approach is that there is no efficient data structure for this because you have to iterate through all the collection of images in order to estimate the log likelihood for the query."
        ],
        [
            "Now the.",
            "What we really want to estimate is not the hood is the predictive density.",
            "That is, we want to estimate the conditional probability of the feature vectors in an image in the query image given what we have observed from the.",
            "Collection image the image in the collection.",
            "We have the document D and.",
            "The way to do that is that we have to take the integral of the likelihood of a new vector X given the parameter Theta, and we have to marginalized out at the integral using weighted by the posterior probability of the parameters given the observed document that we have now.",
            "The maximum likelihood and maximum a posteriori estimates are in fact an approximation to this integral, and they rely on the asymptotic properties of the posterior.",
            "So if you have a lot of data, the posterior is going to be very sharply peaked around one point, which would be the maximum likelihood or the maximum posteriori point.",
            "So because the density is normalized, the point is a good approximation for the integral.",
            "Now the problem is that usually.",
            "The document has more parameters than the observations we have.",
            "So in the information retrieval case, the problem with a zero frequencies terms.",
            "Because we don't, we can observe in the document all the terms same in the image.",
            "Context you observe feature vectors, but you can't observe all possible feature vectors from all natural images."
        ],
        [
            "Now.",
            "Estimating this integral for the discretized case when we have quantized features, it turns out to be very easy because then take those are all calculated analytically.",
            "The actual posterior is again a direct let where the parameters of the direct light is the term frequencies plus the hyperparameters of the prior, which are basically the collection frequencies.",
            "And if you integrate that out with the query likelihood, you can get the log of the query given the document which has again similar form as.",
            "The the previous location I showed before again.",
            "The interesting part is that you can see that again it relies on terms which are non zero in the document in the query, which again it can implement it efficiently using an inverted index data structure.",
            "The only difference is that you have these sums for each query term which come from the gamma functions.",
            "Something more technical."
        ],
        [
            "Now.",
            "For the Gaussian mixture model, obtaining the posterior an.",
            "As a result, the predicted densities is not possible to do it analytically anymore, so you have basically two options.",
            "One is to use Markov chain Monte Carlo and the 2nd is to resort to some approximation.",
            "Now there are pros and cons for both approaches like.",
            "MCMC is asymptotically optimal.",
            "The more samples you draw from the posterior, the better your estimate will be.",
            "And there is actually theoretical guarantees of how many samples you need in order to achieve a particular accuracy.",
            "The problem with that is that we need several change, several market change to run for each document for each image in the collection.",
            "In order to monitor the convergence, and be sure that you are sampling points from the posterior distribution.",
            "And once you have the points, you have to store all the sample points.",
            "The posterior samples in your indexing structure, because when you want to evaluate the predictive density for a new query, you have to.",
            "Calculate the weighted sum of these points.",
            "Another approach which is more suitable for systems like information retrieval systems and image retrieval systems, is to use an approximation which we use a variational approximation which I will describe later.",
            "That gives us a local approximation of the posterior.",
            "And this results in our analytic results.",
            "So we have a formula which can we can directly evaluate and we don't have to resort in in any sort of numerical estimate by weighted average of points."
        ],
        [
            "So I won't describe fully the variational inference method.",
            "Just give a sketch about it.",
            "So basically we have to start with a.",
            "An equivalent expression for the mixture model where we just include the Latin variables Zetton K. So basically in the previous slide."
        ],
        [
            "You can see that there is a sum over the number of components.",
            "He"
        ],
        [
            "Here, by including the Latin variables NK, this sum becomes a product because Latin variables at 10K there are binary, and if it's one then the sample N is associated with the K component so that.",
            "Transforms this into a product form.",
            "Basically, if you take the issue marginalized out the Latin variables by taking the sum over all possible combinations, you get back to the original expression.",
            "Now, once we have this convenient form, we can start associating priors with all the parameters.",
            "So for the so.",
            "That's a pretty quick city of the variational method.",
            "In order to be able to get analytical expressions, we have to get priors which result into analytical solutions.",
            "So we have to use conjugate priors so the conjugate prior for the.",
            "Make sure coefficients Pi is again a dicklet where we have a hyperparameter A0.",
            "Now if we set Alpha to one, this results in the uniform prior.",
            "This means that all pies are priority have equal likelihood.",
            "Are considered the same.",
            "Now if we set a 0 to a small value that favors by case which have small value as well, so we're resorting to sparse solutions like if we have a mixture model where K is large, like 100, and we have a prior on Pi switches.",
            "0.0031 then we might end up only with 50 or 20 components having a non zero mixing coefficient.",
            "Now for the means, we can use a prior which is normal and it's based on a hyper parameter mu 0M zero and better where we can set M 0 to be at the center of the observed.",
            "The center of the observed feature space.",
            "So we take all the features in the collection.",
            "Whoever reads them, and we use that M. The reason why we do that is because we want to have a regular pretty much flat prior in the region where we have the observations.",
            "So basically we assume that everything that we haven't observed it has.",
            "Very low probability and things that we have observed are pretty much equal a priority.",
            "Of course.",
            "Now for the prior over the.",
            "Positive definite covariance matrices.",
            "We have to use an inverse Wishart prior.",
            "This is slightly technical, but you can take it as a prior distribution over matrices.",
            "Again, the hyperparameters WO and V0W0 is a precision matrix which we can set it to the inverse of the covariance over all the features we have in the collection.",
            "Again to give collection statistics for the prior and visit V0 is the degrees of freedom for the inverse.",
            "Which again set it into some high value in order to result into a flat prior in high likelihood regions."
        ],
        [
            "Now, once we have this.",
            "We can augment the parameters Theta D with the Latin variable set and get this new augmented parameter space capital Theta, and now we have to make some approximations about the posterior of pithy to capital Theta.",
            "Given the observed document.",
            "Now the only approximation we have to consider is that this posterior approximate posterior which we call Q Theta, Capitol Theatre.",
            "It factorizes between the two.",
            "Meaning that the parameter Citadis and the Latin verb set are independent with each other.",
            "That's an approximation.",
            "That's the only approximation we have to make.",
            "We don't make any other approximation about the form the particular forms of the posterior, so the parameters are.",
            "Everything will come out from that.",
            "Now, once we have that, we can use the same trick that we use when we derive the EM algorithm that is using the Jensen's inequality to write the marginal.",
            "Likelihood the probability of the document will have marginalized out all the parameters using the prior as these two terms.",
            "These two integrals now the first, the second integral.",
            "You can see, that is the KL diversions between the approximate posterior Q and the true posterior Beefeater given D, right?",
            "So if are true posterior if I are approximate posterior approaches the true posterior.",
            "This term will start to become zero.",
            "Then on the left hand side of the equation they the first step is the lower bound and the interesting thing with that is that this this term is, we can compute it analytically because Q, Theta we have specified it in a convenient form because we know it factorizes and we can compute it exactly.",
            "PD, Theta is a joint which we have it from the joint log likelihood and Q Theta is again something that.",
            "That we know.",
            "So we have something that we can calculate, and if we maximize this lower lower bound we can see that the KL divergent will be minimized.",
            "So it's equivalent.",
            "Instead of Max minimizing the KL divergent, which we can't because we don't have the true posterior P, Theta given D, we can maximize the lower bound and that equivalent equivalent to minimizing KL.",
            "The way to do that is, again, slightly technical.",
            "Basically, we take its term in the factorized posterior that we assume that the Q, Theta is equal to the product of custody and queues at typo that's accused here.",
            "And we take it step in dependently we we plug it in into the lower bound to creation.",
            "We consider all other terms to be constant and then we get this expression at the bottom which is basically an expectation over the factory variational posteriors for each pyramid."
        ],
        [
            "And that's a pretty standard algorithm actually.",
            "You can find it in Bishop's book in the 7th chapter you end up with variational posteriors, which are exact approximate, but they have an analytic solution.",
            "That's what I mean exact.",
            "So the posterior variational posterior for the Latin variables is just a multinomial with parameters are the variational posterior for the mixing coefficients is difficult with hyperparameters Alpha.",
            "And the variational posterior for mu, the means and the covariance matrices is a inverse Wishart distribution with parameters M Betta WMV.",
            "Now all these parameters like R, Alpha, MWK, NVC are estimated by an equivalent algorithm like the M, which is the variational em and it works in the same way.",
            "It first estimates the expectation for the zed variables and then it updates maximizes.",
            "The variational posted parameters WM and Alpha.",
            "The nice thing is that with that is that once we have the exact forms for the variational posterius, we can plug them in and we can estimate the predictive density directly, because it's just we can integrate.",
            "Take all this integral analytically, 'cause if you notice, they're just expectations under standard distribution.",
            "So for example, that something that is easy to see that we know that QP variational posterior for the mixing coefficients easily clipped, and that's the function.",
            "That we want to integrate PK.",
            "All other terms we can put them outside and that is.",
            "We know that the expectation of PK under directly is just the parameters Alpha K divided by the sum of Alpha K. So the same holds and for the rest of the parameters and we end up with a. Mastication, which is basically a mixture again but not of normal densities about of student densities, now the implication is that the students have longer tails, so they capture outliers better in terms of.",
            "Yeah, in contrast to normal density."
        ],
        [
            "Now yesterday Thomas mentioned the problem of finding the number of components in when you do clustering, and here mixture models are similar to clustering.",
            "Now when you resort to this variational framework and in general Bayesian inference you can.",
            "For example, for the for this particular application, you can estimate get an estimate of the number of components by using the properties of the of the posteriors you end up.",
            "So if you from the variational posterior for the mixing coefficients, which is a direct let, we know that the expectation for its of the mixing coefficients is just the parameter Alpha K divided by the sum of all alphas and the variances again depended on this Alpha K parameters now.",
            "In the variational EM algorithm, when we update this Alpha K parameters, we just use these situations that end up.",
            "That's that comes out from the variational approximation framework, right?",
            "So A0 is the prior hyperparameter we set and this is the prior hyperparameter plus the sum of these variables, which are basically the expectations of the Latin variables.",
            "Now you can see that if you set A0 to a small value which result ends up in sparse solutions.",
            "Right when.",
            "You don't have any points contributed to one component.",
            "Then the Alpha K will be small and cause that will be."
        ],
        [
            "Well, if you go back you can see that the predicted density.",
            "Will also be small for that particular component, so you can neglect it completely."
        ],
        [
            "And that's also the mass.",
            "Similar method is used in a paper by Bishop Anne Condo."
        ],
        [
            "2001 now yeah.",
            "To test this for models in total for different approaches.",
            "For ranking images, we use the Coral 5K collection, which is a collection of 5000 images in total, divided into 4500 images, which are we consider US training images is a static collection and 500 test images which we will consider as queries.",
            "Now the collection is divided into 50 categories in the training set there are 19 messages from each category.",
            "And in the training set, the 500 images there are 10 images from each category, so we treat each image as a query from this 50 categories and given one image, we expect the 90 images in the static collection to be ranked 1st, and that's how we evaluate the retrieval performance.",
            "That's just a."
        ],
        [
            "Other way of evaluating information retrieval algorithm.",
            "Now the preprocessing we do in the collection is fast.",
            "We convert the images into the luminance and chrominance channels, chrominance color space.",
            "We segment the images using a regular grid of 8 by 8 pixels an we use also sliding window, so we overlap every four pixels.",
            "We apply the DCT transform from each region.",
            "And we I don't mention it here.",
            "From the density coefficients, we keep only the the ones from the luminance channels and only the first 2 coefficient from the prominent style.",
            "This is again slightly technical, but it has to do also with the fact that this is the same transformation is used for JPEG images.",
            "So basically you can take it directly the JPEG from the file without decoding it and use this algorithm index images.",
            "And now for the bug of terms representation.",
            "We use K means an.",
            "We use 2000.",
            "To cluster the features, the feature vectors into 2000 clusters.",
            "For the Gaussian mixture model we run the EM algorithm and we use the state components.",
            "The reason why we fix this number 28 is because previous study with the same model with maximum likelihood estimates and the EM algorithm have showed that.",
            "8 components was the best actually I will show why that's the case later from the analysis I'm doing in the results Now 4.",
            "For the variational EM algorithm where we try to compute the predicted densities, we set the.",
            "The number of components to 40 and we remove components at the end as a post processing step.",
            "With the method I described later, based on the posteriors of the mixing coefficients.",
            "Now in all the M and variational EM algorithms, we initialize them by randomly associating the Latin variables.",
            "That's again a technical issue, but it's important because some people use K means to initialize Umm, but it's not fair for the variational em because you cannot use the same algorithm.",
            "So we decided to.",
            "A simple initialization strategy to be fair in comp."
        ],
        [
            "The two approaches.",
            "So I don't know if you're familiar with valuation measures.",
            "For information retrieval, you want me to go through that a little bit.",
            "OK, so basically.",
            "Information retrieval measure precision and recall recall is the number of so we retrieve a set of documents and recall is the number of relevant documents retrieved.",
            "So how many documents will retrieve relevant to the query?",
            "We judge if they're relevant from the training collection we have, right?",
            "Now the precision measures how accurate this retrieval is from the number, which means how many of the retrieved set number of relevant documents retrieved were actually relevant.",
            "So it's similar to the accuracy in the classification.",
            "Now the problem with these measures with precision and recall, which are similar to true positive and false positive rate in an accuracy in classification, is that they don't take into account the ranking and information retrieval.",
            "We're interested about the ranking because.",
            "If we have a large collection like here you have 5 five 5000 images and we rank.",
            "The most relevant images at the bottom, the users probably will never see them, so we have to have measures that take into account also the ranking.",
            "So the easiest measure is this P at 5:00 PM, ten PM 20, which basically is a precision at the 1st five top ranked relevant documents.",
            "Precision attend is the same.",
            "The precision evaluated at the first 10 relevant documents, Precision 20 at the 1st 20 and so on.",
            "Our precision is the precision after we have seen all the relevant documents in the collection.",
            "Or in the ranking list.",
            "So we go down the ranking list and whenever we find the last relevant document.",
            "In the ranking list we measure the precision there and mean average precision is the fact that you can express precision as a function of recall.",
            "So you integrate out is the average overall record values.",
            "It's similar to the integrated the area under the Roc curve, but when you have precision, recall.",
            "So the first method bought map is a bag of terms approach.",
            "We're using maximum posteriori estimate.",
            "The second method is same bag of their models where we use a predictive density where we take the marginalization, we using the posteriors, you see there is a.",
            "A difference, an increase in performance across all measures results are not statistically significant for that so.",
            "Now for the Gaussian mixture models, you immediately see that if you avoid the quantization step that is running.",
            "K means and model directly the density of continuous image features with mixture models.",
            "You gain a significant increase in performance.",
            "Then that's with a maximum likelihood estimate.",
            "If you use a maximum posteriori estimate, you can again see an increase in performance.",
            "That's the fact that you have integrated also the prior.",
            "Calculated also the prior in the maximization and if you use the predictive density where we use the variational EM algorithm to estimate the approximation of the posteriors and then integrate them out, you see that you get the statistical significant improvement overall results.",
            "Now the statistical significance is estimated from the previous row because so this is statistically significant from the previous row.",
            "And so one.",
            "Now I said.",
            "That we found out why this 8.",
            "When we were there, George of the Gaussian mixture model for the maximum likelihood Emel Nmap we said I said that we said the email EM algorithm too."
        ],
        [
            "8 components Now with the variationally amalgam.",
            "Would we were able to estimate the number of components for its images directly from the output of the algorithm by pruning out unused components.",
            "Now if you plot this for the whole training collection, you get this nice distribution.",
            "And you see that the mean is 8, so the results from that paper that they were arguing that eight from all their experiments was in fact they had results from other values around 8:00 and they were pretty much similar and can be justified because the mean is 8.",
            "So taking them in is a good strategy of course, for this particular collection the mean was eight the easy too, but it's not easy to real in a real setting to do exhaustive results to find what is the number of components.",
            "So by actually allowing the number of components to come out in some automatic way, not only you can tune that parameter automatically, but you can also have allowed this number to change for its image because.",
            "In the EM algorithm, you said this eight and static for the whole collection, which means that complicated images with complicated structures get under estimated while simplified images with simple structure they get overestimated because they use more components while you allow.",
            "When you allow this number of components to alter between images, that's why."
        ],
        [
            "Actually you are.",
            "You get this significant increase in performance.",
            "Over the maximum likelihood estimates.",
            "The other point I can make from this is that.",
            "Since the.",
            "There is a statistical significant increase between the maximum a posteriori like point estimates and the predictive density.",
            "This also highlights some sort of overfitting that all the point estimates in fact are overfitting, and that's why it's important to take into account the posterior distribution when you're making."
        ],
        [
            "Addictions.",
            "Now conclusions I didn't spoke about that, but from the experiments what we have seen is that I said that the bug of their models are efficient because you have sparse representation.",
            "That's true, but running a K means in a collection of 1,000,000 feature vectors to cluster them into 2000 clusters is not very efficient, and it's I found it to be an engineering difficult engineering task to code up VM algorithm.",
            "To work.",
            "So there are alternative to that to K means you can use DB scan or hierarchical clustering.",
            "Or even apply K means and hierarchical fashion.",
            "But then these are approximations to the true.",
            "K means objective function.",
            "So you again end up with having a lot of quantization errors.",
            "Although probabilistic images image retrieval models relying on Gaussian mixture models we have seen they were far superior from the bag of terms approaches.",
            "Also, the retrieval requires a linear scan through the collection of images, so you no longer have an efficient inverted index data structure and.",
            "The predictive density was always better than point estimates, indicating that there was overfitting and we show how we can identify also the number of components automatically from the data and an important thing is that the variational EM algorithm has the same complexity as the EM algorithm, so you get this increase."
        ],
        [
            "In performance, basically for free, that's a free lunch."
        ],
        [
            "And so future work.",
            "I think that for the bag of terms models there is a large literature, especially in the information retrieval community and.",
            "I think they have exhausted all the most of the possibilities of doing something more efficient than what we can do.",
            "Also, improving the performance of bag of their models relies on.",
            "Tweaking many different components of the algorithm, like the region detection, the feature description, the K means algorithm and then the weighting function and all this.",
            "When we get such a big increase in performance by avoiding the quantization step and modeling directly the density of the images, I think it's more appropriate to fix the problems of this approach.",
            "Like scalability problems and work from there so as future work, I think that.",
            "An interesting extension will be to just work on finding indexing structures for probabilistic models for image retrieval now.",
            "There is a way.",
            "Recently there was a paper in CPR I think, or ICE EVP.",
            "I think it was CPR conference on pattern recognition.",
            "So they apply locality sensitive hashing on kernel space is so you have a kernel function and you're allowed to have again hashing over Colonel spaces and that result into sub linear complexity, meaning that you don't have to go through a linear scale from the whole collection, but you just have to have just few elements in the collection that you scan and then you are able to rank the images in the collection using just this.",
            "The good thing with Kernelized locality sensitive hashing is that they also provide a theoretical guarantees of the approximation error.",
            "So you want to set the approximate nearest neighbor error to some value to some precision.",
            "You just have to increase the beams of the hashing, and that means increasing the kernel function dimensionality and so on.",
            "Now, general functions for probabilistic generative generative models.",
            "Now that's in order to use their locality sensitive hashing.",
            "We have to have kernel function between probability model probabilistic models.",
            "Now there are couple of approaches here, like Fisher kernels were basically use the gradients of the likelihood to construct camels or a more recent approach probability.",
            "Product kernels were basically the integrated likelihoods of the two models.",
            "I actually I say this is a future work.",
            "I cheated a little bit because in my thesis I have done a little experiment with some of this and especially the probability product camels using the mixture models approach.",
            "It seems to work quite good compared to Fisher kernels, but that's something not for here."
        ],
        [
            "I think that's it, and that's some ref."
        ],
        [
            "Sheesh.",
            "Thank you questions.",
            "Just to repeat the question, the question is here, we assume that the variational posterior factorizes between the parameters for the density, likelihood and the Latin variables, and the question is if this factorization can be explained into some intuitive way in terms of what happens to images.",
            "This is actually a technical requirement.",
            "I don't think there is any justification in terms of the images themselves, basically because the Gaussian mixture models is a general family of models is not only applicable to images or.",
            "Besides, it's a general model and the only reason why we have to assume basically the variational framework is also a general framework, so the fact that we do we choose this approximation is because it's convenient first of all, because the.",
            "It's convenient cause this has a nice form.",
            "When we factorize that and then we can.",
            "Calculate the lower bound analytically so it's just for the sake of simplicity, and the other reason why we choose this factorization is for variational inference.",
            "You can choose even the parametric for the value of the approximate procedure, so you can set Q. Theta will be normal, for example, and then optimize the parameters.",
            "But then you make very specific assumptions and you are not very sure how these assumptions will be captured.",
            "By the by the true posterior.",
            "So here by making just this the most simple of assumptions in order to get analytical solutions, you guarantee that you have an analytical result and you allow the forms of the variational posteriors to be as close to the true posteriors as possible because you don't specify them in advance.",
            "So basically the only approximation you do is just this conditional independence, that's it's.",
            "Solely for simplicity purposes.",
            "Nothing else.",
            "OK, so the question is if the choice for Gaussian densities is appropriate for images, or if it's the coupling of the features and the modeling that is successful and what happens if you use SIFT features or other types of features.",
            "In fact, the modeling do has to be according to what kind of features you use.",
            "So for example here the city features are very dense.",
            "And when you do, the K means algorithm.",
            "Basically you are.",
            "You rely on the Acadian distance, right?",
            "So that's a natural extension.",
            "The normal density over the K means algorithm.",
            "The question to why you choose the game exactly?",
            "Yeah no no yeah yeah yeah exactly.",
            "You are.",
            "Yeah, the game is algorithm was chosen basically as a as a benchmark because that's the state of the art in bug of terms and then as a natural extension of the over the bag of term model was to use a Gaussian mixture models and now as the discrete cosine transform is.",
            "It's a dense feature vector.",
            "The normal density was again, it was.",
            "Also you can you have I have to say it's also for matter of simplicity now.",
            "If it's appropriate for other types of features, such as shift, I think it's not appropriate for shift, because shifts are basically histograms of oriented gradients, which means that they are again a density.",
            "So if kleidion distances between distributions are not appropriate measures, so probably you have to use some other measure between these two histograms an.",
            "Chi squared distance, so you might have.",
            "A mixture of chi square distributions.",
            "Or you might need to have a mixture of multinomial distributions depending on the features.",
            "But yeah, the modeling you do has to be based on the features you observe.",
            "So the question is, if this A0 hyperparameter for the prior over the mixing coefficients by KA sensitive to the values you choose it because it enforces the sparsity or the dense dense representation of these mixture model.",
            "Well, my feeling from my experiments was that setting it to a relatively small value like.",
            "One into the power of minus three or minus four is is.",
            "I mean that's the setting I used and I always got the original balances now.",
            "The value of course has to do with how sparse your results will be.",
            "Now basically my intuition for that was that.",
            "I will over specify the model, so I will always use much more components that I actually need, and I will set the Alpha value in such a small number that I will get the sparsest solution possible.",
            "So that's the solution for estimating the number of components.",
            "But yeah, you're right.",
            "Depending on how many components you specify in advance, the value you set and the data you have.",
            "You might end up with different solutions, but it's reasonably robust in that.",
            "There are other ways, of course.",
            "You can put like, for example, a popular way to treat that in Markov chain Monte Carlo scheme is to put another prior on top of Alpha.",
            "And then in your sampling scheme, you integrate Alf out.",
            "So basically you allowed to rise or your estimates are even more robust.",
            "But that's you have to resort to MCMC for that.",
            "The thing is, the question is why I chose to present the results in a table and not in a Roc curves.",
            "Now first of all, for the information retrieval we don't use rockets, we use the equivalent, which is precision recall caps.",
            "Right now the precision recall curves.",
            "Tend to show the to highlight the.",
            "Systems that rank more relevant documents or images higher, but judging from that system is very difficult and in the community, especially in the track community, where is a exhaustive information retrieval systems are exhaustively evaluated.",
            "They tend to use M AP for that because judging different systems from curves is more difficult, and because they may be the integral over the region under the curve is a reasonable measure.",
            "That's the main reason why I choose to, and also, if you show if I show the curves like the first two, they overlap pretty much.",
            "Yeah, OK, yeah, probably had to show the right.",
            "Questions we thank speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so yeah I spent as Tom said my name is basilia.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm from the Department of Statistical Science of UCL.",
                    "label": 1
                },
                {
                    "sent": "This is work actually I was doing during my PhD.",
                    "label": 0
                },
                {
                    "sent": "Part of the work I was doing for my PhD and is with my supervisor in German, Josh.",
                    "label": 0
                },
                {
                    "sent": "From the Information Retrieval group in the University of Glasgow.",
                    "label": 0
                },
                {
                    "sent": "So I will be talking about Bayesian probabilistic models in the sense that we apply.",
                    "label": 0
                },
                {
                    "sent": "Bayesian inference for probabilistic models for image retrieval, and by immeasurable I mean ad hoc image retrieval, where the problem is we have a static collection of images and then the user supplies query match and we return a rank list of the static image collection.",
                    "label": 0
                },
                {
                    "sent": "To chat to other ranking more relevant images.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The top.",
                    "label": 0
                },
                {
                    "sent": "So that's a brief outline.",
                    "label": 0
                },
                {
                    "sent": "I will start with some background on bag of terms, image retrieval models, probabilistic models for information retrieval in general, and I will try to give the motivation for what we're doing in this work.",
                    "label": 1
                },
                {
                    "sent": "Then I will go into the more technical section, which is how we apply Bayesian inference methods for probabilistic models for information retrieval, and in particular image retrieval models.",
                    "label": 0
                },
                {
                    "sent": "And then I will show some experiments on a small collection image collection, which is has been extensively used in the field.",
                    "label": 0
                },
                {
                    "sent": "And then I will close with some points for discussion and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Future work so bug of terms.",
                    "label": 1
                },
                {
                    "sent": "Image retrieval is considered.",
                    "label": 1
                },
                {
                    "sent": "Now the state of the art for image retrieval travel systems and the key to their success was that they rely on representation, which is exactly the same as the one used for text retrieval for text documents.",
                    "label": 1
                },
                {
                    "sent": "So you convert the images into representation which where its image is represented by as a bug as a set an ordered set of parts.",
                    "label": 1
                },
                {
                    "sent": "And these parts can be part of objects.",
                    "label": 0
                },
                {
                    "sent": "So for example, if it's a face, it might be the nose.",
                    "label": 1
                },
                {
                    "sent": "There's the the mouth and so on.",
                    "label": 0
                },
                {
                    "sent": "Now, once you have this representation, then we can apply all the standard methods from information retrieval for waiting and ranking images and that provides us as an abstraction layer between the image domain and the information retrieval method.",
                    "label": 1
                },
                {
                    "sent": "So basically we can use out of the box retrieval system such as Lucene or Terrier or if you're familiar with this kind of retrieval systems, an index images instead of text.",
                    "label": 0
                },
                {
                    "sent": "So the bug of terms representation relies basically on three stages, which I explain briefly in the next slides.",
                    "label": 0
                },
                {
                    "sent": "It's region detection features, description and code block generation quantization.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the region detection algorithm basically is responsible for segmenting images into the parts into the components of the objects in the image.",
                    "label": 1
                },
                {
                    "sent": "The basic property of region detection algorithm should be repeatability and what I mean by the repeatability is that when we take the same scene, take a picture of the same scene and different lighting conditions or slight different perspective of the camera like rotation or tilt or something like that, the same.",
                    "label": 0
                },
                {
                    "sent": "Regions have to be.",
                    "label": 0
                },
                {
                    "sent": "Detected and that simplifies the retrieval algorithm because then we can match.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the face context, if we always extract regions from the nose, the mouth and the years it's easier to detect them later on.",
                    "label": 0
                },
                {
                    "sent": "So for this the most there are many ways to do that.",
                    "label": 0
                },
                {
                    "sent": "The most successful and popular in the literature are the interest point detectors, where basically they rely on some notion of curvature an.",
                    "label": 0
                },
                {
                    "sent": "By curvature we mean edges.",
                    "label": 0
                },
                {
                    "sent": "So basically they detect regions around.",
                    "label": 0
                },
                {
                    "sent": "Corners or blobs and features like that and you result in elliptical regions such as those in the second image.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with the interest point is that they rely on edges and for that, for example, regions such as the Sky here, where they have a homogeneous appearance, they are not represented.",
                    "label": 0
                },
                {
                    "sent": "Usually they don't generate any interest point, so we can capture them at all.",
                    "label": 0
                },
                {
                    "sent": "So another approach is to use image segmentation.",
                    "label": 0
                },
                {
                    "sent": "Algorithms like watershed or normalized cuts or just clustering of the pixels in terms of their color, and that tends to cluster the image, segment it into homogeneous regions.",
                    "label": 0
                },
                {
                    "sent": "The problem is that with this approach is that we can capture this kind of regions, but it's very sensitive to illumination changes like lighting conditions and perspective changes, which will result in different regions every time so.",
                    "label": 0
                },
                {
                    "sent": "Kind of balancing between is the first method which is just using a regular grid to segment the image into blocks.",
                    "label": 1
                },
                {
                    "sent": "Now this doesn't capture all the properties that we need, like, especially repeatability, but there are certain tricks we can do for that, like for example, we can start with a regular grid of 8 by 8 pixels and then have several groups of different resolution, so we can capture different scales like objects appearing in different distances from the camera.",
                    "label": 0
                },
                {
                    "sent": "And actually in these two papers aside there, the do an extensive study of different sampling strategies for region detection, and they found that actually regular grades of different resolution are one of the most successful methods for object recognition and image retrieval in general.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second stage just once we have extracted the small regions, is to describe them to extract some features out of them, for which we're going to index later.",
                    "label": 0
                },
                {
                    "sent": "So the most famous is the scale invariant feature transform, where basically from a small region we extract we have the edges and we extract the histogram of edge orientations.",
                    "label": 1
                },
                {
                    "sent": "So basically just a well it's for histograms, waited actually for four subblocks of the.",
                    "label": 0
                },
                {
                    "sent": "Inside the region and.",
                    "label": 0
                },
                {
                    "sent": "Results in a feature vector of 128 dimensions which counting how many times we found an edge with a particular direction.",
                    "label": 0
                },
                {
                    "sent": "Again, the problem with this approach is that we can capture color because we're based on edges.",
                    "label": 1
                },
                {
                    "sent": "There are extensions we can add, like the average color of the of the region in the feature vector.",
                    "label": 1
                },
                {
                    "sent": "But still as we rely on edges.",
                    "label": 0
                },
                {
                    "sent": "Problems with homogeneous regions such as skies or C or whatever, which doesn't have a lot of edges that can be described accurately.",
                    "label": 1
                },
                {
                    "sent": "So another approach which has been recently used in object recognition was to use the discrete cosine transform.",
                    "label": 0
                },
                {
                    "sent": "I think Ben said had used that in one dimensional domain, so in this example basically you represent an image region as a weighted sum of.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal basis functions now in this in the right hand right hand side image you can see like these are the basis functions for an 8 by 8 region and for each of these basis functions you have a real number and if you sum them altogether, weighted by these real numbers, you end up with an image.",
                    "label": 1
                },
                {
                    "sent": "This particular example is just an image with the letter A in a white background.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't really show, but that's the representation you have now.",
                    "label": 0
                },
                {
                    "sent": "The good thing with discrete cosine transform is that you can exploit the compression properties they have, it has, so you can basically represent each region with the feature vector, which is maybe about 70 dimensions, maybe less depending on the compression you choose to use.",
                    "label": 0
                },
                {
                    "sent": "Compared to the density to the SIFT features, it's a big.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reduction, so once you have the.",
                    "label": 0
                },
                {
                    "sent": "Regions and you have extracted features from the regions you have to create the visual vocabulary with.",
                    "label": 0
                },
                {
                    "sent": "That is the visual terms of parts that you are going to use to describe the image, and this is usually done by applying K means to cluster all the features extracted from the images.",
                    "label": 0
                },
                {
                    "sent": "Now once we obtain the clusters, the cluster means we treat them as words in vocabulary of text and so we assigned to each cluster mean.",
                    "label": 0
                },
                {
                    "sent": "A unique identifier.",
                    "label": 0
                },
                {
                    "sent": "Now, once we have that, we can just write everything into a text file and treat an image as text where words are the identifiers for the cluster means.",
                    "label": 0
                },
                {
                    "sent": "And yeah, of course we have to quantize its image by finding its image region, associating its image region to its closest visual term cluster in the vocabulary now.",
                    "label": 1
                },
                {
                    "sent": "Once you have this representation, the we can apply any information retrieval, waiting and ranking scheme.",
                    "label": 0
                },
                {
                    "sent": "So for example, Tom yesterday mentioned the vector space model with TF IDF weighting.",
                    "label": 0
                },
                {
                    "sent": "So in this setting we have a D which is an image or a document is a vector of elements Dita DT where each element DT lower case.",
                    "label": 1
                },
                {
                    "sent": "It's the weight or the importance of the term.",
                    "label": 0
                },
                {
                    "sent": "Of the theater that appears in this image now the TF IDF just associated to this element the number of times term T appears to D that's denoted with N lowercase DD and is multiplied by the idea of the inverse document frequency in the collection.",
                    "label": 0
                },
                {
                    "sent": "So we down weight terms which appear very frequently in the whole collection and using this representation.",
                    "label": 0
                },
                {
                    "sent": "Then as a measure of similarity, you can use the inner product.",
                    "label": 0
                },
                {
                    "sent": "Which is just the sum over the common terms that appear in both the document, the image and the query match.",
                    "label": 0
                },
                {
                    "sent": "If you normalize document vectors to have a unit length and this results into cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty standard.",
                    "label": 0
                },
                {
                    "sent": "Now the problem that comes with that is that the vector space model doesn't give us any guidance on how to select the weight.",
                    "label": 0
                },
                {
                    "sent": "Like here we use the TF IDF weighting and we don't have any insight on how to choose these weights, especially the idea formula is something that has been developed in the 70s.",
                    "label": 0
                },
                {
                    "sent": "It appeared firstly in the Journal of the American Association of Information Systems.",
                    "label": 0
                },
                {
                    "sent": "It's very old.",
                    "label": 0
                },
                {
                    "sent": "Generally they were using that in library science for indexing books.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A way to have a formal method for developing, waiting and ranking functions is to resort to the probabilistic modeling.",
                    "label": 1
                },
                {
                    "sent": "So in information retrieval there are two popular frameworks.",
                    "label": 1
                },
                {
                    "sent": "One is the probabilistic relevance framework.",
                    "label": 1
                },
                {
                    "sent": "Well, I give a reference from 2009, but actually this goes back to 80s.",
                    "label": 0
                },
                {
                    "sent": "The first instance of this, which basically the information retrieval problem, is handled as a classification problem.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "Collection and you have to classify given a query the whole collection into relevant and relevant but you are in a peculiar situation where you don't have any training set to train.",
                    "label": 0
                },
                {
                    "sent": "So you have to make a lot of assumptions.",
                    "label": 0
                },
                {
                    "sent": "Now the second approach, which is the one I'm using here is the language modeling approach which I will explain in the next.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slides, so the language modeling approach you assume that there is a generative process from where the documents are generated so generated, so there is a stochastic process and which is governed by parameters, which here we will denote with filter.",
                    "label": 0
                },
                {
                    "sent": "And each document is a random sample from this process, and so because it's document has its own process, the parameters Theta are indexed with a small D. The simplest model is to assume a multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "Basically, the parameters Theta turn out to be just the probability of each term in the document.",
                    "label": 1
                },
                {
                    "sent": "Write the distribution.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's necessary for here, just included for the sake of completeness.",
                    "label": 0
                },
                {
                    "sent": "Now, once we have the model, we can resort to.",
                    "label": 0
                },
                {
                    "sent": "Statistical literature and estimate the parameters so one.",
                    "label": 0
                },
                {
                    "sent": "Of the simplest way to estimate the parameters to take the maximum likelihood like estimate the parameters such as the likelihood, which is a faster manipulation.",
                    "label": 0
                },
                {
                    "sent": "Is maximized and that leads to simple counting.",
                    "label": 1
                },
                {
                    "sent": "Basically for this particular model, so the parameters are set to the number of times the term appears.",
                    "label": 0
                },
                {
                    "sent": "The document divided by the total terms in the document in the image.",
                    "label": 1
                },
                {
                    "sent": "For the particular case.",
                    "label": 0
                },
                {
                    "sent": "However, this can lead to overfitting because you can imagine that you have a large set of terms and a document can contain all the terms, so it will have terms with zero frequency and the probability of a term.",
                    "label": 0
                },
                {
                    "sent": "With a zero frequency, the maximum likelihood estimate will be 0 and cause in the first equation the likelihood you take the product over this, then any document with the term of zero frequency will have zero probability.",
                    "label": 0
                },
                {
                    "sent": "You will ever retrieve it.",
                    "label": 0
                },
                {
                    "sent": "A solution to that is to resort to smoothing, so add something some pseudo count at the term frequencies.",
                    "label": 0
                },
                {
                    "sent": "The theoretical implication of smoothing is actually.",
                    "label": 0
                },
                {
                    "sent": "Assuming a prior distribution of the over the model parameters, so we can express pretty much like, I don't have the reference here, but there is a nice paper that shows how you can express all the smoothing methods that have been proposed in the information retrieval literature in terms of.",
                    "label": 0
                },
                {
                    "sent": "A small thing using a different distribution.",
                    "label": 0
                },
                {
                    "sent": "So basically you assume that the parameters have a prior distribution, which is a direct which is parameterized by some other parameters which we call hyperparameters Alpha.",
                    "label": 1
                },
                {
                    "sent": "Now once we have the prior and the likelihood which is the first equation, we can estimate the maximum posteriori.",
                    "label": 0
                },
                {
                    "sent": "Point estimate for the parameters, which is actually.",
                    "label": 0
                },
                {
                    "sent": "The set of parameters that maximizes the joint likelihood.",
                    "label": 0
                },
                {
                    "sent": "In this case, it turns out to be very easy to do that, because it's just counting again, but you also add this hyperparameters Alpha from the digital distribution, and you discount minus one, so basically.",
                    "label": 0
                },
                {
                    "sent": "The most popular smoothing methods from IR can be recovered from this.",
                    "label": 1
                },
                {
                    "sent": "If you set these Alpha T to the average term frequency in the whole collection, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you use this for a ranking documents, you have to evaluate for the query match the query document or the query vector.",
                    "label": 0
                },
                {
                    "sent": "The likelihood under the model you have estimated with the parameters.",
                    "label": 0
                },
                {
                    "sent": "Right now the right hand side is the log of this an we have excluded all the terms that log 'cause it doesn't change.",
                    "label": 0
                },
                {
                    "sent": "The ranking is rank equivalent, so you can use it because it simplifies the calculation and also if you exclude terms which don't depend on the document, that just depends on the query.",
                    "label": 0
                },
                {
                    "sent": "Get exactly the same ranking so you can simplify it further.",
                    "label": 0
                },
                {
                    "sent": "So the interesting part is that the first sum is only depended on.",
                    "label": 0
                },
                {
                    "sent": "Terms where they exist like their frequency is non zero in the document and in the query which means that this allows for an efficient implementation.",
                    "label": 1
                },
                {
                    "sent": "You don't have to iterate through all the terms in the vocabulary for this query and you can see that the ranking function takes the form of a term the first time, which is in the log.",
                    "label": 0
                },
                {
                    "sent": "That is the term frequency.",
                    "label": 0
                },
                {
                    "sent": "The number of terms that number of times the terms appears in the document divided by this hyperparameters Alpha T which if you have set them to the.",
                    "label": 0
                },
                {
                    "sent": "Average time frequency of the term in the collection results into something similar to IDF ranking function, so you can see you can recover the TF IDF by resorting to completely probabilistic framework.",
                    "label": 1
                },
                {
                    "sent": "Now the implementation for this, as Tom said yesterday, is just inverted index data structure, which is basically has table of all the terms.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "For image retrieval.",
                    "label": 0
                },
                {
                    "sent": "We don't have like the bag of terms assumes that if you have quantized or limits features into terms discrete terms, the problem with quantization is that leads to quantization errors, and you can see especially the K means you can view it as a special case of the EM algorithm for.",
                    "label": 0
                },
                {
                    "sent": "Components of equal mixing weights and I just dropped covariance matrices, so a better way to handle that is to do directly to estimate directly the density of the features in its image, right?",
                    "label": 0
                },
                {
                    "sent": "And so another popular model is to handle directly like we don't do the quantization stage anymore, we just do the region detection and feature extraction and I will have its image as a collection of vectors in Rd.",
                    "label": 0
                },
                {
                    "sent": "Uh, now 40.",
                    "label": 0
                },
                {
                    "sent": "Too much we have again, a generative model which is governed by the parameter Citadis, which we take it to be a mixture of Gaussian distributions and the parameters set that they're actually the mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "Pi KUK and covariance matrices Sigma K. That's actually a method that has been used also for audio retrieval and music.",
                    "label": 0
                },
                {
                    "sent": "It's pretty general.",
                    "label": 0
                },
                {
                    "sent": "Depends on what kind of extraction you use for the features.",
                    "label": 0
                },
                {
                    "sent": "Now again, you can estimate the parameter spike, a Neuquen Sigma K with the EM algorithm for the mixture models, and once you have a query image you can rank the images in the collection by just using the log of the query likelihood under the estimated model parameters.",
                    "label": 1
                },
                {
                    "sent": "With this approach is that there is no efficient data structure for this because you have to iterate through all the collection of images in order to estimate the log likelihood for the query.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "What we really want to estimate is not the hood is the predictive density.",
                    "label": 1
                },
                {
                    "sent": "That is, we want to estimate the conditional probability of the feature vectors in an image in the query image given what we have observed from the.",
                    "label": 1
                },
                {
                    "sent": "Collection image the image in the collection.",
                    "label": 0
                },
                {
                    "sent": "We have the document D and.",
                    "label": 0
                },
                {
                    "sent": "The way to do that is that we have to take the integral of the likelihood of a new vector X given the parameter Theta, and we have to marginalized out at the integral using weighted by the posterior probability of the parameters given the observed document that we have now.",
                    "label": 1
                },
                {
                    "sent": "The maximum likelihood and maximum a posteriori estimates are in fact an approximation to this integral, and they rely on the asymptotic properties of the posterior.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lot of data, the posterior is going to be very sharply peaked around one point, which would be the maximum likelihood or the maximum posteriori point.",
                    "label": 0
                },
                {
                    "sent": "So because the density is normalized, the point is a good approximation for the integral.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that usually.",
                    "label": 0
                },
                {
                    "sent": "The document has more parameters than the observations we have.",
                    "label": 0
                },
                {
                    "sent": "So in the information retrieval case, the problem with a zero frequencies terms.",
                    "label": 0
                },
                {
                    "sent": "Because we don't, we can observe in the document all the terms same in the image.",
                    "label": 0
                },
                {
                    "sent": "Context you observe feature vectors, but you can't observe all possible feature vectors from all natural images.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Estimating this integral for the discretized case when we have quantized features, it turns out to be very easy because then take those are all calculated analytically.",
                    "label": 1
                },
                {
                    "sent": "The actual posterior is again a direct let where the parameters of the direct light is the term frequencies plus the hyperparameters of the prior, which are basically the collection frequencies.",
                    "label": 0
                },
                {
                    "sent": "And if you integrate that out with the query likelihood, you can get the log of the query given the document which has again similar form as.",
                    "label": 0
                },
                {
                    "sent": "The the previous location I showed before again.",
                    "label": 0
                },
                {
                    "sent": "The interesting part is that you can see that again it relies on terms which are non zero in the document in the query, which again it can implement it efficiently using an inverted index data structure.",
                    "label": 1
                },
                {
                    "sent": "The only difference is that you have these sums for each query term which come from the gamma functions.",
                    "label": 0
                },
                {
                    "sent": "Something more technical.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian mixture model, obtaining the posterior an.",
                    "label": 1
                },
                {
                    "sent": "As a result, the predicted densities is not possible to do it analytically anymore, so you have basically two options.",
                    "label": 0
                },
                {
                    "sent": "One is to use Markov chain Monte Carlo and the 2nd is to resort to some approximation.",
                    "label": 0
                },
                {
                    "sent": "Now there are pros and cons for both approaches like.",
                    "label": 0
                },
                {
                    "sent": "MCMC is asymptotically optimal.",
                    "label": 0
                },
                {
                    "sent": "The more samples you draw from the posterior, the better your estimate will be.",
                    "label": 0
                },
                {
                    "sent": "And there is actually theoretical guarantees of how many samples you need in order to achieve a particular accuracy.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is that we need several change, several market change to run for each document for each image in the collection.",
                    "label": 1
                },
                {
                    "sent": "In order to monitor the convergence, and be sure that you are sampling points from the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And once you have the points, you have to store all the sample points.",
                    "label": 1
                },
                {
                    "sent": "The posterior samples in your indexing structure, because when you want to evaluate the predictive density for a new query, you have to.",
                    "label": 0
                },
                {
                    "sent": "Calculate the weighted sum of these points.",
                    "label": 1
                },
                {
                    "sent": "Another approach which is more suitable for systems like information retrieval systems and image retrieval systems, is to use an approximation which we use a variational approximation which I will describe later.",
                    "label": 0
                },
                {
                    "sent": "That gives us a local approximation of the posterior.",
                    "label": 0
                },
                {
                    "sent": "And this results in our analytic results.",
                    "label": 0
                },
                {
                    "sent": "So we have a formula which can we can directly evaluate and we don't have to resort in in any sort of numerical estimate by weighted average of points.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I won't describe fully the variational inference method.",
                    "label": 1
                },
                {
                    "sent": "Just give a sketch about it.",
                    "label": 0
                },
                {
                    "sent": "So basically we have to start with a.",
                    "label": 0
                },
                {
                    "sent": "An equivalent expression for the mixture model where we just include the Latin variables Zetton K. So basically in the previous slide.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that there is a sum over the number of components.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, by including the Latin variables NK, this sum becomes a product because Latin variables at 10K there are binary, and if it's one then the sample N is associated with the K component so that.",
                    "label": 0
                },
                {
                    "sent": "Transforms this into a product form.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you take the issue marginalized out the Latin variables by taking the sum over all possible combinations, you get back to the original expression.",
                    "label": 0
                },
                {
                    "sent": "Now, once we have this convenient form, we can start associating priors with all the parameters.",
                    "label": 0
                },
                {
                    "sent": "So for the so.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty quick city of the variational method.",
                    "label": 0
                },
                {
                    "sent": "In order to be able to get analytical expressions, we have to get priors which result into analytical solutions.",
                    "label": 0
                },
                {
                    "sent": "So we have to use conjugate priors so the conjugate prior for the.",
                    "label": 1
                },
                {
                    "sent": "Make sure coefficients Pi is again a dicklet where we have a hyperparameter A0.",
                    "label": 0
                },
                {
                    "sent": "Now if we set Alpha to one, this results in the uniform prior.",
                    "label": 0
                },
                {
                    "sent": "This means that all pies are priority have equal likelihood.",
                    "label": 0
                },
                {
                    "sent": "Are considered the same.",
                    "label": 0
                },
                {
                    "sent": "Now if we set a 0 to a small value that favors by case which have small value as well, so we're resorting to sparse solutions like if we have a mixture model where K is large, like 100, and we have a prior on Pi switches.",
                    "label": 1
                },
                {
                    "sent": "0.0031 then we might end up only with 50 or 20 components having a non zero mixing coefficient.",
                    "label": 0
                },
                {
                    "sent": "Now for the means, we can use a prior which is normal and it's based on a hyper parameter mu 0M zero and better where we can set M 0 to be at the center of the observed.",
                    "label": 0
                },
                {
                    "sent": "The center of the observed feature space.",
                    "label": 1
                },
                {
                    "sent": "So we take all the features in the collection.",
                    "label": 0
                },
                {
                    "sent": "Whoever reads them, and we use that M. The reason why we do that is because we want to have a regular pretty much flat prior in the region where we have the observations.",
                    "label": 0
                },
                {
                    "sent": "So basically we assume that everything that we haven't observed it has.",
                    "label": 0
                },
                {
                    "sent": "Very low probability and things that we have observed are pretty much equal a priority.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Now for the prior over the.",
                    "label": 0
                },
                {
                    "sent": "Positive definite covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "We have to use an inverse Wishart prior.",
                    "label": 1
                },
                {
                    "sent": "This is slightly technical, but you can take it as a prior distribution over matrices.",
                    "label": 0
                },
                {
                    "sent": "Again, the hyperparameters WO and V0W0 is a precision matrix which we can set it to the inverse of the covariance over all the features we have in the collection.",
                    "label": 0
                },
                {
                    "sent": "Again to give collection statistics for the prior and visit V0 is the degrees of freedom for the inverse.",
                    "label": 1
                },
                {
                    "sent": "Which again set it into some high value in order to result into a flat prior in high likelihood regions.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, once we have this.",
                    "label": 0
                },
                {
                    "sent": "We can augment the parameters Theta D with the Latin variable set and get this new augmented parameter space capital Theta, and now we have to make some approximations about the posterior of pithy to capital Theta.",
                    "label": 0
                },
                {
                    "sent": "Given the observed document.",
                    "label": 0
                },
                {
                    "sent": "Now the only approximation we have to consider is that this posterior approximate posterior which we call Q Theta, Capitol Theatre.",
                    "label": 0
                },
                {
                    "sent": "It factorizes between the two.",
                    "label": 0
                },
                {
                    "sent": "Meaning that the parameter Citadis and the Latin verb set are independent with each other.",
                    "label": 0
                },
                {
                    "sent": "That's an approximation.",
                    "label": 0
                },
                {
                    "sent": "That's the only approximation we have to make.",
                    "label": 0
                },
                {
                    "sent": "We don't make any other approximation about the form the particular forms of the posterior, so the parameters are.",
                    "label": 0
                },
                {
                    "sent": "Everything will come out from that.",
                    "label": 0
                },
                {
                    "sent": "Now, once we have that, we can use the same trick that we use when we derive the EM algorithm that is using the Jensen's inequality to write the marginal.",
                    "label": 0
                },
                {
                    "sent": "Likelihood the probability of the document will have marginalized out all the parameters using the prior as these two terms.",
                    "label": 0
                },
                {
                    "sent": "These two integrals now the first, the second integral.",
                    "label": 0
                },
                {
                    "sent": "You can see, that is the KL diversions between the approximate posterior Q and the true posterior Beefeater given D, right?",
                    "label": 1
                },
                {
                    "sent": "So if are true posterior if I are approximate posterior approaches the true posterior.",
                    "label": 0
                },
                {
                    "sent": "This term will start to become zero.",
                    "label": 0
                },
                {
                    "sent": "Then on the left hand side of the equation they the first step is the lower bound and the interesting thing with that is that this this term is, we can compute it analytically because Q, Theta we have specified it in a convenient form because we know it factorizes and we can compute it exactly.",
                    "label": 0
                },
                {
                    "sent": "PD, Theta is a joint which we have it from the joint log likelihood and Q Theta is again something that.",
                    "label": 0
                },
                {
                    "sent": "That we know.",
                    "label": 0
                },
                {
                    "sent": "So we have something that we can calculate, and if we maximize this lower lower bound we can see that the KL divergent will be minimized.",
                    "label": 0
                },
                {
                    "sent": "So it's equivalent.",
                    "label": 0
                },
                {
                    "sent": "Instead of Max minimizing the KL divergent, which we can't because we don't have the true posterior P, Theta given D, we can maximize the lower bound and that equivalent equivalent to minimizing KL.",
                    "label": 1
                },
                {
                    "sent": "The way to do that is, again, slightly technical.",
                    "label": 0
                },
                {
                    "sent": "Basically, we take its term in the factorized posterior that we assume that the Q, Theta is equal to the product of custody and queues at typo that's accused here.",
                    "label": 1
                },
                {
                    "sent": "And we take it step in dependently we we plug it in into the lower bound to creation.",
                    "label": 0
                },
                {
                    "sent": "We consider all other terms to be constant and then we get this expression at the bottom which is basically an expectation over the factory variational posteriors for each pyramid.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's a pretty standard algorithm actually.",
                    "label": 0
                },
                {
                    "sent": "You can find it in Bishop's book in the 7th chapter you end up with variational posteriors, which are exact approximate, but they have an analytic solution.",
                    "label": 0
                },
                {
                    "sent": "That's what I mean exact.",
                    "label": 0
                },
                {
                    "sent": "So the posterior variational posterior for the Latin variables is just a multinomial with parameters are the variational posterior for the mixing coefficients is difficult with hyperparameters Alpha.",
                    "label": 1
                },
                {
                    "sent": "And the variational posterior for mu, the means and the covariance matrices is a inverse Wishart distribution with parameters M Betta WMV.",
                    "label": 0
                },
                {
                    "sent": "Now all these parameters like R, Alpha, MWK, NVC are estimated by an equivalent algorithm like the M, which is the variational em and it works in the same way.",
                    "label": 0
                },
                {
                    "sent": "It first estimates the expectation for the zed variables and then it updates maximizes.",
                    "label": 1
                },
                {
                    "sent": "The variational posted parameters WM and Alpha.",
                    "label": 1
                },
                {
                    "sent": "The nice thing is that with that is that once we have the exact forms for the variational posterius, we can plug them in and we can estimate the predictive density directly, because it's just we can integrate.",
                    "label": 0
                },
                {
                    "sent": "Take all this integral analytically, 'cause if you notice, they're just expectations under standard distribution.",
                    "label": 0
                },
                {
                    "sent": "So for example, that something that is easy to see that we know that QP variational posterior for the mixing coefficients easily clipped, and that's the function.",
                    "label": 0
                },
                {
                    "sent": "That we want to integrate PK.",
                    "label": 0
                },
                {
                    "sent": "All other terms we can put them outside and that is.",
                    "label": 0
                },
                {
                    "sent": "We know that the expectation of PK under directly is just the parameters Alpha K divided by the sum of Alpha K. So the same holds and for the rest of the parameters and we end up with a. Mastication, which is basically a mixture again but not of normal densities about of student densities, now the implication is that the students have longer tails, so they capture outliers better in terms of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in contrast to normal density.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now yesterday Thomas mentioned the problem of finding the number of components in when you do clustering, and here mixture models are similar to clustering.",
                    "label": 1
                },
                {
                    "sent": "Now when you resort to this variational framework and in general Bayesian inference you can.",
                    "label": 0
                },
                {
                    "sent": "For example, for the for this particular application, you can estimate get an estimate of the number of components by using the properties of the of the posteriors you end up.",
                    "label": 1
                },
                {
                    "sent": "So if you from the variational posterior for the mixing coefficients, which is a direct let, we know that the expectation for its of the mixing coefficients is just the parameter Alpha K divided by the sum of all alphas and the variances again depended on this Alpha K parameters now.",
                    "label": 1
                },
                {
                    "sent": "In the variational EM algorithm, when we update this Alpha K parameters, we just use these situations that end up.",
                    "label": 0
                },
                {
                    "sent": "That's that comes out from the variational approximation framework, right?",
                    "label": 0
                },
                {
                    "sent": "So A0 is the prior hyperparameter we set and this is the prior hyperparameter plus the sum of these variables, which are basically the expectations of the Latin variables.",
                    "label": 0
                },
                {
                    "sent": "Now you can see that if you set A0 to a small value which result ends up in sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Right when.",
                    "label": 0
                },
                {
                    "sent": "You don't have any points contributed to one component.",
                    "label": 0
                },
                {
                    "sent": "Then the Alpha K will be small and cause that will be.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if you go back you can see that the predicted density.",
                    "label": 0
                },
                {
                    "sent": "Will also be small for that particular component, so you can neglect it completely.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's also the mass.",
                    "label": 0
                },
                {
                    "sent": "Similar method is used in a paper by Bishop Anne Condo.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2001 now yeah.",
                    "label": 0
                },
                {
                    "sent": "To test this for models in total for different approaches.",
                    "label": 0
                },
                {
                    "sent": "For ranking images, we use the Coral 5K collection, which is a collection of 5000 images in total, divided into 4500 images, which are we consider US training images is a static collection and 500 test images which we will consider as queries.",
                    "label": 0
                },
                {
                    "sent": "Now the collection is divided into 50 categories in the training set there are 19 messages from each category.",
                    "label": 1
                },
                {
                    "sent": "And in the training set, the 500 images there are 10 images from each category, so we treat each image as a query from this 50 categories and given one image, we expect the 90 images in the static collection to be ranked 1st, and that's how we evaluate the retrieval performance.",
                    "label": 1
                },
                {
                    "sent": "That's just a.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other way of evaluating information retrieval algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the preprocessing we do in the collection is fast.",
                    "label": 0
                },
                {
                    "sent": "We convert the images into the luminance and chrominance channels, chrominance color space.",
                    "label": 0
                },
                {
                    "sent": "We segment the images using a regular grid of 8 by 8 pixels an we use also sliding window, so we overlap every four pixels.",
                    "label": 1
                },
                {
                    "sent": "We apply the DCT transform from each region.",
                    "label": 0
                },
                {
                    "sent": "And we I don't mention it here.",
                    "label": 0
                },
                {
                    "sent": "From the density coefficients, we keep only the the ones from the luminance channels and only the first 2 coefficient from the prominent style.",
                    "label": 0
                },
                {
                    "sent": "This is again slightly technical, but it has to do also with the fact that this is the same transformation is used for JPEG images.",
                    "label": 0
                },
                {
                    "sent": "So basically you can take it directly the JPEG from the file without decoding it and use this algorithm index images.",
                    "label": 0
                },
                {
                    "sent": "And now for the bug of terms representation.",
                    "label": 1
                },
                {
                    "sent": "We use K means an.",
                    "label": 0
                },
                {
                    "sent": "We use 2000.",
                    "label": 1
                },
                {
                    "sent": "To cluster the features, the feature vectors into 2000 clusters.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian mixture model we run the EM algorithm and we use the state components.",
                    "label": 0
                },
                {
                    "sent": "The reason why we fix this number 28 is because previous study with the same model with maximum likelihood estimates and the EM algorithm have showed that.",
                    "label": 0
                },
                {
                    "sent": "8 components was the best actually I will show why that's the case later from the analysis I'm doing in the results Now 4.",
                    "label": 0
                },
                {
                    "sent": "For the variational EM algorithm where we try to compute the predicted densities, we set the.",
                    "label": 0
                },
                {
                    "sent": "The number of components to 40 and we remove components at the end as a post processing step.",
                    "label": 1
                },
                {
                    "sent": "With the method I described later, based on the posteriors of the mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "Now in all the M and variational EM algorithms, we initialize them by randomly associating the Latin variables.",
                    "label": 0
                },
                {
                    "sent": "That's again a technical issue, but it's important because some people use K means to initialize Umm, but it's not fair for the variational em because you cannot use the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we decided to.",
                    "label": 0
                },
                {
                    "sent": "A simple initialization strategy to be fair in comp.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The two approaches.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if you're familiar with valuation measures.",
                    "label": 0
                },
                {
                    "sent": "For information retrieval, you want me to go through that a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically.",
                    "label": 0
                },
                {
                    "sent": "Information retrieval measure precision and recall recall is the number of so we retrieve a set of documents and recall is the number of relevant documents retrieved.",
                    "label": 0
                },
                {
                    "sent": "So how many documents will retrieve relevant to the query?",
                    "label": 0
                },
                {
                    "sent": "We judge if they're relevant from the training collection we have, right?",
                    "label": 0
                },
                {
                    "sent": "Now the precision measures how accurate this retrieval is from the number, which means how many of the retrieved set number of relevant documents retrieved were actually relevant.",
                    "label": 0
                },
                {
                    "sent": "So it's similar to the accuracy in the classification.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with these measures with precision and recall, which are similar to true positive and false positive rate in an accuracy in classification, is that they don't take into account the ranking and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "We're interested about the ranking because.",
                    "label": 0
                },
                {
                    "sent": "If we have a large collection like here you have 5 five 5000 images and we rank.",
                    "label": 0
                },
                {
                    "sent": "The most relevant images at the bottom, the users probably will never see them, so we have to have measures that take into account also the ranking.",
                    "label": 0
                },
                {
                    "sent": "So the easiest measure is this P at 5:00 PM, ten PM 20, which basically is a precision at the 1st five top ranked relevant documents.",
                    "label": 0
                },
                {
                    "sent": "Precision attend is the same.",
                    "label": 0
                },
                {
                    "sent": "The precision evaluated at the first 10 relevant documents, Precision 20 at the 1st 20 and so on.",
                    "label": 0
                },
                {
                    "sent": "Our precision is the precision after we have seen all the relevant documents in the collection.",
                    "label": 0
                },
                {
                    "sent": "Or in the ranking list.",
                    "label": 0
                },
                {
                    "sent": "So we go down the ranking list and whenever we find the last relevant document.",
                    "label": 0
                },
                {
                    "sent": "In the ranking list we measure the precision there and mean average precision is the fact that you can express precision as a function of recall.",
                    "label": 0
                },
                {
                    "sent": "So you integrate out is the average overall record values.",
                    "label": 0
                },
                {
                    "sent": "It's similar to the integrated the area under the Roc curve, but when you have precision, recall.",
                    "label": 0
                },
                {
                    "sent": "So the first method bought map is a bag of terms approach.",
                    "label": 0
                },
                {
                    "sent": "We're using maximum posteriori estimate.",
                    "label": 0
                },
                {
                    "sent": "The second method is same bag of their models where we use a predictive density where we take the marginalization, we using the posteriors, you see there is a.",
                    "label": 0
                },
                {
                    "sent": "A difference, an increase in performance across all measures results are not statistically significant for that so.",
                    "label": 0
                },
                {
                    "sent": "Now for the Gaussian mixture models, you immediately see that if you avoid the quantization step that is running.",
                    "label": 0
                },
                {
                    "sent": "K means and model directly the density of continuous image features with mixture models.",
                    "label": 0
                },
                {
                    "sent": "You gain a significant increase in performance.",
                    "label": 0
                },
                {
                    "sent": "Then that's with a maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "If you use a maximum posteriori estimate, you can again see an increase in performance.",
                    "label": 0
                },
                {
                    "sent": "That's the fact that you have integrated also the prior.",
                    "label": 0
                },
                {
                    "sent": "Calculated also the prior in the maximization and if you use the predictive density where we use the variational EM algorithm to estimate the approximation of the posteriors and then integrate them out, you see that you get the statistical significant improvement overall results.",
                    "label": 0
                },
                {
                    "sent": "Now the statistical significance is estimated from the previous row because so this is statistically significant from the previous row.",
                    "label": 0
                },
                {
                    "sent": "And so one.",
                    "label": 0
                },
                {
                    "sent": "Now I said.",
                    "label": 0
                },
                {
                    "sent": "That we found out why this 8.",
                    "label": 0
                },
                {
                    "sent": "When we were there, George of the Gaussian mixture model for the maximum likelihood Emel Nmap we said I said that we said the email EM algorithm too.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "8 components Now with the variationally amalgam.",
                    "label": 0
                },
                {
                    "sent": "Would we were able to estimate the number of components for its images directly from the output of the algorithm by pruning out unused components.",
                    "label": 1
                },
                {
                    "sent": "Now if you plot this for the whole training collection, you get this nice distribution.",
                    "label": 0
                },
                {
                    "sent": "And you see that the mean is 8, so the results from that paper that they were arguing that eight from all their experiments was in fact they had results from other values around 8:00 and they were pretty much similar and can be justified because the mean is 8.",
                    "label": 0
                },
                {
                    "sent": "So taking them in is a good strategy of course, for this particular collection the mean was eight the easy too, but it's not easy to real in a real setting to do exhaustive results to find what is the number of components.",
                    "label": 0
                },
                {
                    "sent": "So by actually allowing the number of components to come out in some automatic way, not only you can tune that parameter automatically, but you can also have allowed this number to change for its image because.",
                    "label": 0
                },
                {
                    "sent": "In the EM algorithm, you said this eight and static for the whole collection, which means that complicated images with complicated structures get under estimated while simplified images with simple structure they get overestimated because they use more components while you allow.",
                    "label": 1
                },
                {
                    "sent": "When you allow this number of components to alter between images, that's why.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually you are.",
                    "label": 0
                },
                {
                    "sent": "You get this significant increase in performance.",
                    "label": 0
                },
                {
                    "sent": "Over the maximum likelihood estimates.",
                    "label": 0
                },
                {
                    "sent": "The other point I can make from this is that.",
                    "label": 0
                },
                {
                    "sent": "Since the.",
                    "label": 0
                },
                {
                    "sent": "There is a statistical significant increase between the maximum a posteriori like point estimates and the predictive density.",
                    "label": 0
                },
                {
                    "sent": "This also highlights some sort of overfitting that all the point estimates in fact are overfitting, and that's why it's important to take into account the posterior distribution when you're making.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addictions.",
                    "label": 0
                },
                {
                    "sent": "Now conclusions I didn't spoke about that, but from the experiments what we have seen is that I said that the bug of their models are efficient because you have sparse representation.",
                    "label": 0
                },
                {
                    "sent": "That's true, but running a K means in a collection of 1,000,000 feature vectors to cluster them into 2000 clusters is not very efficient, and it's I found it to be an engineering difficult engineering task to code up VM algorithm.",
                    "label": 0
                },
                {
                    "sent": "To work.",
                    "label": 0
                },
                {
                    "sent": "So there are alternative to that to K means you can use DB scan or hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "Or even apply K means and hierarchical fashion.",
                    "label": 0
                },
                {
                    "sent": "But then these are approximations to the true.",
                    "label": 0
                },
                {
                    "sent": "K means objective function.",
                    "label": 0
                },
                {
                    "sent": "So you again end up with having a lot of quantization errors.",
                    "label": 0
                },
                {
                    "sent": "Although probabilistic images image retrieval models relying on Gaussian mixture models we have seen they were far superior from the bag of terms approaches.",
                    "label": 1
                },
                {
                    "sent": "Also, the retrieval requires a linear scan through the collection of images, so you no longer have an efficient inverted index data structure and.",
                    "label": 1
                },
                {
                    "sent": "The predictive density was always better than point estimates, indicating that there was overfitting and we show how we can identify also the number of components automatically from the data and an important thing is that the variational EM algorithm has the same complexity as the EM algorithm, so you get this increase.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In performance, basically for free, that's a free lunch.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so future work.",
                    "label": 0
                },
                {
                    "sent": "I think that for the bag of terms models there is a large literature, especially in the information retrieval community and.",
                    "label": 0
                },
                {
                    "sent": "I think they have exhausted all the most of the possibilities of doing something more efficient than what we can do.",
                    "label": 0
                },
                {
                    "sent": "Also, improving the performance of bag of their models relies on.",
                    "label": 0
                },
                {
                    "sent": "Tweaking many different components of the algorithm, like the region detection, the feature description, the K means algorithm and then the weighting function and all this.",
                    "label": 0
                },
                {
                    "sent": "When we get such a big increase in performance by avoiding the quantization step and modeling directly the density of the images, I think it's more appropriate to fix the problems of this approach.",
                    "label": 0
                },
                {
                    "sent": "Like scalability problems and work from there so as future work, I think that.",
                    "label": 0
                },
                {
                    "sent": "An interesting extension will be to just work on finding indexing structures for probabilistic models for image retrieval now.",
                    "label": 0
                },
                {
                    "sent": "There is a way.",
                    "label": 0
                },
                {
                    "sent": "Recently there was a paper in CPR I think, or ICE EVP.",
                    "label": 0
                },
                {
                    "sent": "I think it was CPR conference on pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "So they apply locality sensitive hashing on kernel space is so you have a kernel function and you're allowed to have again hashing over Colonel spaces and that result into sub linear complexity, meaning that you don't have to go through a linear scale from the whole collection, but you just have to have just few elements in the collection that you scan and then you are able to rank the images in the collection using just this.",
                    "label": 0
                },
                {
                    "sent": "The good thing with Kernelized locality sensitive hashing is that they also provide a theoretical guarantees of the approximation error.",
                    "label": 0
                },
                {
                    "sent": "So you want to set the approximate nearest neighbor error to some value to some precision.",
                    "label": 0
                },
                {
                    "sent": "You just have to increase the beams of the hashing, and that means increasing the kernel function dimensionality and so on.",
                    "label": 0
                },
                {
                    "sent": "Now, general functions for probabilistic generative generative models.",
                    "label": 1
                },
                {
                    "sent": "Now that's in order to use their locality sensitive hashing.",
                    "label": 0
                },
                {
                    "sent": "We have to have kernel function between probability model probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "Now there are couple of approaches here, like Fisher kernels were basically use the gradients of the likelihood to construct camels or a more recent approach probability.",
                    "label": 1
                },
                {
                    "sent": "Product kernels were basically the integrated likelihoods of the two models.",
                    "label": 0
                },
                {
                    "sent": "I actually I say this is a future work.",
                    "label": 0
                },
                {
                    "sent": "I cheated a little bit because in my thesis I have done a little experiment with some of this and especially the probability product camels using the mixture models approach.",
                    "label": 0
                },
                {
                    "sent": "It seems to work quite good compared to Fisher kernels, but that's something not for here.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that's it, and that's some ref.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sheesh.",
                    "label": 0
                },
                {
                    "sent": "Thank you questions.",
                    "label": 0
                },
                {
                    "sent": "Just to repeat the question, the question is here, we assume that the variational posterior factorizes between the parameters for the density, likelihood and the Latin variables, and the question is if this factorization can be explained into some intuitive way in terms of what happens to images.",
                    "label": 0
                },
                {
                    "sent": "This is actually a technical requirement.",
                    "label": 0
                },
                {
                    "sent": "I don't think there is any justification in terms of the images themselves, basically because the Gaussian mixture models is a general family of models is not only applicable to images or.",
                    "label": 0
                },
                {
                    "sent": "Besides, it's a general model and the only reason why we have to assume basically the variational framework is also a general framework, so the fact that we do we choose this approximation is because it's convenient first of all, because the.",
                    "label": 0
                },
                {
                    "sent": "It's convenient cause this has a nice form.",
                    "label": 0
                },
                {
                    "sent": "When we factorize that and then we can.",
                    "label": 0
                },
                {
                    "sent": "Calculate the lower bound analytically so it's just for the sake of simplicity, and the other reason why we choose this factorization is for variational inference.",
                    "label": 0
                },
                {
                    "sent": "You can choose even the parametric for the value of the approximate procedure, so you can set Q. Theta will be normal, for example, and then optimize the parameters.",
                    "label": 0
                },
                {
                    "sent": "But then you make very specific assumptions and you are not very sure how these assumptions will be captured.",
                    "label": 0
                },
                {
                    "sent": "By the by the true posterior.",
                    "label": 0
                },
                {
                    "sent": "So here by making just this the most simple of assumptions in order to get analytical solutions, you guarantee that you have an analytical result and you allow the forms of the variational posteriors to be as close to the true posteriors as possible because you don't specify them in advance.",
                    "label": 0
                },
                {
                    "sent": "So basically the only approximation you do is just this conditional independence, that's it's.",
                    "label": 0
                },
                {
                    "sent": "Solely for simplicity purposes.",
                    "label": 0
                },
                {
                    "sent": "Nothing else.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is if the choice for Gaussian densities is appropriate for images, or if it's the coupling of the features and the modeling that is successful and what happens if you use SIFT features or other types of features.",
                    "label": 0
                },
                {
                    "sent": "In fact, the modeling do has to be according to what kind of features you use.",
                    "label": 0
                },
                {
                    "sent": "So for example here the city features are very dense.",
                    "label": 0
                },
                {
                    "sent": "And when you do, the K means algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically you are.",
                    "label": 0
                },
                {
                    "sent": "You rely on the Acadian distance, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a natural extension.",
                    "label": 0
                },
                {
                    "sent": "The normal density over the K means algorithm.",
                    "label": 0
                },
                {
                    "sent": "The question to why you choose the game exactly?",
                    "label": 0
                },
                {
                    "sent": "Yeah no no yeah yeah yeah exactly.",
                    "label": 0
                },
                {
                    "sent": "You are.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the game is algorithm was chosen basically as a as a benchmark because that's the state of the art in bug of terms and then as a natural extension of the over the bag of term model was to use a Gaussian mixture models and now as the discrete cosine transform is.",
                    "label": 0
                },
                {
                    "sent": "It's a dense feature vector.",
                    "label": 0
                },
                {
                    "sent": "The normal density was again, it was.",
                    "label": 0
                },
                {
                    "sent": "Also you can you have I have to say it's also for matter of simplicity now.",
                    "label": 0
                },
                {
                    "sent": "If it's appropriate for other types of features, such as shift, I think it's not appropriate for shift, because shifts are basically histograms of oriented gradients, which means that they are again a density.",
                    "label": 0
                },
                {
                    "sent": "So if kleidion distances between distributions are not appropriate measures, so probably you have to use some other measure between these two histograms an.",
                    "label": 0
                },
                {
                    "sent": "Chi squared distance, so you might have.",
                    "label": 0
                },
                {
                    "sent": "A mixture of chi square distributions.",
                    "label": 0
                },
                {
                    "sent": "Or you might need to have a mixture of multinomial distributions depending on the features.",
                    "label": 0
                },
                {
                    "sent": "But yeah, the modeling you do has to be based on the features you observe.",
                    "label": 0
                },
                {
                    "sent": "So the question is, if this A0 hyperparameter for the prior over the mixing coefficients by KA sensitive to the values you choose it because it enforces the sparsity or the dense dense representation of these mixture model.",
                    "label": 0
                },
                {
                    "sent": "Well, my feeling from my experiments was that setting it to a relatively small value like.",
                    "label": 0
                },
                {
                    "sent": "One into the power of minus three or minus four is is.",
                    "label": 0
                },
                {
                    "sent": "I mean that's the setting I used and I always got the original balances now.",
                    "label": 0
                },
                {
                    "sent": "The value of course has to do with how sparse your results will be.",
                    "label": 0
                },
                {
                    "sent": "Now basically my intuition for that was that.",
                    "label": 0
                },
                {
                    "sent": "I will over specify the model, so I will always use much more components that I actually need, and I will set the Alpha value in such a small number that I will get the sparsest solution possible.",
                    "label": 0
                },
                {
                    "sent": "So that's the solution for estimating the number of components.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "Depending on how many components you specify in advance, the value you set and the data you have.",
                    "label": 0
                },
                {
                    "sent": "You might end up with different solutions, but it's reasonably robust in that.",
                    "label": 0
                },
                {
                    "sent": "There are other ways, of course.",
                    "label": 0
                },
                {
                    "sent": "You can put like, for example, a popular way to treat that in Markov chain Monte Carlo scheme is to put another prior on top of Alpha.",
                    "label": 0
                },
                {
                    "sent": "And then in your sampling scheme, you integrate Alf out.",
                    "label": 0
                },
                {
                    "sent": "So basically you allowed to rise or your estimates are even more robust.",
                    "label": 0
                },
                {
                    "sent": "But that's you have to resort to MCMC for that.",
                    "label": 0
                },
                {
                    "sent": "The thing is, the question is why I chose to present the results in a table and not in a Roc curves.",
                    "label": 0
                },
                {
                    "sent": "Now first of all, for the information retrieval we don't use rockets, we use the equivalent, which is precision recall caps.",
                    "label": 0
                },
                {
                    "sent": "Right now the precision recall curves.",
                    "label": 0
                },
                {
                    "sent": "Tend to show the to highlight the.",
                    "label": 0
                },
                {
                    "sent": "Systems that rank more relevant documents or images higher, but judging from that system is very difficult and in the community, especially in the track community, where is a exhaustive information retrieval systems are exhaustively evaluated.",
                    "label": 0
                },
                {
                    "sent": "They tend to use M AP for that because judging different systems from curves is more difficult, and because they may be the integral over the region under the curve is a reasonable measure.",
                    "label": 0
                },
                {
                    "sent": "That's the main reason why I choose to, and also, if you show if I show the curves like the first two, they overlap pretty much.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yeah, probably had to show the right.",
                    "label": 0
                },
                {
                    "sent": "Questions we thank speaker again.",
                    "label": 0
                }
            ]
        }
    }
}