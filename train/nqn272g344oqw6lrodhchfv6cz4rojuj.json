{
    "id": "nqn272g344oqw6lrodhchfv6cz4rojuj",
    "title": "Learning languages from bounded resources: the case of the DFA and the balls of strings",
    "info": {
        "author": [
            "Colin de la Higuera, LINA, University of Nantes"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/icgi08_higuera_llbr/",
    "segmentation": [
        [
            "Given by.",
            "And.",
            "The title is still learning languages from boundary sources.",
            "The case of the DFA and a bowl of strings.",
            "OK, can you hear me?",
            "Stock price.",
            "Right, but I'm very happy to be here.",
            "I'm sad about one thing I can't find my watch so I'll have to rely on PF saying 5 minutes and things like that.",
            "So the talk is a theoretical talk to start with and we're just looking or revisiting the typical particles of language learning and grammatical inference.",
            "And instead of visiting them purely abstractly, we're concentrating on two classes of languages, one which is well known, which is the terministic finite state automata and the other one is the balls of strings.",
            "On this other slide down at the."
        ],
        [
            "Bottom, There are my authors who are all here.",
            "So if you've got questions, you can ask them just as much as asked me.",
            "Come in front.",
            "No."
        ],
        [
            "Good OK, so the outline of the talk what and why?",
            "Why are we doing theory and what sort of theory describes the balls in automata?",
            "Talk about the generally results just present very quickly.",
            "All the results in the paper and then concentrate on three just after that conclude."
        ],
        [
            "So the problem was grammatically different amounts of the definition.",
            "After looking at it for a long long time is mine is something like.",
            "This is about learning grammars given information about the language, so it's more like getting things out of the definition for it to work.",
            "There's two points of view, at least.",
            "One is pragmatic.",
            "Get something working.",
            "Find a system that works and learn something, and one is more theoretical point of view which is involving the idea of saying where can I find an algorithm that can actually learn something in a theoretical sense?",
            "Meaning can I prove the convergence?"
        ],
        [
            "So.",
            "Who better OK?",
            "The typical setting would be something like.",
            "This is the first talk I thought I'd just go through everything very slowly.",
            "We're giving the class of grammars, which corresponds to a class of languages which is somewhere what we're trying to learn from.",
            "So in the theoretical setting, we start by drawing or saying that there is some hidden grammar G, which is usually called the target.",
            "From this target, G corresponds to a language L, so this doesn't mean many any algorithm in here just says there is something.",
            "So for each grammar there should be a language.",
            "If not, there's not much.",
            "Point in doing things and then what's going to happen in the setting is that somehow and there are a lot of some house involved here.",
            "There's a presentation of the information from which we're going to learn.",
            "The information typically is text, meaning positive examples or is counter examples and examples and best we've got an informant or all sorts of other things, like with more interactive settings where we can interrogate somebody in Oracle to find things out.",
            "And then there's the learner.",
            "The learner is an algorithm which is going to be receiving this information.",
            "And build a grammar G prime SEPTA there.",
            "This is really not much difference with the practical setting.",
            "Apart from that, there's no G right?",
            "The real difference is when we start asking ourselves well how we rather self found the same sort of thing we're looking for.",
            "Is this G prime that we've learned?",
            "Has it got something to do with G?",
            "Notice there's not an equality between G&G prime becauses there is all sorts of equivalence of grammars possibilities there.",
            "So it's more like just saying, well, do they do more or less the same thing?",
            "Do they do the same thing?",
            "Meaning it's the same language or nearly the same thing which would bring us to probabilistic questions like saying are we close to the solution on this, we can see all the different parameters that are going to intervene.",
            "G Well, probably the harder the grammar of the hardware is going to be to learn, so sizes will matter.",
            "The presentation here is a very loose thing.",
            "Clearly the presentation.",
            "Once we have refined that definition and saw the same well, this definition of self presentation or this other well that leads to one problem or another.",
            "And of course here the learner being an algorithm, we can measure all sort of complexity questions.",
            "Inside this."
        ],
        [
            "Remember that to study things, it's often the case when we study things to use an online process saying that the examples of the information is arriving one after the other and after each one of these we try the algorithm the learner is supposed to say something.",
            "Say, well, I think my hypothesis is this.",
            "This is not a practical setting.",
            "It's very rare that in practice you're just doing after one example.",
            "So I think my hypothesis is this, so it's a theoretical setting, but can be adapted because once you know that you're.",
            "Can do this.",
            "You can then decide to skip all the first steps and go straight to the last one, say, well, actually I'm really going to come out with just one hypothesis.",
            "But you study things is what?"
        ],
        [
            "Actually it's done.",
            "So summarizing theory theory is about describing things with respect to finding a target, the target being the key word.",
            "If you don't have a target, it's very difficult to do theory.",
            "Second thing is that obviously it's an idealized situation you don't usually have a target specifically, not in computational linguistics, but what we can say is that when we fail in a theoretical beautiful situation that may be hard to do it in practice.",
            "Maybe we can also study in this setting all the different resources from which we're learning, which is the examples, which is the way we can get them.",
            "The time we're allowed, etc.",
            "All that will matter and we can also talk about approximate learning.",
            "Obviously some targets are harder than others."
        ],
        [
            "Who said before?",
            "So this is just to say that we are not in our paper.",
            "We are certainly not claiming to have rediscovered are discovered everything.",
            "There are enormous amount of further results, only added this this morning.",
            "But I should have probably put in a whole list of references of other people who have been working on the."
        ],
        [
            "Settings.",
            "So what are the balls of the classes we are looking into so we could have taken all the typical classes of context free of regular, but we wanted to take two sufficiently different classes to look at the different models.",
            "The first one was balls for the edit distance and the second one where that."
        ],
        [
            "Cafe.",
            "So the balls for the edit distance where we use the typical edges distance which is used in biology which is used in pattern recognition which is used in number of fields where you have editing operations to get from one string to another, and you're trying to minimize the number of editor operations and that number is going to give you the edit distance.",
            "We are therefore only using situation whether each operation has cost one more complex situations are usually used with costs specific to each each operation each.",
            "Symbol that has been rewritten so for this we then define balls topological rules.",
            "You take a center which is a string array.",
            "Jasen, you take all the strings that are a distance at most K from that center."
        ],
        [
            "Just little word about it.",
            "Is that intuition on these objects is not a tool.",
            "Helpful, well now it isn't there.",
            "I mean, you get more puzzled by intuition than anything is.",
            "1 example is you was expecting a ball that to have sort of some even distribution with the short strings, the long strings and so on.",
            "In fact, it just happens with half the strings are very long, so at each moment for each half this if you got a two letter alphabet but you just you don't have a nice idea of saying, for example, that most of the strings are tucked into the middle of the ball.",
            "Actually most of the strings are on the outside of the board.",
            "OK, so that all that is very puzzling and an brings in new problems.",
            "They're not just theoretical things.",
            "There's a number of things in critics PHD's come out with enormous amount of literature where people are actually using these sort of things in different application."
        ],
        [
            "You can ask him.",
            "So DFA I'm not going to go through just saying that passing is easy.",
            "Equivalence problem is easy, which is why usually most of our algorithms seem to work."
        ],
        [
            "And we just named the classes, the classes, GB, good balls and DVA DFA the DFA.",
            "So why good balls?",
            "Well, there's a tricky issue about having the radius larger than the center.",
            "If you've got a very very big radius, you can find yourself with a silly ball which whose center is Lambda, the empty string.",
            "The raid uses something huge, and actually there's a sort of proportion there that is not nice where you can find yourself with the.",
            "Interesting examples being exponentially longer than the actual ball itself, or the representation of the bosses.",
            "The technical issue with the nasty one, so the sizes when we're talking about the ball well clearly to encode a board, you need to encode the center and the logarithm of the radius, and an automaton typically is taken as the number of states.",
            "I mean there is a question about the size of the alphabet."
        ],
        [
            "We were not mentioning it here.",
            "So this is what the paper is about, so that's why I'm not going to give all the results.",
            "The paper is about.",
            "We have studied systematic way, the good balls, the DFA and the bad balls.",
            "So bad balls is when we drop that condition on the radius, and we've looked at a dozen or so.",
            "Settings, so in grammatical inference, each one has got his favorite setting works in that setting, so we thought it was nice to sort of look at the different settings people were looking at and trying to see how they compare.",
            "So pack pack learning is one that people have been interested in logically.",
            "PAC Learning is usually with an informant, so this positive and negative examples.",
            "We've also checked the case of Pack text, which sounds silly, but actually there's some amusing things, even if you can't do anything in this case.",
            "So then IP is about implicit prediction errors.",
            "It's an old model that was introduced.",
            "I believe by Pitt.",
            "So that's counting how many errors you make during the process of learning.",
            "MC is not mind changes this time you count how many times you've got to change your mind before learning.",
            "CSV is about characteristic samples.",
            "It's more about teaching, saying how much stuff do you need to put in a set so as to be able to learn from, and then the three last corresponds to active learning, learning from membership queries, membership queries and equivalence queries, and correction queries for addition, which in our case will return.",
            "We give a string and if the string is not in the language, one closest string.",
            "In the language is returned, so green, blue, and red means well.",
            "The blue or well known, the green, or either sufficiently well known or easily derived from, and the red are new or newish, or the proof is new in the paper."
        ],
        [
            "OK, selected results.",
            "The first one, strikingly, that it may be easier to learn from text than to learn from and in forward.",
            "So text is when you've only got the positive examples.",
            "The informant.",
            "You also have the negative ones, so you think no, if you give me more information, is going to be easier.",
            "There is a trick, obviously.",
            "Through this the trick is the fact that if you're counting errors, I said there was a model called IP.",
            "If you're counting errors and somehow you're able to have a sort of a security, better, you know a safe solution.",
            "And you sort of just spend your time saying, well, my solution is the safe solution until you're sure that you should switch and say, well, no, I think it's this.",
            "Then you can do this and you usually can't do it when you've got positive and negative examples, because the positive and negative examples are actually making your search space very limited.",
            "But if you really got positive examples, you can take some over general solutions.",
            "Saying, well, you know everything Sigma star until you're absolutely sure and you say, well now I change my mind.",
            "I come to this or newly sure.",
            "OK, so the."
        ],
        [
            "The idea is that the IP model I sit count errors of prediction.",
            "There's something you have to remember is that it's not just about counting errors of prediction, you still have to identify in the limit you not just allowed to say OK Sigma star till the end you have to at some point or another Salem.",
            "I now should change and take a risk only in this case the risk isn't gonna be large.",
            "There is an ocean interesting notion that we started to explore, and there's more into it, which is notice of certain notion of certificate, which is a bit like learning with certificate, where we say we're learning.",
            "Once we've got the certificates to say, well, I can now claim something, then I changed my mind."
        ],
        [
            "So this we see in the table here where effectively we've got the case of good balls or not learnable from an informant, become learnable from text."
        ],
        [
            "Second selected result is about PAC Learning, so PAC learning is something incorrectly in France.",
            "We don't have that many results about.",
            "There are some, but they haven't been that many compared to other settings.",
            "So the what is all about is that there's an unknown distribution.",
            "The examples are arriving from this unknown distribution and somehow the distribution is also used to measure success means we're allowed to solve somehow transform it into a distance and say, well, now if I kept on taking some new examples once I've got a classifier.",
            "How wrong am I and I can weigh that so it's it can be used in that sense.",
            "So it's a very successful model invented by Valiant."
        ],
        [
            "Should have put his name, sorry.",
            "Now, the typical technique that is used this Finder grammar G or a concept G in a Class C consistent with the data, and if that problem is NP hard, then the class in question is not PAC learnable.",
            "So the sort of thing that has been widely used in the field of machine learning or computational machine learning, usually in grammatical inference we have not been able to use this because consistency or simple consistency has never been a hard problem.",
            "You can also return a DFA consistent with the data with the.",
            "The PTA is consistent with the data.",
            "So then you got you replace that with something called smallest consistency, but the pack doesn't work exactly the same way, and makes it a little bit more messy."
        ],
        [
            "Anyhow, here we can because in the case of good balls we do have an NP hard problem.",
            "Finding a ball.",
            "Is there a ball consistent with a set of positive and negative data is?",
            "MNP hardball.",
            "Therefore, PAC learning balls from data becomes.",
            "Backloading is impossible.",
            "You can't back learn balls."
        ],
        [
            "OK, and this is the results we got here.",
            "We can't learn it from text either."
        ],
        [
            "Right?",
            "So now my 3rd result isn't really a result, it's an open question.",
            "The question arose when I think sending the paper and a review said, well, is it really interesting to look at these two classes?",
            "Is perhaps one of the classes is just a simplified version of the other and therefore sort of you've learned one?",
            "You could learn the other, that sort of thing, and so you notice anything.",
            "Well, yes, perhaps you know all the balls can be represented by simple DFA and therefore, well, you could think of doing things like learning.",
            "Bulls in terms of DFA or something of that sort.",
            "So could we do that?",
            "Can we sort of transform any ball into a DFA?"
        ],
        [
            "So the problem would be something that we've called polynomial determinism for balls and prepared to swap that for a better title.",
            "You may suggest we.",
            "That's not very nice question is is anymore encodable by a polynomial sized DFA?",
            "Or is there a given DFA?",
            "Is there given polynomials such that the smallest DFA that represents the same language as the ball is of polynomial size?",
            "That's the the question that is here.",
            "So what do we know about this question?"
        ],
        [
            "We know about this question that people have been working on this for a long time, actually having using transformations from balls into DFU, knowing that their transformations were not polynomial but unable to say that it was.",
            "If it's their transformations that were not polynomial or the actual result.",
            "So there's a number of people here who can.",
            "In Navarro Melick Char and number of private communications, I mean, if Mail sent out to people who worked on it.",
            "So yes, you claim this where you sure?",
            "No, I'm not, so we're a little bit lost on knowing if you can do it.",
            "So approximate string matching is the problem of trying to find in a dictionary string that looks like it's related to finding a ball around a string and seeing if you belong to it.",
            "So if they that's what they do, they will actually build a DFA around the string they're looking for an try and search inside this DFA."
        ],
        [
            "So note it is easy for the case of not in deterministic finite state automata.",
            "So suppose you've got a string X one X2 up to make sense or string of length layer length N, and you want to build a non deterministic finite state automata representing a ball of radius K. Around this, what do you do?",
            "While you build K copies of the automaton.",
            "And then you relate all this with every possible letter.",
            "This is to have the insertions OK, because here you're saying if I go from here to here, I'm going to insert a letter.",
            "And then if I follow on there with this line, I will have an insertion of one and so on up to here.",
            "So that's the insertions dealt with.",
            "I mean you can clearly simplify and perhaps you don't want all Sigma and you just want Sigma minus X one.",
            "But I mean that's not very important.",
            "The question isn't.",
            "Isn't if interest here and the second thing you do is then you deal with the case of suppressions of deletions, which is by putting a Lambda or an epsilon transition, skipping one OK, skipping one letter or substitutions will also corresponds to just putting a letter.",
            "Here X2 will be substituted by something else and obviously you will count 1 extra to get here.",
            "This is non deterministic, it's simple, but if you try and determine eyes this then there are examples where determinization of this.",
            "Gives way to an exponential automaton, but that doesn't prove that there isn't minimizations version of it blah blah blah, or we don't have the proof please."
        ],
        [
            "OK, so we could try and go for grammatical inference.",
            "Proof of combinatorics.",
            "That was a hope you're thinking.",
            "Oh well, OK, but suppose in our table we had something like this, so we've got, you know, we've got a no for good balls or yes for DFA so we could learn DFAS, but we can't learn good balls in some parodying.",
            "But perhaps perhaps we could try and go for something like saying I could learn the good balls if sorry if God wills were representable by DFA, then I could learn the good balls in terms of DFA.",
            "And therefore I should have a yes here.",
            "Yeah, it's a very loose argument, but perhaps in certain cases of certain paradigms we could hope to have that, at least for example, was suggesting a half suggesting the review."
        ],
        [
            "So do we have that in the table?",
            "Well, yes we do.",
            "We do have it with membership queries and equivalence queries with membership queries and equivalence queries.",
            "We can't learn good balls where we can learn DFA.",
            "So so so so so."
        ],
        [
            "I have a kind of notorious results.",
            "No?",
            "Could we learn a DFA instead of a ball and then do some transform back?",
            "Whether the answer is going?"
        ],
        [
            "Be negative.",
            "Becausw when you doing equivalence queries, what's essentially is that equivalence queries have to be proper, meaning by that you've got to do an equivalence query with concepts or grammars in the class, which is going to mean that at some moment in the DFA learning process you're going to equivalence queries with things that are going to be DFA but not balls or not DFA that corresponds towards, so that would correspond in the learning OK in the learning of the.",
            "What you call them of the balls to doing non proper queries?",
            "That's rather than that's rather sad.",
            "Let's see what would happen and why is it difficult to learn balls."
        ],
        [
            "Things like that find a ball in a space where this is difficult, as this you do a membership query and you say, well, you know, does this little string belong?",
            "And the answer is going to be no.",
            "Oracle says no.",
            "Sorry.",
            "So next.",
            "Does this string belong?",
            "And Oracle says no.",
            "So every time the artist has no, all you know is that this string doesn't belong to the ball, is somewhere else.",
            "OK, fair enough.",
            "So now.",
            "Does this ring belong?",
            "No, so we try something else.",
            "Since the strings were not getting anywhere with membership queries, we train equivalent square.",
            "We propose a ball and we say, well, what about this world?",
            "Could this be the world we're hoping for?",
            "You know, we think it may be, and then we perhaps can get some information through the counterexample.",
            "So we tried and it says, well, no, because you know this string you've actually put inside the wall.",
            "This string is not inside the language, so all we know with this is the ball is doesn't contain that string here.",
            "But you're left with all this space here.",
            "I mean, you could think OK, my little ball is really ridiculously small.",
            "I could make it bigger.",
            "Remember that the plane is infinite, just the same way as infinite as Sigma store, so it's not going to help anything.",
            "OK, so that's why you know your membership queries in query."
        ],
        [
            "And equivalence queries don't really help in this.",
            "So basically what we're saying is that we don't have simple reductions class to class.",
            "Now we can't use something like saying, well, I can learn DFA thanks to the sorry balls.",
            "Thanks for the FA.",
            "In another way, it is reassuring.",
            "Machine learning is full of results of that type where you can't say that just because the class is included in another class, right?",
            "You can inherit the learnability result.",
            "You don't have that sort of thing.",
            "So in this case, well, it's fair enough that we don't have it.",
            "So if you can."
        ],
        [
            "Dude.",
            "Conclusion is we've got nice question of combinatorics on strings, which we don't have an answer to.",
            "Which one I probably nobody determinism for wars we have agreed with the 2 two and majority to claim that it is false.",
            "Well, I'm not sure that really gives her gives her gives it the room, the word conjecture.",
            "Oh, we don't have now.",
            "This is disappointing, but in a way we were wanting to look.",
            "Visit a class like the balls or the DFA through all these things, but we perhaps finished one step early in this, so there's perhaps room for further work.",
            "I encourage people to do, which is to obtain.",
            "Thanks to this.",
            "Some role in comparability results like saying, well, you know the fact that you can learn with membership queries an informant.",
            "Doesn't help a tool at the time of learning with.",
            "In the in the way in, in the case of IP, for example, OK, we don't have these class to class theorems that we would like to have.",
            "Some things seem they seem obvious, but having them is difficult.",
            "Being able to say how the fact of membership queries off.",
            "Sorry of mind changes seems so much easier than IP.",
            "Once you said that you don't really have the results, there are some little things, but we don't have enough.",
            "What else and the last thing is, we have introduced the new techniques to avoid mind changes or prediction errors indicating different cases and try to find out more things about both of strings and I said I'm ready for questions.",
            "Thank you.",
            "Questions.",
            "Yes, I'm just wondering to what extent do you think the character of the problem changes, if at all.",
            "If you change the way you calculate the edit distance on the size of the ball, so you shouldn't be insertion deletion each time as one unit, but if you were to make those counseling more than a substitution or something that changes in a terrible way, I mean we've looked at that in paper, which has just appeared two weeks ago in Las in the summer in GM NR.",
            "And the algorithms to actually do one thing or another just because you change the balance of the weights are completely different.",
            "I mean, you can learn balls through what we were looking in the case of correction queries, and it was completely different.",
            "The sort of things you were getting.",
            "So here I would put my bet on saying this is different.",
            "Intuitions, again, are tricky.",
            "Any other questions?",
            "So you mentioned different learning paradigm.",
            "And if we're looking at, you know, machine learning research, kind of in a broader perspective there I would say you to their aprons, among others of ethnic.",
            "I would say there is this this idea that basically in order to learn you have somehow to try to fit the data and to control the capacity of your function class and regularize and all that stuff.",
            "I was wondering whether there is some hook translate this kind of ideas in the Romantical inference field.",
            "Ha.",
            "The key question is can we get?",
            "Well I think is can we get rid of the target and study formal learning in a sense where the target is no longer there is in all these settings there is one thing in common, which is you always have got to target even in the pack one.",
            "Target you tried to approximate.",
            "You don't necessarily have a.",
            "You're not measuring distance.",
            "OK, but you're not trying to measure how far away you are from it yourself.",
            "You're still working.",
            "You're trying to do it, but that's not what you're measuring.",
            "Well, in the sense of the risk that you try to minimize it.",
            "The structural risk that's what you are trying to.",
            "I don't know.",
            "For example, when you're playing with margins, I mean you really only on the data.",
            "You know that that is going to also ensure you some sort of convergence towards but.",
            "I don't know.",
            "I've got the feeling of the key.",
            "The key question is there is the key question is to study formal convergence with or without a target.",
            "And with the target being the element from which we discuss things.",
            "We typically assume that the target is within.",
            "I mean, in some areas of machine learning.",
            "Do you think that perhaps aware formalizing that might go in that way?",
            "Yeah, I I certainly don't know how to do it because you're still having to.",
            "But then you're going to have distributions anyhow to be able to say how far away from an idealized thing you are there.",
            "The problem is that we are always able to.",
            "In the typical classes we look at, like DFA were always able to approximate as close as we want, right?",
            "Which with a huge DFH timer, with a huge probabilistic automaton or something really small is approximate anything.",
            "And that's probably all.",
            "There's been a problem because we always thought that therefore we didn't need to address that problem.",
            "But perhaps we should perhaps.",
            "Yeah, well, I'm welcome to to look at that.",
            "Ideas are or I ideas or would be nice.",
            "Yes.",
            "Search collecting.",
            "Who's worth exactly?",
            "Something like this inside.",
            "Difference that's that was the hope we then forgot about the hope because we had technical problems.",
            "I think we should go back to the hope and think, OK, let's fill in the holes to fill in the holes you need."
        ],
        [
            "You need to have sort of squares like this.",
            "Yes no no yes.",
            "OK, those are the nice squares, which means that this class and this class are incomparable, but you can't get enough of these squares because we've only got one.",
            "No, yes?",
            "OK, so you need more columns to be able to get more squares of that sort to be able to actually finish the job.",
            "OK. Speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given by.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The title is still learning languages from boundary sources.",
                    "label": 0
                },
                {
                    "sent": "The case of the DFA and a bowl of strings.",
                    "label": 1
                },
                {
                    "sent": "OK, can you hear me?",
                    "label": 0
                },
                {
                    "sent": "Stock price.",
                    "label": 0
                },
                {
                    "sent": "Right, but I'm very happy to be here.",
                    "label": 0
                },
                {
                    "sent": "I'm sad about one thing I can't find my watch so I'll have to rely on PF saying 5 minutes and things like that.",
                    "label": 0
                },
                {
                    "sent": "So the talk is a theoretical talk to start with and we're just looking or revisiting the typical particles of language learning and grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "And instead of visiting them purely abstractly, we're concentrating on two classes of languages, one which is well known, which is the terministic finite state automata and the other one is the balls of strings.",
                    "label": 0
                },
                {
                    "sent": "On this other slide down at the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottom, There are my authors who are all here.",
                    "label": 0
                },
                {
                    "sent": "So if you've got questions, you can ask them just as much as asked me.",
                    "label": 0
                },
                {
                    "sent": "Come in front.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good OK, so the outline of the talk what and why?",
                    "label": 1
                },
                {
                    "sent": "Why are we doing theory and what sort of theory describes the balls in automata?",
                    "label": 0
                },
                {
                    "sent": "Talk about the generally results just present very quickly.",
                    "label": 0
                },
                {
                    "sent": "All the results in the paper and then concentrate on three just after that conclude.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem was grammatically different amounts of the definition.",
                    "label": 0
                },
                {
                    "sent": "After looking at it for a long long time is mine is something like.",
                    "label": 0
                },
                {
                    "sent": "This is about learning grammars given information about the language, so it's more like getting things out of the definition for it to work.",
                    "label": 1
                },
                {
                    "sent": "There's two points of view, at least.",
                    "label": 0
                },
                {
                    "sent": "One is pragmatic.",
                    "label": 0
                },
                {
                    "sent": "Get something working.",
                    "label": 0
                },
                {
                    "sent": "Find a system that works and learn something, and one is more theoretical point of view which is involving the idea of saying where can I find an algorithm that can actually learn something in a theoretical sense?",
                    "label": 1
                },
                {
                    "sent": "Meaning can I prove the convergence?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Who better OK?",
                    "label": 0
                },
                {
                    "sent": "The typical setting would be something like.",
                    "label": 1
                },
                {
                    "sent": "This is the first talk I thought I'd just go through everything very slowly.",
                    "label": 0
                },
                {
                    "sent": "We're giving the class of grammars, which corresponds to a class of languages which is somewhere what we're trying to learn from.",
                    "label": 0
                },
                {
                    "sent": "So in the theoretical setting, we start by drawing or saying that there is some hidden grammar G, which is usually called the target.",
                    "label": 0
                },
                {
                    "sent": "From this target, G corresponds to a language L, so this doesn't mean many any algorithm in here just says there is something.",
                    "label": 0
                },
                {
                    "sent": "So for each grammar there should be a language.",
                    "label": 0
                },
                {
                    "sent": "If not, there's not much.",
                    "label": 0
                },
                {
                    "sent": "Point in doing things and then what's going to happen in the setting is that somehow and there are a lot of some house involved here.",
                    "label": 1
                },
                {
                    "sent": "There's a presentation of the information from which we're going to learn.",
                    "label": 0
                },
                {
                    "sent": "The information typically is text, meaning positive examples or is counter examples and examples and best we've got an informant or all sorts of other things, like with more interactive settings where we can interrogate somebody in Oracle to find things out.",
                    "label": 0
                },
                {
                    "sent": "And then there's the learner.",
                    "label": 0
                },
                {
                    "sent": "The learner is an algorithm which is going to be receiving this information.",
                    "label": 0
                },
                {
                    "sent": "And build a grammar G prime SEPTA there.",
                    "label": 0
                },
                {
                    "sent": "This is really not much difference with the practical setting.",
                    "label": 0
                },
                {
                    "sent": "Apart from that, there's no G right?",
                    "label": 0
                },
                {
                    "sent": "The real difference is when we start asking ourselves well how we rather self found the same sort of thing we're looking for.",
                    "label": 0
                },
                {
                    "sent": "Is this G prime that we've learned?",
                    "label": 0
                },
                {
                    "sent": "Has it got something to do with G?",
                    "label": 0
                },
                {
                    "sent": "Notice there's not an equality between G&G prime becauses there is all sorts of equivalence of grammars possibilities there.",
                    "label": 0
                },
                {
                    "sent": "So it's more like just saying, well, do they do more or less the same thing?",
                    "label": 0
                },
                {
                    "sent": "Do they do the same thing?",
                    "label": 0
                },
                {
                    "sent": "Meaning it's the same language or nearly the same thing which would bring us to probabilistic questions like saying are we close to the solution on this, we can see all the different parameters that are going to intervene.",
                    "label": 0
                },
                {
                    "sent": "G Well, probably the harder the grammar of the hardware is going to be to learn, so sizes will matter.",
                    "label": 0
                },
                {
                    "sent": "The presentation here is a very loose thing.",
                    "label": 0
                },
                {
                    "sent": "Clearly the presentation.",
                    "label": 0
                },
                {
                    "sent": "Once we have refined that definition and saw the same well, this definition of self presentation or this other well that leads to one problem or another.",
                    "label": 0
                },
                {
                    "sent": "And of course here the learner being an algorithm, we can measure all sort of complexity questions.",
                    "label": 0
                },
                {
                    "sent": "Inside this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember that to study things, it's often the case when we study things to use an online process saying that the examples of the information is arriving one after the other and after each one of these we try the algorithm the learner is supposed to say something.",
                    "label": 0
                },
                {
                    "sent": "Say, well, I think my hypothesis is this.",
                    "label": 0
                },
                {
                    "sent": "This is not a practical setting.",
                    "label": 0
                },
                {
                    "sent": "It's very rare that in practice you're just doing after one example.",
                    "label": 0
                },
                {
                    "sent": "So I think my hypothesis is this, so it's a theoretical setting, but can be adapted because once you know that you're.",
                    "label": 0
                },
                {
                    "sent": "Can do this.",
                    "label": 0
                },
                {
                    "sent": "You can then decide to skip all the first steps and go straight to the last one, say, well, actually I'm really going to come out with just one hypothesis.",
                    "label": 0
                },
                {
                    "sent": "But you study things is what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually it's done.",
                    "label": 0
                },
                {
                    "sent": "So summarizing theory theory is about describing things with respect to finding a target, the target being the key word.",
                    "label": 1
                },
                {
                    "sent": "If you don't have a target, it's very difficult to do theory.",
                    "label": 0
                },
                {
                    "sent": "Second thing is that obviously it's an idealized situation you don't usually have a target specifically, not in computational linguistics, but what we can say is that when we fail in a theoretical beautiful situation that may be hard to do it in practice.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can also study in this setting all the different resources from which we're learning, which is the examples, which is the way we can get them.",
                    "label": 0
                },
                {
                    "sent": "The time we're allowed, etc.",
                    "label": 0
                },
                {
                    "sent": "All that will matter and we can also talk about approximate learning.",
                    "label": 1
                },
                {
                    "sent": "Obviously some targets are harder than others.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who said before?",
                    "label": 0
                },
                {
                    "sent": "So this is just to say that we are not in our paper.",
                    "label": 0
                },
                {
                    "sent": "We are certainly not claiming to have rediscovered are discovered everything.",
                    "label": 0
                },
                {
                    "sent": "There are enormous amount of further results, only added this this morning.",
                    "label": 0
                },
                {
                    "sent": "But I should have probably put in a whole list of references of other people who have been working on the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Settings.",
                    "label": 0
                },
                {
                    "sent": "So what are the balls of the classes we are looking into so we could have taken all the typical classes of context free of regular, but we wanted to take two sufficiently different classes to look at the different models.",
                    "label": 0
                },
                {
                    "sent": "The first one was balls for the edit distance and the second one where that.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cafe.",
                    "label": 0
                },
                {
                    "sent": "So the balls for the edit distance where we use the typical edges distance which is used in biology which is used in pattern recognition which is used in number of fields where you have editing operations to get from one string to another, and you're trying to minimize the number of editor operations and that number is going to give you the edit distance.",
                    "label": 1
                },
                {
                    "sent": "We are therefore only using situation whether each operation has cost one more complex situations are usually used with costs specific to each each operation each.",
                    "label": 0
                },
                {
                    "sent": "Symbol that has been rewritten so for this we then define balls topological rules.",
                    "label": 0
                },
                {
                    "sent": "You take a center which is a string array.",
                    "label": 0
                },
                {
                    "sent": "Jasen, you take all the strings that are a distance at most K from that center.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just little word about it.",
                    "label": 0
                },
                {
                    "sent": "Is that intuition on these objects is not a tool.",
                    "label": 0
                },
                {
                    "sent": "Helpful, well now it isn't there.",
                    "label": 0
                },
                {
                    "sent": "I mean, you get more puzzled by intuition than anything is.",
                    "label": 0
                },
                {
                    "sent": "1 example is you was expecting a ball that to have sort of some even distribution with the short strings, the long strings and so on.",
                    "label": 0
                },
                {
                    "sent": "In fact, it just happens with half the strings are very long, so at each moment for each half this if you got a two letter alphabet but you just you don't have a nice idea of saying, for example, that most of the strings are tucked into the middle of the ball.",
                    "label": 0
                },
                {
                    "sent": "Actually most of the strings are on the outside of the board.",
                    "label": 0
                },
                {
                    "sent": "OK, so that all that is very puzzling and an brings in new problems.",
                    "label": 0
                },
                {
                    "sent": "They're not just theoretical things.",
                    "label": 0
                },
                {
                    "sent": "There's a number of things in critics PHD's come out with enormous amount of literature where people are actually using these sort of things in different application.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can ask him.",
                    "label": 0
                },
                {
                    "sent": "So DFA I'm not going to go through just saying that passing is easy.",
                    "label": 0
                },
                {
                    "sent": "Equivalence problem is easy, which is why usually most of our algorithms seem to work.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we just named the classes, the classes, GB, good balls and DVA DFA the DFA.",
                    "label": 1
                },
                {
                    "sent": "So why good balls?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a tricky issue about having the radius larger than the center.",
                    "label": 0
                },
                {
                    "sent": "If you've got a very very big radius, you can find yourself with a silly ball which whose center is Lambda, the empty string.",
                    "label": 0
                },
                {
                    "sent": "The raid uses something huge, and actually there's a sort of proportion there that is not nice where you can find yourself with the.",
                    "label": 0
                },
                {
                    "sent": "Interesting examples being exponentially longer than the actual ball itself, or the representation of the bosses.",
                    "label": 0
                },
                {
                    "sent": "The technical issue with the nasty one, so the sizes when we're talking about the ball well clearly to encode a board, you need to encode the center and the logarithm of the radius, and an automaton typically is taken as the number of states.",
                    "label": 1
                },
                {
                    "sent": "I mean there is a question about the size of the alphabet.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were not mentioning it here.",
                    "label": 0
                },
                {
                    "sent": "So this is what the paper is about, so that's why I'm not going to give all the results.",
                    "label": 0
                },
                {
                    "sent": "The paper is about.",
                    "label": 0
                },
                {
                    "sent": "We have studied systematic way, the good balls, the DFA and the bad balls.",
                    "label": 0
                },
                {
                    "sent": "So bad balls is when we drop that condition on the radius, and we've looked at a dozen or so.",
                    "label": 0
                },
                {
                    "sent": "Settings, so in grammatical inference, each one has got his favorite setting works in that setting, so we thought it was nice to sort of look at the different settings people were looking at and trying to see how they compare.",
                    "label": 0
                },
                {
                    "sent": "So pack pack learning is one that people have been interested in logically.",
                    "label": 0
                },
                {
                    "sent": "PAC Learning is usually with an informant, so this positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "We've also checked the case of Pack text, which sounds silly, but actually there's some amusing things, even if you can't do anything in this case.",
                    "label": 0
                },
                {
                    "sent": "So then IP is about implicit prediction errors.",
                    "label": 0
                },
                {
                    "sent": "It's an old model that was introduced.",
                    "label": 0
                },
                {
                    "sent": "I believe by Pitt.",
                    "label": 0
                },
                {
                    "sent": "So that's counting how many errors you make during the process of learning.",
                    "label": 0
                },
                {
                    "sent": "MC is not mind changes this time you count how many times you've got to change your mind before learning.",
                    "label": 0
                },
                {
                    "sent": "CSV is about characteristic samples.",
                    "label": 0
                },
                {
                    "sent": "It's more about teaching, saying how much stuff do you need to put in a set so as to be able to learn from, and then the three last corresponds to active learning, learning from membership queries, membership queries and equivalence queries, and correction queries for addition, which in our case will return.",
                    "label": 0
                },
                {
                    "sent": "We give a string and if the string is not in the language, one closest string.",
                    "label": 0
                },
                {
                    "sent": "In the language is returned, so green, blue, and red means well.",
                    "label": 0
                },
                {
                    "sent": "The blue or well known, the green, or either sufficiently well known or easily derived from, and the red are new or newish, or the proof is new in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, selected results.",
                    "label": 0
                },
                {
                    "sent": "The first one, strikingly, that it may be easier to learn from text than to learn from and in forward.",
                    "label": 1
                },
                {
                    "sent": "So text is when you've only got the positive examples.",
                    "label": 0
                },
                {
                    "sent": "The informant.",
                    "label": 0
                },
                {
                    "sent": "You also have the negative ones, so you think no, if you give me more information, is going to be easier.",
                    "label": 1
                },
                {
                    "sent": "There is a trick, obviously.",
                    "label": 0
                },
                {
                    "sent": "Through this the trick is the fact that if you're counting errors, I said there was a model called IP.",
                    "label": 0
                },
                {
                    "sent": "If you're counting errors and somehow you're able to have a sort of a security, better, you know a safe solution.",
                    "label": 0
                },
                {
                    "sent": "And you sort of just spend your time saying, well, my solution is the safe solution until you're sure that you should switch and say, well, no, I think it's this.",
                    "label": 0
                },
                {
                    "sent": "Then you can do this and you usually can't do it when you've got positive and negative examples, because the positive and negative examples are actually making your search space very limited.",
                    "label": 0
                },
                {
                    "sent": "But if you really got positive examples, you can take some over general solutions.",
                    "label": 0
                },
                {
                    "sent": "Saying, well, you know everything Sigma star until you're absolutely sure and you say, well now I change my mind.",
                    "label": 0
                },
                {
                    "sent": "I come to this or newly sure.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is that the IP model I sit count errors of prediction.",
                    "label": 0
                },
                {
                    "sent": "There's something you have to remember is that it's not just about counting errors of prediction, you still have to identify in the limit you not just allowed to say OK Sigma star till the end you have to at some point or another Salem.",
                    "label": 1
                },
                {
                    "sent": "I now should change and take a risk only in this case the risk isn't gonna be large.",
                    "label": 0
                },
                {
                    "sent": "There is an ocean interesting notion that we started to explore, and there's more into it, which is notice of certain notion of certificate, which is a bit like learning with certificate, where we say we're learning.",
                    "label": 0
                },
                {
                    "sent": "Once we've got the certificates to say, well, I can now claim something, then I changed my mind.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this we see in the table here where effectively we've got the case of good balls or not learnable from an informant, become learnable from text.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second selected result is about PAC Learning, so PAC learning is something incorrectly in France.",
                    "label": 1
                },
                {
                    "sent": "We don't have that many results about.",
                    "label": 0
                },
                {
                    "sent": "There are some, but they haven't been that many compared to other settings.",
                    "label": 0
                },
                {
                    "sent": "So the what is all about is that there's an unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "The examples are arriving from this unknown distribution and somehow the distribution is also used to measure success means we're allowed to solve somehow transform it into a distance and say, well, now if I kept on taking some new examples once I've got a classifier.",
                    "label": 1
                },
                {
                    "sent": "How wrong am I and I can weigh that so it's it can be used in that sense.",
                    "label": 0
                },
                {
                    "sent": "So it's a very successful model invented by Valiant.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Should have put his name, sorry.",
                    "label": 0
                },
                {
                    "sent": "Now, the typical technique that is used this Finder grammar G or a concept G in a Class C consistent with the data, and if that problem is NP hard, then the class in question is not PAC learnable.",
                    "label": 1
                },
                {
                    "sent": "So the sort of thing that has been widely used in the field of machine learning or computational machine learning, usually in grammatical inference we have not been able to use this because consistency or simple consistency has never been a hard problem.",
                    "label": 0
                },
                {
                    "sent": "You can also return a DFA consistent with the data with the.",
                    "label": 0
                },
                {
                    "sent": "The PTA is consistent with the data.",
                    "label": 0
                },
                {
                    "sent": "So then you got you replace that with something called smallest consistency, but the pack doesn't work exactly the same way, and makes it a little bit more messy.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyhow, here we can because in the case of good balls we do have an NP hard problem.",
                    "label": 1
                },
                {
                    "sent": "Finding a ball.",
                    "label": 0
                },
                {
                    "sent": "Is there a ball consistent with a set of positive and negative data is?",
                    "label": 0
                },
                {
                    "sent": "MNP hardball.",
                    "label": 0
                },
                {
                    "sent": "Therefore, PAC learning balls from data becomes.",
                    "label": 0
                },
                {
                    "sent": "Backloading is impossible.",
                    "label": 0
                },
                {
                    "sent": "You can't back learn balls.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this is the results we got here.",
                    "label": 0
                },
                {
                    "sent": "We can't learn it from text either.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So now my 3rd result isn't really a result, it's an open question.",
                    "label": 0
                },
                {
                    "sent": "The question arose when I think sending the paper and a review said, well, is it really interesting to look at these two classes?",
                    "label": 0
                },
                {
                    "sent": "Is perhaps one of the classes is just a simplified version of the other and therefore sort of you've learned one?",
                    "label": 0
                },
                {
                    "sent": "You could learn the other, that sort of thing, and so you notice anything.",
                    "label": 0
                },
                {
                    "sent": "Well, yes, perhaps you know all the balls can be represented by simple DFA and therefore, well, you could think of doing things like learning.",
                    "label": 1
                },
                {
                    "sent": "Bulls in terms of DFA or something of that sort.",
                    "label": 0
                },
                {
                    "sent": "So could we do that?",
                    "label": 1
                },
                {
                    "sent": "Can we sort of transform any ball into a DFA?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem would be something that we've called polynomial determinism for balls and prepared to swap that for a better title.",
                    "label": 1
                },
                {
                    "sent": "You may suggest we.",
                    "label": 0
                },
                {
                    "sent": "That's not very nice question is is anymore encodable by a polynomial sized DFA?",
                    "label": 1
                },
                {
                    "sent": "Or is there a given DFA?",
                    "label": 0
                },
                {
                    "sent": "Is there given polynomials such that the smallest DFA that represents the same language as the ball is of polynomial size?",
                    "label": 0
                },
                {
                    "sent": "That's the the question that is here.",
                    "label": 0
                },
                {
                    "sent": "So what do we know about this question?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know about this question that people have been working on this for a long time, actually having using transformations from balls into DFU, knowing that their transformations were not polynomial but unable to say that it was.",
                    "label": 0
                },
                {
                    "sent": "If it's their transformations that were not polynomial or the actual result.",
                    "label": 0
                },
                {
                    "sent": "So there's a number of people here who can.",
                    "label": 1
                },
                {
                    "sent": "In Navarro Melick Char and number of private communications, I mean, if Mail sent out to people who worked on it.",
                    "label": 1
                },
                {
                    "sent": "So yes, you claim this where you sure?",
                    "label": 0
                },
                {
                    "sent": "No, I'm not, so we're a little bit lost on knowing if you can do it.",
                    "label": 0
                },
                {
                    "sent": "So approximate string matching is the problem of trying to find in a dictionary string that looks like it's related to finding a ball around a string and seeing if you belong to it.",
                    "label": 0
                },
                {
                    "sent": "So if they that's what they do, they will actually build a DFA around the string they're looking for an try and search inside this DFA.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So note it is easy for the case of not in deterministic finite state automata.",
                    "label": 1
                },
                {
                    "sent": "So suppose you've got a string X one X2 up to make sense or string of length layer length N, and you want to build a non deterministic finite state automata representing a ball of radius K. Around this, what do you do?",
                    "label": 0
                },
                {
                    "sent": "While you build K copies of the automaton.",
                    "label": 0
                },
                {
                    "sent": "And then you relate all this with every possible letter.",
                    "label": 0
                },
                {
                    "sent": "This is to have the insertions OK, because here you're saying if I go from here to here, I'm going to insert a letter.",
                    "label": 0
                },
                {
                    "sent": "And then if I follow on there with this line, I will have an insertion of one and so on up to here.",
                    "label": 0
                },
                {
                    "sent": "So that's the insertions dealt with.",
                    "label": 0
                },
                {
                    "sent": "I mean you can clearly simplify and perhaps you don't want all Sigma and you just want Sigma minus X one.",
                    "label": 0
                },
                {
                    "sent": "But I mean that's not very important.",
                    "label": 0
                },
                {
                    "sent": "The question isn't.",
                    "label": 0
                },
                {
                    "sent": "Isn't if interest here and the second thing you do is then you deal with the case of suppressions of deletions, which is by putting a Lambda or an epsilon transition, skipping one OK, skipping one letter or substitutions will also corresponds to just putting a letter.",
                    "label": 0
                },
                {
                    "sent": "Here X2 will be substituted by something else and obviously you will count 1 extra to get here.",
                    "label": 0
                },
                {
                    "sent": "This is non deterministic, it's simple, but if you try and determine eyes this then there are examples where determinization of this.",
                    "label": 0
                },
                {
                    "sent": "Gives way to an exponential automaton, but that doesn't prove that there isn't minimizations version of it blah blah blah, or we don't have the proof please.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we could try and go for grammatical inference.",
                    "label": 0
                },
                {
                    "sent": "Proof of combinatorics.",
                    "label": 0
                },
                {
                    "sent": "That was a hope you're thinking.",
                    "label": 0
                },
                {
                    "sent": "Oh well, OK, but suppose in our table we had something like this, so we've got, you know, we've got a no for good balls or yes for DFA so we could learn DFAS, but we can't learn good balls in some parodying.",
                    "label": 1
                },
                {
                    "sent": "But perhaps perhaps we could try and go for something like saying I could learn the good balls if sorry if God wills were representable by DFA, then I could learn the good balls in terms of DFA.",
                    "label": 1
                },
                {
                    "sent": "And therefore I should have a yes here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a very loose argument, but perhaps in certain cases of certain paradigms we could hope to have that, at least for example, was suggesting a half suggesting the review.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do we have that in the table?",
                    "label": 0
                },
                {
                    "sent": "Well, yes we do.",
                    "label": 0
                },
                {
                    "sent": "We do have it with membership queries and equivalence queries with membership queries and equivalence queries.",
                    "label": 0
                },
                {
                    "sent": "We can't learn good balls where we can learn DFA.",
                    "label": 0
                },
                {
                    "sent": "So so so so so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a kind of notorious results.",
                    "label": 0
                },
                {
                    "sent": "No?",
                    "label": 0
                },
                {
                    "sent": "Could we learn a DFA instead of a ball and then do some transform back?",
                    "label": 1
                },
                {
                    "sent": "Whether the answer is going?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be negative.",
                    "label": 0
                },
                {
                    "sent": "Becausw when you doing equivalence queries, what's essentially is that equivalence queries have to be proper, meaning by that you've got to do an equivalence query with concepts or grammars in the class, which is going to mean that at some moment in the DFA learning process you're going to equivalence queries with things that are going to be DFA but not balls or not DFA that corresponds towards, so that would correspond in the learning OK in the learning of the.",
                    "label": 1
                },
                {
                    "sent": "What you call them of the balls to doing non proper queries?",
                    "label": 0
                },
                {
                    "sent": "That's rather than that's rather sad.",
                    "label": 0
                },
                {
                    "sent": "Let's see what would happen and why is it difficult to learn balls.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like that find a ball in a space where this is difficult, as this you do a membership query and you say, well, you know, does this little string belong?",
                    "label": 1
                },
                {
                    "sent": "And the answer is going to be no.",
                    "label": 0
                },
                {
                    "sent": "Oracle says no.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So next.",
                    "label": 0
                },
                {
                    "sent": "Does this string belong?",
                    "label": 0
                },
                {
                    "sent": "And Oracle says no.",
                    "label": 0
                },
                {
                    "sent": "So every time the artist has no, all you know is that this string doesn't belong to the ball, is somewhere else.",
                    "label": 0
                },
                {
                    "sent": "OK, fair enough.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Does this ring belong?",
                    "label": 0
                },
                {
                    "sent": "No, so we try something else.",
                    "label": 0
                },
                {
                    "sent": "Since the strings were not getting anywhere with membership queries, we train equivalent square.",
                    "label": 0
                },
                {
                    "sent": "We propose a ball and we say, well, what about this world?",
                    "label": 0
                },
                {
                    "sent": "Could this be the world we're hoping for?",
                    "label": 0
                },
                {
                    "sent": "You know, we think it may be, and then we perhaps can get some information through the counterexample.",
                    "label": 0
                },
                {
                    "sent": "So we tried and it says, well, no, because you know this string you've actually put inside the wall.",
                    "label": 0
                },
                {
                    "sent": "This string is not inside the language, so all we know with this is the ball is doesn't contain that string here.",
                    "label": 0
                },
                {
                    "sent": "But you're left with all this space here.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could think OK, my little ball is really ridiculously small.",
                    "label": 0
                },
                {
                    "sent": "I could make it bigger.",
                    "label": 0
                },
                {
                    "sent": "Remember that the plane is infinite, just the same way as infinite as Sigma store, so it's not going to help anything.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why you know your membership queries in query.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And equivalence queries don't really help in this.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're saying is that we don't have simple reductions class to class.",
                    "label": 1
                },
                {
                    "sent": "Now we can't use something like saying, well, I can learn DFA thanks to the sorry balls.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the FA.",
                    "label": 0
                },
                {
                    "sent": "In another way, it is reassuring.",
                    "label": 0
                },
                {
                    "sent": "Machine learning is full of results of that type where you can't say that just because the class is included in another class, right?",
                    "label": 0
                },
                {
                    "sent": "You can inherit the learnability result.",
                    "label": 0
                },
                {
                    "sent": "You don't have that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So in this case, well, it's fair enough that we don't have it.",
                    "label": 0
                },
                {
                    "sent": "So if you can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dude.",
                    "label": 0
                },
                {
                    "sent": "Conclusion is we've got nice question of combinatorics on strings, which we don't have an answer to.",
                    "label": 0
                },
                {
                    "sent": "Which one I probably nobody determinism for wars we have agreed with the 2 two and majority to claim that it is false.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not sure that really gives her gives her gives it the room, the word conjecture.",
                    "label": 0
                },
                {
                    "sent": "Oh, we don't have now.",
                    "label": 0
                },
                {
                    "sent": "This is disappointing, but in a way we were wanting to look.",
                    "label": 0
                },
                {
                    "sent": "Visit a class like the balls or the DFA through all these things, but we perhaps finished one step early in this, so there's perhaps room for further work.",
                    "label": 0
                },
                {
                    "sent": "I encourage people to do, which is to obtain.",
                    "label": 0
                },
                {
                    "sent": "Thanks to this.",
                    "label": 0
                },
                {
                    "sent": "Some role in comparability results like saying, well, you know the fact that you can learn with membership queries an informant.",
                    "label": 0
                },
                {
                    "sent": "Doesn't help a tool at the time of learning with.",
                    "label": 0
                },
                {
                    "sent": "In the in the way in, in the case of IP, for example, OK, we don't have these class to class theorems that we would like to have.",
                    "label": 0
                },
                {
                    "sent": "Some things seem they seem obvious, but having them is difficult.",
                    "label": 0
                },
                {
                    "sent": "Being able to say how the fact of membership queries off.",
                    "label": 0
                },
                {
                    "sent": "Sorry of mind changes seems so much easier than IP.",
                    "label": 0
                },
                {
                    "sent": "Once you said that you don't really have the results, there are some little things, but we don't have enough.",
                    "label": 0
                },
                {
                    "sent": "What else and the last thing is, we have introduced the new techniques to avoid mind changes or prediction errors indicating different cases and try to find out more things about both of strings and I said I'm ready for questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm just wondering to what extent do you think the character of the problem changes, if at all.",
                    "label": 0
                },
                {
                    "sent": "If you change the way you calculate the edit distance on the size of the ball, so you shouldn't be insertion deletion each time as one unit, but if you were to make those counseling more than a substitution or something that changes in a terrible way, I mean we've looked at that in paper, which has just appeared two weeks ago in Las in the summer in GM NR.",
                    "label": 0
                },
                {
                    "sent": "And the algorithms to actually do one thing or another just because you change the balance of the weights are completely different.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can learn balls through what we were looking in the case of correction queries, and it was completely different.",
                    "label": 0
                },
                {
                    "sent": "The sort of things you were getting.",
                    "label": 0
                },
                {
                    "sent": "So here I would put my bet on saying this is different.",
                    "label": 0
                },
                {
                    "sent": "Intuitions, again, are tricky.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So you mentioned different learning paradigm.",
                    "label": 0
                },
                {
                    "sent": "And if we're looking at, you know, machine learning research, kind of in a broader perspective there I would say you to their aprons, among others of ethnic.",
                    "label": 0
                },
                {
                    "sent": "I would say there is this this idea that basically in order to learn you have somehow to try to fit the data and to control the capacity of your function class and regularize and all that stuff.",
                    "label": 0
                },
                {
                    "sent": "I was wondering whether there is some hook translate this kind of ideas in the Romantical inference field.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "The key question is can we get?",
                    "label": 0
                },
                {
                    "sent": "Well I think is can we get rid of the target and study formal learning in a sense where the target is no longer there is in all these settings there is one thing in common, which is you always have got to target even in the pack one.",
                    "label": 0
                },
                {
                    "sent": "Target you tried to approximate.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily have a.",
                    "label": 0
                },
                {
                    "sent": "You're not measuring distance.",
                    "label": 0
                },
                {
                    "sent": "OK, but you're not trying to measure how far away you are from it yourself.",
                    "label": 0
                },
                {
                    "sent": "You're still working.",
                    "label": 0
                },
                {
                    "sent": "You're trying to do it, but that's not what you're measuring.",
                    "label": 0
                },
                {
                    "sent": "Well, in the sense of the risk that you try to minimize it.",
                    "label": 0
                },
                {
                    "sent": "The structural risk that's what you are trying to.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "For example, when you're playing with margins, I mean you really only on the data.",
                    "label": 0
                },
                {
                    "sent": "You know that that is going to also ensure you some sort of convergence towards but.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I've got the feeling of the key.",
                    "label": 0
                },
                {
                    "sent": "The key question is there is the key question is to study formal convergence with or without a target.",
                    "label": 0
                },
                {
                    "sent": "And with the target being the element from which we discuss things.",
                    "label": 0
                },
                {
                    "sent": "We typically assume that the target is within.",
                    "label": 0
                },
                {
                    "sent": "I mean, in some areas of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Do you think that perhaps aware formalizing that might go in that way?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I certainly don't know how to do it because you're still having to.",
                    "label": 0
                },
                {
                    "sent": "But then you're going to have distributions anyhow to be able to say how far away from an idealized thing you are there.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we are always able to.",
                    "label": 0
                },
                {
                    "sent": "In the typical classes we look at, like DFA were always able to approximate as close as we want, right?",
                    "label": 0
                },
                {
                    "sent": "Which with a huge DFH timer, with a huge probabilistic automaton or something really small is approximate anything.",
                    "label": 0
                },
                {
                    "sent": "And that's probably all.",
                    "label": 0
                },
                {
                    "sent": "There's been a problem because we always thought that therefore we didn't need to address that problem.",
                    "label": 0
                },
                {
                    "sent": "But perhaps we should perhaps.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, I'm welcome to to look at that.",
                    "label": 0
                },
                {
                    "sent": "Ideas are or I ideas or would be nice.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Search collecting.",
                    "label": 0
                },
                {
                    "sent": "Who's worth exactly?",
                    "label": 0
                },
                {
                    "sent": "Something like this inside.",
                    "label": 0
                },
                {
                    "sent": "Difference that's that was the hope we then forgot about the hope because we had technical problems.",
                    "label": 0
                },
                {
                    "sent": "I think we should go back to the hope and think, OK, let's fill in the holes to fill in the holes you need.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You need to have sort of squares like this.",
                    "label": 0
                },
                {
                    "sent": "Yes no no yes.",
                    "label": 0
                },
                {
                    "sent": "OK, those are the nice squares, which means that this class and this class are incomparable, but you can't get enough of these squares because we've only got one.",
                    "label": 0
                },
                {
                    "sent": "No, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, so you need more columns to be able to get more squares of that sort to be able to actually finish the job.",
                    "label": 0
                },
                {
                    "sent": "OK. Speaker again.",
                    "label": 0
                }
            ]
        }
    }
}