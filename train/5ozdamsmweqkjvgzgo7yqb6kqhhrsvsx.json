{
    "id": "5ozdamsmweqkjvgzgo7yqb6kqhhrsvsx",
    "title": "A fast algorithm for structured gene selection",
    "info": {
        "author": [
            "Sofi a Mosci, DISI - Department of Information Engineering and Computer Science, University of Genova"
        ],
        "published": "Nov. 8, 2010",
        "recorded": "October 2010",
        "category": [
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/mlsb2010_mosci_afa/",
    "segmentation": [
        [
            "I'm going to present this word about a fast algorithm for structure gene selection, which is a joint work with Alessandro Very Lorenzo Rosasco in Sevilla.",
            "We deal with."
        ],
        [
            "Problem of gene selection, which is which amounts to extracting a predictive model which the fans on a more subset of genes.",
            "So this is this as a two goals.",
            "One is to build a model which has a good prediction performance and the other is to build a mother which is easy to interpret.",
            "An as unclear, just highlighted.",
            "There are actually many variable selection algorithms that can be used.",
            "Filters, rappers, Animat, did an assemble and whatever but most."
        ],
        [
            "Many of them often lack either accuracy, stability, or interpretability, or all of them.",
            "So that's because learning in high dimensions and my career really high dimensions is a very challenging task unless we make some strong assumptions.",
            "Some, unless we have some further information from the domain expert on the biological problem under study so."
        ],
        [
            "To become this this thing.",
            "To add some prior to the machine learning tool, we will use an.",
            "For example, we might.",
            "We may want to select genes according to groups.",
            "We can think of groups from only online databases.",
            "The group genes in according to biological pathways oh may have some ad hoc grouping given rub by our biologist collaborators.",
            "So in order to deal with this problem.",
            "Few years ago, group Lasso algorithm was introduced.",
            "There actually is able to select GroupWise Anna.",
            "Lot of literature have been proposed to solve their group loss regularization problem might have one main drawback which is is this unfortunate constraint with that is that groups must be a partition of the gene of the total genes.",
            "That means that the groups cannot be overlapping, so we cannot have a gene that belongs to more than one group so."
        ],
        [
            "Last year OK. Last year Jacob the unclear just mentioned with Brezinski Anver came out with a very nice penalty with generalized the group lasso penalty to possibly overlapping groups.",
            "And so it's as many advantages, which is to bring higher stability, higher accuracy and also higher interpretability because it forces this GroupWise selection but has another big disadvantage, which is the implementation, because as it was originally proposed, it could deal with just a few hundreds of genes at the time."
        ],
        [
            "Well, we could focus on was to develop an approach which could scale well for solving the group lasso with overlap regularization problem and the procedure we propose is based on proximate methods.",
            "So now I'm going to go through how we can use this proximal methods for sparsity based regularization.",
            "Then I will recall the group cluster with overlap penalty and show how it was initially implemented.",
            "And then we will come out with our projection algorithms.",
            "Our proposal for solving this regularization problem.",
            "Man, I wish to say."
        ],
        [
            "Numerical experiment.",
            "So in general, sparsity based regular.",
            "The problem of sparsity based regularization amounts to solving this optimization problem where we have to minimize a functional which is the sum of two terms.",
            "One is the data term and the other is the penalty term which encodes the prior.",
            "So we can think of the general sparsity prior as saying this, that variables must be selected according to separate.",
            "Nested or also overlapping groups.",
            "This is is possible is a very general prior and the penalty term actually forces the solutions to satisfy this prior, and then I'll show you how we can do that in some specific cases.",
            "The data term I put here is just the least square error, but it actually could be any possible convex and smooth data term.",
            "The penalty term has not to be smooth, and actually it's in most cases it's not smooth and that's why we have to resort to some not standard mathematical tool to solve this regularization problem, because it's not.",
            "So I'm gonna close for form like Ridge regression.",
            "So there are many tool mathematical tools that can be used.",
            "Among them we can recall multiple method coordinate, dissent and proximate methods and proximate methods are very nice because there allows us to de couple the two terms."
        ],
        [
            "In fact, as that appended to the data term and the penalty term, so a general approximate algorithm is sketchy.",
            "Here is very simple because it's an iterative procedure where at each step."
        ],
        [
            "We first take the gradient of the data term, so we're there with the square loss.",
            "Linear model it involves the covariance matrix, so it's very simple.",
            "And then too it will."
        ],
        [
            "To apply the proximity operator of the penalty term.",
            "OK, I'm not going to give you the definition of proximity operator, but I'm going to show you how it looks like in some specific and well known cases.",
            "So you see, it's very simple because there is a data term is the same for different algorithms.",
            "You have the same step, the same gradient step and all the works will be too.",
            "Define what the proximity operator has to look like given the penalty."
        ],
        [
            "Even the prior.",
            "So for example, for just lost where we want to do variable selection.",
            "In this case, the prior is that the relevant variables are possibly small subset of the total variables, so a simple, not structured sparsity prior.",
            "And to do that, we can use the L1 norm.",
            "So the L1 regularization, also known as LA, so that you can see up there an you can solve that regularization problem by means of proximal method an.",
            "We obtain this iterative software Shoulding algorithm where again inside we have a usual gradient step and to which we apply this software shoulding operator which is nothing else that the proximity operator of the L1 norm.",
            "OK, so this proximal method that mess around like something very Mathematica.",
            "In the end they produce this little softer shoulder algorithm which is very common in machine learning community.",
            "So when we extend this to group."
        ],
        [
            "Blah so in this case the fires at the relevant variables to be selected according to unit groups given a priority and we have this concern that the groups might make a block partition of the total variables.",
            "So in this case we somehow have to come out with a different penalty that encodes the prior and to do that we can use this combination of L1L2.",
            "Norms where we are coefficient vector beta.",
            "We restrict to one group and then over the group we take it's normal and then we sum over the groups and doing that we are actually able to enforce this group wide selection.",
            "And the result when we use approximate method to solve this algorithm is that we obtain the usual agreed in bar where we have applied this different operator which is no longer there.",
            "Software show the operator by is the GroupWise of the Shawnee operator which does not act on company wise but acts on the norm of each group and it acts just as the software showed the acts on each component.",
            "So it's very simple."
        ],
        [
            "But then when we want to generalize to via groups that are overlapping.",
            "We cannot we have to introduce this more complicated penalty which was actually introduced by Isaca Basinski and very last year an it's more complicated.",
            "I'm not going into the details because there are three tickets theoretical properties.",
            "It's already been study an actually does this thing of selecting variables which are union of a small subset of the groups given our priority.",
            "And So what is not so nice of this penalty is that now it does not take up any longer on the groups, because the groups are overlapping.",
            "So in terms of approximate methods, we cannot compute the proximity operator of this penalty in a closed form.",
            "So in order to overcome this problem, Jacobin cold."
        ],
        [
            "Big.",
            "When around it an use mathematical trick that tell us that instead of working in the original space, we can move into an expanded space where we replicate the variables.",
            "That belongs to more than one group.",
            "For instance, here we can consider the Group One and Group 2, the green and the yellow.",
            "OK, they have some variables that belong to both groups, and then in to build this expanded space, we have to take those variables and replicate them.",
            "So if if those variables belong to a third or more groups, than we have to replicate them three or more times.",
            "So we replicate each variable as many times as the number of groups each variable belongs to.",
            "So in this expanded space, Now we can apply just through the standard group lasso algorithm an any optimization procedure for group lasso.",
            "For standard group lasso can be used, which is very nice.",
            "But it has one main door and doing that we will obtain the same solution if we were solving the original problem.",
            "Actually this is going to give a coefficient vector vector in higher dimensional space.",
            "So we have to build the coefficient vector in the original space by summing the coefficient corresponding to the same variable.",
            "So if we have like this variable that is replicated here we have two beat us here and we sum them together and we get the weight of that genes of their variable.",
            "In the original space, so very nice we can use any group lasso algorithm, But again there is a drawback, which is a degree of overlap increases than the expanded space can get very big an with degree of overlap.",
            "I mean the ever number of groups each variable belongs to, which sometimes can be high because we can think of genes belonging to several pathways.",
            "So the degree of overlap can be quite high.",
            "And so we want to avoid this."
        ],
        [
            "That's why we decide to focus on computing the proximity operator.",
            "Even if we cannot compute in a close for, we decide.",
            "Well, why don't we compute in approximate way?",
            "Because the overall iterative procedure is approximate and we can, we can demonstrate that if we compute the proximity operator in an approximate way, then we can still guarantee convergence of the overall algorithm, so, so let's do that.",
            "And to do that we need 3 steps.",
            "The first one is to note that the proximity operator of this penalty can be written as one minus the projection onto a. Convex set which is the intersection of cylinders, center in coordinates at sub sub."
        ],
        [
            "Space, let's make this example.",
            "Here we can think of this cylinder that involved the aksan.",
            "And the Z axis and actually on the Y and the zed axis and the other one they involve the zed and access the Y&X axis so.",
            "OK, I'm sorry.",
            "Are you alright?",
            "This will be axed.",
            "This will be sad and then why is going to be inside so you can think like this cylinder corresponds to the group?",
            "Of the viable cell along the circle, that means that not the X.",
            "Because it's not involved, it's not constrained, OK, but the constraint of the cylinder will be on the Y and the and the set on this cylinder does.",
            "Add is not constrained by the other variable constraint are going to be the X&Y.",
            "So for each group we have a cylinder and they the subset K is the intersection of all these cylinders and we want to project on this intersection."
        ],
        [
            "So another step to build the operator is to know it do not is only a small subset of these cylinders are actually active.",
            "That means that just to see that, let's see."
        ],
        [
            "Example, so when we want to project, we take a point."
        ],
        [
            "We project it onto the intersection, but."
        ],
        [
            "What if one of these points is already inside one of the cylinders of more cylinders?"
        ],
        [
            "Then when we project we can just forget about that cylinder.",
            "OK, so we just have to identify which are the active cylinders, the active groups and drop the other.",
            "So that always depends on on the coefficient vector that we want to project.",
            "So each time the set are active groups will change."
        ],
        [
            "So finally we propose to compute this projection.",
            "So with solving the dual problem, I'm not going through the math, but now this the nice thing is that the dual problem will be in a space which is R2.",
            "The number of active groups to the number of active cylinder that survived the discarding the rejection step."
        ],
        [
            "So overall, the algorithm we propose is based on the standard gradient step and this projection which is based on the three steps that I just showed you and over the the nice properties that overall convergence is still guarantee.",
            "So even if I'm computing this projection in an approximate way, because this this.",
            "Part here.",
            "Is going to be computed in an approximate way.",
            "Because the solution they do a problem as a primal problem has not a closed form, so I have to compute in approximate weight, but overall convergence is still guarantee."
        ],
        [
            "So this was our was our proposal.",
            "So in our numerical experiments we wanted to compare the running times of our algorithm to the other implementation, the one on the expanded space, and we compared it to the expanded space where we use the proximal algorithm for standard group lasso.",
            "So there are two proximal algorithms, one on the.",
            "Expanded space with future replication and one on the original space but with their product with the projection.",
            "So we called projection an replication.",
            "The two approaches and we did it on a synthetic data set where we built where there are three relevant groups and all of them have 20% overlap, so the three groups are relevant.",
            "Groups are overlapping and the other groups that just built by randomly drawing industries from.",
            "The whole set of genes or variables.",
            "So what we did we computed the entire regularization path.",
            "For both algorithm, because that's what is actually done in experiments that you compute the entire regularization path and then you assess the performance and find the best one.",
            "So we compute the entire regularization path for different values of the number of total variables an for different values of the overlap degree, again with the overlap degree I mean the average number of groups each variables belongs to.",
            "OK, so if each gene belongs in an average 2 two pathways then I will have overlap degree two.",
            "OK, so we did that and we see that we are the blue and the replication on the expanded.",
            "Approximately on the expanded space is the red.",
            "We see there.",
            "We almost always has much shorter running time than the replication approach, and this actually this.",
            "This gap increases as they overlap degree increases, so as they expanded, space gets bigger and bigger than our arm.",
            "Approach is preferable.",
            "Here we deny, deny, put their running time because it took more than 12 hours and we stopped.",
            "Since this is an average and we didn't want to go forever on that.",
            "So to conclude this slide, we actually are faster than the other approach, even if the other approach is simpler from an intuitive point of."
        ],
        [
            "You then this is the last experiment where we compare our algorithm to the replication.",
            "I must say one thing."
        ],
        [
            "We did not compare the prediction performance of the two algorithm because the solution is exactly the same.",
            "We're solving the same regularization problem, so we didn't want to compare the prediction because the solution is going to be the same, it's going to be in the same prediction.",
            "Result, but now that we are faster it can deal with larger datasets.",
            "We can come."
        ],
        [
            "Appeared in in problems where the other approach could not work well because in the original paper Jacobo Brezinski, Anver, they.",
            "In the test that the group lasso with overlap algorithm on a data set that was more than 8000 genes.",
            "But since their algorithm could not work on such a big data set, they had to apply some preprocessing.",
            "They reduce the data from more than 1000 two 300.",
            "So what we wanted to see was my wife.",
            "If instead of doing this preprocessing we use our algorithm on the entire data set in.",
            "Now that we can do it, is that going to improve the result and actually it did?",
            "Because we can see, look at the prediction error that we were able to slightly improve to reduce it, but then also in terms of selection performance were much much more stable because.",
            "We have one thing we follow.",
            "The regional Experimental Protocol where they used three fold cross validation.",
            "So in the end there will be 3 solutions.",
            "3 values of prediction error, three signature somehow.",
            "So for each split here with six 778.",
            "For the first Fleet, six groups were selected, then five and then 78 for the third split, whereas in our case we were able to select less viable, less groups, less pathways, and these are actually quite stable, because here you can see there are one pathway there is recurrent in all the splits and the other are appear always two times.",
            "So from this experiment was just one.",
            "So we would like to.",
            "Tested on other data or we could see that we could actually improve prediction and also selection performance."
        ],
        [
            "Conclude what I've presented you is optimization procedure to solve the regularization problem of group lasso with overlap and the way we faced this problem is by means of approximate methods.",
            "So we came out with the proximal algorithm.",
            "And the way we solve it was by using some ADOC theorems that we're able to improve the speed of the computation of the proximity operator.",
            "And overall, this algorithm is.",
            "This procedure is convergent and allows allows us dealing with a large data set and is actually fast."
        ],
        [
            "So what I've not discussion we have are the code available for a set of proximum algorithms.",
            "But if not, not discuss is some oscillation issues, because what I presented in the previous slide was just a basic proximal algorithm which has linear convergence rate.",
            "But then there are some likeness, terrible method that can bring from linear to quadratic convergence.",
            "Actually the results were using the Nesterov method, but it's a little bit more complicated.",
            "I just wanted to give you the idea, so I used a basic approximate algorithm and then it can be extended to other loss function.",
            "Even even though as Florence just mentioned earlier, the square loss is pretty good even when we are dealing with a classification problem, because there are some theoretical.",
            "Properties that guarantees the even the base risk is bounded by the empirical by the expected risk.",
            "So by the expected risk.",
            "So the other thing I did not, so if want to design another loss function, they could just put it in as long as it is convex and smooth.",
            "So this was some summary of the thing, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to present this word about a fast algorithm for structure gene selection, which is a joint work with Alessandro Very Lorenzo Rosasco in Sevilla.",
                    "label": 0
                },
                {
                    "sent": "We deal with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem of gene selection, which is which amounts to extracting a predictive model which the fans on a more subset of genes.",
                    "label": 1
                },
                {
                    "sent": "So this is this as a two goals.",
                    "label": 0
                },
                {
                    "sent": "One is to build a model which has a good prediction performance and the other is to build a mother which is easy to interpret.",
                    "label": 0
                },
                {
                    "sent": "An as unclear, just highlighted.",
                    "label": 1
                },
                {
                    "sent": "There are actually many variable selection algorithms that can be used.",
                    "label": 0
                },
                {
                    "sent": "Filters, rappers, Animat, did an assemble and whatever but most.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many of them often lack either accuracy, stability, or interpretability, or all of them.",
                    "label": 0
                },
                {
                    "sent": "So that's because learning in high dimensions and my career really high dimensions is a very challenging task unless we make some strong assumptions.",
                    "label": 0
                },
                {
                    "sent": "Some, unless we have some further information from the domain expert on the biological problem under study so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To become this this thing.",
                    "label": 0
                },
                {
                    "sent": "To add some prior to the machine learning tool, we will use an.",
                    "label": 0
                },
                {
                    "sent": "For example, we might.",
                    "label": 0
                },
                {
                    "sent": "We may want to select genes according to groups.",
                    "label": 0
                },
                {
                    "sent": "We can think of groups from only online databases.",
                    "label": 0
                },
                {
                    "sent": "The group genes in according to biological pathways oh may have some ad hoc grouping given rub by our biologist collaborators.",
                    "label": 1
                },
                {
                    "sent": "So in order to deal with this problem.",
                    "label": 1
                },
                {
                    "sent": "Few years ago, group Lasso algorithm was introduced.",
                    "label": 0
                },
                {
                    "sent": "There actually is able to select GroupWise Anna.",
                    "label": 0
                },
                {
                    "sent": "Lot of literature have been proposed to solve their group loss regularization problem might have one main drawback which is is this unfortunate constraint with that is that groups must be a partition of the gene of the total genes.",
                    "label": 1
                },
                {
                    "sent": "That means that the groups cannot be overlapping, so we cannot have a gene that belongs to more than one group so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last year OK. Last year Jacob the unclear just mentioned with Brezinski Anver came out with a very nice penalty with generalized the group lasso penalty to possibly overlapping groups.",
                    "label": 0
                },
                {
                    "sent": "And so it's as many advantages, which is to bring higher stability, higher accuracy and also higher interpretability because it forces this GroupWise selection but has another big disadvantage, which is the implementation, because as it was originally proposed, it could deal with just a few hundreds of genes at the time.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we could focus on was to develop an approach which could scale well for solving the group lasso with overlap regularization problem and the procedure we propose is based on proximate methods.",
                    "label": 1
                },
                {
                    "sent": "So now I'm going to go through how we can use this proximal methods for sparsity based regularization.",
                    "label": 1
                },
                {
                    "sent": "Then I will recall the group cluster with overlap penalty and show how it was initially implemented.",
                    "label": 0
                },
                {
                    "sent": "And then we will come out with our projection algorithms.",
                    "label": 0
                },
                {
                    "sent": "Our proposal for solving this regularization problem.",
                    "label": 0
                },
                {
                    "sent": "Man, I wish to say.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Numerical experiment.",
                    "label": 0
                },
                {
                    "sent": "So in general, sparsity based regular.",
                    "label": 1
                },
                {
                    "sent": "The problem of sparsity based regularization amounts to solving this optimization problem where we have to minimize a functional which is the sum of two terms.",
                    "label": 0
                },
                {
                    "sent": "One is the data term and the other is the penalty term which encodes the prior.",
                    "label": 1
                },
                {
                    "sent": "So we can think of the general sparsity prior as saying this, that variables must be selected according to separate.",
                    "label": 1
                },
                {
                    "sent": "Nested or also overlapping groups.",
                    "label": 0
                },
                {
                    "sent": "This is is possible is a very general prior and the penalty term actually forces the solutions to satisfy this prior, and then I'll show you how we can do that in some specific cases.",
                    "label": 0
                },
                {
                    "sent": "The data term I put here is just the least square error, but it actually could be any possible convex and smooth data term.",
                    "label": 0
                },
                {
                    "sent": "The penalty term has not to be smooth, and actually it's in most cases it's not smooth and that's why we have to resort to some not standard mathematical tool to solve this regularization problem, because it's not.",
                    "label": 0
                },
                {
                    "sent": "So I'm gonna close for form like Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So there are many tool mathematical tools that can be used.",
                    "label": 0
                },
                {
                    "sent": "Among them we can recall multiple method coordinate, dissent and proximate methods and proximate methods are very nice because there allows us to de couple the two terms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, as that appended to the data term and the penalty term, so a general approximate algorithm is sketchy.",
                    "label": 0
                },
                {
                    "sent": "Here is very simple because it's an iterative procedure where at each step.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We first take the gradient of the data term, so we're there with the square loss.",
                    "label": 1
                },
                {
                    "sent": "Linear model it involves the covariance matrix, so it's very simple.",
                    "label": 0
                },
                {
                    "sent": "And then too it will.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To apply the proximity operator of the penalty term.",
                    "label": 1
                },
                {
                    "sent": "OK, I'm not going to give you the definition of proximity operator, but I'm going to show you how it looks like in some specific and well known cases.",
                    "label": 1
                },
                {
                    "sent": "So you see, it's very simple because there is a data term is the same for different algorithms.",
                    "label": 0
                },
                {
                    "sent": "You have the same step, the same gradient step and all the works will be too.",
                    "label": 0
                },
                {
                    "sent": "Define what the proximity operator has to look like given the penalty.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even the prior.",
                    "label": 0
                },
                {
                    "sent": "So for example, for just lost where we want to do variable selection.",
                    "label": 0
                },
                {
                    "sent": "In this case, the prior is that the relevant variables are possibly small subset of the total variables, so a simple, not structured sparsity prior.",
                    "label": 1
                },
                {
                    "sent": "And to do that, we can use the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So the L1 regularization, also known as LA, so that you can see up there an you can solve that regularization problem by means of proximal method an.",
                    "label": 0
                },
                {
                    "sent": "We obtain this iterative software Shoulding algorithm where again inside we have a usual gradient step and to which we apply this software shoulding operator which is nothing else that the proximity operator of the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this proximal method that mess around like something very Mathematica.",
                    "label": 0
                },
                {
                    "sent": "In the end they produce this little softer shoulder algorithm which is very common in machine learning community.",
                    "label": 0
                },
                {
                    "sent": "So when we extend this to group.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blah so in this case the fires at the relevant variables to be selected according to unit groups given a priority and we have this concern that the groups might make a block partition of the total variables.",
                    "label": 1
                },
                {
                    "sent": "So in this case we somehow have to come out with a different penalty that encodes the prior and to do that we can use this combination of L1L2.",
                    "label": 0
                },
                {
                    "sent": "Norms where we are coefficient vector beta.",
                    "label": 0
                },
                {
                    "sent": "We restrict to one group and then over the group we take it's normal and then we sum over the groups and doing that we are actually able to enforce this group wide selection.",
                    "label": 0
                },
                {
                    "sent": "And the result when we use approximate method to solve this algorithm is that we obtain the usual agreed in bar where we have applied this different operator which is no longer there.",
                    "label": 0
                },
                {
                    "sent": "Software show the operator by is the GroupWise of the Shawnee operator which does not act on company wise but acts on the norm of each group and it acts just as the software showed the acts on each component.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But then when we want to generalize to via groups that are overlapping.",
                    "label": 0
                },
                {
                    "sent": "We cannot we have to introduce this more complicated penalty which was actually introduced by Isaca Basinski and very last year an it's more complicated.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into the details because there are three tickets theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "It's already been study an actually does this thing of selecting variables which are union of a small subset of the groups given our priority.",
                    "label": 1
                },
                {
                    "sent": "And So what is not so nice of this penalty is that now it does not take up any longer on the groups, because the groups are overlapping.",
                    "label": 0
                },
                {
                    "sent": "So in terms of approximate methods, we cannot compute the proximity operator of this penalty in a closed form.",
                    "label": 0
                },
                {
                    "sent": "So in order to overcome this problem, Jacobin cold.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big.",
                    "label": 0
                },
                {
                    "sent": "When around it an use mathematical trick that tell us that instead of working in the original space, we can move into an expanded space where we replicate the variables.",
                    "label": 0
                },
                {
                    "sent": "That belongs to more than one group.",
                    "label": 1
                },
                {
                    "sent": "For instance, here we can consider the Group One and Group 2, the green and the yellow.",
                    "label": 0
                },
                {
                    "sent": "OK, they have some variables that belong to both groups, and then in to build this expanded space, we have to take those variables and replicate them.",
                    "label": 0
                },
                {
                    "sent": "So if if those variables belong to a third or more groups, than we have to replicate them three or more times.",
                    "label": 0
                },
                {
                    "sent": "So we replicate each variable as many times as the number of groups each variable belongs to.",
                    "label": 0
                },
                {
                    "sent": "So in this expanded space, Now we can apply just through the standard group lasso algorithm an any optimization procedure for group lasso.",
                    "label": 1
                },
                {
                    "sent": "For standard group lasso can be used, which is very nice.",
                    "label": 0
                },
                {
                    "sent": "But it has one main door and doing that we will obtain the same solution if we were solving the original problem.",
                    "label": 0
                },
                {
                    "sent": "Actually this is going to give a coefficient vector vector in higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we have to build the coefficient vector in the original space by summing the coefficient corresponding to the same variable.",
                    "label": 0
                },
                {
                    "sent": "So if we have like this variable that is replicated here we have two beat us here and we sum them together and we get the weight of that genes of their variable.",
                    "label": 0
                },
                {
                    "sent": "In the original space, so very nice we can use any group lasso algorithm, But again there is a drawback, which is a degree of overlap increases than the expanded space can get very big an with degree of overlap.",
                    "label": 0
                },
                {
                    "sent": "I mean the ever number of groups each variable belongs to, which sometimes can be high because we can think of genes belonging to several pathways.",
                    "label": 1
                },
                {
                    "sent": "So the degree of overlap can be quite high.",
                    "label": 0
                },
                {
                    "sent": "And so we want to avoid this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why we decide to focus on computing the proximity operator.",
                    "label": 0
                },
                {
                    "sent": "Even if we cannot compute in a close for, we decide.",
                    "label": 1
                },
                {
                    "sent": "Well, why don't we compute in approximate way?",
                    "label": 0
                },
                {
                    "sent": "Because the overall iterative procedure is approximate and we can, we can demonstrate that if we compute the proximity operator in an approximate way, then we can still guarantee convergence of the overall algorithm, so, so let's do that.",
                    "label": 0
                },
                {
                    "sent": "And to do that we need 3 steps.",
                    "label": 0
                },
                {
                    "sent": "The first one is to note that the proximity operator of this penalty can be written as one minus the projection onto a. Convex set which is the intersection of cylinders, center in coordinates at sub sub.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space, let's make this example.",
                    "label": 0
                },
                {
                    "sent": "Here we can think of this cylinder that involved the aksan.",
                    "label": 0
                },
                {
                    "sent": "And the Z axis and actually on the Y and the zed axis and the other one they involve the zed and access the Y&X axis so.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Are you alright?",
                    "label": 0
                },
                {
                    "sent": "This will be axed.",
                    "label": 0
                },
                {
                    "sent": "This will be sad and then why is going to be inside so you can think like this cylinder corresponds to the group?",
                    "label": 0
                },
                {
                    "sent": "Of the viable cell along the circle, that means that not the X.",
                    "label": 0
                },
                {
                    "sent": "Because it's not involved, it's not constrained, OK, but the constraint of the cylinder will be on the Y and the and the set on this cylinder does.",
                    "label": 0
                },
                {
                    "sent": "Add is not constrained by the other variable constraint are going to be the X&Y.",
                    "label": 0
                },
                {
                    "sent": "So for each group we have a cylinder and they the subset K is the intersection of all these cylinders and we want to project on this intersection.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another step to build the operator is to know it do not is only a small subset of these cylinders are actually active.",
                    "label": 0
                },
                {
                    "sent": "That means that just to see that, let's see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, so when we want to project, we take a point.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We project it onto the intersection, but.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What if one of these points is already inside one of the cylinders of more cylinders?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then when we project we can just forget about that cylinder.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just have to identify which are the active cylinders, the active groups and drop the other.",
                    "label": 0
                },
                {
                    "sent": "So that always depends on on the coefficient vector that we want to project.",
                    "label": 0
                },
                {
                    "sent": "So each time the set are active groups will change.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally we propose to compute this projection.",
                    "label": 0
                },
                {
                    "sent": "So with solving the dual problem, I'm not going through the math, but now this the nice thing is that the dual problem will be in a space which is R2.",
                    "label": 1
                },
                {
                    "sent": "The number of active groups to the number of active cylinder that survived the discarding the rejection step.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall, the algorithm we propose is based on the standard gradient step and this projection which is based on the three steps that I just showed you and over the the nice properties that overall convergence is still guarantee.",
                    "label": 1
                },
                {
                    "sent": "So even if I'm computing this projection in an approximate way, because this this.",
                    "label": 0
                },
                {
                    "sent": "Part here.",
                    "label": 0
                },
                {
                    "sent": "Is going to be computed in an approximate way.",
                    "label": 0
                },
                {
                    "sent": "Because the solution they do a problem as a primal problem has not a closed form, so I have to compute in approximate weight, but overall convergence is still guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was our was our proposal.",
                    "label": 0
                },
                {
                    "sent": "So in our numerical experiments we wanted to compare the running times of our algorithm to the other implementation, the one on the expanded space, and we compared it to the expanded space where we use the proximal algorithm for standard group lasso.",
                    "label": 0
                },
                {
                    "sent": "So there are two proximal algorithms, one on the.",
                    "label": 0
                },
                {
                    "sent": "Expanded space with future replication and one on the original space but with their product with the projection.",
                    "label": 0
                },
                {
                    "sent": "So we called projection an replication.",
                    "label": 0
                },
                {
                    "sent": "The two approaches and we did it on a synthetic data set where we built where there are three relevant groups and all of them have 20% overlap, so the three groups are relevant.",
                    "label": 1
                },
                {
                    "sent": "Groups are overlapping and the other groups that just built by randomly drawing industries from.",
                    "label": 0
                },
                {
                    "sent": "The whole set of genes or variables.",
                    "label": 0
                },
                {
                    "sent": "So what we did we computed the entire regularization path.",
                    "label": 0
                },
                {
                    "sent": "For both algorithm, because that's what is actually done in experiments that you compute the entire regularization path and then you assess the performance and find the best one.",
                    "label": 0
                },
                {
                    "sent": "So we compute the entire regularization path for different values of the number of total variables an for different values of the overlap degree, again with the overlap degree I mean the average number of groups each variables belongs to.",
                    "label": 0
                },
                {
                    "sent": "OK, so if each gene belongs in an average 2 two pathways then I will have overlap degree two.",
                    "label": 0
                },
                {
                    "sent": "OK, so we did that and we see that we are the blue and the replication on the expanded.",
                    "label": 0
                },
                {
                    "sent": "Approximately on the expanded space is the red.",
                    "label": 1
                },
                {
                    "sent": "We see there.",
                    "label": 0
                },
                {
                    "sent": "We almost always has much shorter running time than the replication approach, and this actually this.",
                    "label": 0
                },
                {
                    "sent": "This gap increases as they overlap degree increases, so as they expanded, space gets bigger and bigger than our arm.",
                    "label": 0
                },
                {
                    "sent": "Approach is preferable.",
                    "label": 0
                },
                {
                    "sent": "Here we deny, deny, put their running time because it took more than 12 hours and we stopped.",
                    "label": 0
                },
                {
                    "sent": "Since this is an average and we didn't want to go forever on that.",
                    "label": 0
                },
                {
                    "sent": "So to conclude this slide, we actually are faster than the other approach, even if the other approach is simpler from an intuitive point of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You then this is the last experiment where we compare our algorithm to the replication.",
                    "label": 0
                },
                {
                    "sent": "I must say one thing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did not compare the prediction performance of the two algorithm because the solution is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "We're solving the same regularization problem, so we didn't want to compare the prediction because the solution is going to be the same, it's going to be in the same prediction.",
                    "label": 0
                },
                {
                    "sent": "Result, but now that we are faster it can deal with larger datasets.",
                    "label": 0
                },
                {
                    "sent": "We can come.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Appeared in in problems where the other approach could not work well because in the original paper Jacobo Brezinski, Anver, they.",
                    "label": 0
                },
                {
                    "sent": "In the test that the group lasso with overlap algorithm on a data set that was more than 8000 genes.",
                    "label": 0
                },
                {
                    "sent": "But since their algorithm could not work on such a big data set, they had to apply some preprocessing.",
                    "label": 0
                },
                {
                    "sent": "They reduce the data from more than 1000 two 300.",
                    "label": 0
                },
                {
                    "sent": "So what we wanted to see was my wife.",
                    "label": 0
                },
                {
                    "sent": "If instead of doing this preprocessing we use our algorithm on the entire data set in.",
                    "label": 0
                },
                {
                    "sent": "Now that we can do it, is that going to improve the result and actually it did?",
                    "label": 0
                },
                {
                    "sent": "Because we can see, look at the prediction error that we were able to slightly improve to reduce it, but then also in terms of selection performance were much much more stable because.",
                    "label": 0
                },
                {
                    "sent": "We have one thing we follow.",
                    "label": 0
                },
                {
                    "sent": "The regional Experimental Protocol where they used three fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So in the end there will be 3 solutions.",
                    "label": 0
                },
                {
                    "sent": "3 values of prediction error, three signature somehow.",
                    "label": 0
                },
                {
                    "sent": "So for each split here with six 778.",
                    "label": 0
                },
                {
                    "sent": "For the first Fleet, six groups were selected, then five and then 78 for the third split, whereas in our case we were able to select less viable, less groups, less pathways, and these are actually quite stable, because here you can see there are one pathway there is recurrent in all the splits and the other are appear always two times.",
                    "label": 0
                },
                {
                    "sent": "So from this experiment was just one.",
                    "label": 0
                },
                {
                    "sent": "So we would like to.",
                    "label": 0
                },
                {
                    "sent": "Tested on other data or we could see that we could actually improve prediction and also selection performance.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclude what I've presented you is optimization procedure to solve the regularization problem of group lasso with overlap and the way we faced this problem is by means of approximate methods.",
                    "label": 1
                },
                {
                    "sent": "So we came out with the proximal algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the way we solve it was by using some ADOC theorems that we're able to improve the speed of the computation of the proximity operator.",
                    "label": 1
                },
                {
                    "sent": "And overall, this algorithm is.",
                    "label": 1
                },
                {
                    "sent": "This procedure is convergent and allows allows us dealing with a large data set and is actually fast.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I've not discussion we have are the code available for a set of proximum algorithms.",
                    "label": 0
                },
                {
                    "sent": "But if not, not discuss is some oscillation issues, because what I presented in the previous slide was just a basic proximal algorithm which has linear convergence rate.",
                    "label": 0
                },
                {
                    "sent": "But then there are some likeness, terrible method that can bring from linear to quadratic convergence.",
                    "label": 0
                },
                {
                    "sent": "Actually the results were using the Nesterov method, but it's a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to give you the idea, so I used a basic approximate algorithm and then it can be extended to other loss function.",
                    "label": 0
                },
                {
                    "sent": "Even even though as Florence just mentioned earlier, the square loss is pretty good even when we are dealing with a classification problem, because there are some theoretical.",
                    "label": 0
                },
                {
                    "sent": "Properties that guarantees the even the base risk is bounded by the empirical by the expected risk.",
                    "label": 0
                },
                {
                    "sent": "So by the expected risk.",
                    "label": 0
                },
                {
                    "sent": "So the other thing I did not, so if want to design another loss function, they could just put it in as long as it is convex and smooth.",
                    "label": 0
                },
                {
                    "sent": "So this was some summary of the thing, thank you.",
                    "label": 0
                }
            ]
        }
    }
}