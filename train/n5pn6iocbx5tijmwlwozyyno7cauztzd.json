{
    "id": "n5pn6iocbx5tijmwlwozyyno7cauztzd",
    "title": "Structured Linear Models",
    "info": {
        "author": [
            "Fernando C. N. Pereira, Google, Inc."
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/iiia06_pereira_slm/",
    "segmentation": [
        [
            "Well known for many things.",
            "You listed some of them yesterday or day before yesterday.",
            "What does it look like?",
            "Programming, computational semantic, computational linguistics, information extraction?",
            "Probably about.",
            "Structured linear models.",
            "So the latest of your activities actually.",
            "Well, this is one of the latest things, but so this is I'm going to talk here about.",
            "So the foundation, a number of things that they've been doing and their special in the area of parsing and information extraction.",
            "But so this is very much joint work with a number of people I koby krammer asthma postdoc in my group, right?",
            "McDonald's?",
            "So students who just graduated PhD student in the two other PhD students they shine part to look.",
            "There is many people have been helping in collaborating in this work, and I acknowledge some of them down here as well as.",
            "A number of funding agencies.",
            "So what this is about so?"
        ],
        [
            "I'm going to start for the little bit of motivation and then go into the sort of the technicalities.",
            "So at one sort of 1 level, which is sort of the application oriented level.",
            "We are very soon developing methods to link, document and structured databases so.",
            "And nowadays you know we areas, for instance like in.",
            "In biology you have a growing range of structured databases for genomic databases, for example, and there's a huge amount of literature as well, and you like to be able to search and to interrogate these together.",
            "So the method that we are pursuing to achieve this goal is through techniques that collectively are known is the name information extraction and this is been mentioned this workshop several times and information extraction is sort of considered two parts, one which is to look at text and tag certain entities and relations of interest automatically.",
            "And once those are tagged, you try to map them to dimensions that you've tagged two entities and relations in a database, and neither of these steps is trivial.",
            "This one has to deal with a wide variability in how things I mentioned in text, and this one that white variability of how a particular entity or relation being named.",
            "So there's variation, both structural and syntactic, and also.",
            "Variationally terminological variation.",
            "I will today be focusing on the tagging part, not on the normalization part.",
            "And one of the main motivating examples, or at least one we doing a lot of these work for is for biomedical databases.",
            "But some of the examples that I will use are drawn from other application."
        ],
        [
            "Areas.",
            "Like this one.",
            "So this is a kind of slide I've carried around for a long time.",
            "I don't even know where I got it from.",
            "I think I may have gotten it from an interpretation of some slides from someone from BBN, so sort of the general picture information extraction for the like fraction of you that hasn't maybe less than very small fraction that hasn't seen this thing with kind of thing before is given some text.",
            "I'm going to label mentions of entities in that text with.",
            "According to force in certain categories that I'm interested in.",
            "For instance, if I'm looking at for persons an organized person and organization categories, I might find this is a name of a person, and that's the name of an organization, and this big thing here, which embeds that the organization name is a descriptor for a person, and they might be.",
            "You might also want to identify certain relations relations, such as we have here this person.",
            "Is an employee of this organization and this person is actually the same.",
            "These two mentions.",
            "NASA paid consultant Apical Saturday between use actually refer to the same person, so this is sort of the high at the high level with the information extraction systems want to do is to identify these mentions and potentially some of the relations in question.",
            "I will not be talking about relations much today, mostly focus, although we do some work on the relation extraction, I just had to call from.",
            "The line of work we do, the things that a few things that I think will significant.",
            "So I'll just focus on the tagging, the entity tagging, identifying mentions of different types of entities here."
        ],
        [
            "Now.",
            "So in the biomedical case, in fact this is from 2 project.",
            "The projects that we've been working on.",
            "One thing that a lot of people are interested in doing is to tag mentions of genes or the corresponding proteins in text, and you notice here that this can be fairly complex descriptions, so this is a this is a particular.",
            "A particular protein that we mentioned that's kind of the witches belongs to a particular species, but the whole the descriptor is.",
            "This whole thing is a multi word phrases and you like you would like to be able to do this tagging automatically and later on the figure out what is refers to.",
            "Maybe by mapping this through some existing genomic database to particular standard IDs in that database.",
            "The sort of relation side one of the projects we working with, working with some with some oncologists.",
            "We are quite interested in identifying mutations, genetic variations that are associated to cancer in certain ways and the genetic variation event has a number of different parts.",
            "For instance, here we have the state change in that variation.",
            "The particular nucleotide that's changed in what type of mutation where so their location?",
            "Type and state change that you have to identify to form your relation and some of these may be missing in any particular case, so these are the types of problems we're trying to solve and achieve a reasonable degree of accuracy in these so that then we can integrate the.",
            "That actually, that actual information with other data with databases that exist in these researchers have developed for their own perp."
        ],
        [
            "This is.",
            "So the general approach, which is very common for two in this area, you start by developing some text annotation guidelines.",
            "So what because you're going to get some people, some individuals who are going to be paid to label some training data.",
            "By hand that you can then use for machine learning algorithms.",
            "So we're going to develop some text annotations, the guidelines and get some crew of graduate students.",
            "Or you know other other generally available people that can be paid the moderate amount of money to do this, to annotate initial training documents that can be then used to train machine learning algorithms for extraction.",
            "And typically we're going to do then is iterative process, so you might train some algorithms, run it over some data, have people look at the results.",
            "And on the base of that, recognize, for example, that the original annotation was not actually quite right, and so the algorithms are not doing so well, that's because maybe your guidelines were not sharp enough, and so you might do some awesome re annotation, correct some materials, training materials and go around the loop."
        ],
        [
            "A couple of times.",
            "The way that this done, that annotation initialization is done, you will have some tool.",
            "This is the one that the people that work with us at the Linguistic Data Consortium have been using.",
            "They will.",
            "So this is some abstract which has been medical abstracts being tagged and so here we have exactly a variation event which is translocation involving the particular igh, which is a particular.",
            "In fact, this is some mention which is a gene slash, actually a protein associated with particular gene that is in this particular chromosome and associated.",
            "And this is related to a certain type of malignancy.",
            "Or in fact, there's two mentions of different languages and so on, so you have different colors here, representing representing the annotations.",
            "Of here of the processes for the event, the particular gene protein meant particular malignance is and they would be location information and so on so forth.",
            "So this is given these annotations and this is being done by basically people that are, you know, graduate students in biomedical areas for this particular, then we train the taggers that will.",
            "We'll discuss later.",
            "Now how do we so most of the talk is going to be about the techniques we use to train this the taggers."
        ],
        [
            "So in general, you're going to thinking about and analyzing text, and there are many things you could be doing now using text from.",
            "Identifying various units in text or layout, being able to parse, say, web page into elements corresponding to lists of things in the identifying each list element doing part of speech tagging or sense tagging of text, we're going to focus on information extraction.",
            "Although we've done a lot of work on parsing as well that I will not discuss here."
        ],
        [
            "And the general approach that we're going to take here is what we call or another many names that people have used for this.",
            "But let's call it for the purpose of this talk, structure classification, and we have an object which typically might be a document, and you're going to want to map it or sentence within the document or paragraph within the document depends on your object type of object depends on your application, but you have some object which is a complex objects or some kind.",
            "And you want to map it a learn a mapping from such objects to their structures.",
            "Ann and I represented those structures.",
            "Here is little trees because in at all though, for the purpose of the talk often would be more like labelings of.",
            "Of sequence of words in text, but the techniques that I'm discussing here.",
            "We also have said they've applied them for to parsing and so in there by using the Little Tracy.",
            "I want to indicate that these techniques are quite general.",
            "Then they are not just about tagging individuals in text, but can be used for for activities like parsing or relation extraction and so on.",
            "The."
        ],
        [
            "The main challenge you have to deal with and when you doing this kind of classification."
        ],
        [
            "Structure classification, that is mapping from a complex object to a complex label, which is a representation of this truck."
        ],
        [
            "That object is that decision.",
            "Decisions interact.",
            "So for instance, if I decide that these two go together in some way in my structure, then that forces me that forces that one to go together with these where is there?",
            "I decided to go together and that forces me, makes forces some of the decision with respect to that.",
            "So the ambiguity in the phrase cap is captured by the which of these two decisions you make first, and how.",
            "Interacts with the other decision.",
            "Now, so you have interacting decisions then the other thing you want to the other challenge you have is that to make these decisions well, decisions design.",
            "You have many types of features of the input sequence object, which in typical be a sequence that you made exploit, like in this case, the identity of the words lists of words that you dictionaries you have that tell you what the say, for instance that news might be, say, noun.",
            "From the preprocessing you may do like for instance part of speech tagging is words and so on.",
            "And the second challenge is that in general, although I will not discuss this much, finding all, finding an answer, finding the structure might be relatively costly.",
            "You have to search over all possible structures, and typically for anything interesting, the number of structures is exponential on the on, the length of your input, and you have to find an efficient way of if you can of searching in that set of structures."
        ],
        [
            "Now the particular setting that I will discuss today is sort of like I call analysis by tagging where you have an input sequence.",
            "So my ex I use represents an input element might be, for instance a token of some kind of word.",
            "You have done, some technician might be a character if you were working at a different level.",
            "But let's suppose these are the size of words and you're going to apply and run that sequence to a structure classifier.",
            "This is the box you want to learn to produce a another sequence of tags that label each of the individual elements of that sequence, and those tags represent.",
            "What they do is encode the role of each particular element of the input sequence.",
            "So for instance, being part of speech tagging, the tags are the parts of speech tags in shallow parsing, or at the segmentation tasks might be represent something or information extraction.",
            "They represent some information that describes what's the role of that particular element in the extractions that you were trying to obtain."
        ],
        [
            "So in fact, the general sort of there's a general trick.",
            "We use that to represent sort of extractions or segmentation in terms of tags, which is to have essentially suppose here I want to identify these particular elements.",
            "As in terms of tagging, I will say there's A tag be that denotes the beginning of a segment, A tag.",
            "I have the notes the being inside the segment, not the beginning, and then tag.",
            "Oh, that represents being outside the segment, so this is a very standard encoding that you could use, for instance information extraction.",
            "We have A tag says the beginning of a person name, another tag that represents the mean of the person name, and then the one I'm outside.",
            "Any name?"
        ],
        [
            "So there traditionally there have been two ways in which people have approached this problems.",
            "One is the sort of the kind of family of methods called generative methods includes hidden Markov models, context, probabilistic context free grammars, and so on that are very, very elegant, but have some limits which I'll discuss later on in more detail in how to deal with in an independent features of the input.",
            "And the other approach that also been quite widely used is to say, well, if I have to make all these decisions, say for each XI have to get the corresponding W. Why I why not run you some machine classification technique to learn how to put that label on that on that word?",
            "So the composed the process of assigning a structure into a sequence of decisions.",
            "And the main challenge with that is that you cannot really trade off if you divide the problem into problems doing individual decisions, you cannot trade off decisions at different places.",
            "You cannot say well, yeah, if I make this interesting here then this way, then the decision over there is affected in a way that I may not want.",
            "So doing learning so that you manage that tradeoff is something that is fundamentally the focus of the approaches will be discussing."
        ],
        [
            "So just kind of as a reminder, just if you are doing this with a hidden this type of analysis with a hidden Markov model.",
            "So the labels for the tags you want to assign, essentially the so the individual tasks are going to be the states of a hidden Markov model and the input.",
            "Oh, there's enough sense that there's a little typo here.",
            "This my notation, I didn't realize this, so if I my notation in.",
            "How that's interesting, sorry this should be a wise.",
            "This is the axis of this slide.",
            "Those used before and nobody notice this.",
            "I always find my own errors, which is kind of embarrassing.",
            "So this is these are wise in this RX is I'm sorry for the typo here in the in the picture.",
            "In the over there it's correct.",
            "So basically you have wise you know.",
            "So now you have to mentally after lunch.",
            "This is hard mentally think why there an X here so.",
            "You thinking of generating the process of generating each particular word you see given the state you are in, which represents the label that you're signing and the problem is that words have many different, so the type of this type of model has the challenge that you know.",
            "There are many words, and the prediction of what word occurs at position 3 is not just dependent that say, I know that this is the beginning of a person's name.",
            "I know you there are many other pieces of evidence.",
            "For instance, say the word sort of there's correlations between features have to do with.",
            "For instance, say things like capitalization and layout, which is very strict.",
            "Conditional independence model does not allow you to capture."
        ],
        [
            "Still, people have used hidden Markov models for extraction, where the states represent essentially class.",
            "You know, for instance here is 1 example from her, some work of Andrew McCallum and collaborators, where each state represents, so this was for parsing the title pages of documents where you have things like the title and the affiliation of the authors and the name of the author, and so on.",
            "And each of these states generates then words that correspond to OK, I'm in an author position, so here's you know.",
            "Maybe John is a good word to generate.",
            "And so this so this was one of the ways that people were doing information extraction like 7 eight years ago.",
            "Many of the deflection system based on hidden Markov models."
        ],
        [
            "But so as I was saying that the problem with."
        ],
        [
            "This is that I'm assuming that I generate each word independently, conditional on the states that I chose, generate each word independently and that."
        ],
        [
            "Doesn't take into account, in fact, that there are many words features that I might be interested in dealing with, and those are not independent.",
            "For instance, where the identity capitalization are not independent or wins.",
            "In particular suffix anwood identity are not independent, so in general.",
            "If you want to deal with the fact that there are words you've never seen, you might say I don't know that, so I've never seen the word particularly in training, but I know it ends in particular suffix so.",
            "I should like that if it wins in the particular suffix, it's more likely or less likely to be the beginning sight of."
        ],
        [
            "A person name.",
            "So one somewhere you can think of doing this is instead of generating individual words, you generate probabilistically individual features of the words.",
            "So you should specially for those of you familiar those things.",
            "This is like having a set of naive Bayes models, one for each position, with condition with dependencies between the labels.",
            "But that actually doesn't work so well, because these particular features are not.",
            "This assumes that condition on the on the label.",
            "The features are independent and they are not."
        ],
        [
            "So given all the sort of considerations.",
            "A number of others sort of gravitated towards a different way of thinking.",
            "Which is the one that we use for a large range of applications that I including the extraction applications, and that's sort of the idea and this is not the way we thought of it originally, but I think this is the kind of best way to think about it.",
            "At least the best way I know is to think of what we're doing is generalizing the theory and the methods of the linear classification.",
            "So suppose that I have a wait, so some way capital F of taking an input X and output Y&X is going to be typically here a sequence of words saying why is sequence of labels of tags and produce some kind of a vector out of that.",
            "And this bold F means a vector computed out out of all that stuff.",
            "And I can also I have also some weight vector W that assigns importance to the elements of the to those features.",
            "So this is the feature vector and the weights he assigned positive or negative importance to that saying I like I like it.",
            "So wait is positive.",
            "I like when I see that feature.",
            "So I like this particular pairing of X&Y when I see that feature.",
            "If the weight is negative is I don't like that particular pairing of X&Y in this is that feature.",
            "And then you have some means which I won't discuss immediately of searching over all possible wise, forgiven, X and find the one that maximizes this.",
            "So this is a linear linear classifier.",
            "Essentially, I find the labeling that maximizes this and whether I can do that efficiently or not depends exactly how the labeling, huh?",
            "What the form of this function F is.",
            "So how do I make it something that I can computationally handle?",
            "I if I can decompose this function F. So remember, X&Y are big objects into a bunch of small functions which only depend on a bounded subset of the labels.",
            "And so each of these sees here represents a small group of nodes or small group of positions in X and.",
            "And the dependence so that the function of these features F only depend on the labels for those positions cause that described by C. So if I can, so this is so these functions only depend on a subset of the labels, and these labels are organized in a nice way and I want, which essentially you can think of it as the this particular sees form a tree, certain tree structure.",
            "Then I can use efficient decoding to find this Y to maximize that maximizes this as we can do dynamic programming over the assignments.",
            "So I can basically.",
            "Look at each of these local feature functions and do the dot product with W and find locally what's the maximum assignment given the assignments to the previous thing so I can start.",
            "I can start here and then do the assignment for there and the assignment for there is assignment for there and I can do efficient Viterbi code.",
            "So that's the general picture of the types of models we're using here."
        ],
        [
            "So how do we learn these things?",
            "So we have, so we have to have some prior knowledge we have to do some knowledge engineering and think of what?"
        ],
        [
            "At the local domains of an input, what a disease that that we can get our.",
            "Are we going to use an?",
            "We are always motivated by trying to find local domains that make the problem of doing this maximization efficient."
        ],
        [
            "And we we come up with local feature functions that essentially capture the features of the input and of the labels that we think are informative about about the problem of choosing the right labels.",
            "And then we're going to do is to adjust W this weight vector W to Max optimize some objective function that we believe.",
            "For the notion of generalization to unseen data with low error and the sort of general form of that tends to be something like this, so we have something that tells us that we don't want to wait vector to be too wild.",
            "So basically the for instance here might be that the square norm of the way vector is.",
            "It's going to be small with some multiplier Bellevue, and that the error.",
            "So this is L is called the loss, which basically says that the the training time I do well so L if L is high for a particular training instance subscript I.",
            "That means that if this is high means that that weight vector is not good at producing the intended output Yi for the given input XI.",
            "So we have a labeling, labeling, training set of pairs, xiy I and I want to my W star be such that it makes the some of these things small, so this is hopefully this is small and that is small.",
            "So and Lambda is a trade off parameter between those two things.",
            "So I want to his weight vector to be small and I want the error the loss of each my www.star.",
            "Gives me when I applied to wins used to classify XI into some particular why I want those to be small, so that's the general yes.",
            "Are there local domains saying for features or they can be different for different features?",
            "In general.",
            "Yes, in general we have different.",
            "So for example, for example use.",
            "We might for certain features, a local domain might be a single label, so a single position.",
            "So for instance you have features that have to do with word identity that you might just say I want only to look at.",
            "What I did is positioned for other features I might want to say.",
            "I want to look at for instance part of speech tag information for two consecutive positions and the corresponding labels.",
            "So the domain there will be larger.",
            "So in general you combine you have features just in practice you tend to make have features that are very specialized, very precise, but which only look at one.",
            "Say for instance one label and then features which have little bit more less specific that look at larger Windows.",
            "And the reason you want to do that is the cause.",
            "If you say you have a very specific feature that is looking at large window be cause there are so many combinations of possible labels.",
            "The probability you will, in the training that you might never see particular combinations."
        ],
        [
            "So I'll just use some notation here so.",
            "Given a particular two, can a target so we have an input X and I want to produce a target structure Y.",
            "Let's suppose you have a competitor.",
            "Why prime this quantity here, which called the margin, is advantage in favor of.",
            "Why against Y prime?",
            "So if this is positive, it means that the my current W likes.",
            "Why better than Y prime?",
            "If this is negative, means that my W likes, why less than Y prime?",
            "So in general, if Y is the correct labeling and why prime is incorrect competitor?",
            "I want this kind of thing to be positive.",
            "So that's what I'm going to be training the kind of intuitively my training algorithm want to do is to make the margin positive, where, when, for the correct labeling against incorrect labeling that many ways of trying to do that, and I won't have much time to go into."
        ],
        [
            "Details of the various ways you can do it.",
            "But the sort of two main ways that we use in our work, one is to essentially maximize the probability of the correct output, and for that the loss that you're going to use.",
            "Essentially, if you do the little algebra that you need is the negative.",
            "So because you're trying to minimize is the minus lock probability of the particular of the intended output Y against all possible other outputs Y prime.",
            "So you basically want you want to say I want Y to be better.",
            "So notice that if why is it gets better score than Y prime, then this number here is positive.",
            "So this exponent is negative.",
            "So this is a small number, so the loss is going to be small.",
            "On the other hand, if Y prime has higher, probably has higher score than Y, then this is going to be a number large number larger than one.",
            "So this log is going to be.",
            "Larger than, larger than zero.",
            "So you basically ended.",
            "This is always bounded below by zero Becausw.",
            "One of the possible I primes is why itself, which makes this mix is 1 and therefore this log is 0, so this is a non negative quantity and the worst the higher it is the more the indicates that why the sort of the probability mass that why gets relative to the rest of the competitors is lower and lower.",
            "So you want to make that small to maximize the probability of the correct help.",
            "Another loss function that is often used which is sort of a.",
            "Which basically is trying to say is I want to.",
            "So if Y gets if the margin between Y&Y prime is small, in fact even worse if it is negative.",
            "So if this is negative then this is a positive number an let's say this here be the disagreement between Y&Y Prime and this agreement may be measuring many different ways than we typically measure it.",
            "In the Hamming loss disagreement, to say if two labels are so you are looking at two sequences in the two labels are difference Taipei 1 if they are the same.",
            "I pay 0, so it's a number of errors in the sequence, so that means that these numbers positive.",
            "This means the positive parts, so I Max.",
            "So I'm finding the Y prime that basically the Y prime that is most misclassified one that basically is wrong.",
            "And Furthermore, he's on the wrong side, is it gets a score that is greater than Y?",
            "So I basically 5 make this number small.",
            "That means that everything every Y prime.",
            "So if this number is 0 means that every candidate at every competitor is going to be have a score lower than Y, and how much lower it is depends?",
            "How wrong what this competitor is?",
            "So if this competitor is so, how wrong it is?",
            "It's measured by this, so if Y prime is just one one off, so it's one label that's incorrect, then this number needs to be 1 S. If Y prime is say, 1010 labels are wrong, then this number needs to be 10 to make the last zero.",
            "So you see that basically the further why prime is from Y in terms of error.",
            "The more I want the score of why prime to be below the score of Y, so these are two different losses that we use in different in different experiments and basically in both cases I could need to search over Y prime again if I if I have a nicely behaved graph I could do dynamic programming.",
            "If not I have problems that I don't want to discuss that."
        ],
        [
            "Unless I have, people can ask me later if you have time.",
            "Why do we do this?",
            "So basically by doing looking at these as a global optimization problem, we can trade off labeling decisions at different positions in the input 'cause we are looking at the entire input sequence in the entire output sequence.",
            "Although we can we exploit some decomposition in terms of local domains, we don't have any issues about what kind of features to use.",
            "We can anything can go into those features, functions and it's modular approach in that first of all, the scoring can be factor according to the local domains.",
            "And the last you can choose a loss function that satisfies that is appropriate to your application.",
            "So there's a general way of thinking about these types of problems that come up in information extraction.",
            "We've used it for parsing for relation extraction and and many other people use for many, many other applications."
        ],
        [
            "Now the probabilistic version, which is in fact the one we developed.",
            "First we used, gave it the number of conditional random fields and basically what you do is you apply a softmax that scoring function to produce a conditional probability of a labeling given an input.",
            "And so you basically you take the exponent and apply normalization an for the sequential case, say the first or the sequential case.",
            "What we do is we make the features.",
            "The feature functions depend just on two consecutive labels, so that's the 1st order case, so your local domains at most involved two consecutive labels.",
            "They might look at anything in the input they care about, because the input is fixed and the user training criterion the log loss.",
            "So basically you maximize the probability of the label sequence training label sequence given the corresponding input."
        ],
        [
            "So those are the math.",
            "This is sort of.",
            "The general methods are now now that I've introduced the methods, I'm going to talk a little bit about the applications.",
            "So for information extraction applications we tend to what features do you tend to use?",
            "We'll sort of basically conjunctions of two things.",
            "One is the configuration of labels."
        ],
        [
            "So in a case like this would be the choice of what tag you have for why the previous tag and the current tag."
        ],
        [
            "An answer in properties of the input might be at the identity of the terms membership of the term list.",
            "So for instance of these words is a first name in the American census listing.",
            "Orthographic patterns, like for instance suffixes.",
            "Capitalization like an conjunctions of these for the current and surrounding words.",
            "Now if you start looking at larger and larger conjunctions, this could be many many many different.",
            "You can create a very large number of features and so one option that we've used in some situations is work that Andrew McCallum did, and we've used for certain applications is that you generate only those features that help prediction, so you have a greedy algorithm that generates conjunctions.",
            "Calling insofar as those conjunctions can be validated in the training data by improving the classification accuracy in a training date."
        ],
        [
            "So a lot of this technology, especially the probabilistic, this conditional random fields is implemented actually in a package called mallet kind of URL down there, which is available freely on an open source license out of University Massachusetts.",
            "So Andrew and his students have done a lot.",
            "Most of the implementation we have introduced, we implemented some particular learning algorithms, some variants.",
            "Some particular pieces of the conditional random field package and a few other things, and we constantly adding to it so this is open source limited documentation.",
            "We also have actually put a.",
            "We added this scripting layer to it, so you can actually create your own extract as easily by writing a Python script that drives.",
            "This is all written in Java and we use Jython which is a Python interpreter in Java to which.",
            "Can orchestrate from a higher level.",
            "This is very large software package and we have some tutorial.",
            "A couple tutorials that explain how to do information setup is simply an information extraction sort of pipeline using that sort of scripting."
        ],
        [
            "Fire.",
            "So typically people evaluate is very much like information retrieval.",
            "We evaluate information extraction in terms of precision and recall.",
            "These are exact matches.",
            "So when I say what proportion of predicting an exact correct with exact boundaries.",
            "So I'm looking at the mentions and I want to know whether they have the exact correct boundaries and recall what what proportion of the entities are predicted and usual F1 measure.",
            "So this is sort of typical.",
            "Way we measure these.",
            "We could also use accuracy per token accuracy, but that there's."
        ],
        [
            "Can be quite misleading, so example this is actually not the latest results we have, but sort of an example of the kind of results you get.",
            "When we started working on this, this sort of standard sort of the numbers that people were giving for the gene protein identification we ordered the 63% recall and precision on on some standard test sets.",
            "660 four F1.",
            "When we built this conditional random field model where we use just a word identity and some spelling features, we go up to something like 81 point 20.1.",
            "When you throw in a number of other features that have to do with basically words that you know are not typical gene words, But you can do that by looking a lot of text like news text an find all these tokens that don't ever appear in a gene name and also trigrams.",
            "They typically do not appear in the gene name, so you can improve precision substantially and recall a little bit at this number is not the best you can do for this task for this particular.",
            "This particular test sets you can you know some other people with some additional feature engineering.",
            "And similar methods have got up to like 83 or something around that.",
            "Maybe there's a little bit better than that.",
            "I can't.",
            "I don't know.",
            "There's actually new evaluation coming up, a new competition coming up where we're going to try again.",
            "I will see what the numbers are on that one.",
            "So these are exact match precision recall, so why can we get better results with these methods?",
            "That basically is what causes very easy throwing a large number of different features and have the optimization algorithms find a good set of the weights setting of the ways for that.",
            "So in this is required very limited engineering.",
            "Basically this was the work of a graduates once we had all the software developed, which of course took a long time to put one graduate student, we can have to create the features that got us these results compared."
        ],
        [
            "Impact that.",
            "For the variation tagger which actually does not are not the latest numbers for these either, I just realized that I was I couldn't find the paper.",
            "We actually lost my coat.",
            "My collaborators in this project have done the separate paper where they have a different version of variation tag and there's just couldn't remember where it was.",
            "So things like state change is very simple and you get some F value of 85 when you get things like location and type you get worse reads.",
            "This is actually on the test set so that the the label data for this is actually available from our one of our websites."
        ],
        [
            "In fact, so they just give you a little bit of a.",
            "So the point of whether some of those results are so there is this.",
            "You can download the tagger itself.",
            "You can javadocs for this.",
            "We have also couple several papers that describe these results.",
            "The variation stuff that this is this is the paper and then a couple of papers on Gen protein mentions an another variation tagger paper.",
            "So there's a number of different papers that describe the specific applications.",
            "So you are welcome to download download the code, all it's all open source so you can.",
            "Mess around with."
        ],
        [
            "And finally, to give it to give you a little bit of an application here.",
            "So there's a system Fable which is built by our collaborators Children's Hospital in Philadelphia, and this is a search a search engine for all of Medline, and what it adds to something like pub.",
            "Med is simply the fact that you can type the gene name Ann, search all the documents that refer mentioned that gene, even in by a different name.",
            "So there are many different ways in which a particular gene is mentioned.",
            "So how do we do this?",
            "So the first state, so we.",
            "Process all of Medline using the gene protein tagger that I described and then they apply the different component, which I don't describe for normalize those names those mentions to find which particular gene in the.",
            "In various standard databases, that is a mention of this is mostly this is focused on human genes.",
            "We haven't done nearly as much or most reliable and unknown human genes.",
            "And so, in fact, there's some studies that this group have done.",
            "They have some references to do that work in the out of these websites that show that for a variety of areas in cancer that they work on using the search engine it could find were able to find a lot more articles relevant to their research than they could do it with Pub Med.",
            "So we have much higher recall at comparable precision then so.",
            "I mean, you cannot measure exact recall because you don't know how many artists are really relevant, But what you can do is take pub Med and how many.",
            "How many articles that you get when you type a particular query and then you take this fable, do the same and you see of the ones you get?",
            "How many?",
            "How many more you get and whether you get some that you shouldn't get and basically you have very high precision and you get a lot more articles this way using this way then you use.",
            "Just a standard pub med's keyword search."
        ],
        [
            "So there's some technical challenges and I'll sort of spend the rest of the talk mentioned a couple of techniques we've been exploring to deal with.",
            "This challenge is so we have very large number of features, so for instance, for the for one of the taggers we have things like.",
            "If the number of tests you do on the input to conjoin with state with label combinations can be as big as something like.",
            "You know in one case.",
            "Almost 4 million and the number of features you know these very large number features.",
            "That means that all of these things correspond to weights that you have to maintain.",
            "Many, many of these features are turn based, so is the word kinase.",
            "Say for gene for gene protein tag.",
            "Which leads to a very slow training or relatively slow training.",
            "So we actually explore the new set of new methods for training, which I'll mention a minute.",
            "This online methods, which are used low memory, and they're fairly fast.",
            "There's also a recent paper, they CML, which is beautiful paper on using stochastic gradient methods, training these models and which achieved very fast training on very large models, so I think that's a very competitive method.",
            "That's why Kevin Murphy and a number of other people.",
            "And the other thing is, because you have so many features, you have a potential for overfitting in the sense that these some of these features fairly rare and they may not be representative of the test data, and we played with the number of different things which I lost.",
            "Discuss in what follows.",
            "Basically, method of getting better terms or list of terms for your term based features and also using these large margin techniques or sort of Hamming.",
            "Loss was an example."
        ],
        [
            "In training.",
            "So far from the point of view of performance, and so both training performance and also is this online training methods are very attractive.",
            "So remember when I stated the training problem originally I was thinking find that we started that optimizes function.",
            "That's a general global optimization problem that optimization can be very expensive to compute, so maybe if I did one do something which is not optimal, which is suboptimal but only works with one training instance at the time.",
            "I can trade off the cost of doing this globalization against so I can get much faster learning while not not sacrificing accuracy that much.",
            "And in fact that's what we find in practice, so so so.",
            "The basic notion we use is very much like the style of a perception algorithm.",
            "We have.",
            "Some start with the weight vectors of zero and then for the number of epochs we iterate over all our training data.",
            "Train instance so classifying since I with the current weight vector produce some loss L and then update W to reduce L. Basically so you would just W so that L is going to be lower."
        ],
        [
            "And the method that we found most effective is this sort of idea.",
            "This method that was developed my in my postdoc Kobe cramming his dissertation at University for fall for classification but multiclass classification, but is also applicable here, which basically works as follows.",
            "You take what you're going to do, and I use this in words at each time.",
            "Whenever you find this, you have a training instance.",
            "You produce the its current, you have your correct labeling and you take any other labeling for that and what you want is to make sure that any other labeling.",
            "So remember in the in the Hamming loss.",
            "So we wanted basically that the advantage in favor.",
            "Of the correct labeling of the correct labeling against any competitor be at least at least the mismatch between the two of them.",
            "So we actually explicitly enforce that as a constraint in this little optimization, so I going to update W. Least amount, so I minimize the difference between UW in the old W such that every so such that Y Zhao is correctly classified as why I with an advantage with the margin, is at least the difference between the correctly.",
            "That's why I sort of the mismatch between I and I for every possible wine.",
            "Now every possible way, that's exponentially many.",
            "That's not something I can do efficiently.",
            "There are various ways you could.",
            "To solve this problem, but the simplest one we found, we just find the K best wise.",
            "The ones that score highest and enforce those constraints.",
            "As often K equals one is enough, so it just found.",
            "Find the best the best way according to the current model and you forced this and an intuitive what you want to do is to take what you do is take the the current lead vector and project it onto the subspace in which.",
            "At the.",
            "Which wire is correctly classified?",
            "And when I incorrectly means correctly and we did not margin to overcome the error between why in the competitor?",
            "OK, so that's basically the so much for the problem of the.",
            "Sorry for the methods that we use for this and this is this method is competitive in terms of accuracy, is much faster than the methods based on optimizing likelihoods that implemented in the CRF's."
        ],
        [
            "Now another sort of question is where do these identity word identity features come from?",
            "And one thing we've been playing with is the following.",
            "Suppose I have a bunch of entities of a certain type, say here company names and they have a lot of unlabeled text.",
            "So can I construct and then get more entities of that type out of the unlabeled text?",
            "OK, create a better list, and So what we do basically the way we do this is by inducing using these entities.",
            "You can do some, learn a bunch of patterns that are patterns that surround these entities.",
            "So analysts at Morgan Stanley companies such as Google, a joint venture between.",
            "Google and whatever so.",
            "Then use those patterns to propose more entities that occur in those contexts, and there's some scoring that you have to do to do this, right?"
        ],
        [
            "Um, so the general picture here is we have a seed list.",
            "We have a lot of fun labeled text.",
            "We learn to find the contexts, find the words in those contexts that indicate that are very common in context.",
            "For these, create some automata that represent those patterns and then applies automata as extractors to the unlabeled data to produce a new list, and then you take that list in the take it and use it.",
            "As a feature in that in A tag."
        ],
        [
            "So here's an example of actual patterns that you found for person names with a small seed listen instead of the people you find in the news.",
            "This is in the news.",
            "So you know, here's some actors.",
            "Here's some sports people, most golfers, maybe some people that have been given honors of various kinds.",
            "Some people have been elected to Congress and so on so forth."
        ],
        [
            "So an example of how this works.",
            "So if I have so this is particularly useful when you have very little training data labeled training data.",
            "So suppose I have just a very.",
            "This is for entity for named entity Extraction, persons, locations and organisations on the standard test set.",
            "If I have a very small amount of training data without the list like that, here's my F measure.",
            "Here's what you get with with list and.",
            "These are different two different test sets.",
            "You see a very substantial improvement, but even with a large in the largest training, much larger training set, you still get some significant statistically significant improvement.",
            "By using this induced lists.",
            "So basically what this is addressing is the fact that the word identity is you get from.",
            "Just from your training data may be too narrow, maybe two overfitting.",
            "The training that you have.",
            "And by inducing this these lists from a lot of unlabeled data, you enrich your sets of your features so that you hopefully will do better on on stuff you have unlabeled stuff later and we so we tested that hypothesis in this work and it's conclusively correct."
        ],
        [
            "So just to kind of talk about, finished with a few extensions we've developed, so I mentioned this pattern induction, which is basically focusing on reducing training data requirements.",
            "The other thing actually, one other thing which is really excited about some work that we about to present at the MLP on learning say parsers and and taggers on one domain and applying them to another domain without new labeled data.",
            "So we use this.",
            "It would have this method called structural correspondence learning, which is a method for linear classifiers which takes the lot.",
            "So you have labeled data for the main A.",
            "You have a lot of unlabeled data for them in a fun domain domain, BU as in use a certain correspondence between the features and the two domains and exploiting that correspondence you can have you can adapt.",
            "A classifier structure structure classifier that you learn from the main name to run on the main be better than it would if you didn't have any day.",
            "For.",
            "Maybe for instance we use it for part of speech, tagging of biomedical text.",
            "It will get much better with significantly better part of speech tagging this way.",
            "Then we would if we just took a particular tag trained on Wall Street Journal and Applied biomedical text, or if we take taken a or if you had trained the part of speech tagger just on the very small amount of labeled.",
            "Part of speech label training data for biomedical data.",
            "Text that we have.",
            "We also looking at another obvious direction is to look at the deeper information about the text in doing your entity extraction, particular syntactic features of various kinds.",
            "So we've done a lot of work on applying this structure, linear models to dependency parsing, and we have results for variety of languages.",
            "We show that you can have this framework without much feature engineering.",
            "In fact, no feature engineering can perform fairly well across a variety of languages.",
            "And also it's we show that we can adapt the parsers from one domain to another, again from the news text to biomedical text in a convenient way and the final kind of area, which is sort of more open is that when you try to do entity in relation extraction like actually look at the global coherence of the what the entities and the relations between them and that leads to inference.",
            "To the problem of representing the set of decisions you make in terms of labeling, those no longer form a nice chain, or must restructure.",
            "Do you have a complex graph?",
            "So the computational problem becomes much more challenging there, both on the inference and learning, but there are some interesting questions, and I think there's some.",
            "There's been some interesting progress along those lines.",
            "So that's that's all so.",
            "Question.",
            "So 'cause the?",
            "The whole approach we should describe still is pretty shallow in comparison with some.",
            "Well.",
            "Semantic approaches emerging column so possible to include some kind of semantics, deeper semantics, or channel deeper semantic development costs in some other way, as well as to improve.",
            "Certain domains better phone, so I'm not sure what you mean here.",
            "By deeper semantic analyze.",
            "Police reports they said that it would certainly help you with understanding business process a little bit better than.",
            "Just producing fan show, I mean thousands and thousands of features.",
            "Shallow features.",
            "You guys said there is still pretty shallow, mostly so first there first of all, what you're stating is a hypothesis, right?",
            "And and the question is.",
            "So let me say just one thing, so I haven't worked on that particular problem, but I've worked on the different thing which is.",
            "This sort of looking at what parsing can do for some of these things and we just have some very preliminary work, but the traditional way that people have tried to use parsing information extraction is I'm going to do this sparse tree and then I'm going to find in that part straight the pieces that say are the entities and relations of interest.",
            "I think that's actually the wrong way to go.",
            "The right way to go is to look at the parse tree as a set of additional features.",
            "The reason is the parse tree has mistakes and I don't want to commit to those mistakes, and now this is this.",
            "Early overcommitment is typically a very you know in NLP.",
            "People have very often gone to these pipeline model where I I use a pipeline of steps from shallower to deeper and I always think somehow I say I'm going to build the results for the output of the next step from the input from the input on next step is the output of the previous step.",
            "I think that's very risky way to go.",
            "You really make you because you're believing the mistakes of the early stages basically.",
            "So instead I we use is the parser is producing features that can be used for the later process, but later process he's not committed to the decisions at the parsing with, so if the parser makes lots of mistakes and the training time that's predicted features are not going to be very believed.",
            "This now can I involve larger structures so so the way I would think that I would approach to what you say is suppose I had say some rules of some rules of thumb that I had developed based on some understanding of business processes.",
            "What I would use want to use those ads as again is to suggest features that I should use for my for my tagging rather than to say that everything that I produce has to obey those rules 'cause I know people are very good at coming up with high precision rules of that kind.",
            "But very bad at producing high recall rules, so because they are high precision that great features, but they're a lousy sort of constraints at the output.",
            "Just kind of like my philosophy on this.",
            "The way you call is NP, maybe just to finish this.",
            "Call ever be.",
            "Did it work mainly by the Note 1015 years ago with this?",
            "This pipeline approach is this correct?",
            "Yeah yeah.",
            "And somehow it went very much to this very shallow stuff and now somehow it's coming back towards more surface over from this from this side.",
            "This is what I'm advocating, I mean, but I know that image, the hypothesis that has to be tested in practice.",
            "So couple of questions so.",
            "Propose that I take away the benefit.",
            "From the user declared loss between the correct, why in the computer?",
            "So if you can get there in the end function.",
            "Held between the correct Y and the best computer minus.",
            "And some competitor yeah.",
            "Well, minus I mean these basic.",
            "Let's say these are the new models, so the score tells me how much do I like this and I would just say I want to like this guy better than those are the guys sort of additive in the exponent?"
        ],
        [
            "I mean, that's just the log loss.",
            "Right, that's just for the log loss, which is not the only one.",
            "I mean, it is only one possible one, right?",
            "So this is this is just too.",
            "If this is the loss that is right.",
            "If you want to maximize the conditional probability of the correct labeling.",
            "Not necessarily the one.",
            "This is the last that I use in the case where so that online algorithm I described approximately maximizes this problem minimizes this.",
            "It doesn't exactly do so, but approximately.",
            "Talk about if I had a small level of normal extractor appealing data I could take some unstructured text to match them up to try to get contexts, national contexts and then learn more.",
            "Yes, right?",
            "So I can have C for personal names and forcing sometimes.",
            "Because you know he will be matching, you know, Michael Jordan, different context and picking up some garbage.",
            "So forwarding error types will do thing, so I'm not sure that I understand what the point is.",
            "So so we we kind of try to make sure that the context, the context that I, so the signal that I use around several that appears several times around the things that in my soon list is not.",
            "So yes, there's some filtering.",
            "I mean there's a whole paper on this that coin.",
            "Cannal just you know a month ago that I left out of some of the details you have to have some voting.",
            "You have to make sure that those Contacts are Contacts that are incredible in the sense that I used for a variety of of elements you know, often with elements of my seed list, right?",
            "If I pick the system it is very yeah.",
            "So you know, I was unfortunately this.",
            "So this is sort of it's.",
            "It's kind of so I had."
        ],
        [
            "I have this little point here.",
            "The prior knowledge part, you know it's basically that is in the sort of this Department.",
            "Here has nothing to do with this kind of ugly actually.",
            "I mean, it's a great technique and the student who started it.",
            "He really enthusiastic about developing, but but I would like to find a better way, one that is more incorporating the overall objective rather than being some kind of heuristic discovery of potential features that are valuable.",
            "I don't quite know how to do that.",
            "I would like so the other stuff I mentioned this structure correspondence for adaptation is very much fits into this framework beautifully, but this is sort of outside and it's just a tool that we use to get better features so it's bite of kind of.",
            "It's a enhancement of our prior knowledge from by exploiting a lot of unlabeled data.",
            "I think we should leave it at this because we're running a bit late, so let's think another again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well known for many things.",
                    "label": 0
                },
                {
                    "sent": "You listed some of them yesterday or day before yesterday.",
                    "label": 0
                },
                {
                    "sent": "What does it look like?",
                    "label": 0
                },
                {
                    "sent": "Programming, computational semantic, computational linguistics, information extraction?",
                    "label": 0
                },
                {
                    "sent": "Probably about.",
                    "label": 0
                },
                {
                    "sent": "Structured linear models.",
                    "label": 0
                },
                {
                    "sent": "So the latest of your activities actually.",
                    "label": 0
                },
                {
                    "sent": "Well, this is one of the latest things, but so this is I'm going to talk here about.",
                    "label": 0
                },
                {
                    "sent": "So the foundation, a number of things that they've been doing and their special in the area of parsing and information extraction.",
                    "label": 0
                },
                {
                    "sent": "But so this is very much joint work with a number of people I koby krammer asthma postdoc in my group, right?",
                    "label": 0
                },
                {
                    "sent": "McDonald's?",
                    "label": 0
                },
                {
                    "sent": "So students who just graduated PhD student in the two other PhD students they shine part to look.",
                    "label": 0
                },
                {
                    "sent": "There is many people have been helping in collaborating in this work, and I acknowledge some of them down here as well as.",
                    "label": 0
                },
                {
                    "sent": "A number of funding agencies.",
                    "label": 0
                },
                {
                    "sent": "So what this is about so?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to start for the little bit of motivation and then go into the sort of the technicalities.",
                    "label": 0
                },
                {
                    "sent": "So at one sort of 1 level, which is sort of the application oriented level.",
                    "label": 0
                },
                {
                    "sent": "We are very soon developing methods to link, document and structured databases so.",
                    "label": 1
                },
                {
                    "sent": "And nowadays you know we areas, for instance like in.",
                    "label": 0
                },
                {
                    "sent": "In biology you have a growing range of structured databases for genomic databases, for example, and there's a huge amount of literature as well, and you like to be able to search and to interrogate these together.",
                    "label": 0
                },
                {
                    "sent": "So the method that we are pursuing to achieve this goal is through techniques that collectively are known is the name information extraction and this is been mentioned this workshop several times and information extraction is sort of considered two parts, one which is to look at text and tag certain entities and relations of interest automatically.",
                    "label": 0
                },
                {
                    "sent": "And once those are tagged, you try to map them to dimensions that you've tagged two entities and relations in a database, and neither of these steps is trivial.",
                    "label": 0
                },
                {
                    "sent": "This one has to deal with a wide variability in how things I mentioned in text, and this one that white variability of how a particular entity or relation being named.",
                    "label": 0
                },
                {
                    "sent": "So there's variation, both structural and syntactic, and also.",
                    "label": 0
                },
                {
                    "sent": "Variationally terminological variation.",
                    "label": 0
                },
                {
                    "sent": "I will today be focusing on the tagging part, not on the normalization part.",
                    "label": 0
                },
                {
                    "sent": "And one of the main motivating examples, or at least one we doing a lot of these work for is for biomedical databases.",
                    "label": 0
                },
                {
                    "sent": "But some of the examples that I will use are drawn from other application.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Areas.",
                    "label": 0
                },
                {
                    "sent": "Like this one.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of slide I've carried around for a long time.",
                    "label": 0
                },
                {
                    "sent": "I don't even know where I got it from.",
                    "label": 0
                },
                {
                    "sent": "I think I may have gotten it from an interpretation of some slides from someone from BBN, so sort of the general picture information extraction for the like fraction of you that hasn't maybe less than very small fraction that hasn't seen this thing with kind of thing before is given some text.",
                    "label": 0
                },
                {
                    "sent": "I'm going to label mentions of entities in that text with.",
                    "label": 0
                },
                {
                    "sent": "According to force in certain categories that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I'm looking at for persons an organized person and organization categories, I might find this is a name of a person, and that's the name of an organization, and this big thing here, which embeds that the organization name is a descriptor for a person, and they might be.",
                    "label": 0
                },
                {
                    "sent": "You might also want to identify certain relations relations, such as we have here this person.",
                    "label": 0
                },
                {
                    "sent": "Is an employee of this organization and this person is actually the same.",
                    "label": 0
                },
                {
                    "sent": "These two mentions.",
                    "label": 0
                },
                {
                    "sent": "NASA paid consultant Apical Saturday between use actually refer to the same person, so this is sort of the high at the high level with the information extraction systems want to do is to identify these mentions and potentially some of the relations in question.",
                    "label": 1
                },
                {
                    "sent": "I will not be talking about relations much today, mostly focus, although we do some work on the relation extraction, I just had to call from.",
                    "label": 0
                },
                {
                    "sent": "The line of work we do, the things that a few things that I think will significant.",
                    "label": 0
                },
                {
                    "sent": "So I'll just focus on the tagging, the entity tagging, identifying mentions of different types of entities here.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So in the biomedical case, in fact this is from 2 project.",
                    "label": 1
                },
                {
                    "sent": "The projects that we've been working on.",
                    "label": 0
                },
                {
                    "sent": "One thing that a lot of people are interested in doing is to tag mentions of genes or the corresponding proteins in text, and you notice here that this can be fairly complex descriptions, so this is a this is a particular.",
                    "label": 0
                },
                {
                    "sent": "A particular protein that we mentioned that's kind of the witches belongs to a particular species, but the whole the descriptor is.",
                    "label": 0
                },
                {
                    "sent": "This whole thing is a multi word phrases and you like you would like to be able to do this tagging automatically and later on the figure out what is refers to.",
                    "label": 0
                },
                {
                    "sent": "Maybe by mapping this through some existing genomic database to particular standard IDs in that database.",
                    "label": 0
                },
                {
                    "sent": "The sort of relation side one of the projects we working with, working with some with some oncologists.",
                    "label": 0
                },
                {
                    "sent": "We are quite interested in identifying mutations, genetic variations that are associated to cancer in certain ways and the genetic variation event has a number of different parts.",
                    "label": 0
                },
                {
                    "sent": "For instance, here we have the state change in that variation.",
                    "label": 0
                },
                {
                    "sent": "The particular nucleotide that's changed in what type of mutation where so their location?",
                    "label": 0
                },
                {
                    "sent": "Type and state change that you have to identify to form your relation and some of these may be missing in any particular case, so these are the types of problems we're trying to solve and achieve a reasonable degree of accuracy in these so that then we can integrate the.",
                    "label": 1
                },
                {
                    "sent": "That actually, that actual information with other data with databases that exist in these researchers have developed for their own perp.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "So the general approach, which is very common for two in this area, you start by developing some text annotation guidelines.",
                    "label": 0
                },
                {
                    "sent": "So what because you're going to get some people, some individuals who are going to be paid to label some training data.",
                    "label": 0
                },
                {
                    "sent": "By hand that you can then use for machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we're going to develop some text annotations, the guidelines and get some crew of graduate students.",
                    "label": 0
                },
                {
                    "sent": "Or you know other other generally available people that can be paid the moderate amount of money to do this, to annotate initial training documents that can be then used to train machine learning algorithms for extraction.",
                    "label": 1
                },
                {
                    "sent": "And typically we're going to do then is iterative process, so you might train some algorithms, run it over some data, have people look at the results.",
                    "label": 0
                },
                {
                    "sent": "And on the base of that, recognize, for example, that the original annotation was not actually quite right, and so the algorithms are not doing so well, that's because maybe your guidelines were not sharp enough, and so you might do some awesome re annotation, correct some materials, training materials and go around the loop.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of times.",
                    "label": 0
                },
                {
                    "sent": "The way that this done, that annotation initialization is done, you will have some tool.",
                    "label": 0
                },
                {
                    "sent": "This is the one that the people that work with us at the Linguistic Data Consortium have been using.",
                    "label": 0
                },
                {
                    "sent": "They will.",
                    "label": 0
                },
                {
                    "sent": "So this is some abstract which has been medical abstracts being tagged and so here we have exactly a variation event which is translocation involving the particular igh, which is a particular.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is some mention which is a gene slash, actually a protein associated with particular gene that is in this particular chromosome and associated.",
                    "label": 0
                },
                {
                    "sent": "And this is related to a certain type of malignancy.",
                    "label": 0
                },
                {
                    "sent": "Or in fact, there's two mentions of different languages and so on, so you have different colors here, representing representing the annotations.",
                    "label": 0
                },
                {
                    "sent": "Of here of the processes for the event, the particular gene protein meant particular malignance is and they would be location information and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is given these annotations and this is being done by basically people that are, you know, graduate students in biomedical areas for this particular, then we train the taggers that will.",
                    "label": 0
                },
                {
                    "sent": "We'll discuss later.",
                    "label": 0
                },
                {
                    "sent": "Now how do we so most of the talk is going to be about the techniques we use to train this the taggers.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, you're going to thinking about and analyzing text, and there are many things you could be doing now using text from.",
                    "label": 0
                },
                {
                    "sent": "Identifying various units in text or layout, being able to parse, say, web page into elements corresponding to lists of things in the identifying each list element doing part of speech tagging or sense tagging of text, we're going to focus on information extraction.",
                    "label": 1
                },
                {
                    "sent": "Although we've done a lot of work on parsing as well that I will not discuss here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the general approach that we're going to take here is what we call or another many names that people have used for this.",
                    "label": 0
                },
                {
                    "sent": "But let's call it for the purpose of this talk, structure classification, and we have an object which typically might be a document, and you're going to want to map it or sentence within the document or paragraph within the document depends on your object type of object depends on your application, but you have some object which is a complex objects or some kind.",
                    "label": 0
                },
                {
                    "sent": "And you want to map it a learn a mapping from such objects to their structures.",
                    "label": 1
                },
                {
                    "sent": "Ann and I represented those structures.",
                    "label": 0
                },
                {
                    "sent": "Here is little trees because in at all though, for the purpose of the talk often would be more like labelings of.",
                    "label": 0
                },
                {
                    "sent": "Of sequence of words in text, but the techniques that I'm discussing here.",
                    "label": 0
                },
                {
                    "sent": "We also have said they've applied them for to parsing and so in there by using the Little Tracy.",
                    "label": 0
                },
                {
                    "sent": "I want to indicate that these techniques are quite general.",
                    "label": 0
                },
                {
                    "sent": "Then they are not just about tagging individuals in text, but can be used for for activities like parsing or relation extraction and so on.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main challenge you have to deal with and when you doing this kind of classification.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure classification, that is mapping from a complex object to a complex label, which is a representation of this truck.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That object is that decision.",
                    "label": 0
                },
                {
                    "sent": "Decisions interact.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if I decide that these two go together in some way in my structure, then that forces me that forces that one to go together with these where is there?",
                    "label": 0
                },
                {
                    "sent": "I decided to go together and that forces me, makes forces some of the decision with respect to that.",
                    "label": 0
                },
                {
                    "sent": "So the ambiguity in the phrase cap is captured by the which of these two decisions you make first, and how.",
                    "label": 0
                },
                {
                    "sent": "Interacts with the other decision.",
                    "label": 0
                },
                {
                    "sent": "Now, so you have interacting decisions then the other thing you want to the other challenge you have is that to make these decisions well, decisions design.",
                    "label": 0
                },
                {
                    "sent": "You have many types of features of the input sequence object, which in typical be a sequence that you made exploit, like in this case, the identity of the words lists of words that you dictionaries you have that tell you what the say, for instance that news might be, say, noun.",
                    "label": 0
                },
                {
                    "sent": "From the preprocessing you may do like for instance part of speech tagging is words and so on.",
                    "label": 0
                },
                {
                    "sent": "And the second challenge is that in general, although I will not discuss this much, finding all, finding an answer, finding the structure might be relatively costly.",
                    "label": 1
                },
                {
                    "sent": "You have to search over all possible structures, and typically for anything interesting, the number of structures is exponential on the on, the length of your input, and you have to find an efficient way of if you can of searching in that set of structures.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the particular setting that I will discuss today is sort of like I call analysis by tagging where you have an input sequence.",
                    "label": 1
                },
                {
                    "sent": "So my ex I use represents an input element might be, for instance a token of some kind of word.",
                    "label": 0
                },
                {
                    "sent": "You have done, some technician might be a character if you were working at a different level.",
                    "label": 0
                },
                {
                    "sent": "But let's suppose these are the size of words and you're going to apply and run that sequence to a structure classifier.",
                    "label": 0
                },
                {
                    "sent": "This is the box you want to learn to produce a another sequence of tags that label each of the individual elements of that sequence, and those tags represent.",
                    "label": 1
                },
                {
                    "sent": "What they do is encode the role of each particular element of the input sequence.",
                    "label": 1
                },
                {
                    "sent": "So for instance, being part of speech tagging, the tags are the parts of speech tags in shallow parsing, or at the segmentation tasks might be represent something or information extraction.",
                    "label": 0
                },
                {
                    "sent": "They represent some information that describes what's the role of that particular element in the extractions that you were trying to obtain.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in fact, the general sort of there's a general trick.",
                    "label": 0
                },
                {
                    "sent": "We use that to represent sort of extractions or segmentation in terms of tags, which is to have essentially suppose here I want to identify these particular elements.",
                    "label": 0
                },
                {
                    "sent": "As in terms of tagging, I will say there's A tag be that denotes the beginning of a segment, A tag.",
                    "label": 0
                },
                {
                    "sent": "I have the notes the being inside the segment, not the beginning, and then tag.",
                    "label": 0
                },
                {
                    "sent": "Oh, that represents being outside the segment, so this is a very standard encoding that you could use, for instance information extraction.",
                    "label": 0
                },
                {
                    "sent": "We have A tag says the beginning of a person name, another tag that represents the mean of the person name, and then the one I'm outside.",
                    "label": 0
                },
                {
                    "sent": "Any name?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there traditionally there have been two ways in which people have approached this problems.",
                    "label": 0
                },
                {
                    "sent": "One is the sort of the kind of family of methods called generative methods includes hidden Markov models, context, probabilistic context free grammars, and so on that are very, very elegant, but have some limits which I'll discuss later on in more detail in how to deal with in an independent features of the input.",
                    "label": 0
                },
                {
                    "sent": "And the other approach that also been quite widely used is to say, well, if I have to make all these decisions, say for each XI have to get the corresponding W. Why I why not run you some machine classification technique to learn how to put that label on that on that word?",
                    "label": 0
                },
                {
                    "sent": "So the composed the process of assigning a structure into a sequence of decisions.",
                    "label": 1
                },
                {
                    "sent": "And the main challenge with that is that you cannot really trade off if you divide the problem into problems doing individual decisions, you cannot trade off decisions at different places.",
                    "label": 0
                },
                {
                    "sent": "You cannot say well, yeah, if I make this interesting here then this way, then the decision over there is affected in a way that I may not want.",
                    "label": 0
                },
                {
                    "sent": "So doing learning so that you manage that tradeoff is something that is fundamentally the focus of the approaches will be discussing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just kind of as a reminder, just if you are doing this with a hidden this type of analysis with a hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "So the labels for the tags you want to assign, essentially the so the individual tasks are going to be the states of a hidden Markov model and the input.",
                    "label": 0
                },
                {
                    "sent": "Oh, there's enough sense that there's a little typo here.",
                    "label": 0
                },
                {
                    "sent": "This my notation, I didn't realize this, so if I my notation in.",
                    "label": 0
                },
                {
                    "sent": "How that's interesting, sorry this should be a wise.",
                    "label": 0
                },
                {
                    "sent": "This is the axis of this slide.",
                    "label": 0
                },
                {
                    "sent": "Those used before and nobody notice this.",
                    "label": 0
                },
                {
                    "sent": "I always find my own errors, which is kind of embarrassing.",
                    "label": 0
                },
                {
                    "sent": "So this is these are wise in this RX is I'm sorry for the typo here in the in the picture.",
                    "label": 0
                },
                {
                    "sent": "In the over there it's correct.",
                    "label": 0
                },
                {
                    "sent": "So basically you have wise you know.",
                    "label": 0
                },
                {
                    "sent": "So now you have to mentally after lunch.",
                    "label": 0
                },
                {
                    "sent": "This is hard mentally think why there an X here so.",
                    "label": 0
                },
                {
                    "sent": "You thinking of generating the process of generating each particular word you see given the state you are in, which represents the label that you're signing and the problem is that words have many different, so the type of this type of model has the challenge that you know.",
                    "label": 0
                },
                {
                    "sent": "There are many words, and the prediction of what word occurs at position 3 is not just dependent that say, I know that this is the beginning of a person's name.",
                    "label": 0
                },
                {
                    "sent": "I know you there are many other pieces of evidence.",
                    "label": 0
                },
                {
                    "sent": "For instance, say the word sort of there's correlations between features have to do with.",
                    "label": 0
                },
                {
                    "sent": "For instance, say things like capitalization and layout, which is very strict.",
                    "label": 0
                },
                {
                    "sent": "Conditional independence model does not allow you to capture.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still, people have used hidden Markov models for extraction, where the states represent essentially class.",
                    "label": 0
                },
                {
                    "sent": "You know, for instance here is 1 example from her, some work of Andrew McCallum and collaborators, where each state represents, so this was for parsing the title pages of documents where you have things like the title and the affiliation of the authors and the name of the author, and so on.",
                    "label": 0
                },
                {
                    "sent": "And each of these states generates then words that correspond to OK, I'm in an author position, so here's you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe John is a good word to generate.",
                    "label": 0
                },
                {
                    "sent": "And so this so this was one of the ways that people were doing information extraction like 7 eight years ago.",
                    "label": 0
                },
                {
                    "sent": "Many of the deflection system based on hidden Markov models.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But so as I was saying that the problem with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is that I'm assuming that I generate each word independently, conditional on the states that I chose, generate each word independently and that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't take into account, in fact, that there are many words features that I might be interested in dealing with, and those are not independent.",
                    "label": 0
                },
                {
                    "sent": "For instance, where the identity capitalization are not independent or wins.",
                    "label": 0
                },
                {
                    "sent": "In particular suffix anwood identity are not independent, so in general.",
                    "label": 0
                },
                {
                    "sent": "If you want to deal with the fact that there are words you've never seen, you might say I don't know that, so I've never seen the word particularly in training, but I know it ends in particular suffix so.",
                    "label": 0
                },
                {
                    "sent": "I should like that if it wins in the particular suffix, it's more likely or less likely to be the beginning sight of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A person name.",
                    "label": 0
                },
                {
                    "sent": "So one somewhere you can think of doing this is instead of generating individual words, you generate probabilistically individual features of the words.",
                    "label": 0
                },
                {
                    "sent": "So you should specially for those of you familiar those things.",
                    "label": 0
                },
                {
                    "sent": "This is like having a set of naive Bayes models, one for each position, with condition with dependencies between the labels.",
                    "label": 0
                },
                {
                    "sent": "But that actually doesn't work so well, because these particular features are not.",
                    "label": 0
                },
                {
                    "sent": "This assumes that condition on the on the label.",
                    "label": 0
                },
                {
                    "sent": "The features are independent and they are not.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given all the sort of considerations.",
                    "label": 0
                },
                {
                    "sent": "A number of others sort of gravitated towards a different way of thinking.",
                    "label": 0
                },
                {
                    "sent": "Which is the one that we use for a large range of applications that I including the extraction applications, and that's sort of the idea and this is not the way we thought of it originally, but I think this is the kind of best way to think about it.",
                    "label": 0
                },
                {
                    "sent": "At least the best way I know is to think of what we're doing is generalizing the theory and the methods of the linear classification.",
                    "label": 0
                },
                {
                    "sent": "So suppose that I have a wait, so some way capital F of taking an input X and output Y&X is going to be typically here a sequence of words saying why is sequence of labels of tags and produce some kind of a vector out of that.",
                    "label": 0
                },
                {
                    "sent": "And this bold F means a vector computed out out of all that stuff.",
                    "label": 0
                },
                {
                    "sent": "And I can also I have also some weight vector W that assigns importance to the elements of the to those features.",
                    "label": 0
                },
                {
                    "sent": "So this is the feature vector and the weights he assigned positive or negative importance to that saying I like I like it.",
                    "label": 0
                },
                {
                    "sent": "So wait is positive.",
                    "label": 0
                },
                {
                    "sent": "I like when I see that feature.",
                    "label": 0
                },
                {
                    "sent": "So I like this particular pairing of X&Y when I see that feature.",
                    "label": 0
                },
                {
                    "sent": "If the weight is negative is I don't like that particular pairing of X&Y in this is that feature.",
                    "label": 0
                },
                {
                    "sent": "And then you have some means which I won't discuss immediately of searching over all possible wise, forgiven, X and find the one that maximizes this.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I find the labeling that maximizes this and whether I can do that efficiently or not depends exactly how the labeling, huh?",
                    "label": 0
                },
                {
                    "sent": "What the form of this function F is.",
                    "label": 0
                },
                {
                    "sent": "So how do I make it something that I can computationally handle?",
                    "label": 0
                },
                {
                    "sent": "I if I can decompose this function F. So remember, X&Y are big objects into a bunch of small functions which only depend on a bounded subset of the labels.",
                    "label": 0
                },
                {
                    "sent": "And so each of these sees here represents a small group of nodes or small group of positions in X and.",
                    "label": 0
                },
                {
                    "sent": "And the dependence so that the function of these features F only depend on the labels for those positions cause that described by C. So if I can, so this is so these functions only depend on a subset of the labels, and these labels are organized in a nice way and I want, which essentially you can think of it as the this particular sees form a tree, certain tree structure.",
                    "label": 0
                },
                {
                    "sent": "Then I can use efficient decoding to find this Y to maximize that maximizes this as we can do dynamic programming over the assignments.",
                    "label": 0
                },
                {
                    "sent": "So I can basically.",
                    "label": 0
                },
                {
                    "sent": "Look at each of these local feature functions and do the dot product with W and find locally what's the maximum assignment given the assignments to the previous thing so I can start.",
                    "label": 0
                },
                {
                    "sent": "I can start here and then do the assignment for there and the assignment for there is assignment for there and I can do efficient Viterbi code.",
                    "label": 0
                },
                {
                    "sent": "So that's the general picture of the types of models we're using here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we learn these things?",
                    "label": 0
                },
                {
                    "sent": "So we have, so we have to have some prior knowledge we have to do some knowledge engineering and think of what?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the local domains of an input, what a disease that that we can get our.",
                    "label": 0
                },
                {
                    "sent": "Are we going to use an?",
                    "label": 0
                },
                {
                    "sent": "We are always motivated by trying to find local domains that make the problem of doing this maximization efficient.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we we come up with local feature functions that essentially capture the features of the input and of the labels that we think are informative about about the problem of choosing the right labels.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do is to adjust W this weight vector W to Max optimize some objective function that we believe.",
                    "label": 1
                },
                {
                    "sent": "For the notion of generalization to unseen data with low error and the sort of general form of that tends to be something like this, so we have something that tells us that we don't want to wait vector to be too wild.",
                    "label": 0
                },
                {
                    "sent": "So basically the for instance here might be that the square norm of the way vector is.",
                    "label": 0
                },
                {
                    "sent": "It's going to be small with some multiplier Bellevue, and that the error.",
                    "label": 0
                },
                {
                    "sent": "So this is L is called the loss, which basically says that the the training time I do well so L if L is high for a particular training instance subscript I.",
                    "label": 0
                },
                {
                    "sent": "That means that if this is high means that that weight vector is not good at producing the intended output Yi for the given input XI.",
                    "label": 0
                },
                {
                    "sent": "So we have a labeling, labeling, training set of pairs, xiy I and I want to my W star be such that it makes the some of these things small, so this is hopefully this is small and that is small.",
                    "label": 0
                },
                {
                    "sent": "So and Lambda is a trade off parameter between those two things.",
                    "label": 0
                },
                {
                    "sent": "So I want to his weight vector to be small and I want the error the loss of each my www.star.",
                    "label": 0
                },
                {
                    "sent": "Gives me when I applied to wins used to classify XI into some particular why I want those to be small, so that's the general yes.",
                    "label": 0
                },
                {
                    "sent": "Are there local domains saying for features or they can be different for different features?",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "Yes, in general we have different.",
                    "label": 0
                },
                {
                    "sent": "So for example, for example use.",
                    "label": 0
                },
                {
                    "sent": "We might for certain features, a local domain might be a single label, so a single position.",
                    "label": 0
                },
                {
                    "sent": "So for instance you have features that have to do with word identity that you might just say I want only to look at.",
                    "label": 0
                },
                {
                    "sent": "What I did is positioned for other features I might want to say.",
                    "label": 0
                },
                {
                    "sent": "I want to look at for instance part of speech tag information for two consecutive positions and the corresponding labels.",
                    "label": 0
                },
                {
                    "sent": "So the domain there will be larger.",
                    "label": 0
                },
                {
                    "sent": "So in general you combine you have features just in practice you tend to make have features that are very specialized, very precise, but which only look at one.",
                    "label": 0
                },
                {
                    "sent": "Say for instance one label and then features which have little bit more less specific that look at larger Windows.",
                    "label": 0
                },
                {
                    "sent": "And the reason you want to do that is the cause.",
                    "label": 0
                },
                {
                    "sent": "If you say you have a very specific feature that is looking at large window be cause there are so many combinations of possible labels.",
                    "label": 0
                },
                {
                    "sent": "The probability you will, in the training that you might never see particular combinations.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just use some notation here so.",
                    "label": 0
                },
                {
                    "sent": "Given a particular two, can a target so we have an input X and I want to produce a target structure Y.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose you have a competitor.",
                    "label": 0
                },
                {
                    "sent": "Why prime this quantity here, which called the margin, is advantage in favor of.",
                    "label": 0
                },
                {
                    "sent": "Why against Y prime?",
                    "label": 0
                },
                {
                    "sent": "So if this is positive, it means that the my current W likes.",
                    "label": 0
                },
                {
                    "sent": "Why better than Y prime?",
                    "label": 0
                },
                {
                    "sent": "If this is negative, means that my W likes, why less than Y prime?",
                    "label": 0
                },
                {
                    "sent": "So in general, if Y is the correct labeling and why prime is incorrect competitor?",
                    "label": 0
                },
                {
                    "sent": "I want this kind of thing to be positive.",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm going to be training the kind of intuitively my training algorithm want to do is to make the margin positive, where, when, for the correct labeling against incorrect labeling that many ways of trying to do that, and I won't have much time to go into.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Details of the various ways you can do it.",
                    "label": 0
                },
                {
                    "sent": "But the sort of two main ways that we use in our work, one is to essentially maximize the probability of the correct output, and for that the loss that you're going to use.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you do the little algebra that you need is the negative.",
                    "label": 0
                },
                {
                    "sent": "So because you're trying to minimize is the minus lock probability of the particular of the intended output Y against all possible other outputs Y prime.",
                    "label": 0
                },
                {
                    "sent": "So you basically want you want to say I want Y to be better.",
                    "label": 0
                },
                {
                    "sent": "So notice that if why is it gets better score than Y prime, then this number here is positive.",
                    "label": 0
                },
                {
                    "sent": "So this exponent is negative.",
                    "label": 0
                },
                {
                    "sent": "So this is a small number, so the loss is going to be small.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if Y prime has higher, probably has higher score than Y, then this is going to be a number large number larger than one.",
                    "label": 0
                },
                {
                    "sent": "So this log is going to be.",
                    "label": 0
                },
                {
                    "sent": "Larger than, larger than zero.",
                    "label": 0
                },
                {
                    "sent": "So you basically ended.",
                    "label": 0
                },
                {
                    "sent": "This is always bounded below by zero Becausw.",
                    "label": 0
                },
                {
                    "sent": "One of the possible I primes is why itself, which makes this mix is 1 and therefore this log is 0, so this is a non negative quantity and the worst the higher it is the more the indicates that why the sort of the probability mass that why gets relative to the rest of the competitors is lower and lower.",
                    "label": 0
                },
                {
                    "sent": "So you want to make that small to maximize the probability of the correct help.",
                    "label": 0
                },
                {
                    "sent": "Another loss function that is often used which is sort of a.",
                    "label": 0
                },
                {
                    "sent": "Which basically is trying to say is I want to.",
                    "label": 0
                },
                {
                    "sent": "So if Y gets if the margin between Y&Y prime is small, in fact even worse if it is negative.",
                    "label": 0
                },
                {
                    "sent": "So if this is negative then this is a positive number an let's say this here be the disagreement between Y&Y Prime and this agreement may be measuring many different ways than we typically measure it.",
                    "label": 0
                },
                {
                    "sent": "In the Hamming loss disagreement, to say if two labels are so you are looking at two sequences in the two labels are difference Taipei 1 if they are the same.",
                    "label": 0
                },
                {
                    "sent": "I pay 0, so it's a number of errors in the sequence, so that means that these numbers positive.",
                    "label": 0
                },
                {
                    "sent": "This means the positive parts, so I Max.",
                    "label": 0
                },
                {
                    "sent": "So I'm finding the Y prime that basically the Y prime that is most misclassified one that basically is wrong.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, he's on the wrong side, is it gets a score that is greater than Y?",
                    "label": 0
                },
                {
                    "sent": "So I basically 5 make this number small.",
                    "label": 0
                },
                {
                    "sent": "That means that everything every Y prime.",
                    "label": 0
                },
                {
                    "sent": "So if this number is 0 means that every candidate at every competitor is going to be have a score lower than Y, and how much lower it is depends?",
                    "label": 0
                },
                {
                    "sent": "How wrong what this competitor is?",
                    "label": 0
                },
                {
                    "sent": "So if this competitor is so, how wrong it is?",
                    "label": 0
                },
                {
                    "sent": "It's measured by this, so if Y prime is just one one off, so it's one label that's incorrect, then this number needs to be 1 S. If Y prime is say, 1010 labels are wrong, then this number needs to be 10 to make the last zero.",
                    "label": 0
                },
                {
                    "sent": "So you see that basically the further why prime is from Y in terms of error.",
                    "label": 0
                },
                {
                    "sent": "The more I want the score of why prime to be below the score of Y, so these are two different losses that we use in different in different experiments and basically in both cases I could need to search over Y prime again if I if I have a nicely behaved graph I could do dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "If not I have problems that I don't want to discuss that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unless I have, people can ask me later if you have time.",
                    "label": 0
                },
                {
                    "sent": "Why do we do this?",
                    "label": 0
                },
                {
                    "sent": "So basically by doing looking at these as a global optimization problem, we can trade off labeling decisions at different positions in the input 'cause we are looking at the entire input sequence in the entire output sequence.",
                    "label": 1
                },
                {
                    "sent": "Although we can we exploit some decomposition in terms of local domains, we don't have any issues about what kind of features to use.",
                    "label": 0
                },
                {
                    "sent": "We can anything can go into those features, functions and it's modular approach in that first of all, the scoring can be factor according to the local domains.",
                    "label": 0
                },
                {
                    "sent": "And the last you can choose a loss function that satisfies that is appropriate to your application.",
                    "label": 0
                },
                {
                    "sent": "So there's a general way of thinking about these types of problems that come up in information extraction.",
                    "label": 0
                },
                {
                    "sent": "We've used it for parsing for relation extraction and and many other people use for many, many other applications.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the probabilistic version, which is in fact the one we developed.",
                    "label": 1
                },
                {
                    "sent": "First we used, gave it the number of conditional random fields and basically what you do is you apply a softmax that scoring function to produce a conditional probability of a labeling given an input.",
                    "label": 1
                },
                {
                    "sent": "And so you basically you take the exponent and apply normalization an for the sequential case, say the first or the sequential case.",
                    "label": 0
                },
                {
                    "sent": "What we do is we make the features.",
                    "label": 0
                },
                {
                    "sent": "The feature functions depend just on two consecutive labels, so that's the 1st order case, so your local domains at most involved two consecutive labels.",
                    "label": 0
                },
                {
                    "sent": "They might look at anything in the input they care about, because the input is fixed and the user training criterion the log loss.",
                    "label": 1
                },
                {
                    "sent": "So basically you maximize the probability of the label sequence training label sequence given the corresponding input.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So those are the math.",
                    "label": 0
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "The general methods are now now that I've introduced the methods, I'm going to talk a little bit about the applications.",
                    "label": 0
                },
                {
                    "sent": "So for information extraction applications we tend to what features do you tend to use?",
                    "label": 0
                },
                {
                    "sent": "We'll sort of basically conjunctions of two things.",
                    "label": 1
                },
                {
                    "sent": "One is the configuration of labels.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in a case like this would be the choice of what tag you have for why the previous tag and the current tag.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An answer in properties of the input might be at the identity of the terms membership of the term list.",
                    "label": 0
                },
                {
                    "sent": "So for instance of these words is a first name in the American census listing.",
                    "label": 0
                },
                {
                    "sent": "Orthographic patterns, like for instance suffixes.",
                    "label": 1
                },
                {
                    "sent": "Capitalization like an conjunctions of these for the current and surrounding words.",
                    "label": 1
                },
                {
                    "sent": "Now if you start looking at larger and larger conjunctions, this could be many many many different.",
                    "label": 0
                },
                {
                    "sent": "You can create a very large number of features and so one option that we've used in some situations is work that Andrew McCallum did, and we've used for certain applications is that you generate only those features that help prediction, so you have a greedy algorithm that generates conjunctions.",
                    "label": 0
                },
                {
                    "sent": "Calling insofar as those conjunctions can be validated in the training data by improving the classification accuracy in a training date.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a lot of this technology, especially the probabilistic, this conditional random fields is implemented actually in a package called mallet kind of URL down there, which is available freely on an open source license out of University Massachusetts.",
                    "label": 0
                },
                {
                    "sent": "So Andrew and his students have done a lot.",
                    "label": 0
                },
                {
                    "sent": "Most of the implementation we have introduced, we implemented some particular learning algorithms, some variants.",
                    "label": 0
                },
                {
                    "sent": "Some particular pieces of the conditional random field package and a few other things, and we constantly adding to it so this is open source limited documentation.",
                    "label": 0
                },
                {
                    "sent": "We also have actually put a.",
                    "label": 0
                },
                {
                    "sent": "We added this scripting layer to it, so you can actually create your own extract as easily by writing a Python script that drives.",
                    "label": 0
                },
                {
                    "sent": "This is all written in Java and we use Jython which is a Python interpreter in Java to which.",
                    "label": 0
                },
                {
                    "sent": "Can orchestrate from a higher level.",
                    "label": 0
                },
                {
                    "sent": "This is very large software package and we have some tutorial.",
                    "label": 0
                },
                {
                    "sent": "A couple tutorials that explain how to do information setup is simply an information extraction sort of pipeline using that sort of scripting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "So typically people evaluate is very much like information retrieval.",
                    "label": 0
                },
                {
                    "sent": "We evaluate information extraction in terms of precision and recall.",
                    "label": 0
                },
                {
                    "sent": "These are exact matches.",
                    "label": 0
                },
                {
                    "sent": "So when I say what proportion of predicting an exact correct with exact boundaries.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking at the mentions and I want to know whether they have the exact correct boundaries and recall what what proportion of the entities are predicted and usual F1 measure.",
                    "label": 1
                },
                {
                    "sent": "So this is sort of typical.",
                    "label": 0
                },
                {
                    "sent": "Way we measure these.",
                    "label": 0
                },
                {
                    "sent": "We could also use accuracy per token accuracy, but that there's.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can be quite misleading, so example this is actually not the latest results we have, but sort of an example of the kind of results you get.",
                    "label": 0
                },
                {
                    "sent": "When we started working on this, this sort of standard sort of the numbers that people were giving for the gene protein identification we ordered the 63% recall and precision on on some standard test sets.",
                    "label": 0
                },
                {
                    "sent": "660 four F1.",
                    "label": 0
                },
                {
                    "sent": "When we built this conditional random field model where we use just a word identity and some spelling features, we go up to something like 81 point 20.1.",
                    "label": 0
                },
                {
                    "sent": "When you throw in a number of other features that have to do with basically words that you know are not typical gene words, But you can do that by looking a lot of text like news text an find all these tokens that don't ever appear in a gene name and also trigrams.",
                    "label": 0
                },
                {
                    "sent": "They typically do not appear in the gene name, so you can improve precision substantially and recall a little bit at this number is not the best you can do for this task for this particular.",
                    "label": 0
                },
                {
                    "sent": "This particular test sets you can you know some other people with some additional feature engineering.",
                    "label": 0
                },
                {
                    "sent": "And similar methods have got up to like 83 or something around that.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a little bit better than that.",
                    "label": 0
                },
                {
                    "sent": "I can't.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "There's actually new evaluation coming up, a new competition coming up where we're going to try again.",
                    "label": 0
                },
                {
                    "sent": "I will see what the numbers are on that one.",
                    "label": 0
                },
                {
                    "sent": "So these are exact match precision recall, so why can we get better results with these methods?",
                    "label": 1
                },
                {
                    "sent": "That basically is what causes very easy throwing a large number of different features and have the optimization algorithms find a good set of the weights setting of the ways for that.",
                    "label": 0
                },
                {
                    "sent": "So in this is required very limited engineering.",
                    "label": 0
                },
                {
                    "sent": "Basically this was the work of a graduates once we had all the software developed, which of course took a long time to put one graduate student, we can have to create the features that got us these results compared.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Impact that.",
                    "label": 0
                },
                {
                    "sent": "For the variation tagger which actually does not are not the latest numbers for these either, I just realized that I was I couldn't find the paper.",
                    "label": 0
                },
                {
                    "sent": "We actually lost my coat.",
                    "label": 0
                },
                {
                    "sent": "My collaborators in this project have done the separate paper where they have a different version of variation tag and there's just couldn't remember where it was.",
                    "label": 0
                },
                {
                    "sent": "So things like state change is very simple and you get some F value of 85 when you get things like location and type you get worse reads.",
                    "label": 0
                },
                {
                    "sent": "This is actually on the test set so that the the label data for this is actually available from our one of our websites.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, so they just give you a little bit of a.",
                    "label": 0
                },
                {
                    "sent": "So the point of whether some of those results are so there is this.",
                    "label": 0
                },
                {
                    "sent": "You can download the tagger itself.",
                    "label": 0
                },
                {
                    "sent": "You can javadocs for this.",
                    "label": 0
                },
                {
                    "sent": "We have also couple several papers that describe these results.",
                    "label": 0
                },
                {
                    "sent": "The variation stuff that this is this is the paper and then a couple of papers on Gen protein mentions an another variation tagger paper.",
                    "label": 0
                },
                {
                    "sent": "So there's a number of different papers that describe the specific applications.",
                    "label": 0
                },
                {
                    "sent": "So you are welcome to download download the code, all it's all open source so you can.",
                    "label": 0
                },
                {
                    "sent": "Mess around with.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, to give it to give you a little bit of an application here.",
                    "label": 0
                },
                {
                    "sent": "So there's a system Fable which is built by our collaborators Children's Hospital in Philadelphia, and this is a search a search engine for all of Medline, and what it adds to something like pub.",
                    "label": 0
                },
                {
                    "sent": "Med is simply the fact that you can type the gene name Ann, search all the documents that refer mentioned that gene, even in by a different name.",
                    "label": 0
                },
                {
                    "sent": "So there are many different ways in which a particular gene is mentioned.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "So the first state, so we.",
                    "label": 0
                },
                {
                    "sent": "Process all of Medline using the gene protein tagger that I described and then they apply the different component, which I don't describe for normalize those names those mentions to find which particular gene in the.",
                    "label": 0
                },
                {
                    "sent": "In various standard databases, that is a mention of this is mostly this is focused on human genes.",
                    "label": 0
                },
                {
                    "sent": "We haven't done nearly as much or most reliable and unknown human genes.",
                    "label": 0
                },
                {
                    "sent": "And so, in fact, there's some studies that this group have done.",
                    "label": 0
                },
                {
                    "sent": "They have some references to do that work in the out of these websites that show that for a variety of areas in cancer that they work on using the search engine it could find were able to find a lot more articles relevant to their research than they could do it with Pub Med.",
                    "label": 0
                },
                {
                    "sent": "So we have much higher recall at comparable precision then so.",
                    "label": 0
                },
                {
                    "sent": "I mean, you cannot measure exact recall because you don't know how many artists are really relevant, But what you can do is take pub Med and how many.",
                    "label": 0
                },
                {
                    "sent": "How many articles that you get when you type a particular query and then you take this fable, do the same and you see of the ones you get?",
                    "label": 0
                },
                {
                    "sent": "How many?",
                    "label": 0
                },
                {
                    "sent": "How many more you get and whether you get some that you shouldn't get and basically you have very high precision and you get a lot more articles this way using this way then you use.",
                    "label": 0
                },
                {
                    "sent": "Just a standard pub med's keyword search.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's some technical challenges and I'll sort of spend the rest of the talk mentioned a couple of techniques we've been exploring to deal with.",
                    "label": 0
                },
                {
                    "sent": "This challenge is so we have very large number of features, so for instance, for the for one of the taggers we have things like.",
                    "label": 0
                },
                {
                    "sent": "If the number of tests you do on the input to conjoin with state with label combinations can be as big as something like.",
                    "label": 0
                },
                {
                    "sent": "You know in one case.",
                    "label": 0
                },
                {
                    "sent": "Almost 4 million and the number of features you know these very large number features.",
                    "label": 1
                },
                {
                    "sent": "That means that all of these things correspond to weights that you have to maintain.",
                    "label": 0
                },
                {
                    "sent": "Many, many of these features are turn based, so is the word kinase.",
                    "label": 0
                },
                {
                    "sent": "Say for gene for gene protein tag.",
                    "label": 1
                },
                {
                    "sent": "Which leads to a very slow training or relatively slow training.",
                    "label": 1
                },
                {
                    "sent": "So we actually explore the new set of new methods for training, which I'll mention a minute.",
                    "label": 0
                },
                {
                    "sent": "This online methods, which are used low memory, and they're fairly fast.",
                    "label": 0
                },
                {
                    "sent": "There's also a recent paper, they CML, which is beautiful paper on using stochastic gradient methods, training these models and which achieved very fast training on very large models, so I think that's a very competitive method.",
                    "label": 0
                },
                {
                    "sent": "That's why Kevin Murphy and a number of other people.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, because you have so many features, you have a potential for overfitting in the sense that these some of these features fairly rare and they may not be representative of the test data, and we played with the number of different things which I lost.",
                    "label": 0
                },
                {
                    "sent": "Discuss in what follows.",
                    "label": 0
                },
                {
                    "sent": "Basically, method of getting better terms or list of terms for your term based features and also using these large margin techniques or sort of Hamming.",
                    "label": 0
                },
                {
                    "sent": "Loss was an example.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In training.",
                    "label": 0
                },
                {
                    "sent": "So far from the point of view of performance, and so both training performance and also is this online training methods are very attractive.",
                    "label": 0
                },
                {
                    "sent": "So remember when I stated the training problem originally I was thinking find that we started that optimizes function.",
                    "label": 0
                },
                {
                    "sent": "That's a general global optimization problem that optimization can be very expensive to compute, so maybe if I did one do something which is not optimal, which is suboptimal but only works with one training instance at the time.",
                    "label": 0
                },
                {
                    "sent": "I can trade off the cost of doing this globalization against so I can get much faster learning while not not sacrificing accuracy that much.",
                    "label": 0
                },
                {
                    "sent": "And in fact that's what we find in practice, so so so.",
                    "label": 0
                },
                {
                    "sent": "The basic notion we use is very much like the style of a perception algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Some start with the weight vectors of zero and then for the number of epochs we iterate over all our training data.",
                    "label": 0
                },
                {
                    "sent": "Train instance so classifying since I with the current weight vector produce some loss L and then update W to reduce L. Basically so you would just W so that L is going to be lower.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the method that we found most effective is this sort of idea.",
                    "label": 0
                },
                {
                    "sent": "This method that was developed my in my postdoc Kobe cramming his dissertation at University for fall for classification but multiclass classification, but is also applicable here, which basically works as follows.",
                    "label": 0
                },
                {
                    "sent": "You take what you're going to do, and I use this in words at each time.",
                    "label": 0
                },
                {
                    "sent": "Whenever you find this, you have a training instance.",
                    "label": 0
                },
                {
                    "sent": "You produce the its current, you have your correct labeling and you take any other labeling for that and what you want is to make sure that any other labeling.",
                    "label": 0
                },
                {
                    "sent": "So remember in the in the Hamming loss.",
                    "label": 1
                },
                {
                    "sent": "So we wanted basically that the advantage in favor.",
                    "label": 0
                },
                {
                    "sent": "Of the correct labeling of the correct labeling against any competitor be at least at least the mismatch between the two of them.",
                    "label": 0
                },
                {
                    "sent": "So we actually explicitly enforce that as a constraint in this little optimization, so I going to update W. Least amount, so I minimize the difference between UW in the old W such that every so such that Y Zhao is correctly classified as why I with an advantage with the margin, is at least the difference between the correctly.",
                    "label": 0
                },
                {
                    "sent": "That's why I sort of the mismatch between I and I for every possible wine.",
                    "label": 0
                },
                {
                    "sent": "Now every possible way, that's exponentially many.",
                    "label": 1
                },
                {
                    "sent": "That's not something I can do efficiently.",
                    "label": 0
                },
                {
                    "sent": "There are various ways you could.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem, but the simplest one we found, we just find the K best wise.",
                    "label": 0
                },
                {
                    "sent": "The ones that score highest and enforce those constraints.",
                    "label": 0
                },
                {
                    "sent": "As often K equals one is enough, so it just found.",
                    "label": 0
                },
                {
                    "sent": "Find the best the best way according to the current model and you forced this and an intuitive what you want to do is to take what you do is take the the current lead vector and project it onto the subspace in which.",
                    "label": 0
                },
                {
                    "sent": "At the.",
                    "label": 0
                },
                {
                    "sent": "Which wire is correctly classified?",
                    "label": 0
                },
                {
                    "sent": "And when I incorrectly means correctly and we did not margin to overcome the error between why in the competitor?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically the so much for the problem of the.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the methods that we use for this and this is this method is competitive in terms of accuracy, is much faster than the methods based on optimizing likelihoods that implemented in the CRF's.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now another sort of question is where do these identity word identity features come from?",
                    "label": 0
                },
                {
                    "sent": "And one thing we've been playing with is the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a bunch of entities of a certain type, say here company names and they have a lot of unlabeled text.",
                    "label": 0
                },
                {
                    "sent": "So can I construct and then get more entities of that type out of the unlabeled text?",
                    "label": 0
                },
                {
                    "sent": "OK, create a better list, and So what we do basically the way we do this is by inducing using these entities.",
                    "label": 0
                },
                {
                    "sent": "You can do some, learn a bunch of patterns that are patterns that surround these entities.",
                    "label": 0
                },
                {
                    "sent": "So analysts at Morgan Stanley companies such as Google, a joint venture between.",
                    "label": 1
                },
                {
                    "sent": "Google and whatever so.",
                    "label": 0
                },
                {
                    "sent": "Then use those patterns to propose more entities that occur in those contexts, and there's some scoring that you have to do to do this, right?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, so the general picture here is we have a seed list.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of fun labeled text.",
                    "label": 0
                },
                {
                    "sent": "We learn to find the contexts, find the words in those contexts that indicate that are very common in context.",
                    "label": 0
                },
                {
                    "sent": "For these, create some automata that represent those patterns and then applies automata as extractors to the unlabeled data to produce a new list, and then you take that list in the take it and use it.",
                    "label": 1
                },
                {
                    "sent": "As a feature in that in A tag.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of actual patterns that you found for person names with a small seed listen instead of the people you find in the news.",
                    "label": 0
                },
                {
                    "sent": "This is in the news.",
                    "label": 0
                },
                {
                    "sent": "So you know, here's some actors.",
                    "label": 0
                },
                {
                    "sent": "Here's some sports people, most golfers, maybe some people that have been given honors of various kinds.",
                    "label": 0
                },
                {
                    "sent": "Some people have been elected to Congress and so on so forth.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an example of how this works.",
                    "label": 0
                },
                {
                    "sent": "So if I have so this is particularly useful when you have very little training data labeled training data.",
                    "label": 0
                },
                {
                    "sent": "So suppose I have just a very.",
                    "label": 0
                },
                {
                    "sent": "This is for entity for named entity Extraction, persons, locations and organisations on the standard test set.",
                    "label": 0
                },
                {
                    "sent": "If I have a very small amount of training data without the list like that, here's my F measure.",
                    "label": 0
                },
                {
                    "sent": "Here's what you get with with list and.",
                    "label": 0
                },
                {
                    "sent": "These are different two different test sets.",
                    "label": 0
                },
                {
                    "sent": "You see a very substantial improvement, but even with a large in the largest training, much larger training set, you still get some significant statistically significant improvement.",
                    "label": 0
                },
                {
                    "sent": "By using this induced lists.",
                    "label": 0
                },
                {
                    "sent": "So basically what this is addressing is the fact that the word identity is you get from.",
                    "label": 0
                },
                {
                    "sent": "Just from your training data may be too narrow, maybe two overfitting.",
                    "label": 0
                },
                {
                    "sent": "The training that you have.",
                    "label": 0
                },
                {
                    "sent": "And by inducing this these lists from a lot of unlabeled data, you enrich your sets of your features so that you hopefully will do better on on stuff you have unlabeled stuff later and we so we tested that hypothesis in this work and it's conclusively correct.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to kind of talk about, finished with a few extensions we've developed, so I mentioned this pattern induction, which is basically focusing on reducing training data requirements.",
                    "label": 1
                },
                {
                    "sent": "The other thing actually, one other thing which is really excited about some work that we about to present at the MLP on learning say parsers and and taggers on one domain and applying them to another domain without new labeled data.",
                    "label": 0
                },
                {
                    "sent": "So we use this.",
                    "label": 1
                },
                {
                    "sent": "It would have this method called structural correspondence learning, which is a method for linear classifiers which takes the lot.",
                    "label": 0
                },
                {
                    "sent": "So you have labeled data for the main A.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of unlabeled data for them in a fun domain domain, BU as in use a certain correspondence between the features and the two domains and exploiting that correspondence you can have you can adapt.",
                    "label": 0
                },
                {
                    "sent": "A classifier structure structure classifier that you learn from the main name to run on the main be better than it would if you didn't have any day.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Maybe for instance we use it for part of speech, tagging of biomedical text.",
                    "label": 0
                },
                {
                    "sent": "It will get much better with significantly better part of speech tagging this way.",
                    "label": 0
                },
                {
                    "sent": "Then we would if we just took a particular tag trained on Wall Street Journal and Applied biomedical text, or if we take taken a or if you had trained the part of speech tagger just on the very small amount of labeled.",
                    "label": 0
                },
                {
                    "sent": "Part of speech label training data for biomedical data.",
                    "label": 0
                },
                {
                    "sent": "Text that we have.",
                    "label": 0
                },
                {
                    "sent": "We also looking at another obvious direction is to look at the deeper information about the text in doing your entity extraction, particular syntactic features of various kinds.",
                    "label": 0
                },
                {
                    "sent": "So we've done a lot of work on applying this structure, linear models to dependency parsing, and we have results for variety of languages.",
                    "label": 0
                },
                {
                    "sent": "We show that you can have this framework without much feature engineering.",
                    "label": 0
                },
                {
                    "sent": "In fact, no feature engineering can perform fairly well across a variety of languages.",
                    "label": 0
                },
                {
                    "sent": "And also it's we show that we can adapt the parsers from one domain to another, again from the news text to biomedical text in a convenient way and the final kind of area, which is sort of more open is that when you try to do entity in relation extraction like actually look at the global coherence of the what the entities and the relations between them and that leads to inference.",
                    "label": 0
                },
                {
                    "sent": "To the problem of representing the set of decisions you make in terms of labeling, those no longer form a nice chain, or must restructure.",
                    "label": 0
                },
                {
                    "sent": "Do you have a complex graph?",
                    "label": 0
                },
                {
                    "sent": "So the computational problem becomes much more challenging there, both on the inference and learning, but there are some interesting questions, and I think there's some.",
                    "label": 0
                },
                {
                    "sent": "There's been some interesting progress along those lines.",
                    "label": 0
                },
                {
                    "sent": "So that's that's all so.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So 'cause the?",
                    "label": 0
                },
                {
                    "sent": "The whole approach we should describe still is pretty shallow in comparison with some.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Semantic approaches emerging column so possible to include some kind of semantics, deeper semantics, or channel deeper semantic development costs in some other way, as well as to improve.",
                    "label": 0
                },
                {
                    "sent": "Certain domains better phone, so I'm not sure what you mean here.",
                    "label": 0
                },
                {
                    "sent": "By deeper semantic analyze.",
                    "label": 0
                },
                {
                    "sent": "Police reports they said that it would certainly help you with understanding business process a little bit better than.",
                    "label": 0
                },
                {
                    "sent": "Just producing fan show, I mean thousands and thousands of features.",
                    "label": 0
                },
                {
                    "sent": "Shallow features.",
                    "label": 0
                },
                {
                    "sent": "You guys said there is still pretty shallow, mostly so first there first of all, what you're stating is a hypothesis, right?",
                    "label": 0
                },
                {
                    "sent": "And and the question is.",
                    "label": 0
                },
                {
                    "sent": "So let me say just one thing, so I haven't worked on that particular problem, but I've worked on the different thing which is.",
                    "label": 0
                },
                {
                    "sent": "This sort of looking at what parsing can do for some of these things and we just have some very preliminary work, but the traditional way that people have tried to use parsing information extraction is I'm going to do this sparse tree and then I'm going to find in that part straight the pieces that say are the entities and relations of interest.",
                    "label": 0
                },
                {
                    "sent": "I think that's actually the wrong way to go.",
                    "label": 0
                },
                {
                    "sent": "The right way to go is to look at the parse tree as a set of additional features.",
                    "label": 0
                },
                {
                    "sent": "The reason is the parse tree has mistakes and I don't want to commit to those mistakes, and now this is this.",
                    "label": 0
                },
                {
                    "sent": "Early overcommitment is typically a very you know in NLP.",
                    "label": 0
                },
                {
                    "sent": "People have very often gone to these pipeline model where I I use a pipeline of steps from shallower to deeper and I always think somehow I say I'm going to build the results for the output of the next step from the input from the input on next step is the output of the previous step.",
                    "label": 0
                },
                {
                    "sent": "I think that's very risky way to go.",
                    "label": 0
                },
                {
                    "sent": "You really make you because you're believing the mistakes of the early stages basically.",
                    "label": 0
                },
                {
                    "sent": "So instead I we use is the parser is producing features that can be used for the later process, but later process he's not committed to the decisions at the parsing with, so if the parser makes lots of mistakes and the training time that's predicted features are not going to be very believed.",
                    "label": 0
                },
                {
                    "sent": "This now can I involve larger structures so so the way I would think that I would approach to what you say is suppose I had say some rules of some rules of thumb that I had developed based on some understanding of business processes.",
                    "label": 0
                },
                {
                    "sent": "What I would use want to use those ads as again is to suggest features that I should use for my for my tagging rather than to say that everything that I produce has to obey those rules 'cause I know people are very good at coming up with high precision rules of that kind.",
                    "label": 0
                },
                {
                    "sent": "But very bad at producing high recall rules, so because they are high precision that great features, but they're a lousy sort of constraints at the output.",
                    "label": 0
                },
                {
                    "sent": "Just kind of like my philosophy on this.",
                    "label": 0
                },
                {
                    "sent": "The way you call is NP, maybe just to finish this.",
                    "label": 0
                },
                {
                    "sent": "Call ever be.",
                    "label": 0
                },
                {
                    "sent": "Did it work mainly by the Note 1015 years ago with this?",
                    "label": 0
                },
                {
                    "sent": "This pipeline approach is this correct?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And somehow it went very much to this very shallow stuff and now somehow it's coming back towards more surface over from this from this side.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm advocating, I mean, but I know that image, the hypothesis that has to be tested in practice.",
                    "label": 0
                },
                {
                    "sent": "So couple of questions so.",
                    "label": 0
                },
                {
                    "sent": "Propose that I take away the benefit.",
                    "label": 0
                },
                {
                    "sent": "From the user declared loss between the correct, why in the computer?",
                    "label": 0
                },
                {
                    "sent": "So if you can get there in the end function.",
                    "label": 0
                },
                {
                    "sent": "Held between the correct Y and the best computer minus.",
                    "label": 0
                },
                {
                    "sent": "And some competitor yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, minus I mean these basic.",
                    "label": 0
                },
                {
                    "sent": "Let's say these are the new models, so the score tells me how much do I like this and I would just say I want to like this guy better than those are the guys sort of additive in the exponent?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean, that's just the log loss.",
                    "label": 1
                },
                {
                    "sent": "Right, that's just for the log loss, which is not the only one.",
                    "label": 0
                },
                {
                    "sent": "I mean, it is only one possible one, right?",
                    "label": 0
                },
                {
                    "sent": "So this is this is just too.",
                    "label": 0
                },
                {
                    "sent": "If this is the loss that is right.",
                    "label": 1
                },
                {
                    "sent": "If you want to maximize the conditional probability of the correct labeling.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily the one.",
                    "label": 0
                },
                {
                    "sent": "This is the last that I use in the case where so that online algorithm I described approximately maximizes this problem minimizes this.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exactly do so, but approximately.",
                    "label": 0
                },
                {
                    "sent": "Talk about if I had a small level of normal extractor appealing data I could take some unstructured text to match them up to try to get contexts, national contexts and then learn more.",
                    "label": 0
                },
                {
                    "sent": "Yes, right?",
                    "label": 0
                },
                {
                    "sent": "So I can have C for personal names and forcing sometimes.",
                    "label": 0
                },
                {
                    "sent": "Because you know he will be matching, you know, Michael Jordan, different context and picking up some garbage.",
                    "label": 0
                },
                {
                    "sent": "So forwarding error types will do thing, so I'm not sure that I understand what the point is.",
                    "label": 0
                },
                {
                    "sent": "So so we we kind of try to make sure that the context, the context that I, so the signal that I use around several that appears several times around the things that in my soon list is not.",
                    "label": 0
                },
                {
                    "sent": "So yes, there's some filtering.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a whole paper on this that coin.",
                    "label": 0
                },
                {
                    "sent": "Cannal just you know a month ago that I left out of some of the details you have to have some voting.",
                    "label": 0
                },
                {
                    "sent": "You have to make sure that those Contacts are Contacts that are incredible in the sense that I used for a variety of of elements you know, often with elements of my seed list, right?",
                    "label": 0
                },
                {
                    "sent": "If I pick the system it is very yeah.",
                    "label": 0
                },
                {
                    "sent": "So you know, I was unfortunately this.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of it's.",
                    "label": 0
                },
                {
                    "sent": "It's kind of so I had.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have this little point here.",
                    "label": 0
                },
                {
                    "sent": "The prior knowledge part, you know it's basically that is in the sort of this Department.",
                    "label": 0
                },
                {
                    "sent": "Here has nothing to do with this kind of ugly actually.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a great technique and the student who started it.",
                    "label": 0
                },
                {
                    "sent": "He really enthusiastic about developing, but but I would like to find a better way, one that is more incorporating the overall objective rather than being some kind of heuristic discovery of potential features that are valuable.",
                    "label": 0
                },
                {
                    "sent": "I don't quite know how to do that.",
                    "label": 0
                },
                {
                    "sent": "I would like so the other stuff I mentioned this structure correspondence for adaptation is very much fits into this framework beautifully, but this is sort of outside and it's just a tool that we use to get better features so it's bite of kind of.",
                    "label": 0
                },
                {
                    "sent": "It's a enhancement of our prior knowledge from by exploiting a lot of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I think we should leave it at this because we're running a bit late, so let's think another again.",
                    "label": 0
                }
            ]
        }
    }
}