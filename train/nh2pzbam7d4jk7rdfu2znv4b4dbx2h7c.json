{
    "id": "nh2pzbam7d4jk7rdfu2znv4b4dbx2h7c",
    "title": "Dynamic Asset Allocation for Bivariate Enhanced Index Tracking using Sparse PLS",
    "info": {
        "author": [
            "Brian McWilliams, Department of Mathematics, Imperial College London"
        ],
        "published": "Aug. 21, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Finance"
        ]
    },
    "url": "http://videolectures.net/amlcf09_mcwilliams_daab/",
    "segmentation": [
        [
            "Thank you so this is just to introduce this is joint work with my supervisor Giovanni Montana Imperial College and I think a couple of caveats are necessary.",
            "The 1st and probably most important of which is I'm not really a finance researcher and this this talk.",
            "I think I slightly misjudged the audience here and it's really first and foremost, presenting a novel online kind of data stream mining algorithm.",
            "Kind of how we arrived at that, but then with perhaps a naive application to index tracking or other kind of pointers on how that could be used for index tracking.",
            "But something."
        ],
        [
            "Something that I get to later is that there's still a lot a lot left to do before it becomes a truly viable index tracking strategy and kind of along the way I hope to justify some of the intuition between behind the techniques that I'm using here.",
            "So I mean just by way of introduction.",
            "This is just this is just the DAX.",
            "That you're looking at and so the index.",
            "We're just saying it's a weighted sum of its constituent assets, and our problem here is to minimize the variance of the error between the index returns and artificial portfolio returns.",
            "And we want to do this by.",
            "Finding the most predictive subset of of X and then assigning it some weights beta.",
            "And we're assuming that the most predictive subset of X and the weights will be changing overtime.",
            "And again, this is an important caveat.",
            "We're ignoring the kind of costs, transaction costs inherent and the constraints for now, which I think a lot of people have problems with.",
            "But talk about this later at the end."
        ],
        [
            "And so we kind of formulate this in a more general problem I guess.",
            "Where we have.",
            "The data which which are the input streams or the asset returns X which RP dimensional within observations and the response dreams which is the index returns WHI which is which is Q dimensional where Q may be greater than one.",
            "And the key point here is the observations are arriving one at a time sequentially.",
            "And so we just have this kind of standard linear model.",
            "Where we know that X is just a weighted sum of.",
            "I'm sorry.",
            "Why is a weighted sum of X and?",
            "One thing another thing we assume is that.",
            "The dimensionality of X is often very large and a lot of the variables are highly correlated, so.",
            "We want to reduce the dimensionality of X to some value R, which is which is less than P. Hopefully a lot less than P. And so really, the objective is to predict.",
            "To predict the index returns using a subset of X so that the tracking error is minimized.",
            "I've already said that, but then doing this efficiently and updating this efficiently online.",
            "And so just kind of by way of introducing some of the techniques that I'm going to be using kind of key key to all.",
            "This is a singular value decomposition.",
            "I'm sure a lot of people are already familiar with this."
        ],
        [
            "We just decompose the matrix accent into 3 into 3 matrices UDNV where U&V importantly are orthonormal and are the eigenvectors of X. Transpose XX, transpose X, transpose X respectively and D matrix D is diagonal and.",
            "Can entries contain the square root eigen values of X, transpose X and kind of an important property that we're going to be using later?",
            "Is a low rank approximation approximation property of the SVD, whereby the sum of the first are smaller singular values in singular vectors constitute the best rank R approximation of X?"
        ],
        [
            "And so related to the SVD quite closely is concept principle.",
            "Components analysis.",
            "Again, it's a very standard tool and machine learning where we're trying to find an orthogonal basis basis vectors which which maximize the covariance of the data.",
            "And we achieve this by computing the eigenvectors of this matrix C, which is equivalent to the singular vectors of X.",
            "And we obtain the principle components by projecting X into this R dimensional space spanned by the 1st R set of singular vectors, and the reason we're talking about this is because it's been used in index tracking by Alexander quite recently and they put forward this this kind of notion that first principle component, so only looking at the first principle component.",
            "And that captures this concept, which is not particularly well defined, but they call it the market factor.",
            "And So what they basically just do is just normalize the first principal component and use that directly as the weights in an index tracking application, and that seems.",
            "That seems quite naive and more naive than than what we're doing so.",
            "So PCA, we're assuming that large variances are more important, but this doesn't take into account.",
            "How the index is moving doesn't take into account the response, so.",
            "This is why."
        ],
        [
            "Are partially squares comes in.",
            "And this is I'm not sure how familiar people are with this.",
            "But it is quite closely related again to PCA, except that.",
            "As well as dimensionality reduction, there's also regression built in as well.",
            "And we assume that this kind of latent factor model of the data whereby the data in the response both depend on a small number of the same latent factors S. As demonstrated in this diagram.",
            "And you can compose, decompose the data in the response into the latent factors multiplied by some loading vectors.",
            "And the regression step comes in when we notice that we can write Y in terms of X. X * V * W and those turn out to be the regression weights, where V are some weights such that the latent factors we find our X projected onto those weights and we find those weights by maximizing the covariance between the data and the response.",
            "And that's the key point here is that we're trying to find latent factors.",
            "Which are most predictive?",
            "Of or should be most predictive of the response."
        ],
        [
            "And kind of a sort of run through the P LS algorithm is the solution.",
            "The solution to this is clearly defined.",
            "The largest eigenvector of the Matrix X transpose, YY transpose X.",
            "And there's many variations and algorithms which exist to compute this, which mainly occur in the chemometrics and chemical processes literature.",
            "But the problem with most of these algorithms is that the rank of Y is generally less than the rank of X.",
            "So in order to extract all RPLS directions.",
            "We have to iteratively recompute the singular value decomposition for each of these dimensions.",
            "And.",
            "As your as P gets larger, this gets more and more expensive, and because what we're looking for here is kind of an efficient online algorithm, this is.",
            "Or it's not really ideal, so we just introduced this.",
            "Very basically just sort of analogous to Ridge regression, where we introduce a.",
            "A small regularising parameter onto the covariance matrix of Y and that allows us to obtain this full rank matrix X, which turns out to be kind of a weighted sum between PCA MPLS and that allows us to extract up to the rank of X latent factors with one SVD, and that's more than enough really to extract the first R. PLS components."
        ],
        [
            "Um?",
            "And so that's the dimensionality reduction part.",
            "Moving on to the variable selection.",
            "Again, the last two are very very common method in sort of machine learning and statistical learning where we can write a penalized regression where we apply a penalty to the L1 norm of the regression coefficients and a large enough regularization term.",
            "Results in a sparse solution.",
            "So some of these.",
            "Some of the values of beta or forced exactly to 0.",
            "And there are numerous efficient algorithm algorithms to solve this, such as Lars and coordinate descent which have come out with statistics literature.",
            "And kind of sort of a key result is that in some cases it can be solved simply with a soft threshold.",
            "So you just translate all of the values of the all of the absolute values of beta by by the regularization parameter gamma.",
            "And again, the reason that I'm sort of introducing lesu here is that it too is being used.",
            "In index tracking, so Brody and others have have introduced sort of the index tracking problem as a as a simple regression where they perform asset selection by simply applying the lesser."
        ],
        [
            "Now we want to put these two things together to obtain the."
        ],
        [
            "Sparse P LS algorithm so recently again.",
            "The number of papers have come out whereby.",
            "We take advantage of the of the low rank approximation property of the singular value decomposition, and we're able to rewrite the PCA eigenvector problem as a regression of this form where U&V are both restricted to be unit norm vectors.",
            "And so this can be solved using the low rank approximation property of the SVD.",
            "And.",
            "So in order to in order to obtain a sparse PCA solution, we can just apply again.",
            "Like in less you just apply a penalty to the L1 norm of.",
            "AV.",
            "And then this can be solved iteratively using the soft thresholding result I showed earlier.",
            "And the exact same.",
            "The exact same applies to P LS in order to obtain sparse PLS.",
            "So again, we just use the large covariance matrix H this time instead of X, and we penalize that should be the L1 norm of.",
            "Of V, which is again solved using the SVD and then iteratively self thresholded.",
            "And this leads to sparse PS regression coefficients.",
            "And so the final part of the algorithm is then being able to."
        ],
        [
            "All this online, so we're essentially performing.",
            "Update at each time point every time a new observation comes in.",
            "And so, solving PCA online is easy.",
            "There's plenty of techniques are already out there to do that.",
            "But for PLS we had this this problem where where we have this matrix and we don't really have access.",
            "We only have access to a covariance matrix.",
            "We don't really have access to the data individually, so we can't use the recursive least squares algorithms or some of the more common SVD updating algorithms.",
            "So our solution here was to use.",
            "What is essentially a generalization of the power method?",
            "For updating eigenvectors of a covariance matrix where you only have access to the covariance matrix.",
            "And so quick sort of run through the algorithm."
        ],
        [
            "So every time a data pair arrives, we update these two covariance matrices, which are sort of which are integral to the PLS algorithm.",
            "And the key point here is that we use.",
            "We use this for getting factor Lambda T which is adaptive so that this also changes changes in time.",
            "And.",
            "Yeah, so so we have this adaptive forgetting factor which changes in time and then sort of the rest of the rest of the algorithm, kind of.",
            "Kind of proceeds us as normal, we just update the update.",
            "The eigenvectors of H using this adapter.",
            "Generalization of power method and then apply self thresholding.",
            "So just a bit more sort of intuition on what this adaptive forgetting factor is doing."
        ],
        [
            "If we consider these residual errors, the prior error which is using.",
            "Using the coefficient of the time point before the time point at T -- 1 to predict.",
            "To predict the response at time T and the posterior error, which is the which is the newly computed regression coefficient, to predict the time point of the current response at the current time.",
            "We can arrive at via the work by Piggly Yoga.",
            "Very recently we can arrive at this expression for an adaptive forgetting factor which really relies on on the difference between these two these two quantities, so that if there's a large difference between them.",
            "There's probably you know the data is probably changing quite quickly, so we assign this very small forgetting factor, so there's a lot of forgetting.",
            "Otherwise, if the difference is very small between them.",
            "The forgetting factor is very close to 1, so we keep using.",
            "We keep using the past data to update our result.",
            "And so some quick sort of simulation."
        ],
        [
            "Notes to just show sort of intuitively what this algorithm is doing before I move on to the financial applications.",
            "So we generate some.",
            "Some test data using.",
            "The test data based on latent factors.",
            "So we generate three factors based as autoregressive one processors.",
            "And.",
            "And for each factor, we generate a number of variables.",
            "So 100 variables correspond to each factor and all of the variables only correspond to one to one factor and.",
            "We introduce some non stationarity's here at time 100 at a time, 100.",
            "Between time zero time 100, we assign a large coefficient to the first 100 variables.",
            "A slightly smaller coefficient to the next 100 variables, and no coefficient whatsoever, so the final final hundred variables don't contribute to the response at all.",
            "And again this changes at time 100 and then time 300, so.",
            "We sort of had.",
            "This response is really correlated to these latent factors."
        ],
        [
            "And the result we obtain.",
            "This is just one run through.",
            "We see that as expected, the first PS component is able to pick out.",
            "Is able to pick out the variables corresponding to the first latent factor.",
            "Similarly, the second class component is able to pick out the.",
            "Variables corresponding to the second latent factor.",
            "And the noise really doesn't affect it."
        ],
        [
            "Much.",
            "Or at all?",
            "And this is just showing the the average sensitivity over 500 simulations with and you can see that the self tuning Lambda is really able to too.",
            "To see where there's been a change in the data, and.",
            "And this is also picked up in the sensitivity of the algorithm so that you see that the forgetting factor changes in response to the to the latent factors changing, and the solution converges relatively quickly to the maximum sensitivity.",
            "And this is actually just a."
        ],
        [
            "A comparison between the average sensitivity of our algorithm over that time.",
            "With the only two other algorithms that we found actually do anything similar, so it's.",
            "Two other algorithms which perform sort of recursive and adaptive variable selection.",
            "And under this, under this model where we generated test data using latent factors.",
            "Our algorithm really outperforms these.",
            "So sort of moving on to the index tracking application.",
            "Our aim here is to.",
            "Perform."
        ],
        [
            "Bivariant index tracking.",
            "Because I mean, it's really a test of how well partially squares can handle multiple.",
            "Higher dimensional Y then were the dimensions larger than one.",
            "And our aim here is to perform enhanced tracking, so we're trying to over over perform the espian, the Nikkei.",
            "By 15% annually.",
            "And so the combination of these these two datasets that we have.",
            "Gives us 323 available assets and our aim is to select a subset of only ten of those.",
            "Hopefully the 10 most productive and sort of in line with the PCA approach of Alexander.",
            "We're only using the first BLS component.",
            "And we're kind of comparing this to a.",
            "Or"
        ],
        [
            "A random portfolio which is sort of the straw man at first to just kind of see to see how how well we're doing.",
            "And so.",
            "In both cases.",
            "Our portfolio is able to outperform the index returns by 15% and the random portfolio.",
            "So this is averaged over 500 random portfolios, which is updated using recursively squares that consistently underperformed the index.",
            "And this bottom plot is just a heat map which shows.",
            "Which stock, which assets are being held by our algorithm?",
            "So in certain cases, so along the bottom there's this blue line which shows that this this asset here, which is somewhere around what asset number 100 and something that's not that important.",
            "But it shows that it's being held for the entire period, so that it's it's probably very predictive of why, whereas a lot of the others are kind of changing a lot through time.",
            "And."
        ],
        [
            "Here we've compared it again with the recursive Lars and the adaptive lesu algorithms of before, and again it shows that these two other kind of competing.",
            "Algorithms?",
            "Really, I mean, they're not.",
            "They're not performing good index tracking, which is probably due to the fact that we have all these highly correlated assets.",
            "Um and yeah.",
            "So I mean, really, the one thing to say about this is yes, we are overfitting, we're."
        ],
        [
            "Overfitting massively, and that's because we haven't.",
            "I mean, we've ignored these constraints, which is what I'm just about to talk about, and that is OK, so.",
            "First of all, there's very, very few approaches to online variable selection in the literature.",
            "I mean, these are the only two that we found which are both sort of adaptive realizations of the lasso, just both slightly different, and neither of these importantly take into account the possible existence of latent factors underlying the data.",
            "Whereas our algorithm does.",
            "Online dimensionality reduction and variable selection.",
            "So we take into account these latent factors and importantly, it does it quickly.",
            "And it performs well with simulated data when the data and the response both depend on latent factors.",
            "But and these are the important parts here, and a more general."
        ],
        [
            "At present, we're specifying the number of PLS components and variables, and that's sort of like a general model selection problem, which is quite hard to do online.",
            "There's I mean, one solution that I've seen in the case of PEO S is online sort of an online approximation of cross validation.",
            "But sort of importantly, if we want to apply this.",
            "More properly to index tracking, I mean we haven't taken to taking into account the equality constraints so that all.",
            "All coefficients have to sum to one possible nonnegativity constraints as well.",
            "And also the existence of transaction costs which cause us to massively overfit the data.",
            "So I mean we have some solutions, sort of in the works for these.",
            "And but I mean kind of applying these non negativity inequality constraints isn't trivial in this in the PLS case.",
            "We have we have some sort of ideas of.",
            "Of applying a smoothness constraint to the coefficients as we update them overtime.",
            "But this is again all ongoing work.",
            "And.",
            "Yeah, some references, that's."
        ],
        [
            "Any?"
        ],
        [
            "If you look at the clerical diagonal values of the, the metrics like respose, X or Y equals because actually you're using a rank.",
            "Wonderful summation of that matrix.",
            "Yes, so therefore summation is good is that measures is really low rank.",
            "Also it will give you a good deal of estimating the rank of the PLS right?",
            "If you have a very sharp because it means that you can take Kate will be small, the number of peerless components.",
            "Yeah, for instance for white noise it could be.",
            "If there is an additional nothing the other way, if it, if it was the result.",
            "This is the this one number of components yeah?",
            "So if you look at that they figure of the decay of the eigenvalues.",
            "I haven't looked at that in in this case, but I have in other cases.",
            "And I mean generally, you find that sort of.",
            "Between sort of two and three P LS components is generally.",
            "Generally fine, I've also found that.",
            "So this is kind of like.",
            "The.",
            "What we're using here?",
            "Yeah, this thing here H, which is sort of really an approximation of PLSI found that sort of comparing comparing this with different values of Alpha as well gives a very good approximation of PLS with values of Alpha around around that.",
            "Well, I'm up if you kill him as a final thing, yeah.",
            "Right, yeah, that's something I haven't considered yet.",
            "Anymore questions.",
            "If it is solving the sparsity less communism, soft brush Holder, yeah.",
            "Doing something more type.",
            "No, I haven't.",
            "Yeah.",
            "I mean, I've seen.",
            "I've seen Alex is work on on sparse right, right?",
            "I would think maybe for right.",
            "Yeah.",
            "Basically, yeah.",
            "Yeah, I mean like one thing is this is really again an approximation of the lesser because because we're doing online, we kind of we.",
            "I mean, we can theoretically prove that it's converging, but sort of looking at how it works.",
            "It does seem to be converging, but again, this all thresholding.",
            "It's actually it's.",
            "I think it's the simplest way to solve it in this, yeah.",
            "Yeah, I mean I think yeah, either Lars or coordinate descent as well is.",
            "I mean something else that I'm looking at.",
            "What is my question is speaker?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you so this is just to introduce this is joint work with my supervisor Giovanni Montana Imperial College and I think a couple of caveats are necessary.",
                    "label": 1
                },
                {
                    "sent": "The 1st and probably most important of which is I'm not really a finance researcher and this this talk.",
                    "label": 0
                },
                {
                    "sent": "I think I slightly misjudged the audience here and it's really first and foremost, presenting a novel online kind of data stream mining algorithm.",
                    "label": 0
                },
                {
                    "sent": "Kind of how we arrived at that, but then with perhaps a naive application to index tracking or other kind of pointers on how that could be used for index tracking.",
                    "label": 0
                },
                {
                    "sent": "But something.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something that I get to later is that there's still a lot a lot left to do before it becomes a truly viable index tracking strategy and kind of along the way I hope to justify some of the intuition between behind the techniques that I'm using here.",
                    "label": 0
                },
                {
                    "sent": "So I mean just by way of introduction.",
                    "label": 0
                },
                {
                    "sent": "This is just this is just the DAX.",
                    "label": 0
                },
                {
                    "sent": "That you're looking at and so the index.",
                    "label": 0
                },
                {
                    "sent": "We're just saying it's a weighted sum of its constituent assets, and our problem here is to minimize the variance of the error between the index returns and artificial portfolio returns.",
                    "label": 1
                },
                {
                    "sent": "And we want to do this by.",
                    "label": 1
                },
                {
                    "sent": "Finding the most predictive subset of of X and then assigning it some weights beta.",
                    "label": 1
                },
                {
                    "sent": "And we're assuming that the most predictive subset of X and the weights will be changing overtime.",
                    "label": 0
                },
                {
                    "sent": "And again, this is an important caveat.",
                    "label": 0
                },
                {
                    "sent": "We're ignoring the kind of costs, transaction costs inherent and the constraints for now, which I think a lot of people have problems with.",
                    "label": 0
                },
                {
                    "sent": "But talk about this later at the end.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we kind of formulate this in a more general problem I guess.",
                    "label": 0
                },
                {
                    "sent": "Where we have.",
                    "label": 0
                },
                {
                    "sent": "The data which which are the input streams or the asset returns X which RP dimensional within observations and the response dreams which is the index returns WHI which is which is Q dimensional where Q may be greater than one.",
                    "label": 0
                },
                {
                    "sent": "And the key point here is the observations are arriving one at a time sequentially.",
                    "label": 1
                },
                {
                    "sent": "And so we just have this kind of standard linear model.",
                    "label": 0
                },
                {
                    "sent": "Where we know that X is just a weighted sum of.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Why is a weighted sum of X and?",
                    "label": 0
                },
                {
                    "sent": "One thing another thing we assume is that.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality of X is often very large and a lot of the variables are highly correlated, so.",
                    "label": 1
                },
                {
                    "sent": "We want to reduce the dimensionality of X to some value R, which is which is less than P. Hopefully a lot less than P. And so really, the objective is to predict.",
                    "label": 0
                },
                {
                    "sent": "To predict the index returns using a subset of X so that the tracking error is minimized.",
                    "label": 1
                },
                {
                    "sent": "I've already said that, but then doing this efficiently and updating this efficiently online.",
                    "label": 0
                },
                {
                    "sent": "And so just kind of by way of introducing some of the techniques that I'm going to be using kind of key key to all.",
                    "label": 0
                },
                {
                    "sent": "This is a singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "I'm sure a lot of people are already familiar with this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We just decompose the matrix accent into 3 into 3 matrices UDNV where U&V importantly are orthonormal and are the eigenvectors of X. Transpose XX, transpose X, transpose X respectively and D matrix D is diagonal and.",
                    "label": 1
                },
                {
                    "sent": "Can entries contain the square root eigen values of X, transpose X and kind of an important property that we're going to be using later?",
                    "label": 0
                },
                {
                    "sent": "Is a low rank approximation approximation property of the SVD, whereby the sum of the first are smaller singular values in singular vectors constitute the best rank R approximation of X?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so related to the SVD quite closely is concept principle.",
                    "label": 0
                },
                {
                    "sent": "Components analysis.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a very standard tool and machine learning where we're trying to find an orthogonal basis basis vectors which which maximize the covariance of the data.",
                    "label": 1
                },
                {
                    "sent": "And we achieve this by computing the eigenvectors of this matrix C, which is equivalent to the singular vectors of X.",
                    "label": 0
                },
                {
                    "sent": "And we obtain the principle components by projecting X into this R dimensional space spanned by the 1st R set of singular vectors, and the reason we're talking about this is because it's been used in index tracking by Alexander quite recently and they put forward this this kind of notion that first principle component, so only looking at the first principle component.",
                    "label": 0
                },
                {
                    "sent": "And that captures this concept, which is not particularly well defined, but they call it the market factor.",
                    "label": 0
                },
                {
                    "sent": "And So what they basically just do is just normalize the first principal component and use that directly as the weights in an index tracking application, and that seems.",
                    "label": 0
                },
                {
                    "sent": "That seems quite naive and more naive than than what we're doing so.",
                    "label": 0
                },
                {
                    "sent": "So PCA, we're assuming that large variances are more important, but this doesn't take into account.",
                    "label": 1
                },
                {
                    "sent": "How the index is moving doesn't take into account the response, so.",
                    "label": 0
                },
                {
                    "sent": "This is why.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are partially squares comes in.",
                    "label": 0
                },
                {
                    "sent": "And this is I'm not sure how familiar people are with this.",
                    "label": 0
                },
                {
                    "sent": "But it is quite closely related again to PCA, except that.",
                    "label": 0
                },
                {
                    "sent": "As well as dimensionality reduction, there's also regression built in as well.",
                    "label": 0
                },
                {
                    "sent": "And we assume that this kind of latent factor model of the data whereby the data in the response both depend on a small number of the same latent factors S. As demonstrated in this diagram.",
                    "label": 1
                },
                {
                    "sent": "And you can compose, decompose the data in the response into the latent factors multiplied by some loading vectors.",
                    "label": 0
                },
                {
                    "sent": "And the regression step comes in when we notice that we can write Y in terms of X. X * V * W and those turn out to be the regression weights, where V are some weights such that the latent factors we find our X projected onto those weights and we find those weights by maximizing the covariance between the data and the response.",
                    "label": 0
                },
                {
                    "sent": "And that's the key point here is that we're trying to find latent factors.",
                    "label": 0
                },
                {
                    "sent": "Which are most predictive?",
                    "label": 0
                },
                {
                    "sent": "Of or should be most predictive of the response.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And kind of a sort of run through the P LS algorithm is the solution.",
                    "label": 0
                },
                {
                    "sent": "The solution to this is clearly defined.",
                    "label": 0
                },
                {
                    "sent": "The largest eigenvector of the Matrix X transpose, YY transpose X.",
                    "label": 1
                },
                {
                    "sent": "And there's many variations and algorithms which exist to compute this, which mainly occur in the chemometrics and chemical processes literature.",
                    "label": 0
                },
                {
                    "sent": "But the problem with most of these algorithms is that the rank of Y is generally less than the rank of X.",
                    "label": 0
                },
                {
                    "sent": "So in order to extract all RPLS directions.",
                    "label": 0
                },
                {
                    "sent": "We have to iteratively recompute the singular value decomposition for each of these dimensions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As your as P gets larger, this gets more and more expensive, and because what we're looking for here is kind of an efficient online algorithm, this is.",
                    "label": 0
                },
                {
                    "sent": "Or it's not really ideal, so we just introduced this.",
                    "label": 0
                },
                {
                    "sent": "Very basically just sort of analogous to Ridge regression, where we introduce a.",
                    "label": 0
                },
                {
                    "sent": "A small regularising parameter onto the covariance matrix of Y and that allows us to obtain this full rank matrix X, which turns out to be kind of a weighted sum between PCA MPLS and that allows us to extract up to the rank of X latent factors with one SVD, and that's more than enough really to extract the first R. PLS components.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so that's the dimensionality reduction part.",
                    "label": 0
                },
                {
                    "sent": "Moving on to the variable selection.",
                    "label": 0
                },
                {
                    "sent": "Again, the last two are very very common method in sort of machine learning and statistical learning where we can write a penalized regression where we apply a penalty to the L1 norm of the regression coefficients and a large enough regularization term.",
                    "label": 0
                },
                {
                    "sent": "Results in a sparse solution.",
                    "label": 1
                },
                {
                    "sent": "So some of these.",
                    "label": 0
                },
                {
                    "sent": "Some of the values of beta or forced exactly to 0.",
                    "label": 0
                },
                {
                    "sent": "And there are numerous efficient algorithm algorithms to solve this, such as Lars and coordinate descent which have come out with statistics literature.",
                    "label": 0
                },
                {
                    "sent": "And kind of sort of a key result is that in some cases it can be solved simply with a soft threshold.",
                    "label": 0
                },
                {
                    "sent": "So you just translate all of the values of the all of the absolute values of beta by by the regularization parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "And again, the reason that I'm sort of introducing lesu here is that it too is being used.",
                    "label": 0
                },
                {
                    "sent": "In index tracking, so Brody and others have have introduced sort of the index tracking problem as a as a simple regression where they perform asset selection by simply applying the lesser.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we want to put these two things together to obtain the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sparse P LS algorithm so recently again.",
                    "label": 0
                },
                {
                    "sent": "The number of papers have come out whereby.",
                    "label": 0
                },
                {
                    "sent": "We take advantage of the of the low rank approximation property of the singular value decomposition, and we're able to rewrite the PCA eigenvector problem as a regression of this form where U&V are both restricted to be unit norm vectors.",
                    "label": 0
                },
                {
                    "sent": "And so this can be solved using the low rank approximation property of the SVD.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So in order to in order to obtain a sparse PCA solution, we can just apply again.",
                    "label": 0
                },
                {
                    "sent": "Like in less you just apply a penalty to the L1 norm of.",
                    "label": 0
                },
                {
                    "sent": "AV.",
                    "label": 0
                },
                {
                    "sent": "And then this can be solved iteratively using the soft thresholding result I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "And the exact same.",
                    "label": 0
                },
                {
                    "sent": "The exact same applies to P LS in order to obtain sparse PLS.",
                    "label": 1
                },
                {
                    "sent": "So again, we just use the large covariance matrix H this time instead of X, and we penalize that should be the L1 norm of.",
                    "label": 0
                },
                {
                    "sent": "Of V, which is again solved using the SVD and then iteratively self thresholded.",
                    "label": 1
                },
                {
                    "sent": "And this leads to sparse PS regression coefficients.",
                    "label": 0
                },
                {
                    "sent": "And so the final part of the algorithm is then being able to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All this online, so we're essentially performing.",
                    "label": 0
                },
                {
                    "sent": "Update at each time point every time a new observation comes in.",
                    "label": 0
                },
                {
                    "sent": "And so, solving PCA online is easy.",
                    "label": 1
                },
                {
                    "sent": "There's plenty of techniques are already out there to do that.",
                    "label": 0
                },
                {
                    "sent": "But for PLS we had this this problem where where we have this matrix and we don't really have access.",
                    "label": 0
                },
                {
                    "sent": "We only have access to a covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "We don't really have access to the data individually, so we can't use the recursive least squares algorithms or some of the more common SVD updating algorithms.",
                    "label": 0
                },
                {
                    "sent": "So our solution here was to use.",
                    "label": 0
                },
                {
                    "sent": "What is essentially a generalization of the power method?",
                    "label": 0
                },
                {
                    "sent": "For updating eigenvectors of a covariance matrix where you only have access to the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And so quick sort of run through the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So every time a data pair arrives, we update these two covariance matrices, which are sort of which are integral to the PLS algorithm.",
                    "label": 1
                },
                {
                    "sent": "And the key point here is that we use.",
                    "label": 0
                },
                {
                    "sent": "We use this for getting factor Lambda T which is adaptive so that this also changes changes in time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so we have this adaptive forgetting factor which changes in time and then sort of the rest of the rest of the algorithm, kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of proceeds us as normal, we just update the update.",
                    "label": 0
                },
                {
                    "sent": "The eigenvectors of H using this adapter.",
                    "label": 1
                },
                {
                    "sent": "Generalization of power method and then apply self thresholding.",
                    "label": 0
                },
                {
                    "sent": "So just a bit more sort of intuition on what this adaptive forgetting factor is doing.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we consider these residual errors, the prior error which is using.",
                    "label": 1
                },
                {
                    "sent": "Using the coefficient of the time point before the time point at T -- 1 to predict.",
                    "label": 0
                },
                {
                    "sent": "To predict the response at time T and the posterior error, which is the which is the newly computed regression coefficient, to predict the time point of the current response at the current time.",
                    "label": 0
                },
                {
                    "sent": "We can arrive at via the work by Piggly Yoga.",
                    "label": 0
                },
                {
                    "sent": "Very recently we can arrive at this expression for an adaptive forgetting factor which really relies on on the difference between these two these two quantities, so that if there's a large difference between them.",
                    "label": 1
                },
                {
                    "sent": "There's probably you know the data is probably changing quite quickly, so we assign this very small forgetting factor, so there's a lot of forgetting.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if the difference is very small between them.",
                    "label": 0
                },
                {
                    "sent": "The forgetting factor is very close to 1, so we keep using.",
                    "label": 0
                },
                {
                    "sent": "We keep using the past data to update our result.",
                    "label": 0
                },
                {
                    "sent": "And so some quick sort of simulation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notes to just show sort of intuitively what this algorithm is doing before I move on to the financial applications.",
                    "label": 0
                },
                {
                    "sent": "So we generate some.",
                    "label": 0
                },
                {
                    "sent": "Some test data using.",
                    "label": 0
                },
                {
                    "sent": "The test data based on latent factors.",
                    "label": 0
                },
                {
                    "sent": "So we generate three factors based as autoregressive one processors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And for each factor, we generate a number of variables.",
                    "label": 0
                },
                {
                    "sent": "So 100 variables correspond to each factor and all of the variables only correspond to one to one factor and.",
                    "label": 0
                },
                {
                    "sent": "We introduce some non stationarity's here at time 100 at a time, 100.",
                    "label": 0
                },
                {
                    "sent": "Between time zero time 100, we assign a large coefficient to the first 100 variables.",
                    "label": 0
                },
                {
                    "sent": "A slightly smaller coefficient to the next 100 variables, and no coefficient whatsoever, so the final final hundred variables don't contribute to the response at all.",
                    "label": 0
                },
                {
                    "sent": "And again this changes at time 100 and then time 300, so.",
                    "label": 0
                },
                {
                    "sent": "We sort of had.",
                    "label": 0
                },
                {
                    "sent": "This response is really correlated to these latent factors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the result we obtain.",
                    "label": 0
                },
                {
                    "sent": "This is just one run through.",
                    "label": 0
                },
                {
                    "sent": "We see that as expected, the first PS component is able to pick out.",
                    "label": 0
                },
                {
                    "sent": "Is able to pick out the variables corresponding to the first latent factor.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the second class component is able to pick out the.",
                    "label": 0
                },
                {
                    "sent": "Variables corresponding to the second latent factor.",
                    "label": 0
                },
                {
                    "sent": "And the noise really doesn't affect it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much.",
                    "label": 0
                },
                {
                    "sent": "Or at all?",
                    "label": 0
                },
                {
                    "sent": "And this is just showing the the average sensitivity over 500 simulations with and you can see that the self tuning Lambda is really able to too.",
                    "label": 0
                },
                {
                    "sent": "To see where there's been a change in the data, and.",
                    "label": 1
                },
                {
                    "sent": "And this is also picked up in the sensitivity of the algorithm so that you see that the forgetting factor changes in response to the to the latent factors changing, and the solution converges relatively quickly to the maximum sensitivity.",
                    "label": 1
                },
                {
                    "sent": "And this is actually just a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A comparison between the average sensitivity of our algorithm over that time.",
                    "label": 1
                },
                {
                    "sent": "With the only two other algorithms that we found actually do anything similar, so it's.",
                    "label": 1
                },
                {
                    "sent": "Two other algorithms which perform sort of recursive and adaptive variable selection.",
                    "label": 0
                },
                {
                    "sent": "And under this, under this model where we generated test data using latent factors.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm really outperforms these.",
                    "label": 0
                },
                {
                    "sent": "So sort of moving on to the index tracking application.",
                    "label": 0
                },
                {
                    "sent": "Our aim here is to.",
                    "label": 0
                },
                {
                    "sent": "Perform.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bivariant index tracking.",
                    "label": 0
                },
                {
                    "sent": "Because I mean, it's really a test of how well partially squares can handle multiple.",
                    "label": 0
                },
                {
                    "sent": "Higher dimensional Y then were the dimensions larger than one.",
                    "label": 0
                },
                {
                    "sent": "And our aim here is to perform enhanced tracking, so we're trying to over over perform the espian, the Nikkei.",
                    "label": 0
                },
                {
                    "sent": "By 15% annually.",
                    "label": 0
                },
                {
                    "sent": "And so the combination of these these two datasets that we have.",
                    "label": 0
                },
                {
                    "sent": "Gives us 323 available assets and our aim is to select a subset of only ten of those.",
                    "label": 1
                },
                {
                    "sent": "Hopefully the 10 most productive and sort of in line with the PCA approach of Alexander.",
                    "label": 1
                },
                {
                    "sent": "We're only using the first BLS component.",
                    "label": 0
                },
                {
                    "sent": "And we're kind of comparing this to a.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A random portfolio which is sort of the straw man at first to just kind of see to see how how well we're doing.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "In both cases.",
                    "label": 0
                },
                {
                    "sent": "Our portfolio is able to outperform the index returns by 15% and the random portfolio.",
                    "label": 1
                },
                {
                    "sent": "So this is averaged over 500 random portfolios, which is updated using recursively squares that consistently underperformed the index.",
                    "label": 1
                },
                {
                    "sent": "And this bottom plot is just a heat map which shows.",
                    "label": 0
                },
                {
                    "sent": "Which stock, which assets are being held by our algorithm?",
                    "label": 0
                },
                {
                    "sent": "So in certain cases, so along the bottom there's this blue line which shows that this this asset here, which is somewhere around what asset number 100 and something that's not that important.",
                    "label": 0
                },
                {
                    "sent": "But it shows that it's being held for the entire period, so that it's it's probably very predictive of why, whereas a lot of the others are kind of changing a lot through time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we've compared it again with the recursive Lars and the adaptive lesu algorithms of before, and again it shows that these two other kind of competing.",
                    "label": 0
                },
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "Really, I mean, they're not.",
                    "label": 0
                },
                {
                    "sent": "They're not performing good index tracking, which is probably due to the fact that we have all these highly correlated assets.",
                    "label": 0
                },
                {
                    "sent": "Um and yeah.",
                    "label": 0
                },
                {
                    "sent": "So I mean, really, the one thing to say about this is yes, we are overfitting, we're.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overfitting massively, and that's because we haven't.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've ignored these constraints, which is what I'm just about to talk about, and that is OK, so.",
                    "label": 0
                },
                {
                    "sent": "First of all, there's very, very few approaches to online variable selection in the literature.",
                    "label": 1
                },
                {
                    "sent": "I mean, these are the only two that we found which are both sort of adaptive realizations of the lasso, just both slightly different, and neither of these importantly take into account the possible existence of latent factors underlying the data.",
                    "label": 0
                },
                {
                    "sent": "Whereas our algorithm does.",
                    "label": 1
                },
                {
                    "sent": "Online dimensionality reduction and variable selection.",
                    "label": 1
                },
                {
                    "sent": "So we take into account these latent factors and importantly, it does it quickly.",
                    "label": 0
                },
                {
                    "sent": "And it performs well with simulated data when the data and the response both depend on latent factors.",
                    "label": 0
                },
                {
                    "sent": "But and these are the important parts here, and a more general.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At present, we're specifying the number of PLS components and variables, and that's sort of like a general model selection problem, which is quite hard to do online.",
                    "label": 1
                },
                {
                    "sent": "There's I mean, one solution that I've seen in the case of PEO S is online sort of an online approximation of cross validation.",
                    "label": 0
                },
                {
                    "sent": "But sort of importantly, if we want to apply this.",
                    "label": 0
                },
                {
                    "sent": "More properly to index tracking, I mean we haven't taken to taking into account the equality constraints so that all.",
                    "label": 0
                },
                {
                    "sent": "All coefficients have to sum to one possible nonnegativity constraints as well.",
                    "label": 0
                },
                {
                    "sent": "And also the existence of transaction costs which cause us to massively overfit the data.",
                    "label": 0
                },
                {
                    "sent": "So I mean we have some solutions, sort of in the works for these.",
                    "label": 0
                },
                {
                    "sent": "And but I mean kind of applying these non negativity inequality constraints isn't trivial in this in the PLS case.",
                    "label": 0
                },
                {
                    "sent": "We have we have some sort of ideas of.",
                    "label": 0
                },
                {
                    "sent": "Of applying a smoothness constraint to the coefficients as we update them overtime.",
                    "label": 0
                },
                {
                    "sent": "But this is again all ongoing work.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, some references, that's.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the clerical diagonal values of the, the metrics like respose, X or Y equals because actually you're using a rank.",
                    "label": 0
                },
                {
                    "sent": "Wonderful summation of that matrix.",
                    "label": 0
                },
                {
                    "sent": "Yes, so therefore summation is good is that measures is really low rank.",
                    "label": 0
                },
                {
                    "sent": "Also it will give you a good deal of estimating the rank of the PLS right?",
                    "label": 0
                },
                {
                    "sent": "If you have a very sharp because it means that you can take Kate will be small, the number of peerless components.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for instance for white noise it could be.",
                    "label": 0
                },
                {
                    "sent": "If there is an additional nothing the other way, if it, if it was the result.",
                    "label": 0
                },
                {
                    "sent": "This is the this one number of components yeah?",
                    "label": 0
                },
                {
                    "sent": "So if you look at that they figure of the decay of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at that in in this case, but I have in other cases.",
                    "label": 0
                },
                {
                    "sent": "And I mean generally, you find that sort of.",
                    "label": 0
                },
                {
                    "sent": "Between sort of two and three P LS components is generally.",
                    "label": 0
                },
                {
                    "sent": "Generally fine, I've also found that.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of like.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "What we're using here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this thing here H, which is sort of really an approximation of PLSI found that sort of comparing comparing this with different values of Alpha as well gives a very good approximation of PLS with values of Alpha around around that.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm up if you kill him as a final thing, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, that's something I haven't considered yet.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "If it is solving the sparsity less communism, soft brush Holder, yeah.",
                    "label": 0
                },
                {
                    "sent": "Doing something more type.",
                    "label": 0
                },
                {
                    "sent": "No, I haven't.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've seen.",
                    "label": 0
                },
                {
                    "sent": "I've seen Alex is work on on sparse right, right?",
                    "label": 0
                },
                {
                    "sent": "I would think maybe for right.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Basically, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean like one thing is this is really again an approximation of the lesser because because we're doing online, we kind of we.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can theoretically prove that it's converging, but sort of looking at how it works.",
                    "label": 0
                },
                {
                    "sent": "It does seem to be converging, but again, this all thresholding.",
                    "label": 0
                },
                {
                    "sent": "It's actually it's.",
                    "label": 0
                },
                {
                    "sent": "I think it's the simplest way to solve it in this, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean I think yeah, either Lars or coordinate descent as well is.",
                    "label": 0
                },
                {
                    "sent": "I mean something else that I'm looking at.",
                    "label": 0
                },
                {
                    "sent": "What is my question is speaker?",
                    "label": 0
                }
            ]
        }
    }
}