{
    "id": "jbmrbi67yoyr5eiaigovdhib2uy6qoxw",
    "title": "A Tri-Partite Neural Document Language Model for Semantic Information Retrieval",
    "info": {
        "author": [
            "Gia-Hung Nguyen, University of Toulouse I: Social Sciences"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_nguyen_information_retrieval/",
    "segmentation": [
        [
            "Hello I want my name is John when I'm from University of Toulouse, three in France and I'm a PhD student.",
            "Today we'll talk about our work for model to learn Document language, document representation for semantic information retrieval."
        ],
        [
            "And here is the plan of up my talk.",
            "And first of all I will talk about the context and motivation for our work.",
            "Anne."
        ],
        [
            "Our work is far in the context of information retrieval, and in order to satisfy the user information needs.",
            "The system, the information retrieval system, have to find the most relevant document.",
            "To reply to the query of the user and to do this document and the query, our first index to obtain the intermediate representation.",
            "And usually they use the inverted index.",
            "In order to facilitate the term matching process, however, the term matching process is quite complicated due to the semantic gap.",
            "And one the semantic gaps can be implied by three core issue.",
            "The first one in the vocabulary mismatch which we can see an example like car wasn't auto mobile.",
            "Let mean mean two work.",
            "This item.",
            "Same concept but with different representation.",
            "The second one is the Randall Randall Arity mismatch.",
            "Assemble, for example, we have cut without anymore.",
            "For example, we want to file the document talk about anymore, but we miss some document talk about Cat.",
            "If you want to find exactly only one with animal."
        ],
        [
            "Cost issue with the police aiming for example we have them work bus.",
            "The bus has many meaning.",
            "For example had fish or bust sound low beach.",
            "And how to improve this retrieval performance is important to reduce the semantic gap between document and query.",
            "So in our context, we are interested in two approach.",
            "The first one which explored the relational semantic in the Nordic research to take advantage to leverage the sense of work and relation.",
            "The second one we call that distribute this reason or semantic, which is based on the novel network to learn the reservation.",
            "Or we can we call them picking up the work on the document.",
            "An hour.",
            "Sorry.",
            "And our motivation is combining the combining two.",
            "These two semantic.",
            "To enhance the text representation to inject the relational relation relational knowledge into the district relational embedding to reduce the semantic gaps."
        ],
        [
            "And I will talk about some related work for in our context."
        ],
        [
            "The first line works is.",
            "I'm to learn the test reputation with neural network and we have the famous assemble for.",
            "For example, we have a famous work to work with the.",
            "With me to learn the work building by using the neighbor.",
            "Contest.",
            "For example, we have the sebo version.",
            "With the objective function is to predict the target work using the concept works.",
            "Is that mean then work in neighborhood?",
            "An to go beyond the work level, we have the power factor, which is the extension of work to work.",
            "It learns the document at the same time with the word embedding.",
            "And for example, we have a version of PDM not learn or not used our objective function to predict work using the document document and work in context."
        ],
        [
            "The second category.",
            "We have to rip the first group them to learn the work reputation with the.",
            "Alright with the correction.",
            "Of their relation in the knowledge resource that mean they want to correct the work by using their relation in the knowledge base.",
            "The intention behind is that the work related to similar concept in the knowledge base should have similar embedding.",
            "And there's so many time works the first time you use the regular regularization inside learning process and the second time use the correction.",
            "That means they correct the outside after the learning process."
        ],
        [
            "The second group has interest in learning work work.",
            "What representation and concept representation at the same time they called the Joint Learning of Work and concept.",
            "The idea behind it, the concept or the sense of work can have the learning of the work building.",
            "And they use some objective function, for example to estimate the probability of a world concept or.",
            "But about."
        ],
        [
            "And our contribution, our position in the related work.",
            "The listing."
        ],
        [
            "And we use the strong learning of War concept, a concept and document.",
            "On tripartite.",
            "In the same model to learn at the same time 3 reservation for test.",
            "And we also use."
        ],
        [
            "The raw text and there are no class ratios in the.",
            "In the learning phase.",
            "And our model will provide a vector embedding for the test which can be used in order task.",
            "Here."
        ],
        [
            "Will detail our model.",
            "Like we like, I have said why we call this trouble time?",
            "Because we have document work and concept in the same learning process and to do so we.",
            "We have to.",
            "We have a document with the must be annotated with the concepts that mean we.",
            "First, we want to we have to extract the concept of the words in the document.",
            "Anne.",
            "We will learn the reputation for the uh, document, even though it's work and concept in this text.",
            "To do this, we based on two assumption.",
            "The first one is that.",
            "We can learn some simultaneously the representation.",
            "How they work and the concept.",
            "To have better learning the empathy.",
            "The second assumption is that.",
            "If you can, try the embedding learning with the local screenshot, we can have better.",
            "Meaning of the word and the document admitting."
        ],
        [
            "So we have the objective function with two parts.",
            "The first one.",
            "I'm to learn the distributional semantic of document of the work and the concept that we can't else.",
            "The second one is to learn the relational semantic via the concept in the knowledge base that we can't LR.",
            "And finally we will combine these two objectives into the final function.",
            "For the."
        ],
        [
            "First part of the objective function.",
            "We tried to learn the distribution list distributional semantic.",
            "We extend the power factor model which initially learned the document.",
            "Everything from the work in this text.",
            "And here we consider the work with the concept that mean even the work we identify, the concept or the sense behind this work and we will learn by predicting this work target work if.",
            "It work have, if that work has a concept, we will predict the work and the concept.",
            "We won the contest in our case.",
            "In the document the work in the construct and concept in the works.",
            "Anne, here we have the.",
            "The objective function to predict the probability of Rewards River in this context."
        ],
        [
            "For the second objective, we want to.",
            "Combine the relational semantic in the learning process.",
            "Intuition is to push the representation of work and the.",
            "To press the registration of work that are connected in the knowledge base.",
            "To do so, we first we identify the pair of work that is connected in the knowledge base.",
            "For example, we have this pool of connected work and from this pool from this on pair work we will.",
            "In the objective function we will reduce we will reduce the.",
            "At this time.",
            "Oh sorry.",
            "We try to maximize the similarity, the cosine similarity between two work building.",
            "And finally, we will combine two objective function with our LC and LR to define the final HD for learning process.",
            "And in the end we have a model to learn representation of the text is.",
            "Here is the world and the concept and the document."
        ],
        [
            "An how we how we evaluate our model our even our model?",
            "First, we have used the robust data set that is a new document on the web."
        ],
        [
            "Then we do the.",
            "We have two tasks.",
            "The first one is document embedding quality to measure the quality of the presentation document.",
            "The second one is we test in the IR in the information retrieval tasks, which are the ranking or increase pension.",
            "And we have three compilation.",
            "The first one is B2B para vector and the second S. D2 with our model with our.",
            "Without the relational contract and the first one is our model, our full model with relation constraint."
        ],
        [
            "In the document embedding quality evaluation, we use the protocol last introduced in the paper of power after.",
            "First we will.",
            "Identify the document triplet.",
            "Tribles tree document.",
            "The first one is similar and the third one, third one is randomly randomly choose in the order.",
            "That means that the one and two are similar and the tree on different from the first 2 and our objective is to measure to calculate error rate when.",
            "When we when we calculate the similarity of the vector of the first one document, if this similarity is a lower than the similarity of their thoughts and the 1st and the first one that we call that an error.",
            "So.",
            "For the in the type of result paper we have.",
            "We can see that our model SD2 we are have a bet.",
            "The best reason with the lowest error rate.",
            "Compared to the TF IDF, we can see that the we had we can see the benefit of this traditional semantic.",
            "Anne, sorry.",
            "And compared to the character.",
            "Our SD to we are better have better reason but we can see there is a symmetric effect when we combine the relation and relation and the distributional semantic."
        ],
        [
            "To better understand the effect behind this.",
            "We have visualized.",
            "Example here we have a.",
            "Here we have an image of a. TSNE projection of a query and relevant or irrelevant documents.",
            "Then like not order 11 document and the white dots are the relevant document for the query.",
            "In to picture the left one is the.",
            "Plot of the vector.",
            "Obtained my character and there I want you obtained by our model.",
            "We can see that in two picture there is a distinction between the relevant and irrelevant document.",
            "However, in the in the power factor the queries in file B.",
            "In the center of the cluster and there, there's not much distinction.",
            "In which cluster should be found?",
            "In our in our example here, the query is found near the relevant document, so we can say that and with our model we can.",
            "Obtain better vector for document.",
            "So the query.",
            "Now."
        ],
        [
            "We I present here the result of the.",
            "Information retrieval tasks that are ranking document ranking and query expansion.",
            "And we we parked in the matrix map.",
            "Metrican recall.",
            "4th, we look at the Document ranking task for which the rain table here.",
            "We can see that.",
            "Our model have better reason than the baseline IR baseline.",
            "That we can see that narrow scope can have ranking on the document and carry matching.",
            "An comparing our two confirmation, the phone and without relation.",
            "We can see that the reason is slightly better when we have an intern check the relation semantic relational semantics.",
            "In the queries mentioned plus we also see the same.",
            "Tendence this.",
            "The same reason that the our model, our neural model have a better result than the baseline.",
            "And we can see also that the best confirmation is the.",
            "Extension with the concept that mean when we use the.",
            "Relational semantic, we have better result in the.",
            "Matching process."
        ],
        [
            "However, here we see, we see that our model have a worse result than the previous model, so we have to.",
            "Make some analysts analysis to better understand the problem.",
            "Hey, we make a cross analysis to.",
            "To understand why our model is perform worse than the PV, the power factor baseline.",
            "So we have identified three sets of query where our SD two.",
            "We perform better or works equal or better than the PV.",
            "And in each set with identify the number of concept in the document in the in the relevant document for the query and we can see that in the query manners in the work set in the work area set.",
            "We there, there are many concept another set when we can.",
            "This suggests that our model is not able to catch the semantic.",
            "When the document have many when document Hello, higher number of concepts.",
            "That mean we have an app that had that.",
            "The noise in the.",
            "In the annotation process, when we preprocess the document.",
            "An we also.",
            "We also look at the text of the document.",
            "For example, in the in the query plus set that mean well, our model work better than the previous baseline.",
            "With we see that.",
            "They let the concept and the annotation the alignment of concept and the work is better than the in the query minus.",
            "For example, here we can see that there is some missing annotation.",
            "For example, here sharks and surfing and.",
            "So that."
        ],
        [
            "And to conclude.",
            "We can see that.",
            "We have a unified learning for concept work and document everything and we have reduced the vocabulary mismatch between document and query will also improve the quality of document embedding an.",
            "Encouraging results for the information retrieval task.",
            "However, we will have some.",
            "Reasons that are not very.",
            "Very perfect then we have some error in the annotation phase so we have to.",
            "We need to have further investigation."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello I want my name is John when I'm from University of Toulouse, three in France and I'm a PhD student.",
                    "label": 0
                },
                {
                    "sent": "Today we'll talk about our work for model to learn Document language, document representation for semantic information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the plan of up my talk.",
                    "label": 0
                },
                {
                    "sent": "And first of all I will talk about the context and motivation for our work.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our work is far in the context of information retrieval, and in order to satisfy the user information needs.",
                    "label": 0
                },
                {
                    "sent": "The system, the information retrieval system, have to find the most relevant document.",
                    "label": 1
                },
                {
                    "sent": "To reply to the query of the user and to do this document and the query, our first index to obtain the intermediate representation.",
                    "label": 0
                },
                {
                    "sent": "And usually they use the inverted index.",
                    "label": 0
                },
                {
                    "sent": "In order to facilitate the term matching process, however, the term matching process is quite complicated due to the semantic gap.",
                    "label": 1
                },
                {
                    "sent": "And one the semantic gaps can be implied by three core issue.",
                    "label": 0
                },
                {
                    "sent": "The first one in the vocabulary mismatch which we can see an example like car wasn't auto mobile.",
                    "label": 0
                },
                {
                    "sent": "Let mean mean two work.",
                    "label": 0
                },
                {
                    "sent": "This item.",
                    "label": 0
                },
                {
                    "sent": "Same concept but with different representation.",
                    "label": 0
                },
                {
                    "sent": "The second one is the Randall Randall Arity mismatch.",
                    "label": 0
                },
                {
                    "sent": "Assemble, for example, we have cut without anymore.",
                    "label": 0
                },
                {
                    "sent": "For example, we want to file the document talk about anymore, but we miss some document talk about Cat.",
                    "label": 0
                },
                {
                    "sent": "If you want to find exactly only one with animal.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cost issue with the police aiming for example we have them work bus.",
                    "label": 0
                },
                {
                    "sent": "The bus has many meaning.",
                    "label": 0
                },
                {
                    "sent": "For example had fish or bust sound low beach.",
                    "label": 0
                },
                {
                    "sent": "And how to improve this retrieval performance is important to reduce the semantic gap between document and query.",
                    "label": 1
                },
                {
                    "sent": "So in our context, we are interested in two approach.",
                    "label": 0
                },
                {
                    "sent": "The first one which explored the relational semantic in the Nordic research to take advantage to leverage the sense of work and relation.",
                    "label": 1
                },
                {
                    "sent": "The second one we call that distribute this reason or semantic, which is based on the novel network to learn the reservation.",
                    "label": 0
                },
                {
                    "sent": "Or we can we call them picking up the work on the document.",
                    "label": 0
                },
                {
                    "sent": "An hour.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And our motivation is combining the combining two.",
                    "label": 0
                },
                {
                    "sent": "These two semantic.",
                    "label": 0
                },
                {
                    "sent": "To enhance the text representation to inject the relational relation relational knowledge into the district relational embedding to reduce the semantic gaps.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will talk about some related work for in our context.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first line works is.",
                    "label": 0
                },
                {
                    "sent": "I'm to learn the test reputation with neural network and we have the famous assemble for.",
                    "label": 0
                },
                {
                    "sent": "For example, we have a famous work to work with the.",
                    "label": 0
                },
                {
                    "sent": "With me to learn the work building by using the neighbor.",
                    "label": 0
                },
                {
                    "sent": "Contest.",
                    "label": 0
                },
                {
                    "sent": "For example, we have the sebo version.",
                    "label": 0
                },
                {
                    "sent": "With the objective function is to predict the target work using the concept works.",
                    "label": 0
                },
                {
                    "sent": "Is that mean then work in neighborhood?",
                    "label": 0
                },
                {
                    "sent": "An to go beyond the work level, we have the power factor, which is the extension of work to work.",
                    "label": 0
                },
                {
                    "sent": "It learns the document at the same time with the word embedding.",
                    "label": 0
                },
                {
                    "sent": "And for example, we have a version of PDM not learn or not used our objective function to predict work using the document document and work in context.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second category.",
                    "label": 0
                },
                {
                    "sent": "We have to rip the first group them to learn the work reputation with the.",
                    "label": 0
                },
                {
                    "sent": "Alright with the correction.",
                    "label": 0
                },
                {
                    "sent": "Of their relation in the knowledge resource that mean they want to correct the work by using their relation in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "The intention behind is that the work related to similar concept in the knowledge base should have similar embedding.",
                    "label": 0
                },
                {
                    "sent": "And there's so many time works the first time you use the regular regularization inside learning process and the second time use the correction.",
                    "label": 0
                },
                {
                    "sent": "That means they correct the outside after the learning process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second group has interest in learning work work.",
                    "label": 0
                },
                {
                    "sent": "What representation and concept representation at the same time they called the Joint Learning of Work and concept.",
                    "label": 0
                },
                {
                    "sent": "The idea behind it, the concept or the sense of work can have the learning of the work building.",
                    "label": 0
                },
                {
                    "sent": "And they use some objective function, for example to estimate the probability of a world concept or.",
                    "label": 0
                },
                {
                    "sent": "But about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our contribution, our position in the related work.",
                    "label": 0
                },
                {
                    "sent": "The listing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we use the strong learning of War concept, a concept and document.",
                    "label": 0
                },
                {
                    "sent": "On tripartite.",
                    "label": 0
                },
                {
                    "sent": "In the same model to learn at the same time 3 reservation for test.",
                    "label": 0
                },
                {
                    "sent": "And we also use.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The raw text and there are no class ratios in the.",
                    "label": 0
                },
                {
                    "sent": "In the learning phase.",
                    "label": 0
                },
                {
                    "sent": "And our model will provide a vector embedding for the test which can be used in order task.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will detail our model.",
                    "label": 0
                },
                {
                    "sent": "Like we like, I have said why we call this trouble time?",
                    "label": 0
                },
                {
                    "sent": "Because we have document work and concept in the same learning process and to do so we.",
                    "label": 1
                },
                {
                    "sent": "We have to.",
                    "label": 0
                },
                {
                    "sent": "We have a document with the must be annotated with the concepts that mean we.",
                    "label": 1
                },
                {
                    "sent": "First, we want to we have to extract the concept of the words in the document.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We will learn the reputation for the uh, document, even though it's work and concept in this text.",
                    "label": 0
                },
                {
                    "sent": "To do this, we based on two assumption.",
                    "label": 0
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "We can learn some simultaneously the representation.",
                    "label": 0
                },
                {
                    "sent": "How they work and the concept.",
                    "label": 0
                },
                {
                    "sent": "To have better learning the empathy.",
                    "label": 0
                },
                {
                    "sent": "The second assumption is that.",
                    "label": 0
                },
                {
                    "sent": "If you can, try the embedding learning with the local screenshot, we can have better.",
                    "label": 0
                },
                {
                    "sent": "Meaning of the word and the document admitting.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the objective function with two parts.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                },
                {
                    "sent": "I'm to learn the distributional semantic of document of the work and the concept that we can't else.",
                    "label": 0
                },
                {
                    "sent": "The second one is to learn the relational semantic via the concept in the knowledge base that we can't LR.",
                    "label": 0
                },
                {
                    "sent": "And finally we will combine these two objectives into the final function.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First part of the objective function.",
                    "label": 0
                },
                {
                    "sent": "We tried to learn the distribution list distributional semantic.",
                    "label": 0
                },
                {
                    "sent": "We extend the power factor model which initially learned the document.",
                    "label": 0
                },
                {
                    "sent": "Everything from the work in this text.",
                    "label": 0
                },
                {
                    "sent": "And here we consider the work with the concept that mean even the work we identify, the concept or the sense behind this work and we will learn by predicting this work target work if.",
                    "label": 0
                },
                {
                    "sent": "It work have, if that work has a concept, we will predict the work and the concept.",
                    "label": 0
                },
                {
                    "sent": "We won the contest in our case.",
                    "label": 0
                },
                {
                    "sent": "In the document the work in the construct and concept in the works.",
                    "label": 0
                },
                {
                    "sent": "Anne, here we have the.",
                    "label": 0
                },
                {
                    "sent": "The objective function to predict the probability of Rewards River in this context.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the second objective, we want to.",
                    "label": 0
                },
                {
                    "sent": "Combine the relational semantic in the learning process.",
                    "label": 0
                },
                {
                    "sent": "Intuition is to push the representation of work and the.",
                    "label": 0
                },
                {
                    "sent": "To press the registration of work that are connected in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "To do so, we first we identify the pair of work that is connected in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "For example, we have this pool of connected work and from this pool from this on pair work we will.",
                    "label": 0
                },
                {
                    "sent": "In the objective function we will reduce we will reduce the.",
                    "label": 1
                },
                {
                    "sent": "At this time.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry.",
                    "label": 0
                },
                {
                    "sent": "We try to maximize the similarity, the cosine similarity between two work building.",
                    "label": 0
                },
                {
                    "sent": "And finally, we will combine two objective function with our LC and LR to define the final HD for learning process.",
                    "label": 0
                },
                {
                    "sent": "And in the end we have a model to learn representation of the text is.",
                    "label": 0
                },
                {
                    "sent": "Here is the world and the concept and the document.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An how we how we evaluate our model our even our model?",
                    "label": 0
                },
                {
                    "sent": "First, we have used the robust data set that is a new document on the web.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we do the.",
                    "label": 0
                },
                {
                    "sent": "We have two tasks.",
                    "label": 0
                },
                {
                    "sent": "The first one is document embedding quality to measure the quality of the presentation document.",
                    "label": 0
                },
                {
                    "sent": "The second one is we test in the IR in the information retrieval tasks, which are the ranking or increase pension.",
                    "label": 1
                },
                {
                    "sent": "And we have three compilation.",
                    "label": 0
                },
                {
                    "sent": "The first one is B2B para vector and the second S. D2 with our model with our.",
                    "label": 0
                },
                {
                    "sent": "Without the relational contract and the first one is our model, our full model with relation constraint.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the document embedding quality evaluation, we use the protocol last introduced in the paper of power after.",
                    "label": 0
                },
                {
                    "sent": "First we will.",
                    "label": 0
                },
                {
                    "sent": "Identify the document triplet.",
                    "label": 0
                },
                {
                    "sent": "Tribles tree document.",
                    "label": 0
                },
                {
                    "sent": "The first one is similar and the third one, third one is randomly randomly choose in the order.",
                    "label": 0
                },
                {
                    "sent": "That means that the one and two are similar and the tree on different from the first 2 and our objective is to measure to calculate error rate when.",
                    "label": 0
                },
                {
                    "sent": "When we when we calculate the similarity of the vector of the first one document, if this similarity is a lower than the similarity of their thoughts and the 1st and the first one that we call that an error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For the in the type of result paper we have.",
                    "label": 0
                },
                {
                    "sent": "We can see that our model SD2 we are have a bet.",
                    "label": 0
                },
                {
                    "sent": "The best reason with the lowest error rate.",
                    "label": 0
                },
                {
                    "sent": "Compared to the TF IDF, we can see that the we had we can see the benefit of this traditional semantic.",
                    "label": 0
                },
                {
                    "sent": "Anne, sorry.",
                    "label": 0
                },
                {
                    "sent": "And compared to the character.",
                    "label": 0
                },
                {
                    "sent": "Our SD to we are better have better reason but we can see there is a symmetric effect when we combine the relation and relation and the distributional semantic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To better understand the effect behind this.",
                    "label": 0
                },
                {
                    "sent": "We have visualized.",
                    "label": 0
                },
                {
                    "sent": "Example here we have a.",
                    "label": 0
                },
                {
                    "sent": "Here we have an image of a. TSNE projection of a query and relevant or irrelevant documents.",
                    "label": 0
                },
                {
                    "sent": "Then like not order 11 document and the white dots are the relevant document for the query.",
                    "label": 0
                },
                {
                    "sent": "In to picture the left one is the.",
                    "label": 0
                },
                {
                    "sent": "Plot of the vector.",
                    "label": 0
                },
                {
                    "sent": "Obtained my character and there I want you obtained by our model.",
                    "label": 0
                },
                {
                    "sent": "We can see that in two picture there is a distinction between the relevant and irrelevant document.",
                    "label": 0
                },
                {
                    "sent": "However, in the in the power factor the queries in file B.",
                    "label": 0
                },
                {
                    "sent": "In the center of the cluster and there, there's not much distinction.",
                    "label": 0
                },
                {
                    "sent": "In which cluster should be found?",
                    "label": 0
                },
                {
                    "sent": "In our in our example here, the query is found near the relevant document, so we can say that and with our model we can.",
                    "label": 0
                },
                {
                    "sent": "Obtain better vector for document.",
                    "label": 0
                },
                {
                    "sent": "So the query.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We I present here the result of the.",
                    "label": 0
                },
                {
                    "sent": "Information retrieval tasks that are ranking document ranking and query expansion.",
                    "label": 0
                },
                {
                    "sent": "And we we parked in the matrix map.",
                    "label": 0
                },
                {
                    "sent": "Metrican recall.",
                    "label": 0
                },
                {
                    "sent": "4th, we look at the Document ranking task for which the rain table here.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "Our model have better reason than the baseline IR baseline.",
                    "label": 0
                },
                {
                    "sent": "That we can see that narrow scope can have ranking on the document and carry matching.",
                    "label": 0
                },
                {
                    "sent": "An comparing our two confirmation, the phone and without relation.",
                    "label": 0
                },
                {
                    "sent": "We can see that the reason is slightly better when we have an intern check the relation semantic relational semantics.",
                    "label": 0
                },
                {
                    "sent": "In the queries mentioned plus we also see the same.",
                    "label": 0
                },
                {
                    "sent": "Tendence this.",
                    "label": 0
                },
                {
                    "sent": "The same reason that the our model, our neural model have a better result than the baseline.",
                    "label": 0
                },
                {
                    "sent": "And we can see also that the best confirmation is the.",
                    "label": 0
                },
                {
                    "sent": "Extension with the concept that mean when we use the.",
                    "label": 0
                },
                {
                    "sent": "Relational semantic, we have better result in the.",
                    "label": 0
                },
                {
                    "sent": "Matching process.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, here we see, we see that our model have a worse result than the previous model, so we have to.",
                    "label": 0
                },
                {
                    "sent": "Make some analysts analysis to better understand the problem.",
                    "label": 0
                },
                {
                    "sent": "Hey, we make a cross analysis to.",
                    "label": 0
                },
                {
                    "sent": "To understand why our model is perform worse than the PV, the power factor baseline.",
                    "label": 0
                },
                {
                    "sent": "So we have identified three sets of query where our SD two.",
                    "label": 0
                },
                {
                    "sent": "We perform better or works equal or better than the PV.",
                    "label": 0
                },
                {
                    "sent": "And in each set with identify the number of concept in the document in the in the relevant document for the query and we can see that in the query manners in the work set in the work area set.",
                    "label": 0
                },
                {
                    "sent": "We there, there are many concept another set when we can.",
                    "label": 0
                },
                {
                    "sent": "This suggests that our model is not able to catch the semantic.",
                    "label": 0
                },
                {
                    "sent": "When the document have many when document Hello, higher number of concepts.",
                    "label": 0
                },
                {
                    "sent": "That mean we have an app that had that.",
                    "label": 0
                },
                {
                    "sent": "The noise in the.",
                    "label": 0
                },
                {
                    "sent": "In the annotation process, when we preprocess the document.",
                    "label": 0
                },
                {
                    "sent": "An we also.",
                    "label": 0
                },
                {
                    "sent": "We also look at the text of the document.",
                    "label": 0
                },
                {
                    "sent": "For example, in the in the query plus set that mean well, our model work better than the previous baseline.",
                    "label": 0
                },
                {
                    "sent": "With we see that.",
                    "label": 0
                },
                {
                    "sent": "They let the concept and the annotation the alignment of concept and the work is better than the in the query minus.",
                    "label": 0
                },
                {
                    "sent": "For example, here we can see that there is some missing annotation.",
                    "label": 0
                },
                {
                    "sent": "For example, here sharks and surfing and.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to conclude.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "We have a unified learning for concept work and document everything and we have reduced the vocabulary mismatch between document and query will also improve the quality of document embedding an.",
                    "label": 0
                },
                {
                    "sent": "Encouraging results for the information retrieval task.",
                    "label": 0
                },
                {
                    "sent": "However, we will have some.",
                    "label": 0
                },
                {
                    "sent": "Reasons that are not very.",
                    "label": 0
                },
                {
                    "sent": "Very perfect then we have some error in the annotation phase so we have to.",
                    "label": 0
                },
                {
                    "sent": "We need to have further investigation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}