{
    "id": "5zyd6gxxiv34sfxctt2yrwbq2jiht6sx",
    "title": "Machine Learning with Knowledge Graphs",
    "info": {
        "author": [
            "Volker Tresp, Siemens AG"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_tresp_machine_learning/",
    "segmentation": [
        [
            "So the title is machine learning with knowledge graphs.",
            "This is joint work with Maximilian Nico, who is now a postdoc at MIT and with contributions from Sharon Young and Dennis Krampus to current students."
        ],
        [
            "Yeah, so my background is in machine learning really and I got involved in semantic web problems or challenges maybe six years ago and I really enjoyed it because thinking and learning about the semantic web really clarified many things which sort of were unclear before for me.",
            "So it was really great.",
            "I had an immediate love affair with RDF.",
            "Nothing is ever wrong in RDF and no contradictions.",
            "I learned that from Frank.",
            "So we have a English chef who teaches the world how to cook.",
            "I don't want to explain the other images.",
            "No contradictions in RDF.",
            "This is the first workshop I attended here and we published our first paper in this domain.",
            "Great organizing committee.",
            "Claudio Nicola Marco, Agnieszka and Voytek.",
            "And it has been 1823 days since the workshop happened, and this number is increasing daily."
        ],
        [
            "So today you're more of a machine learner and I try to convince you that semantic web has good ideas."
        ],
        [
            "So why does machine learning need knowledge?"
        ],
        [
            "Yes, sort of remind you what is machine learning 'cause there's statistics, state of mining statistiques.",
            "You typically care about one parameter which tells you if this treatment.",
            "The medication had a positive effect.",
            "No effect or negative effects.",
            "So really care about interpretable parameters.",
            "And and data mining.",
            "You focus on the discovery of meaningful patterns to explain the data to find interesting structures to solve different tasks.",
            "Machine learning traditionally focuses on prediction accuracy.",
            "You want to have the best predictor for some task and you don't care too much about the meaning of the parameters or even interpretability of your model.",
            "Or sometimes you do, but often you don't, and I think these are really.",
            "This explains, although there's a common based on data analytics statistics.",
            "That there are really also a lot of differences in the way people work with these with data.",
            "So you want to think about prediction."
        ],
        [
            "The workers, of course, is a classifier, so here you have 10 digits handwritten digits.",
            "You want to classify you have 10 different classifiers, one for each digit, and you predict how likely it is that this picture is a one or a zero or two.",
            "And this is a very powerful concept, and it's also surprisingly general.",
            "So we actually need a little bit."
        ],
        [
            "I miss him, so let's say FK of ZL tells you the probability or some sort of score for class K4 digit K. Given your input the error so it would be some future derived from these images.",
            "And then here are some classic models for F, so that's an important part.",
            "The first one I fixed basis functions, so basis functions can be very general or generic, like ocean basis functions are polynomial basis functions, or they're very domain specific, like edge detectors and images, and then the only thing you have to do is to learn tune these weights on these spaces functions and that's what gives you the FK of CL.",
            "Related approach of the kernel approach the kernel approach mathematically, it's exactly the same, only that it's more efficient if you have many basis functions or maybe even infinite basis function, then it's more efficient to work with kernels because the summer is over the number of data points and before it was about on the number of basis functions.",
            "Of course, this end can also be problematic if you go to really big datasets, and this is really the inherent problem in kernels.",
            "If you want to go to big data, but mathematically it's it's the same, you can transform one into the other.",
            "And of course the old and new big thing is newer networks since the couple years they have been coming back with force Deep Now means that people work with maybe 10 layers.",
            "And 1000 neurons per layer.",
            "So really big neural networks, and they're really extremely hot and most of you probably know about it."
        ],
        [
            "So they've been really successful in reducing errors in some very important benchmark, but also real world application specific datasets.",
            "For example, they got 30% reduction in important task and speech recognition, and if you have an Android smartphone then you have a deep neural network running on your Android Android smartphone for speech recognition.",
            "And if you are connected to the server and even better one is running on the server and it's also speaker independent.",
            "No, that's it.",
            "You don't have to retrain for each speaker, so it's a huge success.",
            "Other successes, an object recognition images, and analyzing of textual data.",
            "And all the major companies in this area.",
            "Google, Microsoft, Facebook, Baidu are all investing heavily in deep learning and my own student, former student Kieu is, and our research director at Baidu.",
            "And he's just setting up a research lab for deep learning and Silicon Valley.",
            "So this."
        ],
        [
            "One of the guys entering, Oh yeah, he's also hiring and running away from Google to Baidu.",
            "Kurt was in the news, so entering got famous for discovering cats and images.",
            "And here's the New York Times.",
            "Very famous newspaper.",
            "So how many computers to identify a cat?",
            "The answer is 16,000 because he's using the Google infrastructure for his task.",
            "I guess in the future we will use the Baidu infrastructure for this task.",
            "So."
        ],
        [
            "That's what we're from here.",
            "Deep learning networks sees more cat images in any child.",
            "But it's not as good as at this task that children, of course, better so deep learning community thinks we need better unsupervised learning to pre structure the network to 1st learn about the statistics of the real world as a basis for solving specific tasks.",
            "But maybe from the semantic web community you think maybe we need background knowledge and also we don't just want to detect cats images, we want to do more."
        ],
        [
            "So we wanted also attack dogs.",
            "How is this people and so on?",
            "The instances of many other classes we want to recognize specific instances or entities, so this is my cat mix.",
            "We work with 10,000,000 entities.",
            "We want to predict attributes.",
            "Max is Eva and we predict relationships.",
            "Makes likes Mary and our experiments enter the 14 possible triples.",
            "We can predict this is the on the order of the number of synapses in your brain.",
            "That selection of Masco School to be sort of the brain scale somewhere."
        ],
        [
            "Maybe this is the vision that you want to have a system which not only do text as a person, but also recognize the person as being Obama has some background information that he's married to Michelle and then nicely ask how's your wife Michelle doing?",
            "So."
        ],
        [
            "This is what I'm doing now.",
            "Anybody from Greece, yeah.",
            "Nobody can can you?",
            "Can you say it?",
            "Very good, so the central stuff is essence and one of the other ones is ours and the other one is bringing, so bringing us to essence more.",
            "I guess to create and it's not always but semantic web ideas."
        ],
        [
            "We need to know about entities, attributes and classes in the world and the various relationships that do or might exist between those we need ontologies."
        ],
        [
            "And this is of course I'm not just some strange things invented by semantic Web people there.",
            "Real ones heavily used in medicine and biomedical Sciences, and to classify diseases to other diseases or thoughts of things they might not be as formal as some people want them to be, but they used heavily in in medicine and then of course."
        ],
        [
            "Have our own favorite ontologies here.",
            "More dimension, pedia.",
            "Jago, Freebase and Google Knowledge graph.",
            "You know that Freebase was bought up by by Google and but still as a public version.",
            "So recently there were two billion facts and Freebase version and Google is using it to improve ranking proof information extraction and also to display information about entities and persons on your search result.",
            "So it's really used in a major industry which is quite."
        ],
        [
            "We don't have to take talk about linked open data here."
        ],
        [
            "And we know that the basis for this knowledge representation is the Triple Max likes Mary, so I can be fast here."
        ],
        [
            "One interesting thing is about these structures is that they are easy enough to understand that they are also accessible to people from machine learning for example.",
            "So this can be a really important bridge from the semantic web community to the machine learning community.",
            "Because this is not too much.",
            "Logical background required here.",
            "You understand these facts very easily.",
            "You can make sense out of them and so I would expect that this will be become even more important in the future and will be integrated and in more machine learning tasks."
        ],
        [
            "So let's talk about.",
            "The.",
            "The knowledge of statistical relational learning.",
            "So the task."
        ],
        [
            "So so I said we had machine learning.",
            "We care about prediction tasks, so I think the sort of the Canonical or the general task is to predict the existence or non existence of triples.",
            "So you have subject, predicate, object, EIRKEJ and you want to predict as in reasoning for example, but with other methods.",
            "If this triple makes sense, if it's true of its faults.",
            "And since machine learning typically cannot make a 01 decision, you want maybe get a probability out that if this triple is true or false, and so we can have one classifier for each relation type for each property.",
            "And have some input describing the subject and object and then simply train as many classifiers as we need and we're done.",
            "So very simple.",
            "Let me build one classifier for each relation type K. The only question is what is the input?",
            "The input describing this triple really with the information which is important to classify.",
            "This triple is true or false."
        ],
        [
            "So the first approach we work with known features, so these are futures which can be extracted from.",
            "Data from the Knowledge Graph from the from the from the RDF graph.",
            "So simple ones, and if these are persons features might be age, sex.",
            "And so an income that could be also features derived from a neighborhood of the entity in the environment of the RDF graph.",
            "This is what typically people are using, so the number of connections coming out of this connecting to this entity or specific entities.",
            "Is it a human as it's connected over so many links to something else?",
            "But they are really sort of done as a preprocessing step using prior knowledge of the.",
            "Of the people who is designing the system.",
            "Maybe there's also some structural search, but at the end you have a set of known attributes describing the subject and the objects object.",
            "We have this AI one to ARR.",
            "Which are are different features describing entity I?",
            "And we have the same number of features describing entity J.",
            "And then we just contact concatenate them.",
            "These are known attributes, known features and form this Z input vector, which is just the other credit dated version of these two individual attributes.",
            "And then we put them into our different systems.",
            "And we can train the classifier.",
            "So in the semantic web community, the kernel approach is most popular.",
            "I would say because you cannot be very smart and describing interesting kernels, which sort of tell you the similarity between entities.",
            "And but then again, it's a kernel learning problem.",
            "You have to have to learn the visa in this case and you're done.",
            "And I think I'm getting a did a very nice paper on this two years ago at this conference.",
            "Connecting like different kernel approaches and testing them.",
            "So, but this is in some sense it's simpler.",
            "Now you have no one features.",
            "The future might be different, difficult to calculate, or might be difficult to sign the right features, but after you have the features, everything is simple.",
            "Again some sense, of course you have scalability and all these things.",
            "So in this talk, it's mostly about latent features.",
            "So we have the same thing.",
            "Well, only that we have no idea what these AA stand for.",
            "Well, they are.",
            "They also have to be learned unknown to us.",
            "Again, our UI is described by AI 12A IR.",
            "The same thing with DJ.",
            "And again, we concatenate them into the input vector Z and we can put them into a system with fixed basis functions, or we can put it into another network.",
            "But now is the problem.",
            "Of course, it's much more difficult now because not only are the WS unknown like in any system based function system.",
            "Also we have no idea what these ads are.",
            "They also unknown and they form the Z in this basis function setting.",
            "Interesting and passing.",
            "I want to also say this something relation related going on and help community and a paper by Colbert unquote coworkers.",
            "They do similar things, only that they replace the basis function.",
            "Everything by neural network and they got very good result and a lot of deep learning tasks or learning tasks and.",
            "I call it NLP from scratch.",
            "You might have heard of this paper, but we're working today on the left side.",
            "So we have basis functions.",
            "We have inputs we don't know about and we want to learn this prediction model.",
            "That"
        ],
        [
            "One nice thing about.",
            "Advantage also that the ace are unknown or just shown over here, so these are entities and each entity has its own set of features, latent features, so they are unknown.",
            "And then this is what they want to predict.",
            "The existence of this triple for some relation R. And if you studied Bayesian networks or its graphical models, this is a sort of a nice repeatedly structure, and in this we structure.",
            "This is Collider down there and the Collider opens up if it's known.",
            "So if we have in our training data triples which are known information can flow through these colliders and the bottom and the parents open up if they are unknown.",
            "And of course our ASR unknown so they're open so we can have global propagation of information in these.",
            "This network of random variables.",
            "And the advantage is that we get something we call collective learning.",
            "For example, if you want to decide if Jack is rich, maybe you know his father is rich.",
            "This is relatively local, but maybe you don't know the income of the father, but you know that the father of the father is rich.",
            "So this information is further away in this knowledge graph.",
            "And by this trick or this exploiting the structure, there's some chance that information propagates.",
            "In this graph, so we can have can make information which is far away local for the decision."
        ],
        [
            "So what basis functions should we use?",
            "So we have our features for I&R features for J and makes sense?",
            "Maybe to consider all individual products of the different features, so we get these product basis functions are polynomial interacting basis function shown over here.",
            "So this captures sort of the different interactions between the different latent representations of the two involved entities, and these are basis functions.",
            "And now we have a double sum.",
            "Just because we have more indices.",
            "But still we have to learn the WS and the AAC is still very difficult problem."
        ],
        [
            "Now we can rewrite it a little bit.",
            "Potentially we can write this sum this double sum, so now absolutely substituted the ace for the B in this nice compact form.",
            "It's a, it's a it's a vector of attributes for energy.",
            "I transpose times matrix RK and times AJ, which is the latent representation for the entity J and OK simply contains all these weights in the left equation.",
            "So we change notation a little bit.",
            "So now our K is, in our terms are matrix before it was W as a vector turned into matrix.",
            "Now are is a matrix and we have one of these matrices for each relation type and we just can stack them on top of one another and we get something which is called a tensor.",
            "And we call it court enzyme.",
            "So we take the R one or two or three.",
            "It's like come on top of each other.",
            "Get a quote tensor.",
            "And you can also take our raw data, so which describes which triples we know to exist or not exist in the in the raw data into a tensor, 'cause we have three indices, subject, predicate, object, and we also get it turns out.",
            "So now the learning of all these difficult things up there WAA becomes a problem of factorizing this tensor X.",
            "So this is."
        ],
        [
            "Graphical visualization of what I was just saying on the left side you see the raw data tensor which contains mostly zeros and a few ones representing the effects or the triples.",
            "We known to be true and we have to of course compress it heavily because this is huge.",
            "This is this 10 to the four elements, and then we just vectorize it into the matrix and the R Quad 10s or the Aqua tensor is shown in the middle.",
            "This cube over there.",
            "And one slice or one matrix is responsible for one relation type for one property.",
            "And the other two matrices are identical and they contain all the latent vectors for all the entities we are concerned with.",
            "So this is the definition of.",
            "The entry in the in the 10s are.",
            "This is just our raw data, mostly consisting of zeros and a few ones.",
            "And and the ASR really these days we need in the in the.",
            "In our modeling our basis function model and the court ends are really gives us the weights.",
            "On the basis functions."
        ],
        [
            "And we could put this as we can put this into a nice optimization problem.",
            "We penalize large weights.",
            "Regularization always helps in machine learning.",
            "And we can work with different cost functions.",
            "We can work with Gaussian cost functions which give you the squared error or Bernoulli which is more appropriate because the data is sort of binary."
        ],
        [
            "We can do very efficient updates using an alternating least squares update rule where we can exploit data sparsity so all the zeros drop out in the update equations.",
            "We only concerned about the ones and this is very fast and this allows us to scale up to these large domains.",
            "Other people are using stochastic gradient descent, but we didn't get too good results with stochastic into sent yet.",
            "And we're not live."
        ],
        [
            "And it's a binary relations.",
            "We can also work with simulations where ascential it's an inner product of the latent representation of the entity, with some vector describing the unit relation.",
            "And we can go to higher orders if you're interested in.",
            "Of course, things become more complicated."
        ],
        [
            "Scalability is quite good, so it's linear in most important quantities.",
            "It's linear in the number of entities linear in the number of predicates or relation types linear in the number of known facts.",
            "So we have to work with sparse matrices of sparse tensors if at all possible.",
            "On the bottom left you see if we include unary relations, so attributes in a smart way in the.",
            "Tensor is this.",
            "Red Curve is a smart way we they don't really play an important role in the computation if we do it in a not smart way.",
            "We also get a linear increase there and the last one is also important because it's the rank over approximation that there was this R which tells you how many how many latent features you wanted for each entity, and in our new results is that it's cubic in the number in the rank which is the red line down there, and in a naive implementation it was.",
            "Out of the power five or six even, and this was one of Max's major breakthroughs that he could reduce it to the power of three, which is typical complexity even in matrix factorization."
        ],
        [
            "So 1st result is on sort of typical benchmark data.",
            "So essentially what you're doing you take, you take your knowledge base, you take out some of the triples as I mean, assume you don't know that they are true, then predict a lot of triples once, which are really not there.",
            "The data and the one which is there in the data which we took out.",
            "And then you hope that the one you took out get the best results.",
            "So it's not really ground truth because we don't really know if the zeros are zeros or not, simply not known to be true.",
            "But this is the way people in this community sort of compare different approaches.",
            "So this is a data set on kinship between an Indian tribe in Australia who molests data from the Unified Medical Language System and Nations describes political interactions between countries in the Cold War and the right one, the brownish one yellowish brownish one is the rest kind model, and the plot is AOC in an area under curve precision recall and you can see that it's either the best method or it's.",
            "Among the best methods so it gives you confidence that it really performs well on different data sets on the benchmark datasets.",
            "So this would be predicting relations like doesn't Mary Max like Mary, so it's a relationship prediction problem."
        ],
        [
            "This is an experiment on the Cora data set, also often used in relational learning, so it's a noisy data set where some authors duplicates some titles, have duplicates in publications of public duplicates, and so on.",
            "And the task is to identify these duplicates and say these two people are really the same.",
            "So in some sense it's like object recognition or you have some description of an object and you want to find the nearest neighbor which is closest to your.",
            "Object you're interested in, and So what we're looking at essentially, after the factorization, we look at the A matrix and they describe the entities, and then we look at distance in terms of this, a representation in terms of the latent representation of the entities, and if they are very similar after some normalization.",
            "Of course, then we can say these are likely the same ones.",
            "And again, here's some AOC prisoner recall results.",
            "Rest Kyle gives you.",
            "I mean, all methods give you quite good results.",
            "Here we have some results on Markov logic with basic rules, so there we are much better if you invest a lot in the rules and get to complex rules, then becomes sort of comparative, but the results are all in this measure quite high, which is not so easy to get."
        ],
        [
            "There's no.",
            "How do we apply this to the YAGO Knowledge Graph?"
        ],
        [
            "So we factorize and yoga knowledge graph with the version of a couple years ago they are going to call ontology 2.6 million entities, 340,000 classes, 87 predicates and 33 million known facts and potentially 10 to the 14 effects we can predict."
        ],
        [
            "So here we do a classification task.",
            "So predict that something is a cat.",
            "For example, in our case, we predict that something is a personal location or a movie, and this is statistics.",
            "There were 800,000 persons, little database for details locations and 62,000 movies.",
            "So through the random prediction sort of reflects this statistics a little bit.",
            "It's easier to predict person because so many persons in there in the setting a, we only took the type information we want to predict.",
            "So only if it's a personal location or a movie and predicted that.",
            "But it could conclude that something is an action.",
            "Movies, also movie, which of course in terms of semantic web reasoning is very simple.",
            "But we had to learn it, sort of in the system, but learned it very well.",
            "So the.",
            "We've got values close to one for personal location and movies is always more difficult, but the lift from 06 to 75 is also quite impressive.",
            "And section setting B, we removed all type information so also action movie and so on.",
            "And but the performance is still quite OK. That's not too bad, and but then you can play and you can add textual attributes.",
            "For example for the person in the movies and so on, and then improve your classification classification accuracy again."
        ],
        [
            "This is something where we show that this collective learning is so important, so we want to predict if some writer is a French right or German right or British writer.",
            "And this we know from the database he was born in Paris, but not too many writers in the service of one in Paris.",
            "So that doesn't give you so much information.",
            "It's important to Paris is in France because it's correlates with that.",
            "He's a French group, right?",
            "So so this information had to propagate in this system in this collective learning.",
            "Way and helps you then to predict the writer nationality and see the left three methods.",
            "Don't have this collective learning random.",
            "Of course sons and CP and you so show that there is a performance is not as good.",
            "The rest: The right explores this information which is further away and gets you very good results in Markov logic networks.",
            "Also if you really nice handcrafted rules also gives you compare results."
        ],
        [
            "We can also learn taxonomies also by having this AI with later representation of the of the entities and just do a hierarchical clustering on on these and.",
            "And on the top level there's also movie domain.",
            "We got very nice agreement with ontology and then if you go further down of course the number of data points for each cluster goes down so it becomes a little bit more difficult."
        ],
        [
            "Um, sometime.",
            "There are different extensions and we are currently working on many of them.",
            "One is to insist that A and I must be non negative.",
            "Put that constraint on the parameters.",
            "The performance goes down a little bit.",
            "What you're getting is very sparse effectors, because this negative constraint encourages sort of attributes to become zero.",
            "So the attribute vector here for the original rest call on the left side is full, 100%, non zeros, but for the different versions of the non negative factorization you get very sparse solutions.",
            "Also, it's much easier to interpret.",
            "Your result if you want to interpret what you're getting out.",
            "If you have nonnegativity constraints, because then everything has to add in a positive way and it cannot cancel out in a complex way.",
            "If you have plus and minus is.",
            "Also, sometimes you get nicer probabilistic interpretation of if things are not negative.",
            "Worry what you do with negative numbers at the end, so this is an interesting direction.",
            "I think in particular, if you're interested in visualization, nonnegativity really helps a lot."
        ],
        [
            "Max also did some theoretical analysis and essentially he showed on a theoretical way and then also experimentally that if your data are really comes from a tensor structure, so you get the best results.",
            "Also obeying this tensor structure because what you always can do is take your tenzer and sort of make it flat, making a big matrix out of it by just concatenating all the slices in different ways.",
            "So essentially making a turning the tensor into matrix and he could show theoretically.",
            "And also experimentally that this is not optimal if you.",
            "Care about reconstruction accuracy?"
        ],
        [
            "So this was more on sort of benchmark data and now we want to also talk a little bit about one application we care about."
        ],
        [
            "We want to apply these things so this is one problem a little bit in relational learning.",
            "It's not clear where you really say.",
            "Here's the real world application where people are using a relational learning, except maybe for recommendation systems, which is sort of an instance of relational learning problem in some ways.",
            "So we can think about issues within the domain, so you have the entities in your training data and you what we did before ever so far was always predicting for the same entities.",
            "Something like new relations or so.",
            "So Biblical triples that classification, clustering, taxonomy, ontology, learning, entity resolution visualization we didn't show up.",
            "But this is always an important thing in tensor factorization, and we can also do simple forms of.",
            "Privacy querying also take into account the probability values we get out of the rest car model.",
            "And I've got simple ones who wants to be friends is just a triple prediction.",
            "Very simple and the more challenging one is.",
            "If you have more like.",
            "Sparker types of queries with variables and so on, and this is what we are currently also exploring.",
            "Then you can also think of new entities outside of the domain.",
            "So the first approach is to simply calculate.",
            "I mean sort of fold the new entity and with its information you have the new entity into the current system you're having.",
            "So essentially what you're doing, you're calculating the latent factors for the new entity, and then if you have that, then you can apply all the methods before and in some domains we get pretty good results here, not everywhere.",
            "So this is something we are still exploring, but it seems sometimes it works quite well.",
            "And she want to recognize new objects, but you can also turn an object into a query and I find something similar to my query object and there's also something we actively exploring."
        ],
        [
            "So this is the.",
            "Application we are currently mostly interested in because we're not really working on the web on web data at Siemens, not so much interested in social media, but of course a clinic is sort of a small world on its on its own very complex.",
            "Domain with the personalized medicine where you really want to do specific recommendations for a specific person for specific patient.",
            "The idea is also to find to do a global modeling of the clinical data.",
            "I'm not go away from like I want to model something small here and something small.",
            "There of course is very challenging.",
            "We have different use cases in one of them we say all data from all patients, so this is how I sort of big data.",
            "Some of you and things just give us everything you have or as much as you want, and we want to find interesting structure in this patient data.",
            "Also maybe predict unusual things.",
            "So if you suddenly something is not the breast cancer with something else, there's a physician at the right time.",
            "Refer to the other Department and things.",
            "Things things like that.",
            "So this is probably also the most difficult data set.",
            "Also, if anybody who worked with medical data, it's not the way you want it to be nicely.",
            "Timely ordered and you can nicely see cause and effect.",
            "It's probably not as nice, but we want to get our medical partners to improve data quality on these issues.",
            "Then specific cases breast cancer, very complex disease with genomics plays an increasing role.",
            "Also something where the patient is a patient.",
            "Data over longer time period then come back to the clinic with several times and same thing in their pharology so kidney transplantation where the patients are also observed over the lifetime of the kidney at 1020 years or so.",
            "It's a very interesting sort of optimization routine you.",
            "Have to give medication but you don't want to medication side effect.",
            "You want to minimize side effect.",
            "You have to find the right level of medication and you clearly have this temporal effect temporal aspect that you have a sort of multi stage optimization problem and our partners in charity.",
            "On the right there in Berlin.",
            "Really good data.",
            "The breast cancer and the other day to come from the clinic in Erlangen and we also have another group partner in there who has.",
            "Medical studies and have some rights on this data, so this could be interesting to compare different clinics if that is at all possible.",
            "Of course all these issues have a critical in terms of data safety and data security, privacy and so on, but the challenge is it sort of fits into the semantic web word.",
            "Ontologies play a big role and medical domains.",
            "Their patient complex relational data.",
            "The patient in a clinic.",
            "Many diagnosis procedures, measurements, attributes, patient history and so on.",
            "Presenting time is important.",
            "These are often sequential data and knowing what happened first, what happened X is important.",
            "Patients sometimes come back to the same clinic.",
            "They shouldn't come too soon because then they didn't really sharp and in America you get penalized for that, but they come back maybe with the same problem.",
            "Then you have also multi multi stage data.",
            "Another is issue is decision modeling or decision.",
            "Invitation that you want to maybe also give recommendations.",
            "So in the first level we want to just observe and model data and just say this is what typically is done and you're doing something unusual.",
            "Are you sure you want to do it?",
            "Sort of.",
            "These sort of things at a later stage.",
            "Of course, we also want to say, OK, maybe this seems to give you better result if you increase the dosage.",
            "Sorry, decrease the dosage here.",
            "This type of things.",
            "So this is partially find the futures of course, but then you get into all these.",
            "Issues of confire, confounders, and causality.",
            "There is this really a causal relationship or just an observed relationship?",
            "Then suddenly we have to care about parameters again, so that's an interesting challenge.",
            "Then there are a lot of tons of unstructured data, alot of reportes background information.",
            "Of course on the web a lot about diseases and symptoms and all sorts of things.",
            "Recent research results, so this was a little bit in the direction of what IBM Watson is doing, but also in the clinics you have tons of reports very difficult to understand for machine, they don't.",
            "They have their own grammar.",
            "Only way of formulating things.",
            "And it's a very challenging task too.",
            "Make me to go beyond Entity resolution entity detection in these texts.",
            "But on the other hand, we're not too much interested in.",
            "Really understanding the text, we want the text to help our other tasks.",
            "So if there's information in the text we want to use it.",
            "If it's not there, we have to live with that word or whatever we have.",
            "But of course there are clinical situations that not all the information you want to have is in structured form, and a lot of the information you really care about.",
            "Truly, clinical evaluation of patients is really in textual data, so we have a very strong company in Germany or specializing in German.",
            "Medical texts are verbs.",
            "In the team also images is a big issue here because this is some strength of Siemens.",
            "We're really good in image analysis, so we have a very strong team in the US in Princeton.",
            "And there really world leading in this image analysis tasks.",
            "So we also want to include.",
            "So one thing is we want to include image features to help the other tasks, but also the other way around.",
            "They are interested in having background information on the patients to improve their image Ng tasks.",
            "So the idea is that image Ng alone is.",
            "If you think of it as a diagnostic task, maybe not as quite as good as physicians are today, but if you include all the background information the physician has then it gets up to the level of the ovary physician.",
            "And of course I mixed data is increasing in importance.",
            "There expression data, genetic data, inherited daytime inheritance, data data in your germ line and your maybe your cancer cells.",
            "So this is definitely the future and we are sort of.",
            "It's not the main focus of this project, but we also want to connect to OMICS data.",
            "And these are in issue."
        ],
        [
            "Results predicting diagnosis and procedures.",
            "This NDC zscore something used in.",
            "And ranking systems.",
            "And we see some structure at the Black one is the random prediction and we get much better results.",
            "We also see one problem with the factorization approach.",
            "The highest rank gives you the best results and this is typical.",
            "So we also one thing we're exploring is how to.",
            "Modify the system that we can work with a lower rank in the approximation.",
            "I mean the rank of 1000 tells you the medical domain is highly complex and not too surprising.",
            "So this is not yet where we want to be, and Wilson indication that there is some structure in the data we can explore."
        ],
        [
            "And of course, here, as I said images.",
            "So this is like maybe like the sensor information we have to work with images and textual data.",
            "So this is also an interesting challenge to make this work.",
            "So we have, as I said, we have a small world."
        ],
        [
            "The big word here.",
            "A little bit.",
            "This is some more closely related work.",
            "Of course there's a lot of work and learning on semantic web.",
            "Our own publications, then here.",
            "Of course, Stephen stabs Triple Rank first approach, probably to do to use tensor factorization for tasks in the semantic web.",
            "And so this is our own paper here at this conference, where we still using matrix factorization at that time.",
            "The Red one is maybe interesting, because if you think this is all like nice but not very important.",
            "But I was talking about the Americans are doing it too.",
            "And this is not a very good group and we're in the same entering.",
            "I was talking about earlier.",
            "Chris Manning is one of the leading guys in information retrieval.",
            "Richard Tucker is a German guy.",
            "Like really big on neural network technologies.",
            "So they're doing something slightly different, but there's a lot of similarities.",
            "They also used, sort of the rescue factorization, and now we even get them to cite us.",
            "There was a big challenge, but so and then they got a grant from Google to work with.",
            "The approach on the Google graph.",
            "I also got the same offer, but I was not.",
            "I was working at home, so I'm still working at 7, so I couldn't.",
            "They wouldn't want to give me the money for some reason other."
        ],
        [
            "Coming to conclusion.",
            "So I mean, it's really amazing the knowledge graph, everything related, like the pedia, Wikipedia Jago, and Freebase and so on.",
            "With the first time in history is a large general ontologies are available and I would think they are become increasingly important also for machine learning tasks and not just for task within the semantic web domain but also to support different machine learning tasks.",
            "Supposedly knowledge graph is already used in search.",
            "Uh, applications at Google, and so I think these are really interesting future developments.",
            "And then I talked about relation machine learning, mostly focusing on the Red Skull approach.",
            "I don't know if I mentioned that you quoted Mexico dress code, so we got scalable relational learning with very competitive performance.",
            "I think the numbers are presented a pretty state of the arts in terms of scalability.",
            "We showed that exhibits collective learning, which is important if you wanted to relational learning and we're currently working on many improvements and extensions, and I think you see a lot of things similar to other ideas, some quite exciting.",
            "So then we applied it to the YAGO Knowledge graph and short experimental results, and a number of relational learning tasks.",
            "And yeah, one challenge is too.",
            "No get to really relevant and maybe application relevant use cases.",
            "Text understanding with ontology support is important.",
            "I mean there's tons of work in this Community, of course here to support image understanding to work in the clinical data and also question answering problems or these sort of things.",
            "So thank you for your patience and your attention.",
            "This is."
        ],
        [
            "Thank you for the interesting talk for faster so I would like to ask about do you have any tips for semantic web people who who still don't know about machine learning so much?",
            "And then they won't say to incorporate some some features and benefits of missing learning to the web since, say, massive learning is good for working with.",
            "Predict if for predicting information and semantic web is usually good for for things that are clips that are that are strict.",
            "I mean it's it's there.",
            "Both of them are actually different worlds.",
            "And then for example I'm I'm very new for machine learning, so I've just stopped at learning, say classification clustering and then I'm for me because I have a bit of background for semantic web.",
            "And then I want to learn machine learning.",
            "Do you have any tips for for me and also for other guys who have the same back one for me too and also missing then of course there are great tutorials now in the web and this online course from entering and machine learning very popular.",
            "It's a typically have to find your own way a little bit.",
            "It's there's no like Golden Way, and you have to invest some effort.",
            "I mean, I think one bridges that was machine learning.",
            "For example, we can predict triples, a faulty triples attributes which are there, but maybe should not be there, and also predict the other way triples which should be there, and I'm not there, so it's it's also an interesting link between 2 the both domains.",
            "I think it's not so easy to say.",
            "Well, how do I think the important thing is that you understand how a machine learner looks at the world and the machine learning should stand.",
            "How semantic web person looks at the world.",
            "And although I learned a lot, I mean I don't think I understood everything you guys talk about in semantic web domain and particularly people talk about ontology.",
            "So but then one guy once said me, I think was here is intact.",
            "They only seven people in the world who understand ontologies really.",
            "Or whatever.",
            "So that's a little bit of a problem.",
            "So I much difficult is for you to understand how machine learning works.",
            "A little things.",
            "And for machine learners definitely also difficult to understand how a semantic web person thinks about the world.",
            "So there's definitely some culture like effort to understand each other.",
            "In machine learning people find certain problems absolutely exciting where you probably think it's not very challenging and the other way around.",
            "Machine learning people typically get confused by all the notations and and logics and stuff, and I think it's it's.",
            "It's one of these things where cultures are really different and you can make some effort to understand the culture and get a lot out of it, but 100% is always difficult.",
            "I think both ways.",
            "I think I mean, I think the bridge is a knowledge graph and it's really important.",
            "It's really contains a lot of information, I mean including of course, YAGO, anti PD and all these things and it's something semantic web people are happy about and I think machine learner also can relate to, for example, the effort on learning and the Knowledge Graph is done by Kevin Murphy who comes from the machine learning and he is leading this team.",
            "So I guess he thinks he has enough understanding of the structures that he knows how to do.",
            "Learning in these structures.",
            "So I think this is a very important connector.",
            "Hi Folker, thank you very much for a great talk.",
            "Thank you for being able to convey so much statistical stuff in such a short time to all of us.",
            "I have a question that will actually 2 questions.",
            "Question number one.",
            "There was a paper here in 2008 that showed that if you combine logical reasoning with statistical relational learning that you got superior performance in doing either of them by themselves.",
            "The tensor stuff you've been showing us is kind of trying to make statistical learning so smart you can.",
            "It can exploit some of the structure in in in the graph as background data, it does not exploit logical reasoning.",
            "Do you think that that statement from 2008 is still true today that we could actually use logic to enhance it?",
            "Or certainly I mean of course we always think of the simplest solution, which means you first do your logic and materialize everything you you can conclude.",
            "And then we have a more complete database.",
            "And when we do machine learning on that.",
            "But there are definitely many other ways of combining it.",
            "A lot of stuff is presented here.",
            "Also, a machine learning on the semantic web.",
            "I mean you have your own work and other people have very different approaches of doing that.",
            "I think in this Community lot's LP is probably an important approach to machine learning here, or I think people have more lenient towards data mining.",
            "Way of thinking about data.",
            "So maybe today you understand a little better how machine learner.",
            "Typically whatever statistical machine learner things looks looks on the world and things about the world.",
            "But this does not solve all the problems.",
            "This is more appropriate if the data is noisy.",
            "So if this if you have a very dependable logical background knowledge, then you probably would not do this type of machine learning and I think for example in the chemical domain where you can explore chemical structures and there are a lot of really almost deterministic dependencies, IO P for example is very successful, so it's definitely not one.",
            "Method fits all but, but this is something that we thought was interesting, and now it seems other people in the machine learning community thinking about similar things.",
            "I think there are really interesting links to NLP type of problems.",
            "To be able to integrate different knowledge sources and they use sometimes more neural network structures.",
            "And we think maybe 10s or structures would be more appropriate or more useful and easier to use, but I think there's a lot of crossover going on now suddenly and that's what I find exciting, but definitely true.",
            "So maybe it's second, completely different question.",
            "You mentioned projects in the in the medical domain.",
            "How would you feel about using background knowledge that comes from the web which is sometimes?",
            "Questionable for medical, you know, inference where you may incur some liability in terms of what you're actually predicting.",
            "I think this would be very useful information because machine learning like Markov logic network just will start to ignore information which is not reliable.",
            "I mean the whole issue of how to make this into something which can be used in a clinic.",
            "There's still a long way in many directions.",
            "And the question, I mean how OK?",
            "How is, for example IBM Watson trying to present their knowledge to the end user have also a problem there.",
            "So I think the feeling is that this these are important problems which have to be addressed, but it's not clear yet how they will become part of the medical practice.",
            "So one option might be if it's clear there are ten options for treatment and they all seem to be OK. And then we just do a ranking which might be helpful for the physician.",
            "One way is just to say what you're doing seems to be unusual.",
            "This is what people are typically doing.",
            "Very close to the data, maybe this can be accepted, but all these issues are very difficult to deal with, but hopefully we find a way of solving some of them, I would think.",
            "Could you also say a few words about data mining?",
            "You have this nice delineation between machine learning and data mining.",
            "Could you also say something about what the state of the art is in that direction?",
            "Not really too much now.",
            "I mean, I could say few things, but they're probably wrong.",
            "But I know there's a lot of interest in the community on more on the data mining side.",
            "Of course, my the chair of the Department at the University is critical, and he's more data mining guy.",
            "And in the next years we're going to work more closely together because we will have a common project and this will be quite interesting for me to understand more in the first level.",
            "You say it's almost the same, but if you look a little bit deeper.",
            "At some point you notice that people also think differently about the world and really have preference for different method methodology's.",
            "But of course, for example to find interesting structures in data.",
            "That's what I think is is very interesting.",
            "Now we.",
            "In the medical domain, for example, maybe you find interesting patterns.",
            "First with which the machine learning then can work almost like a conditional random fields and Markov logic networks.",
            "So I mean there's this issue that Yahoo account supposedly has this magic touch on.",
            "Doing some restructuring of his.",
            "Jellico was always doing deep learning machine learning but there saying was always like he's the only guy who get it gets it to work and he seems to have some secrets.",
            "So maybe it is that he is doing some interesting pre analysis by some data mining finding interesting patterns and then use these patterns as a structure for this machine learning because we did some analysis of order sets of orders previously.",
            "And of course you find certain patterns like a pregnant woman coming to clinic always gets the same sets of.",
            "Things, and finding these sets as a preprocessing step from a machine learning POV could be very interesting.",
            "Yes, I'm curious.",
            "Do you see any possibilities for these latent factors for explaining or assigning a kind of meaning or semantics to them to give some insight in in what what the machine learners actually using to to make these judgments?",
            "Yeah, I think that's it's possible.",
            "You probably have to work with the non negative factorization approaches 'cause they give you a better interpretability reside.",
            "We're just exploring this using some computation communication data from Milano and from 10 to.",
            "And we see some interesting call patterns there.",
            "And, of course, interpretability visualization of things is a main topic in tensor factorization.",
            "So if you say here's a tensor data, for example, one is.",
            "Mathematically, compare the chemical components in solutions and then you have different mixtures and you have.",
            "I have incoming laser light and then reflecting laser laser lights.",
            "We also have three indices and then you do the get the data you do and factorization and you can really analyze and detect the components which were hidden to you in the liquid.",
            "And you get very good results.",
            "Are they analyze with tensor models ALOT brain data from e.g ECG data e.g data?",
            "So traditionally it seems the tensor factorization community was very interested into interpretability and getting visualization results out of that.",
            "Also better with non negativity, I think, so that's something we want to learn more about.",
            "I think for them it's very surprising that we think of it as a predictive model and we have to learn why they are so interested.",
            "And I think this is a great basis for analyzing data.",
            "So we want to explore this link further in the future.",
            "We haven't really tried to analyze the data over the understand for some reason.",
            "I mean because because we have a machine learning and we think.",
            "Forget about disability.",
            "Wanted to have pretty good modeling, so I'm not sure if in our data we see this interesting structures.",
            "I don't think it's impossible, but we haven't really explored it.",
            "But this is definitely one big topic in terms of modeling.",
            "OK, two questions actually was one and a half OK, you know the commenting briefly the respective of you have any comparison results besides those that you have mentioned between Markov logic networks and your tensor factorization results?",
            "The other one actually, if there are any plans to extend these techniques to arbitrary energy relation, arbitrary energy relation discount ya.",
            "Um?",
            "Somebody here for Michael.",
            "No, I think everybody loves Markov logic networks except for the people who have tried it.",
            "It's very convincing concept.",
            "It's a little bit more difficult to use than people would want them to be used.",
            "So of course the work was done by the students and they always complained a little bit how much work it was to get it to work right so?",
            "So in some sense I mean this.",
            "Of course this code is available if you go to Max's website, you can play with the code.",
            "The nice thing is, here we don't have to put in any rule rules or patterns for initialization.",
            "Of course, Mac of Logic also has some structure learning we didn't get to results to get results with structure learning, so in some sense we are.",
            "Our stuff is easier to use, but if you have deep background knowledge, you definitely want to include in your problem solution, then Markov logic is probably better.",
            "And the.",
            "Oh, any relations?",
            "Have you had the yeah?",
            "Of course.",
            "You know relations pioneer relations.",
            "There's some.",
            "Interesting focus, definitely.",
            "If you look at this work he did some good stuff on higher order relations.",
            "So yet for example, movie movie user matrix and then the third.",
            "Dimension was the time, so we had different time dependencies there, so there was quite successful.",
            "But we we did some something in this direction, but not with the rest.",
            "Can model yet.",
            "So Max I think was really really cared about the scalability of everything.",
            "So anything where the feeling the scalability goes down, he was not too interested in and so the so clear that the LS works as elegantly in higher order tensors, but we haven't really explored it.",
            "OK, I forgot.",
            "A question also concerning the connection to more expressive reasoning.",
            "So if you imagine that your knowledge base has something like domain and range and you could assert that certain elements can never be true, how can you comment on how Rascal deals with that?",
            "Because right now you basically treat all the entities similar and you have big chunks in your tensor that never can be anything different to zero and.",
            "Rascal even doesn't know that this can be the case, right?",
            "Yeah, that's a good question.",
            "I discussed this a lot with Max and I mean of course when we compress the 10s or we only get compressed the one, so the zeros are never in the data in some sense, and the LS update doesn't care about the zeros.",
            "So it doesn't seem to hurt as much as you think it would.",
            "Hurt is our conclusion, but of course, if you have more detailed knowledge about.",
            "And I mean, for example, often you're really only interested in predicting one relation, and you're not want to predict all the relations and then OK, one important thing is what you might want to do is to put weights on relations you care more about or you think have a higher influence on what you're predicting.",
            "And this can work, for example.",
            "So if you think if you want to predict like whatever movie user relationships, and you have a huge number of other relations in your tensor, and you know there cannot be very important.",
            "It makes sense to weigh them down and two way the ones which you think are more important given higher weight and this has worked in some of our experiments and other peoples doing this more systematically in there for a reason.",
            "A dissertation and Hildesheim were definitely coming from and.",
            "Guys are Fatima something?",
            "You know they never got the name.",
            "Yeah, that's what IMA so there they have been pursuing this very deeply that they put different weights for different tasks on the different relation types.",
            "Of course, then becomes more complicated.",
            "I guess they end up with two parameters, morlas essentially weighing down all the other ones accordingly.",
            "So this seems to be this seems to help.",
            "Zeros don't seem to hurt in the way we're doing the modeling right now.",
            "Of course, it's not true.",
            "OK, after make one point, there really efficient version of Rascal comes when you use the squared error cost function, 'cause then the zeros can really really drop out if you use Bernoulli cost functions or other cost functions, you don't always have this effect.",
            "So in other words, the zeros really become non zeros and this becomes important in the update of the of the equations.",
            "So then you would have a big problem there if you use the resco model in the simplest form of the squared error, I think it's not a problem, But if you.",
            "Have more complex cost functions than the LS is not as effective anymore.",
            "You might have to use gradient descent learning and then the zeros really become non zeros and you have to always take them into each iteration.",
            "So then you would definitely try to split up.",
            "Rescale into simpler.",
            "Forms and have more emphasis on the relations you really care about.",
            "Actually, I have the last question.",
            "We have our ontologies are our data out there.",
            "They are very nice.",
            "They're increasing over the time, but we also have problem because there are or there could be information that are conflicting that are noising.",
            "Information above are over the time but then on the web we find the old information, new information.",
            "We don't know exactly which is informed which information is reliable or not.",
            "We have a certain information we don't know how to treat the information that are out there.",
            "Which space do you see for machine learning methods for coping with this kind of problems, and do you see enough space for managing successfully this kind of problem?",
            "Yeah, that's a good question, of course, and concerns about the semantic web community.",
            "I mean and we have one answer which is essentially saying look at the reconstruction of your tents, or if one.",
            "So triple you thought was true gets a very small weight.",
            "After training, it's at least inconsistent with the rest of the ontology and you want might want to check that one if that's really a valid triple or not.",
            "And the other way around of course.",
            "Also, you predict supposed to be there, which are not really in the data, so that's probably the easier answer for us to do this systematically on a web scale is definitely a big challenge, and this might be one important tool you might might want to use, but.",
            "Probably cannot solve all your audio problems with the data consistency, and although I think this machine learning people approaches can easily work with uncertain data, of course it's also garbage in, garbage out.",
            "If your data is very faulty, the best learning system cannot really.",
            "So I think it's a can have a big contribution, but it will not solve the whole problem, but it just by itself.",
            "OK, I think there are no more questions, so that's the end of this session.",
            "Thanks to everyone and thanks to Forker Ann.",
            "Enjoy your lunch and don't forget to give your vote."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the title is machine learning with knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Maximilian Nico, who is now a postdoc at MIT and with contributions from Sharon Young and Dennis Krampus to current students.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so my background is in machine learning really and I got involved in semantic web problems or challenges maybe six years ago and I really enjoyed it because thinking and learning about the semantic web really clarified many things which sort of were unclear before for me.",
                    "label": 1
                },
                {
                    "sent": "So it was really great.",
                    "label": 0
                },
                {
                    "sent": "I had an immediate love affair with RDF.",
                    "label": 0
                },
                {
                    "sent": "Nothing is ever wrong in RDF and no contradictions.",
                    "label": 0
                },
                {
                    "sent": "I learned that from Frank.",
                    "label": 0
                },
                {
                    "sent": "So we have a English chef who teaches the world how to cook.",
                    "label": 0
                },
                {
                    "sent": "I don't want to explain the other images.",
                    "label": 0
                },
                {
                    "sent": "No contradictions in RDF.",
                    "label": 0
                },
                {
                    "sent": "This is the first workshop I attended here and we published our first paper in this domain.",
                    "label": 0
                },
                {
                    "sent": "Great organizing committee.",
                    "label": 0
                },
                {
                    "sent": "Claudio Nicola Marco, Agnieszka and Voytek.",
                    "label": 0
                },
                {
                    "sent": "And it has been 1823 days since the workshop happened, and this number is increasing daily.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today you're more of a machine learner and I try to convince you that semantic web has good ideas.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why does machine learning need knowledge?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, sort of remind you what is machine learning 'cause there's statistics, state of mining statistiques.",
                    "label": 1
                },
                {
                    "sent": "You typically care about one parameter which tells you if this treatment.",
                    "label": 0
                },
                {
                    "sent": "The medication had a positive effect.",
                    "label": 0
                },
                {
                    "sent": "No effect or negative effects.",
                    "label": 1
                },
                {
                    "sent": "So really care about interpretable parameters.",
                    "label": 0
                },
                {
                    "sent": "And and data mining.",
                    "label": 0
                },
                {
                    "sent": "You focus on the discovery of meaningful patterns to explain the data to find interesting structures to solve different tasks.",
                    "label": 1
                },
                {
                    "sent": "Machine learning traditionally focuses on prediction accuracy.",
                    "label": 1
                },
                {
                    "sent": "You want to have the best predictor for some task and you don't care too much about the meaning of the parameters or even interpretability of your model.",
                    "label": 0
                },
                {
                    "sent": "Or sometimes you do, but often you don't, and I think these are really.",
                    "label": 0
                },
                {
                    "sent": "This explains, although there's a common based on data analytics statistics.",
                    "label": 0
                },
                {
                    "sent": "That there are really also a lot of differences in the way people work with these with data.",
                    "label": 0
                },
                {
                    "sent": "So you want to think about prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The workers, of course, is a classifier, so here you have 10 digits handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "You want to classify you have 10 different classifiers, one for each digit, and you predict how likely it is that this picture is a one or a zero or two.",
                    "label": 0
                },
                {
                    "sent": "And this is a very powerful concept, and it's also surprisingly general.",
                    "label": 1
                },
                {
                    "sent": "So we actually need a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I miss him, so let's say FK of ZL tells you the probability or some sort of score for class K4 digit K. Given your input the error so it would be some future derived from these images.",
                    "label": 0
                },
                {
                    "sent": "And then here are some classic models for F, so that's an important part.",
                    "label": 0
                },
                {
                    "sent": "The first one I fixed basis functions, so basis functions can be very general or generic, like ocean basis functions are polynomial basis functions, or they're very domain specific, like edge detectors and images, and then the only thing you have to do is to learn tune these weights on these spaces functions and that's what gives you the FK of CL.",
                    "label": 0
                },
                {
                    "sent": "Related approach of the kernel approach the kernel approach mathematically, it's exactly the same, only that it's more efficient if you have many basis functions or maybe even infinite basis function, then it's more efficient to work with kernels because the summer is over the number of data points and before it was about on the number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Of course, this end can also be problematic if you go to really big datasets, and this is really the inherent problem in kernels.",
                    "label": 1
                },
                {
                    "sent": "If you want to go to big data, but mathematically it's it's the same, you can transform one into the other.",
                    "label": 1
                },
                {
                    "sent": "And of course the old and new big thing is newer networks since the couple years they have been coming back with force Deep Now means that people work with maybe 10 layers.",
                    "label": 0
                },
                {
                    "sent": "And 1000 neurons per layer.",
                    "label": 1
                },
                {
                    "sent": "So really big neural networks, and they're really extremely hot and most of you probably know about it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they've been really successful in reducing errors in some very important benchmark, but also real world application specific datasets.",
                    "label": 0
                },
                {
                    "sent": "For example, they got 30% reduction in important task and speech recognition, and if you have an Android smartphone then you have a deep neural network running on your Android Android smartphone for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "And if you are connected to the server and even better one is running on the server and it's also speaker independent.",
                    "label": 0
                },
                {
                    "sent": "No, that's it.",
                    "label": 0
                },
                {
                    "sent": "You don't have to retrain for each speaker, so it's a huge success.",
                    "label": 0
                },
                {
                    "sent": "Other successes, an object recognition images, and analyzing of textual data.",
                    "label": 0
                },
                {
                    "sent": "And all the major companies in this area.",
                    "label": 0
                },
                {
                    "sent": "Google, Microsoft, Facebook, Baidu are all investing heavily in deep learning and my own student, former student Kieu is, and our research director at Baidu.",
                    "label": 1
                },
                {
                    "sent": "And he's just setting up a research lab for deep learning and Silicon Valley.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the guys entering, Oh yeah, he's also hiring and running away from Google to Baidu.",
                    "label": 0
                },
                {
                    "sent": "Kurt was in the news, so entering got famous for discovering cats and images.",
                    "label": 0
                },
                {
                    "sent": "And here's the New York Times.",
                    "label": 0
                },
                {
                    "sent": "Very famous newspaper.",
                    "label": 0
                },
                {
                    "sent": "So how many computers to identify a cat?",
                    "label": 0
                },
                {
                    "sent": "The answer is 16,000 because he's using the Google infrastructure for his task.",
                    "label": 0
                },
                {
                    "sent": "I guess in the future we will use the Baidu infrastructure for this task.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's what we're from here.",
                    "label": 0
                },
                {
                    "sent": "Deep learning networks sees more cat images in any child.",
                    "label": 1
                },
                {
                    "sent": "But it's not as good as at this task that children, of course, better so deep learning community thinks we need better unsupervised learning to pre structure the network to 1st learn about the statistics of the real world as a basis for solving specific tasks.",
                    "label": 1
                },
                {
                    "sent": "But maybe from the semantic web community you think maybe we need background knowledge and also we don't just want to detect cats images, we want to do more.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we wanted also attack dogs.",
                    "label": 0
                },
                {
                    "sent": "How is this people and so on?",
                    "label": 0
                },
                {
                    "sent": "The instances of many other classes we want to recognize specific instances or entities, so this is my cat mix.",
                    "label": 1
                },
                {
                    "sent": "We work with 10,000,000 entities.",
                    "label": 0
                },
                {
                    "sent": "We want to predict attributes.",
                    "label": 0
                },
                {
                    "sent": "Max is Eva and we predict relationships.",
                    "label": 1
                },
                {
                    "sent": "Makes likes Mary and our experiments enter the 14 possible triples.",
                    "label": 0
                },
                {
                    "sent": "We can predict this is the on the order of the number of synapses in your brain.",
                    "label": 0
                },
                {
                    "sent": "That selection of Masco School to be sort of the brain scale somewhere.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe this is the vision that you want to have a system which not only do text as a person, but also recognize the person as being Obama has some background information that he's married to Michelle and then nicely ask how's your wife Michelle doing?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what I'm doing now.",
                    "label": 0
                },
                {
                    "sent": "Anybody from Greece, yeah.",
                    "label": 0
                },
                {
                    "sent": "Nobody can can you?",
                    "label": 0
                },
                {
                    "sent": "Can you say it?",
                    "label": 0
                },
                {
                    "sent": "Very good, so the central stuff is essence and one of the other ones is ours and the other one is bringing, so bringing us to essence more.",
                    "label": 0
                },
                {
                    "sent": "I guess to create and it's not always but semantic web ideas.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to know about entities, attributes and classes in the world and the various relationships that do or might exist between those we need ontologies.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is of course I'm not just some strange things invented by semantic Web people there.",
                    "label": 0
                },
                {
                    "sent": "Real ones heavily used in medicine and biomedical Sciences, and to classify diseases to other diseases or thoughts of things they might not be as formal as some people want them to be, but they used heavily in in medicine and then of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have our own favorite ontologies here.",
                    "label": 0
                },
                {
                    "sent": "More dimension, pedia.",
                    "label": 0
                },
                {
                    "sent": "Jago, Freebase and Google Knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "You know that Freebase was bought up by by Google and but still as a public version.",
                    "label": 0
                },
                {
                    "sent": "So recently there were two billion facts and Freebase version and Google is using it to improve ranking proof information extraction and also to display information about entities and persons on your search result.",
                    "label": 0
                },
                {
                    "sent": "So it's really used in a major industry which is quite.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't have to take talk about linked open data here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we know that the basis for this knowledge representation is the Triple Max likes Mary, so I can be fast here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One interesting thing is about these structures is that they are easy enough to understand that they are also accessible to people from machine learning for example.",
                    "label": 1
                },
                {
                    "sent": "So this can be a really important bridge from the semantic web community to the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "Because this is not too much.",
                    "label": 0
                },
                {
                    "sent": "Logical background required here.",
                    "label": 0
                },
                {
                    "sent": "You understand these facts very easily.",
                    "label": 0
                },
                {
                    "sent": "You can make sense out of them and so I would expect that this will be become even more important in the future and will be integrated and in more machine learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The knowledge of statistical relational learning.",
                    "label": 1
                },
                {
                    "sent": "So the task.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so I said we had machine learning.",
                    "label": 0
                },
                {
                    "sent": "We care about prediction tasks, so I think the sort of the Canonical or the general task is to predict the existence or non existence of triples.",
                    "label": 0
                },
                {
                    "sent": "So you have subject, predicate, object, EIRKEJ and you want to predict as in reasoning for example, but with other methods.",
                    "label": 0
                },
                {
                    "sent": "If this triple makes sense, if it's true of its faults.",
                    "label": 0
                },
                {
                    "sent": "And since machine learning typically cannot make a 01 decision, you want maybe get a probability out that if this triple is true or false, and so we can have one classifier for each relation type for each property.",
                    "label": 0
                },
                {
                    "sent": "And have some input describing the subject and object and then simply train as many classifiers as we need and we're done.",
                    "label": 0
                },
                {
                    "sent": "So very simple.",
                    "label": 0
                },
                {
                    "sent": "Let me build one classifier for each relation type K. The only question is what is the input?",
                    "label": 1
                },
                {
                    "sent": "The input describing this triple really with the information which is important to classify.",
                    "label": 1
                },
                {
                    "sent": "This triple is true or false.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first approach we work with known features, so these are futures which can be extracted from.",
                    "label": 0
                },
                {
                    "sent": "Data from the Knowledge Graph from the from the from the RDF graph.",
                    "label": 0
                },
                {
                    "sent": "So simple ones, and if these are persons features might be age, sex.",
                    "label": 0
                },
                {
                    "sent": "And so an income that could be also features derived from a neighborhood of the entity in the environment of the RDF graph.",
                    "label": 0
                },
                {
                    "sent": "This is what typically people are using, so the number of connections coming out of this connecting to this entity or specific entities.",
                    "label": 0
                },
                {
                    "sent": "Is it a human as it's connected over so many links to something else?",
                    "label": 0
                },
                {
                    "sent": "But they are really sort of done as a preprocessing step using prior knowledge of the.",
                    "label": 0
                },
                {
                    "sent": "Of the people who is designing the system.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's also some structural search, but at the end you have a set of known attributes describing the subject and the objects object.",
                    "label": 0
                },
                {
                    "sent": "We have this AI one to ARR.",
                    "label": 0
                },
                {
                    "sent": "Which are are different features describing entity I?",
                    "label": 0
                },
                {
                    "sent": "And we have the same number of features describing entity J.",
                    "label": 0
                },
                {
                    "sent": "And then we just contact concatenate them.",
                    "label": 0
                },
                {
                    "sent": "These are known attributes, known features and form this Z input vector, which is just the other credit dated version of these two individual attributes.",
                    "label": 0
                },
                {
                    "sent": "And then we put them into our different systems.",
                    "label": 0
                },
                {
                    "sent": "And we can train the classifier.",
                    "label": 0
                },
                {
                    "sent": "So in the semantic web community, the kernel approach is most popular.",
                    "label": 0
                },
                {
                    "sent": "I would say because you cannot be very smart and describing interesting kernels, which sort of tell you the similarity between entities.",
                    "label": 0
                },
                {
                    "sent": "And but then again, it's a kernel learning problem.",
                    "label": 0
                },
                {
                    "sent": "You have to have to learn the visa in this case and you're done.",
                    "label": 0
                },
                {
                    "sent": "And I think I'm getting a did a very nice paper on this two years ago at this conference.",
                    "label": 0
                },
                {
                    "sent": "Connecting like different kernel approaches and testing them.",
                    "label": 0
                },
                {
                    "sent": "So, but this is in some sense it's simpler.",
                    "label": 0
                },
                {
                    "sent": "Now you have no one features.",
                    "label": 0
                },
                {
                    "sent": "The future might be different, difficult to calculate, or might be difficult to sign the right features, but after you have the features, everything is simple.",
                    "label": 0
                },
                {
                    "sent": "Again some sense, of course you have scalability and all these things.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, it's mostly about latent features.",
                    "label": 0
                },
                {
                    "sent": "So we have the same thing.",
                    "label": 0
                },
                {
                    "sent": "Well, only that we have no idea what these AA stand for.",
                    "label": 0
                },
                {
                    "sent": "Well, they are.",
                    "label": 0
                },
                {
                    "sent": "They also have to be learned unknown to us.",
                    "label": 0
                },
                {
                    "sent": "Again, our UI is described by AI 12A IR.",
                    "label": 0
                },
                {
                    "sent": "The same thing with DJ.",
                    "label": 0
                },
                {
                    "sent": "And again, we concatenate them into the input vector Z and we can put them into a system with fixed basis functions, or we can put it into another network.",
                    "label": 0
                },
                {
                    "sent": "But now is the problem.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's much more difficult now because not only are the WS unknown like in any system based function system.",
                    "label": 0
                },
                {
                    "sent": "Also we have no idea what these ads are.",
                    "label": 0
                },
                {
                    "sent": "They also unknown and they form the Z in this basis function setting.",
                    "label": 0
                },
                {
                    "sent": "Interesting and passing.",
                    "label": 0
                },
                {
                    "sent": "I want to also say this something relation related going on and help community and a paper by Colbert unquote coworkers.",
                    "label": 0
                },
                {
                    "sent": "They do similar things, only that they replace the basis function.",
                    "label": 0
                },
                {
                    "sent": "Everything by neural network and they got very good result and a lot of deep learning tasks or learning tasks and.",
                    "label": 0
                },
                {
                    "sent": "I call it NLP from scratch.",
                    "label": 0
                },
                {
                    "sent": "You might have heard of this paper, but we're working today on the left side.",
                    "label": 0
                },
                {
                    "sent": "So we have basis functions.",
                    "label": 0
                },
                {
                    "sent": "We have inputs we don't know about and we want to learn this prediction model.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One nice thing about.",
                    "label": 0
                },
                {
                    "sent": "Advantage also that the ace are unknown or just shown over here, so these are entities and each entity has its own set of features, latent features, so they are unknown.",
                    "label": 0
                },
                {
                    "sent": "And then this is what they want to predict.",
                    "label": 0
                },
                {
                    "sent": "The existence of this triple for some relation R. And if you studied Bayesian networks or its graphical models, this is a sort of a nice repeatedly structure, and in this we structure.",
                    "label": 0
                },
                {
                    "sent": "This is Collider down there and the Collider opens up if it's known.",
                    "label": 0
                },
                {
                    "sent": "So if we have in our training data triples which are known information can flow through these colliders and the bottom and the parents open up if they are unknown.",
                    "label": 0
                },
                {
                    "sent": "And of course our ASR unknown so they're open so we can have global propagation of information in these.",
                    "label": 0
                },
                {
                    "sent": "This network of random variables.",
                    "label": 1
                },
                {
                    "sent": "And the advantage is that we get something we call collective learning.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to decide if Jack is rich, maybe you know his father is rich.",
                    "label": 1
                },
                {
                    "sent": "This is relatively local, but maybe you don't know the income of the father, but you know that the father of the father is rich.",
                    "label": 0
                },
                {
                    "sent": "So this information is further away in this knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "And by this trick or this exploiting the structure, there's some chance that information propagates.",
                    "label": 0
                },
                {
                    "sent": "In this graph, so we can have can make information which is far away local for the decision.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what basis functions should we use?",
                    "label": 1
                },
                {
                    "sent": "So we have our features for I&R features for J and makes sense?",
                    "label": 0
                },
                {
                    "sent": "Maybe to consider all individual products of the different features, so we get these product basis functions are polynomial interacting basis function shown over here.",
                    "label": 0
                },
                {
                    "sent": "So this captures sort of the different interactions between the different latent representations of the two involved entities, and these are basis functions.",
                    "label": 1
                },
                {
                    "sent": "And now we have a double sum.",
                    "label": 0
                },
                {
                    "sent": "Just because we have more indices.",
                    "label": 0
                },
                {
                    "sent": "But still we have to learn the WS and the AAC is still very difficult problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can rewrite it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Potentially we can write this sum this double sum, so now absolutely substituted the ace for the B in this nice compact form.",
                    "label": 1
                },
                {
                    "sent": "It's a, it's a it's a vector of attributes for energy.",
                    "label": 0
                },
                {
                    "sent": "I transpose times matrix RK and times AJ, which is the latent representation for the entity J and OK simply contains all these weights in the left equation.",
                    "label": 0
                },
                {
                    "sent": "So we change notation a little bit.",
                    "label": 0
                },
                {
                    "sent": "So now our K is, in our terms are matrix before it was W as a vector turned into matrix.",
                    "label": 0
                },
                {
                    "sent": "Now are is a matrix and we have one of these matrices for each relation type and we just can stack them on top of one another and we get something which is called a tensor.",
                    "label": 1
                },
                {
                    "sent": "And we call it court enzyme.",
                    "label": 0
                },
                {
                    "sent": "So we take the R one or two or three.",
                    "label": 1
                },
                {
                    "sent": "It's like come on top of each other.",
                    "label": 0
                },
                {
                    "sent": "Get a quote tensor.",
                    "label": 0
                },
                {
                    "sent": "And you can also take our raw data, so which describes which triples we know to exist or not exist in the in the raw data into a tensor, 'cause we have three indices, subject, predicate, object, and we also get it turns out.",
                    "label": 0
                },
                {
                    "sent": "So now the learning of all these difficult things up there WAA becomes a problem of factorizing this tensor X.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphical visualization of what I was just saying on the left side you see the raw data tensor which contains mostly zeros and a few ones representing the effects or the triples.",
                    "label": 0
                },
                {
                    "sent": "We known to be true and we have to of course compress it heavily because this is huge.",
                    "label": 0
                },
                {
                    "sent": "This is this 10 to the four elements, and then we just vectorize it into the matrix and the R Quad 10s or the Aqua tensor is shown in the middle.",
                    "label": 0
                },
                {
                    "sent": "This cube over there.",
                    "label": 0
                },
                {
                    "sent": "And one slice or one matrix is responsible for one relation type for one property.",
                    "label": 0
                },
                {
                    "sent": "And the other two matrices are identical and they contain all the latent vectors for all the entities we are concerned with.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of.",
                    "label": 0
                },
                {
                    "sent": "The entry in the in the 10s are.",
                    "label": 0
                },
                {
                    "sent": "This is just our raw data, mostly consisting of zeros and a few ones.",
                    "label": 0
                },
                {
                    "sent": "And and the ASR really these days we need in the in the.",
                    "label": 0
                },
                {
                    "sent": "In our modeling our basis function model and the court ends are really gives us the weights.",
                    "label": 0
                },
                {
                    "sent": "On the basis functions.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we could put this as we can put this into a nice optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We penalize large weights.",
                    "label": 0
                },
                {
                    "sent": "Regularization always helps in machine learning.",
                    "label": 1
                },
                {
                    "sent": "And we can work with different cost functions.",
                    "label": 0
                },
                {
                    "sent": "We can work with Gaussian cost functions which give you the squared error or Bernoulli which is more appropriate because the data is sort of binary.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do very efficient updates using an alternating least squares update rule where we can exploit data sparsity so all the zeros drop out in the update equations.",
                    "label": 1
                },
                {
                    "sent": "We only concerned about the ones and this is very fast and this allows us to scale up to these large domains.",
                    "label": 1
                },
                {
                    "sent": "Other people are using stochastic gradient descent, but we didn't get too good results with stochastic into sent yet.",
                    "label": 0
                },
                {
                    "sent": "And we're not live.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's a binary relations.",
                    "label": 0
                },
                {
                    "sent": "We can also work with simulations where ascential it's an inner product of the latent representation of the entity, with some vector describing the unit relation.",
                    "label": 0
                },
                {
                    "sent": "And we can go to higher orders if you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Of course, things become more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scalability is quite good, so it's linear in most important quantities.",
                    "label": 0
                },
                {
                    "sent": "It's linear in the number of entities linear in the number of predicates or relation types linear in the number of known facts.",
                    "label": 0
                },
                {
                    "sent": "So we have to work with sparse matrices of sparse tensors if at all possible.",
                    "label": 0
                },
                {
                    "sent": "On the bottom left you see if we include unary relations, so attributes in a smart way in the.",
                    "label": 1
                },
                {
                    "sent": "Tensor is this.",
                    "label": 0
                },
                {
                    "sent": "Red Curve is a smart way we they don't really play an important role in the computation if we do it in a not smart way.",
                    "label": 0
                },
                {
                    "sent": "We also get a linear increase there and the last one is also important because it's the rank over approximation that there was this R which tells you how many how many latent features you wanted for each entity, and in our new results is that it's cubic in the number in the rank which is the red line down there, and in a naive implementation it was.",
                    "label": 0
                },
                {
                    "sent": "Out of the power five or six even, and this was one of Max's major breakthroughs that he could reduce it to the power of three, which is typical complexity even in matrix factorization.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So 1st result is on sort of typical benchmark data.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you're doing you take, you take your knowledge base, you take out some of the triples as I mean, assume you don't know that they are true, then predict a lot of triples once, which are really not there.",
                    "label": 0
                },
                {
                    "sent": "The data and the one which is there in the data which we took out.",
                    "label": 0
                },
                {
                    "sent": "And then you hope that the one you took out get the best results.",
                    "label": 0
                },
                {
                    "sent": "So it's not really ground truth because we don't really know if the zeros are zeros or not, simply not known to be true.",
                    "label": 0
                },
                {
                    "sent": "But this is the way people in this community sort of compare different approaches.",
                    "label": 0
                },
                {
                    "sent": "So this is a data set on kinship between an Indian tribe in Australia who molests data from the Unified Medical Language System and Nations describes political interactions between countries in the Cold War and the right one, the brownish one yellowish brownish one is the rest kind model, and the plot is AOC in an area under curve precision recall and you can see that it's either the best method or it's.",
                    "label": 1
                },
                {
                    "sent": "Among the best methods so it gives you confidence that it really performs well on different data sets on the benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "So this would be predicting relations like doesn't Mary Max like Mary, so it's a relationship prediction problem.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an experiment on the Cora data set, also often used in relational learning, so it's a noisy data set where some authors duplicates some titles, have duplicates in publications of public duplicates, and so on.",
                    "label": 0
                },
                {
                    "sent": "And the task is to identify these duplicates and say these two people are really the same.",
                    "label": 0
                },
                {
                    "sent": "So in some sense it's like object recognition or you have some description of an object and you want to find the nearest neighbor which is closest to your.",
                    "label": 0
                },
                {
                    "sent": "Object you're interested in, and So what we're looking at essentially, after the factorization, we look at the A matrix and they describe the entities, and then we look at distance in terms of this, a representation in terms of the latent representation of the entities, and if they are very similar after some normalization.",
                    "label": 0
                },
                {
                    "sent": "Of course, then we can say these are likely the same ones.",
                    "label": 0
                },
                {
                    "sent": "And again, here's some AOC prisoner recall results.",
                    "label": 0
                },
                {
                    "sent": "Rest Kyle gives you.",
                    "label": 0
                },
                {
                    "sent": "I mean, all methods give you quite good results.",
                    "label": 0
                },
                {
                    "sent": "Here we have some results on Markov logic with basic rules, so there we are much better if you invest a lot in the rules and get to complex rules, then becomes sort of comparative, but the results are all in this measure quite high, which is not so easy to get.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's no.",
                    "label": 0
                },
                {
                    "sent": "How do we apply this to the YAGO Knowledge Graph?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we factorize and yoga knowledge graph with the version of a couple years ago they are going to call ontology 2.6 million entities, 340,000 classes, 87 predicates and 33 million known facts and potentially 10 to the 14 effects we can predict.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we do a classification task.",
                    "label": 0
                },
                {
                    "sent": "So predict that something is a cat.",
                    "label": 1
                },
                {
                    "sent": "For example, in our case, we predict that something is a personal location or a movie, and this is statistics.",
                    "label": 0
                },
                {
                    "sent": "There were 800,000 persons, little database for details locations and 62,000 movies.",
                    "label": 0
                },
                {
                    "sent": "So through the random prediction sort of reflects this statistics a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's easier to predict person because so many persons in there in the setting a, we only took the type information we want to predict.",
                    "label": 0
                },
                {
                    "sent": "So only if it's a personal location or a movie and predicted that.",
                    "label": 0
                },
                {
                    "sent": "But it could conclude that something is an action.",
                    "label": 0
                },
                {
                    "sent": "Movies, also movie, which of course in terms of semantic web reasoning is very simple.",
                    "label": 0
                },
                {
                    "sent": "But we had to learn it, sort of in the system, but learned it very well.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "We've got values close to one for personal location and movies is always more difficult, but the lift from 06 to 75 is also quite impressive.",
                    "label": 0
                },
                {
                    "sent": "And section setting B, we removed all type information so also action movie and so on.",
                    "label": 0
                },
                {
                    "sent": "And but the performance is still quite OK. That's not too bad, and but then you can play and you can add textual attributes.",
                    "label": 0
                },
                {
                    "sent": "For example for the person in the movies and so on, and then improve your classification classification accuracy again.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is something where we show that this collective learning is so important, so we want to predict if some writer is a French right or German right or British writer.",
                    "label": 0
                },
                {
                    "sent": "And this we know from the database he was born in Paris, but not too many writers in the service of one in Paris.",
                    "label": 0
                },
                {
                    "sent": "So that doesn't give you so much information.",
                    "label": 0
                },
                {
                    "sent": "It's important to Paris is in France because it's correlates with that.",
                    "label": 0
                },
                {
                    "sent": "He's a French group, right?",
                    "label": 0
                },
                {
                    "sent": "So so this information had to propagate in this system in this collective learning.",
                    "label": 0
                },
                {
                    "sent": "Way and helps you then to predict the writer nationality and see the left three methods.",
                    "label": 0
                },
                {
                    "sent": "Don't have this collective learning random.",
                    "label": 1
                },
                {
                    "sent": "Of course sons and CP and you so show that there is a performance is not as good.",
                    "label": 0
                },
                {
                    "sent": "The rest: The right explores this information which is further away and gets you very good results in Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "Also if you really nice handcrafted rules also gives you compare results.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also learn taxonomies also by having this AI with later representation of the of the entities and just do a hierarchical clustering on on these and.",
                    "label": 0
                },
                {
                    "sent": "And on the top level there's also movie domain.",
                    "label": 1
                },
                {
                    "sent": "We got very nice agreement with ontology and then if you go further down of course the number of data points for each cluster goes down so it becomes a little bit more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, sometime.",
                    "label": 0
                },
                {
                    "sent": "There are different extensions and we are currently working on many of them.",
                    "label": 0
                },
                {
                    "sent": "One is to insist that A and I must be non negative.",
                    "label": 0
                },
                {
                    "sent": "Put that constraint on the parameters.",
                    "label": 0
                },
                {
                    "sent": "The performance goes down a little bit.",
                    "label": 0
                },
                {
                    "sent": "What you're getting is very sparse effectors, because this negative constraint encourages sort of attributes to become zero.",
                    "label": 0
                },
                {
                    "sent": "So the attribute vector here for the original rest call on the left side is full, 100%, non zeros, but for the different versions of the non negative factorization you get very sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Also, it's much easier to interpret.",
                    "label": 0
                },
                {
                    "sent": "Your result if you want to interpret what you're getting out.",
                    "label": 0
                },
                {
                    "sent": "If you have nonnegativity constraints, because then everything has to add in a positive way and it cannot cancel out in a complex way.",
                    "label": 0
                },
                {
                    "sent": "If you have plus and minus is.",
                    "label": 0
                },
                {
                    "sent": "Also, sometimes you get nicer probabilistic interpretation of if things are not negative.",
                    "label": 0
                },
                {
                    "sent": "Worry what you do with negative numbers at the end, so this is an interesting direction.",
                    "label": 0
                },
                {
                    "sent": "I think in particular, if you're interested in visualization, nonnegativity really helps a lot.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Max also did some theoretical analysis and essentially he showed on a theoretical way and then also experimentally that if your data are really comes from a tensor structure, so you get the best results.",
                    "label": 0
                },
                {
                    "sent": "Also obeying this tensor structure because what you always can do is take your tenzer and sort of make it flat, making a big matrix out of it by just concatenating all the slices in different ways.",
                    "label": 0
                },
                {
                    "sent": "So essentially making a turning the tensor into matrix and he could show theoretically.",
                    "label": 0
                },
                {
                    "sent": "And also experimentally that this is not optimal if you.",
                    "label": 0
                },
                {
                    "sent": "Care about reconstruction accuracy?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was more on sort of benchmark data and now we want to also talk a little bit about one application we care about.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to apply these things so this is one problem a little bit in relational learning.",
                    "label": 0
                },
                {
                    "sent": "It's not clear where you really say.",
                    "label": 0
                },
                {
                    "sent": "Here's the real world application where people are using a relational learning, except maybe for recommendation systems, which is sort of an instance of relational learning problem in some ways.",
                    "label": 0
                },
                {
                    "sent": "So we can think about issues within the domain, so you have the entities in your training data and you what we did before ever so far was always predicting for the same entities.",
                    "label": 0
                },
                {
                    "sent": "Something like new relations or so.",
                    "label": 0
                },
                {
                    "sent": "So Biblical triples that classification, clustering, taxonomy, ontology, learning, entity resolution visualization we didn't show up.",
                    "label": 1
                },
                {
                    "sent": "But this is always an important thing in tensor factorization, and we can also do simple forms of.",
                    "label": 0
                },
                {
                    "sent": "Privacy querying also take into account the probability values we get out of the rest car model.",
                    "label": 1
                },
                {
                    "sent": "And I've got simple ones who wants to be friends is just a triple prediction.",
                    "label": 0
                },
                {
                    "sent": "Very simple and the more challenging one is.",
                    "label": 0
                },
                {
                    "sent": "If you have more like.",
                    "label": 0
                },
                {
                    "sent": "Sparker types of queries with variables and so on, and this is what we are currently also exploring.",
                    "label": 0
                },
                {
                    "sent": "Then you can also think of new entities outside of the domain.",
                    "label": 1
                },
                {
                    "sent": "So the first approach is to simply calculate.",
                    "label": 0
                },
                {
                    "sent": "I mean sort of fold the new entity and with its information you have the new entity into the current system you're having.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you're doing, you're calculating the latent factors for the new entity, and then if you have that, then you can apply all the methods before and in some domains we get pretty good results here, not everywhere.",
                    "label": 1
                },
                {
                    "sent": "So this is something we are still exploring, but it seems sometimes it works quite well.",
                    "label": 0
                },
                {
                    "sent": "And she want to recognize new objects, but you can also turn an object into a query and I find something similar to my query object and there's also something we actively exploring.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Application we are currently mostly interested in because we're not really working on the web on web data at Siemens, not so much interested in social media, but of course a clinic is sort of a small world on its on its own very complex.",
                    "label": 0
                },
                {
                    "sent": "Domain with the personalized medicine where you really want to do specific recommendations for a specific person for specific patient.",
                    "label": 0
                },
                {
                    "sent": "The idea is also to find to do a global modeling of the clinical data.",
                    "label": 1
                },
                {
                    "sent": "I'm not go away from like I want to model something small here and something small.",
                    "label": 0
                },
                {
                    "sent": "There of course is very challenging.",
                    "label": 1
                },
                {
                    "sent": "We have different use cases in one of them we say all data from all patients, so this is how I sort of big data.",
                    "label": 0
                },
                {
                    "sent": "Some of you and things just give us everything you have or as much as you want, and we want to find interesting structure in this patient data.",
                    "label": 0
                },
                {
                    "sent": "Also maybe predict unusual things.",
                    "label": 0
                },
                {
                    "sent": "So if you suddenly something is not the breast cancer with something else, there's a physician at the right time.",
                    "label": 0
                },
                {
                    "sent": "Refer to the other Department and things.",
                    "label": 0
                },
                {
                    "sent": "Things things like that.",
                    "label": 0
                },
                {
                    "sent": "So this is probably also the most difficult data set.",
                    "label": 0
                },
                {
                    "sent": "Also, if anybody who worked with medical data, it's not the way you want it to be nicely.",
                    "label": 0
                },
                {
                    "sent": "Timely ordered and you can nicely see cause and effect.",
                    "label": 0
                },
                {
                    "sent": "It's probably not as nice, but we want to get our medical partners to improve data quality on these issues.",
                    "label": 1
                },
                {
                    "sent": "Then specific cases breast cancer, very complex disease with genomics plays an increasing role.",
                    "label": 0
                },
                {
                    "sent": "Also something where the patient is a patient.",
                    "label": 0
                },
                {
                    "sent": "Data over longer time period then come back to the clinic with several times and same thing in their pharology so kidney transplantation where the patients are also observed over the lifetime of the kidney at 1020 years or so.",
                    "label": 0
                },
                {
                    "sent": "It's a very interesting sort of optimization routine you.",
                    "label": 0
                },
                {
                    "sent": "Have to give medication but you don't want to medication side effect.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize side effect.",
                    "label": 0
                },
                {
                    "sent": "You have to find the right level of medication and you clearly have this temporal effect temporal aspect that you have a sort of multi stage optimization problem and our partners in charity.",
                    "label": 0
                },
                {
                    "sent": "On the right there in Berlin.",
                    "label": 0
                },
                {
                    "sent": "Really good data.",
                    "label": 0
                },
                {
                    "sent": "The breast cancer and the other day to come from the clinic in Erlangen and we also have another group partner in there who has.",
                    "label": 0
                },
                {
                    "sent": "Medical studies and have some rights on this data, so this could be interesting to compare different clinics if that is at all possible.",
                    "label": 0
                },
                {
                    "sent": "Of course all these issues have a critical in terms of data safety and data security, privacy and so on, but the challenge is it sort of fits into the semantic web word.",
                    "label": 1
                },
                {
                    "sent": "Ontologies play a big role and medical domains.",
                    "label": 0
                },
                {
                    "sent": "Their patient complex relational data.",
                    "label": 1
                },
                {
                    "sent": "The patient in a clinic.",
                    "label": 0
                },
                {
                    "sent": "Many diagnosis procedures, measurements, attributes, patient history and so on.",
                    "label": 0
                },
                {
                    "sent": "Presenting time is important.",
                    "label": 1
                },
                {
                    "sent": "These are often sequential data and knowing what happened first, what happened X is important.",
                    "label": 0
                },
                {
                    "sent": "Patients sometimes come back to the same clinic.",
                    "label": 0
                },
                {
                    "sent": "They shouldn't come too soon because then they didn't really sharp and in America you get penalized for that, but they come back maybe with the same problem.",
                    "label": 0
                },
                {
                    "sent": "Then you have also multi multi stage data.",
                    "label": 0
                },
                {
                    "sent": "Another is issue is decision modeling or decision.",
                    "label": 0
                },
                {
                    "sent": "Invitation that you want to maybe also give recommendations.",
                    "label": 0
                },
                {
                    "sent": "So in the first level we want to just observe and model data and just say this is what typically is done and you're doing something unusual.",
                    "label": 0
                },
                {
                    "sent": "Are you sure you want to do it?",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "These sort of things at a later stage.",
                    "label": 0
                },
                {
                    "sent": "Of course, we also want to say, OK, maybe this seems to give you better result if you increase the dosage.",
                    "label": 0
                },
                {
                    "sent": "Sorry, decrease the dosage here.",
                    "label": 0
                },
                {
                    "sent": "This type of things.",
                    "label": 0
                },
                {
                    "sent": "So this is partially find the futures of course, but then you get into all these.",
                    "label": 0
                },
                {
                    "sent": "Issues of confire, confounders, and causality.",
                    "label": 0
                },
                {
                    "sent": "There is this really a causal relationship or just an observed relationship?",
                    "label": 0
                },
                {
                    "sent": "Then suddenly we have to care about parameters again, so that's an interesting challenge.",
                    "label": 0
                },
                {
                    "sent": "Then there are a lot of tons of unstructured data, alot of reportes background information.",
                    "label": 0
                },
                {
                    "sent": "Of course on the web a lot about diseases and symptoms and all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Recent research results, so this was a little bit in the direction of what IBM Watson is doing, but also in the clinics you have tons of reports very difficult to understand for machine, they don't.",
                    "label": 0
                },
                {
                    "sent": "They have their own grammar.",
                    "label": 0
                },
                {
                    "sent": "Only way of formulating things.",
                    "label": 0
                },
                {
                    "sent": "And it's a very challenging task too.",
                    "label": 0
                },
                {
                    "sent": "Make me to go beyond Entity resolution entity detection in these texts.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, we're not too much interested in.",
                    "label": 0
                },
                {
                    "sent": "Really understanding the text, we want the text to help our other tasks.",
                    "label": 0
                },
                {
                    "sent": "So if there's information in the text we want to use it.",
                    "label": 0
                },
                {
                    "sent": "If it's not there, we have to live with that word or whatever we have.",
                    "label": 0
                },
                {
                    "sent": "But of course there are clinical situations that not all the information you want to have is in structured form, and a lot of the information you really care about.",
                    "label": 0
                },
                {
                    "sent": "Truly, clinical evaluation of patients is really in textual data, so we have a very strong company in Germany or specializing in German.",
                    "label": 0
                },
                {
                    "sent": "Medical texts are verbs.",
                    "label": 0
                },
                {
                    "sent": "In the team also images is a big issue here because this is some strength of Siemens.",
                    "label": 0
                },
                {
                    "sent": "We're really good in image analysis, so we have a very strong team in the US in Princeton.",
                    "label": 0
                },
                {
                    "sent": "And there really world leading in this image analysis tasks.",
                    "label": 0
                },
                {
                    "sent": "So we also want to include.",
                    "label": 0
                },
                {
                    "sent": "So one thing is we want to include image features to help the other tasks, but also the other way around.",
                    "label": 0
                },
                {
                    "sent": "They are interested in having background information on the patients to improve their image Ng tasks.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that image Ng alone is.",
                    "label": 0
                },
                {
                    "sent": "If you think of it as a diagnostic task, maybe not as quite as good as physicians are today, but if you include all the background information the physician has then it gets up to the level of the ovary physician.",
                    "label": 0
                },
                {
                    "sent": "And of course I mixed data is increasing in importance.",
                    "label": 0
                },
                {
                    "sent": "There expression data, genetic data, inherited daytime inheritance, data data in your germ line and your maybe your cancer cells.",
                    "label": 0
                },
                {
                    "sent": "So this is definitely the future and we are sort of.",
                    "label": 0
                },
                {
                    "sent": "It's not the main focus of this project, but we also want to connect to OMICS data.",
                    "label": 0
                },
                {
                    "sent": "And these are in issue.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results predicting diagnosis and procedures.",
                    "label": 1
                },
                {
                    "sent": "This NDC zscore something used in.",
                    "label": 0
                },
                {
                    "sent": "And ranking systems.",
                    "label": 0
                },
                {
                    "sent": "And we see some structure at the Black one is the random prediction and we get much better results.",
                    "label": 0
                },
                {
                    "sent": "We also see one problem with the factorization approach.",
                    "label": 0
                },
                {
                    "sent": "The highest rank gives you the best results and this is typical.",
                    "label": 0
                },
                {
                    "sent": "So we also one thing we're exploring is how to.",
                    "label": 0
                },
                {
                    "sent": "Modify the system that we can work with a lower rank in the approximation.",
                    "label": 0
                },
                {
                    "sent": "I mean the rank of 1000 tells you the medical domain is highly complex and not too surprising.",
                    "label": 0
                },
                {
                    "sent": "So this is not yet where we want to be, and Wilson indication that there is some structure in the data we can explore.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, here, as I said images.",
                    "label": 0
                },
                {
                    "sent": "So this is like maybe like the sensor information we have to work with images and textual data.",
                    "label": 1
                },
                {
                    "sent": "So this is also an interesting challenge to make this work.",
                    "label": 0
                },
                {
                    "sent": "So we have, as I said, we have a small world.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The big word here.",
                    "label": 0
                },
                {
                    "sent": "A little bit.",
                    "label": 0
                },
                {
                    "sent": "This is some more closely related work.",
                    "label": 0
                },
                {
                    "sent": "Of course there's a lot of work and learning on semantic web.",
                    "label": 0
                },
                {
                    "sent": "Our own publications, then here.",
                    "label": 0
                },
                {
                    "sent": "Of course, Stephen stabs Triple Rank first approach, probably to do to use tensor factorization for tasks in the semantic web.",
                    "label": 0
                },
                {
                    "sent": "And so this is our own paper here at this conference, where we still using matrix factorization at that time.",
                    "label": 0
                },
                {
                    "sent": "The Red one is maybe interesting, because if you think this is all like nice but not very important.",
                    "label": 0
                },
                {
                    "sent": "But I was talking about the Americans are doing it too.",
                    "label": 0
                },
                {
                    "sent": "And this is not a very good group and we're in the same entering.",
                    "label": 0
                },
                {
                    "sent": "I was talking about earlier.",
                    "label": 0
                },
                {
                    "sent": "Chris Manning is one of the leading guys in information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Richard Tucker is a German guy.",
                    "label": 0
                },
                {
                    "sent": "Like really big on neural network technologies.",
                    "label": 0
                },
                {
                    "sent": "So they're doing something slightly different, but there's a lot of similarities.",
                    "label": 0
                },
                {
                    "sent": "They also used, sort of the rescue factorization, and now we even get them to cite us.",
                    "label": 0
                },
                {
                    "sent": "There was a big challenge, but so and then they got a grant from Google to work with.",
                    "label": 0
                },
                {
                    "sent": "The approach on the Google graph.",
                    "label": 0
                },
                {
                    "sent": "I also got the same offer, but I was not.",
                    "label": 0
                },
                {
                    "sent": "I was working at home, so I'm still working at 7, so I couldn't.",
                    "label": 0
                },
                {
                    "sent": "They wouldn't want to give me the money for some reason other.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coming to conclusion.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's really amazing the knowledge graph, everything related, like the pedia, Wikipedia Jago, and Freebase and so on.",
                    "label": 0
                },
                {
                    "sent": "With the first time in history is a large general ontologies are available and I would think they are become increasingly important also for machine learning tasks and not just for task within the semantic web domain but also to support different machine learning tasks.",
                    "label": 1
                },
                {
                    "sent": "Supposedly knowledge graph is already used in search.",
                    "label": 0
                },
                {
                    "sent": "Uh, applications at Google, and so I think these are really interesting future developments.",
                    "label": 0
                },
                {
                    "sent": "And then I talked about relation machine learning, mostly focusing on the Red Skull approach.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I mentioned that you quoted Mexico dress code, so we got scalable relational learning with very competitive performance.",
                    "label": 1
                },
                {
                    "sent": "I think the numbers are presented a pretty state of the arts in terms of scalability.",
                    "label": 0
                },
                {
                    "sent": "We showed that exhibits collective learning, which is important if you wanted to relational learning and we're currently working on many improvements and extensions, and I think you see a lot of things similar to other ideas, some quite exciting.",
                    "label": 0
                },
                {
                    "sent": "So then we applied it to the YAGO Knowledge graph and short experimental results, and a number of relational learning tasks.",
                    "label": 1
                },
                {
                    "sent": "And yeah, one challenge is too.",
                    "label": 0
                },
                {
                    "sent": "No get to really relevant and maybe application relevant use cases.",
                    "label": 0
                },
                {
                    "sent": "Text understanding with ontology support is important.",
                    "label": 0
                },
                {
                    "sent": "I mean there's tons of work in this Community, of course here to support image understanding to work in the clinical data and also question answering problems or these sort of things.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your patience and your attention.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you for the interesting talk for faster so I would like to ask about do you have any tips for semantic web people who who still don't know about machine learning so much?",
                    "label": 0
                },
                {
                    "sent": "And then they won't say to incorporate some some features and benefits of missing learning to the web since, say, massive learning is good for working with.",
                    "label": 0
                },
                {
                    "sent": "Predict if for predicting information and semantic web is usually good for for things that are clips that are that are strict.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's there.",
                    "label": 0
                },
                {
                    "sent": "Both of them are actually different worlds.",
                    "label": 0
                },
                {
                    "sent": "And then for example I'm I'm very new for machine learning, so I've just stopped at learning, say classification clustering and then I'm for me because I have a bit of background for semantic web.",
                    "label": 0
                },
                {
                    "sent": "And then I want to learn machine learning.",
                    "label": 1
                },
                {
                    "sent": "Do you have any tips for for me and also for other guys who have the same back one for me too and also missing then of course there are great tutorials now in the web and this online course from entering and machine learning very popular.",
                    "label": 0
                },
                {
                    "sent": "It's a typically have to find your own way a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's there's no like Golden Way, and you have to invest some effort.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think one bridges that was machine learning.",
                    "label": 0
                },
                {
                    "sent": "For example, we can predict triples, a faulty triples attributes which are there, but maybe should not be there, and also predict the other way triples which should be there, and I'm not there, so it's it's also an interesting link between 2 the both domains.",
                    "label": 0
                },
                {
                    "sent": "I think it's not so easy to say.",
                    "label": 0
                },
                {
                    "sent": "Well, how do I think the important thing is that you understand how a machine learner looks at the world and the machine learning should stand.",
                    "label": 0
                },
                {
                    "sent": "How semantic web person looks at the world.",
                    "label": 0
                },
                {
                    "sent": "And although I learned a lot, I mean I don't think I understood everything you guys talk about in semantic web domain and particularly people talk about ontology.",
                    "label": 0
                },
                {
                    "sent": "So but then one guy once said me, I think was here is intact.",
                    "label": 0
                },
                {
                    "sent": "They only seven people in the world who understand ontologies really.",
                    "label": 0
                },
                {
                    "sent": "Or whatever.",
                    "label": 0
                },
                {
                    "sent": "So that's a little bit of a problem.",
                    "label": 0
                },
                {
                    "sent": "So I much difficult is for you to understand how machine learning works.",
                    "label": 0
                },
                {
                    "sent": "A little things.",
                    "label": 0
                },
                {
                    "sent": "And for machine learners definitely also difficult to understand how a semantic web person thinks about the world.",
                    "label": 0
                },
                {
                    "sent": "So there's definitely some culture like effort to understand each other.",
                    "label": 0
                },
                {
                    "sent": "In machine learning people find certain problems absolutely exciting where you probably think it's not very challenging and the other way around.",
                    "label": 0
                },
                {
                    "sent": "Machine learning people typically get confused by all the notations and and logics and stuff, and I think it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's one of these things where cultures are really different and you can make some effort to understand the culture and get a lot out of it, but 100% is always difficult.",
                    "label": 0
                },
                {
                    "sent": "I think both ways.",
                    "label": 0
                },
                {
                    "sent": "I think I mean, I think the bridge is a knowledge graph and it's really important.",
                    "label": 0
                },
                {
                    "sent": "It's really contains a lot of information, I mean including of course, YAGO, anti PD and all these things and it's something semantic web people are happy about and I think machine learner also can relate to, for example, the effort on learning and the Knowledge Graph is done by Kevin Murphy who comes from the machine learning and he is leading this team.",
                    "label": 0
                },
                {
                    "sent": "So I guess he thinks he has enough understanding of the structures that he knows how to do.",
                    "label": 0
                },
                {
                    "sent": "Learning in these structures.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a very important connector.",
                    "label": 0
                },
                {
                    "sent": "Hi Folker, thank you very much for a great talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you for being able to convey so much statistical stuff in such a short time to all of us.",
                    "label": 0
                },
                {
                    "sent": "I have a question that will actually 2 questions.",
                    "label": 0
                },
                {
                    "sent": "Question number one.",
                    "label": 0
                },
                {
                    "sent": "There was a paper here in 2008 that showed that if you combine logical reasoning with statistical relational learning that you got superior performance in doing either of them by themselves.",
                    "label": 0
                },
                {
                    "sent": "The tensor stuff you've been showing us is kind of trying to make statistical learning so smart you can.",
                    "label": 0
                },
                {
                    "sent": "It can exploit some of the structure in in in the graph as background data, it does not exploit logical reasoning.",
                    "label": 0
                },
                {
                    "sent": "Do you think that that statement from 2008 is still true today that we could actually use logic to enhance it?",
                    "label": 0
                },
                {
                    "sent": "Or certainly I mean of course we always think of the simplest solution, which means you first do your logic and materialize everything you you can conclude.",
                    "label": 0
                },
                {
                    "sent": "And then we have a more complete database.",
                    "label": 0
                },
                {
                    "sent": "And when we do machine learning on that.",
                    "label": 0
                },
                {
                    "sent": "But there are definitely many other ways of combining it.",
                    "label": 0
                },
                {
                    "sent": "A lot of stuff is presented here.",
                    "label": 0
                },
                {
                    "sent": "Also, a machine learning on the semantic web.",
                    "label": 0
                },
                {
                    "sent": "I mean you have your own work and other people have very different approaches of doing that.",
                    "label": 0
                },
                {
                    "sent": "I think in this Community lot's LP is probably an important approach to machine learning here, or I think people have more lenient towards data mining.",
                    "label": 0
                },
                {
                    "sent": "Way of thinking about data.",
                    "label": 0
                },
                {
                    "sent": "So maybe today you understand a little better how machine learner.",
                    "label": 0
                },
                {
                    "sent": "Typically whatever statistical machine learner things looks looks on the world and things about the world.",
                    "label": 0
                },
                {
                    "sent": "But this does not solve all the problems.",
                    "label": 0
                },
                {
                    "sent": "This is more appropriate if the data is noisy.",
                    "label": 0
                },
                {
                    "sent": "So if this if you have a very dependable logical background knowledge, then you probably would not do this type of machine learning and I think for example in the chemical domain where you can explore chemical structures and there are a lot of really almost deterministic dependencies, IO P for example is very successful, so it's definitely not one.",
                    "label": 0
                },
                {
                    "sent": "Method fits all but, but this is something that we thought was interesting, and now it seems other people in the machine learning community thinking about similar things.",
                    "label": 0
                },
                {
                    "sent": "I think there are really interesting links to NLP type of problems.",
                    "label": 0
                },
                {
                    "sent": "To be able to integrate different knowledge sources and they use sometimes more neural network structures.",
                    "label": 0
                },
                {
                    "sent": "And we think maybe 10s or structures would be more appropriate or more useful and easier to use, but I think there's a lot of crossover going on now suddenly and that's what I find exciting, but definitely true.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's second, completely different question.",
                    "label": 0
                },
                {
                    "sent": "You mentioned projects in the in the medical domain.",
                    "label": 0
                },
                {
                    "sent": "How would you feel about using background knowledge that comes from the web which is sometimes?",
                    "label": 0
                },
                {
                    "sent": "Questionable for medical, you know, inference where you may incur some liability in terms of what you're actually predicting.",
                    "label": 0
                },
                {
                    "sent": "I think this would be very useful information because machine learning like Markov logic network just will start to ignore information which is not reliable.",
                    "label": 0
                },
                {
                    "sent": "I mean the whole issue of how to make this into something which can be used in a clinic.",
                    "label": 0
                },
                {
                    "sent": "There's still a long way in many directions.",
                    "label": 0
                },
                {
                    "sent": "And the question, I mean how OK?",
                    "label": 0
                },
                {
                    "sent": "How is, for example IBM Watson trying to present their knowledge to the end user have also a problem there.",
                    "label": 0
                },
                {
                    "sent": "So I think the feeling is that this these are important problems which have to be addressed, but it's not clear yet how they will become part of the medical practice.",
                    "label": 0
                },
                {
                    "sent": "So one option might be if it's clear there are ten options for treatment and they all seem to be OK. And then we just do a ranking which might be helpful for the physician.",
                    "label": 0
                },
                {
                    "sent": "One way is just to say what you're doing seems to be unusual.",
                    "label": 0
                },
                {
                    "sent": "This is what people are typically doing.",
                    "label": 0
                },
                {
                    "sent": "Very close to the data, maybe this can be accepted, but all these issues are very difficult to deal with, but hopefully we find a way of solving some of them, I would think.",
                    "label": 0
                },
                {
                    "sent": "Could you also say a few words about data mining?",
                    "label": 0
                },
                {
                    "sent": "You have this nice delineation between machine learning and data mining.",
                    "label": 0
                },
                {
                    "sent": "Could you also say something about what the state of the art is in that direction?",
                    "label": 0
                },
                {
                    "sent": "Not really too much now.",
                    "label": 0
                },
                {
                    "sent": "I mean, I could say few things, but they're probably wrong.",
                    "label": 0
                },
                {
                    "sent": "But I know there's a lot of interest in the community on more on the data mining side.",
                    "label": 0
                },
                {
                    "sent": "Of course, my the chair of the Department at the University is critical, and he's more data mining guy.",
                    "label": 0
                },
                {
                    "sent": "And in the next years we're going to work more closely together because we will have a common project and this will be quite interesting for me to understand more in the first level.",
                    "label": 0
                },
                {
                    "sent": "You say it's almost the same, but if you look a little bit deeper.",
                    "label": 0
                },
                {
                    "sent": "At some point you notice that people also think differently about the world and really have preference for different method methodology's.",
                    "label": 0
                },
                {
                    "sent": "But of course, for example to find interesting structures in data.",
                    "label": 0
                },
                {
                    "sent": "That's what I think is is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                },
                {
                    "sent": "In the medical domain, for example, maybe you find interesting patterns.",
                    "label": 0
                },
                {
                    "sent": "First with which the machine learning then can work almost like a conditional random fields and Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "So I mean there's this issue that Yahoo account supposedly has this magic touch on.",
                    "label": 0
                },
                {
                    "sent": "Doing some restructuring of his.",
                    "label": 0
                },
                {
                    "sent": "Jellico was always doing deep learning machine learning but there saying was always like he's the only guy who get it gets it to work and he seems to have some secrets.",
                    "label": 0
                },
                {
                    "sent": "So maybe it is that he is doing some interesting pre analysis by some data mining finding interesting patterns and then use these patterns as a structure for this machine learning because we did some analysis of order sets of orders previously.",
                    "label": 0
                },
                {
                    "sent": "And of course you find certain patterns like a pregnant woman coming to clinic always gets the same sets of.",
                    "label": 0
                },
                {
                    "sent": "Things, and finding these sets as a preprocessing step from a machine learning POV could be very interesting.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm curious.",
                    "label": 0
                },
                {
                    "sent": "Do you see any possibilities for these latent factors for explaining or assigning a kind of meaning or semantics to them to give some insight in in what what the machine learners actually using to to make these judgments?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's it's possible.",
                    "label": 0
                },
                {
                    "sent": "You probably have to work with the non negative factorization approaches 'cause they give you a better interpretability reside.",
                    "label": 0
                },
                {
                    "sent": "We're just exploring this using some computation communication data from Milano and from 10 to.",
                    "label": 0
                },
                {
                    "sent": "And we see some interesting call patterns there.",
                    "label": 0
                },
                {
                    "sent": "And, of course, interpretability visualization of things is a main topic in tensor factorization.",
                    "label": 0
                },
                {
                    "sent": "So if you say here's a tensor data, for example, one is.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, compare the chemical components in solutions and then you have different mixtures and you have.",
                    "label": 0
                },
                {
                    "sent": "I have incoming laser light and then reflecting laser laser lights.",
                    "label": 0
                },
                {
                    "sent": "We also have three indices and then you do the get the data you do and factorization and you can really analyze and detect the components which were hidden to you in the liquid.",
                    "label": 0
                },
                {
                    "sent": "And you get very good results.",
                    "label": 0
                },
                {
                    "sent": "Are they analyze with tensor models ALOT brain data from e.g ECG data e.g data?",
                    "label": 0
                },
                {
                    "sent": "So traditionally it seems the tensor factorization community was very interested into interpretability and getting visualization results out of that.",
                    "label": 0
                },
                {
                    "sent": "Also better with non negativity, I think, so that's something we want to learn more about.",
                    "label": 0
                },
                {
                    "sent": "I think for them it's very surprising that we think of it as a predictive model and we have to learn why they are so interested.",
                    "label": 0
                },
                {
                    "sent": "And I think this is a great basis for analyzing data.",
                    "label": 0
                },
                {
                    "sent": "So we want to explore this link further in the future.",
                    "label": 0
                },
                {
                    "sent": "We haven't really tried to analyze the data over the understand for some reason.",
                    "label": 0
                },
                {
                    "sent": "I mean because because we have a machine learning and we think.",
                    "label": 0
                },
                {
                    "sent": "Forget about disability.",
                    "label": 0
                },
                {
                    "sent": "Wanted to have pretty good modeling, so I'm not sure if in our data we see this interesting structures.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's impossible, but we haven't really explored it.",
                    "label": 0
                },
                {
                    "sent": "But this is definitely one big topic in terms of modeling.",
                    "label": 0
                },
                {
                    "sent": "OK, two questions actually was one and a half OK, you know the commenting briefly the respective of you have any comparison results besides those that you have mentioned between Markov logic networks and your tensor factorization results?",
                    "label": 0
                },
                {
                    "sent": "The other one actually, if there are any plans to extend these techniques to arbitrary energy relation, arbitrary energy relation discount ya.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Somebody here for Michael.",
                    "label": 0
                },
                {
                    "sent": "No, I think everybody loves Markov logic networks except for the people who have tried it.",
                    "label": 0
                },
                {
                    "sent": "It's very convincing concept.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more difficult to use than people would want them to be used.",
                    "label": 0
                },
                {
                    "sent": "So of course the work was done by the students and they always complained a little bit how much work it was to get it to work right so?",
                    "label": 0
                },
                {
                    "sent": "So in some sense I mean this.",
                    "label": 0
                },
                {
                    "sent": "Of course this code is available if you go to Max's website, you can play with the code.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is, here we don't have to put in any rule rules or patterns for initialization.",
                    "label": 0
                },
                {
                    "sent": "Of course, Mac of Logic also has some structure learning we didn't get to results to get results with structure learning, so in some sense we are.",
                    "label": 0
                },
                {
                    "sent": "Our stuff is easier to use, but if you have deep background knowledge, you definitely want to include in your problem solution, then Markov logic is probably better.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Oh, any relations?",
                    "label": 0
                },
                {
                    "sent": "Have you had the yeah?",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "You know relations pioneer relations.",
                    "label": 0
                },
                {
                    "sent": "There's some.",
                    "label": 0
                },
                {
                    "sent": "Interesting focus, definitely.",
                    "label": 0
                },
                {
                    "sent": "If you look at this work he did some good stuff on higher order relations.",
                    "label": 0
                },
                {
                    "sent": "So yet for example, movie movie user matrix and then the third.",
                    "label": 0
                },
                {
                    "sent": "Dimension was the time, so we had different time dependencies there, so there was quite successful.",
                    "label": 0
                },
                {
                    "sent": "But we we did some something in this direction, but not with the rest.",
                    "label": 0
                },
                {
                    "sent": "Can model yet.",
                    "label": 0
                },
                {
                    "sent": "So Max I think was really really cared about the scalability of everything.",
                    "label": 0
                },
                {
                    "sent": "So anything where the feeling the scalability goes down, he was not too interested in and so the so clear that the LS works as elegantly in higher order tensors, but we haven't really explored it.",
                    "label": 0
                },
                {
                    "sent": "OK, I forgot.",
                    "label": 0
                },
                {
                    "sent": "A question also concerning the connection to more expressive reasoning.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine that your knowledge base has something like domain and range and you could assert that certain elements can never be true, how can you comment on how Rascal deals with that?",
                    "label": 0
                },
                {
                    "sent": "Because right now you basically treat all the entities similar and you have big chunks in your tensor that never can be anything different to zero and.",
                    "label": 0
                },
                {
                    "sent": "Rascal even doesn't know that this can be the case, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I discussed this a lot with Max and I mean of course when we compress the 10s or we only get compressed the one, so the zeros are never in the data in some sense, and the LS update doesn't care about the zeros.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't seem to hurt as much as you think it would.",
                    "label": 0
                },
                {
                    "sent": "Hurt is our conclusion, but of course, if you have more detailed knowledge about.",
                    "label": 0
                },
                {
                    "sent": "And I mean, for example, often you're really only interested in predicting one relation, and you're not want to predict all the relations and then OK, one important thing is what you might want to do is to put weights on relations you care more about or you think have a higher influence on what you're predicting.",
                    "label": 0
                },
                {
                    "sent": "And this can work, for example.",
                    "label": 0
                },
                {
                    "sent": "So if you think if you want to predict like whatever movie user relationships, and you have a huge number of other relations in your tensor, and you know there cannot be very important.",
                    "label": 0
                },
                {
                    "sent": "It makes sense to weigh them down and two way the ones which you think are more important given higher weight and this has worked in some of our experiments and other peoples doing this more systematically in there for a reason.",
                    "label": 0
                },
                {
                    "sent": "A dissertation and Hildesheim were definitely coming from and.",
                    "label": 0
                },
                {
                    "sent": "Guys are Fatima something?",
                    "label": 0
                },
                {
                    "sent": "You know they never got the name.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what IMA so there they have been pursuing this very deeply that they put different weights for different tasks on the different relation types.",
                    "label": 0
                },
                {
                    "sent": "Of course, then becomes more complicated.",
                    "label": 0
                },
                {
                    "sent": "I guess they end up with two parameters, morlas essentially weighing down all the other ones accordingly.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be this seems to help.",
                    "label": 0
                },
                {
                    "sent": "Zeros don't seem to hurt in the way we're doing the modeling right now.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's not true.",
                    "label": 0
                },
                {
                    "sent": "OK, after make one point, there really efficient version of Rascal comes when you use the squared error cost function, 'cause then the zeros can really really drop out if you use Bernoulli cost functions or other cost functions, you don't always have this effect.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the zeros really become non zeros and this becomes important in the update of the of the equations.",
                    "label": 0
                },
                {
                    "sent": "So then you would have a big problem there if you use the resco model in the simplest form of the squared error, I think it's not a problem, But if you.",
                    "label": 0
                },
                {
                    "sent": "Have more complex cost functions than the LS is not as effective anymore.",
                    "label": 0
                },
                {
                    "sent": "You might have to use gradient descent learning and then the zeros really become non zeros and you have to always take them into each iteration.",
                    "label": 0
                },
                {
                    "sent": "So then you would definitely try to split up.",
                    "label": 0
                },
                {
                    "sent": "Rescale into simpler.",
                    "label": 0
                },
                {
                    "sent": "Forms and have more emphasis on the relations you really care about.",
                    "label": 0
                },
                {
                    "sent": "Actually, I have the last question.",
                    "label": 0
                },
                {
                    "sent": "We have our ontologies are our data out there.",
                    "label": 0
                },
                {
                    "sent": "They are very nice.",
                    "label": 0
                },
                {
                    "sent": "They're increasing over the time, but we also have problem because there are or there could be information that are conflicting that are noising.",
                    "label": 0
                },
                {
                    "sent": "Information above are over the time but then on the web we find the old information, new information.",
                    "label": 0
                },
                {
                    "sent": "We don't know exactly which is informed which information is reliable or not.",
                    "label": 0
                },
                {
                    "sent": "We have a certain information we don't know how to treat the information that are out there.",
                    "label": 0
                },
                {
                    "sent": "Which space do you see for machine learning methods for coping with this kind of problems, and do you see enough space for managing successfully this kind of problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question, of course, and concerns about the semantic web community.",
                    "label": 0
                },
                {
                    "sent": "I mean and we have one answer which is essentially saying look at the reconstruction of your tents, or if one.",
                    "label": 0
                },
                {
                    "sent": "So triple you thought was true gets a very small weight.",
                    "label": 0
                },
                {
                    "sent": "After training, it's at least inconsistent with the rest of the ontology and you want might want to check that one if that's really a valid triple or not.",
                    "label": 0
                },
                {
                    "sent": "And the other way around of course.",
                    "label": 0
                },
                {
                    "sent": "Also, you predict supposed to be there, which are not really in the data, so that's probably the easier answer for us to do this systematically on a web scale is definitely a big challenge, and this might be one important tool you might might want to use, but.",
                    "label": 0
                },
                {
                    "sent": "Probably cannot solve all your audio problems with the data consistency, and although I think this machine learning people approaches can easily work with uncertain data, of course it's also garbage in, garbage out.",
                    "label": 0
                },
                {
                    "sent": "If your data is very faulty, the best learning system cannot really.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a can have a big contribution, but it will not solve the whole problem, but it just by itself.",
                    "label": 0
                },
                {
                    "sent": "OK, I think there are no more questions, so that's the end of this session.",
                    "label": 0
                },
                {
                    "sent": "Thanks to everyone and thanks to Forker Ann.",
                    "label": 0
                },
                {
                    "sent": "Enjoy your lunch and don't forget to give your vote.",
                    "label": 0
                }
            ]
        }
    }
}