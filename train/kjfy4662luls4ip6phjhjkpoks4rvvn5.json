{
    "id": "kjfy4662luls4ip6phjhjkpoks4rvvn5",
    "title": "Contextual Dueling Bandits",
    "info": {
        "author": [
            "Katja Hofmann, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_hofmann_dueling_bandits/",
    "segmentation": [
        [
            "My talk will be about contextual dueling bandits and this is joint work with my colleagues from your dudak rupture Perry who are both in the audience.",
            "Alex lift guns like us at Microsoft Research and Mustard Story from the University of Amsterdam.",
            "So you have just heard about."
        ],
        [
            "The dueling bandits setting just a brief reminder, in contrast to the K armed bandit setting, we now have a learner that selects two actions in each round, and the only feedback that it observes is whether action A beats B, or whether it be beats a.",
            "So the learner observed this outcome and the goal is to learn to select the best action in contrast to the previous talk, we look at the contextual bandit setting this means."
        ],
        [
            "That in every round nature picks some context vector context information X that can give us information about which action to which actions would be good in this context, and now the task of the learners to learn to select good actions given the context.",
            "This formulation of bended problems is particularly important in practice because in a lot of scenarios where systems interact with users, this interaction with users can often very naturally be captured.",
            "In terms of relative preferences, for example, between items that are recommended to users or for example in search engines where you can interpret user clicks as relative feedback between documents or for example ranking functions.",
            "Also in this scenario we typically observe some kind of contextual information such as the query or aspects of the users or characteristics of the users.",
            "The problem is particularly challenging for a number of reasons, and two of those."
        ],
        [
            "I want to point out here first of all, in the dueling bandit setting, it is not immediately clear what's the best best action.",
            "Is that the algorithm should learn to pick our contribution here is that we introduce a new solution concept that we called the phone Norman winner that is a natural extension of existing solution concepts and it has very nice properties.",
            "We also look at how to find the solution to this problem efficiently.",
            "And particularly we want in particular.",
            "We want to deal with a setting where we can handle huge policy classes that deal with context information and represent three learning algorithms for computing or approximating the best solution in this setting."
        ],
        [
            "Let me give you a brief overview of those.",
            "Some main contributions.",
            "First of all, let's look briefly at the existing solution concepts.",
            "All solution concepts for the Dueling bandit setting that have been proposed so far make some restrictive assumptions about the structure of the preferences that underlie our observations.",
            "For example, they might assume transitivity between preferences, or like we saw in the previous talk, the least restrictive assumption so far is the existence of a Condor set winner.",
            "That means that there is a single action that beats any other action with probability at least 1/2.",
            "Unfortunately, in practice, even this Condor set assumption is very frequently violated, so it is.",
            "It is not guaranteed to exist, and in practice it almost never exists."
        ],
        [
            "In contrast, our solution is based on the idea of instead of selecting a single best action too.",
            "Generalize this idea to sample instead from a distribution of actions.",
            "This means that we want to find a distribution over actions that is guaranteed to beat any other action in the set with probability at least 1/2.",
            "And if we have found such an action, we call this the for Norman Winner and it very nicely holds that this phenomenon winner always is guaranteed to exist, and that's a consequence of looking at this problem as a 0 sum game between two players.",
            "And the Phenomen winner would then be the a solution to this zero sum game.",
            "An hour."
        ],
        [
            "Paper we extend this solution concept to the context to a case.",
            "Up, sorry.",
            "And prepare solutions to this proposed solutions to this problem.",
            "The main difficulty here is that we want to deal with policy classes that are very large.",
            "For concreteness.",
            "Sorry, you can think about the possible policies as the set of all decision trees that could operate on our context vector, and we want to deal with the size of the policy space somehow efficiently.",
            "So as I mentioned, the policy space can typically be large and it can be hard to represent this even.",
            "And we would like to find algorithms that are logarithmic in the time space and amount of data that they require.",
            "Very briefly, we propose three algorithms in two settings.",
            "They are the first one has optimal regret, but unfortunately linear dependency on time and space.",
            "The other two are sub optimal in the regret or in their approximation bound of the Phenomen winner.",
            "But they have a nice logarithmic dependency on space and time.",
            "As you can see, there is an interesting open problem.",
            "Never need to see whether we can find optimal regret algorithms that are efficient in the size of the policies for more detail."
        ],
        [
            "Yes, come and visit our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My talk will be about contextual dueling bandits and this is joint work with my colleagues from your dudak rupture Perry who are both in the audience.",
                    "label": 0
                },
                {
                    "sent": "Alex lift guns like us at Microsoft Research and Mustard Story from the University of Amsterdam.",
                    "label": 0
                },
                {
                    "sent": "So you have just heard about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dueling bandits setting just a brief reminder, in contrast to the K armed bandit setting, we now have a learner that selects two actions in each round, and the only feedback that it observes is whether action A beats B, or whether it be beats a.",
                    "label": 0
                },
                {
                    "sent": "So the learner observed this outcome and the goal is to learn to select the best action in contrast to the previous talk, we look at the contextual bandit setting this means.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That in every round nature picks some context vector context information X that can give us information about which action to which actions would be good in this context, and now the task of the learners to learn to select good actions given the context.",
                    "label": 1
                },
                {
                    "sent": "This formulation of bended problems is particularly important in practice because in a lot of scenarios where systems interact with users, this interaction with users can often very naturally be captured.",
                    "label": 1
                },
                {
                    "sent": "In terms of relative preferences, for example, between items that are recommended to users or for example in search engines where you can interpret user clicks as relative feedback between documents or for example ranking functions.",
                    "label": 0
                },
                {
                    "sent": "Also in this scenario we typically observe some kind of contextual information such as the query or aspects of the users or characteristics of the users.",
                    "label": 0
                },
                {
                    "sent": "The problem is particularly challenging for a number of reasons, and two of those.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to point out here first of all, in the dueling bandit setting, it is not immediately clear what's the best best action.",
                    "label": 1
                },
                {
                    "sent": "Is that the algorithm should learn to pick our contribution here is that we introduce a new solution concept that we called the phone Norman winner that is a natural extension of existing solution concepts and it has very nice properties.",
                    "label": 0
                },
                {
                    "sent": "We also look at how to find the solution to this problem efficiently.",
                    "label": 0
                },
                {
                    "sent": "And particularly we want in particular.",
                    "label": 0
                },
                {
                    "sent": "We want to deal with a setting where we can handle huge policy classes that deal with context information and represent three learning algorithms for computing or approximating the best solution in this setting.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me give you a brief overview of those.",
                    "label": 0
                },
                {
                    "sent": "Some main contributions.",
                    "label": 0
                },
                {
                    "sent": "First of all, let's look briefly at the existing solution concepts.",
                    "label": 0
                },
                {
                    "sent": "All solution concepts for the Dueling bandit setting that have been proposed so far make some restrictive assumptions about the structure of the preferences that underlie our observations.",
                    "label": 0
                },
                {
                    "sent": "For example, they might assume transitivity between preferences, or like we saw in the previous talk, the least restrictive assumption so far is the existence of a Condor set winner.",
                    "label": 0
                },
                {
                    "sent": "That means that there is a single action that beats any other action with probability at least 1/2.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in practice, even this Condor set assumption is very frequently violated, so it is.",
                    "label": 0
                },
                {
                    "sent": "It is not guaranteed to exist, and in practice it almost never exists.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In contrast, our solution is based on the idea of instead of selecting a single best action too.",
                    "label": 1
                },
                {
                    "sent": "Generalize this idea to sample instead from a distribution of actions.",
                    "label": 0
                },
                {
                    "sent": "This means that we want to find a distribution over actions that is guaranteed to beat any other action in the set with probability at least 1/2.",
                    "label": 0
                },
                {
                    "sent": "And if we have found such an action, we call this the for Norman Winner and it very nicely holds that this phenomenon winner always is guaranteed to exist, and that's a consequence of looking at this problem as a 0 sum game between two players.",
                    "label": 0
                },
                {
                    "sent": "And the Phenomen winner would then be the a solution to this zero sum game.",
                    "label": 0
                },
                {
                    "sent": "An hour.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper we extend this solution concept to the context to a case.",
                    "label": 0
                },
                {
                    "sent": "Up, sorry.",
                    "label": 0
                },
                {
                    "sent": "And prepare solutions to this proposed solutions to this problem.",
                    "label": 0
                },
                {
                    "sent": "The main difficulty here is that we want to deal with policy classes that are very large.",
                    "label": 0
                },
                {
                    "sent": "For concreteness.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you can think about the possible policies as the set of all decision trees that could operate on our context vector, and we want to deal with the size of the policy space somehow efficiently.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, the policy space can typically be large and it can be hard to represent this even.",
                    "label": 0
                },
                {
                    "sent": "And we would like to find algorithms that are logarithmic in the time space and amount of data that they require.",
                    "label": 0
                },
                {
                    "sent": "Very briefly, we propose three algorithms in two settings.",
                    "label": 0
                },
                {
                    "sent": "They are the first one has optimal regret, but unfortunately linear dependency on time and space.",
                    "label": 1
                },
                {
                    "sent": "The other two are sub optimal in the regret or in their approximation bound of the Phenomen winner.",
                    "label": 0
                },
                {
                    "sent": "But they have a nice logarithmic dependency on space and time.",
                    "label": 1
                },
                {
                    "sent": "As you can see, there is an interesting open problem.",
                    "label": 0
                },
                {
                    "sent": "Never need to see whether we can find optimal regret algorithms that are efficient in the size of the policies for more detail.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, come and visit our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}