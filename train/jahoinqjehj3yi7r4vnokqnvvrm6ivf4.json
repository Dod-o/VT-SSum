{
    "id": "jahoinqjehj3yi7r4vnokqnvvrm6ivf4",
    "title": "Mining for the Most Certain Predictions from Dyadic Data",
    "info": {
        "author": [
            "Meghana Deodhar, Electrical and Computer Engineering, University of Texas at Austin"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_deodhar_mmcpdd/",
    "segmentation": [
        [
            "Hi everyone, I'm IGN from UT Austin this is joint work with my advisor Joydeep Ghosh.",
            "In this talk we're going to discuss some ways that we have developed for mining the most certain or most reliable predictions from Dyadic data."
        ],
        [
            "So begin by briefly motivating the problem.",
            "I'll then go on to discuss a proposed fix.",
            "An will then see experimental results on to real life datasets."
        ],
        [
            "So in several applications, one wants to make decisions or take actions only on the most accurate or most reliable predictions rather than on the entire test set or entire set of unknown values.",
            "So such a scenario typically arises when active acting on a prediction requires some quantity of our limited resource or the cost incurred on taking an action is proportional to the inaccuracy of the prediction.",
            "So classic example of this in the classification domain is that of direct marketing, where the aim is to identify or subset of the target population.",
            "That's the most likely to respond to advertising.",
            "And your rather than the overall classification accuracy.",
            "What's more important is maximizing the response rate in the topics quintiles.",
            "Which one evaluates violife charts.",
            "Another example is that of finding the most relevant documents for the search query.",
            "So what's common to both these examples is that there is a rare positive class, and the objective is trying to accurately identify some members of this class rather than maximizing the classification accuracy on the entire test set.",
            "So our focus in this talk is going to be slightly different.",
            "We're going to be looking at reliability of regression predictions, and this can be explained by digging an example of stock market player.",
            "So imagine a player in the stock market who is trying to place his bets by predicting stock price values.",
            "He has a limited budget, and so to try and maximize his profit he is going to places bets on the stocks whose prices he's the most confident of predicting accurately.",
            "So not that the exact sign of stock price change does not really matter because he can use both puts and calls, but he's trying to find out what the most accurate predictions are."
        ],
        [
            "So this is the motivation behind our problem.",
            "We want to try and rank predictions by an estimate of their uncertainty or accuracy, and once we obtain such ranking, it can be used to select the top 10 most reliable accurate predictions.",
            "So our primary focus through the stock is going to be on regression problems, but these approaches that we propose can also be extended to classification varies."
        ],
        [
            "Lee so if you have one single linear regression model, then finding the error variance is standard textbook material."
        ],
        [
            "There has also been some work in the neural network community on determining the uncertainty of neural network outputs, and here are some examples."
        ],
        [
            "Another commonly used technique is resampling, where one trains an ensemble of models on bootstrap samples of the training set and the prediction uncertainties, then estimated as the variance across the outputs of the models in the ensemble."
        ],
        [
            "If one uses Bayesian approaches, then the variance of the predictive distribution can be used to estimate the prediction uncertainty."
        ],
        [
            "So all these methods are applicable to a general regression setting.",
            "But now what if the data has a certain special structure?",
            "So we're going to consider Dyadic data through the stock.",
            "An example of dyadic data displayed in the figure here.",
            "So let's say we have a bunch of analysts and a bunch of stocks, and each analyst is making a prediction for the closing price of the stock.",
            "So that's the response variable, and one can think of the response variable as the entries of the stocks.",
            "Analysts matrix.",
            "Some values are unknown or indicated by the question marks.",
            "The independent variables can now be partitioned into two groups, a group of stock attributes, attributes associated with each stock, and set of attributes associated with each analyst.",
            "So one could also have a third set of attributes which attributes associated with the stock analyst pair, like for instance, how accurate was the analysts prediction for the stock in the previous year?",
            "And there are many examples of this kind of this kind of data in your life applications today."
        ],
        [
            "So this is the formal problem definition.",
            "We have a bunch of data points.",
            "XK is the vector of independent variables Y case the target, and we want to learn a map from X to Y along with some way of ranking test points by their prediction reliability.",
            "Now if we just choose one single linear regression model, then the error variance is given by the standard equation.",
            "So one can use that."
        ],
        [
            "Now let's translate this problem statement into the dyadic data annotation your our target variable is the entry in the Matrix Z, so we represent our response variables by ZIJ.",
            "The independent variables.",
            "That's the vector XIJ is consists of a concatenated set of the row and column attributes.",
            "The stock and analyst attributes that you saw in the previous example.",
            "We also associate await WIJ with each matrix entry and this can be set to one for the known entries and O for the missing ones, which effectively ignores the missing ones during the modeling process.",
            "So now can we use the dyadic structure with a single linear regression model?"
        ],
        [
            "To come up with a different way of ranking.",
            "So we proposed global rank or ranking technique which ranks predictions by estimating the error associated with each prediction.",
            "So let's say we're trying to estimate the prediction error for entry by Jay.",
            "This error is estimated by a sum of the row errors and the column errors EI and EJ and the ROEI is estimated as a weighted average of the errors of each element in the room, which is the squared error that you see here."
        ],
        [
            "OK, so now a single model might be fine if the data set is relatively homogeneous, but any.",
            "Large real life data set is typically very heterogeneous, like for example an E Commerce business has a customer product preferences data set which involves customers from various diverse demographics and products across wide ranges.",
            "So simple model is typically not adequate to represent the heterogeneity inherent in this data, and in Cary 2007, we propose simultaneous clustering and learning or technique for predictive modeling of large heterogeneous data.",
            "So the way school works.",
            "Partitioning the data matrix into a grid of blocks and simultaneously learning a prediction model in each block.",
            "So if we have Karo clusters and L column clusters, this technique ends up learning K * L local linear regression models.",
            "So you actually learn one linear regression model in each cluster and one can develop novel ranking techniques based on these local models that are now learned on the data.",
            "So let's look at this."
        ],
        [
            "Of these techniques, so before I actually go to the techniques, let's look at school and a little bit of detail.",
            "Let Roman gamma be assignments of the M Rose to the Karo clusters, and then columns to clusters respectively.",
            "So we want to find a pro clustering sets of cluster models represent.",
            "And this objective function is nothing but the squared error between the original entry in the matrix, which is ZUV and the entry which is predicted by the local model in the code cluster that this entry belongs to, which is Z had TV and the squared error simply summed over all the entries in the matrix."
        ],
        [
            "One can use simple iterative algorithm to obtain a local minimum of this function, and even though we are focusing on linear regression models here for this piece of explaining, this can also be extended to several other cost functions and models."
        ],
        [
            "So the first ranking technique we propose, based on school is rocholl ranking, where again, like in the global model case, we estimate the prediction error as a sum of the estimated row and column errors, just the way that the row and column arrows are estimated is now different.",
            "They are based on the local models rather than one global."
        ],
        [
            "Marie.",
            "The next technique we propose is block rank, so the intuition behind this is that.",
            "Models in different clusters will have different fits.",
            "I can do it if Lee one can see that models in sparse or noisy regions of the data input space won't really have a very good fit and will have lower accuracy.",
            "So the first step is to rank the test points or the predictions by the fit of the model that is supposed to represent them.",
            "So by by the cluster error mean squared error in this case and then one can refine the ranking within each cluster.",
            "But again predicting the.",
            "Error associated with the prediction as a sum of the Coke luster rule.",
            "Error in the column error."
        ],
        [
            "Next, we propose robust school, a technique for modeling only a selected subset of the data.",
            "So the motivation behind this is that in several applications we don't really need to make a prediction for every test point.",
            "Let's take a recommender system like Netflix.",
            "As long as you make very accurate predictions for a bunch of users, for some movies, that's fine.",
            "You don't really need to find out about make predictions about every user in every movie pair.",
            "Also, datasets definitely might have outliers, and these outliers can skew the models that include them.",
            "So one might be able to learn more accurate and better generalizable models by detecting these outliers, discarding them, and not really including them in the models that are trained.",
            "So robust school aims at selecting and clustering only a subset of the rows and columns.",
            "So ES are Rosen SC columns in two K * L Co clusters as before.",
            "The objective function in this case is the average squared error sound only over the selected matrix entries.",
            "So sound only over Sr Times AC entries of the Matrix."
        ],
        [
            "This objective function can be reduced to a local minimum by an iterative algorithm which begins with an initial clustering and iterates over three steps until convergence.",
            "So the first step updates the linear regression model in each class.",
            "Stop this by using simple least squares solution in Step 2, the row cluster assignments updated.",
            "So this is done by first assigning each row to the closest row cluster.",
            "My closest children is in the sense of the smallest prediction error.",
            "Then one needs to select a set of Sr rows from amongst the total number of rows that minimizes the objective function, and this can be posed as a problem very similar to the knapsack problem and solved by dynamic programming.",
            "Similarly, one can update the column cluster assignments, so each of these three steps reduces the objective function and so convergence to a local optimum is guaranteed."
        ],
        [
            "OK, so so far we have looked at several different ranking techniques we can think of them as three conceptual techniques based on the unit of selection of each.",
            "So global rank and row call ranking recalled the select individual entries of the Matrix and so these techniques are the most flexible block ranks Alexco clusters, whereas robust school selects individual matrix rows and columns.",
            "Now which technique to use would depend on the application requirements.",
            "So if the application requirements dictate certain constraints on the predictions that are selected, like for instance an E Commerce business wants to advertise in a certain geographical area, then block ranking might be the best approach to use because you can select Co clusters that correspond to that constraint from amongst the top ones.",
            "Robust code is also very actionable and cost effective because it can prune away the non informative parts of the data and restrict focus to just a small subset of the data."
        ],
        [
            "Another interesting point is that these ranking techniques can be combined to make predictions or select predictions at multiple resolutions like one could use robust call first to select matrix rows and columns, then rank Co clusters and then rank the entries within each cluster."
        ],
        [
            "So now how do we evaluate ranking?",
            "So we propose a technique called certainty lift or a measure called certainty left to do so, which evaluates the improvement in the error for selected subset of predictions as compared to the baseline error on the entire test set.",
            "So the certainty lift of a model on a selected subset of data is the ratio of the average squared error of the baseline on all the predictions to the average squared error of the model on the selected subset.",
            "So certainty left of, say, 5 basically implies that the model does 5 times better on the selected subset as compared to the baseline error on the entire test set of predictions."
        ],
        [
            "Let's look at some results now.",
            "We experimented in the MOVIELENS data set.",
            "And you."
        ],
        [
            "The results, so here we are comparing different ranking techniques.",
            "Global Re Sample is the resampling based technique global.",
            "Where is the standard error estimation technique for a single linear regression model.",
            "So both these techniques don't use the dyadic structure and you can see that they are right at the top.",
            "They don't really do that well and the rest of the techniques actually use the dyadic structure of the data.",
            "So the way we've obtained these plots is by taking the ranking output by each of these techniques and using it to order the test points.",
            "In decreasing order of the prediction accuracy or estimated prediction accuracy, so in the X axis you have the fraction of the test entries predicted and the Y axis.",
            "You have the mean squared error of that selected fraction.",
            "So the steeper the plot, the better it is, and you can see that the techniques like robust school and local ranking which actually user ID structure do a lot better."
        ],
        [
            "The next data set we used is the M household panel data.",
            "It consists of purchase information off around 1700 households across 6 product categories involving around 1:20 products and the problem here is to predict the number of units of a product purchased by a household and the distribution of the response variables is actually very skewed and there are some very large outliers."
        ],
        [
            "So you have the results, the plot and the left as before shows different or compares different ranking techniques.",
            "And here again you can see that the steeper plots are for the techniques that actually use the dyadic structure on the right hand side you see certainty lift of the different approaches, so you can see rocholl ranking doing almost 25 times better than the baseline on the selected 10% of the data.",
            "In terms of mean squared error."
        ],
        [
            "OK, so we then evaluated robust scores on this data set to try and see how good it is at identifying and discarding outliers.",
            "And we defined outliers as you by using a certain threshold of the response variable.",
            "So how are we evaluating robust goal for different fractions of the data model given by different S R&S Y values?",
            "So the last row includes the entire data set being modeled.",
            "So you can think of this as just regular school and not robust cool.",
            "You can see that the percentage of outliers discovered in each case is much higher than the percentage of data discovered.",
            "So robust code is actually able to selectively target the outliers and discard them.",
            "And we found the mean squared error on the product on the missing entries that were included in this model subset was much lower than the prediction error in their entire tests on the entire data set of the model, everything.",
            "So, discarding outliers actually improve the prediction error."
        ],
        [
            "So to conclude, we have presented new ranking techniques that show promising performance on two real life datasets.",
            "One point I would like to make is that from an application viewpoint, robust school and block ranking might be more important just because of the structured way in which they select the predictions that might be more actionable and more interpretable for certain applications.",
            "Also, these techniques are applicable to classification problems as well, and we obtain good results on.",
            "Some classification problems also.",
            "Now this local models give us another way of estimating prediction uncertainty.",
            "Very different way than as one would get with a global model and we're now working on using this for developing some novel active learning techniques.",
            "Finally, another research direction is to use robust error functions to do a soft selection of the data subset to be modeled rather than a hard selection.",
            "Which robust scold us by actually pruning away entire rows and columns.",
            "So thank you and I can now take questions, if any.",
            "My birthday questions.",
            "So instead of discarding outliers the way it is, is it possible to model the data by using some heavy tail distribution?",
            "Yeah, that's the thing that we wanted to try that out, like some robust error functions.",
            "We haven't done it yet, but that's definitely something we want to try, because then we would have sort of a soft selection."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, I'm IGN from UT Austin this is joint work with my advisor Joydeep Ghosh.",
                    "label": 0
                },
                {
                    "sent": "In this talk we're going to discuss some ways that we have developed for mining the most certain or most reliable predictions from Dyadic data.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So begin by briefly motivating the problem.",
                    "label": 0
                },
                {
                    "sent": "I'll then go on to discuss a proposed fix.",
                    "label": 0
                },
                {
                    "sent": "An will then see experimental results on to real life datasets.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in several applications, one wants to make decisions or take actions only on the most accurate or most reliable predictions rather than on the entire test set or entire set of unknown values.",
                    "label": 0
                },
                {
                    "sent": "So such a scenario typically arises when active acting on a prediction requires some quantity of our limited resource or the cost incurred on taking an action is proportional to the inaccuracy of the prediction.",
                    "label": 0
                },
                {
                    "sent": "So classic example of this in the classification domain is that of direct marketing, where the aim is to identify or subset of the target population.",
                    "label": 0
                },
                {
                    "sent": "That's the most likely to respond to advertising.",
                    "label": 0
                },
                {
                    "sent": "And your rather than the overall classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "What's more important is maximizing the response rate in the topics quintiles.",
                    "label": 0
                },
                {
                    "sent": "Which one evaluates violife charts.",
                    "label": 0
                },
                {
                    "sent": "Another example is that of finding the most relevant documents for the search query.",
                    "label": 1
                },
                {
                    "sent": "So what's common to both these examples is that there is a rare positive class, and the objective is trying to accurately identify some members of this class rather than maximizing the classification accuracy on the entire test set.",
                    "label": 0
                },
                {
                    "sent": "So our focus in this talk is going to be slightly different.",
                    "label": 1
                },
                {
                    "sent": "We're going to be looking at reliability of regression predictions, and this can be explained by digging an example of stock market player.",
                    "label": 0
                },
                {
                    "sent": "So imagine a player in the stock market who is trying to place his bets by predicting stock price values.",
                    "label": 0
                },
                {
                    "sent": "He has a limited budget, and so to try and maximize his profit he is going to places bets on the stocks whose prices he's the most confident of predicting accurately.",
                    "label": 0
                },
                {
                    "sent": "So not that the exact sign of stock price change does not really matter because he can use both puts and calls, but he's trying to find out what the most accurate predictions are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the motivation behind our problem.",
                    "label": 0
                },
                {
                    "sent": "We want to try and rank predictions by an estimate of their uncertainty or accuracy, and once we obtain such ranking, it can be used to select the top 10 most reliable accurate predictions.",
                    "label": 1
                },
                {
                    "sent": "So our primary focus through the stock is going to be on regression problems, but these approaches that we propose can also be extended to classification varies.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee so if you have one single linear regression model, then finding the error variance is standard textbook material.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There has also been some work in the neural network community on determining the uncertainty of neural network outputs, and here are some examples.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another commonly used technique is resampling, where one trains an ensemble of models on bootstrap samples of the training set and the prediction uncertainties, then estimated as the variance across the outputs of the models in the ensemble.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If one uses Bayesian approaches, then the variance of the predictive distribution can be used to estimate the prediction uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all these methods are applicable to a general regression setting.",
                    "label": 0
                },
                {
                    "sent": "But now what if the data has a certain special structure?",
                    "label": 1
                },
                {
                    "sent": "So we're going to consider Dyadic data through the stock.",
                    "label": 0
                },
                {
                    "sent": "An example of dyadic data displayed in the figure here.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have a bunch of analysts and a bunch of stocks, and each analyst is making a prediction for the closing price of the stock.",
                    "label": 0
                },
                {
                    "sent": "So that's the response variable, and one can think of the response variable as the entries of the stocks.",
                    "label": 0
                },
                {
                    "sent": "Analysts matrix.",
                    "label": 0
                },
                {
                    "sent": "Some values are unknown or indicated by the question marks.",
                    "label": 1
                },
                {
                    "sent": "The independent variables can now be partitioned into two groups, a group of stock attributes, attributes associated with each stock, and set of attributes associated with each analyst.",
                    "label": 0
                },
                {
                    "sent": "So one could also have a third set of attributes which attributes associated with the stock analyst pair, like for instance, how accurate was the analysts prediction for the stock in the previous year?",
                    "label": 0
                },
                {
                    "sent": "And there are many examples of this kind of this kind of data in your life applications today.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the formal problem definition.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of data points.",
                    "label": 0
                },
                {
                    "sent": "XK is the vector of independent variables Y case the target, and we want to learn a map from X to Y along with some way of ranking test points by their prediction reliability.",
                    "label": 1
                },
                {
                    "sent": "Now if we just choose one single linear regression model, then the error variance is given by the standard equation.",
                    "label": 0
                },
                {
                    "sent": "So one can use that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's translate this problem statement into the dyadic data annotation your our target variable is the entry in the Matrix Z, so we represent our response variables by ZIJ.",
                    "label": 1
                },
                {
                    "sent": "The independent variables.",
                    "label": 0
                },
                {
                    "sent": "That's the vector XIJ is consists of a concatenated set of the row and column attributes.",
                    "label": 1
                },
                {
                    "sent": "The stock and analyst attributes that you saw in the previous example.",
                    "label": 1
                },
                {
                    "sent": "We also associate await WIJ with each matrix entry and this can be set to one for the known entries and O for the missing ones, which effectively ignores the missing ones during the modeling process.",
                    "label": 1
                },
                {
                    "sent": "So now can we use the dyadic structure with a single linear regression model?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To come up with a different way of ranking.",
                    "label": 1
                },
                {
                    "sent": "So we proposed global rank or ranking technique which ranks predictions by estimating the error associated with each prediction.",
                    "label": 1
                },
                {
                    "sent": "So let's say we're trying to estimate the prediction error for entry by Jay.",
                    "label": 0
                },
                {
                    "sent": "This error is estimated by a sum of the row errors and the column errors EI and EJ and the ROEI is estimated as a weighted average of the errors of each element in the room, which is the squared error that you see here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now a single model might be fine if the data set is relatively homogeneous, but any.",
                    "label": 0
                },
                {
                    "sent": "Large real life data set is typically very heterogeneous, like for example an E Commerce business has a customer product preferences data set which involves customers from various diverse demographics and products across wide ranges.",
                    "label": 0
                },
                {
                    "sent": "So simple model is typically not adequate to represent the heterogeneity inherent in this data, and in Cary 2007, we propose simultaneous clustering and learning or technique for predictive modeling of large heterogeneous data.",
                    "label": 0
                },
                {
                    "sent": "So the way school works.",
                    "label": 0
                },
                {
                    "sent": "Partitioning the data matrix into a grid of blocks and simultaneously learning a prediction model in each block.",
                    "label": 0
                },
                {
                    "sent": "So if we have Karo clusters and L column clusters, this technique ends up learning K * L local linear regression models.",
                    "label": 1
                },
                {
                    "sent": "So you actually learn one linear regression model in each cluster and one can develop novel ranking techniques based on these local models that are now learned on the data.",
                    "label": 1
                },
                {
                    "sent": "So let's look at this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of these techniques, so before I actually go to the techniques, let's look at school and a little bit of detail.",
                    "label": 0
                },
                {
                    "sent": "Let Roman gamma be assignments of the M Rose to the Karo clusters, and then columns to clusters respectively.",
                    "label": 0
                },
                {
                    "sent": "So we want to find a pro clustering sets of cluster models represent.",
                    "label": 0
                },
                {
                    "sent": "And this objective function is nothing but the squared error between the original entry in the matrix, which is ZUV and the entry which is predicted by the local model in the code cluster that this entry belongs to, which is Z had TV and the squared error simply summed over all the entries in the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One can use simple iterative algorithm to obtain a local minimum of this function, and even though we are focusing on linear regression models here for this piece of explaining, this can also be extended to several other cost functions and models.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first ranking technique we propose, based on school is rocholl ranking, where again, like in the global model case, we estimate the prediction error as a sum of the estimated row and column errors, just the way that the row and column arrows are estimated is now different.",
                    "label": 0
                },
                {
                    "sent": "They are based on the local models rather than one global.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marie.",
                    "label": 0
                },
                {
                    "sent": "The next technique we propose is block rank, so the intuition behind this is that.",
                    "label": 0
                },
                {
                    "sent": "Models in different clusters will have different fits.",
                    "label": 0
                },
                {
                    "sent": "I can do it if Lee one can see that models in sparse or noisy regions of the data input space won't really have a very good fit and will have lower accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to rank the test points or the predictions by the fit of the model that is supposed to represent them.",
                    "label": 0
                },
                {
                    "sent": "So by by the cluster error mean squared error in this case and then one can refine the ranking within each cluster.",
                    "label": 0
                },
                {
                    "sent": "But again predicting the.",
                    "label": 0
                },
                {
                    "sent": "Error associated with the prediction as a sum of the Coke luster rule.",
                    "label": 0
                },
                {
                    "sent": "Error in the column error.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next, we propose robust school, a technique for modeling only a selected subset of the data.",
                    "label": 1
                },
                {
                    "sent": "So the motivation behind this is that in several applications we don't really need to make a prediction for every test point.",
                    "label": 0
                },
                {
                    "sent": "Let's take a recommender system like Netflix.",
                    "label": 0
                },
                {
                    "sent": "As long as you make very accurate predictions for a bunch of users, for some movies, that's fine.",
                    "label": 1
                },
                {
                    "sent": "You don't really need to find out about make predictions about every user in every movie pair.",
                    "label": 0
                },
                {
                    "sent": "Also, datasets definitely might have outliers, and these outliers can skew the models that include them.",
                    "label": 0
                },
                {
                    "sent": "So one might be able to learn more accurate and better generalizable models by detecting these outliers, discarding them, and not really including them in the models that are trained.",
                    "label": 0
                },
                {
                    "sent": "So robust school aims at selecting and clustering only a subset of the rows and columns.",
                    "label": 0
                },
                {
                    "sent": "So ES are Rosen SC columns in two K * L Co clusters as before.",
                    "label": 1
                },
                {
                    "sent": "The objective function in this case is the average squared error sound only over the selected matrix entries.",
                    "label": 0
                },
                {
                    "sent": "So sound only over Sr Times AC entries of the Matrix.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This objective function can be reduced to a local minimum by an iterative algorithm which begins with an initial clustering and iterates over three steps until convergence.",
                    "label": 0
                },
                {
                    "sent": "So the first step updates the linear regression model in each class.",
                    "label": 0
                },
                {
                    "sent": "Stop this by using simple least squares solution in Step 2, the row cluster assignments updated.",
                    "label": 0
                },
                {
                    "sent": "So this is done by first assigning each row to the closest row cluster.",
                    "label": 1
                },
                {
                    "sent": "My closest children is in the sense of the smallest prediction error.",
                    "label": 1
                },
                {
                    "sent": "Then one needs to select a set of Sr rows from amongst the total number of rows that minimizes the objective function, and this can be posed as a problem very similar to the knapsack problem and solved by dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "Similarly, one can update the column cluster assignments, so each of these three steps reduces the objective function and so convergence to a local optimum is guaranteed.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so far we have looked at several different ranking techniques we can think of them as three conceptual techniques based on the unit of selection of each.",
                    "label": 1
                },
                {
                    "sent": "So global rank and row call ranking recalled the select individual entries of the Matrix and so these techniques are the most flexible block ranks Alexco clusters, whereas robust school selects individual matrix rows and columns.",
                    "label": 0
                },
                {
                    "sent": "Now which technique to use would depend on the application requirements.",
                    "label": 0
                },
                {
                    "sent": "So if the application requirements dictate certain constraints on the predictions that are selected, like for instance an E Commerce business wants to advertise in a certain geographical area, then block ranking might be the best approach to use because you can select Co clusters that correspond to that constraint from amongst the top ones.",
                    "label": 0
                },
                {
                    "sent": "Robust code is also very actionable and cost effective because it can prune away the non informative parts of the data and restrict focus to just a small subset of the data.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another interesting point is that these ranking techniques can be combined to make predictions or select predictions at multiple resolutions like one could use robust call first to select matrix rows and columns, then rank Co clusters and then rank the entries within each cluster.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now how do we evaluate ranking?",
                    "label": 0
                },
                {
                    "sent": "So we propose a technique called certainty lift or a measure called certainty left to do so, which evaluates the improvement in the error for selected subset of predictions as compared to the baseline error on the entire test set.",
                    "label": 1
                },
                {
                    "sent": "So the certainty lift of a model on a selected subset of data is the ratio of the average squared error of the baseline on all the predictions to the average squared error of the model on the selected subset.",
                    "label": 0
                },
                {
                    "sent": "So certainty left of, say, 5 basically implies that the model does 5 times better on the selected subset as compared to the baseline error on the entire test set of predictions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at some results now.",
                    "label": 0
                },
                {
                    "sent": "We experimented in the MOVIELENS data set.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results, so here we are comparing different ranking techniques.",
                    "label": 0
                },
                {
                    "sent": "Global Re Sample is the resampling based technique global.",
                    "label": 0
                },
                {
                    "sent": "Where is the standard error estimation technique for a single linear regression model.",
                    "label": 0
                },
                {
                    "sent": "So both these techniques don't use the dyadic structure and you can see that they are right at the top.",
                    "label": 0
                },
                {
                    "sent": "They don't really do that well and the rest of the techniques actually use the dyadic structure of the data.",
                    "label": 0
                },
                {
                    "sent": "So the way we've obtained these plots is by taking the ranking output by each of these techniques and using it to order the test points.",
                    "label": 0
                },
                {
                    "sent": "In decreasing order of the prediction accuracy or estimated prediction accuracy, so in the X axis you have the fraction of the test entries predicted and the Y axis.",
                    "label": 0
                },
                {
                    "sent": "You have the mean squared error of that selected fraction.",
                    "label": 0
                },
                {
                    "sent": "So the steeper the plot, the better it is, and you can see that the techniques like robust school and local ranking which actually user ID structure do a lot better.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next data set we used is the M household panel data.",
                    "label": 0
                },
                {
                    "sent": "It consists of purchase information off around 1700 households across 6 product categories involving around 1:20 products and the problem here is to predict the number of units of a product purchased by a household and the distribution of the response variables is actually very skewed and there are some very large outliers.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have the results, the plot and the left as before shows different or compares different ranking techniques.",
                    "label": 1
                },
                {
                    "sent": "And here again you can see that the steeper plots are for the techniques that actually use the dyadic structure on the right hand side you see certainty lift of the different approaches, so you can see rocholl ranking doing almost 25 times better than the baseline on the selected 10% of the data.",
                    "label": 1
                },
                {
                    "sent": "In terms of mean squared error.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we then evaluated robust scores on this data set to try and see how good it is at identifying and discarding outliers.",
                    "label": 0
                },
                {
                    "sent": "And we defined outliers as you by using a certain threshold of the response variable.",
                    "label": 0
                },
                {
                    "sent": "So how are we evaluating robust goal for different fractions of the data model given by different S R&S Y values?",
                    "label": 0
                },
                {
                    "sent": "So the last row includes the entire data set being modeled.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as just regular school and not robust cool.",
                    "label": 0
                },
                {
                    "sent": "You can see that the percentage of outliers discovered in each case is much higher than the percentage of data discovered.",
                    "label": 0
                },
                {
                    "sent": "So robust code is actually able to selectively target the outliers and discard them.",
                    "label": 0
                },
                {
                    "sent": "And we found the mean squared error on the product on the missing entries that were included in this model subset was much lower than the prediction error in their entire tests on the entire data set of the model, everything.",
                    "label": 0
                },
                {
                    "sent": "So, discarding outliers actually improve the prediction error.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we have presented new ranking techniques that show promising performance on two real life datasets.",
                    "label": 0
                },
                {
                    "sent": "One point I would like to make is that from an application viewpoint, robust school and block ranking might be more important just because of the structured way in which they select the predictions that might be more actionable and more interpretable for certain applications.",
                    "label": 0
                },
                {
                    "sent": "Also, these techniques are applicable to classification problems as well, and we obtain good results on.",
                    "label": 1
                },
                {
                    "sent": "Some classification problems also.",
                    "label": 0
                },
                {
                    "sent": "Now this local models give us another way of estimating prediction uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Very different way than as one would get with a global model and we're now working on using this for developing some novel active learning techniques.",
                    "label": 0
                },
                {
                    "sent": "Finally, another research direction is to use robust error functions to do a soft selection of the data subset to be modeled rather than a hard selection.",
                    "label": 1
                },
                {
                    "sent": "Which robust scold us by actually pruning away entire rows and columns.",
                    "label": 0
                },
                {
                    "sent": "So thank you and I can now take questions, if any.",
                    "label": 0
                },
                {
                    "sent": "My birthday questions.",
                    "label": 0
                },
                {
                    "sent": "So instead of discarding outliers the way it is, is it possible to model the data by using some heavy tail distribution?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the thing that we wanted to try that out, like some robust error functions.",
                    "label": 0
                },
                {
                    "sent": "We haven't done it yet, but that's definitely something we want to try, because then we would have sort of a soft selection.",
                    "label": 0
                }
            ]
        }
    }
}