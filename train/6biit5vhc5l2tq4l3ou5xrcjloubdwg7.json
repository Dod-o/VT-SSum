{
    "id": "6biit5vhc5l2tq4l3ou5xrcjloubdwg7",
    "title": "Unfolding an Indoor Origami World",
    "info": {
        "author": [
            "David Ford Fouhey, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2014_ford_fouhey_origami_world/",
    "segmentation": [
        [
            "So I'm David Bowie.",
            "This is unfolding origami world and its joint work with my advisors Audemar Schobert all at Carnegie Mellon Robotics Institute."
        ],
        [
            "So the goal of this work is to take a single input image, such as the one I'm showing you here, and to infer a 3D interpretation of the scene, and specifically we're interested in inferring an interpretation in."
        ],
        [
            "And the large planar pieces of the scene, as well as their orientations.",
            "So in this interpretation, each color corresponds to different direction or surface normal.",
            "And so humans are able to do this very easily from any single image, and we'd like to give the same ability to computers.",
            "So the question is, how can you infer the three properties of a scene from a single image?",
            "Because of course, as we all know, this is mathematically impossible.",
            "So the first answer is at the local level."
        ],
        [
            "There's enough to tell you things, so if you look at parts of this kitchen scene, this is actually true.",
            "So if you look at the back of the kitchen and we look at these cabinets, the texture on the cabinets just tells us the orientation right away.",
            "And if you look at the bottom of the image where the floor meets the cabinets, this is also very, very obvious.",
            "But on the other hand, if we look at the walls, or if you look at the top of the counter, we just have two beige blobs and these don't tell us anything about the orientation.",
            "So what we can see is there are parts of the scene which are easy to infer from local evidence and parts of the scene which are hard to infer from local Evans.",
            "So naturally what we need to do is you."
        ],
        [
            "Apply constraints on our interpretation and these constraints.",
            "Let us propagate information from the parts of the scene which are easy to recognize, such as the cabinets in the floor to the parts of the scene that are hard to recognize, such as the wall on the top of this counter."
        ],
        [
            "So what constraints do we have at our disposal in single image 3D?",
            "So in the past 10 years there have been two dominant sources of constraints and the first one is a local smoothness idea and it's a basic low level generic constraint that can be applied in all sorts of different problems, like segmentation for instance."
        ],
        [
            "Basic idea is that if you look at two parts of the scene and they have the same color in their nearby, then they should probably have the same label.",
            "Of course, this constraint is generic and it doesn't take advantage of the nature of the problem, and if we're trying to infer 3D interpretation or interpretation in the end needs to be a valid 3D understanding, and so recognizing this people have turned to physical constraints."
        ],
        [
            "And the idea here is that you use physical properties of the scene to produce constraints on your interpretation, which allows you to get better results.",
            "These"
        ],
        [
            "Trains have tended to be very high level.",
            "So for instance, there's a Manhattan Rd constraint that's been extremely popular and the idea is that there are three orthogonal directions in the scene, and these are typically estimated with vanishing points.",
            "Oh"
        ],
        [
            "Another very high level constraint has been the idea that rooms in most man made scenes can be approximated as a 3D cuboid, and this has also been very popular, but this doesn't it."
        ],
        [
            "Things like furniture and so people have also turned to inferring interpretations by fitting more cuboids inside of scenes.",
            "So what you can?"
        ],
        [
            "She is a constraint that we have are either low level and generic or extremely high level and physical.",
            "So it seems that in order to use physical constraints for our interpretation, we're going to have to go all the way up to other global constraints or fitting volumetric primitives to image data, which is a very hard task.",
            "So the question we had was, are there other physical constraints which are physical but don't require going all the way up to volumetric primitives?"
        ],
        [
            "So actually if you go back 30 or 40 years, you can find a rich literature on mid level primitives and this is from the line labeling era and these parameters were in terms of planar pieces as well as the edges between them and the junctions at which these edges joined and so of course we can't actually use these constraints directly on any of our problems today because the input assumptions are very different, But we can draw a lot of inspiration from these techniques and the way that they approach problems and the types of outputs they produced and so specifically in this work, the constraint that we're going to try to use.",
            "Is a fall."
        ],
        [
            "Going and it's based on convex and concave edges that we detect in the scene.",
            "If we want to infer the top of the counter, then here's one way we can figure out that it's facing upwards.",
            "We first recognize this convex edge, and this is very easy to recognize in this kitchen scene.",
            "We then also recognize the distinctive front of the cabinets, and this is a vertical surface, and using the fact that we had to have a physical interpretation were able to fold over an interpretation and pull out the top of this counter because we have this convex edge in the front of the cabinets.",
            "And Additionally."
        ],
        [
            "Our output is going to be inspired by these past work and that it's a discrete sine parts.",
            "It's a parse in terms of large planar pieces of the scene and their orientations as opposed to a continuous estimate of the surface normal at every single pixel, and we think this is a richer interpretation for many applications so."
        ],
        [
            "Now that I've introduced a problem to discuss the rest of the talk, first to introduce a parameterisation that we use, and then I'll discuss the formulation that we used to figure out the interpretation of the scene.",
            "And finally I'll show some experimental results of our method in action."
        ],
        [
            "So begin with the parameterisation."
        ],
        [
            "So in this work we again deal with single images.",
            "So the first thing that we do, like many works in this field."
        ],
        [
            "Is that we estimate three orthogonal vanishing points and we use this using a standard detector.",
            "Now that we have three orthogonal vanishing points, what we do is we sweep two Rays from one of the vanishing points and then we can sweep to raise from another vanishing point.",
            "And what this does is it defines a quadrilateral that defines an oriented plane and we can do this with all."
        ],
        [
            "Pairs of vanishing points and we can get three overlapping grids and these grids are overlapping and they define what we call like.",
            "Top down super pixels their grid cells.",
            "They encode not only what pixels are part of them, but also orientation as well.",
            "So what we can do is we can take these grids."
        ],
        [
            "And we can encode surface normals.",
            "So here we have an interpretation of our algorithm, and each of the colors here indicates a different orientation.",
            "So Green is facing upwards, for instance, and we can encode this orientation using the grid cells."
        ],
        [
            "So what we do is we take the green upwards facing surface is and we."
        ],
        [
            "Take the green upward facing grid, which you can see on the left, and we turn.",
            "We encode the green upward facing surfaces by turning on grid cells and we can do the exact same thing for the purple grid cells which are facing to the left and two for the blue ones which are facing towards the right.",
            "And what this means is that we can encode."
        ],
        [
            "The orientation of the surface is in the scene with a binary indicator vector that goes over all the grid cells and we can optimize over this."
        ],
        [
            "We note that our parameterization isn't is inspired by an.",
            "It's an extension of many of the past works in single image 3D which are based on bounding Rays."
        ],
        [
            "OK, so now they have introduced the basic parameterisation.",
            "We're going to use.",
            "I'm not going to explain the formulation that we use to figure out an interpretation of the scene.",
            "So."
        ],
        [
            "So recall that our indicator variable that we're optimizing over is a binary indicator vector which says which grid cells are on.",
            "And so this is a binary indicator vector that goes over all the grid."
        ],
        [
            "We formulate the problem as a binary quadratic program subject to some basic linear constraints and to go over each of these terms in turn.",
            "So."
        ],
        [
            "So we have unary terms and.",
            "These indicate whether a grid cell should be on or off, and."
        ],
        [
            "Basic idea is that you have any form of 3D evidence that you can pull out of it 2D image and you can check whether it agrees with the grid cell.",
            "So for instance, we think that the bottom of this image should be facing upwards, so we would reward the green upwards facing grid cells for turning on.",
            "And we would pay NYS the blue vertical surfaces for turning on at the bottom of the image as well.",
            "And we can use any form of 3D evidence and in this work we use 2."
        ],
        [
            "Forms the 1st is a local way of getting surface normals, and it's from data driven 3D primitives which we presented at last I see CV which has appeared detector and surface normal configuration bank and enables you to transfer its surface normals wherever your detectors have a high response and it produces an output something like this.",
            "We also use a complementary source of 3D evidence."
        ],
        [
            "It is based on the standard cuboid fitting method.",
            "Had all and we use this to also produce a unary."
        ],
        [
            "So now that we have unary's, we can then include binary terms and these determine whether two grid cells should be rewarded for turning on at the same time."
        ],
        [
            "So the binary terms that we introduce are based on convex and concave constraints, and the idea is that you can enumerate the configurations that are possible in terms of planar surfaces and their orientations as well as convex and concave edges in a scene.",
            "So for instance, in this kitchen scene you have the upward facing counter, you have a convex edge beneath it, and you have the vertical surface beneath it as well."
        ],
        [
            "So what we do is we detect convex and concave edges in an image and they use to produce constraints.",
            "So here I'm showing detected concave edges.",
            "So."
        ],
        [
            "If we have a piece of the scene, for instance this blue rightwards facing piece, we can determine what sorts of things we should reward for turning on at the same time."
        ],
        [
            "So for instance, if we turn on this purple surface, it forms a concave edge, and so it agrees with the detected concave edges.",
            "On the other hand, if"
        ],
        [
            "We produce a flat interpretation.",
            "This is not an agreement with the detected concave edge, and so we do not reward it for being turned on."
        ],
        [
            "And Furthermore, if we, if we form a convex edge, this does not agree with the concave edge, and so we do not reward it."
        ],
        [
            "And in order to take convex and concave edges, you can use any method that you want, and our approach is generic, can handle any sort of method, and for simplicity we just use a 3D primitives and we simply swap out the surface normal configurations with convex and concave edges and proceed as usual, and this produces an output such as this in terms of convex edges.",
            "OK."
        ],
        [
            "We also Additionally use a local generic smoothness term and this just says that if two grid cells have this or have the same orientation and they have the same appearance, then they should both be on.",
            "So for instance, the top of the bed, they both are Leopard print, so they should both be on at the same time."
        ],
        [
            "OK, so we also need some constraints in order for this problem to work, so the constraint that we add is that you can't.",
            "You can only have one interpretation per part of the scene, so if you have two grid cells which have different orientations and they overlap considerably, then we add a linear constraint that prevents him from turning on both at the same time.",
            "So now that I've introduced the objective in the terms."
        ],
        [
            "We also have to solve the problem and so this is a NP hard problem in general, but we find that we get very good solutions using the gurobi optimizer for our problem."
        ],
        [
            "So now they have introduced the Parameterisation and the formulation for a problem.",
            "I'll go to the experimental results.",
            "So."
        ],
        [
            "So we train and test on the NYU .32 data set, which has become a standard data set for this task, and these scenes contain large amounts of clutter and are very challenging."
        ],
        [
            "So first show some qualitative results of the method in action.",
            "So on the left we have so in."
        ],
        [
            "Image we have a fairly cuboid room and our method is able to infer a basic interpretation of the scene in terms of the walls in the floors.",
            "He"
        ],
        [
            "Here we have a kitchen scene with an upward facing surface on the right and our method is able to pull out the upward facing top of the counter very nicely."
        ],
        [
            "And finally, if you have a scene like this.",
            "We have a very complicated scene and our method is able to pull out the top of the desk as well as both sides of the bookshelves very nicely and is able to infer a good result for this very complicated looking scene."
        ],
        [
            "Our method is also able to produce surface connection graphs in the style of communities origami world.",
            "And the basic idea is that we're able to take are interpreted planar pieces, and we're able to go back and try to decode the edges that must exist between these planner pieces.",
            "So here I'm showing convex and concave edges that have inferred buyer method, and we believe that this interpretation of the scene in terms of large planar pieces in the edges is a much richer and maybe more useful representation than a single continuous estimate of the surface normal at every single pixel."
        ],
        [
            "So we evaluate our method quantitatively as well, and our primary baseline is going to be the 3D printers method that we presented last year at ICC V, and this takes as input a single image and produces an output in terms of every pixel having an orientation.",
            "And so."
        ],
        [
            "We compare against the three printers method 3D P, as well as all the baselines, and we use all the evaluation metrics used by the 3D premise approach, and our method is able to improve upon all the baselines across all the error.",
            "Different error metrics that are used, and this is important because each of these error metrics captures a different aspect of performance, and it's very difficult to come up with one error metric that does that explains everything very nicely, so these results are using the angular error over all the pixels in the scene, and so our method is also able to produce confidence as well, indicating what parts of scene it's confident about and what parts of scene it's not confident about.",
            "And so."
        ],
        [
            "We can produce precision recall like curves in terms of coverage.",
            "The number of pixels predicted and accuracy, how accurately predicted, and so we're able to outperform the baselines at every operating point in terms of the coverage versus precision.",
            "But also importantly, we're able to indicate what parts of the scene were very confident about and provide a large number of really high quality surface normals.",
            "Say below 10 degrees in error.",
            "OK."
        ],
        [
            "So of course our method has failure modes as well, so one big failure mode right now is mistaken, but confident evidence.",
            "So our method is very generic in terms of the inputs it can take, and you can switch out any of the unit reason binaries and replace it with better performing methods or different performing methods that might have complementary error modes, and so in this image we've inferred a concave edge underneath the TV and so our method has folded over and interpretation to agree.",
            "This concave edge and this can be fixed with better inputs for the system.",
            "A dish."
        ],
        [
            "Only our method is also not performing higher level modeling in terms of volume metric primitives as well, and so we believe that we can do this as a first step and then we can use volumetric primitives as a subsequent step and produce much better results and so this is a future direction for work."
        ],
        [
            "OK, so in conclusion, what I've shown you is a way where you can take a single input image and single RGB image and infer a discrete parts of the scene.",
            "And in order to do this, what I've shown is a new parameterisation for indoor Manhattan world scenes and a new formulation which enables you to pull out discrete, parses the scene, and with that I'd like to thank you."
        ],
        [
            "And take any questions that you have.",
            "The question was, does the method involve any occlusion edges and this was of course when the this is a very good question.",
            "This was one of the original line labels.",
            "There is convex, concave and inclusion and in this work we only focus on convex and concave edges because we believe there's a large body of work on inclusion and there has not been as much work on convex and concave edges.",
            "This is a very interesting future direction that we want to do because we believe that integrating all three convex concave and occlusion will provide even better solutions.",
            "But just for this work we focus on convex and concave.",
            "Yeah I have a question.",
            "Your input data is a single image here.",
            "So yeah, yeah, input images, single image.",
            "At some point you mention the algorithm start to determine an edge is convex or concave.",
            "My question.",
            "Isn't this?",
            "Nope.",
            "Possible, or?",
            "I mean, there's ambiguity.",
            "We all know the neck cube ambiguity, so the reason.",
            "So in principle it should be impossible to determine convex and concave and in general, for all situations that might not be possible, but what the method picks up is on the statistical regularity's of human scenes in the sense that, for instance, in the kitchen example, the distinctive cabinets below and the countertop above would tell you that it is convex edge.",
            "In principle could be a concave edge, but just because of the way people construct kitchens or scenes are very boring, there's just so much statistical regularity that we're able to pick up on this, and in practice provide very nice distinctions between convex and concave edges.",
            "Thanks, thank you.",
            "Are there any more questions?",
            "OK, so we'll thank the speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm David Bowie.",
                    "label": 0
                },
                {
                    "sent": "This is unfolding origami world and its joint work with my advisors Audemar Schobert all at Carnegie Mellon Robotics Institute.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the goal of this work is to take a single input image, such as the one I'm showing you here, and to infer a 3D interpretation of the scene, and specifically we're interested in inferring an interpretation in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the large planar pieces of the scene, as well as their orientations.",
                    "label": 0
                },
                {
                    "sent": "So in this interpretation, each color corresponds to different direction or surface normal.",
                    "label": 0
                },
                {
                    "sent": "And so humans are able to do this very easily from any single image, and we'd like to give the same ability to computers.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how can you infer the three properties of a scene from a single image?",
                    "label": 0
                },
                {
                    "sent": "Because of course, as we all know, this is mathematically impossible.",
                    "label": 0
                },
                {
                    "sent": "So the first answer is at the local level.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's enough to tell you things, so if you look at parts of this kitchen scene, this is actually true.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the back of the kitchen and we look at these cabinets, the texture on the cabinets just tells us the orientation right away.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the bottom of the image where the floor meets the cabinets, this is also very, very obvious.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if we look at the walls, or if you look at the top of the counter, we just have two beige blobs and these don't tell us anything about the orientation.",
                    "label": 0
                },
                {
                    "sent": "So what we can see is there are parts of the scene which are easy to infer from local evidence and parts of the scene which are hard to infer from local Evans.",
                    "label": 0
                },
                {
                    "sent": "So naturally what we need to do is you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply constraints on our interpretation and these constraints.",
                    "label": 0
                },
                {
                    "sent": "Let us propagate information from the parts of the scene which are easy to recognize, such as the cabinets in the floor to the parts of the scene that are hard to recognize, such as the wall on the top of this counter.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what constraints do we have at our disposal in single image 3D?",
                    "label": 0
                },
                {
                    "sent": "So in the past 10 years there have been two dominant sources of constraints and the first one is a local smoothness idea and it's a basic low level generic constraint that can be applied in all sorts of different problems, like segmentation for instance.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basic idea is that if you look at two parts of the scene and they have the same color in their nearby, then they should probably have the same label.",
                    "label": 0
                },
                {
                    "sent": "Of course, this constraint is generic and it doesn't take advantage of the nature of the problem, and if we're trying to infer 3D interpretation or interpretation in the end needs to be a valid 3D understanding, and so recognizing this people have turned to physical constraints.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea here is that you use physical properties of the scene to produce constraints on your interpretation, which allows you to get better results.",
                    "label": 0
                },
                {
                    "sent": "These",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trains have tended to be very high level.",
                    "label": 1
                },
                {
                    "sent": "So for instance, there's a Manhattan Rd constraint that's been extremely popular and the idea is that there are three orthogonal directions in the scene, and these are typically estimated with vanishing points.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another very high level constraint has been the idea that rooms in most man made scenes can be approximated as a 3D cuboid, and this has also been very popular, but this doesn't it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things like furniture and so people have also turned to inferring interpretations by fitting more cuboids inside of scenes.",
                    "label": 0
                },
                {
                    "sent": "So what you can?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She is a constraint that we have are either low level and generic or extremely high level and physical.",
                    "label": 1
                },
                {
                    "sent": "So it seems that in order to use physical constraints for our interpretation, we're going to have to go all the way up to other global constraints or fitting volumetric primitives to image data, which is a very hard task.",
                    "label": 0
                },
                {
                    "sent": "So the question we had was, are there other physical constraints which are physical but don't require going all the way up to volumetric primitives?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually if you go back 30 or 40 years, you can find a rich literature on mid level primitives and this is from the line labeling era and these parameters were in terms of planar pieces as well as the edges between them and the junctions at which these edges joined and so of course we can't actually use these constraints directly on any of our problems today because the input assumptions are very different, But we can draw a lot of inspiration from these techniques and the way that they approach problems and the types of outputs they produced and so specifically in this work, the constraint that we're going to try to use.",
                    "label": 0
                },
                {
                    "sent": "Is a fall.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going and it's based on convex and concave edges that we detect in the scene.",
                    "label": 1
                },
                {
                    "sent": "If we want to infer the top of the counter, then here's one way we can figure out that it's facing upwards.",
                    "label": 0
                },
                {
                    "sent": "We first recognize this convex edge, and this is very easy to recognize in this kitchen scene.",
                    "label": 0
                },
                {
                    "sent": "We then also recognize the distinctive front of the cabinets, and this is a vertical surface, and using the fact that we had to have a physical interpretation were able to fold over an interpretation and pull out the top of this counter because we have this convex edge in the front of the cabinets.",
                    "label": 0
                },
                {
                    "sent": "And Additionally.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our output is going to be inspired by these past work and that it's a discrete sine parts.",
                    "label": 0
                },
                {
                    "sent": "It's a parse in terms of large planar pieces of the scene and their orientations as opposed to a continuous estimate of the surface normal at every single pixel, and we think this is a richer interpretation for many applications so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that I've introduced a problem to discuss the rest of the talk, first to introduce a parameterisation that we use, and then I'll discuss the formulation that we used to figure out the interpretation of the scene.",
                    "label": 0
                },
                {
                    "sent": "And finally I'll show some experimental results of our method in action.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So begin with the parameterisation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we again deal with single images.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we do, like many works in this field.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that we estimate three orthogonal vanishing points and we use this using a standard detector.",
                    "label": 0
                },
                {
                    "sent": "Now that we have three orthogonal vanishing points, what we do is we sweep two Rays from one of the vanishing points and then we can sweep to raise from another vanishing point.",
                    "label": 0
                },
                {
                    "sent": "And what this does is it defines a quadrilateral that defines an oriented plane and we can do this with all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pairs of vanishing points and we can get three overlapping grids and these grids are overlapping and they define what we call like.",
                    "label": 0
                },
                {
                    "sent": "Top down super pixels their grid cells.",
                    "label": 0
                },
                {
                    "sent": "They encode not only what pixels are part of them, but also orientation as well.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can take these grids.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can encode surface normals.",
                    "label": 0
                },
                {
                    "sent": "So here we have an interpretation of our algorithm, and each of the colors here indicates a different orientation.",
                    "label": 0
                },
                {
                    "sent": "So Green is facing upwards, for instance, and we can encode this orientation using the grid cells.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we take the green upwards facing surface is and we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the green upward facing grid, which you can see on the left, and we turn.",
                    "label": 0
                },
                {
                    "sent": "We encode the green upward facing surfaces by turning on grid cells and we can do the exact same thing for the purple grid cells which are facing to the left and two for the blue ones which are facing towards the right.",
                    "label": 0
                },
                {
                    "sent": "And what this means is that we can encode.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The orientation of the surface is in the scene with a binary indicator vector that goes over all the grid cells and we can optimize over this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We note that our parameterization isn't is inspired by an.",
                    "label": 0
                },
                {
                    "sent": "It's an extension of many of the past works in single image 3D which are based on bounding Rays.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now they have introduced the basic parameterisation.",
                    "label": 0
                },
                {
                    "sent": "We're going to use.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain the formulation that we use to figure out an interpretation of the scene.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So recall that our indicator variable that we're optimizing over is a binary indicator vector which says which grid cells are on.",
                    "label": 0
                },
                {
                    "sent": "And so this is a binary indicator vector that goes over all the grid.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We formulate the problem as a binary quadratic program subject to some basic linear constraints and to go over each of these terms in turn.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have unary terms and.",
                    "label": 0
                },
                {
                    "sent": "These indicate whether a grid cell should be on or off, and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basic idea is that you have any form of 3D evidence that you can pull out of it 2D image and you can check whether it agrees with the grid cell.",
                    "label": 0
                },
                {
                    "sent": "So for instance, we think that the bottom of this image should be facing upwards, so we would reward the green upwards facing grid cells for turning on.",
                    "label": 0
                },
                {
                    "sent": "And we would pay NYS the blue vertical surfaces for turning on at the bottom of the image as well.",
                    "label": 0
                },
                {
                    "sent": "And we can use any form of 3D evidence and in this work we use 2.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forms the 1st is a local way of getting surface normals, and it's from data driven 3D primitives which we presented at last I see CV which has appeared detector and surface normal configuration bank and enables you to transfer its surface normals wherever your detectors have a high response and it produces an output something like this.",
                    "label": 0
                },
                {
                    "sent": "We also use a complementary source of 3D evidence.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is based on the standard cuboid fitting method.",
                    "label": 0
                },
                {
                    "sent": "Had all and we use this to also produce a unary.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have unary's, we can then include binary terms and these determine whether two grid cells should be rewarded for turning on at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the binary terms that we introduce are based on convex and concave constraints, and the idea is that you can enumerate the configurations that are possible in terms of planar surfaces and their orientations as well as convex and concave edges in a scene.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in this kitchen scene you have the upward facing counter, you have a convex edge beneath it, and you have the vertical surface beneath it as well.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we detect convex and concave edges in an image and they use to produce constraints.",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing detected concave edges.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have a piece of the scene, for instance this blue rightwards facing piece, we can determine what sorts of things we should reward for turning on at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, if we turn on this purple surface, it forms a concave edge, and so it agrees with the detected concave edges.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We produce a flat interpretation.",
                    "label": 0
                },
                {
                    "sent": "This is not an agreement with the detected concave edge, and so we do not reward it for being turned on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Furthermore, if we, if we form a convex edge, this does not agree with the concave edge, and so we do not reward it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in order to take convex and concave edges, you can use any method that you want, and our approach is generic, can handle any sort of method, and for simplicity we just use a 3D primitives and we simply swap out the surface normal configurations with convex and concave edges and proceed as usual, and this produces an output such as this in terms of convex edges.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also Additionally use a local generic smoothness term and this just says that if two grid cells have this or have the same orientation and they have the same appearance, then they should both be on.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the top of the bed, they both are Leopard print, so they should both be on at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we also need some constraints in order for this problem to work, so the constraint that we add is that you can't.",
                    "label": 0
                },
                {
                    "sent": "You can only have one interpretation per part of the scene, so if you have two grid cells which have different orientations and they overlap considerably, then we add a linear constraint that prevents him from turning on both at the same time.",
                    "label": 0
                },
                {
                    "sent": "So now that I've introduced the objective in the terms.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have to solve the problem and so this is a NP hard problem in general, but we find that we get very good solutions using the gurobi optimizer for our problem.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now they have introduced the Parameterisation and the formulation for a problem.",
                    "label": 0
                },
                {
                    "sent": "I'll go to the experimental results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we train and test on the NYU .32 data set, which has become a standard data set for this task, and these scenes contain large amounts of clutter and are very challenging.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first show some qualitative results of the method in action.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have so in.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image we have a fairly cuboid room and our method is able to infer a basic interpretation of the scene in terms of the walls in the floors.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have a kitchen scene with an upward facing surface on the right and our method is able to pull out the upward facing top of the counter very nicely.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, if you have a scene like this.",
                    "label": 0
                },
                {
                    "sent": "We have a very complicated scene and our method is able to pull out the top of the desk as well as both sides of the bookshelves very nicely and is able to infer a good result for this very complicated looking scene.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our method is also able to produce surface connection graphs in the style of communities origami world.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that we're able to take are interpreted planar pieces, and we're able to go back and try to decode the edges that must exist between these planner pieces.",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing convex and concave edges that have inferred buyer method, and we believe that this interpretation of the scene in terms of large planar pieces in the edges is a much richer and maybe more useful representation than a single continuous estimate of the surface normal at every single pixel.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we evaluate our method quantitatively as well, and our primary baseline is going to be the 3D printers method that we presented last year at ICC V, and this takes as input a single image and produces an output in terms of every pixel having an orientation.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compare against the three printers method 3D P, as well as all the baselines, and we use all the evaluation metrics used by the 3D premise approach, and our method is able to improve upon all the baselines across all the error.",
                    "label": 0
                },
                {
                    "sent": "Different error metrics that are used, and this is important because each of these error metrics captures a different aspect of performance, and it's very difficult to come up with one error metric that does that explains everything very nicely, so these results are using the angular error over all the pixels in the scene, and so our method is also able to produce confidence as well, indicating what parts of scene it's confident about and what parts of scene it's not confident about.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can produce precision recall like curves in terms of coverage.",
                    "label": 0
                },
                {
                    "sent": "The number of pixels predicted and accuracy, how accurately predicted, and so we're able to outperform the baselines at every operating point in terms of the coverage versus precision.",
                    "label": 0
                },
                {
                    "sent": "But also importantly, we're able to indicate what parts of the scene were very confident about and provide a large number of really high quality surface normals.",
                    "label": 0
                },
                {
                    "sent": "Say below 10 degrees in error.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course our method has failure modes as well, so one big failure mode right now is mistaken, but confident evidence.",
                    "label": 0
                },
                {
                    "sent": "So our method is very generic in terms of the inputs it can take, and you can switch out any of the unit reason binaries and replace it with better performing methods or different performing methods that might have complementary error modes, and so in this image we've inferred a concave edge underneath the TV and so our method has folded over and interpretation to agree.",
                    "label": 0
                },
                {
                    "sent": "This concave edge and this can be fixed with better inputs for the system.",
                    "label": 0
                },
                {
                    "sent": "A dish.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only our method is also not performing higher level modeling in terms of volume metric primitives as well, and so we believe that we can do this as a first step and then we can use volumetric primitives as a subsequent step and produce much better results and so this is a future direction for work.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion, what I've shown you is a way where you can take a single input image and single RGB image and infer a discrete parts of the scene.",
                    "label": 0
                },
                {
                    "sent": "And in order to do this, what I've shown is a new parameterisation for indoor Manhattan world scenes and a new formulation which enables you to pull out discrete, parses the scene, and with that I'd like to thank you.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And take any questions that you have.",
                    "label": 0
                },
                {
                    "sent": "The question was, does the method involve any occlusion edges and this was of course when the this is a very good question.",
                    "label": 0
                },
                {
                    "sent": "This was one of the original line labels.",
                    "label": 0
                },
                {
                    "sent": "There is convex, concave and inclusion and in this work we only focus on convex and concave edges because we believe there's a large body of work on inclusion and there has not been as much work on convex and concave edges.",
                    "label": 0
                },
                {
                    "sent": "This is a very interesting future direction that we want to do because we believe that integrating all three convex concave and occlusion will provide even better solutions.",
                    "label": 0
                },
                {
                    "sent": "But just for this work we focus on convex and concave.",
                    "label": 0
                },
                {
                    "sent": "Yeah I have a question.",
                    "label": 0
                },
                {
                    "sent": "Your input data is a single image here.",
                    "label": 0
                },
                {
                    "sent": "So yeah, yeah, input images, single image.",
                    "label": 1
                },
                {
                    "sent": "At some point you mention the algorithm start to determine an edge is convex or concave.",
                    "label": 0
                },
                {
                    "sent": "My question.",
                    "label": 0
                },
                {
                    "sent": "Isn't this?",
                    "label": 0
                },
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "Possible, or?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's ambiguity.",
                    "label": 0
                },
                {
                    "sent": "We all know the neck cube ambiguity, so the reason.",
                    "label": 0
                },
                {
                    "sent": "So in principle it should be impossible to determine convex and concave and in general, for all situations that might not be possible, but what the method picks up is on the statistical regularity's of human scenes in the sense that, for instance, in the kitchen example, the distinctive cabinets below and the countertop above would tell you that it is convex edge.",
                    "label": 0
                },
                {
                    "sent": "In principle could be a concave edge, but just because of the way people construct kitchens or scenes are very boring, there's just so much statistical regularity that we're able to pick up on this, and in practice provide very nice distinctions between convex and concave edges.",
                    "label": 0
                },
                {
                    "sent": "Thanks, thank you.",
                    "label": 0
                },
                {
                    "sent": "Are there any more questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll thank the speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}