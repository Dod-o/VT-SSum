{
    "id": "qy66zyj3lyegf36qygabh5u3e5x46lrp",
    "title": "Approximating Concavely Parameterized Optimization Problems",
    "info": {
        "author": [
            "S\u00f6ren Laue, Fakult\u00e4t f\u00fcr Mathematik und Informatik, Friedrich Schiller University of Jena"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Optimization Methods->Convex Optimization",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_laue_optimization_problems/",
    "segmentation": [
        [
            "Welcome to my talk this.",
            "Joint work with Yahoo Giesen.",
            "Jens Mueller, who is also here and social sphere C and we are from the theory Department of the University of Vienna.",
            "And before I start my talk, I'd like to answer a question that I always get in my colleagues as well."
        ],
        [
            "That is, where is Jenna?",
            "And."
        ],
        [
            "Yeah, it's right here.",
            "It's basically in the middle of Germany, halfway between Munich and Berlin.",
            "So now you know the."
        ],
        [
            "OK, now coming back to my talk.",
            "Well, this talk is about general regularization path that was hard to deduct from the title, so that's why I put it here and basically I'll be talking about the combinatorial complexity of generalization path regularization path and also about algorithms.",
            "How to compute them."
        ],
        [
            "And the theme we were following in this paper was basically generalize and simplify.",
            "Generalize in the sense that we tried to.",
            "Basically I create a very general framework for regularization, path and simplify in the sense that we've worked really hard to get the algorithm very simple.",
            "So.",
            "Most well, actually many.",
            "Machine learning problems can be cast as a convex optimization problem, often or usually you have.",
            "So usually you have a convex nonnegative loss function and a convex nonnegative regularizer, and what you want is you want to.",
            "Basically, minimize them at the same time, since this is not possible because they contradict each other, you are trying to kind of balance between them.",
            "You do this by introducing.",
            "Yeah, just some fact or some regularization parameter.",
            "So that's one view.",
            "The other form is what you want, which is basically equivalent.",
            "You are trying to minimize the loss function and you're just pounding your complexity of your model class by some parameter T. So basically both formulations are equivalent.",
            "And I'll be presenting regularization paths for both of them in this talk.",
            "Now, as a running example for the first for the constraint formulation, I used the standard dual SVM and for the additive formulation I'll use the primal SVM.",
            "Now I have to add that actually this formulation here that is previous work that we have published in some theory conference awhile ago, but for the whole picture I will include this here in this talk.",
            "So you get a coherent view on this.",
            "OK, so the most interesting question or that everybody faces."
        ],
        [
            "Actually, how do you find the best regularization parameter?",
            "In these problems.",
            "And there's a few methods you can do, and one thing if you can look at the whole regular."
        ],
        [
            "Path.",
            "And so you can compute, for instance, the exact regularization path and it's by now a classic result that can be piecewise linear.",
            "So for SVM's and in general for arbitrary quadratic programs it's piecewise linear.",
            "Unfortunately, on the downside, the complexity can be exponential.",
            "That's a rather new result, which.",
            "Yeah, basically the proof idea follows the same ideas, showing that simplex method can have exponential running time.",
            "And some of these methods actually can also be numerically instable, and some of these methods you have to invert the matrix, and often if that's kernel matrix, that is, if you have a lot of data points, this matrix is singular and the whole thing breaks down.",
            "So what do you do in practice?",
            "You can just resort to grid search.",
            "Or what you can also do if you resort to approximate regularization path."
        ],
        [
            "So instead of computing the exact one, you just computed approximately.",
            "And that's what we're doing and will just have a bit of a more abstract view on it.",
            "So what we're given is a parameterized optimization problem, which is parameterized in T, and you can define function F of T by the pointwise minimum of this optimization problem, and then F of T is called the regularization parameter path, sorry.",
            "Now what do we do?",
            "Ingrids"
        ],
        [
            "Search you start with.",
            "Some parameter T and you solve you optimization problem and then you."
        ],
        [
            "Take the next."
        ],
        [
            "Parameter T and again yourself you optimization problem."
        ],
        [
            "Well, anyway."
        ],
        [
            "Continue doing this.",
            "Straight forward."
        ],
        [
            "Well, the downside on this is it's not adapt."
        ],
        [
            "Active.",
            "In the sense that you basically treat the interesting regions and the non interesting regions the same like in this function here in the middle there's really nothing happening.",
            "Your function doesn't change, but you waste a lot of computation time, whereas in the area where your function changes rapidly you basically you might miss interesting points.",
            "So what can you do?",
            "You can just.",
            "Change this and."
        ],
        [
            "Have some adaptive stepsize.",
            "And the idea is basically instead of putting a grid on the X axis, it just put a grid on the Y axis."
        ],
        [
            "And that is."
        ],
        [
            "You start with your first parameter."
        ],
        [
            "And then you check.",
            "Um?",
            "When is your function changing by some pre given amount of?",
            "Yeah by some pre given amount."
        ],
        [
            "And this will be your next parameter."
        ],
        [
            "And you continue doing this."
        ],
        [
            "All."
        ],
        [
            "Way until the end."
        ],
        [
            "And."
        ],
        [
            "Here you see that basically in the interest in the interesting regions you spent more.",
            "Amount and in the regions where there is really nothing going on, you spend less computation.",
            "Now that's a very simple idea, and actually it's also not new, so if you remember from your calculus class, it's actually the same."
        ],
        [
            "Idea that aurela back used for defining his low back integral.",
            "But more than 100 years old.",
            "But still very powerful so.",
            "'cause I mean now these days it's actually much easier to work with a big integral then with the Riemann integral.",
            "So in this same idea, we're just going to use for.",
            "For regularization path.",
            "Now, why is that good?"
        ],
        [
            "That is, if you check every epsilon step when your function changes.",
            "You actually can cover the whole path."
        ],
        [
            "And you get the guarantee that you get an absolute guarantee, so you're guaranteed not to miss the interesting part of your regularization path.",
            "And also when you just look at the picture, it also comes with a bound.",
            "You immediately see that you."
        ],
        [
            "Read over of one over Epsilon.",
            "Many constant solutions now that's the upper bound as well.",
            "Also the lower bound."
        ],
        [
            "Easy to see now this idea would just apply.",
            "To the constraint version of.",
            "Yeah, off the machine."
        ],
        [
            "Problems.",
            "So as I said, I use the dual SVM as the running example and F of T so directly rotation path in this case and actually in all these cases will be a concave function that is monotone increasing."
        ],
        [
            "So what do you do?",
            "You pick your first."
        ],
        [
            "Parameter T. You saw a few optimization problem."
        ],
        [
            "And then you just compute linear approximation to your function F of TLF of T. We cannot really access, but the linear approximation."
        ],
        [
            "Can always compute.",
            "Now you check when has this linear approximation changed more by and by."
        ],
        [
            "Silent well and this is your new parameter T."
        ],
        [
            "And at this new parameter you solve the optimization problem again and that's it.",
            "And now."
        ],
        [
            "Just continue."
        ],
        [
            "Find the linear approximation."
        ],
        [
            "Check when."
        ],
        [
            "Change this more than."
        ],
        [
            "Style on."
        ],
        [
            "That's your new."
        ],
        [
            "Parameter T."
        ],
        [
            "You solve it."
        ],
        [
            "And you can't."
        ],
        [
            "Any until."
        ],
        [
            "Actually, you found the end.",
            "Now it's horizontal, that's it.",
            "And that's everything.",
            "Well and this I."
        ],
        [
            "Idea you can apply to any optimization problem that has this form and you can cover the whole path with an absolute guarantee.",
            "And you need one or epsilon many constant solutions.",
            "Yeah, and as I said, this works for any convex machine learning problem.",
            "So any kernelized SVM, SSDP from matrix factorization, whatever fits in this framework.",
            "OK, now let's apply the same."
        ],
        [
            "Here to the second formulation, so the additive form.",
            "Anne."
        ],
        [
            "Here I just used the primal SVM formulation as the running example.",
            "And here."
        ],
        [
            "Again, F of T is a concave increasing function, monotonically increasing function."
        ],
        [
            "OK, so we start with the parameter T1.",
            "We find the optimum of the of this optimization problem W. And now something different happens.",
            "If you have a fixed solution W and you increase T."
        ],
        [
            "It's nothing else.",
            "But the linear function now.",
            "So if you increase T, you'll actually get an upper bound on your regularization path.",
            "Now what you can also compute is."
        ],
        [
            "A lower bound to your regular to your optimization problem and this.",
            "Will always be a quadratic function.",
            "Now, in this case you can just get it."
        ],
        [
            "And the dual, the dual of SVM is a quadratic function.",
            "There you get it and now you just check when does the upper bound and lower bound deviate more than epsilon.",
            "Once it deviates more than asylum.",
            "That's here now."
        ],
        [
            "Parameter key too.",
            "Now for the whole interval, the solution you found for T1 will actually."
        ],
        [
            "Stay a valid solution.",
            "Up to a small epsilon.",
            "Now, since this is a linear function, that's a quadratic function.",
            "The."
        ],
        [
            "The really nice and surprising thing is the distance you're traveling is square root of silent.",
            "So you make much more further progress than before.",
            "Well, OK, OK. And then you're."
        ],
        [
            "Just continued."
        ],
        [
            "In the same process."
        ],
        [
            "And."
        ],
        [
            "Right now, since."
        ],
        [
            "The upper.",
            "Yeah, since the upper bound in the lower bound."
        ],
        [
            "Match you're done basically, so that's the whole algorithm.",
            "Then again, you can use it for.",
            "For any machine learning problem this form.",
            "But now you only need one over square root of silent many solutions, and again you can show that's an upper bound and also a lower bound.",
            "And again, it works for a lot of problems."
        ],
        [
            "Now here's the algorithm and I'll just modify it tiny bit from what I just said.",
            "You start with your minimum.",
            "Parameter and instead of computing the exact solution, it suffices to compute an approximate solution, and that's what you always do in machine learning.",
            "You always compute some approximate solution, so now you can just use your favorite solver and plug it in there, and that's it.",
            "You just need to compute an epsilon over 2 approximation.",
            "And then you just check while you can compute the next TI.",
            "And you repeat and that's all."
        ],
        [
            "And this algorithm.",
            "Now I mean you can apply this now to various things.",
            "We applied it for instance to some."
        ],
        [
            "For vector machine classification, once you have the regularization path you can do cross validation."
        ],
        [
            "In the past you get nice pictures.",
            "You can do the same for nuclear norm regulars.",
            "Matrix factorization you get even nicer pictures, so you really get the things that you always say in theory.",
            "And all you need is basically a solver for solving the optimization problem an for nuclear norm regularization matrix completion.",
            "You can just formulate this as a SDP, and there's really fast and nice algorithm in this paper for solving generalized EPS."
        ],
        [
            "OK, and that was so far for the theory and in practice you actually really see the one over square root of silent dependency on real datasets.",
            "Discuss now SVM classification with kernels.",
            "So it really this is the one over square root epsilon and the other searches some datasets.",
            "So theory and practice somehow really go hand in hand here, which is really nice.",
            "OK, so."
        ],
        [
            "Let me sum up.",
            "A presented general regularization path method that really works for a lot of optimization problems.",
            "an A lot of machine learning problems and the path is actually piecewise constant, but you but it comes with a guarantee, so it's really general and also very simple at the same time.",
            "And you get a one over epsilon complexity for the constraint version and the really surprising and very nice result is you get a complexity of one over squared epsilon for the editor form.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome to my talk this.",
                    "label": 0
                },
                {
                    "sent": "Joint work with Yahoo Giesen.",
                    "label": 0
                },
                {
                    "sent": "Jens Mueller, who is also here and social sphere C and we are from the theory Department of the University of Vienna.",
                    "label": 1
                },
                {
                    "sent": "And before I start my talk, I'd like to answer a question that I always get in my colleagues as well.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, where is Jenna?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, it's right here.",
                    "label": 0
                },
                {
                    "sent": "It's basically in the middle of Germany, halfway between Munich and Berlin.",
                    "label": 0
                },
                {
                    "sent": "So now you know the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now coming back to my talk.",
                    "label": 0
                },
                {
                    "sent": "Well, this talk is about general regularization path that was hard to deduct from the title, so that's why I put it here and basically I'll be talking about the combinatorial complexity of generalization path regularization path and also about algorithms.",
                    "label": 1
                },
                {
                    "sent": "How to compute them.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the theme we were following in this paper was basically generalize and simplify.",
                    "label": 0
                },
                {
                    "sent": "Generalize in the sense that we tried to.",
                    "label": 0
                },
                {
                    "sent": "Basically I create a very general framework for regularization, path and simplify in the sense that we've worked really hard to get the algorithm very simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Most well, actually many.",
                    "label": 0
                },
                {
                    "sent": "Machine learning problems can be cast as a convex optimization problem, often or usually you have.",
                    "label": 0
                },
                {
                    "sent": "So usually you have a convex nonnegative loss function and a convex nonnegative regularizer, and what you want is you want to.",
                    "label": 0
                },
                {
                    "sent": "Basically, minimize them at the same time, since this is not possible because they contradict each other, you are trying to kind of balance between them.",
                    "label": 0
                },
                {
                    "sent": "You do this by introducing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just some fact or some regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "So that's one view.",
                    "label": 0
                },
                {
                    "sent": "The other form is what you want, which is basically equivalent.",
                    "label": 0
                },
                {
                    "sent": "You are trying to minimize the loss function and you're just pounding your complexity of your model class by some parameter T. So basically both formulations are equivalent.",
                    "label": 0
                },
                {
                    "sent": "And I'll be presenting regularization paths for both of them in this talk.",
                    "label": 0
                },
                {
                    "sent": "Now, as a running example for the first for the constraint formulation, I used the standard dual SVM and for the additive formulation I'll use the primal SVM.",
                    "label": 1
                },
                {
                    "sent": "Now I have to add that actually this formulation here that is previous work that we have published in some theory conference awhile ago, but for the whole picture I will include this here in this talk.",
                    "label": 0
                },
                {
                    "sent": "So you get a coherent view on this.",
                    "label": 0
                },
                {
                    "sent": "OK, so the most interesting question or that everybody faces.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, how do you find the best regularization parameter?",
                    "label": 0
                },
                {
                    "sent": "In these problems.",
                    "label": 0
                },
                {
                    "sent": "And there's a few methods you can do, and one thing if you can look at the whole regular.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Path.",
                    "label": 0
                },
                {
                    "sent": "And so you can compute, for instance, the exact regularization path and it's by now a classic result that can be piecewise linear.",
                    "label": 1
                },
                {
                    "sent": "So for SVM's and in general for arbitrary quadratic programs it's piecewise linear.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, on the downside, the complexity can be exponential.",
                    "label": 0
                },
                {
                    "sent": "That's a rather new result, which.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically the proof idea follows the same ideas, showing that simplex method can have exponential running time.",
                    "label": 0
                },
                {
                    "sent": "And some of these methods actually can also be numerically instable, and some of these methods you have to invert the matrix, and often if that's kernel matrix, that is, if you have a lot of data points, this matrix is singular and the whole thing breaks down.",
                    "label": 0
                },
                {
                    "sent": "So what do you do in practice?",
                    "label": 0
                },
                {
                    "sent": "You can just resort to grid search.",
                    "label": 0
                },
                {
                    "sent": "Or what you can also do if you resort to approximate regularization path.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of computing the exact one, you just computed approximately.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're doing and will just have a bit of a more abstract view on it.",
                    "label": 0
                },
                {
                    "sent": "So what we're given is a parameterized optimization problem, which is parameterized in T, and you can define function F of T by the pointwise minimum of this optimization problem, and then F of T is called the regularization parameter path, sorry.",
                    "label": 0
                },
                {
                    "sent": "Now what do we do?",
                    "label": 0
                },
                {
                    "sent": "Ingrids",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search you start with.",
                    "label": 0
                },
                {
                    "sent": "Some parameter T and you solve you optimization problem and then you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the next.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter T and again yourself you optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, anyway.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue doing this.",
                    "label": 0
                },
                {
                    "sent": "Straight forward.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the downside on this is it's not adapt.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active.",
                    "label": 0
                },
                {
                    "sent": "In the sense that you basically treat the interesting regions and the non interesting regions the same like in this function here in the middle there's really nothing happening.",
                    "label": 1
                },
                {
                    "sent": "Your function doesn't change, but you waste a lot of computation time, whereas in the area where your function changes rapidly you basically you might miss interesting points.",
                    "label": 0
                },
                {
                    "sent": "So what can you do?",
                    "label": 0
                },
                {
                    "sent": "You can just.",
                    "label": 0
                },
                {
                    "sent": "Change this and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have some adaptive stepsize.",
                    "label": 0
                },
                {
                    "sent": "And the idea is basically instead of putting a grid on the X axis, it just put a grid on the Y axis.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You start with your first parameter.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you check.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "When is your function changing by some pre given amount of?",
                    "label": 0
                },
                {
                    "sent": "Yeah by some pre given amount.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this will be your next parameter.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you continue doing this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way until the end.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you see that basically in the interest in the interesting regions you spent more.",
                    "label": 0
                },
                {
                    "sent": "Amount and in the regions where there is really nothing going on, you spend less computation.",
                    "label": 0
                },
                {
                    "sent": "Now that's a very simple idea, and actually it's also not new, so if you remember from your calculus class, it's actually the same.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea that aurela back used for defining his low back integral.",
                    "label": 0
                },
                {
                    "sent": "But more than 100 years old.",
                    "label": 0
                },
                {
                    "sent": "But still very powerful so.",
                    "label": 0
                },
                {
                    "sent": "'cause I mean now these days it's actually much easier to work with a big integral then with the Riemann integral.",
                    "label": 0
                },
                {
                    "sent": "So in this same idea, we're just going to use for.",
                    "label": 0
                },
                {
                    "sent": "For regularization path.",
                    "label": 0
                },
                {
                    "sent": "Now, why is that good?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, if you check every epsilon step when your function changes.",
                    "label": 0
                },
                {
                    "sent": "You actually can cover the whole path.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get the guarantee that you get an absolute guarantee, so you're guaranteed not to miss the interesting part of your regularization path.",
                    "label": 0
                },
                {
                    "sent": "And also when you just look at the picture, it also comes with a bound.",
                    "label": 0
                },
                {
                    "sent": "You immediately see that you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Read over of one over Epsilon.",
                    "label": 0
                },
                {
                    "sent": "Many constant solutions now that's the upper bound as well.",
                    "label": 0
                },
                {
                    "sent": "Also the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy to see now this idea would just apply.",
                    "label": 0
                },
                {
                    "sent": "To the constraint version of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, off the machine.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "So as I said, I use the dual SVM as the running example and F of T so directly rotation path in this case and actually in all these cases will be a concave function that is monotone increasing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do you do?",
                    "label": 0
                },
                {
                    "sent": "You pick your first.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter T. You saw a few optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you just compute linear approximation to your function F of TLF of T. We cannot really access, but the linear approximation.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can always compute.",
                    "label": 0
                },
                {
                    "sent": "Now you check when has this linear approximation changed more by and by.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Silent well and this is your new parameter T.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at this new parameter you solve the optimization problem again and that's it.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just continue.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the linear approximation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check when.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change this more than.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Style on.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's your new.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter T.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You solve it.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can't.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any until.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, you found the end.",
                    "label": 0
                },
                {
                    "sent": "Now it's horizontal, that's it.",
                    "label": 0
                },
                {
                    "sent": "And that's everything.",
                    "label": 0
                },
                {
                    "sent": "Well and this I.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea you can apply to any optimization problem that has this form and you can cover the whole path with an absolute guarantee.",
                    "label": 0
                },
                {
                    "sent": "And you need one or epsilon many constant solutions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and as I said, this works for any convex machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "So any kernelized SVM, SSDP from matrix factorization, whatever fits in this framework.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's apply the same.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here to the second formulation, so the additive form.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I just used the primal SVM formulation as the running example.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, F of T is a concave increasing function, monotonically increasing function.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we start with the parameter T1.",
                    "label": 0
                },
                {
                    "sent": "We find the optimum of the of this optimization problem W. And now something different happens.",
                    "label": 0
                },
                {
                    "sent": "If you have a fixed solution W and you increase T.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's nothing else.",
                    "label": 0
                },
                {
                    "sent": "But the linear function now.",
                    "label": 0
                },
                {
                    "sent": "So if you increase T, you'll actually get an upper bound on your regularization path.",
                    "label": 0
                },
                {
                    "sent": "Now what you can also compute is.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lower bound to your regular to your optimization problem and this.",
                    "label": 0
                },
                {
                    "sent": "Will always be a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "Now, in this case you can just get it.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the dual, the dual of SVM is a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "There you get it and now you just check when does the upper bound and lower bound deviate more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "Once it deviates more than asylum.",
                    "label": 0
                },
                {
                    "sent": "That's here now.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter key too.",
                    "label": 0
                },
                {
                    "sent": "Now for the whole interval, the solution you found for T1 will actually.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stay a valid solution.",
                    "label": 0
                },
                {
                    "sent": "Up to a small epsilon.",
                    "label": 0
                },
                {
                    "sent": "Now, since this is a linear function, that's a quadratic function.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The really nice and surprising thing is the distance you're traveling is square root of silent.",
                    "label": 0
                },
                {
                    "sent": "So you make much more further progress than before.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, OK. And then you're.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just continued.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the same process.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now, since.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The upper.",
                    "label": 0
                },
                {
                    "sent": "Yeah, since the upper bound in the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Match you're done basically, so that's the whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then again, you can use it for.",
                    "label": 0
                },
                {
                    "sent": "For any machine learning problem this form.",
                    "label": 0
                },
                {
                    "sent": "But now you only need one over square root of silent many solutions, and again you can show that's an upper bound and also a lower bound.",
                    "label": 0
                },
                {
                    "sent": "And again, it works for a lot of problems.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here's the algorithm and I'll just modify it tiny bit from what I just said.",
                    "label": 0
                },
                {
                    "sent": "You start with your minimum.",
                    "label": 0
                },
                {
                    "sent": "Parameter and instead of computing the exact solution, it suffices to compute an approximate solution, and that's what you always do in machine learning.",
                    "label": 0
                },
                {
                    "sent": "You always compute some approximate solution, so now you can just use your favorite solver and plug it in there, and that's it.",
                    "label": 0
                },
                {
                    "sent": "You just need to compute an epsilon over 2 approximation.",
                    "label": 0
                },
                {
                    "sent": "And then you just check while you can compute the next TI.",
                    "label": 0
                },
                {
                    "sent": "And you repeat and that's all.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now I mean you can apply this now to various things.",
                    "label": 0
                },
                {
                    "sent": "We applied it for instance to some.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For vector machine classification, once you have the regularization path you can do cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the past you get nice pictures.",
                    "label": 0
                },
                {
                    "sent": "You can do the same for nuclear norm regulars.",
                    "label": 0
                },
                {
                    "sent": "Matrix factorization you get even nicer pictures, so you really get the things that you always say in theory.",
                    "label": 0
                },
                {
                    "sent": "And all you need is basically a solver for solving the optimization problem an for nuclear norm regularization matrix completion.",
                    "label": 0
                },
                {
                    "sent": "You can just formulate this as a SDP, and there's really fast and nice algorithm in this paper for solving generalized EPS.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and that was so far for the theory and in practice you actually really see the one over square root of silent dependency on real datasets.",
                    "label": 0
                },
                {
                    "sent": "Discuss now SVM classification with kernels.",
                    "label": 0
                },
                {
                    "sent": "So it really this is the one over square root epsilon and the other searches some datasets.",
                    "label": 0
                },
                {
                    "sent": "So theory and practice somehow really go hand in hand here, which is really nice.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me sum up.",
                    "label": 0
                },
                {
                    "sent": "A presented general regularization path method that really works for a lot of optimization problems.",
                    "label": 0
                },
                {
                    "sent": "an A lot of machine learning problems and the path is actually piecewise constant, but you but it comes with a guarantee, so it's really general and also very simple at the same time.",
                    "label": 0
                },
                {
                    "sent": "And you get a one over epsilon complexity for the constraint version and the really surprising and very nice result is you get a complexity of one over squared epsilon for the editor form.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}