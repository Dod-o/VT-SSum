{
    "id": "aalb5wyqgzwmwpnweptzxnnojbxmppoy",
    "title": "Explorations in Computer Go, Web Search, and Online Advertising",
    "info": {
        "author": [
            "Thore Graepel, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "July 25, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning->Exploration vs. Exploitation"
        ]
    },
    "url": "http://videolectures.net/explorationexploitation2011_graepel_explorations/",
    "segmentation": [
        [
            "Thank you very much for the invitation to talk here.",
            "I was actually not sure if I wanted to accept this at first because I thought, OK, I haven't actually worked explicitly on the problem of exploration versus exploitation, or was thinking do I really have something to contribute?",
            "And then I thought back a little bit over my time over the last 10 years.",
            "What I had done and I had encountered the problem of exploration versus exploitation so often in different contexts that I thought maybe it would be best if I just give you a little guided tour of these.",
            "Practical aspects of exploration versus exploitation, and this is work with tracking kiniro candela David Stern.",
            "Refer break Michael Condo for Leon Bottou and and a lot of other colleagues who were involved and it's going to be mostly explorations, so don't expect any real deep theoretical insights.",
            "I also view myself a little bit as the opener of the workshop the Entertainer and because I'm the first one.",
            "I think I can also have the freedom of asking more questions than deliverying answers.",
            "Um?"
        ],
        [
            "So here's the overview.",
            "I'll give a short introduction, motivation, and then I have these five vignettes from my past work that I'll discuss.",
            "I'll talk about fighting, I'll talk about racing, talk about computer, go, and I'll talk about web search and online advertising as well.",
            "So, and I hope we'll have little lesson to learn from each one of these."
        ],
        [
            "So in terms of introduction."
        ],
        [
            "I don't know who of you is familiar with this experiment.",
            "The so called marshmallow experiment.",
            "This experiment is about maximizing long-term reward and about the difficulty this problem holds for people, and it is the study involved people be, or in particular children being given a marshmallow.",
            "And then being put into room being given a marshmallow.",
            "And then they were told that if they could wait for some unknown period of time, someone would come back and give them another marshmallow and then they would have two that they would be able to eat.",
            "If they however decided to immediately eat the marshmallow, there would be no second marshmallow right?",
            "And let me just show you a little video where we see how these folks reacted and.",
            "Alright, here's the deal.",
            "Marshmallow, for you.",
            "You can either wait and I'll give you another one.",
            "If you wait.",
            "Or you can eat it now when I come back, I'll give you another one.",
            "So then you have to, but stay in here and stay in the chair till I come back.",
            "OK alright?",
            "I'm gonna go do something and then I'll come back.",
            "Really good.",
            "Sampling.",
            "You can have it now, or you can wait.",
            "I'll be back, stay in the chair, OK?",
            "OK.",
            "So hard, isn't it?",
            "Alright, so I'm going to leave and then I'll come back.",
            "OK, so you can either eat it right now or you can wait.",
            "Either way.",
            "OK, OK?",
            "How do you do?",
            "Did you do good?",
            "You did you want to eat it and you, yeah, so they tell you to give you another one.",
            "You can have both.",
            "You need to.",
            "OK, so.",
            "I think we have established how hard this is.",
            "And what I found interesting about these studies is that when you look, this was the longitudinal study.",
            "So these these subjects were followed through the years, so to speak.",
            "And then there was just kind of follow up 30 years later and they looked at the SAT scores and of drug usage of these people they had carried out these tests with and it was found that people who were able to delay the gratification had on some of these scores like.",
            "I had on average higher SAT scores, lower drug abuse tendencies, and so on.",
            "So I think it's not exactly the topic that we're talking about, but it comes close because exploration is all about sacrificing something for the moment in order to gain a long-term advantage.",
            "So that was just."
        ],
        [
            "To set the scene and now just to make sure that we're all on the same page.",
            "Roughly the problem of exploration exploitation is that if there's a given agent in an unknown environment, then the agent faces a dilemma.",
            "They can either exploit the current knowledge and optimize for the short term reward, or they can explore the environment to discover opportunities for long-term reward.",
            "And here's just a little sketch if we think of this landscape as a utility landscape, so to speak, and the agent at any point in time gets utility proportional to the height where is then.",
            "I can if there's too little exploration, the agent will be locali stuck maybe on this little Hill here and will never get really good pay offs.",
            "If the agent explores too much, then he will discover the high payoff areas, but he'll spend too little time in them and hence overall not together or the free Ward.",
            "And if we find the sweet spot or if the agent finds this sweet spot, then you'll spend most of the time in these higher reward areas and that's that's the ideal scenario.",
            "So."
        ],
        [
            "That's the basic setup and now I want to dive into my first vignette which is about Q learning and."
        ],
        [
            "Epsilon exploration for fighting.",
            "So I'm not actually sure how many of you have a background really in reinforcement learning, but obviously one of the old favorite classics of Reinforcement Learning is the Q learning algorithm, and I'll just give you a little sketch here of how this works.",
            "We were working in the context of video games at the time, which was great fun.",
            "And so here's a little illustration of how tabular Q learning works, so this is the queue table, and its rows correspond to action.",
            "Sorry States and its columns correspond to actions.",
            "And in this case we have these states indexed by the distance between the opponents and their state in the game, and the actions are throacic or stand.",
            "And the way the algorithm works is that we would basically initialize randomly this table and then from the table we can easily derive a policy of what to do because we will find in which state our agent is.",
            "And then we look up the Q value, the state action value across these columns and find the column with the highest Q value, and then we'll carry out that action.",
            "So that's basically how this algorithm works and.",
            "It does require sufficient exploration, namely visiting all the states infinitely many times in order to guarantee convergence.",
            "So here let's see how this works in practice.",
            "This is the distance between the agents and they're both on the ground, so our agent, which is this friendly guy here, his name, is exiled.",
            "By the way, I spent quite a few hours with him.",
            "So this is the state we're in.",
            "This is the action we take because that maximizes across columns here and then.",
            "That action is actually.",
            "He taken, I thought there was a bit of sound for this.",
            "Let me see.",
            "Maybe it's a bit too low, OK, and then the question is about the reward.",
            "Where do we get the reward?",
            "Which the reward?",
            "Well, everyone who's played these games before knows that the health bar gives the reward right?",
            "The opponents health bar decreasing is the greatest to reward a player of these fighting games can get.",
            "Oops, you seem a banging this guy with his head on the floor did result in a decrease in health as one would expect from any normal biological creature.",
            "His name is view long growth, by the way, which I think sounds really evil.",
            "OK, so we observe that reward.",
            "Here we observe that we're in a new state.",
            "Maybe it greater distance now and knocked and we see another set of Q values here in this row.",
            "And now the trick with Q learning, of course, is that the seller, with respect to which we're learning, is going to be the maximiser again of this row, which is not necessarily the action that the agent will take.",
            "And that's what makes it.",
            "And off policy learning algorithm OK. And then we have this update rule where the Q value here will be adjusted to approximate the actual reward observed and the discounted and discounted Max of this row.",
            "Here this will be updated and the algorithm will keep going and exploration is really key in this scenario because you need to visit these cells in order to update."
        ],
        [
            "Them.",
            "So just to introduce our.",
            "Prepare for pain.",
            "Come forward creepy.",
            "And accept your inevitable defeat.",
            "OK, obviously we picked this as our guy and that's the bad guy, so to speak.",
            "Right controlled by the game AI, so in fact we didn't use tabular Q learning, but we used a one layer neural net.",
            "We used the richest state set of features that I had mentioned before, and there's also many more.",
            "Actions that you can carry out, and if you've played these games before, it's really all about the actions because you want to do left, left, right, left, up, down, and that gives you the Super combo that will destroy everything.",
            "Have you played any of these done?",
            "I don't have to admit it, OK?"
        ],
        [
            "And just I just wanted to give you some some idea of what happens if there's poor exploration here.",
            "So this is early on in the learning process.",
            "And you see, our agent is clearly stuck with this one action.",
            "I don't even know what that is called in.",
            "When you see, there's very little exploration, but.",
            "We we used epsilon greedy exploration then later and we find that.",
            "That our man exile after 15 minutes of training is really quite capable of kicking the behind of this green monster here.",
            "And obviously this depends on the learning process here.",
            "Depends on how we define the reward, and in this case we choose the reward according to the disk decrease in the Ugly Guys Health, which is exactly what I was."
        ],
        [
            "Saying before, but we can also.",
            "Choose a different reward function.",
            "In this case, there's punishment for decrease in either players health, and we've called this the Aikido style learning, because that's what one would expect, and we can take a look here.",
            "You know at the beginning, our friend is unfortunately again slightly stuck in his old ways and gets a bad beating.",
            "Um?",
            "But after.",
            "15 minutes of training.",
            "You see how our brutal exile has become a super peaceful character who avoids any kind of conflict.",
            "Except for that, and that's why I was telling you this whole story, because that's epsilon greedy exploration.",
            "What you just saw.",
            "In a even this most peaceful guy, at some point tries again.",
            "If it might help him in his ways to kick.",
            "This guy gets bad news.",
            "Of course, from the reward function.",
            "Or you can see here is that this kick trail thing we mark this as random.",
            "So this was a random action.",
            "This wasn't the policy optimal one, and so that's that's how this this whole thing works.",
            "OK, maybe a lesson more for Aikido Masters."
        ],
        [
            "OK.",
            "So listen Lesson 1 here.",
            "Simple epsilon greedy exploration is hard to beat.",
            "Or cake.",
            "And if you want to read more about it, there's this interesting empirical evaluation of Vermorel and Mori, where they compare different, relatively simple ways of doing exploration and.",
            "And it just turns out that despite its simplicity, namely always choosing the best action according to your current policy, except for in an epsilon fraction of time to do something utterly random, this is actually quite a powerful exploration."
        ],
        [
            "OK, here's another example.",
            "In the area of car racing.",
            "So we also wanted to make a S4 for car racing.",
            "Better and more adaptive, and actually a game came out of this Forza Motor Sports which was using.",
            "Elements of the AI that we had developed, but this is more of a side arm.",
            "We didn't end up using reinforcement learning in the real game.",
            "Game designers can be very nervous people when it comes to the predictability of their agents in the games they want some control over what these characters and learning is not really giving them that type of control.",
            "So here's the algorithm that we were using.",
            "Amps.",
            "I can't quite remember what it stands for.",
            "You know a ludicrous.",
            "Acronym I think."
        ],
        [
            "It's a model based reinforcement learning algorithm, so we have these, say state features and we're discretizing the state space.",
            "And we make observations and some of them fail.",
            "You know the car crashes into a wall, and some of them succeed.",
            "And we're collecting this experiment.",
            "Then we learn the transition dynamics between these states, or within this state abstraction.",
            "And we look at the rewards.",
            "And once we've learned this, then in the manner of model based reinforcement learning, we revised the value function and the policy based on that during using prioritized sweeping and then after that there's a step where the state action abstraction is revised.",
            "So in this case, because this state had such different outcomes, these positive and those negative outcomes there's this state is split into two.",
            "The example and then.",
            "The algorithm returns to one and collects more experience.",
            "Now the interesting thing here is that the algorithm can be primed in a particular way, and I'll."
        ],
        [
            "I'll show that any video just to show you what the problem here is.",
            "If you have two cores, an abstraction, then you cannot discriminate between the states.",
            "If you have to find an abstraction, then you have two little statistical data in order to estimate your transition probabilities in your rewards, and you want to make this adaptive.",
            "Ideally, you know high resolution in some areas low resolution in others.",
            "And that's what this algorithm is trying to do."
        ],
        [
            "And we wanted to build this into a real time racing simulation or game.",
            "If you like.",
            "These games are basically similar."
        ],
        [
            "Patients nowadays, so let's briefly look at the representation.",
            "The state representation was that we had these laser range Finder measurements as features, so the car is basically shooting out a laser range Finder here, here and here, and to the back measures the distances to the next obstacle.",
            "And that's the state representation, and we measure reward as progress along the track.",
            "So the more progress this thing makes."
        ],
        [
            "In a given time interval, but better.",
            "There are certain actions as you would expect in car if you've driven a car before, you know what the."
        ],
        [
            "Actions are.",
            "And I just want to show you a little video of how this learns the special thing here is that the exploration is primed by a human driver.",
            "Well that is human video game driver, right?",
            "Driving the car around a track first and during that time the model based part of the of the algorithm is active is and is learning the transition probabilities is learning the state abstraction and is learning the rewards.",
            "But driven by the human policy.",
            "So basically the human is doing the exploration here and then the algorithm takes over and there's an interesting effect there, because the human in this case I think was me, and I'm not a very good driver and.",
            "So now the reinforcement learning algorithm.",
            "The reason is that I wasn't really showing it how to drive right?",
            "I was just showing it.",
            "What happens when you are driving within that state space and what the rewards are and then the algorithm was using prioritized sweeping and was actually optimizing the value function and was then able to drive in a much better way.",
            "But using the state representation that had been created during my rather well not very good driving, so that's an entirely different way of exploring based on humans so."
        ],
        [
            "The second lesson, if you like, well, maybe I'm not sure what the lesson here is, but I it's that there are ways of exploring which don't necessarily need to be smart.",
            "It can be enough if you have an agent who leads you through the state space and then the algorithm can learn the dynamics and the rewards and the structure of that space, and then can learn a policy much better than the original expiration policy."
        ],
        [
            "OK, here's one of my favorite topics that I've been working on for a long time, which is computer go and it also offers a few lessons."
        ],
        [
            "Um in in exploration.",
            "This is a recent game that we published on Xbox Live Arcade.",
            "It's called the path of go.",
            "If you happen to have an Xbox, I can only encourage you to download it and try it, so this is a game in which we basically used a lot of the insights of the computer.",
            "Go community about how to build computer go agents based on Monte Carlo Tree search and put that into into an Xbox game which you can play.",
            "Actually the other function was to teach people how to play go becausw.",
            "We had to assume that most people are on Xbox Live, didn't actually know how to play goal.",
            "So that's why we also paid some."
        ],
        [
            "Attention to graphics and stuff.",
            "So in case there's anyone among you who doesn't know about the game, it's a very old game, just an estimate of how old it is originating in.",
            "China has over 60 million players worldwide.",
            "There's two players, black and white.",
            "Here you see a little render to go board this.",
            "Actually in 9 by 9 board, which is the size of board used for beginners.",
            "But the real game is played on a 19 by 19 grid and the rules are fairly simple.",
            "For example, if it's white turn, white can places stone here on the board on an empty vertex.",
            "After that it would be black, Stern, and so they take turns placing their stones on these vertices.",
            "There's a capture rule, so if white plays another stone here, then these three black stones are entirely surrounded by white stones and would be taken off the board.",
            "And the goal of the game is to gather territory.",
            "So Blacks territory would roughly be this area here because it's surrounded by black whites territory would be this.",
            "I would estimate that white is probably in a better position at this point and would win the game."
        ],
        [
            "Now what's interesting about computer go?",
            "You're probably aware that quite some time ago.",
            "Gary Kasparoff was beaten by Deep Blue by IBM Stress Computer and that was a very impressive achievement, although of course people afterwards said, yeah, you know how?",
            "How is that intelligent?",
            "It's just research, but the fact remains that the best go programs cannot beat strong amateurs.",
            "And it was even worse few years ago when this would have said the best goal.",
            "Programs cannot even beat average or weak amateurs.",
            "Now I had to modify this a little bit.",
            "Because they have improved a lot and."
        ],
        [
            "Let's first look at what the problem with computer go is in comparison to chess.",
            "The problem there are basically 22 problems.",
            "We want to build a game tree and we would like to do Min Max search on this.",
            "We know that that would eventually lead to the best solution if we had infinite compute power.",
            "Now the problem is that the branching factor for goal is roughly 200 on average where it is maybe 20 to 35 in chest.",
            "So the game Tree of Goal is much bigger than the game Tree of chess, so that makes it harder.",
            "Second problem, if you want to do Game Tree search, you need a heuristic evaluation function.",
            "That's the one that you evaluate at the leaf nodes, which are not really terminal nodes, but which are basically the horizon of your search depth.",
            "And it turns out that it's very hard to evaluate goal positions.",
            "In chess.",
            "You can look at the safety of the King and the amount of material the players have, and you can come up with a rough estimate in go.",
            "There's these stones spread.",
            "Over the board.",
            "Very hard to do.",
            "Everything is connected to everything else.",
            "And.",
            "Over the past, maybe five years.",
            "This methodology, called Monte Carlo Tree Search, was developed to address."
        ],
        [
            "This problem and there were a few key insights that led to this and let me just quickly characterize what they are.",
            "This guy band Brickman in 1993, figured out something really interesting about the game.",
            "If you take a position of go and you play randomly from that position and times and you observe that K times black wins on my end minus K times white wins, then there is a tiny signal in this count which.",
            "Tells us about how good the original position was for black or white, so there might be a like a 51% winning chance for black or 49% for white and that will signal contains some truth in that black probably had a slightly better position.",
            "That's something this Martin Brickman figured out.",
            "Then a second step.",
            "Was two to this idea that maybe we shouldn't just randomly play out, but we should buy us our playouts towards better moves.",
            "And you know, in our random searches give higher probabilities to better moves and.",
            "This formalization of this technique basically is UCT and we have one of the authors here.",
            "Java today will not talk about this, I think, but I think it's a.",
            "It's a beautiful paper and I recommend it to everyone here.",
            "And then the third inside that really made this work was that people realize that the move order in goal isn't actually so crucial.",
            "So if in a random play out you first play this move and then play that move, or vice versa in most of the cases doesn't really matter.",
            "And that means that you can.",
            "You can make very fast estimates by ignoring the move order.",
            "And finally you can use all kinds of prior knowledge in order to bias your search."
        ],
        [
            "So just to give you a little idea of how this works.",
            "Suppose we have this position here, and we'd like to evaluate.",
            "Um, how good this position is for black or for white?",
            "Then we could do a random player.",
            "This actually a very peaceful one that I chose here and then this random playout leads to a terminal position groups.",
            "Yeah leads to a terminal position.",
            "A terminal position being one were no legal move can be made anymore, and this leads to a territory hypothesis, namely under display.",
            "Some of these points end up being white territory and some end up being black territory, and now do that 10,000 times or 100,000 times and you will get an estimate of what the territory distribution."
        ],
        [
            "Will be under random play.",
            "And so here's the here's the strategy that you could then use.",
            "You could make try out all of the moves that are available in your given position.",
            "You could do random place from each one down there, and you could just pick the best one, the one that gives you the highest number of wins in these random play hours."
        ],
        [
            "But of course, the smarter thing to do is to keep track of which moves."
        ],
        [
            "Farm how well you do different playoffs but you build up this little game."
        ],
        [
            "We up here and in these internal node nodes you save some information about how how the chances are for this.",
            "If you play this note to win or to lose, which is basically the counts down here and then.",
            "Of course the question that you need to answer is once you have some of these is which of these moves should I try next?",
            "And that's the familiar bandit problem you Now view.",
            "Each of these branches each of the moves you can make as a bandit.",
            "You can pull and figure out how much payoff that one gives you, and so basically the whole tree becomes a collection of little bandits, that which are independent, which are Inter dependent, of course."
        ],
        [
            "And.",
            "Then you can for example, use the UCB one algorithm by hour at all, which which uses an upper confidence interval to determine which move to make.",
            "So it takes an optimistic estimate.",
            "It basically says that you take the arm which has given you the best average payoff.",
            "That's this extra bar, but you have this additional term where N is the number of visits to a node.",
            "And JS is number of times where you played Move J in that node and what this will do is that it will make sure that each of the moves will still be explored sufficiently often, while at the same time you are putting more probability into your more often exploring the moves that are actually more promising in the sense of their payoff so far.",
            "And in this use it paper it was shown that this still leads to sufficient exploration despite the fact.",
            "That, of course these pay offs are drifting because if you have a band it up here and you're trying to make that choice, then there's going to be another bandit further down here where you have to make another choice.",
            "And so this the pay offs for this band, it will actually drift depending on what happens down here."
        ],
        [
            "So this was an incredibly successful push for computer going.",
            "You can't imagine this field which has a lot of very nice and really friendly people working in it kind of amateurish field in many ways, but it's a nice community and these people had been agonizing over 20 years.",
            "How to make better go programs.",
            "And of course, these chess guys were always ahead in some sense, and they beat 'cause power off and didn't go.",
            "They couldn't make progress, and this was really the breakthrough.",
            "And some great things happen.",
            "For example mogul, which was the first program to adopt this strategy, beaten 8 and professional with nine handicap stones on its side.",
            "So you can bias the odds, so to speak by giving it advantage to the program.",
            "But this program is also now on pretty much on par with professionals on 9 by nine goal on the small version, at least when you give it 800 processor so well, it's actually pretty good.",
            "Even a non standard hardware now.",
            "And now in a lot of other programs are following in its footsteps, and I think Morgan might not even be the strongest anymore.",
            "At this point.",
            "There's also commercial applications.",
            "But one thing I would like to mention, which I found really interesting, the best programs now don't use UCB anymore, they just use the average payoff achieved in the node as their criterion for picking the best move."
        ],
        [
            "And the lesson I would like us to learn from this is that theory can really be a powerful guide, and in this case really this the UCT paper induced this whole development of Monte Carlo go and of successful goal programs.",
            "But in the end there's often a deviation from Thierry which just makes things work better.",
            "And I think then that really means that the balls back in theories court to explain why that might be the case.",
            "OK, that's"
        ],
        [
            "In three, let's talk a little bit about web search."
        ],
        [
            "So, um.",
            "In web search you may be familiar with the standard techniques that are being used.",
            "The traditional information retrieval paradigm is Boolean retrieval.",
            "You type in a few keywords and you get documents delivered that contain these keywords that you can do logical combinations of them.",
            "So this quickly hit its boundaries, but then people hit upon this idea of static rank, for example, page rank is such an idea.",
            "So independent of the Cleary, we can order the pages according to some measure.",
            "For example, how high their page rank is, how high they are in degrees or other measures.",
            "And so we get this presorting of the pages, and then if we use Boolean retrieval, we already get a preference order and we can better documents that both satisfy the bullying.",
            "Theory and have a high static rank and then of course.",
            "The search engines are using machine learning to learn ranking models from human labels of query URL pairs, so there's a lot of people whose day job it is to look at query URL pairs and say if that URL is a good answer for that particular query, and then you can use that data to learn machine learning models.",
            "Now this is kind of the traditional methodology if you like if there's such a thing in web search and the challenges with these human labels are they cost a lot of money.",
            "You actually have to pay these people to do this.",
            "It's unclear if these human labels reflect audience preference, because those are people who are professional label as they have this Handbook of how to label stuff guidelines, basically.",
            "But how does that really reflect what people like knew trends may not be picked up quickly enough, so you can imagine that if there's something you coming along, how are the label is going to know about it?",
            "How is it going to get into the system is also difficult to model personal preferences if you have professional labels, right?",
            "I mean, in fact they should keep their personal preferences out of the process, but on the other hand, maybe the search engine would like to serve people with their personal preferences.",
            "And also there's really no relation to user engagement and page composition when you do judgment purely on clear URL pairs.",
            "You're not taking into account the way the results are presented on the page, and if users would be engaged by that kind of."
        ],
        [
            "Presentation so as a consequence.",
            "We would like to use user feedback and clicks.",
            "And the advantages are really that much more data is available, and it's much less costly.",
            "Um?",
            "You can incorporate dynamics of users and documents, so if users change their taste, if this new thing is coming up, if documents disappear or come up, you can take that into account.",
            "It's useful personalization because those people your customers are actually the ones who give you the data instead of this group of judges.",
            "Data may even be available for small collection or some niche scenarios, as long as there's people using a collection, you will get this type of data.",
            "So if there's a totally unpopular collection, yes, there will be no user data.",
            "But then why would you want to do something with it anyway?",
            "So this is then the classical online learning scenario, because you might want to choose documents to the user for their immediate benefit, because that satisfies their needs or you want to choose documents to show to improve your.",
            "Your policy of finding out about the documents and."
        ],
        [
            "Delivering better documents.",
            "And I would just want to point out three interesting areas that come up when you deal with these kind of real world bandit problems and some related literature.",
            "So the first interesting problem is that you would want to generalize across documents.",
            "The problem is there are so many documents.",
            "If you just view them for example as independent bands, Bendit arms, that's relatively hopeless thing to do.",
            "And the question is then, can we devise some kind of feature representation for the documents and use that feature representation in our bandit learning?",
            "So essentially that means that if you pull a banded arm here, it tells you something about the neighboring bandits and for example, one way in which this has been formalized is in the metric pendence work, by Robert Kleinberg at all, where they make smoothness assumptions on the pay offs depending on the metric distance.",
            "So basically.",
            "The assumption is that if two bandits or two documents are similar, then the payoff of showing them or pulling that arm will be similar.",
            "Related by this Lipschitz condition, and then you can infect, and that's what they've shown.",
            "Device in some sense, an adaptive discretization of this space cover.",
            "And.",
            "And in that way, zoom in on the interesting bits of the policy space.",
            "So finding the hive."
        ],
        [
            "Your documents, another interesting problem is the diversity of search results, and here the insight is that when you show 10 blue links.",
            "From N documents, then you really have 10 choose K * K factorial many, many ways of doing that.",
            "First duties, choose the K links.",
            "Then you can or to choose the order in which you show them and this work on ranked bandit algorithms for example by by Radlinski here addresses that problem to use the problem structure that this actually is a ranking they have one way they instantiate one bandit per slot.",
            "For display slot and can then learn."
        ],
        [
            "Efficient way.",
            "Finally, there's the aspect of dynamics and mortality, and I just want to briefly point you to the paper on mortal multi armed bandits.",
            "This models the problem.",
            "That's also specific to web search and advertising that some of the options just go away.",
            "You know, an ad may not be available forever in the bandit setting.",
            "Once you've found that Grey bandit, you can cash in on it forever, right?",
            "And that's just not the case in these real world problems, and you can also formalize that.",
            "And the device algorithms in this case, which find good enough arms and start exploiting much earlier, which is the right thing to do in these scenarios, and that's also shown in this paper.",
            "Chakrabarti ET al 2."
        ],
        [
            "And eight lesson here theory can be enriched to provide guidance in more differentiated practical scenarios.",
            "So coming from this beautiful Gittins index, pure scenario of the bandit, we can move into more complex territory and model more interesting situations that correspond to real world scenarios in web."
        ],
        [
            "Search and advertising."
        ],
        [
            "OK, last thing exploration in online advertising.",
            "I think I have very little time for this.",
            "Let me assume that you know how how it works, how this online search advertising works.",
            "The user puts in a query, advertisers bid for that query.",
            "We multiply that bid with a click through rate to get an expected monetization.",
            "And then the advertisers are ordered according to this.",
            "Expected monetization you can see that here, but they pay according to the generalized second price auction.",
            "The key thing here is that for both the decision what to show in which order and how much to charge, it's crucial to estimate these click through rates here these pies and they are important to have good user satisfaction there important to monetize appropriately, and so that's really an."
        ],
        [
            "Important thing and paid search advertising and we have this Bayesian system at predictor that estimates these."
        ],
        [
            "80 hours now what I would like to just briefly talk about is the problem of putting such a learning system in this case here into the into the advertising environment into the close loop.",
            "Let me show what I mean.",
            "So a user.",
            "Issues a query and we get may get user information that is made available to the learning algorithm.",
            "There's in process of its election and edge filtering.",
            "Certain advertisers have bid for query words and the system figures that out and makes that available to the learning algorithm.",
            "The learning algorithm then provides these click through rates estimates which lead to add ranking and pricing and so on together with the bids that the Edward Teller Tasers have given.",
            "Then those ads are shown to the user and the user makes their choice.",
            "You know, usually they don't click on edge, that's the most frequent case, but if they do, they choose an ad and this label.",
            "Then the fact that they clicked on something or didn't click on something gets fed into the learning algorithm.",
            "So that's basically the scenario.",
            "But the problem here is that we will then use this learning algorithm.",
            "To decide what the city, RSR and so we will essentially filter the training data set according to CTR will only show ads that will have sufficiently high Cdr because that's the interest of the advertisers.",
            "That's in the interest of the search engine is also in the interest of the user.",
            "So in the next step the training sample will only consist of a biased subsample of the original one and then.",
            "We will have the same scenario and this loop will keep going on and this loop can be very problematic for a learning algorithm and we need to be very careful to reweigh the examples for training to do sufficient exploration in order to avoid a scenario in which certain ads will not be shown.",
            "We will not be shown anymore and are therefore kind of disappearing in a dark pool, and there's also a number of other problems, but this causal loop, I think will be more and more prevalent in a lot of applications because it occurs whenever you use what you've learned to make decisions and where those decisions then influence what you later learn from.",
            "And I think that's really quite a common scenario.",
            "It's very common in these online services, but of course it's also the key problem in.",
            "More general scenarios such as reinforcement learning.",
            "I have an example, I'll skip that how that leads to problems."
        ],
        [
            "One thing that we do use is this Thompson heuristic for for exploration.",
            "The click through rate algorithm that we have is a Bayesian algorithm, so it doesn't give us a click through rate.",
            "It gives us a distribution over click through rates and the example you can see here is that for one add, the algorithm may give us this distribution here, which has an average clickthrough rate of 25%, but large variance left, whereas for some other ad.",
            "That we've seen more often, the average may be higher, but there's less uncertainty left, and the idea of this Thompson heuristic for exploration is to sample from these distributions such that most of the time the better one might you know, the one on average, better might come out as the one that gets shown, but every now and then, because of the higher variance, this other Ed with the lower average will also get shown and will be able to reduce the variance on our estimate.",
            "And hence either find out that it is in fact better than the red one or kind of get rid of it and and show it less often in the future."
        ],
        [
            "So just to emphasize the 5th lesson, I think this causal loop will be an interesting beast to study, because so far we often looks at this better scenario of some classification algorithm.",
            "But really, if you start using the output of that algorithm, and that's why you trained it in the 1st place, right?",
            "And that influences the future of your training set generation, then there's a bunch of very interesting problems that you need to.",
            "Take care of in order to make this work."
        ],
        [
            "OK, here are just the five lessons I think I'm out of time, is that right?",
            "Great yeah, so these are really just a summary of the five lessons.",
            "Remember the fighters epsilon greedy can be pretty good.",
            "Exploration is not always what it seems you can do really stupid exploration.",
            "Even me driving a virtual car and it can lead to good algorithmic performance.",
            "I think computer goal is a beautiful example where Thierry kind of leads the way and show us what can be done.",
            "Great success in practice.",
            "But then practitioners do something different and I think Thierry could pick that up and learn something about new problems settings from the world of search.",
            "There are all these interesting differentiated problems that deviate from the standard bandwidth settings, but can be studied in the theoretical context, and I think that's a very promising area.",
            "And finally, watch out for that cause loop and don't be caught up by it.",
            "OK, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much for the invitation to talk here.",
                    "label": 0
                },
                {
                    "sent": "I was actually not sure if I wanted to accept this at first because I thought, OK, I haven't actually worked explicitly on the problem of exploration versus exploitation, or was thinking do I really have something to contribute?",
                    "label": 0
                },
                {
                    "sent": "And then I thought back a little bit over my time over the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "What I had done and I had encountered the problem of exploration versus exploitation so often in different contexts that I thought maybe it would be best if I just give you a little guided tour of these.",
                    "label": 0
                },
                {
                    "sent": "Practical aspects of exploration versus exploitation, and this is work with tracking kiniro candela David Stern.",
                    "label": 0
                },
                {
                    "sent": "Refer break Michael Condo for Leon Bottou and and a lot of other colleagues who were involved and it's going to be mostly explorations, so don't expect any real deep theoretical insights.",
                    "label": 1
                },
                {
                    "sent": "I also view myself a little bit as the opener of the workshop the Entertainer and because I'm the first one.",
                    "label": 0
                },
                {
                    "sent": "I think I can also have the freedom of asking more questions than deliverying answers.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the overview.",
                    "label": 0
                },
                {
                    "sent": "I'll give a short introduction, motivation, and then I have these five vignettes from my past work that I'll discuss.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about fighting, I'll talk about racing, talk about computer, go, and I'll talk about web search and online advertising as well.",
                    "label": 0
                },
                {
                    "sent": "So, and I hope we'll have little lesson to learn from each one of these.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of introduction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know who of you is familiar with this experiment.",
                    "label": 0
                },
                {
                    "sent": "The so called marshmallow experiment.",
                    "label": 1
                },
                {
                    "sent": "This experiment is about maximizing long-term reward and about the difficulty this problem holds for people, and it is the study involved people be, or in particular children being given a marshmallow.",
                    "label": 0
                },
                {
                    "sent": "And then being put into room being given a marshmallow.",
                    "label": 0
                },
                {
                    "sent": "And then they were told that if they could wait for some unknown period of time, someone would come back and give them another marshmallow and then they would have two that they would be able to eat.",
                    "label": 0
                },
                {
                    "sent": "If they however decided to immediately eat the marshmallow, there would be no second marshmallow right?",
                    "label": 0
                },
                {
                    "sent": "And let me just show you a little video where we see how these folks reacted and.",
                    "label": 0
                },
                {
                    "sent": "Alright, here's the deal.",
                    "label": 0
                },
                {
                    "sent": "Marshmallow, for you.",
                    "label": 0
                },
                {
                    "sent": "You can either wait and I'll give you another one.",
                    "label": 0
                },
                {
                    "sent": "If you wait.",
                    "label": 0
                },
                {
                    "sent": "Or you can eat it now when I come back, I'll give you another one.",
                    "label": 0
                },
                {
                    "sent": "So then you have to, but stay in here and stay in the chair till I come back.",
                    "label": 0
                },
                {
                    "sent": "OK alright?",
                    "label": 0
                },
                {
                    "sent": "I'm gonna go do something and then I'll come back.",
                    "label": 0
                },
                {
                    "sent": "Really good.",
                    "label": 0
                },
                {
                    "sent": "Sampling.",
                    "label": 0
                },
                {
                    "sent": "You can have it now, or you can wait.",
                    "label": 0
                },
                {
                    "sent": "I'll be back, stay in the chair, OK?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So hard, isn't it?",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to leave and then I'll come back.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can either eat it right now or you can wait.",
                    "label": 0
                },
                {
                    "sent": "Either way.",
                    "label": 0
                },
                {
                    "sent": "OK, OK?",
                    "label": 0
                },
                {
                    "sent": "How do you do?",
                    "label": 0
                },
                {
                    "sent": "Did you do good?",
                    "label": 0
                },
                {
                    "sent": "You did you want to eat it and you, yeah, so they tell you to give you another one.",
                    "label": 0
                },
                {
                    "sent": "You can have both.",
                    "label": 0
                },
                {
                    "sent": "You need to.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I think we have established how hard this is.",
                    "label": 0
                },
                {
                    "sent": "And what I found interesting about these studies is that when you look, this was the longitudinal study.",
                    "label": 0
                },
                {
                    "sent": "So these these subjects were followed through the years, so to speak.",
                    "label": 0
                },
                {
                    "sent": "And then there was just kind of follow up 30 years later and they looked at the SAT scores and of drug usage of these people they had carried out these tests with and it was found that people who were able to delay the gratification had on some of these scores like.",
                    "label": 1
                },
                {
                    "sent": "I had on average higher SAT scores, lower drug abuse tendencies, and so on.",
                    "label": 0
                },
                {
                    "sent": "So I think it's not exactly the topic that we're talking about, but it comes close because exploration is all about sacrificing something for the moment in order to gain a long-term advantage.",
                    "label": 0
                },
                {
                    "sent": "So that was just.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To set the scene and now just to make sure that we're all on the same page.",
                    "label": 0
                },
                {
                    "sent": "Roughly the problem of exploration exploitation is that if there's a given agent in an unknown environment, then the agent faces a dilemma.",
                    "label": 1
                },
                {
                    "sent": "They can either exploit the current knowledge and optimize for the short term reward, or they can explore the environment to discover opportunities for long-term reward.",
                    "label": 1
                },
                {
                    "sent": "And here's just a little sketch if we think of this landscape as a utility landscape, so to speak, and the agent at any point in time gets utility proportional to the height where is then.",
                    "label": 0
                },
                {
                    "sent": "I can if there's too little exploration, the agent will be locali stuck maybe on this little Hill here and will never get really good pay offs.",
                    "label": 0
                },
                {
                    "sent": "If the agent explores too much, then he will discover the high payoff areas, but he'll spend too little time in them and hence overall not together or the free Ward.",
                    "label": 0
                },
                {
                    "sent": "And if we find the sweet spot or if the agent finds this sweet spot, then you'll spend most of the time in these higher reward areas and that's that's the ideal scenario.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the basic setup and now I want to dive into my first vignette which is about Q learning and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Epsilon exploration for fighting.",
                    "label": 0
                },
                {
                    "sent": "So I'm not actually sure how many of you have a background really in reinforcement learning, but obviously one of the old favorite classics of Reinforcement Learning is the Q learning algorithm, and I'll just give you a little sketch here of how this works.",
                    "label": 0
                },
                {
                    "sent": "We were working in the context of video games at the time, which was great fun.",
                    "label": 0
                },
                {
                    "sent": "And so here's a little illustration of how tabular Q learning works, so this is the queue table, and its rows correspond to action.",
                    "label": 0
                },
                {
                    "sent": "Sorry States and its columns correspond to actions.",
                    "label": 0
                },
                {
                    "sent": "And in this case we have these states indexed by the distance between the opponents and their state in the game, and the actions are throacic or stand.",
                    "label": 0
                },
                {
                    "sent": "And the way the algorithm works is that we would basically initialize randomly this table and then from the table we can easily derive a policy of what to do because we will find in which state our agent is.",
                    "label": 0
                },
                {
                    "sent": "And then we look up the Q value, the state action value across these columns and find the column with the highest Q value, and then we'll carry out that action.",
                    "label": 0
                },
                {
                    "sent": "So that's basically how this algorithm works and.",
                    "label": 0
                },
                {
                    "sent": "It does require sufficient exploration, namely visiting all the states infinitely many times in order to guarantee convergence.",
                    "label": 0
                },
                {
                    "sent": "So here let's see how this works in practice.",
                    "label": 0
                },
                {
                    "sent": "This is the distance between the agents and they're both on the ground, so our agent, which is this friendly guy here, his name, is exiled.",
                    "label": 0
                },
                {
                    "sent": "By the way, I spent quite a few hours with him.",
                    "label": 0
                },
                {
                    "sent": "So this is the state we're in.",
                    "label": 0
                },
                {
                    "sent": "This is the action we take because that maximizes across columns here and then.",
                    "label": 0
                },
                {
                    "sent": "That action is actually.",
                    "label": 0
                },
                {
                    "sent": "He taken, I thought there was a bit of sound for this.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a bit too low, OK, and then the question is about the reward.",
                    "label": 0
                },
                {
                    "sent": "Where do we get the reward?",
                    "label": 0
                },
                {
                    "sent": "Which the reward?",
                    "label": 0
                },
                {
                    "sent": "Well, everyone who's played these games before knows that the health bar gives the reward right?",
                    "label": 0
                },
                {
                    "sent": "The opponents health bar decreasing is the greatest to reward a player of these fighting games can get.",
                    "label": 0
                },
                {
                    "sent": "Oops, you seem a banging this guy with his head on the floor did result in a decrease in health as one would expect from any normal biological creature.",
                    "label": 0
                },
                {
                    "sent": "His name is view long growth, by the way, which I think sounds really evil.",
                    "label": 0
                },
                {
                    "sent": "OK, so we observe that reward.",
                    "label": 0
                },
                {
                    "sent": "Here we observe that we're in a new state.",
                    "label": 0
                },
                {
                    "sent": "Maybe it greater distance now and knocked and we see another set of Q values here in this row.",
                    "label": 0
                },
                {
                    "sent": "And now the trick with Q learning, of course, is that the seller, with respect to which we're learning, is going to be the maximiser again of this row, which is not necessarily the action that the agent will take.",
                    "label": 0
                },
                {
                    "sent": "And that's what makes it.",
                    "label": 0
                },
                {
                    "sent": "And off policy learning algorithm OK. And then we have this update rule where the Q value here will be adjusted to approximate the actual reward observed and the discounted and discounted Max of this row.",
                    "label": 0
                },
                {
                    "sent": "Here this will be updated and the algorithm will keep going and exploration is really key in this scenario because you need to visit these cells in order to update.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them.",
                    "label": 0
                },
                {
                    "sent": "So just to introduce our.",
                    "label": 0
                },
                {
                    "sent": "Prepare for pain.",
                    "label": 0
                },
                {
                    "sent": "Come forward creepy.",
                    "label": 0
                },
                {
                    "sent": "And accept your inevitable defeat.",
                    "label": 0
                },
                {
                    "sent": "OK, obviously we picked this as our guy and that's the bad guy, so to speak.",
                    "label": 0
                },
                {
                    "sent": "Right controlled by the game AI, so in fact we didn't use tabular Q learning, but we used a one layer neural net.",
                    "label": 1
                },
                {
                    "sent": "We used the richest state set of features that I had mentioned before, and there's also many more.",
                    "label": 0
                },
                {
                    "sent": "Actions that you can carry out, and if you've played these games before, it's really all about the actions because you want to do left, left, right, left, up, down, and that gives you the Super combo that will destroy everything.",
                    "label": 0
                },
                {
                    "sent": "Have you played any of these done?",
                    "label": 0
                },
                {
                    "sent": "I don't have to admit it, OK?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just I just wanted to give you some some idea of what happens if there's poor exploration here.",
                    "label": 0
                },
                {
                    "sent": "So this is early on in the learning process.",
                    "label": 1
                },
                {
                    "sent": "And you see, our agent is clearly stuck with this one action.",
                    "label": 0
                },
                {
                    "sent": "I don't even know what that is called in.",
                    "label": 0
                },
                {
                    "sent": "When you see, there's very little exploration, but.",
                    "label": 0
                },
                {
                    "sent": "We we used epsilon greedy exploration then later and we find that.",
                    "label": 1
                },
                {
                    "sent": "That our man exile after 15 minutes of training is really quite capable of kicking the behind of this green monster here.",
                    "label": 0
                },
                {
                    "sent": "And obviously this depends on the learning process here.",
                    "label": 0
                },
                {
                    "sent": "Depends on how we define the reward, and in this case we choose the reward according to the disk decrease in the Ugly Guys Health, which is exactly what I was.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saying before, but we can also.",
                    "label": 0
                },
                {
                    "sent": "Choose a different reward function.",
                    "label": 0
                },
                {
                    "sent": "In this case, there's punishment for decrease in either players health, and we've called this the Aikido style learning, because that's what one would expect, and we can take a look here.",
                    "label": 1
                },
                {
                    "sent": "You know at the beginning, our friend is unfortunately again slightly stuck in his old ways and gets a bad beating.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But after.",
                    "label": 0
                },
                {
                    "sent": "15 minutes of training.",
                    "label": 0
                },
                {
                    "sent": "You see how our brutal exile has become a super peaceful character who avoids any kind of conflict.",
                    "label": 0
                },
                {
                    "sent": "Except for that, and that's why I was telling you this whole story, because that's epsilon greedy exploration.",
                    "label": 0
                },
                {
                    "sent": "What you just saw.",
                    "label": 0
                },
                {
                    "sent": "In a even this most peaceful guy, at some point tries again.",
                    "label": 0
                },
                {
                    "sent": "If it might help him in his ways to kick.",
                    "label": 0
                },
                {
                    "sent": "This guy gets bad news.",
                    "label": 0
                },
                {
                    "sent": "Of course, from the reward function.",
                    "label": 0
                },
                {
                    "sent": "Or you can see here is that this kick trail thing we mark this as random.",
                    "label": 0
                },
                {
                    "sent": "So this was a random action.",
                    "label": 0
                },
                {
                    "sent": "This wasn't the policy optimal one, and so that's that's how this this whole thing works.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe a lesson more for Aikido Masters.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So listen Lesson 1 here.",
                    "label": 0
                },
                {
                    "sent": "Simple epsilon greedy exploration is hard to beat.",
                    "label": 0
                },
                {
                    "sent": "Or cake.",
                    "label": 0
                },
                {
                    "sent": "And if you want to read more about it, there's this interesting empirical evaluation of Vermorel and Mori, where they compare different, relatively simple ways of doing exploration and.",
                    "label": 0
                },
                {
                    "sent": "And it just turns out that despite its simplicity, namely always choosing the best action according to your current policy, except for in an epsilon fraction of time to do something utterly random, this is actually quite a powerful exploration.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's another example.",
                    "label": 0
                },
                {
                    "sent": "In the area of car racing.",
                    "label": 0
                },
                {
                    "sent": "So we also wanted to make a S4 for car racing.",
                    "label": 0
                },
                {
                    "sent": "Better and more adaptive, and actually a game came out of this Forza Motor Sports which was using.",
                    "label": 0
                },
                {
                    "sent": "Elements of the AI that we had developed, but this is more of a side arm.",
                    "label": 0
                },
                {
                    "sent": "We didn't end up using reinforcement learning in the real game.",
                    "label": 0
                },
                {
                    "sent": "Game designers can be very nervous people when it comes to the predictability of their agents in the games they want some control over what these characters and learning is not really giving them that type of control.",
                    "label": 0
                },
                {
                    "sent": "So here's the algorithm that we were using.",
                    "label": 0
                },
                {
                    "sent": "Amps.",
                    "label": 0
                },
                {
                    "sent": "I can't quite remember what it stands for.",
                    "label": 0
                },
                {
                    "sent": "You know a ludicrous.",
                    "label": 0
                },
                {
                    "sent": "Acronym I think.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a model based reinforcement learning algorithm, so we have these, say state features and we're discretizing the state space.",
                    "label": 0
                },
                {
                    "sent": "And we make observations and some of them fail.",
                    "label": 0
                },
                {
                    "sent": "You know the car crashes into a wall, and some of them succeed.",
                    "label": 0
                },
                {
                    "sent": "And we're collecting this experiment.",
                    "label": 0
                },
                {
                    "sent": "Then we learn the transition dynamics between these states, or within this state abstraction.",
                    "label": 1
                },
                {
                    "sent": "And we look at the rewards.",
                    "label": 0
                },
                {
                    "sent": "And once we've learned this, then in the manner of model based reinforcement learning, we revised the value function and the policy based on that during using prioritized sweeping and then after that there's a step where the state action abstraction is revised.",
                    "label": 1
                },
                {
                    "sent": "So in this case, because this state had such different outcomes, these positive and those negative outcomes there's this state is split into two.",
                    "label": 0
                },
                {
                    "sent": "The example and then.",
                    "label": 1
                },
                {
                    "sent": "The algorithm returns to one and collects more experience.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing here is that the algorithm can be primed in a particular way, and I'll.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show that any video just to show you what the problem here is.",
                    "label": 0
                },
                {
                    "sent": "If you have two cores, an abstraction, then you cannot discriminate between the states.",
                    "label": 0
                },
                {
                    "sent": "If you have to find an abstraction, then you have two little statistical data in order to estimate your transition probabilities in your rewards, and you want to make this adaptive.",
                    "label": 0
                },
                {
                    "sent": "Ideally, you know high resolution in some areas low resolution in others.",
                    "label": 0
                },
                {
                    "sent": "And that's what this algorithm is trying to do.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we wanted to build this into a real time racing simulation or game.",
                    "label": 1
                },
                {
                    "sent": "If you like.",
                    "label": 0
                },
                {
                    "sent": "These games are basically similar.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patients nowadays, so let's briefly look at the representation.",
                    "label": 0
                },
                {
                    "sent": "The state representation was that we had these laser range Finder measurements as features, so the car is basically shooting out a laser range Finder here, here and here, and to the back measures the distances to the next obstacle.",
                    "label": 0
                },
                {
                    "sent": "And that's the state representation, and we measure reward as progress along the track.",
                    "label": 0
                },
                {
                    "sent": "So the more progress this thing makes.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a given time interval, but better.",
                    "label": 0
                },
                {
                    "sent": "There are certain actions as you would expect in car if you've driven a car before, you know what the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions are.",
                    "label": 0
                },
                {
                    "sent": "And I just want to show you a little video of how this learns the special thing here is that the exploration is primed by a human driver.",
                    "label": 0
                },
                {
                    "sent": "Well that is human video game driver, right?",
                    "label": 0
                },
                {
                    "sent": "Driving the car around a track first and during that time the model based part of the of the algorithm is active is and is learning the transition probabilities is learning the state abstraction and is learning the rewards.",
                    "label": 0
                },
                {
                    "sent": "But driven by the human policy.",
                    "label": 0
                },
                {
                    "sent": "So basically the human is doing the exploration here and then the algorithm takes over and there's an interesting effect there, because the human in this case I think was me, and I'm not a very good driver and.",
                    "label": 0
                },
                {
                    "sent": "So now the reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "The reason is that I wasn't really showing it how to drive right?",
                    "label": 0
                },
                {
                    "sent": "I was just showing it.",
                    "label": 0
                },
                {
                    "sent": "What happens when you are driving within that state space and what the rewards are and then the algorithm was using prioritized sweeping and was actually optimizing the value function and was then able to drive in a much better way.",
                    "label": 0
                },
                {
                    "sent": "But using the state representation that had been created during my rather well not very good driving, so that's an entirely different way of exploring based on humans so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second lesson, if you like, well, maybe I'm not sure what the lesson here is, but I it's that there are ways of exploring which don't necessarily need to be smart.",
                    "label": 0
                },
                {
                    "sent": "It can be enough if you have an agent who leads you through the state space and then the algorithm can learn the dynamics and the rewards and the structure of that space, and then can learn a policy much better than the original expiration policy.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's one of my favorite topics that I've been working on for a long time, which is computer go and it also offers a few lessons.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um in in exploration.",
                    "label": 0
                },
                {
                    "sent": "This is a recent game that we published on Xbox Live Arcade.",
                    "label": 0
                },
                {
                    "sent": "It's called the path of go.",
                    "label": 0
                },
                {
                    "sent": "If you happen to have an Xbox, I can only encourage you to download it and try it, so this is a game in which we basically used a lot of the insights of the computer.",
                    "label": 0
                },
                {
                    "sent": "Go community about how to build computer go agents based on Monte Carlo Tree search and put that into into an Xbox game which you can play.",
                    "label": 0
                },
                {
                    "sent": "Actually the other function was to teach people how to play go becausw.",
                    "label": 0
                },
                {
                    "sent": "We had to assume that most people are on Xbox Live, didn't actually know how to play goal.",
                    "label": 0
                },
                {
                    "sent": "So that's why we also paid some.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention to graphics and stuff.",
                    "label": 0
                },
                {
                    "sent": "So in case there's anyone among you who doesn't know about the game, it's a very old game, just an estimate of how old it is originating in.",
                    "label": 0
                },
                {
                    "sent": "China has over 60 million players worldwide.",
                    "label": 0
                },
                {
                    "sent": "There's two players, black and white.",
                    "label": 0
                },
                {
                    "sent": "Here you see a little render to go board this.",
                    "label": 0
                },
                {
                    "sent": "Actually in 9 by 9 board, which is the size of board used for beginners.",
                    "label": 0
                },
                {
                    "sent": "But the real game is played on a 19 by 19 grid and the rules are fairly simple.",
                    "label": 0
                },
                {
                    "sent": "For example, if it's white turn, white can places stone here on the board on an empty vertex.",
                    "label": 0
                },
                {
                    "sent": "After that it would be black, Stern, and so they take turns placing their stones on these vertices.",
                    "label": 0
                },
                {
                    "sent": "There's a capture rule, so if white plays another stone here, then these three black stones are entirely surrounded by white stones and would be taken off the board.",
                    "label": 0
                },
                {
                    "sent": "And the goal of the game is to gather territory.",
                    "label": 0
                },
                {
                    "sent": "So Blacks territory would roughly be this area here because it's surrounded by black whites territory would be this.",
                    "label": 0
                },
                {
                    "sent": "I would estimate that white is probably in a better position at this point and would win the game.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what's interesting about computer go?",
                    "label": 0
                },
                {
                    "sent": "You're probably aware that quite some time ago.",
                    "label": 0
                },
                {
                    "sent": "Gary Kasparoff was beaten by Deep Blue by IBM Stress Computer and that was a very impressive achievement, although of course people afterwards said, yeah, you know how?",
                    "label": 0
                },
                {
                    "sent": "How is that intelligent?",
                    "label": 0
                },
                {
                    "sent": "It's just research, but the fact remains that the best go programs cannot beat strong amateurs.",
                    "label": 0
                },
                {
                    "sent": "And it was even worse few years ago when this would have said the best goal.",
                    "label": 0
                },
                {
                    "sent": "Programs cannot even beat average or weak amateurs.",
                    "label": 0
                },
                {
                    "sent": "Now I had to modify this a little bit.",
                    "label": 0
                },
                {
                    "sent": "Because they have improved a lot and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's first look at what the problem with computer go is in comparison to chess.",
                    "label": 1
                },
                {
                    "sent": "The problem there are basically 22 problems.",
                    "label": 0
                },
                {
                    "sent": "We want to build a game tree and we would like to do Min Max search on this.",
                    "label": 0
                },
                {
                    "sent": "We know that that would eventually lead to the best solution if we had infinite compute power.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that the branching factor for goal is roughly 200 on average where it is maybe 20 to 35 in chest.",
                    "label": 0
                },
                {
                    "sent": "So the game Tree of Goal is much bigger than the game Tree of chess, so that makes it harder.",
                    "label": 0
                },
                {
                    "sent": "Second problem, if you want to do Game Tree search, you need a heuristic evaluation function.",
                    "label": 0
                },
                {
                    "sent": "That's the one that you evaluate at the leaf nodes, which are not really terminal nodes, but which are basically the horizon of your search depth.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's very hard to evaluate goal positions.",
                    "label": 0
                },
                {
                    "sent": "In chess.",
                    "label": 0
                },
                {
                    "sent": "You can look at the safety of the King and the amount of material the players have, and you can come up with a rough estimate in go.",
                    "label": 0
                },
                {
                    "sent": "There's these stones spread.",
                    "label": 0
                },
                {
                    "sent": "Over the board.",
                    "label": 0
                },
                {
                    "sent": "Very hard to do.",
                    "label": 0
                },
                {
                    "sent": "Everything is connected to everything else.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Over the past, maybe five years.",
                    "label": 0
                },
                {
                    "sent": "This methodology, called Monte Carlo Tree Search, was developed to address.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem and there were a few key insights that led to this and let me just quickly characterize what they are.",
                    "label": 0
                },
                {
                    "sent": "This guy band Brickman in 1993, figured out something really interesting about the game.",
                    "label": 0
                },
                {
                    "sent": "If you take a position of go and you play randomly from that position and times and you observe that K times black wins on my end minus K times white wins, then there is a tiny signal in this count which.",
                    "label": 0
                },
                {
                    "sent": "Tells us about how good the original position was for black or white, so there might be a like a 51% winning chance for black or 49% for white and that will signal contains some truth in that black probably had a slightly better position.",
                    "label": 0
                },
                {
                    "sent": "That's something this Martin Brickman figured out.",
                    "label": 0
                },
                {
                    "sent": "Then a second step.",
                    "label": 0
                },
                {
                    "sent": "Was two to this idea that maybe we shouldn't just randomly play out, but we should buy us our playouts towards better moves.",
                    "label": 0
                },
                {
                    "sent": "And you know, in our random searches give higher probabilities to better moves and.",
                    "label": 0
                },
                {
                    "sent": "This formalization of this technique basically is UCT and we have one of the authors here.",
                    "label": 0
                },
                {
                    "sent": "Java today will not talk about this, I think, but I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a beautiful paper and I recommend it to everyone here.",
                    "label": 0
                },
                {
                    "sent": "And then the third inside that really made this work was that people realize that the move order in goal isn't actually so crucial.",
                    "label": 0
                },
                {
                    "sent": "So if in a random play out you first play this move and then play that move, or vice versa in most of the cases doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "And that means that you can.",
                    "label": 0
                },
                {
                    "sent": "You can make very fast estimates by ignoring the move order.",
                    "label": 0
                },
                {
                    "sent": "And finally you can use all kinds of prior knowledge in order to bias your search.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you a little idea of how this works.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have this position here, and we'd like to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Um, how good this position is for black or for white?",
                    "label": 0
                },
                {
                    "sent": "Then we could do a random player.",
                    "label": 0
                },
                {
                    "sent": "This actually a very peaceful one that I chose here and then this random playout leads to a terminal position groups.",
                    "label": 0
                },
                {
                    "sent": "Yeah leads to a terminal position.",
                    "label": 0
                },
                {
                    "sent": "A terminal position being one were no legal move can be made anymore, and this leads to a territory hypothesis, namely under display.",
                    "label": 0
                },
                {
                    "sent": "Some of these points end up being white territory and some end up being black territory, and now do that 10,000 times or 100,000 times and you will get an estimate of what the territory distribution.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be under random play.",
                    "label": 0
                },
                {
                    "sent": "And so here's the here's the strategy that you could then use.",
                    "label": 0
                },
                {
                    "sent": "You could make try out all of the moves that are available in your given position.",
                    "label": 0
                },
                {
                    "sent": "You could do random place from each one down there, and you could just pick the best one, the one that gives you the highest number of wins in these random play hours.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But of course, the smarter thing to do is to keep track of which moves.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Farm how well you do different playoffs but you build up this little game.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We up here and in these internal node nodes you save some information about how how the chances are for this.",
                    "label": 0
                },
                {
                    "sent": "If you play this note to win or to lose, which is basically the counts down here and then.",
                    "label": 0
                },
                {
                    "sent": "Of course the question that you need to answer is once you have some of these is which of these moves should I try next?",
                    "label": 0
                },
                {
                    "sent": "And that's the familiar bandit problem you Now view.",
                    "label": 0
                },
                {
                    "sent": "Each of these branches each of the moves you can make as a bandit.",
                    "label": 0
                },
                {
                    "sent": "You can pull and figure out how much payoff that one gives you, and so basically the whole tree becomes a collection of little bandits, that which are independent, which are Inter dependent, of course.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then you can for example, use the UCB one algorithm by hour at all, which which uses an upper confidence interval to determine which move to make.",
                    "label": 0
                },
                {
                    "sent": "So it takes an optimistic estimate.",
                    "label": 0
                },
                {
                    "sent": "It basically says that you take the arm which has given you the best average payoff.",
                    "label": 0
                },
                {
                    "sent": "That's this extra bar, but you have this additional term where N is the number of visits to a node.",
                    "label": 0
                },
                {
                    "sent": "And JS is number of times where you played Move J in that node and what this will do is that it will make sure that each of the moves will still be explored sufficiently often, while at the same time you are putting more probability into your more often exploring the moves that are actually more promising in the sense of their payoff so far.",
                    "label": 0
                },
                {
                    "sent": "And in this use it paper it was shown that this still leads to sufficient exploration despite the fact.",
                    "label": 0
                },
                {
                    "sent": "That, of course these pay offs are drifting because if you have a band it up here and you're trying to make that choice, then there's going to be another bandit further down here where you have to make another choice.",
                    "label": 0
                },
                {
                    "sent": "And so this the pay offs for this band, it will actually drift depending on what happens down here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was an incredibly successful push for computer going.",
                    "label": 0
                },
                {
                    "sent": "You can't imagine this field which has a lot of very nice and really friendly people working in it kind of amateurish field in many ways, but it's a nice community and these people had been agonizing over 20 years.",
                    "label": 0
                },
                {
                    "sent": "How to make better go programs.",
                    "label": 0
                },
                {
                    "sent": "And of course, these chess guys were always ahead in some sense, and they beat 'cause power off and didn't go.",
                    "label": 0
                },
                {
                    "sent": "They couldn't make progress, and this was really the breakthrough.",
                    "label": 0
                },
                {
                    "sent": "And some great things happen.",
                    "label": 0
                },
                {
                    "sent": "For example mogul, which was the first program to adopt this strategy, beaten 8 and professional with nine handicap stones on its side.",
                    "label": 0
                },
                {
                    "sent": "So you can bias the odds, so to speak by giving it advantage to the program.",
                    "label": 0
                },
                {
                    "sent": "But this program is also now on pretty much on par with professionals on 9 by nine goal on the small version, at least when you give it 800 processor so well, it's actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "Even a non standard hardware now.",
                    "label": 0
                },
                {
                    "sent": "And now in a lot of other programs are following in its footsteps, and I think Morgan might not even be the strongest anymore.",
                    "label": 0
                },
                {
                    "sent": "At this point.",
                    "label": 0
                },
                {
                    "sent": "There's also commercial applications.",
                    "label": 0
                },
                {
                    "sent": "But one thing I would like to mention, which I found really interesting, the best programs now don't use UCB anymore, they just use the average payoff achieved in the node as their criterion for picking the best move.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the lesson I would like us to learn from this is that theory can really be a powerful guide, and in this case really this the UCT paper induced this whole development of Monte Carlo go and of successful goal programs.",
                    "label": 0
                },
                {
                    "sent": "But in the end there's often a deviation from Thierry which just makes things work better.",
                    "label": 0
                },
                {
                    "sent": "And I think then that really means that the balls back in theories court to explain why that might be the case.",
                    "label": 0
                },
                {
                    "sent": "OK, that's",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In three, let's talk a little bit about web search.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "In web search you may be familiar with the standard techniques that are being used.",
                    "label": 0
                },
                {
                    "sent": "The traditional information retrieval paradigm is Boolean retrieval.",
                    "label": 0
                },
                {
                    "sent": "You type in a few keywords and you get documents delivered that contain these keywords that you can do logical combinations of them.",
                    "label": 0
                },
                {
                    "sent": "So this quickly hit its boundaries, but then people hit upon this idea of static rank, for example, page rank is such an idea.",
                    "label": 0
                },
                {
                    "sent": "So independent of the Cleary, we can order the pages according to some measure.",
                    "label": 0
                },
                {
                    "sent": "For example, how high their page rank is, how high they are in degrees or other measures.",
                    "label": 0
                },
                {
                    "sent": "And so we get this presorting of the pages, and then if we use Boolean retrieval, we already get a preference order and we can better documents that both satisfy the bullying.",
                    "label": 0
                },
                {
                    "sent": "Theory and have a high static rank and then of course.",
                    "label": 0
                },
                {
                    "sent": "The search engines are using machine learning to learn ranking models from human labels of query URL pairs, so there's a lot of people whose day job it is to look at query URL pairs and say if that URL is a good answer for that particular query, and then you can use that data to learn machine learning models.",
                    "label": 0
                },
                {
                    "sent": "Now this is kind of the traditional methodology if you like if there's such a thing in web search and the challenges with these human labels are they cost a lot of money.",
                    "label": 0
                },
                {
                    "sent": "You actually have to pay these people to do this.",
                    "label": 0
                },
                {
                    "sent": "It's unclear if these human labels reflect audience preference, because those are people who are professional label as they have this Handbook of how to label stuff guidelines, basically.",
                    "label": 0
                },
                {
                    "sent": "But how does that really reflect what people like knew trends may not be picked up quickly enough, so you can imagine that if there's something you coming along, how are the label is going to know about it?",
                    "label": 0
                },
                {
                    "sent": "How is it going to get into the system is also difficult to model personal preferences if you have professional labels, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, in fact they should keep their personal preferences out of the process, but on the other hand, maybe the search engine would like to serve people with their personal preferences.",
                    "label": 0
                },
                {
                    "sent": "And also there's really no relation to user engagement and page composition when you do judgment purely on clear URL pairs.",
                    "label": 0
                },
                {
                    "sent": "You're not taking into account the way the results are presented on the page, and if users would be engaged by that kind of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presentation so as a consequence.",
                    "label": 0
                },
                {
                    "sent": "We would like to use user feedback and clicks.",
                    "label": 0
                },
                {
                    "sent": "And the advantages are really that much more data is available, and it's much less costly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can incorporate dynamics of users and documents, so if users change their taste, if this new thing is coming up, if documents disappear or come up, you can take that into account.",
                    "label": 0
                },
                {
                    "sent": "It's useful personalization because those people your customers are actually the ones who give you the data instead of this group of judges.",
                    "label": 0
                },
                {
                    "sent": "Data may even be available for small collection or some niche scenarios, as long as there's people using a collection, you will get this type of data.",
                    "label": 0
                },
                {
                    "sent": "So if there's a totally unpopular collection, yes, there will be no user data.",
                    "label": 0
                },
                {
                    "sent": "But then why would you want to do something with it anyway?",
                    "label": 0
                },
                {
                    "sent": "So this is then the classical online learning scenario, because you might want to choose documents to the user for their immediate benefit, because that satisfies their needs or you want to choose documents to show to improve your.",
                    "label": 0
                },
                {
                    "sent": "Your policy of finding out about the documents and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Delivering better documents.",
                    "label": 0
                },
                {
                    "sent": "And I would just want to point out three interesting areas that come up when you deal with these kind of real world bandit problems and some related literature.",
                    "label": 0
                },
                {
                    "sent": "So the first interesting problem is that you would want to generalize across documents.",
                    "label": 0
                },
                {
                    "sent": "The problem is there are so many documents.",
                    "label": 0
                },
                {
                    "sent": "If you just view them for example as independent bands, Bendit arms, that's relatively hopeless thing to do.",
                    "label": 0
                },
                {
                    "sent": "And the question is then, can we devise some kind of feature representation for the documents and use that feature representation in our bandit learning?",
                    "label": 0
                },
                {
                    "sent": "So essentially that means that if you pull a banded arm here, it tells you something about the neighboring bandits and for example, one way in which this has been formalized is in the metric pendence work, by Robert Kleinberg at all, where they make smoothness assumptions on the pay offs depending on the metric distance.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that if two bandits or two documents are similar, then the payoff of showing them or pulling that arm will be similar.",
                    "label": 0
                },
                {
                    "sent": "Related by this Lipschitz condition, and then you can infect, and that's what they've shown.",
                    "label": 0
                },
                {
                    "sent": "Device in some sense, an adaptive discretization of this space cover.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And in that way, zoom in on the interesting bits of the policy space.",
                    "label": 0
                },
                {
                    "sent": "So finding the hive.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your documents, another interesting problem is the diversity of search results, and here the insight is that when you show 10 blue links.",
                    "label": 0
                },
                {
                    "sent": "From N documents, then you really have 10 choose K * K factorial many, many ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "First duties, choose the K links.",
                    "label": 0
                },
                {
                    "sent": "Then you can or to choose the order in which you show them and this work on ranked bandit algorithms for example by by Radlinski here addresses that problem to use the problem structure that this actually is a ranking they have one way they instantiate one bandit per slot.",
                    "label": 0
                },
                {
                    "sent": "For display slot and can then learn.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficient way.",
                    "label": 0
                },
                {
                    "sent": "Finally, there's the aspect of dynamics and mortality, and I just want to briefly point you to the paper on mortal multi armed bandits.",
                    "label": 0
                },
                {
                    "sent": "This models the problem.",
                    "label": 0
                },
                {
                    "sent": "That's also specific to web search and advertising that some of the options just go away.",
                    "label": 0
                },
                {
                    "sent": "You know, an ad may not be available forever in the bandit setting.",
                    "label": 0
                },
                {
                    "sent": "Once you've found that Grey bandit, you can cash in on it forever, right?",
                    "label": 0
                },
                {
                    "sent": "And that's just not the case in these real world problems, and you can also formalize that.",
                    "label": 0
                },
                {
                    "sent": "And the device algorithms in this case, which find good enough arms and start exploiting much earlier, which is the right thing to do in these scenarios, and that's also shown in this paper.",
                    "label": 0
                },
                {
                    "sent": "Chakrabarti ET al 2.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And eight lesson here theory can be enriched to provide guidance in more differentiated practical scenarios.",
                    "label": 0
                },
                {
                    "sent": "So coming from this beautiful Gittins index, pure scenario of the bandit, we can move into more complex territory and model more interesting situations that correspond to real world scenarios in web.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search and advertising.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, last thing exploration in online advertising.",
                    "label": 0
                },
                {
                    "sent": "I think I have very little time for this.",
                    "label": 0
                },
                {
                    "sent": "Let me assume that you know how how it works, how this online search advertising works.",
                    "label": 0
                },
                {
                    "sent": "The user puts in a query, advertisers bid for that query.",
                    "label": 0
                },
                {
                    "sent": "We multiply that bid with a click through rate to get an expected monetization.",
                    "label": 0
                },
                {
                    "sent": "And then the advertisers are ordered according to this.",
                    "label": 0
                },
                {
                    "sent": "Expected monetization you can see that here, but they pay according to the generalized second price auction.",
                    "label": 0
                },
                {
                    "sent": "The key thing here is that for both the decision what to show in which order and how much to charge, it's crucial to estimate these click through rates here these pies and they are important to have good user satisfaction there important to monetize appropriately, and so that's really an.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Important thing and paid search advertising and we have this Bayesian system at predictor that estimates these.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "80 hours now what I would like to just briefly talk about is the problem of putting such a learning system in this case here into the into the advertising environment into the close loop.",
                    "label": 0
                },
                {
                    "sent": "Let me show what I mean.",
                    "label": 0
                },
                {
                    "sent": "So a user.",
                    "label": 0
                },
                {
                    "sent": "Issues a query and we get may get user information that is made available to the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's in process of its election and edge filtering.",
                    "label": 0
                },
                {
                    "sent": "Certain advertisers have bid for query words and the system figures that out and makes that available to the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "The learning algorithm then provides these click through rates estimates which lead to add ranking and pricing and so on together with the bids that the Edward Teller Tasers have given.",
                    "label": 0
                },
                {
                    "sent": "Then those ads are shown to the user and the user makes their choice.",
                    "label": 0
                },
                {
                    "sent": "You know, usually they don't click on edge, that's the most frequent case, but if they do, they choose an ad and this label.",
                    "label": 0
                },
                {
                    "sent": "Then the fact that they clicked on something or didn't click on something gets fed into the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the scenario.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that we will then use this learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "To decide what the city, RSR and so we will essentially filter the training data set according to CTR will only show ads that will have sufficiently high Cdr because that's the interest of the advertisers.",
                    "label": 0
                },
                {
                    "sent": "That's in the interest of the search engine is also in the interest of the user.",
                    "label": 0
                },
                {
                    "sent": "So in the next step the training sample will only consist of a biased subsample of the original one and then.",
                    "label": 0
                },
                {
                    "sent": "We will have the same scenario and this loop will keep going on and this loop can be very problematic for a learning algorithm and we need to be very careful to reweigh the examples for training to do sufficient exploration in order to avoid a scenario in which certain ads will not be shown.",
                    "label": 0
                },
                {
                    "sent": "We will not be shown anymore and are therefore kind of disappearing in a dark pool, and there's also a number of other problems, but this causal loop, I think will be more and more prevalent in a lot of applications because it occurs whenever you use what you've learned to make decisions and where those decisions then influence what you later learn from.",
                    "label": 0
                },
                {
                    "sent": "And I think that's really quite a common scenario.",
                    "label": 0
                },
                {
                    "sent": "It's very common in these online services, but of course it's also the key problem in.",
                    "label": 0
                },
                {
                    "sent": "More general scenarios such as reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I have an example, I'll skip that how that leads to problems.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing that we do use is this Thompson heuristic for for exploration.",
                    "label": 0
                },
                {
                    "sent": "The click through rate algorithm that we have is a Bayesian algorithm, so it doesn't give us a click through rate.",
                    "label": 0
                },
                {
                    "sent": "It gives us a distribution over click through rates and the example you can see here is that for one add, the algorithm may give us this distribution here, which has an average clickthrough rate of 25%, but large variance left, whereas for some other ad.",
                    "label": 0
                },
                {
                    "sent": "That we've seen more often, the average may be higher, but there's less uncertainty left, and the idea of this Thompson heuristic for exploration is to sample from these distributions such that most of the time the better one might you know, the one on average, better might come out as the one that gets shown, but every now and then, because of the higher variance, this other Ed with the lower average will also get shown and will be able to reduce the variance on our estimate.",
                    "label": 0
                },
                {
                    "sent": "And hence either find out that it is in fact better than the red one or kind of get rid of it and and show it less often in the future.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to emphasize the 5th lesson, I think this causal loop will be an interesting beast to study, because so far we often looks at this better scenario of some classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "But really, if you start using the output of that algorithm, and that's why you trained it in the 1st place, right?",
                    "label": 0
                },
                {
                    "sent": "And that influences the future of your training set generation, then there's a bunch of very interesting problems that you need to.",
                    "label": 0
                },
                {
                    "sent": "Take care of in order to make this work.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here are just the five lessons I think I'm out of time, is that right?",
                    "label": 0
                },
                {
                    "sent": "Great yeah, so these are really just a summary of the five lessons.",
                    "label": 0
                },
                {
                    "sent": "Remember the fighters epsilon greedy can be pretty good.",
                    "label": 0
                },
                {
                    "sent": "Exploration is not always what it seems you can do really stupid exploration.",
                    "label": 0
                },
                {
                    "sent": "Even me driving a virtual car and it can lead to good algorithmic performance.",
                    "label": 0
                },
                {
                    "sent": "I think computer goal is a beautiful example where Thierry kind of leads the way and show us what can be done.",
                    "label": 0
                },
                {
                    "sent": "Great success in practice.",
                    "label": 0
                },
                {
                    "sent": "But then practitioners do something different and I think Thierry could pick that up and learn something about new problems settings from the world of search.",
                    "label": 0
                },
                {
                    "sent": "There are all these interesting differentiated problems that deviate from the standard bandwidth settings, but can be studied in the theoretical context, and I think that's a very promising area.",
                    "label": 0
                },
                {
                    "sent": "And finally, watch out for that cause loop and don't be caught up by it.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                }
            ]
        }
    }
}