{
    "id": "3ex6wjoyvksy36g6ekzac5i3k5mkgig3",
    "title": "Learning networks of stochastic differential equations",
    "info": {
        "author": [
            "Jos\u00e9 Bento Ayres Pereira, Boston College"
        ],
        "published": "March 7, 2016",
        "recorded": "December 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/netadis2015_bento_ayres_pereira_equations/",
    "segmentation": [
        [
            "So this is joint work with Andrea Montanari and Morteza Ebrahimi, and Ray is actually going to talk about half of the work that I will talk about here in the afternoon.",
            "And so let me start with something similar to all of us, so these are probably timation where we want to.",
            "Estimates from a known parameter from a distribution from observed samples.",
            "Anne."
        ],
        [
            "These you know, fundamental problem in main engineering applications like signal processing, quality control, control theory.",
            "And, you know in a class."
        ],
        [
            "Setting these data as being independent samples, this data is a low dimensional vector and we have many more observations than unknowns."
        ],
        [
            "You know, and maybe the simplest example is the case of a linear regression.",
            "And in this talk."
        ],
        [
            "We're going to defer from this setting, so our data is going to be the trajectory of a stochastic differential equation, and I wouldn't be assuming is to classification of the following form, where F is called drift coefficient that is parameterized by some unknown parameter Theta, and these be.",
            "Here's a Brownian motion.",
            "These state vector X of T are high dimensional vectors.",
            "You cannot see it OK?",
            "You can see it now.",
            "Great and.",
            "And this state is going to be a very large graph.",
            "In our case OK, and just to illustrate, what do I mean by graphing?"
        ],
        [
            "Setting, so consider the case of the following simple linear stochastic differential equation.",
            "Where the rate of change of the state variable X of I is proportional to the difference between the neighboring variables XJ to its own value XI.",
            "In the graph representation.",
            "So every node in the graph is associated to a state variable X of I.",
            "And there's also a damping term that is proportional to M. OK, and you know the problem that interested in addressing this talk is to understand.",
            "OK, given a setting for example like this, and given that I observe you know the trajectory evolution of this trajectory in time."
        ],
        [
            "You know how much data do I need to be able to give some guarantee in terms of the amount of time that I need to observe my system to guarantee that I recover this graph with probability calls to one with high problems?",
            "OK, and just to kind of motivate where these kind of tools could be useful?"
        ],
        [
            "This is an example from biology, by the way.",
            "I'm actually looking for a postdoc to work with me on applying this work precisely to a large.",
            "Biological problem collaborations for other bosses from University around the US.",
            "So if you're interested, please come to me after during the coffee break and so he is one of the Organism that's been mostly studied by biologists."
        ],
        [
            "Is about 6000 genes."
        ],
        [
            "A Thunder of these genes control the cell cycle division process and biologists are interested in understanding how these different genes interact and Co regulate."
        ],
        [
            "And they want to come up with nice pretty representations like the following, where we have a network.",
            "Every node in the network represented gene and green arrows represent activation.",
            "So one gene expressing itself favors the expression of some other gene.",
            "And Red Arrows represent repression.",
            "And then they do some experiments in the lab and they get."
        ],
        [
            "Is very noisy data.",
            "This time series data that is actually not very long.",
            "It is and how many data points, but maybe 30 or 40 data points.",
            "And this is the expression level of different genes in time and then the idea is."
        ],
        [
            "So can we actually go from this data?",
            "These noisy time series data is actually not very long to this kind of graph representation of who is affecting who in this dynamical networks OK?",
            "You know, one way of doing this or that?",
            "One way to reduce this problem to a kind of a learning structure in stochastic differential equations is to use."
        ],
        [
            "These action mass equation again this is simplified model but just to kind of illustrate the point.",
            "So imagine we have a chemical equation wherein be reactive C&B then is action mass.",
            "Equation tells us that the rate of change in the concentration or expression level of.",
            "And the A is proportional to the product of the concentrations of C&D and also proportional concentrations of A&B.",
            "And in this case this noise term.",
            "Accounts both for measuring error and also for modeling certainty.",
            "Becauses definitely not super accurate model.",
            "And now what we can do is then we can come up with that."
        ],
        [
            "General permit rise model.",
            "Where we stack on that tall vector on the rights, all possible pairs of state vectors X one X2 X one X3 and also maybe individual 1X1X2X3, and we have a long vector of parameters Theta that tells us whether or not a certain pair is going to interact and affect expression level of gene X of I.",
            "In this particular case.",
            "For example if Theta I-11 is not zero, then it means.",
            "According to this model that Gene 2 ones in three Co regulate the expression level of gene I OK?",
            "So then we reduce this problem."
        ],
        [
            "To know the problem of understanding the, you know the relation between these genes to the problem of learning the support of these unknown parameter Theta and throughout this talk you know support actually is going to mean sign support.",
            "So not only can recover the support but also the sign of entries of these vector Theta calling.",
            "If you have more conditions on our theorems you can recover the sign if you relax more these conditions we can recover support or a covering of the support.",
            "And.",
            "And so this is the problem at hand.",
            "And so now there are two questions that we want."
        ],
        [
            "Stress, which is, first of all, how do we fit these kind of models to data and you know how much data do we need to actually be able to say something about?",
            "Whether or not we are recovering this support?",
            "You know the quality of interest.",
            "And just give up."
        ],
        [
            "Presentation is this thing that I call sample complexity.",
            "So imagine that then we have the general form of our stochastic differential equation.",
            "Some nonlinear function, big F, the drift coefficient that is parameterized by some large graph data, and we want to come up with an algorithm that received the sample trajectory is continuous simple trajectories and outputs graph that is a good approximation of the real underlying graph of our model, yes.",
            "During this problem here we I mean well.",
            "You're asking about the problem of whether I can deal with unknown, observed and observed states.",
            "Maybe?",
            "Yes, the vector X yes, so you measure you form this thing you observe.",
            "Let's say you have 10 genes.",
            "You observe the evolution of tangent and you can form.",
            "The product is just multiplying data.",
            "So you can.",
            "You can do that so you can extend your feature vectors, right?",
            "So that's fine.",
            "So you can make this thing as large as you want.",
            "We can consider other kind of basis functions if you will more complicated things.",
            "It still works OK. OK. Again, so the problem of interest is understanding how much data do we need to be able to recover this graph with high probability?"
        ],
        [
            "OK, so you know first guess might be well, since these projectors are continuous and any finite interval as any fit number of points.",
            "Maybe no, I only need an arbitrarily small amount of information to be able to solve this problem.",
            "Yeah, that's kind of a first case."
        ],
        [
            "But maybe if the number of parameters in my Theta vector method vectors P dimensional, maybe I need something on the order of P?",
            "But may."
        ],
        [
            "We, you know, since we know from you know this sparse linear regression literature that, for example, to recover Theta in a setting in a linear regression setting if there is sparse telling about log P samples or PS dimension of our unknown vector.",
            "So maybe."
        ],
        [
            "I only need like log P order of observation, time to recover these these this network right?"
        ],
        [
            "OK, and so.",
            "In this talk, you know I'm going to be actually talking all the theoretical results are going to be about linear stochastic special equations.",
            "If you look at our papers, actually we have a general lower bound on the on the sample complexity that actually works for both linear and nonlinear stochastic fragile equations.",
            "But I'm going to state the theorems in the most simple way possible, just to make things more clear.",
            "You know, and you know the first thing that one can point out."
        ],
        [
            "OK, this is actually very much related to the problem of this general field of graphical models, where we want to learn a graph from data, right?",
            "So you might just think OK. Why don't I just look into the literature?",
            "Maybe there's something that I can use and I can solve my problem without a lot of effort, which would be."
        ],
        [
            "Great.",
            "And so let me just talk about some prior work.",
            "So the first thing you might try to do is the following.",
            "So let the system converter stationary, stationary state is going to some stationary distribution.",
            "Some normal stationary distribution given linearity and given the Brownian motion that is driving our process.",
            "And you might sample your your state vector sufficiently spaced in time to consider these samples independent, and then you might say, OK, we know that."
        ],
        [
            "It's.",
            "We know that the support of inverse covariance matrix represents conditional independence among different components of our state vector.",
            "So maybe I can just use a method to estimate support of the inverse covariance matrix and everything will be fine, right?",
            "And there's some?"
        ],
        [
            "Literature in this area.",
            "Particularly, this thing called graphical.",
            "Also, let's solve this problem.",
            "But you know?"
        ],
        [
            "Doesn't really solve our problem for two reasons.",
            "Becausw.",
            "First, this is stationary covariance matrix as less information than our unknown parameter Theta, and I'm going to give an example of that and 2nd we don't actually have independent samples.",
            "Or we could have them if we specially if we space sufficiently spaced in time.",
            "But we will definitely be losing information right?",
            "And just to kind of give you an example of why these covariance mate."
        ],
        [
            "This situation in Theta here is there are two linear systems.",
            "They don't want to two and you can see that they both have different supports and they both have different sign supports, but actually have exactly the same stationary covariance matrix.",
            "So second approach you might."
        ],
        [
            "Try is look again at our linear stochastic special equation and observe that it resembles very much like a normal linear regression problem.",
            "If I stack these DT DX vectors in a Big Y and I stuck my projectors in a large matrix X so I can read wise it could X Theta plus annoys an so."
        ],
        [
            "The problem then reduce itself took a given Y&X.",
            "That is fine.",
            "This linear equation that is perturbed by noise.",
            "Can I recover the support of my unknown parameter Theta?",
            "And again there's been some work done on that."
        ],
        [
            "I guess we all familiar with the Lost Soul and work that followed.",
            "Again, we cannot really apply or directly apply this literature becausw."
        ],
        [
            "Now the roles of Y&X obtained from these trajectories are not IID.",
            "OK, So what is challenging while the challenge can be?"
        ],
        [
            "Reduced to the following picture.",
            "I can sample my trajectory sufficiently space in time to obtain independent samples and just use existing literature.",
            "But then I'm going to be losing information and I can try not lose information sample very finely in time, but then I'm going to be introducing correlation between the samples and then it's not clear whether existing literature works OK.",
            "I'll also like to mention some work actually."
        ],
        [
            "Then directly on HTS.",
            "So initially you know I think this thing works in from the field of economics and people were interested in understanding and estimating these parameters from dynamical systems, and they used likely methods for continuous observations.",
            "They also have."
        ],
        [
            "Some likely with methods for actually sampled.",
            "Trajectories and but you know the problem with these methods."
        ],
        [
            "That focuses mostly on more dimensional setting, and I mean this maximum likelihood method would not give you a selection procedure and there will come normal with no guarantees.",
            "Basic basically basically the kind of guarantee that it will be asymptotically in T in the sense that OK, if I observe my trajectories for infinite amount of time, then our estimator is consistent.",
            "But that's not what we're really interested in.",
            "We want a sharper characterization of exactly how much information we need to be able to recover the support with high probability.",
            "OK, so let me."
        ],
        [
            "Just the algorithm here and introduce algorithm.",
            "I'm going to go a little bit beyond linear setting because the algorithm algorithm actually can be applied to these slightly modified setting where the rate of change of every state variable X or Y is proportional to some function of the state variables.",
            "This function can be nonlinear, but the dependency on the parameter Theta is assumed to be linear.",
            "Again, this data still represent some sort of graphical relationship or dependency between the variables so.",
            "Look at this example in the bottom where the rate of change of I is proportional to some function of X3 and is also proportional to some function of X one X2 and I can draw the graph of dependencies.",
            "OK, and since this HT the dependency with data is still linear, we can try to use.",
            "You know, the same."
        ],
        [
            "We've been using for linear regressions.",
            "You know when I've derivation Orrery derivation, if you wanted the last so."
        ],
        [
            "To give you an expression like this, you want to minimize the integral of the square difference between the DXS and.",
            "The you know the data GS of the States plus some penalty term to induce sparsity, right?",
            "OK, it turns out that is not quite the right way of doing it.",
            "You need some sort of stochastic calculus."
        ],
        [
            "Here, but you know you can actually do the math writes you know, and after you do the math right, you get the following expression and you see that there is actually going to use this thing here.",
            "There's these little integral here, which is stochastic integral, but you know if you want to actually do it in practice in the computer.",
            "It's actually very simple.",
            "And expression might look complicated, but in the end of the day is just a quadratic function, or at least these this term here.",
            "Which we call the likelihood term is just a quadratic function of our unknown parameter Theta.",
            "OK, so this is basically the lost soul in that kind of a hidden way.",
            "OK."
        ],
        [
            "And So what is the problem now?",
            "Or can we characterize or not?",
            "What is simple complexity of this problem?",
            "How much information I need to be able to guarantee that with probability I'm recovering the right support of this little late?",
            "So I'm going to state.",
            "Specializations of our general theorem.",
            "Again, I'm not going to take the general theorem because expresses it look a bit complicated, and I think we will miss the point.",
            "And."
        ],
        [
            "Here's the simpler setting that I'm going to be talking about that I've already introduced.",
            "Its linear FT is even more special than linear.",
            "Steel is a linear steel on a graph where the rate of change of every node in the graph is proportional to the difference between its neighbors and its own value plus the damping factor proportional to M and so the first theorem states that.",
            "So if this graph is sparse.",
            "And it has a maximum degree Delta in all the nodes.",
            "Then if the graph is large, an aim is large, much larger degree then the sample complexity is upper bounded by M square, log P and lower bounded by M log B.",
            "Again, the theorem doesn't need these two.",
            "Assumptions here.",
            "There's a general.",
            "We have a general upper lower bound, but expressions look a bit.",
            "Ugly, so I just kind of represented in this setting here, but the point that we have some sort of sharp characterization in terms of dependency of the size of the graph in terms of the dependency on this damping factor is not really sharp.",
            "There's an extra M on the right, maybe I think can be improved.",
            "And we also have a similar theorem when the."
        ],
        [
            "Graph is not sparse but dense.",
            "In this case we do have a sharp characterization.",
            "The sample complexity is basically proportional to M * P. OK. And so here's the general picture."
        ],
        [
            "Are actually if you actually have the expression, the full expressions.",
            "So this is the same for large M the behavior, so the lower bound grows linearly with M and upper bound grows quadratically with M times some log P. For small M, the lower bound actually converges to a constant wild upper bound diverges to Infinity, so there's some sort of issue to be solved here, so it's not clear whether systems where there's not a lot of damping, whether they are very hard to learn or they're not hard at all to learn.",
            "OK, so we can over well empirically, it seems that.",
            "They are hard to learn, but you know also depends on what kind of empirical test you do right?",
            "So we don't we don't have that gap closed quite yet.",
            "OK, so are they."
        ],
        [
            "Rila theorem does depend on some assumptions.",
            "Holding these assumptions are not very much different from the assumption that had been posed to obtain consistency in the Lost soul, so we need some sort of bound on the smallest output value of the entries of our matrix.",
            "We need some sort of stability of our dynamical system.",
            "We need some restricted convexity assumption and some sort of incoherence condition that is called in this setting actually representable condition OK, and just to give a general kind of a very high level idea of the proof.",
            "How do we obtain this?"
        ],
        [
            "Out of stuff.",
            "So first of all, we don't work with a continuous HTS.",
            "We discretize them and then we take the limit when they are discretization is definitely fine if you will and then the proof structure is very similar to the proof for the consistency of the last so introduced by zone you in 1006.",
            "But we do need some new concentration bounds.",
            "And so this concentration bounds are concentration bounds on.",
            "You know the size if you will, of the gradients of the likelihood term, just to recall it.",
            "Likelihood term is.",
            "This is the likelihood term here.",
            "OK, so we need some sort of.",
            "We need some sort of bounds on the gradient of the likely terminism bound on the Hessian of the likelihood term, and in the case of the Lost soul, both both the gradient, the empirical gradient, and these empirical Hessian could be expressed as linear combinations of independent random variables.",
            "In our case, they cannot, but it can be expressed as sort of a quadratic.",
            "Quadratic form like these where Z is a vector that has the vector of normal 01 variables.",
            "And then we need to analyze eigenvalues.",
            "All these are matrix and you know blah blah blah.",
            "But OK, I'm just not going to go into detail, but this kind of, you know the main innovation is, you know, kind of filling out these bounds and figuring out how to make these reductions and make everything work there.",
            "OK the rest and also this path through the limits also requires some care.",
            "OK, so let me just end up."
        ],
        [
            "By talking about some follow up work after our initial work on analyzing these Estes.",
            "There's been some work on autoregressive processes that are a bit different in our.",
            "There's a delay.",
            "You can see there's a delay here on the right.",
            "Obviously this thing can be reduced to our form, but you know these.",
            "Both uh than others.",
            "What they did is they specialize these inquiries.",
            "Conditions for the case of this auto regressive processes obtained tighter bounds on the sample complexity.",
            "There's also been some work on recovering this relationship between these components of this state.",
            "Vectors when we don't observe the state vector fully, there's some hidden variables, and here the idea is that you can somehow use a similar method where where you have a likelihood term plus a penalty, but now the penalty is actually two penalties.",
            "And L1 penalty to introduce sparsity and a nuclear non penalty to somehow impose a low rank structure on the remaining parts of what you're not observing.",
            "And now we need to increase conditions.",
            "We need one incoherence condition for the sparsity, and we need one incoherence condition for the low rank.",
            "Assumption and there's."
        ],
        [
            "Some work on a nonparametric estimation, some of them work actually done by Manford here.",
            "I think it was Nipsey 1013.",
            "The problem is not so much recovery of support or you know the structure of these tests, but just estimating the drift coefficient and the idea here is to use grousing processes and approximately EM algorithm and some work also then on actually estimating directly graphical processes where instead of having just.",
            "Prem Trizol linear SDE?",
            "We have Gaussian process and.",
            "You know, since the process is not printable, what we do is that we work with these.",
            "We work with these spectral density matrices, so we estimate the spectral matrices and we prove some guarantees on recovering.",
            "You know the structure of the inverse of this matrix.",
            "I just want to finish."
        ],
        [
            "The talk about the you know, an extension of our work for non linearized, which is kind of an open problem yet, but we do know that the algorithm applies, so we've used the algorithm to recover nonlinear season.",
            "It does work and we just don't know whether we can extend theory while the lower bounds hold for nonlinear astis the upper bounds, we need new theory to for the case of nonlinearities even for the case where the SD is nonlinear but still linear in the parameters.",
            "Data."
        ],
        [
            "And I just want to give you a quick exam."
        ],
        [
            "All over how these things look.",
            "So here's an example where I have a system of masses and Springs.",
            "So every node in our network now is a mass.",
            "Every edge is a spring that connected to masses and the whole system can be expressed using classical."
        ],
        [
            "Mix and in this particular case.",
            "Our functions GG functions that have these nonlinear function of the state are going to have one of three forms and our parameters Theta are going to indicate or parameterized the possibility of 1 mass being connected to another mass through some spring OK. And the problem here is then well, can I imagine it's the building and I put some sensors in the building and I want to see whether there's some sort of problem in the building structure, so I hit it with a hammer and the whole thing vibrates a little bit and I want to somehow recover the structure of the building."
        ],
        [
            "So here's a here's an example, so here's a structure mechanical structure of mass and Springs, and here is the on the top right is the sensors that are going to start moving a little bit, and this is physical structure vibrating, and as we get more and more data we can see it.",
            "Eventually we can recover the right structure for a mechanical system.",
            "When you can do more extensive numerical simulations and we can actually confirm that it does seem."
        ],
        [
            "That you know the sample complexity, at least empirically grows like log P, just like Carlo."
        ],
        [
            "Bound predicts OK, although again we don't have.",
            "We don't have a proof for that, OK?"
        ],
        [
            "So just to kind of summarize, we know that we can learn at least linear, Estes, sparsely nestis with much fewer samples than the dimension of our vector.",
            "We can also learn densest is, we know that our results, at least empirically, seems to extend to nonlinearities.",
            "We also know from our bounds that faster dynamical systems that correspond to larger M are harder to learn, is still an open problem to understand whether slow systems with small M. Are hard to learn or not because there's a kind of a exposing gap between our lower bound.",
            "And an upper bound OK, and so now I'll take some questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Andrea Montanari and Morteza Ebrahimi, and Ray is actually going to talk about half of the work that I will talk about here in the afternoon.",
                    "label": 1
                },
                {
                    "sent": "And so let me start with something similar to all of us, so these are probably timation where we want to.",
                    "label": 0
                },
                {
                    "sent": "Estimates from a known parameter from a distribution from observed samples.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These you know, fundamental problem in main engineering applications like signal processing, quality control, control theory.",
                    "label": 0
                },
                {
                    "sent": "And, you know in a class.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting these data as being independent samples, this data is a low dimensional vector and we have many more observations than unknowns.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, and maybe the simplest example is the case of a linear regression.",
                    "label": 0
                },
                {
                    "sent": "And in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to defer from this setting, so our data is going to be the trajectory of a stochastic differential equation, and I wouldn't be assuming is to classification of the following form, where F is called drift coefficient that is parameterized by some unknown parameter Theta, and these be.",
                    "label": 1
                },
                {
                    "sent": "Here's a Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "These state vector X of T are high dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "You cannot see it OK?",
                    "label": 0
                },
                {
                    "sent": "You can see it now.",
                    "label": 0
                },
                {
                    "sent": "Great and.",
                    "label": 0
                },
                {
                    "sent": "And this state is going to be a very large graph.",
                    "label": 1
                },
                {
                    "sent": "In our case OK, and just to illustrate, what do I mean by graphing?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting, so consider the case of the following simple linear stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "Where the rate of change of the state variable X of I is proportional to the difference between the neighboring variables XJ to its own value XI.",
                    "label": 0
                },
                {
                    "sent": "In the graph representation.",
                    "label": 0
                },
                {
                    "sent": "So every node in the graph is associated to a state variable X of I.",
                    "label": 0
                },
                {
                    "sent": "And there's also a damping term that is proportional to M. OK, and you know the problem that interested in addressing this talk is to understand.",
                    "label": 0
                },
                {
                    "sent": "OK, given a setting for example like this, and given that I observe you know the trajectory evolution of this trajectory in time.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know how much data do I need to be able to give some guarantee in terms of the amount of time that I need to observe my system to guarantee that I recover this graph with probability calls to one with high problems?",
                    "label": 0
                },
                {
                    "sent": "OK, and just to kind of motivate where these kind of tools could be useful?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example from biology, by the way.",
                    "label": 0
                },
                {
                    "sent": "I'm actually looking for a postdoc to work with me on applying this work precisely to a large.",
                    "label": 0
                },
                {
                    "sent": "Biological problem collaborations for other bosses from University around the US.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, please come to me after during the coffee break and so he is one of the Organism that's been mostly studied by biologists.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is about 6000 genes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A Thunder of these genes control the cell cycle division process and biologists are interested in understanding how these different genes interact and Co regulate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they want to come up with nice pretty representations like the following, where we have a network.",
                    "label": 0
                },
                {
                    "sent": "Every node in the network represented gene and green arrows represent activation.",
                    "label": 0
                },
                {
                    "sent": "So one gene expressing itself favors the expression of some other gene.",
                    "label": 0
                },
                {
                    "sent": "And Red Arrows represent repression.",
                    "label": 0
                },
                {
                    "sent": "And then they do some experiments in the lab and they get.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is very noisy data.",
                    "label": 0
                },
                {
                    "sent": "This time series data that is actually not very long.",
                    "label": 0
                },
                {
                    "sent": "It is and how many data points, but maybe 30 or 40 data points.",
                    "label": 0
                },
                {
                    "sent": "And this is the expression level of different genes in time and then the idea is.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So can we actually go from this data?",
                    "label": 0
                },
                {
                    "sent": "These noisy time series data is actually not very long to this kind of graph representation of who is affecting who in this dynamical networks OK?",
                    "label": 0
                },
                {
                    "sent": "You know, one way of doing this or that?",
                    "label": 0
                },
                {
                    "sent": "One way to reduce this problem to a kind of a learning structure in stochastic differential equations is to use.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These action mass equation again this is simplified model but just to kind of illustrate the point.",
                    "label": 0
                },
                {
                    "sent": "So imagine we have a chemical equation wherein be reactive C&B then is action mass.",
                    "label": 0
                },
                {
                    "sent": "Equation tells us that the rate of change in the concentration or expression level of.",
                    "label": 0
                },
                {
                    "sent": "And the A is proportional to the product of the concentrations of C&D and also proportional concentrations of A&B.",
                    "label": 0
                },
                {
                    "sent": "And in this case this noise term.",
                    "label": 0
                },
                {
                    "sent": "Accounts both for measuring error and also for modeling certainty.",
                    "label": 0
                },
                {
                    "sent": "Becauses definitely not super accurate model.",
                    "label": 0
                },
                {
                    "sent": "And now what we can do is then we can come up with that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "General permit rise model.",
                    "label": 0
                },
                {
                    "sent": "Where we stack on that tall vector on the rights, all possible pairs of state vectors X one X2 X one X3 and also maybe individual 1X1X2X3, and we have a long vector of parameters Theta that tells us whether or not a certain pair is going to interact and affect expression level of gene X of I.",
                    "label": 0
                },
                {
                    "sent": "In this particular case.",
                    "label": 0
                },
                {
                    "sent": "For example if Theta I-11 is not zero, then it means.",
                    "label": 0
                },
                {
                    "sent": "According to this model that Gene 2 ones in three Co regulate the expression level of gene I OK?",
                    "label": 0
                },
                {
                    "sent": "So then we reduce this problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To know the problem of understanding the, you know the relation between these genes to the problem of learning the support of these unknown parameter Theta and throughout this talk you know support actually is going to mean sign support.",
                    "label": 0
                },
                {
                    "sent": "So not only can recover the support but also the sign of entries of these vector Theta calling.",
                    "label": 0
                },
                {
                    "sent": "If you have more conditions on our theorems you can recover the sign if you relax more these conditions we can recover support or a covering of the support.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so this is the problem at hand.",
                    "label": 0
                },
                {
                    "sent": "And so now there are two questions that we want.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stress, which is, first of all, how do we fit these kind of models to data and you know how much data do we need to actually be able to say something about?",
                    "label": 1
                },
                {
                    "sent": "Whether or not we are recovering this support?",
                    "label": 0
                },
                {
                    "sent": "You know the quality of interest.",
                    "label": 0
                },
                {
                    "sent": "And just give up.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presentation is this thing that I call sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So imagine that then we have the general form of our stochastic differential equation.",
                    "label": 0
                },
                {
                    "sent": "Some nonlinear function, big F, the drift coefficient that is parameterized by some large graph data, and we want to come up with an algorithm that received the sample trajectory is continuous simple trajectories and outputs graph that is a good approximation of the real underlying graph of our model, yes.",
                    "label": 0
                },
                {
                    "sent": "During this problem here we I mean well.",
                    "label": 0
                },
                {
                    "sent": "You're asking about the problem of whether I can deal with unknown, observed and observed states.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Yes, the vector X yes, so you measure you form this thing you observe.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have 10 genes.",
                    "label": 0
                },
                {
                    "sent": "You observe the evolution of tangent and you can form.",
                    "label": 0
                },
                {
                    "sent": "The product is just multiplying data.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can do that so you can extend your feature vectors, right?",
                    "label": 0
                },
                {
                    "sent": "So that's fine.",
                    "label": 0
                },
                {
                    "sent": "So you can make this thing as large as you want.",
                    "label": 0
                },
                {
                    "sent": "We can consider other kind of basis functions if you will more complicated things.",
                    "label": 0
                },
                {
                    "sent": "It still works OK. OK. Again, so the problem of interest is understanding how much data do we need to be able to recover this graph with high probability?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you know first guess might be well, since these projectors are continuous and any finite interval as any fit number of points.",
                    "label": 0
                },
                {
                    "sent": "Maybe no, I only need an arbitrarily small amount of information to be able to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's kind of a first case.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But maybe if the number of parameters in my Theta vector method vectors P dimensional, maybe I need something on the order of P?",
                    "label": 0
                },
                {
                    "sent": "But may.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We, you know, since we know from you know this sparse linear regression literature that, for example, to recover Theta in a setting in a linear regression setting if there is sparse telling about log P samples or PS dimension of our unknown vector.",
                    "label": 0
                },
                {
                    "sent": "So maybe.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I only need like log P order of observation, time to recover these these this network right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "In this talk, you know I'm going to be actually talking all the theoretical results are going to be about linear stochastic special equations.",
                    "label": 0
                },
                {
                    "sent": "If you look at our papers, actually we have a general lower bound on the on the sample complexity that actually works for both linear and nonlinear stochastic fragile equations.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to state the theorems in the most simple way possible, just to make things more clear.",
                    "label": 0
                },
                {
                    "sent": "You know, and you know the first thing that one can point out.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is actually very much related to the problem of this general field of graphical models, where we want to learn a graph from data, right?",
                    "label": 1
                },
                {
                    "sent": "So you might just think OK. Why don't I just look into the literature?",
                    "label": 0
                },
                {
                    "sent": "Maybe there's something that I can use and I can solve my problem without a lot of effort, which would be.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "And so let me just talk about some prior work.",
                    "label": 1
                },
                {
                    "sent": "So the first thing you might try to do is the following.",
                    "label": 1
                },
                {
                    "sent": "So let the system converter stationary, stationary state is going to some stationary distribution.",
                    "label": 0
                },
                {
                    "sent": "Some normal stationary distribution given linearity and given the Brownian motion that is driving our process.",
                    "label": 0
                },
                {
                    "sent": "And you might sample your your state vector sufficiently spaced in time to consider these samples independent, and then you might say, OK, we know that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "We know that the support of inverse covariance matrix represents conditional independence among different components of our state vector.",
                    "label": 0
                },
                {
                    "sent": "So maybe I can just use a method to estimate support of the inverse covariance matrix and everything will be fine, right?",
                    "label": 0
                },
                {
                    "sent": "And there's some?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Literature in this area.",
                    "label": 0
                },
                {
                    "sent": "Particularly, this thing called graphical.",
                    "label": 0
                },
                {
                    "sent": "Also, let's solve this problem.",
                    "label": 0
                },
                {
                    "sent": "But you know?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doesn't really solve our problem for two reasons.",
                    "label": 0
                },
                {
                    "sent": "Becausw.",
                    "label": 0
                },
                {
                    "sent": "First, this is stationary covariance matrix as less information than our unknown parameter Theta, and I'm going to give an example of that and 2nd we don't actually have independent samples.",
                    "label": 1
                },
                {
                    "sent": "Or we could have them if we specially if we space sufficiently spaced in time.",
                    "label": 0
                },
                {
                    "sent": "But we will definitely be losing information right?",
                    "label": 0
                },
                {
                    "sent": "And just to kind of give you an example of why these covariance mate.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This situation in Theta here is there are two linear systems.",
                    "label": 0
                },
                {
                    "sent": "They don't want to two and you can see that they both have different supports and they both have different sign supports, but actually have exactly the same stationary covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So second approach you might.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try is look again at our linear stochastic special equation and observe that it resembles very much like a normal linear regression problem.",
                    "label": 0
                },
                {
                    "sent": "If I stack these DT DX vectors in a Big Y and I stuck my projectors in a large matrix X so I can read wise it could X Theta plus annoys an so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem then reduce itself took a given Y&X.",
                    "label": 0
                },
                {
                    "sent": "That is fine.",
                    "label": 0
                },
                {
                    "sent": "This linear equation that is perturbed by noise.",
                    "label": 0
                },
                {
                    "sent": "Can I recover the support of my unknown parameter Theta?",
                    "label": 0
                },
                {
                    "sent": "And again there's been some work done on that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess we all familiar with the Lost Soul and work that followed.",
                    "label": 0
                },
                {
                    "sent": "Again, we cannot really apply or directly apply this literature becausw.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the roles of Y&X obtained from these trajectories are not IID.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is challenging while the challenge can be?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reduced to the following picture.",
                    "label": 0
                },
                {
                    "sent": "I can sample my trajectory sufficiently space in time to obtain independent samples and just use existing literature.",
                    "label": 0
                },
                {
                    "sent": "But then I'm going to be losing information and I can try not lose information sample very finely in time, but then I'm going to be introducing correlation between the samples and then it's not clear whether existing literature works OK.",
                    "label": 0
                },
                {
                    "sent": "I'll also like to mention some work actually.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then directly on HTS.",
                    "label": 0
                },
                {
                    "sent": "So initially you know I think this thing works in from the field of economics and people were interested in understanding and estimating these parameters from dynamical systems, and they used likely methods for continuous observations.",
                    "label": 0
                },
                {
                    "sent": "They also have.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some likely with methods for actually sampled.",
                    "label": 0
                },
                {
                    "sent": "Trajectories and but you know the problem with these methods.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That focuses mostly on more dimensional setting, and I mean this maximum likelihood method would not give you a selection procedure and there will come normal with no guarantees.",
                    "label": 1
                },
                {
                    "sent": "Basic basically basically the kind of guarantee that it will be asymptotically in T in the sense that OK, if I observe my trajectories for infinite amount of time, then our estimator is consistent.",
                    "label": 0
                },
                {
                    "sent": "But that's not what we're really interested in.",
                    "label": 0
                },
                {
                    "sent": "We want a sharper characterization of exactly how much information we need to be able to recover the support with high probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just the algorithm here and introduce algorithm.",
                    "label": 1
                },
                {
                    "sent": "I'm going to go a little bit beyond linear setting because the algorithm algorithm actually can be applied to these slightly modified setting where the rate of change of every state variable X or Y is proportional to some function of the state variables.",
                    "label": 0
                },
                {
                    "sent": "This function can be nonlinear, but the dependency on the parameter Theta is assumed to be linear.",
                    "label": 0
                },
                {
                    "sent": "Again, this data still represent some sort of graphical relationship or dependency between the variables so.",
                    "label": 0
                },
                {
                    "sent": "Look at this example in the bottom where the rate of change of I is proportional to some function of X3 and is also proportional to some function of X one X2 and I can draw the graph of dependencies.",
                    "label": 0
                },
                {
                    "sent": "OK, and since this HT the dependency with data is still linear, we can try to use.",
                    "label": 0
                },
                {
                    "sent": "You know, the same.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've been using for linear regressions.",
                    "label": 0
                },
                {
                    "sent": "You know when I've derivation Orrery derivation, if you wanted the last so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give you an expression like this, you want to minimize the integral of the square difference between the DXS and.",
                    "label": 0
                },
                {
                    "sent": "The you know the data GS of the States plus some penalty term to induce sparsity, right?",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out that is not quite the right way of doing it.",
                    "label": 0
                },
                {
                    "sent": "You need some sort of stochastic calculus.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, but you know you can actually do the math writes you know, and after you do the math right, you get the following expression and you see that there is actually going to use this thing here.",
                    "label": 0
                },
                {
                    "sent": "There's these little integral here, which is stochastic integral, but you know if you want to actually do it in practice in the computer.",
                    "label": 0
                },
                {
                    "sent": "It's actually very simple.",
                    "label": 0
                },
                {
                    "sent": "And expression might look complicated, but in the end of the day is just a quadratic function, or at least these this term here.",
                    "label": 0
                },
                {
                    "sent": "Which we call the likelihood term is just a quadratic function of our unknown parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically the lost soul in that kind of a hidden way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what is the problem now?",
                    "label": 0
                },
                {
                    "sent": "Or can we characterize or not?",
                    "label": 0
                },
                {
                    "sent": "What is simple complexity of this problem?",
                    "label": 0
                },
                {
                    "sent": "How much information I need to be able to guarantee that with probability I'm recovering the right support of this little late?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to state.",
                    "label": 0
                },
                {
                    "sent": "Specializations of our general theorem.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not going to take the general theorem because expresses it look a bit complicated, and I think we will miss the point.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the simpler setting that I'm going to be talking about that I've already introduced.",
                    "label": 0
                },
                {
                    "sent": "Its linear FT is even more special than linear.",
                    "label": 0
                },
                {
                    "sent": "Steel is a linear steel on a graph where the rate of change of every node in the graph is proportional to the difference between its neighbors and its own value plus the damping factor proportional to M and so the first theorem states that.",
                    "label": 0
                },
                {
                    "sent": "So if this graph is sparse.",
                    "label": 0
                },
                {
                    "sent": "And it has a maximum degree Delta in all the nodes.",
                    "label": 0
                },
                {
                    "sent": "Then if the graph is large, an aim is large, much larger degree then the sample complexity is upper bounded by M square, log P and lower bounded by M log B.",
                    "label": 0
                },
                {
                    "sent": "Again, the theorem doesn't need these two.",
                    "label": 0
                },
                {
                    "sent": "Assumptions here.",
                    "label": 0
                },
                {
                    "sent": "There's a general.",
                    "label": 0
                },
                {
                    "sent": "We have a general upper lower bound, but expressions look a bit.",
                    "label": 0
                },
                {
                    "sent": "Ugly, so I just kind of represented in this setting here, but the point that we have some sort of sharp characterization in terms of dependency of the size of the graph in terms of the dependency on this damping factor is not really sharp.",
                    "label": 0
                },
                {
                    "sent": "There's an extra M on the right, maybe I think can be improved.",
                    "label": 0
                },
                {
                    "sent": "And we also have a similar theorem when the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph is not sparse but dense.",
                    "label": 0
                },
                {
                    "sent": "In this case we do have a sharp characterization.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity is basically proportional to M * P. OK. And so here's the general picture.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are actually if you actually have the expression, the full expressions.",
                    "label": 0
                },
                {
                    "sent": "So this is the same for large M the behavior, so the lower bound grows linearly with M and upper bound grows quadratically with M times some log P. For small M, the lower bound actually converges to a constant wild upper bound diverges to Infinity, so there's some sort of issue to be solved here, so it's not clear whether systems where there's not a lot of damping, whether they are very hard to learn or they're not hard at all to learn.",
                    "label": 1
                },
                {
                    "sent": "OK, so we can over well empirically, it seems that.",
                    "label": 0
                },
                {
                    "sent": "They are hard to learn, but you know also depends on what kind of empirical test you do right?",
                    "label": 0
                },
                {
                    "sent": "So we don't we don't have that gap closed quite yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so are they.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rila theorem does depend on some assumptions.",
                    "label": 1
                },
                {
                    "sent": "Holding these assumptions are not very much different from the assumption that had been posed to obtain consistency in the Lost soul, so we need some sort of bound on the smallest output value of the entries of our matrix.",
                    "label": 0
                },
                {
                    "sent": "We need some sort of stability of our dynamical system.",
                    "label": 0
                },
                {
                    "sent": "We need some restricted convexity assumption and some sort of incoherence condition that is called in this setting actually representable condition OK, and just to give a general kind of a very high level idea of the proof.",
                    "label": 1
                },
                {
                    "sent": "How do we obtain this?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of stuff.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we don't work with a continuous HTS.",
                    "label": 1
                },
                {
                    "sent": "We discretize them and then we take the limit when they are discretization is definitely fine if you will and then the proof structure is very similar to the proof for the consistency of the last so introduced by zone you in 1006.",
                    "label": 1
                },
                {
                    "sent": "But we do need some new concentration bounds.",
                    "label": 0
                },
                {
                    "sent": "And so this concentration bounds are concentration bounds on.",
                    "label": 0
                },
                {
                    "sent": "You know the size if you will, of the gradients of the likelihood term, just to recall it.",
                    "label": 0
                },
                {
                    "sent": "Likelihood term is.",
                    "label": 0
                },
                {
                    "sent": "This is the likelihood term here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need some sort of.",
                    "label": 0
                },
                {
                    "sent": "We need some sort of bounds on the gradient of the likely terminism bound on the Hessian of the likelihood term, and in the case of the Lost soul, both both the gradient, the empirical gradient, and these empirical Hessian could be expressed as linear combinations of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "In our case, they cannot, but it can be expressed as sort of a quadratic.",
                    "label": 0
                },
                {
                    "sent": "Quadratic form like these where Z is a vector that has the vector of normal 01 variables.",
                    "label": 0
                },
                {
                    "sent": "And then we need to analyze eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "All these are matrix and you know blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "But OK, I'm just not going to go into detail, but this kind of, you know the main innovation is, you know, kind of filling out these bounds and figuring out how to make these reductions and make everything work there.",
                    "label": 0
                },
                {
                    "sent": "OK the rest and also this path through the limits also requires some care.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just end up.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By talking about some follow up work after our initial work on analyzing these Estes.",
                    "label": 1
                },
                {
                    "sent": "There's been some work on autoregressive processes that are a bit different in our.",
                    "label": 0
                },
                {
                    "sent": "There's a delay.",
                    "label": 0
                },
                {
                    "sent": "You can see there's a delay here on the right.",
                    "label": 0
                },
                {
                    "sent": "Obviously this thing can be reduced to our form, but you know these.",
                    "label": 0
                },
                {
                    "sent": "Both uh than others.",
                    "label": 0
                },
                {
                    "sent": "What they did is they specialize these inquiries.",
                    "label": 0
                },
                {
                    "sent": "Conditions for the case of this auto regressive processes obtained tighter bounds on the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "There's also been some work on recovering this relationship between these components of this state.",
                    "label": 0
                },
                {
                    "sent": "Vectors when we don't observe the state vector fully, there's some hidden variables, and here the idea is that you can somehow use a similar method where where you have a likelihood term plus a penalty, but now the penalty is actually two penalties.",
                    "label": 0
                },
                {
                    "sent": "And L1 penalty to introduce sparsity and a nuclear non penalty to somehow impose a low rank structure on the remaining parts of what you're not observing.",
                    "label": 1
                },
                {
                    "sent": "And now we need to increase conditions.",
                    "label": 0
                },
                {
                    "sent": "We need one incoherence condition for the sparsity, and we need one incoherence condition for the low rank.",
                    "label": 0
                },
                {
                    "sent": "Assumption and there's.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some work on a nonparametric estimation, some of them work actually done by Manford here.",
                    "label": 0
                },
                {
                    "sent": "I think it was Nipsey 1013.",
                    "label": 0
                },
                {
                    "sent": "The problem is not so much recovery of support or you know the structure of these tests, but just estimating the drift coefficient and the idea here is to use grousing processes and approximately EM algorithm and some work also then on actually estimating directly graphical processes where instead of having just.",
                    "label": 0
                },
                {
                    "sent": "Prem Trizol linear SDE?",
                    "label": 0
                },
                {
                    "sent": "We have Gaussian process and.",
                    "label": 0
                },
                {
                    "sent": "You know, since the process is not printable, what we do is that we work with these.",
                    "label": 0
                },
                {
                    "sent": "We work with these spectral density matrices, so we estimate the spectral matrices and we prove some guarantees on recovering.",
                    "label": 0
                },
                {
                    "sent": "You know the structure of the inverse of this matrix.",
                    "label": 0
                },
                {
                    "sent": "I just want to finish.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The talk about the you know, an extension of our work for non linearized, which is kind of an open problem yet, but we do know that the algorithm applies, so we've used the algorithm to recover nonlinear season.",
                    "label": 0
                },
                {
                    "sent": "It does work and we just don't know whether we can extend theory while the lower bounds hold for nonlinear astis the upper bounds, we need new theory to for the case of nonlinearities even for the case where the SD is nonlinear but still linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just want to give you a quick exam.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All over how these things look.",
                    "label": 0
                },
                {
                    "sent": "So here's an example where I have a system of masses and Springs.",
                    "label": 0
                },
                {
                    "sent": "So every node in our network now is a mass.",
                    "label": 0
                },
                {
                    "sent": "Every edge is a spring that connected to masses and the whole system can be expressed using classical.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mix and in this particular case.",
                    "label": 0
                },
                {
                    "sent": "Our functions GG functions that have these nonlinear function of the state are going to have one of three forms and our parameters Theta are going to indicate or parameterized the possibility of 1 mass being connected to another mass through some spring OK. And the problem here is then well, can I imagine it's the building and I put some sensors in the building and I want to see whether there's some sort of problem in the building structure, so I hit it with a hammer and the whole thing vibrates a little bit and I want to somehow recover the structure of the building.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a here's an example, so here's a structure mechanical structure of mass and Springs, and here is the on the top right is the sensors that are going to start moving a little bit, and this is physical structure vibrating, and as we get more and more data we can see it.",
                    "label": 0
                },
                {
                    "sent": "Eventually we can recover the right structure for a mechanical system.",
                    "label": 0
                },
                {
                    "sent": "When you can do more extensive numerical simulations and we can actually confirm that it does seem.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you know the sample complexity, at least empirically grows like log P, just like Carlo.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bound predicts OK, although again we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have a proof for that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to kind of summarize, we know that we can learn at least linear, Estes, sparsely nestis with much fewer samples than the dimension of our vector.",
                    "label": 0
                },
                {
                    "sent": "We can also learn densest is, we know that our results, at least empirically, seems to extend to nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "We also know from our bounds that faster dynamical systems that correspond to larger M are harder to learn, is still an open problem to understand whether slow systems with small M. Are hard to learn or not because there's a kind of a exposing gap between our lower bound.",
                    "label": 0
                },
                {
                    "sent": "And an upper bound OK, and so now I'll take some questions.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}