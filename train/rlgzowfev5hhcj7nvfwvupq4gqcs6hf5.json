{
    "id": "rlgzowfev5hhcj7nvfwvupq4gqcs6hf5",
    "title": "Binary Decomposition Methods for Multipartite Ranking",
    "info": {
        "author": [
            "Johannes Fuernkranz, Darmstadt University of Technology"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_fuernkranz_bdm/",
    "segmentation": [
        [
            "My name is Jonathan Krantz and this is joint work with alcohol, Amaya and Steam Bundle, OE and we'll talk about binary if decomposition methods for multipartite.",
            "Ranking"
        ],
        [
            "And I'll explain both of these terms in a minute, but this multipartite ranking it's actually a known problem, but maybe not as known.",
            "Not not as well known as it should be, and we also gave you the new name, so I don't expect you to know what we're talking about right now.",
            "I'll explain it.",
            "We'll talk about evaluation measures for this problem.",
            "Then we talk about methods for solving this problem, in particular about methods that use binary decompositions.",
            "As opposed to the conventional approach that transforms the problem into one big binary.",
            "A problem, then we discuss the complexity of these options and then show you some experimental results and what we found in our experiment."
        ],
        [
            "So to explain what multipartite ranking is, we start with the simplest thing binary classification.",
            "You have a bunch of objects here.",
            "Let's say these are a bunch of papers.",
            "And then you have to classify these objects into two categories, zeros and ones or except and rejects.",
            "That is what the reviewer typically has to do.",
            "Then a little bit."
        ],
        [
            "More complex is the problem.",
            "If you have binary classification with scores, there you have again bunch of papers and you want to have zeros and ones.",
            "In the end accepts and rejects, but you're also supposed to give some sort of degree, so there is a green one real green one.",
            "This is a clear except this is a real red one.",
            "This is a clear rechecked and in the middle this is the usual case week except weekly chip mixture.",
            "So you don't really know there are different degrees of being red or being green or being except or being reject.",
            "Of course, you can also sort the papers by this color by discreteness and redness.",
            "Then you get something like this here."
        ],
        [
            "This is binary classification with scores again and just shown in a different way you have here.",
            "The one that is the clear except here the one that is the clearly check and once in between are sorted.",
            "And this is actually doing two things at the same time.",
            "And the first is the ranking step, this is."
        ],
        [
            "The bipartite ranking.",
            "And it's bipartite because it is based on two categories.",
            "And then after the pipette ranking.",
            "You do some sort of split and say above these thresholds.",
            "These will lead through below the threshold they go.",
            "So by Bud Light ranking plus partition, this system binary classification rescores.",
            "So now we come from Piper tighter."
        ],
        [
            "King to multipartite ranking.",
            "And it's actually the same story that we went through now, except that it starts with order classification, not binary classification.",
            "So with order classification again bunch of papers.",
            "Now you don't have two categories except project.",
            "You have three except.",
            "Borderline and rechecked with the idea borderline being sort of between.",
            "You could also have four or five except week.",
            "Recheck borderline weak reject reject.",
            "Or how many is 1?",
            "The important thing is you have more than two categories and these are sorted in some way.",
            "And again, the task of multipartite ranking is."
        ],
        [
            "You want to rank the objects you want to rank rank the paper.",
            "This is the one that is most except this is the one that is the most likely to be rejected, so."
        ],
        [
            "Appetite, ranking and multipartite ranking.",
            "From this point of view are actually the same thing.",
            "I showed you the same picture.",
            "In both cases we wanted to rank the papers by.",
            "Quality and this is where this little analogy to reviewing breaks down, because the difference between bipartite and multipartite ranking is more or less in the training information, so a bipartite rancor.",
            "Is a reviewer that Distrained for the reviewing task on papers with the category except and rechecked on two class papers.",
            "Multipartite rancor is arrancar that this trained for this job on a three class or a multiclass task.",
            "So, in our example, he would have seen from maybe from his advisor, examples for papers that are except rechecked, and also papers where the advisor said these are borderline papers look so, and based on these three categories, he learns to rank papers.",
            "And the difference of course, does not only end with the training information, because we also want to evaluate these rankings.",
            "And if the training information is not two class but in class the.",
            "The information that we can use for evaluating will also be in class, so we will next look at."
        ],
        [
            "Evaluation measures first again for bipartite ranking.",
            "The common evaluation measure for bipartite rankings is the area under the Roc curve, so this is essentially the probability that.",
            "For a pair of examples where the first one is a positive, the second one is a negative.",
            "Positive is ranked.",
            "Before the negative example.",
            "And it is computed in exactly this way you.",
            "Take all pairs P and then where P is a positive and N is a negative class.",
            "If we denote the classes as we did in the previous slides with one for positive and zero for negative, we can say class P is greater than class N. And then you count how many of these pairs are correctly scored, meaning that the class, the score of the positive is greater than the score of the negative.",
            "Actually, I shouldn't say score, it actually only looks at the rank, but all of the methods that we look at that we look at now they produce some sort of score, but you can in principle also rank without the score.",
            "So you can't correct if the score is equivalent with the order that is given by the classes and go see the area under the Roc curve is just the percentage of pairs that respect this order over all pairs.",
            "And now it."
        ],
        [
            "Elevation of multipartite rankings.",
            "So there is a common measure called DC index and that works exactly the same way except you do not pick a positive example.",
            "In the negative example but you pick an example of a Class J and you pick another example of a class I smaller than J.",
            "So you have again.",
            "2 examples.",
            "And this one has a higher rank.",
            "Then this one, and then you again count for all of these pairs.",
            "P&NP is from a higher rank class.",
            "Ann is from some lower ranked class.",
            "You count how many of them respect this relation.",
            "Also, with this core that are produced and again to see index is the percentage of correct correctly ranked examples in all of these pairs and there is now a sum here because you have.",
            "If you have C different classes, you have C * C -- 1/2 different combinations of inj here and for all of these you can select the pairs.",
            "So this is the.",
            "This is the number of all pairs.",
            "This is the percentage of correct examples, and again, obviously.",
            "He you see is a special case of the.",
            "C index with C = 2 seem being the number of classes.",
            "If you only have two classes and if you compute this thing, it will be the UC."
        ],
        [
            "So this index can actually be written rewritten in this way, and this is easy to see because the AUC we have seen that AUC of P&N is the number of correct for P and then divided by the number of P times the number of N examples.",
            "And if now it would be the number of I and the number of Che examples and if we multiply with these two again, then with this is just the number of correctly ranked examples for.",
            "The pair of classes I&J and we sum over all of these classes, so this again is the C index.",
            "But what is interesting here?",
            "It is now is sum of AUC of pairwise AUC values.",
            "And.",
            "This may sound familiar for people that are have worked with multiclass AUC because there is this formula deal that's that is an extension of AOC which only works for binary classes for multi class is not necessarily ordered, but of course you could also use it for other classes and then this simply the sum of all pairwise AUC's and averaged so it's not the same but the average of all pairwise.",
            "You see that this.",
            "Multi class extension of AOC proposed by hand and till in Machine Learning Journal 2001.",
            "It's also known as the young Kerry Terpstra statistic.",
            "And both of these that this interesting and we used that as a motivation for our approach, they reduce.",
            "These measures for multipartite ranking.",
            "Do a sum of pairwise AUC's measures.",
            "So.",
            "Our idea was now to decompose these things into binary classifiers and our one of the things was that we used pairwise classifiers here because there we can directly optimize the probabilities for a pair of classes.",
            "But we'll see."
        ],
        [
            "Out with the conventional approach for multipartite ranking will come to the pairwise kiss in a minute so the conventional approaches you turned it into a big ranking problem.",
            "Again, you objective is you want to have, for each pair of examples P and then peeping from some class higher ranked than N, you want this core of P to be higher than this core of N. If the class of P is higher than the class event in the training examples.",
            "And if you assume a linear scoring function, as is usually done, then you can solve this problem in a fairly straightforward way.",
            "You define a new example X, which is the difference between P&N and then this core of P -- N which is.",
            "The score of this new example.",
            "Is the same as the score of P minus the score of N because the scoring function is linear and you see this difference should be greater than or greater than zero if.",
            "P is a positive example and it should be less than zero.",
            "If this is a negative example, so we've transformed that in the binary classification problem with a new set of examples X and some of these have to be positive, and some of these have to be negative, fairly easy.",
            "But we."
        ],
        [
            "To decompose this approach into smaller binary problems.",
            "So one approach that has been proposed a couple of years ago by Frank and Hall at DCML 2001.",
            "Is an ordered decomposition.",
            "Assume you have four classes.",
            "Then you train three binary classifiers, always for N classes N -- 1 classifiers.",
            "And you start by.",
            "Using one classifier that discriminates one from 234.",
            "Another 112 from 3, four and the other the third 1123 from 4.",
            "So you essentially estimate probabilities.",
            "You can say that this is estimates the probability that the example is greater than one.",
            "This is the probability that example is created in two, and this estimates the probability that the example is greater than three.",
            "So we have three such probability values.",
            "I will, I'll show you in a minute how to use those.",
            "Another approach is to pairwise approach.",
            "There you decompose the problem into.",
            "C * C -- 1/2 problems, each one discriminating a pair of classes, so one against two, one against three, one against 4, two against 3, two against four to three games for.",
            "Trained binary classifier for each of them and then you get the probability out of these classifiers.",
            "For example, this one emits a probability example that you showed to this classifier is rather one than two, or it might be rather 2, then one.",
            "So this can be interpreted.",
            "This sort of preference statement and these two approaches have been compared before by myself in HTML 2002.",
            "For ordered classification.",
            "So for the task of actually predicting, except borderline or reject.",
            "And there it show it came out that the pairwise classifier was better or it was at least not certainly not worse than the order classifier was a little bit better in some cases.",
            "But maybe this was not significant.",
            "Uh.",
            "But now we want to do a different thing.",
            "We do not want to do order classification.",
            "We want to do this ranking thing and again we wanted to compare these two things to each other and also to the common approach.",
            "Before we talk about how to use this for prediction."
        ],
        [
            "Let's think a little bit about the complexity, because this proved also to be an important point in our experiments.",
            "So the conventional approach essentially, for each pair of examples.",
            "You produce a new example.",
            "So the size.",
            "Of the new example set is of the order N square, and you produce a single set of these.",
            "So you have one.",
            "You train one model and this is trained on N square order constraints and these order constraints.",
            "As we have seen before, they are translated into examples.",
            "So you have N square examples.",
            "So if we now have a base learner binary based learner that has a complexity end to the Alpha you have now problem that this end to the.",
            "2A yeah, Alpha goes up there, so pretty expensive one.",
            "Do you want?",
            "Oops.",
            "I think I have to do a lot of clicking now.",
            "No, that's good.",
            "Uh.",
            "The ordered approach, on the other hand, it trains C -- 1 models, each one using all of the examples.",
            "And for each of these C -- 1 models you need end to the Alpha training time, because this is the complexity of the Bayes classifier and you have C -- 1 of them.",
            "So the order is C times and to the Alpha.",
            "And the pairwise approach that sort of depends a little bit on the distribution of the examples.",
            "Of course, in the individual classes.",
            "The first case is if you distribute them equally.",
            "All classes have the same size and then you train C * C -- 1/2 models with two N / C training examples each of.",
            "So each has one, each has two classes and each of the classes as an oversea training examples.",
            "And this yields a complexity of.",
            "C C ^2 * 10 to the Alpha over C to the Alpha, and.",
            "This is this factor here, C2D2 minus Alpha.",
            "So if if you have a linear classifier Alpha equals one.",
            "This is about the same as the order approach, but the more expensive your classifier gets.",
            "If you have a quadratic one.",
            "This one will be already cheaper than the ordered approach.",
            "And this is also clear.",
            "They use the use roughly the same number of or exactly the same number of examples in the total of all of their training sets.",
            "You can simply multiply this with that and multiply this with that and you will see it's the same number C to C -- 1 * N. But this approach distributes these numbers among much smaller sets, so if the complexity for handling a set of size N. Is end to the Alpha.",
            "The sum will be cheaper for this approach because the the ends are much smaller and summing up these end to the office is cheaper than if you sum up first and take the result into the Alpha.",
            "So how are we doing?",
            "Multi."
        ],
        [
            "Partite prediction now.",
            "With Frankenhole, remember we have C -- 1 models, each one primitive probability that the class is greater than I for some class I so we have P1P 2P3.",
            "Uh.",
            "This can be used for computing a prediction.",
            "We don't need to know how this works.",
            "We want to use.",
            "We want to use it for computing a score in the multipartite ranking.",
            "Setting so we want to sort the examples by this score that we're computing here.",
            "So we want to have a score for each example, and it turns out the very straightforward way for computing a score out of this decomposition is simply sum up these probabilities P1 plus P2 plus P3 probability of the example being greater than one plus probability of the example greater than two plus probability of the example greater than three.",
            "Intuitively, this makes sense here.",
            "If you have a large class, this will be have a high probability value on all of these three probabilities.",
            "If you have small class, it will have a low probability on all of these three probability values, so intuitively seems OK. Is you can even show that it is theoretically sensible, because it turns out that the score is actually the expected value of the rank of the example.",
            "And this can be easily seen by replacing this P1.",
            "So this is the probability of example being greater than one, which is the probability of it being 2 plus the probability for three plus the probability for four.",
            "So you have 3 boxes here, 2 boxes here, one box here.",
            "And then you change the order of summation and you sum up.",
            "You have three times the probability for two times the probability for three, one times the probability for two, and that is exactly this.",
            "Expected value, so this makes sense.",
            "How do we do with the pairwise approach?",
            "Again we have see."
        ],
        [
            "Approximately square examples, and each of them emit classifiers partner and each one emits a probability that the class.",
            "So the isce classifier emits a probability that the class of the example is.",
            "I given that we know that the example that the example classes either I or J.",
            "So these are sort of conditional probabilities.",
            "If you know it's either one or two, then they want two classifier will tell you is it.",
            "More likely to be one or more likely.",
            "To be 2 for computing a prediction from that for classification.",
            "The usual thing is weighted voting.",
            "You can show that this minimizes this.",
            "No, it maximizes.",
            "Sorry the Spearman rank correlation.",
            "So it's a good measure, not the bad measure is written here.",
            "You'd simply sum up these.",
            "You can imagine these values as a matrix and you sum up the lines of this matrix and you return the line with the maximum value.",
            "How do you do that for ranking again?",
            "So again, here is this matrix, and here we are summing up all values that are above the diagonal.",
            "So we sum up all predictions that are for all classifiers.",
            "From the point of view of the higher class.",
            "So again, if you have a high class here, all of these probabilities tend to be high.",
            "If you have a low class, all of these probabilities tend to be low.",
            "So intuitively again, it makes sense.",
            "Another motivation for this approach is if you think back for these multi class AUC formula.",
            "There we said this is actually.",
            "This is actually reducing the measure to sum of pairwise AUC values.",
            "And we thought if you optimize these values with a binary classifier then summing up this course is a reasonable thing to do and will give you.",
            "Reasonable score actually.",
            "We tried three variants or two variants of this."
        ],
        [
            "So the first one is they want to just showed you the second one.",
            "Now is not motivated by their multivalued AUC, but it is motivated by the C index that I have showed you in the beginning and here we are just waiting these probabilities with the product of inj.",
            "So in order to diversify discourse little bit more and another version is where we do not wait with the product but with the sum of these two and this one.",
            "If you think about it, turns out to be also very well motivated, because in principle this also estimates this expected value.",
            "The same expected value that is estimated by the frankenhole.",
            "Method, but it estimates it was different probabilities.",
            "So here a product of a conditional probability that is the one that is learned times the probability of an example being in the class I or J.",
            "So this is the number.",
            "This is the sum here.",
            "OK, I'll show you the experiment I.",
            "These three methods were actually more or less equal, and at some point we decided that we will only use these two, so you will only see these two in the experiments, but in particular these two they were almost identical, and as you will see, these two were already quite similar, so we evaluated this honor ranking scenario we used, we simulated."
        ],
        [
            "21 ranking sets by discretizing regression sets with five into five classes with equal frequency, and we also had four real ordered classification sets.",
            "So in total we had 25 sets, 5 iterations of tenfold classification.",
            "The base classifier was logistic regression for the binary decomposition methods because we wanted to get reasonable probabilities and we compare to rank SVM as an example for a method that.",
            "Uses the.",
            "The traditional approach of learning."
        ],
        [
            "The whole thing.",
            "So in brief, the experiment.",
            "The result of the experiment.",
            "Contrary to what we had expected.",
            "The FRANKENHOLE variant outperforms the binary.",
            "The pairwise classifiers, and they're about equal to the SVM rank.",
            "So if you see here the frankenhole, the numbers in brackets.",
            "Here they are the rank in this scenario, this is C and extra Sister York area Step statistic.",
            "So on the first data set, Franklin told was the best.",
            "The weighted version was the second best.",
            "The unweighted was the 3rd president.",
            "ESPN rank for the force.",
            "Here is the average.",
            "Uh.",
            "Here are the average ranks over all these ranks, and if you do and any test, it turns out that this difference is actually significant critical rank difference for this problem would be 0.88, so this is better than all the three and the other three are distinguishable.",
            "There is a little bit of advantage for the weighted version for the index, which is not surprising because this was the measure that motivated this version.",
            "This difference vanish is here if you go to the unweighted version, but essentially there is no diff."
        ],
        [
            "But we need this.",
            "So we double checked our.",
            "OK, we double checked our expectations by also trying again.",
            "Repeating this experiment from for classification and there again it turned out the pairwise classifier 15 wins the Frankenhole method nine wins, so we really have this phenomenon for ranking.",
            "There the pairwise approach performs worse for classification.",
            "It seems to do."
        ],
        [
            "A little bit better and the problem it seems is.",
            "With Discord, Discord is computed as the sum of all probabilities, and some of these probabilities are actually estimated from what we call a non competent classifier.",
            "So if the example is from Class 1.",
            "Then this all the probabilities involving the Class 1.",
            "Have been estimated from examples that include examples from class One.",
            "These probabilities have been estimated from examples that never have seen from datasets that have never seen an example from class One.",
            "So in some sense the same problem also appears in classification, but there it doesn't really matter that much and I don't have the time to explain that now.",
            "But if you want you're welcome to ask me at the poster.",
            "So this problem is much more.",
            "It is much worse for for ranking this is what we think is happening here."
        ],
        [
            "So, just to summarize what we've seen.",
            "We have shown that multipartite problems can be solved by decomposing them into binary problems, which is a technique known from other problems.",
            "We have shown that this approach is more efficient in the convection than the conventional approach and among the different decompositions.",
            "It seems to be the case that they ordered.",
            "That the order decomposition is a little bit better or actually significantly better than the pairwise decomposition.",
            "So this wraps up to talk.",
            "I'm ready to take questions if there is still time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Jonathan Krantz and this is joint work with alcohol, Amaya and Steam Bundle, OE and we'll talk about binary if decomposition methods for multipartite.",
                    "label": 0
                },
                {
                    "sent": "Ranking",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll explain both of these terms in a minute, but this multipartite ranking it's actually a known problem, but maybe not as known.",
                    "label": 0
                },
                {
                    "sent": "Not not as well known as it should be, and we also gave you the new name, so I don't expect you to know what we're talking about right now.",
                    "label": 0
                },
                {
                    "sent": "I'll explain it.",
                    "label": 0
                },
                {
                    "sent": "We'll talk about evaluation measures for this problem.",
                    "label": 1
                },
                {
                    "sent": "Then we talk about methods for solving this problem, in particular about methods that use binary decompositions.",
                    "label": 1
                },
                {
                    "sent": "As opposed to the conventional approach that transforms the problem into one big binary.",
                    "label": 0
                },
                {
                    "sent": "A problem, then we discuss the complexity of these options and then show you some experimental results and what we found in our experiment.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to explain what multipartite ranking is, we start with the simplest thing binary classification.",
                    "label": 1
                },
                {
                    "sent": "You have a bunch of objects here.",
                    "label": 0
                },
                {
                    "sent": "Let's say these are a bunch of papers.",
                    "label": 0
                },
                {
                    "sent": "And then you have to classify these objects into two categories, zeros and ones or except and rejects.",
                    "label": 0
                },
                {
                    "sent": "That is what the reviewer typically has to do.",
                    "label": 0
                },
                {
                    "sent": "Then a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More complex is the problem.",
                    "label": 0
                },
                {
                    "sent": "If you have binary classification with scores, there you have again bunch of papers and you want to have zeros and ones.",
                    "label": 1
                },
                {
                    "sent": "In the end accepts and rejects, but you're also supposed to give some sort of degree, so there is a green one real green one.",
                    "label": 0
                },
                {
                    "sent": "This is a clear except this is a real red one.",
                    "label": 0
                },
                {
                    "sent": "This is a clear rechecked and in the middle this is the usual case week except weekly chip mixture.",
                    "label": 0
                },
                {
                    "sent": "So you don't really know there are different degrees of being red or being green or being except or being reject.",
                    "label": 0
                },
                {
                    "sent": "Of course, you can also sort the papers by this color by discreteness and redness.",
                    "label": 0
                },
                {
                    "sent": "Then you get something like this here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is binary classification with scores again and just shown in a different way you have here.",
                    "label": 1
                },
                {
                    "sent": "The one that is the clear except here the one that is the clearly check and once in between are sorted.",
                    "label": 0
                },
                {
                    "sent": "And this is actually doing two things at the same time.",
                    "label": 0
                },
                {
                    "sent": "And the first is the ranking step, this is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bipartite ranking.",
                    "label": 0
                },
                {
                    "sent": "And it's bipartite because it is based on two categories.",
                    "label": 0
                },
                {
                    "sent": "And then after the pipette ranking.",
                    "label": 0
                },
                {
                    "sent": "You do some sort of split and say above these thresholds.",
                    "label": 0
                },
                {
                    "sent": "These will lead through below the threshold they go.",
                    "label": 0
                },
                {
                    "sent": "So by Bud Light ranking plus partition, this system binary classification rescores.",
                    "label": 1
                },
                {
                    "sent": "So now we come from Piper tighter.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "King to multipartite ranking.",
                    "label": 0
                },
                {
                    "sent": "And it's actually the same story that we went through now, except that it starts with order classification, not binary classification.",
                    "label": 0
                },
                {
                    "sent": "So with order classification again bunch of papers.",
                    "label": 0
                },
                {
                    "sent": "Now you don't have two categories except project.",
                    "label": 0
                },
                {
                    "sent": "You have three except.",
                    "label": 0
                },
                {
                    "sent": "Borderline and rechecked with the idea borderline being sort of between.",
                    "label": 0
                },
                {
                    "sent": "You could also have four or five except week.",
                    "label": 0
                },
                {
                    "sent": "Recheck borderline weak reject reject.",
                    "label": 0
                },
                {
                    "sent": "Or how many is 1?",
                    "label": 0
                },
                {
                    "sent": "The important thing is you have more than two categories and these are sorted in some way.",
                    "label": 0
                },
                {
                    "sent": "And again, the task of multipartite ranking is.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want to rank the objects you want to rank rank the paper.",
                    "label": 0
                },
                {
                    "sent": "This is the one that is most except this is the one that is the most likely to be rejected, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Appetite, ranking and multipartite ranking.",
                    "label": 1
                },
                {
                    "sent": "From this point of view are actually the same thing.",
                    "label": 1
                },
                {
                    "sent": "I showed you the same picture.",
                    "label": 1
                },
                {
                    "sent": "In both cases we wanted to rank the papers by.",
                    "label": 0
                },
                {
                    "sent": "Quality and this is where this little analogy to reviewing breaks down, because the difference between bipartite and multipartite ranking is more or less in the training information, so a bipartite rancor.",
                    "label": 0
                },
                {
                    "sent": "Is a reviewer that Distrained for the reviewing task on papers with the category except and rechecked on two class papers.",
                    "label": 0
                },
                {
                    "sent": "Multipartite rancor is arrancar that this trained for this job on a three class or a multiclass task.",
                    "label": 0
                },
                {
                    "sent": "So, in our example, he would have seen from maybe from his advisor, examples for papers that are except rechecked, and also papers where the advisor said these are borderline papers look so, and based on these three categories, he learns to rank papers.",
                    "label": 0
                },
                {
                    "sent": "And the difference of course, does not only end with the training information, because we also want to evaluate these rankings.",
                    "label": 0
                },
                {
                    "sent": "And if the training information is not two class but in class the.",
                    "label": 1
                },
                {
                    "sent": "The information that we can use for evaluating will also be in class, so we will next look at.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evaluation measures first again for bipartite ranking.",
                    "label": 0
                },
                {
                    "sent": "The common evaluation measure for bipartite rankings is the area under the Roc curve, so this is essentially the probability that.",
                    "label": 1
                },
                {
                    "sent": "For a pair of examples where the first one is a positive, the second one is a negative.",
                    "label": 0
                },
                {
                    "sent": "Positive is ranked.",
                    "label": 0
                },
                {
                    "sent": "Before the negative example.",
                    "label": 0
                },
                {
                    "sent": "And it is computed in exactly this way you.",
                    "label": 0
                },
                {
                    "sent": "Take all pairs P and then where P is a positive and N is a negative class.",
                    "label": 0
                },
                {
                    "sent": "If we denote the classes as we did in the previous slides with one for positive and zero for negative, we can say class P is greater than class N. And then you count how many of these pairs are correctly scored, meaning that the class, the score of the positive is greater than the score of the negative.",
                    "label": 0
                },
                {
                    "sent": "Actually, I shouldn't say score, it actually only looks at the rank, but all of the methods that we look at that we look at now they produce some sort of score, but you can in principle also rank without the score.",
                    "label": 0
                },
                {
                    "sent": "So you can't correct if the score is equivalent with the order that is given by the classes and go see the area under the Roc curve is just the percentage of pairs that respect this order over all pairs.",
                    "label": 0
                },
                {
                    "sent": "And now it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Elevation of multipartite rankings.",
                    "label": 0
                },
                {
                    "sent": "So there is a common measure called DC index and that works exactly the same way except you do not pick a positive example.",
                    "label": 0
                },
                {
                    "sent": "In the negative example but you pick an example of a Class J and you pick another example of a class I smaller than J.",
                    "label": 1
                },
                {
                    "sent": "So you have again.",
                    "label": 0
                },
                {
                    "sent": "2 examples.",
                    "label": 0
                },
                {
                    "sent": "And this one has a higher rank.",
                    "label": 0
                },
                {
                    "sent": "Then this one, and then you again count for all of these pairs.",
                    "label": 0
                },
                {
                    "sent": "P&NP is from a higher rank class.",
                    "label": 0
                },
                {
                    "sent": "Ann is from some lower ranked class.",
                    "label": 0
                },
                {
                    "sent": "You count how many of them respect this relation.",
                    "label": 0
                },
                {
                    "sent": "Also, with this core that are produced and again to see index is the percentage of correct correctly ranked examples in all of these pairs and there is now a sum here because you have.",
                    "label": 0
                },
                {
                    "sent": "If you have C different classes, you have C * C -- 1/2 different combinations of inj here and for all of these you can select the pairs.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the number of all pairs.",
                    "label": 0
                },
                {
                    "sent": "This is the percentage of correct examples, and again, obviously.",
                    "label": 1
                },
                {
                    "sent": "He you see is a special case of the.",
                    "label": 0
                },
                {
                    "sent": "C index with C = 2 seem being the number of classes.",
                    "label": 0
                },
                {
                    "sent": "If you only have two classes and if you compute this thing, it will be the UC.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this index can actually be written rewritten in this way, and this is easy to see because the AUC we have seen that AUC of P&N is the number of correct for P and then divided by the number of P times the number of N examples.",
                    "label": 0
                },
                {
                    "sent": "And if now it would be the number of I and the number of Che examples and if we multiply with these two again, then with this is just the number of correctly ranked examples for.",
                    "label": 0
                },
                {
                    "sent": "The pair of classes I&J and we sum over all of these classes, so this again is the C index.",
                    "label": 0
                },
                {
                    "sent": "But what is interesting here?",
                    "label": 0
                },
                {
                    "sent": "It is now is sum of AUC of pairwise AUC values.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This may sound familiar for people that are have worked with multiclass AUC because there is this formula deal that's that is an extension of AOC which only works for binary classes for multi class is not necessarily ordered, but of course you could also use it for other classes and then this simply the sum of all pairwise AUC's and averaged so it's not the same but the average of all pairwise.",
                    "label": 0
                },
                {
                    "sent": "You see that this.",
                    "label": 1
                },
                {
                    "sent": "Multi class extension of AOC proposed by hand and till in Machine Learning Journal 2001.",
                    "label": 0
                },
                {
                    "sent": "It's also known as the young Kerry Terpstra statistic.",
                    "label": 0
                },
                {
                    "sent": "And both of these that this interesting and we used that as a motivation for our approach, they reduce.",
                    "label": 1
                },
                {
                    "sent": "These measures for multipartite ranking.",
                    "label": 0
                },
                {
                    "sent": "Do a sum of pairwise AUC's measures.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our idea was now to decompose these things into binary classifiers and our one of the things was that we used pairwise classifiers here because there we can directly optimize the probabilities for a pair of classes.",
                    "label": 0
                },
                {
                    "sent": "But we'll see.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out with the conventional approach for multipartite ranking will come to the pairwise kiss in a minute so the conventional approaches you turned it into a big ranking problem.",
                    "label": 1
                },
                {
                    "sent": "Again, you objective is you want to have, for each pair of examples P and then peeping from some class higher ranked than N, you want this core of P to be higher than this core of N. If the class of P is higher than the class event in the training examples.",
                    "label": 0
                },
                {
                    "sent": "And if you assume a linear scoring function, as is usually done, then you can solve this problem in a fairly straightforward way.",
                    "label": 1
                },
                {
                    "sent": "You define a new example X, which is the difference between P&N and then this core of P -- N which is.",
                    "label": 0
                },
                {
                    "sent": "The score of this new example.",
                    "label": 0
                },
                {
                    "sent": "Is the same as the score of P minus the score of N because the scoring function is linear and you see this difference should be greater than or greater than zero if.",
                    "label": 0
                },
                {
                    "sent": "P is a positive example and it should be less than zero.",
                    "label": 0
                },
                {
                    "sent": "If this is a negative example, so we've transformed that in the binary classification problem with a new set of examples X and some of these have to be positive, and some of these have to be negative, fairly easy.",
                    "label": 0
                },
                {
                    "sent": "But we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To decompose this approach into smaller binary problems.",
                    "label": 0
                },
                {
                    "sent": "So one approach that has been proposed a couple of years ago by Frank and Hall at DCML 2001.",
                    "label": 0
                },
                {
                    "sent": "Is an ordered decomposition.",
                    "label": 0
                },
                {
                    "sent": "Assume you have four classes.",
                    "label": 0
                },
                {
                    "sent": "Then you train three binary classifiers, always for N classes N -- 1 classifiers.",
                    "label": 0
                },
                {
                    "sent": "And you start by.",
                    "label": 0
                },
                {
                    "sent": "Using one classifier that discriminates one from 234.",
                    "label": 0
                },
                {
                    "sent": "Another 112 from 3, four and the other the third 1123 from 4.",
                    "label": 0
                },
                {
                    "sent": "So you essentially estimate probabilities.",
                    "label": 0
                },
                {
                    "sent": "You can say that this is estimates the probability that the example is greater than one.",
                    "label": 0
                },
                {
                    "sent": "This is the probability that example is created in two, and this estimates the probability that the example is greater than three.",
                    "label": 0
                },
                {
                    "sent": "So we have three such probability values.",
                    "label": 0
                },
                {
                    "sent": "I will, I'll show you in a minute how to use those.",
                    "label": 0
                },
                {
                    "sent": "Another approach is to pairwise approach.",
                    "label": 0
                },
                {
                    "sent": "There you decompose the problem into.",
                    "label": 0
                },
                {
                    "sent": "C * C -- 1/2 problems, each one discriminating a pair of classes, so one against two, one against three, one against 4, two against 3, two against four to three games for.",
                    "label": 0
                },
                {
                    "sent": "Trained binary classifier for each of them and then you get the probability out of these classifiers.",
                    "label": 0
                },
                {
                    "sent": "For example, this one emits a probability example that you showed to this classifier is rather one than two, or it might be rather 2, then one.",
                    "label": 0
                },
                {
                    "sent": "So this can be interpreted.",
                    "label": 0
                },
                {
                    "sent": "This sort of preference statement and these two approaches have been compared before by myself in HTML 2002.",
                    "label": 0
                },
                {
                    "sent": "For ordered classification.",
                    "label": 0
                },
                {
                    "sent": "So for the task of actually predicting, except borderline or reject.",
                    "label": 0
                },
                {
                    "sent": "And there it show it came out that the pairwise classifier was better or it was at least not certainly not worse than the order classifier was a little bit better in some cases.",
                    "label": 0
                },
                {
                    "sent": "But maybe this was not significant.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "But now we want to do a different thing.",
                    "label": 0
                },
                {
                    "sent": "We do not want to do order classification.",
                    "label": 0
                },
                {
                    "sent": "We want to do this ranking thing and again we wanted to compare these two things to each other and also to the common approach.",
                    "label": 0
                },
                {
                    "sent": "Before we talk about how to use this for prediction.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's think a little bit about the complexity, because this proved also to be an important point in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So the conventional approach essentially, for each pair of examples.",
                    "label": 1
                },
                {
                    "sent": "You produce a new example.",
                    "label": 0
                },
                {
                    "sent": "So the size.",
                    "label": 0
                },
                {
                    "sent": "Of the new example set is of the order N square, and you produce a single set of these.",
                    "label": 0
                },
                {
                    "sent": "So you have one.",
                    "label": 0
                },
                {
                    "sent": "You train one model and this is trained on N square order constraints and these order constraints.",
                    "label": 1
                },
                {
                    "sent": "As we have seen before, they are translated into examples.",
                    "label": 0
                },
                {
                    "sent": "So you have N square examples.",
                    "label": 0
                },
                {
                    "sent": "So if we now have a base learner binary based learner that has a complexity end to the Alpha you have now problem that this end to the.",
                    "label": 0
                },
                {
                    "sent": "2A yeah, Alpha goes up there, so pretty expensive one.",
                    "label": 0
                },
                {
                    "sent": "Do you want?",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "I think I have to do a lot of clicking now.",
                    "label": 0
                },
                {
                    "sent": "No, that's good.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "The ordered approach, on the other hand, it trains C -- 1 models, each one using all of the examples.",
                    "label": 0
                },
                {
                    "sent": "And for each of these C -- 1 models you need end to the Alpha training time, because this is the complexity of the Bayes classifier and you have C -- 1 of them.",
                    "label": 0
                },
                {
                    "sent": "So the order is C times and to the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And the pairwise approach that sort of depends a little bit on the distribution of the examples.",
                    "label": 1
                },
                {
                    "sent": "Of course, in the individual classes.",
                    "label": 0
                },
                {
                    "sent": "The first case is if you distribute them equally.",
                    "label": 1
                },
                {
                    "sent": "All classes have the same size and then you train C * C -- 1/2 models with two N / C training examples each of.",
                    "label": 0
                },
                {
                    "sent": "So each has one, each has two classes and each of the classes as an oversea training examples.",
                    "label": 0
                },
                {
                    "sent": "And this yields a complexity of.",
                    "label": 0
                },
                {
                    "sent": "C C ^2 * 10 to the Alpha over C to the Alpha, and.",
                    "label": 0
                },
                {
                    "sent": "This is this factor here, C2D2 minus Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if if you have a linear classifier Alpha equals one.",
                    "label": 1
                },
                {
                    "sent": "This is about the same as the order approach, but the more expensive your classifier gets.",
                    "label": 0
                },
                {
                    "sent": "If you have a quadratic one.",
                    "label": 0
                },
                {
                    "sent": "This one will be already cheaper than the ordered approach.",
                    "label": 0
                },
                {
                    "sent": "And this is also clear.",
                    "label": 0
                },
                {
                    "sent": "They use the use roughly the same number of or exactly the same number of examples in the total of all of their training sets.",
                    "label": 0
                },
                {
                    "sent": "You can simply multiply this with that and multiply this with that and you will see it's the same number C to C -- 1 * N. But this approach distributes these numbers among much smaller sets, so if the complexity for handling a set of size N. Is end to the Alpha.",
                    "label": 0
                },
                {
                    "sent": "The sum will be cheaper for this approach because the the ends are much smaller and summing up these end to the office is cheaper than if you sum up first and take the result into the Alpha.",
                    "label": 0
                },
                {
                    "sent": "So how are we doing?",
                    "label": 0
                },
                {
                    "sent": "Multi.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Partite prediction now.",
                    "label": 0
                },
                {
                    "sent": "With Frankenhole, remember we have C -- 1 models, each one primitive probability that the class is greater than I for some class I so we have P1P 2P3.",
                    "label": 1
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "This can be used for computing a prediction.",
                    "label": 1
                },
                {
                    "sent": "We don't need to know how this works.",
                    "label": 0
                },
                {
                    "sent": "We want to use.",
                    "label": 0
                },
                {
                    "sent": "We want to use it for computing a score in the multipartite ranking.",
                    "label": 1
                },
                {
                    "sent": "Setting so we want to sort the examples by this score that we're computing here.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a score for each example, and it turns out the very straightforward way for computing a score out of this decomposition is simply sum up these probabilities P1 plus P2 plus P3 probability of the example being greater than one plus probability of the example greater than two plus probability of the example greater than three.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this makes sense here.",
                    "label": 1
                },
                {
                    "sent": "If you have a large class, this will be have a high probability value on all of these three probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you have small class, it will have a low probability on all of these three probability values, so intuitively seems OK. Is you can even show that it is theoretically sensible, because it turns out that the score is actually the expected value of the rank of the example.",
                    "label": 0
                },
                {
                    "sent": "And this can be easily seen by replacing this P1.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability of example being greater than one, which is the probability of it being 2 plus the probability for three plus the probability for four.",
                    "label": 0
                },
                {
                    "sent": "So you have 3 boxes here, 2 boxes here, one box here.",
                    "label": 0
                },
                {
                    "sent": "And then you change the order of summation and you sum up.",
                    "label": 0
                },
                {
                    "sent": "You have three times the probability for two times the probability for three, one times the probability for two, and that is exactly this.",
                    "label": 0
                },
                {
                    "sent": "Expected value, so this makes sense.",
                    "label": 0
                },
                {
                    "sent": "How do we do with the pairwise approach?",
                    "label": 0
                },
                {
                    "sent": "Again we have see.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approximately square examples, and each of them emit classifiers partner and each one emits a probability that the class.",
                    "label": 0
                },
                {
                    "sent": "So the isce classifier emits a probability that the class of the example is.",
                    "label": 0
                },
                {
                    "sent": "I given that we know that the example that the example classes either I or J.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you know it's either one or two, then they want two classifier will tell you is it.",
                    "label": 0
                },
                {
                    "sent": "More likely to be one or more likely.",
                    "label": 0
                },
                {
                    "sent": "To be 2 for computing a prediction from that for classification.",
                    "label": 1
                },
                {
                    "sent": "The usual thing is weighted voting.",
                    "label": 0
                },
                {
                    "sent": "You can show that this minimizes this.",
                    "label": 0
                },
                {
                    "sent": "No, it maximizes.",
                    "label": 0
                },
                {
                    "sent": "Sorry the Spearman rank correlation.",
                    "label": 1
                },
                {
                    "sent": "So it's a good measure, not the bad measure is written here.",
                    "label": 0
                },
                {
                    "sent": "You'd simply sum up these.",
                    "label": 0
                },
                {
                    "sent": "You can imagine these values as a matrix and you sum up the lines of this matrix and you return the line with the maximum value.",
                    "label": 1
                },
                {
                    "sent": "How do you do that for ranking again?",
                    "label": 0
                },
                {
                    "sent": "So again, here is this matrix, and here we are summing up all values that are above the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So we sum up all predictions that are for all classifiers.",
                    "label": 1
                },
                {
                    "sent": "From the point of view of the higher class.",
                    "label": 0
                },
                {
                    "sent": "So again, if you have a high class here, all of these probabilities tend to be high.",
                    "label": 0
                },
                {
                    "sent": "If you have a low class, all of these probabilities tend to be low.",
                    "label": 0
                },
                {
                    "sent": "So intuitively again, it makes sense.",
                    "label": 0
                },
                {
                    "sent": "Another motivation for this approach is if you think back for these multi class AUC formula.",
                    "label": 0
                },
                {
                    "sent": "There we said this is actually.",
                    "label": 0
                },
                {
                    "sent": "This is actually reducing the measure to sum of pairwise AUC values.",
                    "label": 0
                },
                {
                    "sent": "And we thought if you optimize these values with a binary classifier then summing up this course is a reasonable thing to do and will give you.",
                    "label": 0
                },
                {
                    "sent": "Reasonable score actually.",
                    "label": 0
                },
                {
                    "sent": "We tried three variants or two variants of this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one is they want to just showed you the second one.",
                    "label": 0
                },
                {
                    "sent": "Now is not motivated by their multivalued AUC, but it is motivated by the C index that I have showed you in the beginning and here we are just waiting these probabilities with the product of inj.",
                    "label": 1
                },
                {
                    "sent": "So in order to diversify discourse little bit more and another version is where we do not wait with the product but with the sum of these two and this one.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, turns out to be also very well motivated, because in principle this also estimates this expected value.",
                    "label": 0
                },
                {
                    "sent": "The same expected value that is estimated by the frankenhole.",
                    "label": 0
                },
                {
                    "sent": "Method, but it estimates it was different probabilities.",
                    "label": 0
                },
                {
                    "sent": "So here a product of a conditional probability that is the one that is learned times the probability of an example being in the class I or J.",
                    "label": 0
                },
                {
                    "sent": "So this is the number.",
                    "label": 0
                },
                {
                    "sent": "This is the sum here.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll show you the experiment I.",
                    "label": 0
                },
                {
                    "sent": "These three methods were actually more or less equal, and at some point we decided that we will only use these two, so you will only see these two in the experiments, but in particular these two they were almost identical, and as you will see, these two were already quite similar, so we evaluated this honor ranking scenario we used, we simulated.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "21 ranking sets by discretizing regression sets with five into five classes with equal frequency, and we also had four real ordered classification sets.",
                    "label": 1
                },
                {
                    "sent": "So in total we had 25 sets, 5 iterations of tenfold classification.",
                    "label": 1
                },
                {
                    "sent": "The base classifier was logistic regression for the binary decomposition methods because we wanted to get reasonable probabilities and we compare to rank SVM as an example for a method that.",
                    "label": 0
                },
                {
                    "sent": "Uses the.",
                    "label": 0
                },
                {
                    "sent": "The traditional approach of learning.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The whole thing.",
                    "label": 0
                },
                {
                    "sent": "So in brief, the experiment.",
                    "label": 0
                },
                {
                    "sent": "The result of the experiment.",
                    "label": 0
                },
                {
                    "sent": "Contrary to what we had expected.",
                    "label": 0
                },
                {
                    "sent": "The FRANKENHOLE variant outperforms the binary.",
                    "label": 0
                },
                {
                    "sent": "The pairwise classifiers, and they're about equal to the SVM rank.",
                    "label": 0
                },
                {
                    "sent": "So if you see here the frankenhole, the numbers in brackets.",
                    "label": 0
                },
                {
                    "sent": "Here they are the rank in this scenario, this is C and extra Sister York area Step statistic.",
                    "label": 0
                },
                {
                    "sent": "So on the first data set, Franklin told was the best.",
                    "label": 0
                },
                {
                    "sent": "The weighted version was the second best.",
                    "label": 0
                },
                {
                    "sent": "The unweighted was the 3rd president.",
                    "label": 0
                },
                {
                    "sent": "ESPN rank for the force.",
                    "label": 0
                },
                {
                    "sent": "Here is the average.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Here are the average ranks over all these ranks, and if you do and any test, it turns out that this difference is actually significant critical rank difference for this problem would be 0.88, so this is better than all the three and the other three are distinguishable.",
                    "label": 1
                },
                {
                    "sent": "There is a little bit of advantage for the weighted version for the index, which is not surprising because this was the measure that motivated this version.",
                    "label": 0
                },
                {
                    "sent": "This difference vanish is here if you go to the unweighted version, but essentially there is no diff.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we need this.",
                    "label": 0
                },
                {
                    "sent": "So we double checked our.",
                    "label": 0
                },
                {
                    "sent": "OK, we double checked our expectations by also trying again.",
                    "label": 0
                },
                {
                    "sent": "Repeating this experiment from for classification and there again it turned out the pairwise classifier 15 wins the Frankenhole method nine wins, so we really have this phenomenon for ranking.",
                    "label": 0
                },
                {
                    "sent": "There the pairwise approach performs worse for classification.",
                    "label": 0
                },
                {
                    "sent": "It seems to do.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit better and the problem it seems is.",
                    "label": 0
                },
                {
                    "sent": "With Discord, Discord is computed as the sum of all probabilities, and some of these probabilities are actually estimated from what we call a non competent classifier.",
                    "label": 0
                },
                {
                    "sent": "So if the example is from Class 1.",
                    "label": 1
                },
                {
                    "sent": "Then this all the probabilities involving the Class 1.",
                    "label": 0
                },
                {
                    "sent": "Have been estimated from examples that include examples from class One.",
                    "label": 1
                },
                {
                    "sent": "These probabilities have been estimated from examples that never have seen from datasets that have never seen an example from class One.",
                    "label": 0
                },
                {
                    "sent": "So in some sense the same problem also appears in classification, but there it doesn't really matter that much and I don't have the time to explain that now.",
                    "label": 0
                },
                {
                    "sent": "But if you want you're welcome to ask me at the poster.",
                    "label": 0
                },
                {
                    "sent": "So this problem is much more.",
                    "label": 0
                },
                {
                    "sent": "It is much worse for for ranking this is what we think is happening here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to summarize what we've seen.",
                    "label": 0
                },
                {
                    "sent": "We have shown that multipartite problems can be solved by decomposing them into binary problems, which is a technique known from other problems.",
                    "label": 0
                },
                {
                    "sent": "We have shown that this approach is more efficient in the convection than the conventional approach and among the different decompositions.",
                    "label": 0
                },
                {
                    "sent": "It seems to be the case that they ordered.",
                    "label": 0
                },
                {
                    "sent": "That the order decomposition is a little bit better or actually significantly better than the pairwise decomposition.",
                    "label": 0
                },
                {
                    "sent": "So this wraps up to talk.",
                    "label": 0
                },
                {
                    "sent": "I'm ready to take questions if there is still time.",
                    "label": 0
                }
            ]
        }
    }
}