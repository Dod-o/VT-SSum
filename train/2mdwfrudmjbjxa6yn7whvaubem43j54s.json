{
    "id": "2mdwfrudmjbjxa6yn7whvaubem43j54s",
    "title": "Large-Scale Learning and Inference: What We Have Learned with Markov Logic Networks",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_domingos_lsliwwlmln/",
    "segmentation": [
        [
            "Going to be joint work with various people in my group, but I'm also going to cover them by various other people.",
            "So when I say we, I don't just mean my group, I actually mean you know the various.",
            "Research."
        ],
        [
            "Groups working in this area.",
            "So for those of you not familiar with Markov logic, I'm going to start with a one slide introduction for those of you familiar with it.",
            "You know this will just be a review.",
            "So the idea of Markov logic networks is to use first order logic as a language to compactly specify very large non IID models.",
            "So we're going to be doing non IID model mean that we don't assume that the data points that we're learning from our independent.",
            "We're going to have very big graphs where you know there's connections between potentially everything.",
            "And the Markov logic network is just a set of formulas in first order logic with weights.",
            "That's all and each formula.",
            "Is a template for constructing features of a Markov random field or of a log in your model.",
            "And the way it works as a feature is that the formula is going to have variables that range over objects in the domain, like for example people or web pages or whatever.",
            "And we're going to replace those variables by objects in all possible ways and as a result their template becomes materialized.",
            "There's a large, potentially very very large number of features.",
            "So, for example, suppose you wanted to build a standard type of social network model, where, for example, you say that people who are friends influence each other.",
            "Say I smoke and you're my friend.",
            "That makes you more likely to smoke, right?",
            "People do this type of Markov random field.",
            "Routinely in social networks you might write a formula like this smokes of X and friends of XY implies smokes of Y.",
            "Right now when I replace X&Y by say, you know Anna and Bob, this says the defender smokes and she's friends with Bob.",
            "Then Bob smokes, right?",
            "And by giving away to this what it means is that I've created a feature for the particular Anna Bob here, right?",
            "And then if the feature is true, meaning say they both smoke and their friends, then the probability goes up.",
            "Otherwise it goes down OK, so at the end of the day, we're going to have you know when.",
            "Now, of course, this is just one formula, right in a real MLN.",
            "You might have a couple dozen formulas, and you know then you know, just by writing a few things like this correctly set up a very rich and very large model.",
            "At the end of the day, what you have is a Markov random field.",
            "And that Markov random field has the you know the probability has the usual form light, it's you know the normalized exponential did some overall the 1st order formulas of the weight of the Formula Times the number of true groundings of the formula in the in the particular world that you have.",
            "And the world is represented by a database of what relations are true and false.",
            "Like you know anime smoke.",
            "Diana may not smoke and so forth.",
            "So this is basically what's going on.",
            "1st Order Logic is the special case of Markov logic that you get when you let the weights go to Infinity, and then you pay an infinite penalty for violating a formula.",
            "The types of graphical models that we all know and love can for the most part, I'll be easily and compactly represented in Markov logic.",
            "So if you want to do an HMM or CRF, provision network or whatever, you can write those compactly Markov logic, and you can also do a continuous or hybrid Markov logic networks.",
            "Here I'll focus mainly on the on the discrete case, and we've also worked out to the police to a large extent the semantics of infinite Markov logic networks, but again, I won't go into that here."
        ],
        [
            "By the way, I you know I'm going to try to cover a lot of topics in this talk.",
            "Um, there's probably more here than I have time for it.",
            "So what I'm going to try to do is just give the high level ideas and then you know.",
            "Feel free to ask questions because whatever things people are more interested in, we can.",
            "We can go more into them.",
            "OK, and of course you know I'm available during the break and then afterwards you know to talk with people and whatnot.",
            "So Markov logic is actually only a few years old, but in the meantime you know we end.",
            "All other people have made a lot of progress.",
            "And that's what I want to talk about here, not just the algorithms, but also some of the things that we've learned which you know, I think, are more general.",
            "So in Markov logic, so.",
            "In some ways, the details of the language don't actually matter.",
            "You might not be interested in Markov Logic's representation language, so you can think about a lot.",
            "I'm going to talk about here is just dealing with very large Markov random fields, right?",
            "'cause at the end of the day, that's that's what it is.",
            "And of course, as usual, there's four main problems, and they're here in rough order of increasing difficulty.",
            "There's MVP inference, marginal inference, weight learning, and structure learning, structure learning in this context means actually discovering what are the formulas that holding the domain, as well as learning their weights.",
            "There is another problem that I don't have here, which is maybe the hardest of all of them, which is learning with a lot of hidden variables.",
            "But I will be talking about that at the Deep Networks Workshop this afternoon.",
            "So if you're curious, you can stop by there as well.",
            "So the 1st generation.",
            "We use fairly standard things for all of this, but this is not like the Star Trek analogy.",
            "But this one actually came first.",
            "Perhaps the most interesting thing is that we use these weighted satisfiability techniques for MLP inference, and that's actually the one.",
            "So basically what I'm going to do here is I'm going to in the talk I'm going to go through.",
            "I'm going to go through this matrix in roughly row order and I'm going to focus mainly on the upper triangular part because the stuff down here is not so interesting.",
            "It's more just like you know for comparison.",
            "So the one thing in the first generation, the one sort of like different things that we were doing, was that we're using where it says viability for MVP in France.",
            "And I will.",
            "I'm going to talk just a little bit about that.",
            "Promotional inference we use Gibbs sampling which was good to get us off the ground, but Gibbs sampling really really sucks, right?",
            "That's one of the lessons that we've learned.",
            "Probably everybody knows that he already, but we used to the likelihood through it learning, which is actually fast and robust in many ways.",
            "But you know, sometimes it gives very bad results.",
            "Infrastructure learning.",
            "We use standard techniques from the field of inductive logic programming.",
            "And then you know.",
            "But this only scaled so far, so you know the next generation was doing things like lazy inference.",
            "You know I'm going to talk about this empty set algorithm, which is a really super duper MCMC algorithm using word perception for with learning.",
            "And so like a first or like real native approach with structure learning more recently, other developments have been using cutting points for inference.",
            "This actually interesting to this is a different style of cutting plane inference from the one you know that people like people like David have been doing.",
            "So I think this is also pretty pretty interesting.",
            "And more recently, there's been, you know, uh, an explosion of working this problem of doing lifted inference.",
            "I will talk about that too.",
            "I will talk about the most recent and best with learning that we have and the most recent and best structure learning.",
            "We just a second.",
            "We think from these two generations.",
            "These days we and others are able to learn and do inference on Markov logic networks with millions of variables, billions or more features, tree widths in the thousands.",
            "Thousands of hidden variables and strong dependencies, which is of course one of the things that makes things hard, so I believe we've done.",
            "You know some of the largest graphical models that anybody has today, but of course our goal is to go even further.",
            "Another question.",
            "Indeed, so like I said, I'm not going to cover that here because there's no time for everything.",
            "But, you know, for example, I'm going to talk about how this and this are applied to the Lake in case yeah, of course, you know there's additional difficulties there, and you know that's where the state of the art is the furthest behind that actually is also where we've done some of the most exciting things recently, so yeah."
        ],
        [
            "OK, so let me mention weighted satisfiability, so we all know what the satisfiability problem is, right?",
            "Is given a set of formulas and in general you know I'm going to assume that the formulas have been.",
            "Convert it to clauses, meaning that just disjunctions of literals.",
            "This is what people always do, and so the goal of sat is to find the truth assignment.",
            "You know, assigning true and false to all the logical variables in such a way that all the formulas are made true.",
            "And of course, this is the quintessential NP complete problem.",
            "You can solve all sorts of problems, not just you know in in AI, but in hardware, software, verification, whatnot, by reducing them to set and people have done that a lot, and you know there's a huge community of people working on this problem, and variants of the problem.",
            "And there's you know, advances every year.",
            "And the state of VR right now is that people can solve said problems with millions of variables in clauses in minutes.",
            "And we're talking about hard problems, right?",
            "A lot of set problems are actually very easy, which is very good for us.",
            "But even on the hard problems were actually at the point where we can do this type of thing.",
            "And this is the kind of scale that we want to be able to do.",
            "Inference in Emlenton MRF set.",
            "So there's a lot of technology here that we can use and continue to use as it appears.",
            "So you know one of the things.",
            "One of the things that I hope you know.",
            "People.",
            "Remember from this talk is, you know I was at this workshop yesterday on discrete optimization.",
            "There was a whole set of, you know, very interesting techniques there.",
            "This one actually will did not come up, and it's a big community that we should interact more with.",
            "So how can you use set for MLP inference in LNS?",
            "Well, the idea is very simple.",
            "Often in said the problem is too hard.",
            "Then you can't satisfy all the clauses.",
            "But then you can ask the question of you know what is the assignment that makes as many clauses true as possible, and this is called Max set.",
            "So I'm just going to try to satisfy as many as possible and if the problem is satisfiable then the Max is the number of clauses.",
            "Now there's a version of this called weighted Max Sat, where the clauses have weights and what you're trying to do is maximize the sum of the weights of the clauses that are satisfied.",
            "OK. And there's a number of algorithms for this.",
            "The beauty of it is that imapi inferencing lens is just weighted Max sat, no more, no less.",
            "If you look at this expression here, Whoops.",
            "Right, we want to maximize oops.",
            "We want to maximize this right?",
            "To maximize this, all we have to maximize the argument of the exponential.",
            "And this is just the sum over all the ground clauses or for each cause you know one or zero depending on whether the clause is satisfied or not.",
            "So we can take an off the shelf Max SAT solver and just use it for inference in Ellen's right.",
            "And we get the benefit of, you know, 50 years of research without lifting a finger.",
            "And this is exactly what we did.",
            "So what we use is currently is.",
            "So by the way we have this software package called Alchemy that implements these algorithms.",
            "There's also a number of other implementations, the one that we use there is Max Walk set.",
            "This is a local stochastic search type of algorithm because it's currently the best one, but you know tomorrow it could be another one, and in some sense we don't care what is the.",
            "What is the SAT solver that we're using for this?",
            "OK, so the problem with this though.",
            "Is that?",
            "On most problems of nontrivial size, if you try to run a SAT solver on the problem, or indeed anything else, the first thing that happens is that nothing happens because it blows out of memory.",
            "We just have too many groundings, right?",
            "There's an exponential number of ground exponential in the number of arguments of the predicates, right?",
            "So if I have a billion people on Earth, just a friends XY predicate has a billion billion possible groundings.",
            "And that's bad news, right?",
            "So things, don't even you know.",
            "We can't even get off the ground because things don't fit in memory.",
            "And even if they fit in memory, just creating the ground network will take probably too long.",
            "So what can we do?"
        ],
        [
            "Well, this is where the first sort of like new idea comes in and this is to exploit something that you know we all.",
            "We are all busy exploiting all over computer science and machine learning in particular which is sparseness.",
            "Alright, So what kind of sparseness can we exploit here?",
            "We can exploit the fact in most domains most logical atoms, IE most random variables are false.",
            "So for example, take something like the predicate friends XY.",
            "Most people are not friends with most people, right?",
            "There's 6 billion people on Earth, but you're you know friends with, maybe, you know he doesn't.",
            "A few hundred of them, maybe a few 1000 if you're on Facebook, but you know most of them you don't really interact with anyway, right?",
            "So the vast vast majority of these groundings are false.",
            "As a result of the vast majority of the items being false, the vast majority of the clauses are trivially true because their preconditions don't fire.",
            "So for example, looking at the clothes that I had an example before, smoke, sex and friends XY employee implies smokes, Y if X&Y are not friends right then this clause is already satisfied irrespective of whether they smoke or not, so in that case there is no dependency between these people and I can actually just ignore this.",
            "So the idea lazy inference is that instead of you know grounding out all the atoms on all the clauses to start with, which is going to be incredibly wasteful.",
            "What I'm going to do is I'm going to say well by default an Atom is false, and by default clauses true, and then I only create the ones that you know the infant says are different from that, and I do this Leslie.",
            "Basically, I start with the evidence I have said database of who's friends with whom and who smokes, and maybe I want to figure out you know what other people smoke right?",
            "And I start out with only those and then you know, I propagate.",
            "I ground out the clauses that I made false by those right and then I see what happens.",
            "I need to flip to make those clauses too and I just keep doing this.",
            "OK. And you know The upshot of this is that this vastly reduces the time and memory that we need.",
            "To do this.",
            "We've applied this on some problems where we got a reduction in memory and time of a factor of around 400,000.",
            "So we were 400,000 times faster and use 400,000 times less memory.",
            "There is the way we're able to do the experiences that we were not able to run the ungrounded version, but we you know, we took the curves and then we extrapolated them.",
            "They're fairly clean polynomials, and you know if you once we extrapolate the non laser vision to the size of the whole database.",
            "This is the kind of speed up that we got."
        ],
        [
            "Is 1 idea.",
            "More recently, Sebastian Riedel has has developed this cutting plane approach to Marco Logic inference, which I think is also very interesting.",
            "In some ways.",
            "It's an alternative to lazy inference that you have not yet been compared yet, but it would be interesting to do that.",
            "So the idea here is so we're going to have a very large number of constraints, potentially billions of clauses.",
            "There is no way to try to solve the problem with all of them at once.",
            "So the idea is as in general with cutting planes.",
            "I some familiar with the police, the general idea seems like let us start by solving the problem with a small subset of constraints.",
            "And then we have a solution and then we see if there are any any constraints that that solution violates.",
            "If that solution doesn't violate any other constraints, we're in luck.",
            "We we can stop, we're done.",
            "If it does, then we add those violated constraints in and we repeat.",
            "OK, so we're just growing the set of clauses that we have to consider as little as possible.",
            "The reason this is extremely powerful is that typically there is massive redundancy among the constraints.",
            "Massive massive redundancy.",
            "So typically what happens is that by adding in only a small fraction of the constraints, we actually solve the whole problem and the whole thing is much, much faster than before.",
            "OK, so in Markov logic violates or not now.",
            "Now the tricky problem here is how do I efficiently find the violated constraints right?",
            "Because if I can do that efficiently, then I'm hosed anyway, right now in Markov logic, that violated constraints are false clauses, right?",
            "I want all those clauses to be true so that I can add their weight right?",
            "So the violated constraints are false clauses.",
            "Well, the first clause, if you remember your logic is the same thing as a true conjunction, right?",
            "So what we're looking for is true conjunctions in the database.",
            "They represent the evidence in this state of the world at that point, right?",
            "Well, finding that is actually what database people do for a living.",
            "Right databases are all about issuing what are called conjunctive queries in the conjunctive queries.",
            "Just taking your first order formula, it's a conjunction and finding all the true ground.",
            "All the true groundings of that conjunction.",
            "So what suggestion does is he uses an in memory database engine to efficiently find all the violated clauses.",
            "Efficiently meaning linear in the size of the data, right?",
            "So this is very good.",
            "Another good thing about this is that notice that nothing that I said here says anything about what the base constraint solver is that I'm going to use.",
            "In fact, the best constraint solver could be anything, and he actually did this with both LP and with Max Walk set and again tomorrow.",
            "It could be done with something else.",
            "That constraint solver here is treated as a black box.",
            "OK, so notice notice that this is different from the type of cutting plane algorithm that you know people like David and Tommy.",
            "And others have been working on.",
            "And it's it's.",
            "So it's like an orthogonal use of cutting points, right?",
            "Because what we're using here is like we're looking at the original constraints in the problem, and there's too many and we're only adding some of them.",
            "It's also much more scalable for a couple of reasons.",
            "One is that.",
            "This does not materialize all the constraints and is not materialized variables until it needs to.",
            "Right in this case here we are working with all the variables from the one.",
            "Here we only create the variables that we need.",
            "And the other thing is that finding the violated constraints here is much more efficient, because it's done in this linear time using using a database query engine.",
            "So you know, this way we can.",
            "You know the problems with millions of variables and billions of constraints in time that actually does not depend in the best case on that.",
            "And if the base solver is exact.",
            "We can guarantee that by doing this you also get the exact optimum solution, and indeed he observed this in a couple of the problems that he applied this to OK, so let's talk about marginal inference for a second yeah.",
            "It's leading the size of the database that represents the state of the inference.",
            "Right, so remember we're in database land now, right?",
            "So in database line what happens is that you're always linear in the size of the data, otherwise you know it's not, but you're exponentially in the length of the query.",
            "Right, so if the clause is very long, you know you could have a problem because you know your exponential in the length of the clause.",
            "So if you if you have a conjunction over three or four things, you're in good shape.",
            "If you have a conjunction over 10 things, you might not be in good shape.",
            "OK.",
            "Nevertheless, in practice this is very fast.",
            "I mean also the database is being used in memory so you know.",
            "But another interesting thing is that this could potentially use to scale up even more.",
            "By doing this, you know on disk and what not."
        ],
        [
            "OK, so let's talk about marginal inference for a second.",
            "So we started out using you sampling and Gibbs sampling works.",
            "If your model is is not too large and the dependencies are not too strong.",
            "You know this, I think is basically the bottom line and Gibbs sampling.",
            "If you have strong dependencies, you're in trouble.",
            "If you have large models meaning anything more than 10s or even maybe hundreds of variables, you're in trouble.",
            "If you have both strong dependencies in a lot of variables, Gibbs sampling is completely hopeless.",
            "OK, So what can we do instead?",
            "Well, we've developed this empty set algorithm.",
            "Which you know, really, without empty set we wouldn't have gotten very far.",
            "So the basic observation that we started with is that we want to be able to handle deterministic dependencies, right?",
            "Because you know, you could have deterministic dependencies stated by by formalizing Markov logic.",
            "And the terministic dependencies.",
            "They break MCMC right?",
            "Because they make the Markov chain reducible.",
            "And so not now your theorems about.",
            "You know the detail.",
            "Balancing will not don't apply.",
            "You start off in this region.",
            "It doesn't communicate with that region, so you never get there, OK?",
            "In practice, right?",
            "So even even on a fundamental level, you know MCMC can help the terministic dependencies, but in practice what happens is that if you have strong probability dependencies, that breaks things too because you just take far too long, meaning forever to actually move from one region to another.",
            "So the question is, what can we do about this year?",
            "Oh, a subset, a subset.",
            "All it takes.",
            "All it takes is you know one or two, uh, deterministic formulas in the MLN to make stuff you know, go to hell.",
            "Basically.",
            "So what can we use here?",
            "Well, there's this great idea that people in statistical physics had in the 80s called the Swinson Rang algorithm.",
            "Very famous algorithm there, because you know, it's bad things up in things like using models by you know, three or four orders of magnitude compared to Gibbs sampling.",
            "So what's the idea?",
            "Swendsen Wang is what's making life hard for us.",
            "Is the strong dependencies.",
            "So we're going to introduce auxiliary variables to capture those dependencies.",
            "Let's call the auxiliary variables you and the original variables X, and then what we're going to do is, alternately sample you given X&X given you.",
            "Right, and by this unusually sampling you given X is easy, and then sampling X given you is now now that I've captured the dependencies sampling the X is, you know I can move much more efficiently than I could before.",
            "The problem with some mango is that he only works racing models pairwise connections and you know positive interactions.",
            "So what we did with NC set was to generalize Swinson link to arbitrary closets.",
            "What happens in Swenson Wing is that I randomly decide that these guys that are all you know pointing up will have to stay pointing up.",
            "And then I flip them all at the same time.",
            "Or I don't flip them.",
            "OK, this is basically what happens, and so Swinson, when can go from an all white picture to an all black picture in one move, which you know forgive sampling, you wouldn't happen before the end of the universe.",
            "So now the constraints that we have because we have arbitrary clauses are not so easy to satisfy.",
            "So in order to sample X, given you what I need to do is find a satisfying assignment to a random subset of the clauses.",
            "But for that we have a SAT solver.",
            "So we call this senses set because it's using its MCMC with the SAT solver as a subroutine to actually quickly find new.",
            "Places to go to and this is not an intelligent way.",
            "Such detail balance is satisfied and what not.",
            "The bottom line is that this is orders of magnitude faster than things like Gibbs sampling, and, you know, simulated tempering and various other techniques that people use.",
            "How many orders of magnitude?",
            "We don't really know because you know, keep sampling never converges, right?",
            "We're looking at, you, know the whole log like holdout log, likelihood test data for example, and empty set is here and give sampling is over there.",
            "And you know this just goes on like this.",
            "And again, as I make as we Jack up the weights of the clauses Gibbs sampling, you know goes to hell very quickly.",
            "And you know, me said basically."
        ],
        [
            "Mine.",
            "So one more idea on the topic of insurance and this is lifted inference.",
            "List the difference.",
            "Maybe it's the most exciting of all these ideas, for instance so far, and here's what it is.",
            "Consider belief propagation.",
            "Suppose that we're doing both propagation under very large network on a small network.",
            "This.",
            "None of this was really matter.",
            "Many nodes are probably interchangeable in the sense that they send and receive the same messages throughout BP.",
            "I have a million nodes that are all sending the same messages to their factors and getting the same messages back from their factors.",
            "Right, So what I want to do is I want to group these nodes into super nodes.",
            "And form what we call the lifted network, which is going to be much smaller than the original network.",
            "And then I just do you know things back and forth on that lifted network.",
            "On a good day, the size of the lifted network is going to be the size of the MLN.",
            "So if I have if I have an MLN with 100 formulas, I'm going to have 100 features or 100 factors.",
            "On a bad day, this is going to be as big as a full ground network, but on an average day this could give us, you know fantastically large speedups, in fact, potentially infinite speedups.",
            "Doesn't potential in these methods to actually be able to do inference over infinite networks in finite time?",
            "We haven't solved that yet, but I believe it's possible, so actually had a graphical illustration of the idea here, but I'm actually going to skip it."
        ],
        [
            "Time is getting a little short, so let me now talk a little bit about learning.",
            "So weak learning, we want to do it learning in Markov logic networks and you know, we started out using pseudo likelihood with LB.",
            "FGS is the solver standard things.",
            "This is fairly fast and robust, but it can get very bad results sometimes.",
            "The next thing that we tried was to generalize the voted Perceptron algorithm to the case of melons.",
            "So voted Perception is basically using is basically simple gradient descent within, you know in the original perceptron Viterbi to find the MAPY.",
            "And then the gradient is the difference between the MAPY in the data.",
            "Why and there isn't this works is that if the posterior distribution is peaked right, taking them AP State is actually good approximation and all we have to do to apply this to Markov logic is to replace Viterbi with something like Max Walk set.",
            "Unfortunately, there's and this works well for certain things, but there's a couple of big problems with this.",
            "One of the problems is that you know when you're doing part of speech tagging and things like that.",
            "There's only one mode right then.",
            "Life is good.",
            "The problem in general M lens or general MRF is that we have multiple modes.",
            "And not just learning based on find the single best pick.",
            "Be a really bad idea, right?",
            "So what can we do?",
            "Well, how about instead of doing MLP inference we just use MCMC to sample OK, so the next thing the next thing that we tried was contrasted divergent but contrasted, divergent didn't help because all it does is it takes a few steps.",
            "You know there typically will stay within the same mode, so I'm still having the problem that I'm not sampling other modes.",
            "And if at inference time I fall into a different more than the one that I trained for, the results going to be garbage.",
            "With empty set though, this is alleviated because MC side has the ability to jump promote to both in one step, it just runs the SAT solver to find another mode that could be completely different from the one that we're in OK.",
            "So this is actually very.",
            "This works pretty well.",
            "You know this is far far far from a solved problem, but definitely using MC side is a good idea.",
            "Another thing that's a very good idea is that each each step start MC set from the in state of the previous infants.",
            "At each step of weight learning.",
            "So there's been a couple of papers by Tillman and hidden about doing this.",
            "We actually independently found the same thing.",
            "This is definitely a very good idea.",
            "This actually gives you a whole bunch of different benefits, right?",
            "One is that it reduces burning, right?",
            "Because I start from an initial state that's already.",
            "Probably in the right ballpark and the other one is that if the wait start at zero, the learning itself does a kind of deterministic annealing right?",
            "So this this actually gives you a lot of bank for very little bug."
        ],
        [
            "However, there is another big problem.",
            "Which is the problems that we're solving are extremely ill conditioned.",
            "So you know in, for example, in training neural networks, so conditioning happens when you know this.",
            "Let's say this is my.",
            "Likelihood landscape right as a function of the weights, and here's my optimum right when when my what I have is a Valley, right?",
            "Then the gradient doesn't actually point towards the minimum, right?",
            "And you know, in neural network land you know.",
            "So in the conditioning number is the ratio of this dimension to this dimension, and you know conditioning number in the 10s or hundreds is considered bad, right?",
            "In Markov logic we routinely run into conditioning numbers in the thousands.",
            "Condition numbers in the thousands means that this ellipse actually just looks like a straight line going like this.",
            "Another gradient descent, this China zigzag down that street and it's completely hopeless.",
            "And the reason this happens is that I have clauses with millions more groundings than others, right?",
            "So there is no learning rate that's actually going to be able to, you know, not oscillate in one dimension will actually making enough progress in another.",
            "So what can we do?",
            "Well, we can use 2nd order methods like was a Newton conjugate gradient and whatnot, you know, because they can do this problem right?",
            "They can find a better you know, direction and what not.",
            "The problem for us though, and you know for MRF's in general, is that the lines these methods, used line searches and the line searches require inference.",
            "And the inference has to be approximating general right?",
            "And you know, line searches with approximate functions you know could completely blow up because you need to bracket the minimum.",
            "And if you actually think you're breaking the minimum, you're not.",
            "You know your host.",
            "So what can we do?",
            "Well, people often use the casted gradient methods, which are often competitive with these right?",
            "But the problem that we have here is that in Markov logic the date is not IID.",
            "The thing that makes sarcastic gradient comparatives, I do updates after I see each small batch of examples in here.",
            "It's all one big connected graph, so that doesn't work either.",
            "So what can we do?",
            "Well, the solution that we found is actually an idea, again from the neural network literature called scaled conjugate gradient.",
            "Skill conjugate grains is a conjugate gradient algorithm that does not use line searches.",
            "It actually uses, so it picks the search direction using the standard conjugate gradient idea, and then it uses the Hessian to pick the step size right.",
            "It's just solving the quadratic problem as you would in the Newton step, and so we don't need the line search, and that's great.",
            "And now there's still the problem that the Hessian for us is going to be very, very large, right?",
            "If I have a million weights, the hashing is going to be a million by million, so can't do that.",
            "So what we do is.",
            "We can the Hessian Fortunately only appears inside the quadratic form.",
            "The search direction transpose times the Hessian times the search direction, and So what we do is we compute the quadratic form directly inside MC set.",
            "So we pushed the DOT product inside the sum and then for each sample we do that all product that we need to do and So what we get out of empty set is never quadratic in the number of weights, and without that this this wouldn't work.",
            "And then we also it's also very important to the preconditioning we use the universe dayag."
        ],
        [
            "As usual.",
            "OK, finally structure learning.",
            "So to do structure learning in Markov logic networks, you know the first idea is to just use inductive logic programming, right?",
            "That's what people in that field do for living is they discover clauses from data.",
            "And that that indeed was the first thing that we tried.",
            "That didn't work very well though, because in IRP people are typically optimizing things like the accuracy or you know some sort of like measure of coverage and whatnot.",
            "And we're learning a probabilistic model here.",
            "So this is a very different.",
            "This is a very different problem.",
            "So what we need to use is, you know a likelihood or pseudo likelihood as the objective function and some sort of structure prior to avoid overfitting.",
            "And you know this is what we typically do.",
            "More recently, actually Ray Mooney has found that you can actually use this to over generate clauses and then use L1 pruning.",
            "This actually works pretty well.",
            "Now there's a couple of big problems when you do this, which is that for each candidate structure change we need to compute the optimal weights, right?",
            "And if we're going to try millions of structure changes, this isn't going to work.",
            "However, what you can do that again works very well is start from the current weights and relax the convergence thresholds for sale.",
            "BFG S We're just trying to find what the best candidate is once we pick the candidate, we can do things to convergence and this really really speeds things up.",
            "The other problem is that.",
            "We need to compute the sufficient statistics by essentially showing queries against the database of evidence, and that itself can be an intractable problem, and then we learning you only have to do that once at the beginning and then you have the sufficient statistics.",
            "You know for the entire algorithm here.",
            "Every time I create a new clause, I need to go back and gather some more sufficient attic.",
            "So again, done naively, this will really kill you, but here's the thing that really helps us use subsampling.",
            "Right, I may have a billion billion groundings of that clause that I showed with friends XY, but I don't need to look at all of them.",
            "I can subsample 1000 and see which fraction is true and extrapolate there.",
            "OK, so with these two things actually, you know you can actually do structure learning fairly fast, and then you can use various search methods that like.",
            "I mean, this is actually probably the area in which they have been the most papers with.",
            "Markov logic is structured learning algorithms and you know people have tried many different type."
        ],
        [
            "Of search.",
            "So finally the most recent type of structure, learning that we've done, uses a combination of two very useful ideas, clustering and pathfinding.",
            "So the problem with the standard search methods that are so like, you know, think of the Della Pietra at all algorithm for linear arthritis, just creating features by conjoining variables and the problem with this is that it creates Brazilians and Brazilians of completely useless features, and then you know waste time testing them.",
            "The idea of pathfinding is to do something a little bit smarter, which is to actually only consider features that you know a priority, have some support in the data.",
            "So what's the idea?",
            "Suppose I'm trying to learn a class to predict weather.",
            "Anna and Bob are friends.",
            "Right?",
            "Now what I'm going to try to do is I'm going to try to find other paths connecting and Bob like.",
            "For example, if Anna Works review of A&B as a customer of IBM and Bob works for IBM, then they're more likely to be friends because they met while Bob was consulting for IBM.",
            "So I can look for these paths and then what I'm going to do is in this path I'm going to replace the contents by variables and those are going to be my candidate clauses, so this can really really help.",
            "The other thing that we do is clustering, right?",
            "Why do this for N?",
            "For these people one at a time, right?",
            "Maybe what we can do is we can you know, cluster together all the people who work in the IT industry?",
            "Or you know various according to various properties in general cluster objects that have simulations, the similar objects, and then the graph that I'm doing this on?",
            "Or to be precise, the hypergraph that I'm doing this one is going to be much smaller, so and that gives Me 2 advantages, it's going to make things much more efficient and it is also going to make me much more robust in terms of learning.",
            "Because now I'm not creating and just based on single instances and I'm going to be creating canvas based on, you know whole clusters of instances and you know our most the best current approach to structural in Markov logic.",
            "There's this so it causes objects, then it finds between paths between those clusters and extracts rules from from those."
        ],
        [
            "Those paths OK, and I had a bunch here of.",
            "Interesting ideas and directions for future work, but I guess time is up so I'll just leave that up and take questions.",
            "Maybe I can.",
            "Empty side framework.",
            "We mentioned that it works much better than keep sampling for microfluidic networks, but the question is the models that we're working on.",
            "Models is we have models that have very high connectivity.",
            "A lot of.",
            "Jeff Cable in touch.",
            "You think that amsat style?",
            "Innovation will help.",
            "Or would it break down?",
            "Well, actually, very high connectivity is exactly the big problem that we have in LNS, right?",
            "Is that you look at this clause in their groundings and they create networks with very very high connectivity.",
            "So empty that is actually an algorithm that is good for dealing with problems with connectivity.",
            "Right?",
            "And you know you can take other problems in other MFS and just.",
            "You know.",
            "Basically compile them into the clausal form, and then run inside on them and see what happens, right?",
            "MC Set doesn't do miracles, but.",
            "MC Sat only cares about what the features are that are actually there, and the ones that are relevant.",
            "So you could actually have very high connectivity and not blow up in the way that.",
            "Sort of like more graph based inference methods blow up so.",
            "Hey Cortana, call binary.",
            "So the original empty set algorithm that I talked about here worked only for binary features, But then we extended it to multinomial features and then we extended the continuous features.",
            "So we have something called hybrid empty set.",
            "You know that I didn't talk about here that can actually handle any combination of continuous and discrete variables, and I'd be happy to, you know, tell you how the algorithm works, but yeah, so we can do that.",
            "We have the question, what is learning tractable height?",
            "And honestly, Oh yeah, so thanks for asking that question right.",
            "Then give me an opportunity to talk about this.",
            "The idea is the following is that.",
            "We must be crazy trying to learn MRSI.",
            "Here's why.",
            "Learning structure is an intractable problem, right?",
            "I mean, even for vision networks right where you know this already intractable problem, then inside that problem we have with learning right, which cannot be done in closed form, right?",
            "And then inside that problem we have the problem with doing inference, which itself is intractable, right?",
            "So no wonder that not too many people have tried to learn MRF structure, right?",
            "So is there a way to bypass this problem somehow?",
            "And I believe that there is any ideas the following and you know there's been some literature on this in the guise of learning low treewidth.",
            "Markov random fields, right?",
            "People here probably know about this right?",
            "And the idea there is that I'm going to make sure that I only learn the model where inferences tractable.",
            "Right now, something that makes a lot of these problems go away.",
            "I don't have.",
            "Effectively, I don't have to do this anymore.",
            "I can learn the parameters in closed form and now it's just the problem of doing structure search, right?",
            "This is the people like Northeast rebro and you know, Carlos Guestrin.",
            "I'm going to check in Jeff Bilmes have various algorithms for this.",
            "Very nice idea.",
            "The problem with this though is that it's not remotely practical because the algorithms are typically exponential in the in the tree.",
            "With that you impose it.",
            "Typically the number of variables raised to the tree with the model that you learn.",
            "So in practice they've never been used to do to learn models with more than a tree with three.",
            "Right, and you know, there aren't too many real world problems.",
            "Actually they have a truth of three were interesting problems with two is that could be in the thousands, right?",
            "So what can we do right?",
            "We want to learn, but we but the key point is that the tree with could be large and the inference might still be tractable.",
            "Right, so this is all we exploit, so we have the paper UI last year that was about learning tractable hightree with vision networks.",
            "The way we learn, tractable, right with vision networks is that we compiled the vision network into an arithmetic circuit.",
            "As we were learning it.",
            "And then we penalized by the size of the arithmetic circuit, i.e.",
            "The complexity of the inference, not the complexity of the representation.",
            "If you think about it, penalising the complex of the representations that realistically idea, because two moves that look the same right now, suppose I have a chain right and I have one more node at the end of the chain, right?",
            "Versus I added, I added a link that makes this into a big circle, right?",
            "The representation costs.",
            "This is the same, one of them is completely Pacific and the other just made your life impossible.",
            "So what we do is we penalize the size of the arithmetic circuit.",
            "And this way we're very flexible.",
            "What we can learn.",
            "It's not a little with model anymore, right?",
            "And we can learn a model, and we measure this.",
            "We learn Bayesian networks that had very high tree with, and yet the inference was always trackable.",
            "OK, so the idea here is to extend the same idea first to Markov networks and then to Markov logic networks.",
            "Now you know in the worst case you know even the arithmetic circle will be too large, so there are no miracles.",
            "But I think there's a sweet spot here for doing this type of stuff, yeah?",
            "I just want it.",
            "Well actually, great question.",
            "When we when we decide to do this, we first wasted three months trying to learn arithmetic circuits from scratch.",
            "You know, let's do operations of arithmetic circuits, right?",
            "And do a search over this.",
            "The structure that makes the problem with that is it's too low level.",
            "Right most arithmetic circuits do not correspond to any remotely interesting probabilistic model, so I think we can completely throw away our understanding of what the high level model is that we're building.",
            "We just have to make sure that you know it correspond the arithmetic circuit materialization of it is compact.",
            "The important lesson here, I think is not there is a problem with representation, But the problem is the following is that people in graphical models have been overly focused on the graph structure.",
            "If you only look at the graph structure, you miss all these opportunities.",
            "That's the problem.",
            "We need to start thinking of, you know the model in terms of the features.",
            "The actual features the graph structure could be completely intractable, meaning the two with his large but the feature based model could still be good, and this is what I think we need to do.",
            "Keyboard.",
            "You don't know, yeah.",
            "Exactly, yeah, so the you know if I had to picture what this is going to look like a few years from now, is that under the hood we're doing everything with arithmetic circuits or that type of representation and the learning in the inference all operate on this at the level of the user, we don't talk arithmetic circuits, we talk, you know Markov logic.",
            "But what really happens is always on arithmetic circuit.",
            "Readdress the lipid.",
            "The difference, I think, in order to correctly treat two collections of nodes as same with class, need a symmetry in your home network that that's one of the other.",
            "It seems like a very rare condition will application.",
            "Great questions, so the work that we did here was based on an earlier paper by near freedom and a couple of his students where they what they actually pointed out, right?",
            "It was like a really simple paper at some level was that if you have no evidence right, let's think first of the case where you have no evidence and you have an MLN.",
            "Then you have all these glorious symmetries, right?",
            "You know all the groundings we have exactly the same, so you can do inference on the right.",
            "Another problem is what happens when you have inference.",
            "So when you have evidence right?",
            "How do you influence when you have evidence?",
            "This is where things could get very complicated, but now the way I kind of glossed over the details here, But what happens is that we just break each predicate into three cases that true groundings, the false groundings and the unknown ones.",
            "And now, whether or not this pays off depends on what happens when you start joining them.",
            "When you start joining one predicate with another and you know these groundings with these running things could blow up, but the way we do the lifting is that we keep track of which distinctions have to be made.",
            "Right in the beginning, before we do any propagation, there's only three of these supernodes per predicate that, through the unknown and the false, and then when I start bringing the neighboring once, things start to split more.",
            "Now in the world, so we have a proof that argument finds the minimal lifted network, right?",
            "The question is, how large is that going to be in practice?",
            "And that's going to depend entirely on the domain that we lifted.",
            "Network is never larger than the ground network, right?",
            "So there's never a loss, and on a good day it's orders of magnitude smaller.",
            "Now the most powerful thing that you want to do, and I think with a lot of action is right now is in doing approximate lifting.",
            "Approximate lifting is I lumped together.",
            "The nodes that's in approximately the same messages and then you can actually workout some bounds on this in half and some guarantees on the result.",
            "And with approximate lifting on a large network, you almost always get very large gains.",
            "I mean, you can construct pathological because you know as influence propagates, right?",
            "You know the changes tried to make less and less difference.",
            "Right, you know.",
            "Here's a simple case to think of.",
            "Think of a Markov chain, right?",
            "And I put in, you know, evidence at the beginning of the Markov chain, right?",
            "Without lifted inference, every node is going without approximate lifting.",
            "Every node is going to have a different probability at the end.",
            "With approximate lifting, you know as soon as you're close enough to the stationary distribution you're done.",
            "You have an infinite reduction in the number of nodes.",
            "Time let's, let's think I'll speaker thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to be joint work with various people in my group, but I'm also going to cover them by various other people.",
                    "label": 1
                },
                {
                    "sent": "So when I say we, I don't just mean my group, I actually mean you know the various.",
                    "label": 0
                },
                {
                    "sent": "Research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Groups working in this area.",
                    "label": 0
                },
                {
                    "sent": "So for those of you not familiar with Markov logic, I'm going to start with a one slide introduction for those of you familiar with it.",
                    "label": 0
                },
                {
                    "sent": "You know this will just be a review.",
                    "label": 0
                },
                {
                    "sent": "So the idea of Markov logic networks is to use first order logic as a language to compactly specify very large non IID models.",
                    "label": 1
                },
                {
                    "sent": "So we're going to be doing non IID model mean that we don't assume that the data points that we're learning from our independent.",
                    "label": 0
                },
                {
                    "sent": "We're going to have very big graphs where you know there's connections between potentially everything.",
                    "label": 0
                },
                {
                    "sent": "And the Markov logic network is just a set of formulas in first order logic with weights.",
                    "label": 1
                },
                {
                    "sent": "That's all and each formula.",
                    "label": 0
                },
                {
                    "sent": "Is a template for constructing features of a Markov random field or of a log in your model.",
                    "label": 0
                },
                {
                    "sent": "And the way it works as a feature is that the formula is going to have variables that range over objects in the domain, like for example people or web pages or whatever.",
                    "label": 0
                },
                {
                    "sent": "And we're going to replace those variables by objects in all possible ways and as a result their template becomes materialized.",
                    "label": 0
                },
                {
                    "sent": "There's a large, potentially very very large number of features.",
                    "label": 0
                },
                {
                    "sent": "So, for example, suppose you wanted to build a standard type of social network model, where, for example, you say that people who are friends influence each other.",
                    "label": 0
                },
                {
                    "sent": "Say I smoke and you're my friend.",
                    "label": 0
                },
                {
                    "sent": "That makes you more likely to smoke, right?",
                    "label": 0
                },
                {
                    "sent": "People do this type of Markov random field.",
                    "label": 0
                },
                {
                    "sent": "Routinely in social networks you might write a formula like this smokes of X and friends of XY implies smokes of Y.",
                    "label": 0
                },
                {
                    "sent": "Right now when I replace X&Y by say, you know Anna and Bob, this says the defender smokes and she's friends with Bob.",
                    "label": 0
                },
                {
                    "sent": "Then Bob smokes, right?",
                    "label": 0
                },
                {
                    "sent": "And by giving away to this what it means is that I've created a feature for the particular Anna Bob here, right?",
                    "label": 0
                },
                {
                    "sent": "And then if the feature is true, meaning say they both smoke and their friends, then the probability goes up.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it goes down OK, so at the end of the day, we're going to have you know when.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, this is just one formula, right in a real MLN.",
                    "label": 0
                },
                {
                    "sent": "You might have a couple dozen formulas, and you know then you know, just by writing a few things like this correctly set up a very rich and very large model.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day, what you have is a Markov random field.",
                    "label": 0
                },
                {
                    "sent": "And that Markov random field has the you know the probability has the usual form light, it's you know the normalized exponential did some overall the 1st order formulas of the weight of the Formula Times the number of true groundings of the formula in the in the particular world that you have.",
                    "label": 0
                },
                {
                    "sent": "And the world is represented by a database of what relations are true and false.",
                    "label": 0
                },
                {
                    "sent": "Like you know anime smoke.",
                    "label": 0
                },
                {
                    "sent": "Diana may not smoke and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is basically what's going on.",
                    "label": 0
                },
                {
                    "sent": "1st Order Logic is the special case of Markov logic that you get when you let the weights go to Infinity, and then you pay an infinite penalty for violating a formula.",
                    "label": 0
                },
                {
                    "sent": "The types of graphical models that we all know and love can for the most part, I'll be easily and compactly represented in Markov logic.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do an HMM or CRF, provision network or whatever, you can write those compactly Markov logic, and you can also do a continuous or hybrid Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "Here I'll focus mainly on the on the discrete case, and we've also worked out to the police to a large extent the semantics of infinite Markov logic networks, but again, I won't go into that here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By the way, I you know I'm going to try to cover a lot of topics in this talk.",
                    "label": 0
                },
                {
                    "sent": "Um, there's probably more here than I have time for it.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to try to do is just give the high level ideas and then you know.",
                    "label": 0
                },
                {
                    "sent": "Feel free to ask questions because whatever things people are more interested in, we can.",
                    "label": 0
                },
                {
                    "sent": "We can go more into them.",
                    "label": 0
                },
                {
                    "sent": "OK, and of course you know I'm available during the break and then afterwards you know to talk with people and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So Markov logic is actually only a few years old, but in the meantime you know we end.",
                    "label": 0
                },
                {
                    "sent": "All other people have made a lot of progress.",
                    "label": 0
                },
                {
                    "sent": "And that's what I want to talk about here, not just the algorithms, but also some of the things that we've learned which you know, I think, are more general.",
                    "label": 0
                },
                {
                    "sent": "So in Markov logic, so.",
                    "label": 0
                },
                {
                    "sent": "In some ways, the details of the language don't actually matter.",
                    "label": 0
                },
                {
                    "sent": "You might not be interested in Markov Logic's representation language, so you can think about a lot.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about here is just dealing with very large Markov random fields, right?",
                    "label": 0
                },
                {
                    "sent": "'cause at the end of the day, that's that's what it is.",
                    "label": 0
                },
                {
                    "sent": "And of course, as usual, there's four main problems, and they're here in rough order of increasing difficulty.",
                    "label": 0
                },
                {
                    "sent": "There's MVP inference, marginal inference, weight learning, and structure learning, structure learning in this context means actually discovering what are the formulas that holding the domain, as well as learning their weights.",
                    "label": 1
                },
                {
                    "sent": "There is another problem that I don't have here, which is maybe the hardest of all of them, which is learning with a lot of hidden variables.",
                    "label": 0
                },
                {
                    "sent": "But I will be talking about that at the Deep Networks Workshop this afternoon.",
                    "label": 0
                },
                {
                    "sent": "So if you're curious, you can stop by there as well.",
                    "label": 0
                },
                {
                    "sent": "So the 1st generation.",
                    "label": 0
                },
                {
                    "sent": "We use fairly standard things for all of this, but this is not like the Star Trek analogy.",
                    "label": 0
                },
                {
                    "sent": "But this one actually came first.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the most interesting thing is that we use these weighted satisfiability techniques for MLP inference, and that's actually the one.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm going to do here is I'm going to in the talk I'm going to go through.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go through this matrix in roughly row order and I'm going to focus mainly on the upper triangular part because the stuff down here is not so interesting.",
                    "label": 0
                },
                {
                    "sent": "It's more just like you know for comparison.",
                    "label": 1
                },
                {
                    "sent": "So the one thing in the first generation, the one sort of like different things that we were doing, was that we're using where it says viability for MVP in France.",
                    "label": 0
                },
                {
                    "sent": "And I will.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk just a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "Promotional inference we use Gibbs sampling which was good to get us off the ground, but Gibbs sampling really really sucks, right?",
                    "label": 0
                },
                {
                    "sent": "That's one of the lessons that we've learned.",
                    "label": 0
                },
                {
                    "sent": "Probably everybody knows that he already, but we used to the likelihood through it learning, which is actually fast and robust in many ways.",
                    "label": 0
                },
                {
                    "sent": "But you know, sometimes it gives very bad results.",
                    "label": 0
                },
                {
                    "sent": "Infrastructure learning.",
                    "label": 1
                },
                {
                    "sent": "We use standard techniques from the field of inductive logic programming.",
                    "label": 0
                },
                {
                    "sent": "And then you know.",
                    "label": 0
                },
                {
                    "sent": "But this only scaled so far, so you know the next generation was doing things like lazy inference.",
                    "label": 0
                },
                {
                    "sent": "You know I'm going to talk about this empty set algorithm, which is a really super duper MCMC algorithm using word perception for with learning.",
                    "label": 0
                },
                {
                    "sent": "And so like a first or like real native approach with structure learning more recently, other developments have been using cutting points for inference.",
                    "label": 0
                },
                {
                    "sent": "This actually interesting to this is a different style of cutting plane inference from the one you know that people like people like David have been doing.",
                    "label": 0
                },
                {
                    "sent": "So I think this is also pretty pretty interesting.",
                    "label": 0
                },
                {
                    "sent": "And more recently, there's been, you know, uh, an explosion of working this problem of doing lifted inference.",
                    "label": 0
                },
                {
                    "sent": "I will talk about that too.",
                    "label": 0
                },
                {
                    "sent": "I will talk about the most recent and best with learning that we have and the most recent and best structure learning.",
                    "label": 0
                },
                {
                    "sent": "We just a second.",
                    "label": 0
                },
                {
                    "sent": "We think from these two generations.",
                    "label": 0
                },
                {
                    "sent": "These days we and others are able to learn and do inference on Markov logic networks with millions of variables, billions or more features, tree widths in the thousands.",
                    "label": 0
                },
                {
                    "sent": "Thousands of hidden variables and strong dependencies, which is of course one of the things that makes things hard, so I believe we've done.",
                    "label": 0
                },
                {
                    "sent": "You know some of the largest graphical models that anybody has today, but of course our goal is to go even further.",
                    "label": 0
                },
                {
                    "sent": "Another question.",
                    "label": 0
                },
                {
                    "sent": "Indeed, so like I said, I'm not going to cover that here because there's no time for everything.",
                    "label": 0
                },
                {
                    "sent": "But, you know, for example, I'm going to talk about how this and this are applied to the Lake in case yeah, of course, you know there's additional difficulties there, and you know that's where the state of the art is the furthest behind that actually is also where we've done some of the most exciting things recently, so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me mention weighted satisfiability, so we all know what the satisfiability problem is, right?",
                    "label": 0
                },
                {
                    "sent": "Is given a set of formulas and in general you know I'm going to assume that the formulas have been.",
                    "label": 0
                },
                {
                    "sent": "Convert it to clauses, meaning that just disjunctions of literals.",
                    "label": 0
                },
                {
                    "sent": "This is what people always do, and so the goal of sat is to find the truth assignment.",
                    "label": 0
                },
                {
                    "sent": "You know, assigning true and false to all the logical variables in such a way that all the formulas are made true.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is the quintessential NP complete problem.",
                    "label": 0
                },
                {
                    "sent": "You can solve all sorts of problems, not just you know in in AI, but in hardware, software, verification, whatnot, by reducing them to set and people have done that a lot, and you know there's a huge community of people working on this problem, and variants of the problem.",
                    "label": 0
                },
                {
                    "sent": "And there's you know, advances every year.",
                    "label": 0
                },
                {
                    "sent": "And the state of VR right now is that people can solve said problems with millions of variables in clauses in minutes.",
                    "label": 1
                },
                {
                    "sent": "And we're talking about hard problems, right?",
                    "label": 0
                },
                {
                    "sent": "A lot of set problems are actually very easy, which is very good for us.",
                    "label": 0
                },
                {
                    "sent": "But even on the hard problems were actually at the point where we can do this type of thing.",
                    "label": 0
                },
                {
                    "sent": "And this is the kind of scale that we want to be able to do.",
                    "label": 1
                },
                {
                    "sent": "Inference in Emlenton MRF set.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of technology here that we can use and continue to use as it appears.",
                    "label": 0
                },
                {
                    "sent": "So you know one of the things.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I hope you know.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "Remember from this talk is, you know I was at this workshop yesterday on discrete optimization.",
                    "label": 0
                },
                {
                    "sent": "There was a whole set of, you know, very interesting techniques there.",
                    "label": 0
                },
                {
                    "sent": "This one actually will did not come up, and it's a big community that we should interact more with.",
                    "label": 0
                },
                {
                    "sent": "So how can you use set for MLP inference in LNS?",
                    "label": 0
                },
                {
                    "sent": "Well, the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Often in said the problem is too hard.",
                    "label": 0
                },
                {
                    "sent": "Then you can't satisfy all the clauses.",
                    "label": 0
                },
                {
                    "sent": "But then you can ask the question of you know what is the assignment that makes as many clauses true as possible, and this is called Max set.",
                    "label": 1
                },
                {
                    "sent": "So I'm just going to try to satisfy as many as possible and if the problem is satisfiable then the Max is the number of clauses.",
                    "label": 0
                },
                {
                    "sent": "Now there's a version of this called weighted Max Sat, where the clauses have weights and what you're trying to do is maximize the sum of the weights of the clauses that are satisfied.",
                    "label": 0
                },
                {
                    "sent": "OK. And there's a number of algorithms for this.",
                    "label": 0
                },
                {
                    "sent": "The beauty of it is that imapi inferencing lens is just weighted Max sat, no more, no less.",
                    "label": 0
                },
                {
                    "sent": "If you look at this expression here, Whoops.",
                    "label": 0
                },
                {
                    "sent": "Right, we want to maximize oops.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize this right?",
                    "label": 0
                },
                {
                    "sent": "To maximize this, all we have to maximize the argument of the exponential.",
                    "label": 0
                },
                {
                    "sent": "And this is just the sum over all the ground clauses or for each cause you know one or zero depending on whether the clause is satisfied or not.",
                    "label": 0
                },
                {
                    "sent": "So we can take an off the shelf Max SAT solver and just use it for inference in Ellen's right.",
                    "label": 0
                },
                {
                    "sent": "And we get the benefit of, you know, 50 years of research without lifting a finger.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what we did.",
                    "label": 0
                },
                {
                    "sent": "So what we use is currently is.",
                    "label": 0
                },
                {
                    "sent": "So by the way we have this software package called Alchemy that implements these algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's also a number of other implementations, the one that we use there is Max Walk set.",
                    "label": 0
                },
                {
                    "sent": "This is a local stochastic search type of algorithm because it's currently the best one, but you know tomorrow it could be another one, and in some sense we don't care what is the.",
                    "label": 0
                },
                {
                    "sent": "What is the SAT solver that we're using for this?",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem with this though.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "On most problems of nontrivial size, if you try to run a SAT solver on the problem, or indeed anything else, the first thing that happens is that nothing happens because it blows out of memory.",
                    "label": 0
                },
                {
                    "sent": "We just have too many groundings, right?",
                    "label": 0
                },
                {
                    "sent": "There's an exponential number of ground exponential in the number of arguments of the predicates, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have a billion people on Earth, just a friends XY predicate has a billion billion possible groundings.",
                    "label": 0
                },
                {
                    "sent": "And that's bad news, right?",
                    "label": 0
                },
                {
                    "sent": "So things, don't even you know.",
                    "label": 0
                },
                {
                    "sent": "We can't even get off the ground because things don't fit in memory.",
                    "label": 0
                },
                {
                    "sent": "And even if they fit in memory, just creating the ground network will take probably too long.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, this is where the first sort of like new idea comes in and this is to exploit something that you know we all.",
                    "label": 0
                },
                {
                    "sent": "We are all busy exploiting all over computer science and machine learning in particular which is sparseness.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what kind of sparseness can we exploit here?",
                    "label": 0
                },
                {
                    "sent": "We can exploit the fact in most domains most logical atoms, IE most random variables are false.",
                    "label": 1
                },
                {
                    "sent": "So for example, take something like the predicate friends XY.",
                    "label": 0
                },
                {
                    "sent": "Most people are not friends with most people, right?",
                    "label": 0
                },
                {
                    "sent": "There's 6 billion people on Earth, but you're you know friends with, maybe, you know he doesn't.",
                    "label": 0
                },
                {
                    "sent": "A few hundred of them, maybe a few 1000 if you're on Facebook, but you know most of them you don't really interact with anyway, right?",
                    "label": 0
                },
                {
                    "sent": "So the vast vast majority of these groundings are false.",
                    "label": 1
                },
                {
                    "sent": "As a result of the vast majority of the items being false, the vast majority of the clauses are trivially true because their preconditions don't fire.",
                    "label": 0
                },
                {
                    "sent": "So for example, looking at the clothes that I had an example before, smoke, sex and friends XY employee implies smokes, Y if X&Y are not friends right then this clause is already satisfied irrespective of whether they smoke or not, so in that case there is no dependency between these people and I can actually just ignore this.",
                    "label": 0
                },
                {
                    "sent": "So the idea lazy inference is that instead of you know grounding out all the atoms on all the clauses to start with, which is going to be incredibly wasteful.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to say well by default an Atom is false, and by default clauses true, and then I only create the ones that you know the infant says are different from that, and I do this Leslie.",
                    "label": 0
                },
                {
                    "sent": "Basically, I start with the evidence I have said database of who's friends with whom and who smokes, and maybe I want to figure out you know what other people smoke right?",
                    "label": 0
                },
                {
                    "sent": "And I start out with only those and then you know, I propagate.",
                    "label": 0
                },
                {
                    "sent": "I ground out the clauses that I made false by those right and then I see what happens.",
                    "label": 0
                },
                {
                    "sent": "I need to flip to make those clauses too and I just keep doing this.",
                    "label": 0
                },
                {
                    "sent": "OK. And you know The upshot of this is that this vastly reduces the time and memory that we need.",
                    "label": 0
                },
                {
                    "sent": "To do this.",
                    "label": 0
                },
                {
                    "sent": "We've applied this on some problems where we got a reduction in memory and time of a factor of around 400,000.",
                    "label": 0
                },
                {
                    "sent": "So we were 400,000 times faster and use 400,000 times less memory.",
                    "label": 0
                },
                {
                    "sent": "There is the way we're able to do the experiences that we were not able to run the ungrounded version, but we you know, we took the curves and then we extrapolated them.",
                    "label": 0
                },
                {
                    "sent": "They're fairly clean polynomials, and you know if you once we extrapolate the non laser vision to the size of the whole database.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of speed up that we got.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is 1 idea.",
                    "label": 0
                },
                {
                    "sent": "More recently, Sebastian Riedel has has developed this cutting plane approach to Marco Logic inference, which I think is also very interesting.",
                    "label": 0
                },
                {
                    "sent": "In some ways.",
                    "label": 0
                },
                {
                    "sent": "It's an alternative to lazy inference that you have not yet been compared yet, but it would be interesting to do that.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is so we're going to have a very large number of constraints, potentially billions of clauses.",
                    "label": 0
                },
                {
                    "sent": "There is no way to try to solve the problem with all of them at once.",
                    "label": 0
                },
                {
                    "sent": "So the idea is as in general with cutting planes.",
                    "label": 0
                },
                {
                    "sent": "I some familiar with the police, the general idea seems like let us start by solving the problem with a small subset of constraints.",
                    "label": 1
                },
                {
                    "sent": "And then we have a solution and then we see if there are any any constraints that that solution violates.",
                    "label": 0
                },
                {
                    "sent": "If that solution doesn't violate any other constraints, we're in luck.",
                    "label": 0
                },
                {
                    "sent": "We we can stop, we're done.",
                    "label": 0
                },
                {
                    "sent": "If it does, then we add those violated constraints in and we repeat.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're just growing the set of clauses that we have to consider as little as possible.",
                    "label": 0
                },
                {
                    "sent": "The reason this is extremely powerful is that typically there is massive redundancy among the constraints.",
                    "label": 0
                },
                {
                    "sent": "Massive massive redundancy.",
                    "label": 0
                },
                {
                    "sent": "So typically what happens is that by adding in only a small fraction of the constraints, we actually solve the whole problem and the whole thing is much, much faster than before.",
                    "label": 1
                },
                {
                    "sent": "OK, so in Markov logic violates or not now.",
                    "label": 0
                },
                {
                    "sent": "Now the tricky problem here is how do I efficiently find the violated constraints right?",
                    "label": 0
                },
                {
                    "sent": "Because if I can do that efficiently, then I'm hosed anyway, right now in Markov logic, that violated constraints are false clauses, right?",
                    "label": 1
                },
                {
                    "sent": "I want all those clauses to be true so that I can add their weight right?",
                    "label": 0
                },
                {
                    "sent": "So the violated constraints are false clauses.",
                    "label": 0
                },
                {
                    "sent": "Well, the first clause, if you remember your logic is the same thing as a true conjunction, right?",
                    "label": 0
                },
                {
                    "sent": "So what we're looking for is true conjunctions in the database.",
                    "label": 0
                },
                {
                    "sent": "They represent the evidence in this state of the world at that point, right?",
                    "label": 0
                },
                {
                    "sent": "Well, finding that is actually what database people do for a living.",
                    "label": 0
                },
                {
                    "sent": "Right databases are all about issuing what are called conjunctive queries in the conjunctive queries.",
                    "label": 0
                },
                {
                    "sent": "Just taking your first order formula, it's a conjunction and finding all the true ground.",
                    "label": 0
                },
                {
                    "sent": "All the true groundings of that conjunction.",
                    "label": 0
                },
                {
                    "sent": "So what suggestion does is he uses an in memory database engine to efficiently find all the violated clauses.",
                    "label": 0
                },
                {
                    "sent": "Efficiently meaning linear in the size of the data, right?",
                    "label": 0
                },
                {
                    "sent": "So this is very good.",
                    "label": 0
                },
                {
                    "sent": "Another good thing about this is that notice that nothing that I said here says anything about what the base constraint solver is that I'm going to use.",
                    "label": 0
                },
                {
                    "sent": "In fact, the best constraint solver could be anything, and he actually did this with both LP and with Max Walk set and again tomorrow.",
                    "label": 0
                },
                {
                    "sent": "It could be done with something else.",
                    "label": 0
                },
                {
                    "sent": "That constraint solver here is treated as a black box.",
                    "label": 0
                },
                {
                    "sent": "OK, so notice notice that this is different from the type of cutting plane algorithm that you know people like David and Tommy.",
                    "label": 0
                },
                {
                    "sent": "And others have been working on.",
                    "label": 0
                },
                {
                    "sent": "And it's it's.",
                    "label": 0
                },
                {
                    "sent": "So it's like an orthogonal use of cutting points, right?",
                    "label": 0
                },
                {
                    "sent": "Because what we're using here is like we're looking at the original constraints in the problem, and there's too many and we're only adding some of them.",
                    "label": 0
                },
                {
                    "sent": "It's also much more scalable for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "This does not materialize all the constraints and is not materialized variables until it needs to.",
                    "label": 0
                },
                {
                    "sent": "Right in this case here we are working with all the variables from the one.",
                    "label": 0
                },
                {
                    "sent": "Here we only create the variables that we need.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that finding the violated constraints here is much more efficient, because it's done in this linear time using using a database query engine.",
                    "label": 0
                },
                {
                    "sent": "So you know, this way we can.",
                    "label": 1
                },
                {
                    "sent": "You know the problems with millions of variables and billions of constraints in time that actually does not depend in the best case on that.",
                    "label": 0
                },
                {
                    "sent": "And if the base solver is exact.",
                    "label": 0
                },
                {
                    "sent": "We can guarantee that by doing this you also get the exact optimum solution, and indeed he observed this in a couple of the problems that he applied this to OK, so let's talk about marginal inference for a second yeah.",
                    "label": 0
                },
                {
                    "sent": "It's leading the size of the database that represents the state of the inference.",
                    "label": 0
                },
                {
                    "sent": "Right, so remember we're in database land now, right?",
                    "label": 0
                },
                {
                    "sent": "So in database line what happens is that you're always linear in the size of the data, otherwise you know it's not, but you're exponentially in the length of the query.",
                    "label": 0
                },
                {
                    "sent": "Right, so if the clause is very long, you know you could have a problem because you know your exponential in the length of the clause.",
                    "label": 0
                },
                {
                    "sent": "So if you if you have a conjunction over three or four things, you're in good shape.",
                    "label": 0
                },
                {
                    "sent": "If you have a conjunction over 10 things, you might not be in good shape.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, in practice this is very fast.",
                    "label": 0
                },
                {
                    "sent": "I mean also the database is being used in memory so you know.",
                    "label": 0
                },
                {
                    "sent": "But another interesting thing is that this could potentially use to scale up even more.",
                    "label": 0
                },
                {
                    "sent": "By doing this, you know on disk and what not.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk about marginal inference for a second.",
                    "label": 0
                },
                {
                    "sent": "So we started out using you sampling and Gibbs sampling works.",
                    "label": 0
                },
                {
                    "sent": "If your model is is not too large and the dependencies are not too strong.",
                    "label": 0
                },
                {
                    "sent": "You know this, I think is basically the bottom line and Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "If you have strong dependencies, you're in trouble.",
                    "label": 0
                },
                {
                    "sent": "If you have large models meaning anything more than 10s or even maybe hundreds of variables, you're in trouble.",
                    "label": 0
                },
                {
                    "sent": "If you have both strong dependencies in a lot of variables, Gibbs sampling is completely hopeless.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can we do instead?",
                    "label": 0
                },
                {
                    "sent": "Well, we've developed this empty set algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which you know, really, without empty set we wouldn't have gotten very far.",
                    "label": 0
                },
                {
                    "sent": "So the basic observation that we started with is that we want to be able to handle deterministic dependencies, right?",
                    "label": 0
                },
                {
                    "sent": "Because you know, you could have deterministic dependencies stated by by formalizing Markov logic.",
                    "label": 0
                },
                {
                    "sent": "And the terministic dependencies.",
                    "label": 0
                },
                {
                    "sent": "They break MCMC right?",
                    "label": 0
                },
                {
                    "sent": "Because they make the Markov chain reducible.",
                    "label": 0
                },
                {
                    "sent": "And so not now your theorems about.",
                    "label": 0
                },
                {
                    "sent": "You know the detail.",
                    "label": 0
                },
                {
                    "sent": "Balancing will not don't apply.",
                    "label": 0
                },
                {
                    "sent": "You start off in this region.",
                    "label": 0
                },
                {
                    "sent": "It doesn't communicate with that region, so you never get there, OK?",
                    "label": 0
                },
                {
                    "sent": "In practice, right?",
                    "label": 0
                },
                {
                    "sent": "So even even on a fundamental level, you know MCMC can help the terministic dependencies, but in practice what happens is that if you have strong probability dependencies, that breaks things too because you just take far too long, meaning forever to actually move from one region to another.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what can we do about this year?",
                    "label": 0
                },
                {
                    "sent": "Oh, a subset, a subset.",
                    "label": 0
                },
                {
                    "sent": "All it takes.",
                    "label": 0
                },
                {
                    "sent": "All it takes is you know one or two, uh, deterministic formulas in the MLN to make stuff you know, go to hell.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So what can we use here?",
                    "label": 0
                },
                {
                    "sent": "Well, there's this great idea that people in statistical physics had in the 80s called the Swinson Rang algorithm.",
                    "label": 0
                },
                {
                    "sent": "Very famous algorithm there, because you know, it's bad things up in things like using models by you know, three or four orders of magnitude compared to Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Swendsen Wang is what's making life hard for us.",
                    "label": 0
                },
                {
                    "sent": "Is the strong dependencies.",
                    "label": 0
                },
                {
                    "sent": "So we're going to introduce auxiliary variables to capture those dependencies.",
                    "label": 0
                },
                {
                    "sent": "Let's call the auxiliary variables you and the original variables X, and then what we're going to do is, alternately sample you given X&X given you.",
                    "label": 0
                },
                {
                    "sent": "Right, and by this unusually sampling you given X is easy, and then sampling X given you is now now that I've captured the dependencies sampling the X is, you know I can move much more efficiently than I could before.",
                    "label": 0
                },
                {
                    "sent": "The problem with some mango is that he only works racing models pairwise connections and you know positive interactions.",
                    "label": 0
                },
                {
                    "sent": "So what we did with NC set was to generalize Swinson link to arbitrary closets.",
                    "label": 0
                },
                {
                    "sent": "What happens in Swenson Wing is that I randomly decide that these guys that are all you know pointing up will have to stay pointing up.",
                    "label": 0
                },
                {
                    "sent": "And then I flip them all at the same time.",
                    "label": 0
                },
                {
                    "sent": "Or I don't flip them.",
                    "label": 0
                },
                {
                    "sent": "OK, this is basically what happens, and so Swinson, when can go from an all white picture to an all black picture in one move, which you know forgive sampling, you wouldn't happen before the end of the universe.",
                    "label": 0
                },
                {
                    "sent": "So now the constraints that we have because we have arbitrary clauses are not so easy to satisfy.",
                    "label": 0
                },
                {
                    "sent": "So in order to sample X, given you what I need to do is find a satisfying assignment to a random subset of the clauses.",
                    "label": 0
                },
                {
                    "sent": "But for that we have a SAT solver.",
                    "label": 0
                },
                {
                    "sent": "So we call this senses set because it's using its MCMC with the SAT solver as a subroutine to actually quickly find new.",
                    "label": 0
                },
                {
                    "sent": "Places to go to and this is not an intelligent way.",
                    "label": 0
                },
                {
                    "sent": "Such detail balance is satisfied and what not.",
                    "label": 0
                },
                {
                    "sent": "The bottom line is that this is orders of magnitude faster than things like Gibbs sampling, and, you know, simulated tempering and various other techniques that people use.",
                    "label": 1
                },
                {
                    "sent": "How many orders of magnitude?",
                    "label": 0
                },
                {
                    "sent": "We don't really know because you know, keep sampling never converges, right?",
                    "label": 0
                },
                {
                    "sent": "We're looking at, you, know the whole log like holdout log, likelihood test data for example, and empty set is here and give sampling is over there.",
                    "label": 0
                },
                {
                    "sent": "And you know this just goes on like this.",
                    "label": 0
                },
                {
                    "sent": "And again, as I make as we Jack up the weights of the clauses Gibbs sampling, you know goes to hell very quickly.",
                    "label": 0
                },
                {
                    "sent": "And you know, me said basically.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mine.",
                    "label": 0
                },
                {
                    "sent": "So one more idea on the topic of insurance and this is lifted inference.",
                    "label": 0
                },
                {
                    "sent": "List the difference.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's the most exciting of all these ideas, for instance so far, and here's what it is.",
                    "label": 0
                },
                {
                    "sent": "Consider belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we're doing both propagation under very large network on a small network.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "None of this was really matter.",
                    "label": 0
                },
                {
                    "sent": "Many nodes are probably interchangeable in the sense that they send and receive the same messages throughout BP.",
                    "label": 1
                },
                {
                    "sent": "I have a million nodes that are all sending the same messages to their factors and getting the same messages back from their factors.",
                    "label": 0
                },
                {
                    "sent": "Right, So what I want to do is I want to group these nodes into super nodes.",
                    "label": 0
                },
                {
                    "sent": "And form what we call the lifted network, which is going to be much smaller than the original network.",
                    "label": 0
                },
                {
                    "sent": "And then I just do you know things back and forth on that lifted network.",
                    "label": 0
                },
                {
                    "sent": "On a good day, the size of the lifted network is going to be the size of the MLN.",
                    "label": 0
                },
                {
                    "sent": "So if I have if I have an MLN with 100 formulas, I'm going to have 100 features or 100 factors.",
                    "label": 0
                },
                {
                    "sent": "On a bad day, this is going to be as big as a full ground network, but on an average day this could give us, you know fantastically large speedups, in fact, potentially infinite speedups.",
                    "label": 0
                },
                {
                    "sent": "Doesn't potential in these methods to actually be able to do inference over infinite networks in finite time?",
                    "label": 0
                },
                {
                    "sent": "We haven't solved that yet, but I believe it's possible, so actually had a graphical illustration of the idea here, but I'm actually going to skip it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time is getting a little short, so let me now talk a little bit about learning.",
                    "label": 0
                },
                {
                    "sent": "So weak learning, we want to do it learning in Markov logic networks and you know, we started out using pseudo likelihood with LB.",
                    "label": 0
                },
                {
                    "sent": "FGS is the solver standard things.",
                    "label": 0
                },
                {
                    "sent": "This is fairly fast and robust, but it can get very bad results sometimes.",
                    "label": 1
                },
                {
                    "sent": "The next thing that we tried was to generalize the voted Perceptron algorithm to the case of melons.",
                    "label": 0
                },
                {
                    "sent": "So voted Perception is basically using is basically simple gradient descent within, you know in the original perceptron Viterbi to find the MAPY.",
                    "label": 0
                },
                {
                    "sent": "And then the gradient is the difference between the MAPY in the data.",
                    "label": 0
                },
                {
                    "sent": "Why and there isn't this works is that if the posterior distribution is peaked right, taking them AP State is actually good approximation and all we have to do to apply this to Markov logic is to replace Viterbi with something like Max Walk set.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's and this works well for certain things, but there's a couple of big problems with this.",
                    "label": 0
                },
                {
                    "sent": "One of the problems is that you know when you're doing part of speech tagging and things like that.",
                    "label": 0
                },
                {
                    "sent": "There's only one mode right then.",
                    "label": 0
                },
                {
                    "sent": "Life is good.",
                    "label": 0
                },
                {
                    "sent": "The problem in general M lens or general MRF is that we have multiple modes.",
                    "label": 0
                },
                {
                    "sent": "And not just learning based on find the single best pick.",
                    "label": 0
                },
                {
                    "sent": "Be a really bad idea, right?",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, how about instead of doing MLP inference we just use MCMC to sample OK, so the next thing the next thing that we tried was contrasted divergent but contrasted, divergent didn't help because all it does is it takes a few steps.",
                    "label": 0
                },
                {
                    "sent": "You know there typically will stay within the same mode, so I'm still having the problem that I'm not sampling other modes.",
                    "label": 0
                },
                {
                    "sent": "And if at inference time I fall into a different more than the one that I trained for, the results going to be garbage.",
                    "label": 0
                },
                {
                    "sent": "With empty set though, this is alleviated because MC side has the ability to jump promote to both in one step, it just runs the SAT solver to find another mode that could be completely different from the one that we're in OK.",
                    "label": 0
                },
                {
                    "sent": "So this is actually very.",
                    "label": 0
                },
                {
                    "sent": "This works pretty well.",
                    "label": 0
                },
                {
                    "sent": "You know this is far far far from a solved problem, but definitely using MC side is a good idea.",
                    "label": 0
                },
                {
                    "sent": "Another thing that's a very good idea is that each each step start MC set from the in state of the previous infants.",
                    "label": 1
                },
                {
                    "sent": "At each step of weight learning.",
                    "label": 0
                },
                {
                    "sent": "So there's been a couple of papers by Tillman and hidden about doing this.",
                    "label": 0
                },
                {
                    "sent": "We actually independently found the same thing.",
                    "label": 0
                },
                {
                    "sent": "This is definitely a very good idea.",
                    "label": 0
                },
                {
                    "sent": "This actually gives you a whole bunch of different benefits, right?",
                    "label": 0
                },
                {
                    "sent": "One is that it reduces burning, right?",
                    "label": 0
                },
                {
                    "sent": "Because I start from an initial state that's already.",
                    "label": 0
                },
                {
                    "sent": "Probably in the right ballpark and the other one is that if the wait start at zero, the learning itself does a kind of deterministic annealing right?",
                    "label": 0
                },
                {
                    "sent": "So this this actually gives you a lot of bank for very little bug.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there is another big problem.",
                    "label": 0
                },
                {
                    "sent": "Which is the problems that we're solving are extremely ill conditioned.",
                    "label": 0
                },
                {
                    "sent": "So you know in, for example, in training neural networks, so conditioning happens when you know this.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is my.",
                    "label": 0
                },
                {
                    "sent": "Likelihood landscape right as a function of the weights, and here's my optimum right when when my what I have is a Valley, right?",
                    "label": 0
                },
                {
                    "sent": "Then the gradient doesn't actually point towards the minimum, right?",
                    "label": 0
                },
                {
                    "sent": "And you know, in neural network land you know.",
                    "label": 0
                },
                {
                    "sent": "So in the conditioning number is the ratio of this dimension to this dimension, and you know conditioning number in the 10s or hundreds is considered bad, right?",
                    "label": 0
                },
                {
                    "sent": "In Markov logic we routinely run into conditioning numbers in the thousands.",
                    "label": 0
                },
                {
                    "sent": "Condition numbers in the thousands means that this ellipse actually just looks like a straight line going like this.",
                    "label": 0
                },
                {
                    "sent": "Another gradient descent, this China zigzag down that street and it's completely hopeless.",
                    "label": 0
                },
                {
                    "sent": "And the reason this happens is that I have clauses with millions more groundings than others, right?",
                    "label": 0
                },
                {
                    "sent": "So there is no learning rate that's actually going to be able to, you know, not oscillate in one dimension will actually making enough progress in another.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we can use 2nd order methods like was a Newton conjugate gradient and whatnot, you know, because they can do this problem right?",
                    "label": 0
                },
                {
                    "sent": "They can find a better you know, direction and what not.",
                    "label": 0
                },
                {
                    "sent": "The problem for us though, and you know for MRF's in general, is that the lines these methods, used line searches and the line searches require inference.",
                    "label": 0
                },
                {
                    "sent": "And the inference has to be approximating general right?",
                    "label": 0
                },
                {
                    "sent": "And you know, line searches with approximate functions you know could completely blow up because you need to bracket the minimum.",
                    "label": 0
                },
                {
                    "sent": "And if you actually think you're breaking the minimum, you're not.",
                    "label": 0
                },
                {
                    "sent": "You know your host.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, people often use the casted gradient methods, which are often competitive with these right?",
                    "label": 0
                },
                {
                    "sent": "But the problem that we have here is that in Markov logic the date is not IID.",
                    "label": 1
                },
                {
                    "sent": "The thing that makes sarcastic gradient comparatives, I do updates after I see each small batch of examples in here.",
                    "label": 0
                },
                {
                    "sent": "It's all one big connected graph, so that doesn't work either.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, the solution that we found is actually an idea, again from the neural network literature called scaled conjugate gradient.",
                    "label": 0
                },
                {
                    "sent": "Skill conjugate grains is a conjugate gradient algorithm that does not use line searches.",
                    "label": 1
                },
                {
                    "sent": "It actually uses, so it picks the search direction using the standard conjugate gradient idea, and then it uses the Hessian to pick the step size right.",
                    "label": 1
                },
                {
                    "sent": "It's just solving the quadratic problem as you would in the Newton step, and so we don't need the line search, and that's great.",
                    "label": 0
                },
                {
                    "sent": "And now there's still the problem that the Hessian for us is going to be very, very large, right?",
                    "label": 0
                },
                {
                    "sent": "If I have a million weights, the hashing is going to be a million by million, so can't do that.",
                    "label": 1
                },
                {
                    "sent": "So what we do is.",
                    "label": 0
                },
                {
                    "sent": "We can the Hessian Fortunately only appears inside the quadratic form.",
                    "label": 0
                },
                {
                    "sent": "The search direction transpose times the Hessian times the search direction, and So what we do is we compute the quadratic form directly inside MC set.",
                    "label": 0
                },
                {
                    "sent": "So we pushed the DOT product inside the sum and then for each sample we do that all product that we need to do and So what we get out of empty set is never quadratic in the number of weights, and without that this this wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "And then we also it's also very important to the preconditioning we use the universe dayag.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As usual.",
                    "label": 0
                },
                {
                    "sent": "OK, finally structure learning.",
                    "label": 0
                },
                {
                    "sent": "So to do structure learning in Markov logic networks, you know the first idea is to just use inductive logic programming, right?",
                    "label": 1
                },
                {
                    "sent": "That's what people in that field do for living is they discover clauses from data.",
                    "label": 0
                },
                {
                    "sent": "And that that indeed was the first thing that we tried.",
                    "label": 0
                },
                {
                    "sent": "That didn't work very well though, because in IRP people are typically optimizing things like the accuracy or you know some sort of like measure of coverage and whatnot.",
                    "label": 0
                },
                {
                    "sent": "And we're learning a probabilistic model here.",
                    "label": 0
                },
                {
                    "sent": "So this is a very different.",
                    "label": 0
                },
                {
                    "sent": "This is a very different problem.",
                    "label": 0
                },
                {
                    "sent": "So what we need to use is, you know a likelihood or pseudo likelihood as the objective function and some sort of structure prior to avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "And you know this is what we typically do.",
                    "label": 0
                },
                {
                    "sent": "More recently, actually Ray Mooney has found that you can actually use this to over generate clauses and then use L1 pruning.",
                    "label": 0
                },
                {
                    "sent": "This actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "Now there's a couple of big problems when you do this, which is that for each candidate structure change we need to compute the optimal weights, right?",
                    "label": 1
                },
                {
                    "sent": "And if we're going to try millions of structure changes, this isn't going to work.",
                    "label": 0
                },
                {
                    "sent": "However, what you can do that again works very well is start from the current weights and relax the convergence thresholds for sale.",
                    "label": 0
                },
                {
                    "sent": "BFG S We're just trying to find what the best candidate is once we pick the candidate, we can do things to convergence and this really really speeds things up.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that.",
                    "label": 0
                },
                {
                    "sent": "We need to compute the sufficient statistics by essentially showing queries against the database of evidence, and that itself can be an intractable problem, and then we learning you only have to do that once at the beginning and then you have the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "You know for the entire algorithm here.",
                    "label": 0
                },
                {
                    "sent": "Every time I create a new clause, I need to go back and gather some more sufficient attic.",
                    "label": 0
                },
                {
                    "sent": "So again, done naively, this will really kill you, but here's the thing that really helps us use subsampling.",
                    "label": 0
                },
                {
                    "sent": "Right, I may have a billion billion groundings of that clause that I showed with friends XY, but I don't need to look at all of them.",
                    "label": 0
                },
                {
                    "sent": "I can subsample 1000 and see which fraction is true and extrapolate there.",
                    "label": 0
                },
                {
                    "sent": "OK, so with these two things actually, you know you can actually do structure learning fairly fast, and then you can use various search methods that like.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is actually probably the area in which they have been the most papers with.",
                    "label": 0
                },
                {
                    "sent": "Markov logic is structured learning algorithms and you know people have tried many different type.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of search.",
                    "label": 0
                },
                {
                    "sent": "So finally the most recent type of structure, learning that we've done, uses a combination of two very useful ideas, clustering and pathfinding.",
                    "label": 0
                },
                {
                    "sent": "So the problem with the standard search methods that are so like, you know, think of the Della Pietra at all algorithm for linear arthritis, just creating features by conjoining variables and the problem with this is that it creates Brazilians and Brazilians of completely useless features, and then you know waste time testing them.",
                    "label": 1
                },
                {
                    "sent": "The idea of pathfinding is to do something a little bit smarter, which is to actually only consider features that you know a priority, have some support in the data.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Suppose I'm trying to learn a class to predict weather.",
                    "label": 0
                },
                {
                    "sent": "Anna and Bob are friends.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now what I'm going to try to do is I'm going to try to find other paths connecting and Bob like.",
                    "label": 0
                },
                {
                    "sent": "For example, if Anna Works review of A&B as a customer of IBM and Bob works for IBM, then they're more likely to be friends because they met while Bob was consulting for IBM.",
                    "label": 0
                },
                {
                    "sent": "So I can look for these paths and then what I'm going to do is in this path I'm going to replace the contents by variables and those are going to be my candidate clauses, so this can really really help.",
                    "label": 0
                },
                {
                    "sent": "The other thing that we do is clustering, right?",
                    "label": 0
                },
                {
                    "sent": "Why do this for N?",
                    "label": 0
                },
                {
                    "sent": "For these people one at a time, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe what we can do is we can you know, cluster together all the people who work in the IT industry?",
                    "label": 0
                },
                {
                    "sent": "Or you know various according to various properties in general cluster objects that have simulations, the similar objects, and then the graph that I'm doing this on?",
                    "label": 1
                },
                {
                    "sent": "Or to be precise, the hypergraph that I'm doing this one is going to be much smaller, so and that gives Me 2 advantages, it's going to make things much more efficient and it is also going to make me much more robust in terms of learning.",
                    "label": 0
                },
                {
                    "sent": "Because now I'm not creating and just based on single instances and I'm going to be creating canvas based on, you know whole clusters of instances and you know our most the best current approach to structural in Markov logic.",
                    "label": 1
                },
                {
                    "sent": "There's this so it causes objects, then it finds between paths between those clusters and extracts rules from from those.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those paths OK, and I had a bunch here of.",
                    "label": 0
                },
                {
                    "sent": "Interesting ideas and directions for future work, but I guess time is up so I'll just leave that up and take questions.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can.",
                    "label": 0
                },
                {
                    "sent": "Empty side framework.",
                    "label": 0
                },
                {
                    "sent": "We mentioned that it works much better than keep sampling for microfluidic networks, but the question is the models that we're working on.",
                    "label": 0
                },
                {
                    "sent": "Models is we have models that have very high connectivity.",
                    "label": 0
                },
                {
                    "sent": "A lot of.",
                    "label": 0
                },
                {
                    "sent": "Jeff Cable in touch.",
                    "label": 0
                },
                {
                    "sent": "You think that amsat style?",
                    "label": 0
                },
                {
                    "sent": "Innovation will help.",
                    "label": 0
                },
                {
                    "sent": "Or would it break down?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, very high connectivity is exactly the big problem that we have in LNS, right?",
                    "label": 0
                },
                {
                    "sent": "Is that you look at this clause in their groundings and they create networks with very very high connectivity.",
                    "label": 0
                },
                {
                    "sent": "So empty that is actually an algorithm that is good for dealing with problems with connectivity.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And you know you can take other problems in other MFS and just.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Basically compile them into the clausal form, and then run inside on them and see what happens, right?",
                    "label": 0
                },
                {
                    "sent": "MC Set doesn't do miracles, but.",
                    "label": 0
                },
                {
                    "sent": "MC Sat only cares about what the features are that are actually there, and the ones that are relevant.",
                    "label": 0
                },
                {
                    "sent": "So you could actually have very high connectivity and not blow up in the way that.",
                    "label": 0
                },
                {
                    "sent": "Sort of like more graph based inference methods blow up so.",
                    "label": 0
                },
                {
                    "sent": "Hey Cortana, call binary.",
                    "label": 0
                },
                {
                    "sent": "So the original empty set algorithm that I talked about here worked only for binary features, But then we extended it to multinomial features and then we extended the continuous features.",
                    "label": 0
                },
                {
                    "sent": "So we have something called hybrid empty set.",
                    "label": 0
                },
                {
                    "sent": "You know that I didn't talk about here that can actually handle any combination of continuous and discrete variables, and I'd be happy to, you know, tell you how the algorithm works, but yeah, so we can do that.",
                    "label": 0
                },
                {
                    "sent": "We have the question, what is learning tractable height?",
                    "label": 0
                },
                {
                    "sent": "And honestly, Oh yeah, so thanks for asking that question right.",
                    "label": 0
                },
                {
                    "sent": "Then give me an opportunity to talk about this.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following is that.",
                    "label": 0
                },
                {
                    "sent": "We must be crazy trying to learn MRSI.",
                    "label": 0
                },
                {
                    "sent": "Here's why.",
                    "label": 0
                },
                {
                    "sent": "Learning structure is an intractable problem, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, even for vision networks right where you know this already intractable problem, then inside that problem we have with learning right, which cannot be done in closed form, right?",
                    "label": 0
                },
                {
                    "sent": "And then inside that problem we have the problem with doing inference, which itself is intractable, right?",
                    "label": 0
                },
                {
                    "sent": "So no wonder that not too many people have tried to learn MRF structure, right?",
                    "label": 0
                },
                {
                    "sent": "So is there a way to bypass this problem somehow?",
                    "label": 0
                },
                {
                    "sent": "And I believe that there is any ideas the following and you know there's been some literature on this in the guise of learning low treewidth.",
                    "label": 0
                },
                {
                    "sent": "Markov random fields, right?",
                    "label": 0
                },
                {
                    "sent": "People here probably know about this right?",
                    "label": 0
                },
                {
                    "sent": "And the idea there is that I'm going to make sure that I only learn the model where inferences tractable.",
                    "label": 0
                },
                {
                    "sent": "Right now, something that makes a lot of these problems go away.",
                    "label": 0
                },
                {
                    "sent": "I don't have.",
                    "label": 0
                },
                {
                    "sent": "Effectively, I don't have to do this anymore.",
                    "label": 0
                },
                {
                    "sent": "I can learn the parameters in closed form and now it's just the problem of doing structure search, right?",
                    "label": 0
                },
                {
                    "sent": "This is the people like Northeast rebro and you know, Carlos Guestrin.",
                    "label": 0
                },
                {
                    "sent": "I'm going to check in Jeff Bilmes have various algorithms for this.",
                    "label": 0
                },
                {
                    "sent": "Very nice idea.",
                    "label": 0
                },
                {
                    "sent": "The problem with this though is that it's not remotely practical because the algorithms are typically exponential in the in the tree.",
                    "label": 0
                },
                {
                    "sent": "With that you impose it.",
                    "label": 0
                },
                {
                    "sent": "Typically the number of variables raised to the tree with the model that you learn.",
                    "label": 0
                },
                {
                    "sent": "So in practice they've never been used to do to learn models with more than a tree with three.",
                    "label": 0
                },
                {
                    "sent": "Right, and you know, there aren't too many real world problems.",
                    "label": 0
                },
                {
                    "sent": "Actually they have a truth of three were interesting problems with two is that could be in the thousands, right?",
                    "label": 0
                },
                {
                    "sent": "So what can we do right?",
                    "label": 0
                },
                {
                    "sent": "We want to learn, but we but the key point is that the tree with could be large and the inference might still be tractable.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is all we exploit, so we have the paper UI last year that was about learning tractable hightree with vision networks.",
                    "label": 0
                },
                {
                    "sent": "The way we learn, tractable, right with vision networks is that we compiled the vision network into an arithmetic circuit.",
                    "label": 0
                },
                {
                    "sent": "As we were learning it.",
                    "label": 0
                },
                {
                    "sent": "And then we penalized by the size of the arithmetic circuit, i.e.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the inference, not the complexity of the representation.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, penalising the complex of the representations that realistically idea, because two moves that look the same right now, suppose I have a chain right and I have one more node at the end of the chain, right?",
                    "label": 0
                },
                {
                    "sent": "Versus I added, I added a link that makes this into a big circle, right?",
                    "label": 0
                },
                {
                    "sent": "The representation costs.",
                    "label": 0
                },
                {
                    "sent": "This is the same, one of them is completely Pacific and the other just made your life impossible.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we penalize the size of the arithmetic circuit.",
                    "label": 0
                },
                {
                    "sent": "And this way we're very flexible.",
                    "label": 0
                },
                {
                    "sent": "What we can learn.",
                    "label": 0
                },
                {
                    "sent": "It's not a little with model anymore, right?",
                    "label": 0
                },
                {
                    "sent": "And we can learn a model, and we measure this.",
                    "label": 0
                },
                {
                    "sent": "We learn Bayesian networks that had very high tree with, and yet the inference was always trackable.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea here is to extend the same idea first to Markov networks and then to Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "Now you know in the worst case you know even the arithmetic circle will be too large, so there are no miracles.",
                    "label": 0
                },
                {
                    "sent": "But I think there's a sweet spot here for doing this type of stuff, yeah?",
                    "label": 0
                },
                {
                    "sent": "I just want it.",
                    "label": 0
                },
                {
                    "sent": "Well actually, great question.",
                    "label": 0
                },
                {
                    "sent": "When we when we decide to do this, we first wasted three months trying to learn arithmetic circuits from scratch.",
                    "label": 0
                },
                {
                    "sent": "You know, let's do operations of arithmetic circuits, right?",
                    "label": 0
                },
                {
                    "sent": "And do a search over this.",
                    "label": 0
                },
                {
                    "sent": "The structure that makes the problem with that is it's too low level.",
                    "label": 0
                },
                {
                    "sent": "Right most arithmetic circuits do not correspond to any remotely interesting probabilistic model, so I think we can completely throw away our understanding of what the high level model is that we're building.",
                    "label": 0
                },
                {
                    "sent": "We just have to make sure that you know it correspond the arithmetic circuit materialization of it is compact.",
                    "label": 0
                },
                {
                    "sent": "The important lesson here, I think is not there is a problem with representation, But the problem is the following is that people in graphical models have been overly focused on the graph structure.",
                    "label": 0
                },
                {
                    "sent": "If you only look at the graph structure, you miss all these opportunities.",
                    "label": 0
                },
                {
                    "sent": "That's the problem.",
                    "label": 0
                },
                {
                    "sent": "We need to start thinking of, you know the model in terms of the features.",
                    "label": 0
                },
                {
                    "sent": "The actual features the graph structure could be completely intractable, meaning the two with his large but the feature based model could still be good, and this is what I think we need to do.",
                    "label": 0
                },
                {
                    "sent": "Keyboard.",
                    "label": 0
                },
                {
                    "sent": "You don't know, yeah.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah, so the you know if I had to picture what this is going to look like a few years from now, is that under the hood we're doing everything with arithmetic circuits or that type of representation and the learning in the inference all operate on this at the level of the user, we don't talk arithmetic circuits, we talk, you know Markov logic.",
                    "label": 0
                },
                {
                    "sent": "But what really happens is always on arithmetic circuit.",
                    "label": 0
                },
                {
                    "sent": "Readdress the lipid.",
                    "label": 0
                },
                {
                    "sent": "The difference, I think, in order to correctly treat two collections of nodes as same with class, need a symmetry in your home network that that's one of the other.",
                    "label": 0
                },
                {
                    "sent": "It seems like a very rare condition will application.",
                    "label": 0
                },
                {
                    "sent": "Great questions, so the work that we did here was based on an earlier paper by near freedom and a couple of his students where they what they actually pointed out, right?",
                    "label": 0
                },
                {
                    "sent": "It was like a really simple paper at some level was that if you have no evidence right, let's think first of the case where you have no evidence and you have an MLN.",
                    "label": 0
                },
                {
                    "sent": "Then you have all these glorious symmetries, right?",
                    "label": 0
                },
                {
                    "sent": "You know all the groundings we have exactly the same, so you can do inference on the right.",
                    "label": 0
                },
                {
                    "sent": "Another problem is what happens when you have inference.",
                    "label": 0
                },
                {
                    "sent": "So when you have evidence right?",
                    "label": 0
                },
                {
                    "sent": "How do you influence when you have evidence?",
                    "label": 0
                },
                {
                    "sent": "This is where things could get very complicated, but now the way I kind of glossed over the details here, But what happens is that we just break each predicate into three cases that true groundings, the false groundings and the unknown ones.",
                    "label": 0
                },
                {
                    "sent": "And now, whether or not this pays off depends on what happens when you start joining them.",
                    "label": 0
                },
                {
                    "sent": "When you start joining one predicate with another and you know these groundings with these running things could blow up, but the way we do the lifting is that we keep track of which distinctions have to be made.",
                    "label": 0
                },
                {
                    "sent": "Right in the beginning, before we do any propagation, there's only three of these supernodes per predicate that, through the unknown and the false, and then when I start bringing the neighboring once, things start to split more.",
                    "label": 0
                },
                {
                    "sent": "Now in the world, so we have a proof that argument finds the minimal lifted network, right?",
                    "label": 0
                },
                {
                    "sent": "The question is, how large is that going to be in practice?",
                    "label": 0
                },
                {
                    "sent": "And that's going to depend entirely on the domain that we lifted.",
                    "label": 0
                },
                {
                    "sent": "Network is never larger than the ground network, right?",
                    "label": 0
                },
                {
                    "sent": "So there's never a loss, and on a good day it's orders of magnitude smaller.",
                    "label": 0
                },
                {
                    "sent": "Now the most powerful thing that you want to do, and I think with a lot of action is right now is in doing approximate lifting.",
                    "label": 0
                },
                {
                    "sent": "Approximate lifting is I lumped together.",
                    "label": 0
                },
                {
                    "sent": "The nodes that's in approximately the same messages and then you can actually workout some bounds on this in half and some guarantees on the result.",
                    "label": 0
                },
                {
                    "sent": "And with approximate lifting on a large network, you almost always get very large gains.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can construct pathological because you know as influence propagates, right?",
                    "label": 0
                },
                {
                    "sent": "You know the changes tried to make less and less difference.",
                    "label": 0
                },
                {
                    "sent": "Right, you know.",
                    "label": 0
                },
                {
                    "sent": "Here's a simple case to think of.",
                    "label": 0
                },
                {
                    "sent": "Think of a Markov chain, right?",
                    "label": 0
                },
                {
                    "sent": "And I put in, you know, evidence at the beginning of the Markov chain, right?",
                    "label": 0
                },
                {
                    "sent": "Without lifted inference, every node is going without approximate lifting.",
                    "label": 0
                },
                {
                    "sent": "Every node is going to have a different probability at the end.",
                    "label": 0
                },
                {
                    "sent": "With approximate lifting, you know as soon as you're close enough to the stationary distribution you're done.",
                    "label": 0
                },
                {
                    "sent": "You have an infinite reduction in the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "Time let's, let's think I'll speaker thank you.",
                    "label": 0
                }
            ]
        }
    }
}