{
    "id": "f47cbij6mrg6ymovkvx37wj63uconpy3",
    "title": "Identifiability of Priors from Bounded Sample Sizes with Applications to Transfer Learning",
    "info": {
        "author": [
            "Steve Hanneke, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Transfer Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_hanneke_transfer/",
    "segmentation": [
        [
            "I guess before getting into the details of this work, I just want to mention maybe some background on.",
            "Keep in mind as I'm going through it so Luis Young is a student at CMU Ann.",
            "She's well, one of the themes in her thesis is developing a beige and theory of active learning and basically the work here should really be viewed in that context and in particular some of the results I'll mention are really most interesting when applied in conjunction with some other results we have on beige and active learning.",
            "And I'll mention some of those.",
            "As we go along.",
            "Unfortunately, you couldn't be here today due to some visa issues, but you want to mention she's looking for a job right now."
        ],
        [
            "OK.",
            "So.",
            "I'm going to do things a little bit backward from usual will do the motivation after I mention some technical results just because the results themselves I think are kind of cool and simple in themselves.",
            "And then we'll see.",
            "It's just kind of a neat observation.",
            "And then we'll see how it applies and gives us some interesting applications to transfer learning.",
            "And in particular, in conjunction with active learning."
        ],
        [
            "OK, so the the formal model.",
            "Is kind of a statistical setup.",
            "As usual we would have some instance space will have data distribution on the instant space and some space classifiers and will just look at binary classification.",
            "Will have, as usual, this pseudometric between classifiers, which is just the probability they disagree on on a random example.",
            "And will in this talk will assume that the concept space has finite BCS.",
            "Mention will just denote that by VC.",
            "And just to keep things simple here, I'm going to assume that this pseudometric row is actually a metric.",
            "That's really just sort of a simplifying assumption.",
            "You can always just take the equivalence classes and then you have a metric on the set of equivalence classes as usual, so you can always interpret these results in some sense.",
            "But we'll just say it's a metric.",
            "Keep things simple.",
            "So if you just if you need a running example to keep in your mind, just suppose that X is, let's say, the N dimensional reels and we have some, let's say multivariate normal distribution and maybe linear separators as the concept space."
        ],
        [
            "The so far that was sort of like a pack like set up, but the twist here is we have some distribution on the set of classifiers.",
            "If you want to be Asian you can think of this as a prior.",
            "And so it's just if you want to think about the linear separators example, maybe our parameters, the weight vector and the bias would be say normal or something of that sort.",
            "And then we'll have our target function is actually sampled according to that prior.",
            "And then we have our things go as usual.",
            "We have sequence of unlabeled data points, sampled IID.",
            "Independent of the target.",
            "And then we'll just denote by this Z the set of labeled examples."
        ],
        [
            "OK, and just some notation will have this bracket.",
            "M is the first M natural numbers and will do this usual.",
            "We can have a subscript to index any subsequence.",
            "Of the data.",
            "OK, so this is the setup and this is the cool observation which is just that.",
            "The distribution on the 1st VC Dimension number of.",
            "Labeled examples.",
            "Uniquely identifies the prior distribution on the target function.",
            "So I think it's a.",
            "It's a very simple and nice observation, and we'll go through a proof of it.",
            "Maybe I just wanna make sure that every everyone clear on what's going on here.",
            "Feel free to stop me if you have verification.",
            "OK, the notation.",
            "I'm sorry.",
            "\u03a0 One Pi 2.",
            "So these are these are the prior distributions, right?",
            "So we have distributions on the concept space.",
            "And so, in our target function is sampled according to that distribution and we get labeled a sequence of labeled examples as a result label, including the target.",
            "So here where you sample the target according to \u03c0 one and we get VC dimension number of labeled examples.",
            "And here we sample.",
            "So like linear separators, is like N + 1 right?",
            "So so it's just.",
            "This is a sequence of labeled examples, right?",
            "And so we just take a number of them, just D. This season natural numbers.",
            "Right, so this is if M is a natural number bracket, M is the first M points of the season natural number and this is the first VC points.",
            "Is that clear?",
            "So these are Susie, received by one and CBC 5, two each.",
            "One of them is a random sequence.",
            "If the distribution if the distribution of these random sequences is the same.",
            "And so is the distribution of that part of the price.",
            "Citrate.",
            "Right, so it's it's at least us was not obvious right away that this would be true.",
            "So, so let's think what is this equivalent equal super smoothly?",
            "Same distribution.",
            "Is this not a number which is the central distribution?",
            "Yes, if these.",
            "If these have the same distribution, this is a joint distribution on VC dimension.",
            "Number of labeled examples.",
            "And so if these two.",
            "VC dimension number of labeled examples have the same distribution.",
            "Right, so the unlabeled parts that have the distribution there IID, but the labeled parts are all labeled according to the same target function, so that.",
            "Add some dependence between the label.",
            "And then so so you can have a distribution over these points and a distribution over these points.",
            "And if they're the same, then the priors are the same too.",
            "OK, is everyone clear on that?"
        ],
        [
            "OK, so so it's it's nice simple thing and so so there's actually a very simple proof of it.",
            "So there's basically three steps to the proof.",
            "The first step is will show that.",
            "If the full infinite sequence of labeled examples has the same distribution, then the priors have the same.",
            "The priors are the same.",
            "In the second step, is will show that.",
            "Well then show.",
            "Let's see the infinite dimensional one is related to the VC dimensional one.",
            "OK, so.",
            "The.",
            "So for the first step, this is the empirical distance between a pair of classifiers and so this is a VC class.",
            "So these indicators for pairs disagreeing forms of VC class.",
            "As well and so with probability one the empirical.",
            "Distance converges to the true distance for all pairs simultaneously.",
            "So what this says by our assumption RO is a metric.",
            "So that means that this is non 0 for all distinct pairs and therefore with probability one.",
            "The label the sequence of labels is going to be distinct for every distinct classifier in C. And that means that.",
            "With probability one given X, the distribution over the labels.",
            "Is going to identify the prior.",
            "There's a bijection between sequences of labels and classifiers, and so you can map the distributions to each other.",
            "So what this says then is the distribution.",
            "I guess this is just busy.",
            "The distribution of the full infinite sequence of labeled examples identifies the prior.",
            "Which is what this says here.",
            "If the infinite dimensional sequences have the same distribution, then the priors are the same."
        ],
        [
            "OK, now what we have to do is get from we know the things now about this infinite dimensional sequence.",
            "We want to talk about these finite just VC dimension number of points.",
            "So what we're going to do is relate any larger number of points.",
            "The distribution over any larger number of points to the distribution over VC dimension number of points.",
            "And so for that, just fix any endpoints and bigger than BC dimension and labels.",
            "And since the concept space can't shatter these endpoints, there's some sequence, let's call it Y~ labels that can't be realized by classifiers.",
            "In C. OK, so then clearly the probability we get that sequence of labels given whatever points that's going to be 0.",
            "OK, so this is kind of a base case for an inductive proof.",
            "And then here's the step.",
            "So if on the other hand, if you had these labels YC differ from this unrealized sequence by let's say at least in K. Then what we can do is say the probability we get that sequence of labels.",
            "Given that X is this sequence is you can write it just marginalized out that case.",
            "Label, and so this is now the probability we get the other labels as they are minus the probability we get the opposite label Y prime just flips the cake label.",
            "OK, so that's simple enough, and the interesting thing then is that this is a lower dimensional distribution.",
            "We're only dealing with M -- 1 labels here.",
            "And this we're dealing with a label sequence Y prime that's a little bit closer to Y~ in Hamming distance.",
            "So this is kind of a.",
            "We can use this then SS step in sort of the nested inductive proof.",
            "We have sort of 2 two types of induction going on here.",
            "We can either reduce M by one or we can reduce the distance to this unrealized sequence by one and we have base cases for both.",
            "And so then this is by induction.",
            "This conditional distribution is a function of the conditional distribution for VC dimension number of points."
        ],
        [
            "OK, So what we just showed is that if the distributions are the same for VC dimension number of points, then for all finite natural numbers the distributions of the labeled examples are the same.",
            "There's this classic result that says if all the finite dimensional distributions are the same, then the full infinite dimensional distribution has to be the same as well.",
            "If you want, this is the uniqueness half of the Kolmogorov extension theorem.",
            "And then in the first step we showed that if the full infinite dimensional distributions are the same, then the priors have to be the same.",
            "So that's a nice simple thing.",
            "The other direction is is clear, right?",
            "If the prices are the same then every every the marginals on X are the same and the conditional on the wise given access have to be the same.",
            "So this is stab Lish is the theorem.",
            "OK."
        ],
        [
            "So that's the observation there is.",
            "She mentioned there should also be.",
            "The opposite, you might.",
            "You might wonder, maybe just one point would be enough.",
            "Maybe we could identify the prior just using distribution at one point or something simple like that.",
            "Well, that's not the case.",
            "So actually for anything smaller than the VC dimension you can come up with distribution and a pair of priors such that priors are different, But the.",
            "M dimensional distributions are the same.",
            "I guess.",
            "Proof of this is fairly simple.",
            "Just take a shattered will set and I let D be uniform on that shatter will set.",
            "Will have this to the VC number of classifiers and take a uniform distribution on that.",
            "That's one of our priors and we can take half of those classifiers.",
            "Which shattered the first VC minus one points.",
            "And then we get to pick the label on the last point for each of these and we'll just define that to be the parity of the labels of the first points, and so you can see for any.",
            "And will let \u03c0 to be uniform there so so you can see them for any.",
            "A less than VC dimension number of distinct points.",
            "The conditional distribution on the labels is going to be uniform.",
            "So this this, although the priors are different the.",
            "The distribution on on endpoints will be the same for M less than VC dimension."
        ],
        [
            "OK.",
            "So that's the VC is the smallest.",
            "OK, so that's there.",
            "This kind of cute observation an now maybe what we can see is what's it good for?"
        ],
        [
            "And this is where we get into transfer learning.",
            "Now transfer learning is a classic topic.",
            "It's a lot of people have worked on.",
            "It's been around and the general principle is solving a new learning problem should be easier if we've already solved several previous problems.",
            "And there are many different approaches to transfer learning that have been explored.",
            "So for instance, maybe there's a notion that a new task is somehow related to some previous tasks, and then we can use this relatedness to help us.",
            "There's this idea that maybe there's some useful sub concepts that can be extracted from previous tasks that make learning easier in the future.",
            "The idea that will look at here is we can gather statistical information.",
            "About sort of a variety of concepts that will be asked to learn.",
            "And this is something that's actually been studied in some depth already.",
            "We're certainly not the first ones to define this framework."
        ],
        [
            "Anne.",
            "So just to give a motivating example for this sort of statistical information gathering idea, consider let's say the task of speech recognition.",
            "So here you have some speech recognition software and somebody comes and speaks into it and you train your speech recognition software for that person and then another person comes and speaks and you train the speech recognition system for."
        ],
        [
            "That person and this continues and after we do this a few times, eventually what we can figure out is maybe some kind of common speech patterns or dialects that these people have, and then the next person that comes to train the system.",
            "We don't have to start from scratch, we just have to identify the particular dialect of that person.",
            "So once we've sort of figured out the varieties of speech patterns, we just have to Group A person into which category they fit into.",
            "That should be much easier than training a recognizer from scratch."
        ],
        [
            "So formally, what will say is that there's some again some target distribution and will assume that's from some known family of distributions, although this can be fairly expressive, non parametric family if you want.",
            "And then we will have our these T independent target functions samples according to that distribution and for each task will have again IID unlabeled examples and will just denote by ZI the labeled examples, then labeled according to the target for Task I and using the unlabeled points from task I.",
            "So the protocol is the algorithm sort of gets this Z1 and then it produces some H at one classifier.",
            "Then it gets the second labeled data set and produces another classifier, etc.",
            "This happens sequentially in that in that manner we're interested in really two things here.",
            "One is the error rate for task T relative to the target function for task T and two.",
            "The number of labels that the algorithm is going to use.",
            "Remember, we're interested in applying this to active learning, so you're always counting the number of labels that you're going to use in active learning."
        ],
        [
            "And the general principle is geolearning would be a lot easier if we knew this distribution that the target function was sampled according to.",
            "And.",
            "So what we can do is maybe use this fact that we are fact was the distribution of VC dimension.",
            "Number of points identifies the target distribution.",
            "So the general strategy here is let's just take VC dimension number of points from each task that we've seen so far.",
            "Use them to estimate the distribution of the first VC dimension number of labeled examples.",
            "Convert that back into an estimate of the target distribution.",
            "And then use that estimate.",
            "Just plug that into some kind of a prior dependent learning algorithm to learn the target for the teeth task.",
            "OK, now it turns out what we actually proved is identify ability of the target distribution from VC dimension, number of points, distribution.",
            "So that's not quite the same as estimate ability.",
            "So we need a little bit of a stronger condition here on what we'll do here is, say, the set of possible distributions is totally bounded in the total variation distance.",
            "This is actually kind of a mild thing.",
            "So for instance all smooth densities or something of that sort would be fine.",
            "And given this condition, what you can do is estimate the target distribution at a bounded rate.",
            "So this actually just hold with high probability.",
            "But roughly speaking we have an estimator that's within some Theta independent distance of the target distribution, and that goes to 0."
        ],
        [
            "OK, so then the idea is we have some algorithm that depends on the prior and it has some expected number of labels that it uses.",
            "And let's say it produces the classifier with expected error rate less than epsilon.",
            "So then the idea is for each at each time.",
            "OK.",
            "So if we haven't seen enough tasks yet to get a reasonable estimate, will just do some kind of prior independent learning that doesn't really matter too much that only happens for small T anyway, but for reasonably large T will take our estimator Theta hat that I mentioned from the previous slide.",
            "And just for technical reasons will choose within a neighborhood of that distribution of that.",
            "\u03a0 Theta hat distribution will choose the one that the Pi Theta that has the smallest label complexity.",
            "And then run the algorithm using that prior and then that gives us an HT hat.",
            "And the guarantee is, it's sort of correct in the sense that for all times the expected error rate is indeed less than epsilon.",
            "And Furthermore, we have a guarantee on the number of labels that in the limit the average number of labels that we use is going to be bounded by the number that it would use given the true prior plus the DC dimension."
        ],
        [
            "Um?",
            "So just.",
            "This does give us some gains for passive learning.",
            "There are known results that give some constant factor improvements when you know the prior and.",
            "In active learning though, you really do get some benefits.",
            "It's known now that label complexity in active learning.",
            "If you know the prior of the target distribution from which the target sampled, you can always get little 01 over epsilon label complexity.",
            "Whereas there is actually no prior independent algorithm that has this guarantee for some concept spaces.",
            "So plug that into the transfer method that we had before.",
            "That gives us again the little one over Epsilon label."
        ],
        [
            "Complexity."
        ],
        [
            "OK, I think I'll just send there.",
            "Do you think that's one can also try to make similar things for priors on distribution target pairs?",
            "Yeah, it's a good question.",
            "Certainly you would need some restrictions on the types of distributions and you can't estimate an arbitrary distribution, but.",
            "I think something can be done, but it gets tricky.",
            "Do you expect something similar to hold for the noisy label setting?",
            "I.",
            "No, it in some cases yes.",
            "So you can do this for like uniform random classification noise.",
            "But when you get more general noise distributions, it doesn't.",
            "It doesn't workout as well.",
            "Essentially, it's hard to factor out the noise compared to the distribution over the target.",
            "You mentioned this condition on the set of priors being totally bounded, so that's pretty abstract to me, so I was wondering, surely it can be too big.",
            "I mean the theorem works for any totally bounded set I suppose, but could you give an example of what a reasonably sized totally bounded set of priors would be?",
            "Yeah, like like smooth density function if they have smooth density functions.",
            "That should be fine.",
            "OK, yeah, so the totally bounded condition is.",
            "It's fairly abstract.",
            "We're trying to get a general of results we can, but.",
            "You know, anytime like a maximum likelihood estimator would have anytime you have a bounded minimax rate.",
            "Basically it's it's going to be fine.",
            "So if your goal is only to recover their prior, your results indicate that it's enough to have a bunch of short sequences, bunch of sequences, just lengthy dimension.",
            "If I'm limiting you by the total number of examples, you know if the optimal thing is ineffective, a bunch of short sequences or less longer sequences.",
            "Yeah, so there's actually been a bunch of work on estimating the prior and things of that sort we were actually interested in short sequences because we're interested in active learning.",
            "Where you really want to have a small number of labels per task.",
            "So we wanted a bounded number of labels per task.",
            "If you if you allow the number of labels per task to also say go to Infinity at some rate, then certainly you can converge faster to the.",
            "With the same someone who's overall number of examples?",
            "One very long sequence is clearly bad, right?",
            "You need like to balance it somehow.",
            "So right, that's a good question.",
            "I actually so so we've looked a little bit at rates, but it's there's some subtleties there as well, so I guess the answer is it's not known.",
            "But I see what you're saying is that there could be some advantage to having a small number per task instead of a large number for task, but.",
            "I don't know exactly.",
            "Questions.",
            "OK, thank you for this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess before getting into the details of this work, I just want to mention maybe some background on.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind as I'm going through it so Luis Young is a student at CMU Ann.",
                    "label": 0
                },
                {
                    "sent": "She's well, one of the themes in her thesis is developing a beige and theory of active learning and basically the work here should really be viewed in that context and in particular some of the results I'll mention are really most interesting when applied in conjunction with some other results we have on beige and active learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll mention some of those.",
                    "label": 0
                },
                {
                    "sent": "As we go along.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, you couldn't be here today due to some visa issues, but you want to mention she's looking for a job right now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do things a little bit backward from usual will do the motivation after I mention some technical results just because the results themselves I think are kind of cool and simple in themselves.",
                    "label": 0
                },
                {
                    "sent": "And then we'll see.",
                    "label": 0
                },
                {
                    "sent": "It's just kind of a neat observation.",
                    "label": 0
                },
                {
                    "sent": "And then we'll see how it applies and gives us some interesting applications to transfer learning.",
                    "label": 1
                },
                {
                    "sent": "And in particular, in conjunction with active learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the the formal model.",
                    "label": 1
                },
                {
                    "sent": "Is kind of a statistical setup.",
                    "label": 0
                },
                {
                    "sent": "As usual we would have some instance space will have data distribution on the instant space and some space classifiers and will just look at binary classification.",
                    "label": 0
                },
                {
                    "sent": "Will have, as usual, this pseudometric between classifiers, which is just the probability they disagree on on a random example.",
                    "label": 0
                },
                {
                    "sent": "And will in this talk will assume that the concept space has finite BCS.",
                    "label": 0
                },
                {
                    "sent": "Mention will just denote that by VC.",
                    "label": 0
                },
                {
                    "sent": "And just to keep things simple here, I'm going to assume that this pseudometric row is actually a metric.",
                    "label": 0
                },
                {
                    "sent": "That's really just sort of a simplifying assumption.",
                    "label": 0
                },
                {
                    "sent": "You can always just take the equivalence classes and then you have a metric on the set of equivalence classes as usual, so you can always interpret these results in some sense.",
                    "label": 0
                },
                {
                    "sent": "But we'll just say it's a metric.",
                    "label": 0
                },
                {
                    "sent": "Keep things simple.",
                    "label": 0
                },
                {
                    "sent": "So if you just if you need a running example to keep in your mind, just suppose that X is, let's say, the N dimensional reels and we have some, let's say multivariate normal distribution and maybe linear separators as the concept space.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The so far that was sort of like a pack like set up, but the twist here is we have some distribution on the set of classifiers.",
                    "label": 0
                },
                {
                    "sent": "If you want to be Asian you can think of this as a prior.",
                    "label": 0
                },
                {
                    "sent": "And so it's just if you want to think about the linear separators example, maybe our parameters, the weight vector and the bias would be say normal or something of that sort.",
                    "label": 0
                },
                {
                    "sent": "And then we'll have our target function is actually sampled according to that prior.",
                    "label": 0
                },
                {
                    "sent": "And then we have our things go as usual.",
                    "label": 0
                },
                {
                    "sent": "We have sequence of unlabeled data points, sampled IID.",
                    "label": 0
                },
                {
                    "sent": "Independent of the target.",
                    "label": 0
                },
                {
                    "sent": "And then we'll just denote by this Z the set of labeled examples.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and just some notation will have this bracket.",
                    "label": 0
                },
                {
                    "sent": "M is the first M natural numbers and will do this usual.",
                    "label": 0
                },
                {
                    "sent": "We can have a subscript to index any subsequence.",
                    "label": 0
                },
                {
                    "sent": "Of the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the setup and this is the cool observation which is just that.",
                    "label": 0
                },
                {
                    "sent": "The distribution on the 1st VC Dimension number of.",
                    "label": 0
                },
                {
                    "sent": "Labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Uniquely identifies the prior distribution on the target function.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple and nice observation, and we'll go through a proof of it.",
                    "label": 0
                },
                {
                    "sent": "Maybe I just wanna make sure that every everyone clear on what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Feel free to stop me if you have verification.",
                    "label": 0
                },
                {
                    "sent": "OK, the notation.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 One Pi 2.",
                    "label": 0
                },
                {
                    "sent": "So these are these are the prior distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So we have distributions on the concept space.",
                    "label": 0
                },
                {
                    "sent": "And so, in our target function is sampled according to that distribution and we get labeled a sequence of labeled examples as a result label, including the target.",
                    "label": 0
                },
                {
                    "sent": "So here where you sample the target according to \u03c0 one and we get VC dimension number of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "And here we sample.",
                    "label": 0
                },
                {
                    "sent": "So like linear separators, is like N + 1 right?",
                    "label": 0
                },
                {
                    "sent": "So so it's just.",
                    "label": 0
                },
                {
                    "sent": "This is a sequence of labeled examples, right?",
                    "label": 0
                },
                {
                    "sent": "And so we just take a number of them, just D. This season natural numbers.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is if M is a natural number bracket, M is the first M points of the season natural number and this is the first VC points.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "So these are Susie, received by one and CBC 5, two each.",
                    "label": 0
                },
                {
                    "sent": "One of them is a random sequence.",
                    "label": 0
                },
                {
                    "sent": "If the distribution if the distribution of these random sequences is the same.",
                    "label": 0
                },
                {
                    "sent": "And so is the distribution of that part of the price.",
                    "label": 0
                },
                {
                    "sent": "Citrate.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's it's at least us was not obvious right away that this would be true.",
                    "label": 0
                },
                {
                    "sent": "So, so let's think what is this equivalent equal super smoothly?",
                    "label": 0
                },
                {
                    "sent": "Same distribution.",
                    "label": 0
                },
                {
                    "sent": "Is this not a number which is the central distribution?",
                    "label": 0
                },
                {
                    "sent": "Yes, if these.",
                    "label": 0
                },
                {
                    "sent": "If these have the same distribution, this is a joint distribution on VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Number of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "And so if these two.",
                    "label": 0
                },
                {
                    "sent": "VC dimension number of labeled examples have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so the unlabeled parts that have the distribution there IID, but the labeled parts are all labeled according to the same target function, so that.",
                    "label": 0
                },
                {
                    "sent": "Add some dependence between the label.",
                    "label": 0
                },
                {
                    "sent": "And then so so you can have a distribution over these points and a distribution over these points.",
                    "label": 0
                },
                {
                    "sent": "And if they're the same, then the priors are the same too.",
                    "label": 0
                },
                {
                    "sent": "OK, is everyone clear on that?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so it's it's nice simple thing and so so there's actually a very simple proof of it.",
                    "label": 0
                },
                {
                    "sent": "So there's basically three steps to the proof.",
                    "label": 0
                },
                {
                    "sent": "The first step is will show that.",
                    "label": 0
                },
                {
                    "sent": "If the full infinite sequence of labeled examples has the same distribution, then the priors have the same.",
                    "label": 0
                },
                {
                    "sent": "The priors are the same.",
                    "label": 0
                },
                {
                    "sent": "In the second step, is will show that.",
                    "label": 0
                },
                {
                    "sent": "Well then show.",
                    "label": 0
                },
                {
                    "sent": "Let's see the infinite dimensional one is related to the VC dimensional one.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "So for the first step, this is the empirical distance between a pair of classifiers and so this is a VC class.",
                    "label": 0
                },
                {
                    "sent": "So these indicators for pairs disagreeing forms of VC class.",
                    "label": 0
                },
                {
                    "sent": "As well and so with probability one the empirical.",
                    "label": 0
                },
                {
                    "sent": "Distance converges to the true distance for all pairs simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So what this says by our assumption RO is a metric.",
                    "label": 0
                },
                {
                    "sent": "So that means that this is non 0 for all distinct pairs and therefore with probability one.",
                    "label": 0
                },
                {
                    "sent": "The label the sequence of labels is going to be distinct for every distinct classifier in C. And that means that.",
                    "label": 0
                },
                {
                    "sent": "With probability one given X, the distribution over the labels.",
                    "label": 0
                },
                {
                    "sent": "Is going to identify the prior.",
                    "label": 0
                },
                {
                    "sent": "There's a bijection between sequences of labels and classifiers, and so you can map the distributions to each other.",
                    "label": 0
                },
                {
                    "sent": "So what this says then is the distribution.",
                    "label": 0
                },
                {
                    "sent": "I guess this is just busy.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the full infinite sequence of labeled examples identifies the prior.",
                    "label": 0
                },
                {
                    "sent": "Which is what this says here.",
                    "label": 0
                },
                {
                    "sent": "If the infinite dimensional sequences have the same distribution, then the priors are the same.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now what we have to do is get from we know the things now about this infinite dimensional sequence.",
                    "label": 0
                },
                {
                    "sent": "We want to talk about these finite just VC dimension number of points.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is relate any larger number of points.",
                    "label": 0
                },
                {
                    "sent": "The distribution over any larger number of points to the distribution over VC dimension number of points.",
                    "label": 0
                },
                {
                    "sent": "And so for that, just fix any endpoints and bigger than BC dimension and labels.",
                    "label": 0
                },
                {
                    "sent": "And since the concept space can't shatter these endpoints, there's some sequence, let's call it Y~ labels that can't be realized by classifiers.",
                    "label": 0
                },
                {
                    "sent": "In C. OK, so then clearly the probability we get that sequence of labels given whatever points that's going to be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of a base case for an inductive proof.",
                    "label": 0
                },
                {
                    "sent": "And then here's the step.",
                    "label": 0
                },
                {
                    "sent": "So if on the other hand, if you had these labels YC differ from this unrealized sequence by let's say at least in K. Then what we can do is say the probability we get that sequence of labels.",
                    "label": 0
                },
                {
                    "sent": "Given that X is this sequence is you can write it just marginalized out that case.",
                    "label": 0
                },
                {
                    "sent": "Label, and so this is now the probability we get the other labels as they are minus the probability we get the opposite label Y prime just flips the cake label.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's simple enough, and the interesting thing then is that this is a lower dimensional distribution.",
                    "label": 0
                },
                {
                    "sent": "We're only dealing with M -- 1 labels here.",
                    "label": 0
                },
                {
                    "sent": "And this we're dealing with a label sequence Y prime that's a little bit closer to Y~ in Hamming distance.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of a.",
                    "label": 0
                },
                {
                    "sent": "We can use this then SS step in sort of the nested inductive proof.",
                    "label": 0
                },
                {
                    "sent": "We have sort of 2 two types of induction going on here.",
                    "label": 0
                },
                {
                    "sent": "We can either reduce M by one or we can reduce the distance to this unrealized sequence by one and we have base cases for both.",
                    "label": 0
                },
                {
                    "sent": "And so then this is by induction.",
                    "label": 0
                },
                {
                    "sent": "This conditional distribution is a function of the conditional distribution for VC dimension number of points.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we just showed is that if the distributions are the same for VC dimension number of points, then for all finite natural numbers the distributions of the labeled examples are the same.",
                    "label": 0
                },
                {
                    "sent": "There's this classic result that says if all the finite dimensional distributions are the same, then the full infinite dimensional distribution has to be the same as well.",
                    "label": 0
                },
                {
                    "sent": "If you want, this is the uniqueness half of the Kolmogorov extension theorem.",
                    "label": 0
                },
                {
                    "sent": "And then in the first step we showed that if the full infinite dimensional distributions are the same, then the priors have to be the same.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice simple thing.",
                    "label": 0
                },
                {
                    "sent": "The other direction is is clear, right?",
                    "label": 0
                },
                {
                    "sent": "If the prices are the same then every every the marginals on X are the same and the conditional on the wise given access have to be the same.",
                    "label": 0
                },
                {
                    "sent": "So this is stab Lish is the theorem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the observation there is.",
                    "label": 0
                },
                {
                    "sent": "She mentioned there should also be.",
                    "label": 0
                },
                {
                    "sent": "The opposite, you might.",
                    "label": 0
                },
                {
                    "sent": "You might wonder, maybe just one point would be enough.",
                    "label": 0
                },
                {
                    "sent": "Maybe we could identify the prior just using distribution at one point or something simple like that.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not the case.",
                    "label": 0
                },
                {
                    "sent": "So actually for anything smaller than the VC dimension you can come up with distribution and a pair of priors such that priors are different, But the.",
                    "label": 1
                },
                {
                    "sent": "M dimensional distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Proof of this is fairly simple.",
                    "label": 0
                },
                {
                    "sent": "Just take a shattered will set and I let D be uniform on that shatter will set.",
                    "label": 0
                },
                {
                    "sent": "Will have this to the VC number of classifiers and take a uniform distribution on that.",
                    "label": 0
                },
                {
                    "sent": "That's one of our priors and we can take half of those classifiers.",
                    "label": 0
                },
                {
                    "sent": "Which shattered the first VC minus one points.",
                    "label": 0
                },
                {
                    "sent": "And then we get to pick the label on the last point for each of these and we'll just define that to be the parity of the labels of the first points, and so you can see for any.",
                    "label": 0
                },
                {
                    "sent": "And will let \u03c0 to be uniform there so so you can see them for any.",
                    "label": 0
                },
                {
                    "sent": "A less than VC dimension number of distinct points.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution on the labels is going to be uniform.",
                    "label": 0
                },
                {
                    "sent": "So this this, although the priors are different the.",
                    "label": 0
                },
                {
                    "sent": "The distribution on on endpoints will be the same for M less than VC dimension.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the VC is the smallest.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's there.",
                    "label": 0
                },
                {
                    "sent": "This kind of cute observation an now maybe what we can see is what's it good for?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is where we get into transfer learning.",
                    "label": 0
                },
                {
                    "sent": "Now transfer learning is a classic topic.",
                    "label": 1
                },
                {
                    "sent": "It's a lot of people have worked on.",
                    "label": 0
                },
                {
                    "sent": "It's been around and the general principle is solving a new learning problem should be easier if we've already solved several previous problems.",
                    "label": 0
                },
                {
                    "sent": "And there are many different approaches to transfer learning that have been explored.",
                    "label": 0
                },
                {
                    "sent": "So for instance, maybe there's a notion that a new task is somehow related to some previous tasks, and then we can use this relatedness to help us.",
                    "label": 0
                },
                {
                    "sent": "There's this idea that maybe there's some useful sub concepts that can be extracted from previous tasks that make learning easier in the future.",
                    "label": 0
                },
                {
                    "sent": "The idea that will look at here is we can gather statistical information.",
                    "label": 0
                },
                {
                    "sent": "About sort of a variety of concepts that will be asked to learn.",
                    "label": 0
                },
                {
                    "sent": "And this is something that's actually been studied in some depth already.",
                    "label": 0
                },
                {
                    "sent": "We're certainly not the first ones to define this framework.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So just to give a motivating example for this sort of statistical information gathering idea, consider let's say the task of speech recognition.",
                    "label": 1
                },
                {
                    "sent": "So here you have some speech recognition software and somebody comes and speaks into it and you train your speech recognition software for that person and then another person comes and speaks and you train the speech recognition system for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That person and this continues and after we do this a few times, eventually what we can figure out is maybe some kind of common speech patterns or dialects that these people have, and then the next person that comes to train the system.",
                    "label": 0
                },
                {
                    "sent": "We don't have to start from scratch, we just have to identify the particular dialect of that person.",
                    "label": 0
                },
                {
                    "sent": "So once we've sort of figured out the varieties of speech patterns, we just have to Group A person into which category they fit into.",
                    "label": 0
                },
                {
                    "sent": "That should be much easier than training a recognizer from scratch.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So formally, what will say is that there's some again some target distribution and will assume that's from some known family of distributions, although this can be fairly expressive, non parametric family if you want.",
                    "label": 0
                },
                {
                    "sent": "And then we will have our these T independent target functions samples according to that distribution and for each task will have again IID unlabeled examples and will just denote by ZI the labeled examples, then labeled according to the target for Task I and using the unlabeled points from task I.",
                    "label": 0
                },
                {
                    "sent": "So the protocol is the algorithm sort of gets this Z1 and then it produces some H at one classifier.",
                    "label": 0
                },
                {
                    "sent": "Then it gets the second labeled data set and produces another classifier, etc.",
                    "label": 0
                },
                {
                    "sent": "This happens sequentially in that in that manner we're interested in really two things here.",
                    "label": 0
                },
                {
                    "sent": "One is the error rate for task T relative to the target function for task T and two.",
                    "label": 0
                },
                {
                    "sent": "The number of labels that the algorithm is going to use.",
                    "label": 0
                },
                {
                    "sent": "Remember, we're interested in applying this to active learning, so you're always counting the number of labels that you're going to use in active learning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the general principle is geolearning would be a lot easier if we knew this distribution that the target function was sampled according to.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is maybe use this fact that we are fact was the distribution of VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Number of points identifies the target distribution.",
                    "label": 0
                },
                {
                    "sent": "So the general strategy here is let's just take VC dimension number of points from each task that we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "Use them to estimate the distribution of the first VC dimension number of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Convert that back into an estimate of the target distribution.",
                    "label": 0
                },
                {
                    "sent": "And then use that estimate.",
                    "label": 0
                },
                {
                    "sent": "Just plug that into some kind of a prior dependent learning algorithm to learn the target for the teeth task.",
                    "label": 0
                },
                {
                    "sent": "OK, now it turns out what we actually proved is identify ability of the target distribution from VC dimension, number of points, distribution.",
                    "label": 0
                },
                {
                    "sent": "So that's not quite the same as estimate ability.",
                    "label": 0
                },
                {
                    "sent": "So we need a little bit of a stronger condition here on what we'll do here is, say, the set of possible distributions is totally bounded in the total variation distance.",
                    "label": 0
                },
                {
                    "sent": "This is actually kind of a mild thing.",
                    "label": 0
                },
                {
                    "sent": "So for instance all smooth densities or something of that sort would be fine.",
                    "label": 0
                },
                {
                    "sent": "And given this condition, what you can do is estimate the target distribution at a bounded rate.",
                    "label": 0
                },
                {
                    "sent": "So this actually just hold with high probability.",
                    "label": 0
                },
                {
                    "sent": "But roughly speaking we have an estimator that's within some Theta independent distance of the target distribution, and that goes to 0.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then the idea is we have some algorithm that depends on the prior and it has some expected number of labels that it uses.",
                    "label": 0
                },
                {
                    "sent": "And let's say it produces the classifier with expected error rate less than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So then the idea is for each at each time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if we haven't seen enough tasks yet to get a reasonable estimate, will just do some kind of prior independent learning that doesn't really matter too much that only happens for small T anyway, but for reasonably large T will take our estimator Theta hat that I mentioned from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And just for technical reasons will choose within a neighborhood of that distribution of that.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 Theta hat distribution will choose the one that the Pi Theta that has the smallest label complexity.",
                    "label": 0
                },
                {
                    "sent": "And then run the algorithm using that prior and then that gives us an HT hat.",
                    "label": 0
                },
                {
                    "sent": "And the guarantee is, it's sort of correct in the sense that for all times the expected error rate is indeed less than epsilon.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, we have a guarantee on the number of labels that in the limit the average number of labels that we use is going to be bounded by the number that it would use given the true prior plus the DC dimension.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                },
                {
                    "sent": "This does give us some gains for passive learning.",
                    "label": 0
                },
                {
                    "sent": "There are known results that give some constant factor improvements when you know the prior and.",
                    "label": 0
                },
                {
                    "sent": "In active learning though, you really do get some benefits.",
                    "label": 0
                },
                {
                    "sent": "It's known now that label complexity in active learning.",
                    "label": 0
                },
                {
                    "sent": "If you know the prior of the target distribution from which the target sampled, you can always get little 01 over epsilon label complexity.",
                    "label": 0
                },
                {
                    "sent": "Whereas there is actually no prior independent algorithm that has this guarantee for some concept spaces.",
                    "label": 0
                },
                {
                    "sent": "So plug that into the transfer method that we had before.",
                    "label": 0
                },
                {
                    "sent": "That gives us again the little one over Epsilon label.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complexity.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think I'll just send there.",
                    "label": 0
                },
                {
                    "sent": "Do you think that's one can also try to make similar things for priors on distribution target pairs?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "Certainly you would need some restrictions on the types of distributions and you can't estimate an arbitrary distribution, but.",
                    "label": 0
                },
                {
                    "sent": "I think something can be done, but it gets tricky.",
                    "label": 0
                },
                {
                    "sent": "Do you expect something similar to hold for the noisy label setting?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "No, it in some cases yes.",
                    "label": 0
                },
                {
                    "sent": "So you can do this for like uniform random classification noise.",
                    "label": 0
                },
                {
                    "sent": "But when you get more general noise distributions, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't workout as well.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's hard to factor out the noise compared to the distribution over the target.",
                    "label": 0
                },
                {
                    "sent": "You mentioned this condition on the set of priors being totally bounded, so that's pretty abstract to me, so I was wondering, surely it can be too big.",
                    "label": 0
                },
                {
                    "sent": "I mean the theorem works for any totally bounded set I suppose, but could you give an example of what a reasonably sized totally bounded set of priors would be?",
                    "label": 0
                },
                {
                    "sent": "Yeah, like like smooth density function if they have smooth density functions.",
                    "label": 0
                },
                {
                    "sent": "That should be fine.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, so the totally bounded condition is.",
                    "label": 0
                },
                {
                    "sent": "It's fairly abstract.",
                    "label": 0
                },
                {
                    "sent": "We're trying to get a general of results we can, but.",
                    "label": 0
                },
                {
                    "sent": "You know, anytime like a maximum likelihood estimator would have anytime you have a bounded minimax rate.",
                    "label": 0
                },
                {
                    "sent": "Basically it's it's going to be fine.",
                    "label": 0
                },
                {
                    "sent": "So if your goal is only to recover their prior, your results indicate that it's enough to have a bunch of short sequences, bunch of sequences, just lengthy dimension.",
                    "label": 0
                },
                {
                    "sent": "If I'm limiting you by the total number of examples, you know if the optimal thing is ineffective, a bunch of short sequences or less longer sequences.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's actually been a bunch of work on estimating the prior and things of that sort we were actually interested in short sequences because we're interested in active learning.",
                    "label": 0
                },
                {
                    "sent": "Where you really want to have a small number of labels per task.",
                    "label": 0
                },
                {
                    "sent": "So we wanted a bounded number of labels per task.",
                    "label": 0
                },
                {
                    "sent": "If you if you allow the number of labels per task to also say go to Infinity at some rate, then certainly you can converge faster to the.",
                    "label": 0
                },
                {
                    "sent": "With the same someone who's overall number of examples?",
                    "label": 0
                },
                {
                    "sent": "One very long sequence is clearly bad, right?",
                    "label": 0
                },
                {
                    "sent": "You need like to balance it somehow.",
                    "label": 0
                },
                {
                    "sent": "So right, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I actually so so we've looked a little bit at rates, but it's there's some subtleties there as well, so I guess the answer is it's not known.",
                    "label": 0
                },
                {
                    "sent": "But I see what you're saying is that there could be some advantage to having a small number per task instead of a large number for task, but.",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for this.",
                    "label": 0
                }
            ]
        }
    }
}