{
    "id": "expxgzokkm6tglzqxzisqbqjzvydwhm2",
    "title": "Feature Set Embedding for Incomplete Data",
    "info": {
        "author": [
            "David Grangier, NEC Laboratories America, Inc."
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_grangier_fse/",
    "segmentation": [
        [
            "Hi everybody so this is Ron to work with you and Melvin from NEC.",
            "So what we are working here is with the data which have missing features.",
            "So basically we we have a set of whole set of features and when we get an example, some of the features are unavailable in most prior work people represent those examples with vectors.",
            "And divides the classifier of the whole vector space.",
            "So this means the missing value need to be imputed or integrated out.",
            "Another strategy is to consider an example specific subset and classify example in in this.",
            "In this space, however, it seems rather unnatural to use vectors when you basically have a mathematical object which has a vary in size.",
            "So what we do here is that we propose to use sets sets of pairs.",
            "Each pair is basically feature ID, which tells you switch feature has been measured and a future value which is like.",
            "To measure decensor as returned.",
            "So this mean the in this example.",
            "Here we basically get a set of three pairs indicating we only have 3 features available and their value.",
            "Then what we propose is classifier over search sets which rely on embedding strategy so."
        ],
        [
            "So here you basically see from left to right, all the model would work.",
            "So in the first stage this is the embedding stage.",
            "Each pair is mapped into latent space through a function P which basically work on those pair, feature ID, future value.",
            "This means an example is now represented as a cloud of point in the latent space.",
            "Then we apply a summarization operator Phi, which basically summarized this set with a single vector into space.",
            "This can be a linear operator such as an average or.",
            "This can be a non linear operator such as Max function for instance.",
            "Component wise Max and then there's a single vector in this latent space which represent the whole example, and it is then classified with a linear classifier.",
            "So this basically two places where we have parameters in this model as the final classifier and is the way we performed the projection from pairs to latent space.",
            "We propose to learn both those parameters jointly.",
            "By doing a gradient descent over the final objective, we ensure for that that the operator 5 is generalized differentiable, which means we can apply a subgradient techniques.",
            "It's good to notice that when the operator is linear, it's very similar to traditional linear classifier.",
            "So when we use, the average is actually very similar to linear classifier.",
            "When we use the Max it's it can perform much more complex reasoning because it can basically perform some kind of sub soft conjunction or sub junction in the latent space."
        ],
        [
            "So this framework offer great flexibility when we basically select the parameterization of the function, which projects pairs into the light and space.",
            "We can encode a lot of prior knowledge about the features.",
            "And we've compared this to several recent alternatives over various setups for different missing feature task and it will perform very well.",
            "We also would like to stress that this can be used in other context because.",
            "Basically we have a model which can predict from a subset of features.",
            "So this is useful.",
            "For example when you want to do prediction fast, and maybe you want to return the answer when not all features have been computed.",
            "Also, this is useful when you want to do active feature selection.",
            "That is to say, iteratively acquire features according to the confidence of your classifier.",
            "So we we can overview together all those things at poster number 13 I think for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everybody so this is Ron to work with you and Melvin from NEC.",
                    "label": 0
                },
                {
                    "sent": "So what we are working here is with the data which have missing features.",
                    "label": 0
                },
                {
                    "sent": "So basically we we have a set of whole set of features and when we get an example, some of the features are unavailable in most prior work people represent those examples with vectors.",
                    "label": 0
                },
                {
                    "sent": "And divides the classifier of the whole vector space.",
                    "label": 0
                },
                {
                    "sent": "So this means the missing value need to be imputed or integrated out.",
                    "label": 0
                },
                {
                    "sent": "Another strategy is to consider an example specific subset and classify example in in this.",
                    "label": 0
                },
                {
                    "sent": "In this space, however, it seems rather unnatural to use vectors when you basically have a mathematical object which has a vary in size.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is that we propose to use sets sets of pairs.",
                    "label": 0
                },
                {
                    "sent": "Each pair is basically feature ID, which tells you switch feature has been measured and a future value which is like.",
                    "label": 0
                },
                {
                    "sent": "To measure decensor as returned.",
                    "label": 0
                },
                {
                    "sent": "So this mean the in this example.",
                    "label": 0
                },
                {
                    "sent": "Here we basically get a set of three pairs indicating we only have 3 features available and their value.",
                    "label": 0
                },
                {
                    "sent": "Then what we propose is classifier over search sets which rely on embedding strategy so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you basically see from left to right, all the model would work.",
                    "label": 0
                },
                {
                    "sent": "So in the first stage this is the embedding stage.",
                    "label": 0
                },
                {
                    "sent": "Each pair is mapped into latent space through a function P which basically work on those pair, feature ID, future value.",
                    "label": 1
                },
                {
                    "sent": "This means an example is now represented as a cloud of point in the latent space.",
                    "label": 1
                },
                {
                    "sent": "Then we apply a summarization operator Phi, which basically summarized this set with a single vector into space.",
                    "label": 1
                },
                {
                    "sent": "This can be a linear operator such as an average or.",
                    "label": 0
                },
                {
                    "sent": "This can be a non linear operator such as Max function for instance.",
                    "label": 0
                },
                {
                    "sent": "Component wise Max and then there's a single vector in this latent space which represent the whole example, and it is then classified with a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So this basically two places where we have parameters in this model as the final classifier and is the way we performed the projection from pairs to latent space.",
                    "label": 0
                },
                {
                    "sent": "We propose to learn both those parameters jointly.",
                    "label": 0
                },
                {
                    "sent": "By doing a gradient descent over the final objective, we ensure for that that the operator 5 is generalized differentiable, which means we can apply a subgradient techniques.",
                    "label": 0
                },
                {
                    "sent": "It's good to notice that when the operator is linear, it's very similar to traditional linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So when we use, the average is actually very similar to linear classifier.",
                    "label": 0
                },
                {
                    "sent": "When we use the Max it's it can perform much more complex reasoning because it can basically perform some kind of sub soft conjunction or sub junction in the latent space.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this framework offer great flexibility when we basically select the parameterization of the function, which projects pairs into the light and space.",
                    "label": 0
                },
                {
                    "sent": "We can encode a lot of prior knowledge about the features.",
                    "label": 1
                },
                {
                    "sent": "And we've compared this to several recent alternatives over various setups for different missing feature task and it will perform very well.",
                    "label": 0
                },
                {
                    "sent": "We also would like to stress that this can be used in other context because.",
                    "label": 0
                },
                {
                    "sent": "Basically we have a model which can predict from a subset of features.",
                    "label": 0
                },
                {
                    "sent": "So this is useful.",
                    "label": 0
                },
                {
                    "sent": "For example when you want to do prediction fast, and maybe you want to return the answer when not all features have been computed.",
                    "label": 1
                },
                {
                    "sent": "Also, this is useful when you want to do active feature selection.",
                    "label": 0
                },
                {
                    "sent": "That is to say, iteratively acquire features according to the confidence of your classifier.",
                    "label": 0
                },
                {
                    "sent": "So we we can overview together all those things at poster number 13 I think for attention.",
                    "label": 0
                }
            ]
        }
    }
}