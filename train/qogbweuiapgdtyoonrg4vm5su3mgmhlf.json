{
    "id": "qogbweuiapgdtyoonrg4vm5su3mgmhlf",
    "title": "Semantic Scene Segmentation using Random Multinomial Logit",
    "info": {
        "author": [
            "Ananth Ranganathan, Honda Research Institute USA"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/bmvc09_ranganathan_sssr/",
    "segmentation": [
        [
            "The Honda Research Institute and the other Cambridge, the newer Cambridge.",
            "I'm going to talk about semantic scene segmentation.",
            "So let me start by giving some motivation for the problem."
        ],
        [
            "So the problem I'm interested in is analysis of St scenes, traffic scenes, mostly for intelligent transportation systems, so accident avoidance and in the long term for driverless vehicles and so on.",
            "The problems here are that we need to be able to recognize objects in different perspectives, so I have some images here of streets.",
            "For example, if we want to take bikes in these scenes they have to be all different perspectives.",
            "You have white intraclass variability, that's the other.",
            "Images on the side and also we need to able we need to be able to work with video, which means that it needs to be fast and if we do all these things then of course it also translates over to use in robotics an."
        ],
        [
            "It gives me an opportunity to put up a graduate's asthma picture.",
            "That's what Honda research is usually not known for.",
            "So what this talk is about is about two things.",
            "Mostly, I'll talk about an algorithm for classification, which I call random multinomial logistic regression or random multinomial logic.",
            "The good thing about it is that it satisfies most of those conditions that I put up before, so it's fast.",
            "It scales better with large intraclass variability and perspective.",
            "It scales well with the number of labels that we want to.",
            "Identify and most importantly it's very simple to implement an towards the end of my talk.",
            "I also talk about the system for scene analysis, segmentation and labeling, which is built on this random multinomial logic.",
            "So I'll start off with multinomial."
        ],
        [
            "Logistic regression with many of you are probably familiar with, But suppose we want to classify or segment this image over here.",
            "So what do we do?",
            "We detect some features, maybe we select some features from this feature selection can be anything you want to select features that are more discriminating.",
            "So we select some features an if we just combine them linearly and in the process we get the log probability of a particular class, then that is multinomial logistic regression.",
            "So it's a very simple linear model for log probability.",
            "And if we have one such equation for every class that we want to model, so in the end you get a matrix, you get a column vector of features and you have a matrix of parameters which give you a lock probability of the classification.",
            "So."
        ],
        [
            "The more common form in which this is written is if you take care of normalization and so on is in this exponential equation.",
            "That's the second equation on there, and this three well known softmax function that most people in machine learning are familiar with.",
            "And once we have this equation, it's easy to see how to learn the beta parameters.",
            "So if you're given training data, we learn it in supervised manner using nonlinear least squares.",
            "We just minimize the error between the prediction and the actual training data that we have.",
            "You can do it in many ways.",
            "In this work I use this LB FGS optimization algorithm.",
            "It's just nonlinear iterative least squares, and the good thing about it is that we also get the variances of the coefficient estimates, so we get the variances of the beta parameters.",
            "That will be useful.",
            "I'll come to it in a few minutes and actually what we do is or what I do in this work is do the learning with L2 regularization on the beta parameters, so this avoids large values for the parameters which causes overfitting and so on.",
            "It gives nicer properties.",
            "So."
        ],
        [
            "It's good about multinomial logistic regression.",
            "It's fast, it scales well with the number of classes.",
            "Model is stable with respect to slight changes in the training set, and it's used widely everywhere.",
            "It's got well known properties, that's all the good stuff.",
            "What's bad is that the variance of coefficients increases with the number of features.",
            "So if you have a large number of features and you retrain your model even on the same data set, you'll get different coefficient estimates.",
            "It's sensitive to noise in the training data, and if you train with large datasets because of this nonlinear least squares learning, it's a little bit slow.",
            "So that brings me to the beautiful part of the talk, which is random multinomial logic.",
            "So people who supposed to be a joke, by the way, so you know, not many.",
            "Eastwood fans, I guess."
        ],
        [
            "So random multinomial logic.",
            "So when I started when I came up with this thought of this algorithm last year sometime, I just put it in Google and search for a couple of minutes and within within the first one two searches, I found this Wikipedia page and I you know you could guess I was pretty disheartened.",
            "I thought it's already been done, but it turns out that there's only one paper that's done this and it wasn't published at that time.",
            "It came out last year and it's in some obscure corner of marketing, so I took the chance that no one in computer vision had heard of it.",
            "And I guess I'm right.",
            "So OK anyway."
        ],
        [
            "The basic idea is that is similar to random forests, which many of you might be familiar with.",
            "So if you have a single decision tree.",
            "It has high variance, it's prone to overfitting, sensitive to noise, unsuitable for large feature phase spaces.",
            "But if we put together a bunch of these decision trees and randomize the learning process, then you get these random forests, which have much better properties, so it's the same idea here."
        ],
        [
            "This image is again from that paper.",
            "So what we do is we have a bunch of training data and we sample the training data to get a large number of training sets and using each of these training sets we learn one multinomial logistic regression model each and the final result is just the average of all these.",
            "The classification result of all these multinomial regression models, so the way it works for in."
        ],
        [
            "My case for scene segmentation is I'm given a bunch of training data labeled seen segmented training data and you detect some features on all these images, do some sampling to get a bunch of different training sets, randomly select some features which will be used in each of the multinomial logit models.",
            "And then learn one model each for each of these training sets and the final random multinomial logistic model is just the combination of all these individual multinomial regression models.",
            "So this is training during testing."
        ],
        [
            "The way it works is pretty simple.",
            "All the all the features that we, the random features that we selected.",
            "Here we compute all those features on our training on our test images, compute the feature responses, pass them through the logistic regression models, average the result and get the final label.",
            "So it's really simple."
        ],
        [
            "So I'll I'll come to the scene segmentation part now.",
            "So the features that I use are first of all, the system is texture based.",
            "So what I use as input to the random multinomial logic is texture and the particular features that I use are first of all the texture space is discretized into text on, so it's the well known process where you convolve each of your images with the filter bank cluster the responses and assign the clusters each of the pixels.",
            "So you get.",
            "What's called a text on map.",
            "And finally I compute these features called Shape Text on features on text on Maps.",
            "So the shape text on features."
        ],
        [
            "Also, state of the art and they are quite popular nowadays, introduced by Shorten at all in EC 2006.",
            "So what each feature consists of is basically a rectangle and a particular text on and the feature response is basically the proportion of that particular text on inside the rectangle.",
            "So for example, if you have a blue text on then this feature is computing the proportion of blue stuff inside that rectangle and 1st you move the rectangle over and computed for each pixel.",
            "So each response is just one number and the good thing about this feature is that.",
            "It can compute it fast using integral images as was shown in that paper at ecv, and also this feature captures layout and context.",
            "If you have large enough size rectangles, so those are the good things.",
            "Of course, I just use these features mainly for comparison with existing work, but you can pick any other feature you want and use it in the same framework if you know if you have some other favorite feature.",
            "So."
        ],
        [
            "This is what the overall system just based on texture looks like, so there's an input image.",
            "You convolve it with a bunch of with the filterbank discretize it to get a text on map and randomly select a bunch of shape text on features and you have learned RML model in the training.",
            "During the training phase and so you pass these features through these multinomial logit models.",
            "Average the result and get the final label.",
            "So it seems that we're done.",
            "But there is a problem here and the problem is that this feature space is pretty huge.",
            "So usually the number of text dances on the order of thousands.",
            "And of course you can have as many rectangles as you want, so the feature space is huge.",
            "Most of these features are not discriminating enough.",
            "So what that means is that if you just do this random selection of features, you're going to need a large number of logistic regression models to get a reasonable output, and that basically slows down the algorithm, which is not something we want which gets."
        ],
        [
            "The next thing is the need for feature selection.",
            "So suppose here we have a model with four features and feature which has the biggest rectangle.",
            "So the green feature with the biggest rectangle which works on the green text on is probably not going to be discriminative enough because it's this huge rectangle.",
            "It covers most of the image, so it's highly likely that it doesn't have enough information to say you know, detect cows or graphs or things like that.",
            "So what we want to do is replace that particular feature with something else.",
            "So that's the first thing that's the first need for feature selection, and the second thing is this problem where we have correlated features.",
            "So for example, if you have features like this where there's one rectangle inside another, both these features will be computing the same thing.",
            "And if we have both these features in the model, then there's a problem called multi colinearity in logistic regression.",
            "First of all, it's very hard to detect, and Secondly it gives.",
            "It basically makes the model makes the coefficients not have static values, so they will not be stable.",
            "So that's not something we want, so we need feature selection to basically get rid of these two things."
        ],
        [
            "So the first thing is to replace insignificant features, so features that are not discriminating enough so we have this log log probability, which is just a linear function.",
            "So what it means for a feature to be insignificant is that it has small beta coefficients.",
            "Now to define small in a scale invariant manner, what I do is to just compare the coefficient with its standard deviation, so this is where the variance comes in, because the variance can be calculated in the least squares learning phase.",
            "Can use it here to.",
            "To compare it with the actual coefficient value, and if it's too small, then I discovered it and I just select another feature randomly and replace it instead of this Instagram feature and the other random feature might also be insignificant, but it's better than actually searching for good features, so it's just random replacement and then the multinomial logit model is relearned because there's only one feature change this re learning if it started from the initial values of the previous model is pretty fast.",
            "So the."
        ],
        [
            "And part of this is to avoid multi colinearity.",
            "It turns out that detecting Multicollinearity is very hard, so there are things like variance inflation factors.",
            "Statisticians compute to find to detect multi collinearity.",
            "But it's quite expensive.",
            "It's in fact easier in this case.",
            "What I did is to just randomly search in feature space.",
            "So once we all our features are significant, meaning that none of the beta coefficients are very small, I just do a random search.",
            "So pick a random feature, throw it out, put another feature and and if it has a higher log likelihood on the training set.",
            "Then it's highly likely that it's better, so just keep it.",
            "So those are the two pieces of the feature detection, so that's the algorithm."
        ],
        [
            "For each round, if there are any insignificant features, then we replace the feature and relearn the model.",
            "Otherwise, just do a random search in feature space."
        ],
        [
            "So what I talked about was just pixelwise classification of this texture based pixelwise classification.",
            "As you respect it gives noisy results, so these are a couple of results that you can see over here, so to.",
            "Get more smooth and better results.",
            "I integrate all this in a conditional random field framework along with color, location, and edge models, so I'm not going to go into the details here.",
            "The details are as in this paper by Shorten at all, which appeared in AJC this year.",
            "It's just that it puts color, location and edge everything into the CRF's potentials, and the inference is done using graph."
        ],
        [
            "So that's just what I said.",
            "I'll skip over and I'll come to the."
        ],
        [
            "Parents, so I compared the RML algorithm against two existing systems which are texton boost and just random forests or text on boost was both of which I implemented in Matlab and the text on Boost was implemented from this AJC paper and it basically just uses boosting to selection stumps based on shape text on features.",
            "I'm not going to.",
            "I'm not going to go into the details here, but it's state of the art in terms of.",
            "Labling performance.",
            "For the Rand."
        ],
        [
            "Forest implementation what I do is each each tree in the in the random forest has randomly selected shape text.",
            "On feature there's a shape text on feature at each node, and the addition function is just a threshold based on a random value again.",
            "So for example, the root there in the tree is computing.",
            "Whether the proportion of that particular, whether the feature output is greater than point to wait, so if it's greater than .28, it goes to the right.",
            "Otherwise it goes to the left, and so on.",
            "And finally, each leaf has a histogram of the labels, so that's the way the addition is done.",
            "So."
        ],
        [
            "So I have results on two datasets.",
            "The first data set.",
            "Is this what I call the motorbike video datasets?",
            "So these videos I collected.",
            "This is essentially views of what a driver would see in a rearview mirror, so there's a camera pointing backwards, and this is basically for motorbike detection.",
            "Here's but I detect four categories which are Bike, Rd, Sky and other.",
            "Just show.",
            "Sorry.",
            "So what these videos are showing are the bike is labeled as yellow.",
            "The road is labeled in Red, Sky is light blue and everything else is dark blue.",
            "Basically, and there are six 6 video sequences and four different types of bikes, and I did this in a leave one out testing scenario for the type of bike.",
            "Basically the top video is actually easier result as you can see the bike is silhouetted because of saturation in the background.",
            "The bottom video is much harder because the bike is moving through.",
            "Lighting changes go through shadows and brightly lighter regions and so on.",
            "So the center panel is showing just the pixelwise texture based labeling while the right panel shows the complete CRF system."
        ],
        [
            "So here's some quantitative results.",
            "I tested it against just VRML, which is pixelwise VRML with feature selection, the RML feature selection with the CRF in it.",
            "Random forests without any pruning and also depth limited random forest, meaning that I had a constraint on the depth of each tree in the forest and with the CRF in that case and also the text on boost and the text on boost plus CRF.",
            "So the bottom line is that the results are pretty similar to the text on boost with the CRF.",
            "Scenario.",
            "And this these percentages are pixelwise labeling performance.",
            "So the."
        ],
        [
            "Second, data set is the video.",
            "See the Pascal VLC data set, which has 20 classes.",
            "So this is basically to show that this algorithm scales up to a larger number of classes, so I compared it against certain Parolin who were the winners of the VC challenge last year, and.",
            "In this case, they were.",
            "I used the RML consists of 25 multinomial logic regresses each using 20 features."
        ],
        [
            "So these are the quantitative results again.",
            "The performance is better than the current permanent work, which is called XRC segmentation in more than 50% of the cases and the overall pixelwise class classification is about 30%, so the result images that you see are the best ones out there.",
            "So if you want to see the blue person meeting privately.",
            "So."
        ],
        [
            "Some more trends.",
            "So this is this is to show how the performance goes with the number of features included.",
            "So if we as you as you increase the number of features in each of the multinomial logit models.",
            "Performance increases up to a point and then it saturates, unless if you have regularization.",
            "So if you don't have regularization then it falls off because overfitting and so on, and also the number of models.",
            "So as you would expect, if you add number of models first the performance increases and then it saturates.",
            "So this is what you would expect through common sense.",
            "But it's good to see verified.",
            "Finally there's some runtime on the motorbike data set, so the labeling performance is pretty much the same as texton boost, but where?",
            "This random multinomial logit is better in runtime, so it only takes about 1 two seconds per frame as compared to random forests or text on boost.",
            "So the one other important thing is that the reason why I did this comparison is the depth limited forest.",
            "Is that because of large perspective variations aspects, especially in the motorbike data set, the depth depth limited forests are not able to capture those perspective changes.",
            "So a lot of the.",
            "Frames with the extreme perspective changes in the motorbikes are actually misclassified, so that's the reason for the comparison over there."
        ],
        [
            "So for future work we heard this keynote yesterday about L1 optimization, so you got to write the wave, but but.",
            "The good thing is that if we do sparse multinomial logic and this is actually being done in machine learning for text classification, there's no need for feature selection, but on the other hand would be really slow, especially during training, because we would need to evaluate all our thousands of features.",
            "So that's the RML part, which I need to look into in the future for the whole system itself.",
            "There's a number of things that can be done I don't have currently any, even though I'm using video, I don't have any motion in their motion cues, so that's one thing to do.",
            "And then also speed it up using super pixels, region, statistics etc and also shape models geometry.",
            "None of that stuff is in here, so that's all future work.",
            "And I'll just put up this."
        ],
        [
            "Videos again and take any questions.",
            "Thank you, thank you.",
            "So this is per pixel.",
            "You mean I compute histogram of probabilities for the same pixel in my training set and compare it with that, right?",
            "I haven't done it.",
            "Actually, I think that would be a good test.",
            "I haven't done it so far, So what?",
            "What the check I did is to see whether the coefficients that I learned stable.",
            "So it seems that in statistics that's a good enough test."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Honda Research Institute and the other Cambridge, the newer Cambridge.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about semantic scene segmentation.",
                    "label": 1
                },
                {
                    "sent": "So let me start by giving some motivation for the problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem I'm interested in is analysis of St scenes, traffic scenes, mostly for intelligent transportation systems, so accident avoidance and in the long term for driverless vehicles and so on.",
                    "label": 1
                },
                {
                    "sent": "The problems here are that we need to be able to recognize objects in different perspectives, so I have some images here of streets.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to take bikes in these scenes they have to be all different perspectives.",
                    "label": 1
                },
                {
                    "sent": "You have white intraclass variability, that's the other.",
                    "label": 0
                },
                {
                    "sent": "Images on the side and also we need to able we need to be able to work with video, which means that it needs to be fast and if we do all these things then of course it also translates over to use in robotics an.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It gives me an opportunity to put up a graduate's asthma picture.",
                    "label": 0
                },
                {
                    "sent": "That's what Honda research is usually not known for.",
                    "label": 0
                },
                {
                    "sent": "So what this talk is about is about two things.",
                    "label": 1
                },
                {
                    "sent": "Mostly, I'll talk about an algorithm for classification, which I call random multinomial logistic regression or random multinomial logic.",
                    "label": 1
                },
                {
                    "sent": "The good thing about it is that it satisfies most of those conditions that I put up before, so it's fast.",
                    "label": 1
                },
                {
                    "sent": "It scales better with large intraclass variability and perspective.",
                    "label": 0
                },
                {
                    "sent": "It scales well with the number of labels that we want to.",
                    "label": 0
                },
                {
                    "sent": "Identify and most importantly it's very simple to implement an towards the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "I also talk about the system for scene analysis, segmentation and labeling, which is built on this random multinomial logic.",
                    "label": 0
                },
                {
                    "sent": "So I'll start off with multinomial.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Logistic regression with many of you are probably familiar with, But suppose we want to classify or segment this image over here.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We detect some features, maybe we select some features from this feature selection can be anything you want to select features that are more discriminating.",
                    "label": 0
                },
                {
                    "sent": "So we select some features an if we just combine them linearly and in the process we get the log probability of a particular class, then that is multinomial logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple linear model for log probability.",
                    "label": 1
                },
                {
                    "sent": "And if we have one such equation for every class that we want to model, so in the end you get a matrix, you get a column vector of features and you have a matrix of parameters which give you a lock probability of the classification.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The more common form in which this is written is if you take care of normalization and so on is in this exponential equation.",
                    "label": 0
                },
                {
                    "sent": "That's the second equation on there, and this three well known softmax function that most people in machine learning are familiar with.",
                    "label": 0
                },
                {
                    "sent": "And once we have this equation, it's easy to see how to learn the beta parameters.",
                    "label": 0
                },
                {
                    "sent": "So if you're given training data, we learn it in supervised manner using nonlinear least squares.",
                    "label": 1
                },
                {
                    "sent": "We just minimize the error between the prediction and the actual training data that we have.",
                    "label": 0
                },
                {
                    "sent": "You can do it in many ways.",
                    "label": 1
                },
                {
                    "sent": "In this work I use this LB FGS optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's just nonlinear iterative least squares, and the good thing about it is that we also get the variances of the coefficient estimates, so we get the variances of the beta parameters.",
                    "label": 0
                },
                {
                    "sent": "That will be useful.",
                    "label": 0
                },
                {
                    "sent": "I'll come to it in a few minutes and actually what we do is or what I do in this work is do the learning with L2 regularization on the beta parameters, so this avoids large values for the parameters which causes overfitting and so on.",
                    "label": 0
                },
                {
                    "sent": "It gives nicer properties.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's good about multinomial logistic regression.",
                    "label": 1
                },
                {
                    "sent": "It's fast, it scales well with the number of classes.",
                    "label": 1
                },
                {
                    "sent": "Model is stable with respect to slight changes in the training set, and it's used widely everywhere.",
                    "label": 1
                },
                {
                    "sent": "It's got well known properties, that's all the good stuff.",
                    "label": 1
                },
                {
                    "sent": "What's bad is that the variance of coefficients increases with the number of features.",
                    "label": 1
                },
                {
                    "sent": "So if you have a large number of features and you retrain your model even on the same data set, you'll get different coefficient estimates.",
                    "label": 0
                },
                {
                    "sent": "It's sensitive to noise in the training data, and if you train with large datasets because of this nonlinear least squares learning, it's a little bit slow.",
                    "label": 0
                },
                {
                    "sent": "So that brings me to the beautiful part of the talk, which is random multinomial logic.",
                    "label": 0
                },
                {
                    "sent": "So people who supposed to be a joke, by the way, so you know, not many.",
                    "label": 0
                },
                {
                    "sent": "Eastwood fans, I guess.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So random multinomial logic.",
                    "label": 0
                },
                {
                    "sent": "So when I started when I came up with this thought of this algorithm last year sometime, I just put it in Google and search for a couple of minutes and within within the first one two searches, I found this Wikipedia page and I you know you could guess I was pretty disheartened.",
                    "label": 0
                },
                {
                    "sent": "I thought it's already been done, but it turns out that there's only one paper that's done this and it wasn't published at that time.",
                    "label": 0
                },
                {
                    "sent": "It came out last year and it's in some obscure corner of marketing, so I took the chance that no one in computer vision had heard of it.",
                    "label": 0
                },
                {
                    "sent": "And I guess I'm right.",
                    "label": 0
                },
                {
                    "sent": "So OK anyway.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea is that is similar to random forests, which many of you might be familiar with.",
                    "label": 1
                },
                {
                    "sent": "So if you have a single decision tree.",
                    "label": 0
                },
                {
                    "sent": "It has high variance, it's prone to overfitting, sensitive to noise, unsuitable for large feature phase spaces.",
                    "label": 1
                },
                {
                    "sent": "But if we put together a bunch of these decision trees and randomize the learning process, then you get these random forests, which have much better properties, so it's the same idea here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This image is again from that paper.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we have a bunch of training data and we sample the training data to get a large number of training sets and using each of these training sets we learn one multinomial logistic regression model each and the final result is just the average of all these.",
                    "label": 1
                },
                {
                    "sent": "The classification result of all these multinomial regression models, so the way it works for in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My case for scene segmentation is I'm given a bunch of training data labeled seen segmented training data and you detect some features on all these images, do some sampling to get a bunch of different training sets, randomly select some features which will be used in each of the multinomial logit models.",
                    "label": 0
                },
                {
                    "sent": "And then learn one model each for each of these training sets and the final random multinomial logistic model is just the combination of all these individual multinomial regression models.",
                    "label": 0
                },
                {
                    "sent": "So this is training during testing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way it works is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "All the all the features that we, the random features that we selected.",
                    "label": 0
                },
                {
                    "sent": "Here we compute all those features on our training on our test images, compute the feature responses, pass them through the logistic regression models, average the result and get the final label.",
                    "label": 0
                },
                {
                    "sent": "So it's really simple.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll I'll come to the scene segmentation part now.",
                    "label": 1
                },
                {
                    "sent": "So the features that I use are first of all, the system is texture based.",
                    "label": 0
                },
                {
                    "sent": "So what I use as input to the random multinomial logic is texture and the particular features that I use are first of all the texture space is discretized into text on, so it's the well known process where you convolve each of your images with the filter bank cluster the responses and assign the clusters each of the pixels.",
                    "label": 1
                },
                {
                    "sent": "So you get.",
                    "label": 0
                },
                {
                    "sent": "What's called a text on map.",
                    "label": 0
                },
                {
                    "sent": "And finally I compute these features called Shape Text on features on text on Maps.",
                    "label": 0
                },
                {
                    "sent": "So the shape text on features.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, state of the art and they are quite popular nowadays, introduced by Shorten at all in EC 2006.",
                    "label": 0
                },
                {
                    "sent": "So what each feature consists of is basically a rectangle and a particular text on and the feature response is basically the proportion of that particular text on inside the rectangle.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you have a blue text on then this feature is computing the proportion of blue stuff inside that rectangle and 1st you move the rectangle over and computed for each pixel.",
                    "label": 0
                },
                {
                    "sent": "So each response is just one number and the good thing about this feature is that.",
                    "label": 1
                },
                {
                    "sent": "It can compute it fast using integral images as was shown in that paper at ecv, and also this feature captures layout and context.",
                    "label": 0
                },
                {
                    "sent": "If you have large enough size rectangles, so those are the good things.",
                    "label": 0
                },
                {
                    "sent": "Of course, I just use these features mainly for comparison with existing work, but you can pick any other feature you want and use it in the same framework if you know if you have some other favorite feature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what the overall system just based on texture looks like, so there's an input image.",
                    "label": 0
                },
                {
                    "sent": "You convolve it with a bunch of with the filterbank discretize it to get a text on map and randomly select a bunch of shape text on features and you have learned RML model in the training.",
                    "label": 0
                },
                {
                    "sent": "During the training phase and so you pass these features through these multinomial logit models.",
                    "label": 0
                },
                {
                    "sent": "Average the result and get the final label.",
                    "label": 0
                },
                {
                    "sent": "So it seems that we're done.",
                    "label": 0
                },
                {
                    "sent": "But there is a problem here and the problem is that this feature space is pretty huge.",
                    "label": 0
                },
                {
                    "sent": "So usually the number of text dances on the order of thousands.",
                    "label": 1
                },
                {
                    "sent": "And of course you can have as many rectangles as you want, so the feature space is huge.",
                    "label": 1
                },
                {
                    "sent": "Most of these features are not discriminating enough.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that if you just do this random selection of features, you're going to need a large number of logistic regression models to get a reasonable output, and that basically slows down the algorithm, which is not something we want which gets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next thing is the need for feature selection.",
                    "label": 1
                },
                {
                    "sent": "So suppose here we have a model with four features and feature which has the biggest rectangle.",
                    "label": 0
                },
                {
                    "sent": "So the green feature with the biggest rectangle which works on the green text on is probably not going to be discriminative enough because it's this huge rectangle.",
                    "label": 0
                },
                {
                    "sent": "It covers most of the image, so it's highly likely that it doesn't have enough information to say you know, detect cows or graphs or things like that.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is replace that particular feature with something else.",
                    "label": 0
                },
                {
                    "sent": "So that's the first thing that's the first need for feature selection, and the second thing is this problem where we have correlated features.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have features like this where there's one rectangle inside another, both these features will be computing the same thing.",
                    "label": 0
                },
                {
                    "sent": "And if we have both these features in the model, then there's a problem called multi colinearity in logistic regression.",
                    "label": 1
                },
                {
                    "sent": "First of all, it's very hard to detect, and Secondly it gives.",
                    "label": 0
                },
                {
                    "sent": "It basically makes the model makes the coefficients not have static values, so they will not be stable.",
                    "label": 0
                },
                {
                    "sent": "So that's not something we want, so we need feature selection to basically get rid of these two things.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing is to replace insignificant features, so features that are not discriminating enough so we have this log log probability, which is just a linear function.",
                    "label": 1
                },
                {
                    "sent": "So what it means for a feature to be insignificant is that it has small beta coefficients.",
                    "label": 0
                },
                {
                    "sent": "Now to define small in a scale invariant manner, what I do is to just compare the coefficient with its standard deviation, so this is where the variance comes in, because the variance can be calculated in the least squares learning phase.",
                    "label": 0
                },
                {
                    "sent": "Can use it here to.",
                    "label": 1
                },
                {
                    "sent": "To compare it with the actual coefficient value, and if it's too small, then I discovered it and I just select another feature randomly and replace it instead of this Instagram feature and the other random feature might also be insignificant, but it's better than actually searching for good features, so it's just random replacement and then the multinomial logit model is relearned because there's only one feature change this re learning if it started from the initial values of the previous model is pretty fast.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And part of this is to avoid multi colinearity.",
                    "label": 0
                },
                {
                    "sent": "It turns out that detecting Multicollinearity is very hard, so there are things like variance inflation factors.",
                    "label": 0
                },
                {
                    "sent": "Statisticians compute to find to detect multi collinearity.",
                    "label": 1
                },
                {
                    "sent": "But it's quite expensive.",
                    "label": 0
                },
                {
                    "sent": "It's in fact easier in this case.",
                    "label": 0
                },
                {
                    "sent": "What I did is to just randomly search in feature space.",
                    "label": 1
                },
                {
                    "sent": "So once we all our features are significant, meaning that none of the beta coefficients are very small, I just do a random search.",
                    "label": 0
                },
                {
                    "sent": "So pick a random feature, throw it out, put another feature and and if it has a higher log likelihood on the training set.",
                    "label": 0
                },
                {
                    "sent": "Then it's highly likely that it's better, so just keep it.",
                    "label": 0
                },
                {
                    "sent": "So those are the two pieces of the feature detection, so that's the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each round, if there are any insignificant features, then we replace the feature and relearn the model.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, just do a random search in feature space.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I talked about was just pixelwise classification of this texture based pixelwise classification.",
                    "label": 0
                },
                {
                    "sent": "As you respect it gives noisy results, so these are a couple of results that you can see over here, so to.",
                    "label": 0
                },
                {
                    "sent": "Get more smooth and better results.",
                    "label": 0
                },
                {
                    "sent": "I integrate all this in a conditional random field framework along with color, location, and edge models, so I'm not going to go into the details here.",
                    "label": 1
                },
                {
                    "sent": "The details are as in this paper by Shorten at all, which appeared in AJC this year.",
                    "label": 0
                },
                {
                    "sent": "It's just that it puts color, location and edge everything into the CRF's potentials, and the inference is done using graph.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's just what I said.",
                    "label": 0
                },
                {
                    "sent": "I'll skip over and I'll come to the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parents, so I compared the RML algorithm against two existing systems which are texton boost and just random forests or text on boost was both of which I implemented in Matlab and the text on Boost was implemented from this AJC paper and it basically just uses boosting to selection stumps based on shape text on features.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details here, but it's state of the art in terms of.",
                    "label": 0
                },
                {
                    "sent": "Labling performance.",
                    "label": 0
                },
                {
                    "sent": "For the Rand.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forest implementation what I do is each each tree in the in the random forest has randomly selected shape text.",
                    "label": 1
                },
                {
                    "sent": "On feature there's a shape text on feature at each node, and the addition function is just a threshold based on a random value again.",
                    "label": 1
                },
                {
                    "sent": "So for example, the root there in the tree is computing.",
                    "label": 0
                },
                {
                    "sent": "Whether the proportion of that particular, whether the feature output is greater than point to wait, so if it's greater than .28, it goes to the right.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it goes to the left, and so on.",
                    "label": 1
                },
                {
                    "sent": "And finally, each leaf has a histogram of the labels, so that's the way the addition is done.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have results on two datasets.",
                    "label": 0
                },
                {
                    "sent": "The first data set.",
                    "label": 0
                },
                {
                    "sent": "Is this what I call the motorbike video datasets?",
                    "label": 1
                },
                {
                    "sent": "So these videos I collected.",
                    "label": 0
                },
                {
                    "sent": "This is essentially views of what a driver would see in a rearview mirror, so there's a camera pointing backwards, and this is basically for motorbike detection.",
                    "label": 0
                },
                {
                    "sent": "Here's but I detect four categories which are Bike, Rd, Sky and other.",
                    "label": 0
                },
                {
                    "sent": "Just show.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So what these videos are showing are the bike is labeled as yellow.",
                    "label": 0
                },
                {
                    "sent": "The road is labeled in Red, Sky is light blue and everything else is dark blue.",
                    "label": 0
                },
                {
                    "sent": "Basically, and there are six 6 video sequences and four different types of bikes, and I did this in a leave one out testing scenario for the type of bike.",
                    "label": 1
                },
                {
                    "sent": "Basically the top video is actually easier result as you can see the bike is silhouetted because of saturation in the background.",
                    "label": 0
                },
                {
                    "sent": "The bottom video is much harder because the bike is moving through.",
                    "label": 0
                },
                {
                    "sent": "Lighting changes go through shadows and brightly lighter regions and so on.",
                    "label": 0
                },
                {
                    "sent": "So the center panel is showing just the pixelwise texture based labeling while the right panel shows the complete CRF system.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some quantitative results.",
                    "label": 0
                },
                {
                    "sent": "I tested it against just VRML, which is pixelwise VRML with feature selection, the RML feature selection with the CRF in it.",
                    "label": 0
                },
                {
                    "sent": "Random forests without any pruning and also depth limited random forest, meaning that I had a constraint on the depth of each tree in the forest and with the CRF in that case and also the text on boost and the text on boost plus CRF.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line is that the results are pretty similar to the text on boost with the CRF.",
                    "label": 0
                },
                {
                    "sent": "Scenario.",
                    "label": 0
                },
                {
                    "sent": "And this these percentages are pixelwise labeling performance.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, data set is the video.",
                    "label": 0
                },
                {
                    "sent": "See the Pascal VLC data set, which has 20 classes.",
                    "label": 0
                },
                {
                    "sent": "So this is basically to show that this algorithm scales up to a larger number of classes, so I compared it against certain Parolin who were the winners of the VC challenge last year, and.",
                    "label": 0
                },
                {
                    "sent": "In this case, they were.",
                    "label": 0
                },
                {
                    "sent": "I used the RML consists of 25 multinomial logic regresses each using 20 features.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the quantitative results again.",
                    "label": 0
                },
                {
                    "sent": "The performance is better than the current permanent work, which is called XRC segmentation in more than 50% of the cases and the overall pixelwise class classification is about 30%, so the result images that you see are the best ones out there.",
                    "label": 0
                },
                {
                    "sent": "So if you want to see the blue person meeting privately.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more trends.",
                    "label": 0
                },
                {
                    "sent": "So this is this is to show how the performance goes with the number of features included.",
                    "label": 1
                },
                {
                    "sent": "So if we as you as you increase the number of features in each of the multinomial logit models.",
                    "label": 0
                },
                {
                    "sent": "Performance increases up to a point and then it saturates, unless if you have regularization.",
                    "label": 1
                },
                {
                    "sent": "So if you don't have regularization then it falls off because overfitting and so on, and also the number of models.",
                    "label": 0
                },
                {
                    "sent": "So as you would expect, if you add number of models first the performance increases and then it saturates.",
                    "label": 0
                },
                {
                    "sent": "So this is what you would expect through common sense.",
                    "label": 0
                },
                {
                    "sent": "But it's good to see verified.",
                    "label": 0
                },
                {
                    "sent": "Finally there's some runtime on the motorbike data set, so the labeling performance is pretty much the same as texton boost, but where?",
                    "label": 0
                },
                {
                    "sent": "This random multinomial logit is better in runtime, so it only takes about 1 two seconds per frame as compared to random forests or text on boost.",
                    "label": 0
                },
                {
                    "sent": "So the one other important thing is that the reason why I did this comparison is the depth limited forest.",
                    "label": 0
                },
                {
                    "sent": "Is that because of large perspective variations aspects, especially in the motorbike data set, the depth depth limited forests are not able to capture those perspective changes.",
                    "label": 0
                },
                {
                    "sent": "So a lot of the.",
                    "label": 0
                },
                {
                    "sent": "Frames with the extreme perspective changes in the motorbikes are actually misclassified, so that's the reason for the comparison over there.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for future work we heard this keynote yesterday about L1 optimization, so you got to write the wave, but but.",
                    "label": 0
                },
                {
                    "sent": "The good thing is that if we do sparse multinomial logic and this is actually being done in machine learning for text classification, there's no need for feature selection, but on the other hand would be really slow, especially during training, because we would need to evaluate all our thousands of features.",
                    "label": 1
                },
                {
                    "sent": "So that's the RML part, which I need to look into in the future for the whole system itself.",
                    "label": 0
                },
                {
                    "sent": "There's a number of things that can be done I don't have currently any, even though I'm using video, I don't have any motion in their motion cues, so that's one thing to do.",
                    "label": 1
                },
                {
                    "sent": "And then also speed it up using super pixels, region, statistics etc and also shape models geometry.",
                    "label": 0
                },
                {
                    "sent": "None of that stuff is in here, so that's all future work.",
                    "label": 0
                },
                {
                    "sent": "And I'll just put up this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Videos again and take any questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "So this is per pixel.",
                    "label": 0
                },
                {
                    "sent": "You mean I compute histogram of probabilities for the same pixel in my training set and compare it with that, right?",
                    "label": 0
                },
                {
                    "sent": "I haven't done it.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think that would be a good test.",
                    "label": 0
                },
                {
                    "sent": "I haven't done it so far, So what?",
                    "label": 0
                },
                {
                    "sent": "What the check I did is to see whether the coefficients that I learned stable.",
                    "label": 1
                },
                {
                    "sent": "So it seems that in statistics that's a good enough test.",
                    "label": 0
                }
            ]
        }
    }
}