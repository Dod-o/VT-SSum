{
    "id": "dpecpcsfcb62dduy2hxhj43s3zj2yy5o",
    "title": "Online Discovery and Maintenance of Time Series Motifs",
    "info": {
        "author": [
            "Abdullah Al Mueen, Department of Computer Science, University of New Mexico"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/kdd2010_mueen_odmt/",
    "segmentation": [
        [
            "Thanks very much.",
            "So hello everyone, my name is Abdallah.",
            "When I'm from UC Riverside advised by Doctor Amen, Kia.",
            "So the title of my talk is online discovery and maintenance of taxes motives."
        ],
        [
            "So first let's discuss the what is the time series motif dances.",
            "Motives are repeated patterns in a time series.",
            "So here the red one and a blue one are two repetitions of a pattern.",
            "So this is this has been a good primitive for data mining like activity or event discovery, summarization or classification.",
            "In the last years.",
            "There has been works on application, application of transfers, data motive discovery like data center chiller management which is the based application paper.",
            "Our winner last year Kennedy there have been worked on work on designing smart cane for elders and so on.",
            "So it has been very good to discover dances, motive for offline fashion in this paper or in this work.",
            "We have considered the streaming scene."
        ],
        [
            "We have a streaming time series which is generating numerical values continuously and we have a start sliding window of recent history.",
            "So we went.",
            "We want to answer the queries like what is the minute long pattern that repeated in the last hour.",
            "So we have visual illustration of what we.",
            "Tried to do which is a evolving time series which is an easy trace of a patient.",
            "The sliding window size is 1000 samples, so at every timestamp we want to see what are the two occurrences of subsequences that are most similar in the bottom right we show the most sequence most similar subsequences overlaid on one another, and the left on the left bottom.",
            "We show the distribution of motive distances for different sliding windows.",
            "So this is what we want to do.",
            "So now let's formulate the problem as a data mining problem."
        ],
        [
            "So we have a time series.",
            "The sliding window for the first time and we have taken a shorter window, which is probably a minute long or a seconds long and slide it across the sliding window to generate all the subsequences.",
            "They not ask for.",
            "The first window is to discover what is the most similar non overlapping subsequence.",
            "So after we have discovered the most similar pair, our task is to maintain that so at every timestamp we have added one point and delete at one point.",
            "So the sliding window slides across the streaming time series and we have to maintain or update the motive pair.",
            "If one of the pair is deleted or a new closest pair comes in."
        ],
        [
            "So to do that.",
            "What is the challenge in doing that?",
            "So the subsequences that we extracted can be thought of as high dimensional points and then the dances motive detection becomes the dynamic closest pair points pair of points problem in high dimensionality?",
            "So here is an example of two dimensional case like we have the time series subsequences which can be assumed to be lying in a 2 dimensional plane and assuming that the window size is 8.",
            "So this is a toy example actually.",
            "So we have the points.",
            "So if if there is no overlapping issues then one for the points which are closest in the leftmost pane.",
            "So upon upon the deletion of one point, say the first .1, the closest pair changes to 8 and two.",
            "And then again, when the insertion comes in, the 9.9 is inserted.",
            "The closest pair is changed again, so the bottom line is the closest pair may change upon every update.",
            "Both insertion and deletions.",
            "So the nice thing can be done, but that can be done is to use quality comparisons among all possible pairs up in every update, but this is definitely not the desired one.",
            "So our goal in this work is to find an algorithm.",
            "That will linear linear in linear time update.",
            "The that the closest pair.",
            "So when we say the linear update, we mean that we will do linear number of comparisons with the number of points in the sliding window.",
            "So because the comparison of two subsequences is costly because of the normalization and it is a vector operation.",
            "So there has been previous work on dynamic closest pair of points finding problem.",
            "Epstein did that in back back in 2000, So what he did is he maintained the matrix of all pairwise distances and use a quadtree to maintain update the distance matrix so it has the problem of quadratic space requirement."
        ],
        [
            "What we do is we maintain a set of neighbors and reverse nearest neighbor rivers, neighbors for all points and we do it in W * sqrt W space, where W is the size of sliding window."
        ],
        [
            "So here's the outline of the rest of the counts.",
            "So we have.",
            "We will discuss the methods and evaluations and then a couple of extensions to do more things with our algorithm.",
            "And some case studies in different domains, then I'll conclude."
        ],
        [
            "So the main approaches that we want to maintain for every point the nearest neighbor of that point.",
            "So if we can answer the smallest, find the smallest nearest neighbor, then that is the closest pair of points in the data set.",
            "So maintain the nearest neighbor of every point.",
            "Upon insertion we use up our all.",
            "Install W comparisons to find the nearest neighbour whenever a new point is updated, inserted so now we do not have any comparisons left to do upon delation.",
            "So whenever the next whenever a point is deleted, we have to do more task and in constant time without doing anymore comparisons.",
            "Say for example in this in this example we have inserted so in the left we show the nearest neighbor relationship through the arrows.",
            "So one arrow means the three.",
            "Say for example three and seven.",
            "This means that seven is nearest neighbor of three, so there is an arrow from 3 to 7.",
            "So when nine is inserted.",
            "Six is nearest neighbor is changed from 8 to 9 again when one is deleted.",
            "The pair seven and four.",
            "They had one as their nearest neighbor, which has to be reassigned with near nearest neighbors like 7 has new nearest neighbor as three and four has near nearest neighbor as seven.",
            "So it means that when the deletion is occurring, we have to find all the reverse nearest neighbors of the deleting point and update their neighbor nearest neighbor by finding the next one.",
            "So we need an ordering of the points.",
            "In order of their distances."
        ],
        [
            "So here is the data structure.",
            "For that we use to keep track of the neighbors.",
            "So we have 5 off Slide 5 for sliding window.",
            "For this case it is eight points, so the points coming in from the right and going out of the left and for each point we have a list of neighbors in order of the distances.",
            "So every point in this rectangle.",
            "List the rectangular points that couple stuff pointed to the point and the distance value for each point.",
            "We also keep the list of rivers.",
            "Nearest neighbors are the earliest we call it our list and the green ones are endless.",
            "But whenever a point is deleted, say for example, one is going to be deleted, we go to once first rivers nearest neighbor, find that point in the list, delete one from its head.",
            "And then take the next nearest neighbor which is 7, go to 7 and insert for there.",
            "That is the task of updating when deletion occurs and this is constant time.",
            "We do not do any kind of comparisons, any kind of subsequence comparisons.",
            "Similarly for seven we do the same thing.",
            "We go to the next reverse nearest neighbor of 1, which is 7.",
            "We go from 7 from.",
            "Follow that pointer, delete one from that nearest neighbor neighbor list.",
            "And follow through three and add 7 as the reverse nearest neighbor.",
            "Everything is fine up till now, but it does not solve the problem of quadratic space requirement by the Epstein.",
            "So what we do to reduce the size of the squared size of the data structure?",
            "So."
        ],
        [
            "We have two observations.",
            "First, while inserting we do not need to update old points nearest neighbor.",
            "So the main intuition here is this quadratic space structure has every pair two times.",
            "So say for example in list of one we have 1, four and again in list of four we have 4 one.",
            "So every pair is occurring twice here.",
            "So if we impose the temporal ordering explicitly like saying that every point will have the nearest neighbor.",
            "Only from the set of prior points, not from the later points or from the whole set of points.",
            "Then we can reduce it by half.",
            "But obviously now the neighbor lists are the endless than our lists do not have the neighbors for the whole snapshots.",
            "It is changed now.",
            "After that it is still quite ready.",
            "We then use our second observation that a point can be removed from the neighbor list if it violates the temporal order.",
            "So for example, from 48, the list of neighbors is still quadratic.",
            "I mean still W point long, so 265713 and four.",
            "So if we remove the the points that violate the temporal order, it becomes 2, six and seven.",
            "So which means that.",
            "Every list is a longest increasing subsequence of a random permutation of W points, so the expected value for that is square root of W. So which means that we reduced the size from W to square root of W of every enlist."
        ],
        [
            "So here is an example where we show everything in action.",
            "So this is our final data structure.",
            "We will update delete one and insert nine in the next step.",
            "So it changes like this.",
            "Then again D2 is related, 10 is inserted.",
            "It changes like this.",
            "So let me go back again and show it again.",
            "So the the main thing to observe here is that every node is inserted with a list of neighbors and as it passes through the FIFO, it loses neighbors and gains rivers nice neighbors.",
            "So the overall space requirement kind of stabilized by this approach of updating the data structure."
        ],
        [
            "So here is the evaluation of our method.",
            "So we have compared compared to our algorithm with the fast fair implementation which is developed in UC Irvine so fast there is a specific implementation for general dynamic closest pair problem where insertions and deletions can occur at any order.",
            "So say 5 insertions, then two deletions, then five insertions like this.",
            "So we have you so that is more generic problem.",
            "We have considered the more specific one.",
            "Wait, insertions and deletions occur in a specific order.",
            "So we have tried four datasets, insect, random work, EOG, an easy, only the random walk was synthetic so on for all four datasets.",
            "Fast peer perform almost in the same way, so the black line shows that fast bear and our algorithm performs a better way up to 8 times faster than fast here.",
            "So this is the X axis.",
            "We show the window size so as we increase the window size after 40,000 and the Y axis shows the average update time.",
            "So we have another parameter which is the modifying the smaller window lengths, so we vary that and we see the same performance gain for different values of motif length.",
            "What is in terms of space.",
            "So this table we have stables, purse cost per point with increasing window size.",
            "So if we increase the window size in the bottom figure then we see that the average length of that enlist it never reaches the whole W, so it kind of becomes stable.",
            "And this is also observed for different multiflex."
        ],
        [
            "So now I'll show the extensions that are kind of adding some values to our algorithm and the case studies next so."
        ],
        [
            "What if we have multidimensional time series where we have be synchronous time series and we want to find the most similar pair of subsequences which can be from any 2 dimensions?",
            "So there we obviously would need the number of typos and each pointer can point to points in other fibers like this, and this increases the space and time cost by factor of D, so it is linear in number of dimensions.",
            "There is no curse of dimensionality.",
            "The second thing is, what if the data comes faster than the rate we can process, so that in that case there is only one option of load shedding, because no buffer can solve the problem of arbitrary data it because it there is always a chance of over overflowing by arbitrary number of consecutive worst cases.",
            "So what we do is we skip points if we cannot handle it and we see that if we reduce the fraction of data used by more and more high data rate, we have discovered motive in greater factions, which means that even though we are losing data, we do not lose the motives.",
            "The reason is the worst cases do not contribute contribute to the modifier.",
            "That is the main reason why we have seen this observations."
        ],
        [
            "Now the first case study we have done on ECG data.",
            "Here we the settings is in online compression like we have a streaming data source.",
            "We're getting data from a streaming data source, compress it and sending it to some other places.",
            "So when we do complete compression, existing approach does this.",
            "So it takes say 4 five points, approximate it with some linear segment and send it to some other place.",
            "So for every five points it sends two coefficients for the line segment.",
            "So what we do is we run our algorithm on the streaming stream for a particular amount of time and then generate a Dictionary of patterns.",
            "Then we observe this stream.",
            "And see if the pattern occurs.",
            "Industry.",
            "If it occurs, then we replace the pattern by only a pointer to that pattern in the dictionary, so that way it compresses highly and decompresses smoothly.",
            "Like if you see the red one in the middle, it is kind of discontinuous.",
            "But if you see the bottom one, there it is more smooth.",
            "So for signals with her regular repetitions this.",
            "This approach has high compression rate with less error.",
            "Our next case."
        ],
        [
            "Daddy is in robotics, so in robotics are there is a problem called closing the loop problem where the robot is tasked to be the problem of finding whether it is in a position where it was in before.",
            "So for example, we have collected a data set which is called New England data set.",
            "I forgot to put the reference over, so here one robot starts in a given position.",
            "It completes a loop and then.",
            "Changes back so we have their boat has a camera, it snap.",
            "Takes snapshot for both from both sides of it and give streaming the data to the central processor so we have a video stream of photos taken by the robot.",
            "So we change that video stream to see the time series by changing each frame through color histograms and then run our algorithm.",
            "So what what our algorithm is doing here is.",
            "It.",
            "Without including introducing any latency, it determines what are the good candidates for closing the loop problem.",
            "So there's the right part shows the two image segments which found by our algorithm, which are very similar because their corresponding color histograms are very similar, which happens to be the closest.",
            "Closing the loop in the in this data set shown by the loop closure detected error.",
            "So."
        ],
        [
            "With this, I conclude my talk that this is the first at and to maintain cancer is motive online.",
            "So what we do is we maintain minutes long reputation in our long window.",
            "So we have tried up to hundreds of data rate which is sufficient for most natural data sources.",
            "And we do it in linear versus linear update time with less cost.",
            "That what is so we have W sqrt W instead of W squared.",
            "And even though it is worst case linear, we have a faster than faster algorithm than the general dynamic closes pair solution.",
            "Existing solution so."
        ],
        [
            "That's it, thank you.",
            "So we have time for questions.",
            "Any question?",
            "OK, yeah.",
            "Finding the nearest nearest neighbors.",
            "But actually this function is comparing it to you.",
            "Please in distance Euclidean distance.",
            "After normalization there is a part of normalizing, normalizing for the Bayes shifting and scaling variances, and actually this is the question I want to ask you.",
            "Just a follow up that so you just mentioned that this is a little bit high dimensional, so so do you have any idea how large the dimension?",
            "Yes, we have tried if we see the results.",
            "So the the motif length is the dimension, so we have right up 201 thousand.",
            "So the length of the pattern that we are finding can be up to 1000 S for our experiments.",
            "Yeah, because I just had to constantly use Euclidean distance for the high dimensional measuring similarity within the high dimension space.",
            "So usually the Ukraine.",
            "This distance is not that meaningful in high dimension space.",
            "It is so my comment here is it is it is not meaningful when the distance is high.",
            "So say for example if we had three points where each of them having distance of five or 10, then it is not meaningful.",
            "But if you have two things with the bleeding distance zero, then whatever the dimension is they are single.",
            "So when we are measuring the similarity, Euclidean distance does quite well.",
            "I think OK. That's my question.",
            "Let's thank speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much.",
                    "label": 0
                },
                {
                    "sent": "So hello everyone, my name is Abdallah.",
                    "label": 0
                },
                {
                    "sent": "When I'm from UC Riverside advised by Doctor Amen, Kia.",
                    "label": 0
                },
                {
                    "sent": "So the title of my talk is online discovery and maintenance of taxes motives.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let's discuss the what is the time series motif dances.",
                    "label": 0
                },
                {
                    "sent": "Motives are repeated patterns in a time series.",
                    "label": 1
                },
                {
                    "sent": "So here the red one and a blue one are two repetitions of a pattern.",
                    "label": 0
                },
                {
                    "sent": "So this is this has been a good primitive for data mining like activity or event discovery, summarization or classification.",
                    "label": 0
                },
                {
                    "sent": "In the last years.",
                    "label": 1
                },
                {
                    "sent": "There has been works on application, application of transfers, data motive discovery like data center chiller management which is the based application paper.",
                    "label": 0
                },
                {
                    "sent": "Our winner last year Kennedy there have been worked on work on designing smart cane for elders and so on.",
                    "label": 1
                },
                {
                    "sent": "So it has been very good to discover dances, motive for offline fashion in this paper or in this work.",
                    "label": 0
                },
                {
                    "sent": "We have considered the streaming scene.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a streaming time series which is generating numerical values continuously and we have a start sliding window of recent history.",
                    "label": 1
                },
                {
                    "sent": "So we went.",
                    "label": 0
                },
                {
                    "sent": "We want to answer the queries like what is the minute long pattern that repeated in the last hour.",
                    "label": 1
                },
                {
                    "sent": "So we have visual illustration of what we.",
                    "label": 0
                },
                {
                    "sent": "Tried to do which is a evolving time series which is an easy trace of a patient.",
                    "label": 0
                },
                {
                    "sent": "The sliding window size is 1000 samples, so at every timestamp we want to see what are the two occurrences of subsequences that are most similar in the bottom right we show the most sequence most similar subsequences overlaid on one another, and the left on the left bottom.",
                    "label": 0
                },
                {
                    "sent": "We show the distribution of motive distances for different sliding windows.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want to do.",
                    "label": 0
                },
                {
                    "sent": "So now let's formulate the problem as a data mining problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a time series.",
                    "label": 0
                },
                {
                    "sent": "The sliding window for the first time and we have taken a shorter window, which is probably a minute long or a seconds long and slide it across the sliding window to generate all the subsequences.",
                    "label": 0
                },
                {
                    "sent": "They not ask for.",
                    "label": 0
                },
                {
                    "sent": "The first window is to discover what is the most similar non overlapping subsequence.",
                    "label": 0
                },
                {
                    "sent": "So after we have discovered the most similar pair, our task is to maintain that so at every timestamp we have added one point and delete at one point.",
                    "label": 0
                },
                {
                    "sent": "So the sliding window slides across the streaming time series and we have to maintain or update the motive pair.",
                    "label": 0
                },
                {
                    "sent": "If one of the pair is deleted or a new closest pair comes in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do that.",
                    "label": 0
                },
                {
                    "sent": "What is the challenge in doing that?",
                    "label": 0
                },
                {
                    "sent": "So the subsequences that we extracted can be thought of as high dimensional points and then the dances motive detection becomes the dynamic closest pair points pair of points problem in high dimensionality?",
                    "label": 1
                },
                {
                    "sent": "So here is an example of two dimensional case like we have the time series subsequences which can be assumed to be lying in a 2 dimensional plane and assuming that the window size is 8.",
                    "label": 1
                },
                {
                    "sent": "So this is a toy example actually.",
                    "label": 0
                },
                {
                    "sent": "So we have the points.",
                    "label": 0
                },
                {
                    "sent": "So if if there is no overlapping issues then one for the points which are closest in the leftmost pane.",
                    "label": 0
                },
                {
                    "sent": "So upon upon the deletion of one point, say the first .1, the closest pair changes to 8 and two.",
                    "label": 0
                },
                {
                    "sent": "And then again, when the insertion comes in, the 9.9 is inserted.",
                    "label": 0
                },
                {
                    "sent": "The closest pair is changed again, so the bottom line is the closest pair may change upon every update.",
                    "label": 1
                },
                {
                    "sent": "Both insertion and deletions.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing can be done, but that can be done is to use quality comparisons among all possible pairs up in every update, but this is definitely not the desired one.",
                    "label": 0
                },
                {
                    "sent": "So our goal in this work is to find an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That will linear linear in linear time update.",
                    "label": 0
                },
                {
                    "sent": "The that the closest pair.",
                    "label": 0
                },
                {
                    "sent": "So when we say the linear update, we mean that we will do linear number of comparisons with the number of points in the sliding window.",
                    "label": 0
                },
                {
                    "sent": "So because the comparison of two subsequences is costly because of the normalization and it is a vector operation.",
                    "label": 0
                },
                {
                    "sent": "So there has been previous work on dynamic closest pair of points finding problem.",
                    "label": 0
                },
                {
                    "sent": "Epstein did that in back back in 2000, So what he did is he maintained the matrix of all pairwise distances and use a quadtree to maintain update the distance matrix so it has the problem of quadratic space requirement.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is we maintain a set of neighbors and reverse nearest neighbor rivers, neighbors for all points and we do it in W * sqrt W space, where W is the size of sliding window.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline of the rest of the counts.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We will discuss the methods and evaluations and then a couple of extensions to do more things with our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And some case studies in different domains, then I'll conclude.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main approaches that we want to maintain for every point the nearest neighbor of that point.",
                    "label": 0
                },
                {
                    "sent": "So if we can answer the smallest, find the smallest nearest neighbor, then that is the closest pair of points in the data set.",
                    "label": 1
                },
                {
                    "sent": "So maintain the nearest neighbor of every point.",
                    "label": 1
                },
                {
                    "sent": "Upon insertion we use up our all.",
                    "label": 0
                },
                {
                    "sent": "Install W comparisons to find the nearest neighbour whenever a new point is updated, inserted so now we do not have any comparisons left to do upon delation.",
                    "label": 0
                },
                {
                    "sent": "So whenever the next whenever a point is deleted, we have to do more task and in constant time without doing anymore comparisons.",
                    "label": 0
                },
                {
                    "sent": "Say for example in this in this example we have inserted so in the left we show the nearest neighbor relationship through the arrows.",
                    "label": 0
                },
                {
                    "sent": "So one arrow means the three.",
                    "label": 0
                },
                {
                    "sent": "Say for example three and seven.",
                    "label": 0
                },
                {
                    "sent": "This means that seven is nearest neighbor of three, so there is an arrow from 3 to 7.",
                    "label": 0
                },
                {
                    "sent": "So when nine is inserted.",
                    "label": 0
                },
                {
                    "sent": "Six is nearest neighbor is changed from 8 to 9 again when one is deleted.",
                    "label": 0
                },
                {
                    "sent": "The pair seven and four.",
                    "label": 0
                },
                {
                    "sent": "They had one as their nearest neighbor, which has to be reassigned with near nearest neighbors like 7 has new nearest neighbor as three and four has near nearest neighbor as seven.",
                    "label": 1
                },
                {
                    "sent": "So it means that when the deletion is occurring, we have to find all the reverse nearest neighbors of the deleting point and update their neighbor nearest neighbor by finding the next one.",
                    "label": 0
                },
                {
                    "sent": "So we need an ordering of the points.",
                    "label": 0
                },
                {
                    "sent": "In order of their distances.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the data structure.",
                    "label": 1
                },
                {
                    "sent": "For that we use to keep track of the neighbors.",
                    "label": 1
                },
                {
                    "sent": "So we have 5 off Slide 5 for sliding window.",
                    "label": 1
                },
                {
                    "sent": "For this case it is eight points, so the points coming in from the right and going out of the left and for each point we have a list of neighbors in order of the distances.",
                    "label": 1
                },
                {
                    "sent": "So every point in this rectangle.",
                    "label": 0
                },
                {
                    "sent": "List the rectangular points that couple stuff pointed to the point and the distance value for each point.",
                    "label": 0
                },
                {
                    "sent": "We also keep the list of rivers.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbors are the earliest we call it our list and the green ones are endless.",
                    "label": 0
                },
                {
                    "sent": "But whenever a point is deleted, say for example, one is going to be deleted, we go to once first rivers nearest neighbor, find that point in the list, delete one from its head.",
                    "label": 0
                },
                {
                    "sent": "And then take the next nearest neighbor which is 7, go to 7 and insert for there.",
                    "label": 0
                },
                {
                    "sent": "That is the task of updating when deletion occurs and this is constant time.",
                    "label": 0
                },
                {
                    "sent": "We do not do any kind of comparisons, any kind of subsequence comparisons.",
                    "label": 0
                },
                {
                    "sent": "Similarly for seven we do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We go to the next reverse nearest neighbor of 1, which is 7.",
                    "label": 0
                },
                {
                    "sent": "We go from 7 from.",
                    "label": 0
                },
                {
                    "sent": "Follow that pointer, delete one from that nearest neighbor neighbor list.",
                    "label": 0
                },
                {
                    "sent": "And follow through three and add 7 as the reverse nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Everything is fine up till now, but it does not solve the problem of quadratic space requirement by the Epstein.",
                    "label": 0
                },
                {
                    "sent": "So what we do to reduce the size of the squared size of the data structure?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have two observations.",
                    "label": 0
                },
                {
                    "sent": "First, while inserting we do not need to update old points nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So the main intuition here is this quadratic space structure has every pair two times.",
                    "label": 0
                },
                {
                    "sent": "So say for example in list of one we have 1, four and again in list of four we have 4 one.",
                    "label": 0
                },
                {
                    "sent": "So every pair is occurring twice here.",
                    "label": 0
                },
                {
                    "sent": "So if we impose the temporal ordering explicitly like saying that every point will have the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Only from the set of prior points, not from the later points or from the whole set of points.",
                    "label": 0
                },
                {
                    "sent": "Then we can reduce it by half.",
                    "label": 0
                },
                {
                    "sent": "But obviously now the neighbor lists are the endless than our lists do not have the neighbors for the whole snapshots.",
                    "label": 0
                },
                {
                    "sent": "It is changed now.",
                    "label": 0
                },
                {
                    "sent": "After that it is still quite ready.",
                    "label": 0
                },
                {
                    "sent": "We then use our second observation that a point can be removed from the neighbor list if it violates the temporal order.",
                    "label": 1
                },
                {
                    "sent": "So for example, from 48, the list of neighbors is still quadratic.",
                    "label": 0
                },
                {
                    "sent": "I mean still W point long, so 265713 and four.",
                    "label": 0
                },
                {
                    "sent": "So if we remove the the points that violate the temporal order, it becomes 2, six and seven.",
                    "label": 0
                },
                {
                    "sent": "So which means that.",
                    "label": 0
                },
                {
                    "sent": "Every list is a longest increasing subsequence of a random permutation of W points, so the expected value for that is square root of W. So which means that we reduced the size from W to square root of W of every enlist.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example where we show everything in action.",
                    "label": 0
                },
                {
                    "sent": "So this is our final data structure.",
                    "label": 0
                },
                {
                    "sent": "We will update delete one and insert nine in the next step.",
                    "label": 0
                },
                {
                    "sent": "So it changes like this.",
                    "label": 0
                },
                {
                    "sent": "Then again D2 is related, 10 is inserted.",
                    "label": 0
                },
                {
                    "sent": "It changes like this.",
                    "label": 0
                },
                {
                    "sent": "So let me go back again and show it again.",
                    "label": 0
                },
                {
                    "sent": "So the the main thing to observe here is that every node is inserted with a list of neighbors and as it passes through the FIFO, it loses neighbors and gains rivers nice neighbors.",
                    "label": 0
                },
                {
                    "sent": "So the overall space requirement kind of stabilized by this approach of updating the data structure.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the evaluation of our method.",
                    "label": 0
                },
                {
                    "sent": "So we have compared compared to our algorithm with the fast fair implementation which is developed in UC Irvine so fast there is a specific implementation for general dynamic closest pair problem where insertions and deletions can occur at any order.",
                    "label": 1
                },
                {
                    "sent": "So say 5 insertions, then two deletions, then five insertions like this.",
                    "label": 0
                },
                {
                    "sent": "So we have you so that is more generic problem.",
                    "label": 0
                },
                {
                    "sent": "We have considered the more specific one.",
                    "label": 0
                },
                {
                    "sent": "Wait, insertions and deletions occur in a specific order.",
                    "label": 0
                },
                {
                    "sent": "So we have tried four datasets, insect, random work, EOG, an easy, only the random walk was synthetic so on for all four datasets.",
                    "label": 0
                },
                {
                    "sent": "Fast peer perform almost in the same way, so the black line shows that fast bear and our algorithm performs a better way up to 8 times faster than fast here.",
                    "label": 0
                },
                {
                    "sent": "So this is the X axis.",
                    "label": 0
                },
                {
                    "sent": "We show the window size so as we increase the window size after 40,000 and the Y axis shows the average update time.",
                    "label": 1
                },
                {
                    "sent": "So we have another parameter which is the modifying the smaller window lengths, so we vary that and we see the same performance gain for different values of motif length.",
                    "label": 1
                },
                {
                    "sent": "What is in terms of space.",
                    "label": 0
                },
                {
                    "sent": "So this table we have stables, purse cost per point with increasing window size.",
                    "label": 1
                },
                {
                    "sent": "So if we increase the window size in the bottom figure then we see that the average length of that enlist it never reaches the whole W, so it kind of becomes stable.",
                    "label": 0
                },
                {
                    "sent": "And this is also observed for different multiflex.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'll show the extensions that are kind of adding some values to our algorithm and the case studies next so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What if we have multidimensional time series where we have be synchronous time series and we want to find the most similar pair of subsequences which can be from any 2 dimensions?",
                    "label": 1
                },
                {
                    "sent": "So there we obviously would need the number of typos and each pointer can point to points in other fibers like this, and this increases the space and time cost by factor of D, so it is linear in number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "There is no curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "The second thing is, what if the data comes faster than the rate we can process, so that in that case there is only one option of load shedding, because no buffer can solve the problem of arbitrary data it because it there is always a chance of over overflowing by arbitrary number of consecutive worst cases.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we skip points if we cannot handle it and we see that if we reduce the fraction of data used by more and more high data rate, we have discovered motive in greater factions, which means that even though we are losing data, we do not lose the motives.",
                    "label": 1
                },
                {
                    "sent": "The reason is the worst cases do not contribute contribute to the modifier.",
                    "label": 0
                },
                {
                    "sent": "That is the main reason why we have seen this observations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the first case study we have done on ECG data.",
                    "label": 0
                },
                {
                    "sent": "Here we the settings is in online compression like we have a streaming data source.",
                    "label": 0
                },
                {
                    "sent": "We're getting data from a streaming data source, compress it and sending it to some other places.",
                    "label": 0
                },
                {
                    "sent": "So when we do complete compression, existing approach does this.",
                    "label": 0
                },
                {
                    "sent": "So it takes say 4 five points, approximate it with some linear segment and send it to some other place.",
                    "label": 0
                },
                {
                    "sent": "So for every five points it sends two coefficients for the line segment.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we run our algorithm on the streaming stream for a particular amount of time and then generate a Dictionary of patterns.",
                    "label": 0
                },
                {
                    "sent": "Then we observe this stream.",
                    "label": 0
                },
                {
                    "sent": "And see if the pattern occurs.",
                    "label": 0
                },
                {
                    "sent": "Industry.",
                    "label": 0
                },
                {
                    "sent": "If it occurs, then we replace the pattern by only a pointer to that pattern in the dictionary, so that way it compresses highly and decompresses smoothly.",
                    "label": 0
                },
                {
                    "sent": "Like if you see the red one in the middle, it is kind of discontinuous.",
                    "label": 0
                },
                {
                    "sent": "But if you see the bottom one, there it is more smooth.",
                    "label": 0
                },
                {
                    "sent": "So for signals with her regular repetitions this.",
                    "label": 1
                },
                {
                    "sent": "This approach has high compression rate with less error.",
                    "label": 1
                },
                {
                    "sent": "Our next case.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Daddy is in robotics, so in robotics are there is a problem called closing the loop problem where the robot is tasked to be the problem of finding whether it is in a position where it was in before.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have collected a data set which is called New England data set.",
                    "label": 0
                },
                {
                    "sent": "I forgot to put the reference over, so here one robot starts in a given position.",
                    "label": 0
                },
                {
                    "sent": "It completes a loop and then.",
                    "label": 1
                },
                {
                    "sent": "Changes back so we have their boat has a camera, it snap.",
                    "label": 0
                },
                {
                    "sent": "Takes snapshot for both from both sides of it and give streaming the data to the central processor so we have a video stream of photos taken by the robot.",
                    "label": 0
                },
                {
                    "sent": "So we change that video stream to see the time series by changing each frame through color histograms and then run our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what what our algorithm is doing here is.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Without including introducing any latency, it determines what are the good candidates for closing the loop problem.",
                    "label": 1
                },
                {
                    "sent": "So there's the right part shows the two image segments which found by our algorithm, which are very similar because their corresponding color histograms are very similar, which happens to be the closest.",
                    "label": 0
                },
                {
                    "sent": "Closing the loop in the in this data set shown by the loop closure detected error.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this, I conclude my talk that this is the first at and to maintain cancer is motive online.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we maintain minutes long reputation in our long window.",
                    "label": 0
                },
                {
                    "sent": "So we have tried up to hundreds of data rate which is sufficient for most natural data sources.",
                    "label": 0
                },
                {
                    "sent": "And we do it in linear versus linear update time with less cost.",
                    "label": 1
                },
                {
                    "sent": "That what is so we have W sqrt W instead of W squared.",
                    "label": 1
                },
                {
                    "sent": "And even though it is worst case linear, we have a faster than faster algorithm than the general dynamic closes pair solution.",
                    "label": 0
                },
                {
                    "sent": "Existing solution so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "So we have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Any question?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Finding the nearest nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "But actually this function is comparing it to you.",
                    "label": 0
                },
                {
                    "sent": "Please in distance Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "After normalization there is a part of normalizing, normalizing for the Bayes shifting and scaling variances, and actually this is the question I want to ask you.",
                    "label": 0
                },
                {
                    "sent": "Just a follow up that so you just mentioned that this is a little bit high dimensional, so so do you have any idea how large the dimension?",
                    "label": 0
                },
                {
                    "sent": "Yes, we have tried if we see the results.",
                    "label": 0
                },
                {
                    "sent": "So the the motif length is the dimension, so we have right up 201 thousand.",
                    "label": 0
                },
                {
                    "sent": "So the length of the pattern that we are finding can be up to 1000 S for our experiments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because I just had to constantly use Euclidean distance for the high dimensional measuring similarity within the high dimension space.",
                    "label": 0
                },
                {
                    "sent": "So usually the Ukraine.",
                    "label": 0
                },
                {
                    "sent": "This distance is not that meaningful in high dimension space.",
                    "label": 0
                },
                {
                    "sent": "It is so my comment here is it is it is not meaningful when the distance is high.",
                    "label": 0
                },
                {
                    "sent": "So say for example if we had three points where each of them having distance of five or 10, then it is not meaningful.",
                    "label": 0
                },
                {
                    "sent": "But if you have two things with the bleeding distance zero, then whatever the dimension is they are single.",
                    "label": 0
                },
                {
                    "sent": "So when we are measuring the similarity, Euclidean distance does quite well.",
                    "label": 0
                },
                {
                    "sent": "I think OK. That's my question.",
                    "label": 0
                },
                {
                    "sent": "Let's thank speak again.",
                    "label": 0
                }
            ]
        }
    }
}