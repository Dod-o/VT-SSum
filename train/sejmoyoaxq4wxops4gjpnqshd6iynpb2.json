{
    "id": "sejmoyoaxq4wxops4gjpnqshd6iynpb2",
    "title": "Multimodal semi-supervised learning for image classification",
    "info": {
        "author": [
            "Matthieu Guillaumin, INRIA Grenoble Rh\u00f4ne-Alpes"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Computer Vision->Object Recognition"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_guillaumin_mssl/",
    "segmentation": [
        [
            "Good afternoon, so I'm at Yuma and I'm here to talk about our paper called Multimodal Symbolism in supervised learning for image classification.",
            "This is joint work with backup for bacon.",
            "Codilla Schmidt at inria."
        ],
        [
            "So the motivation for this work and for using language for image classification comes from the fact that more and more frequently images come with additional textual information, and that's especially true on the web.",
            "So if we look at this Wikipedia page example, we have this image of the Golden Gate Bridge and well, the text surrounding this image tells us that this suspension bridge that it's in San Francisco and we have GPS coordinates and a lot of information.",
            "So this is not restricted to.",
            "Online data this is also true with videos, for instance that come with script some subtitles."
        ],
        [
            "So here we will still restrict or study two images that we find online and we're going to use.",
            "The user tags that we can find on Flickr to help visual object category recognition.",
            "So if you look at this example image from Flickr, well you can imagine that it's pretty hard for a visual classifier to decide he correctly that this is a bridge.",
            "But if the system is allowed to look at the tags, well, well, some of the tags are not informative, but at least three of them that I highlighted here can tell us a lot about the visual content.",
            "So for instance, San Francisco Golden Gate Bridge and fog."
        ],
        [
            "So the overview of this talk is the following.",
            "I'm going to talk about the datasets and the features we're using an, then I'll go over free learning scenarios that we considered in this work that go from the completely supervised setting.",
            "Multimodal for multimodal classification and then I will remove some supervision to and go to the Cemetery by setting and then even remove completely human annotation in a weakly supervised learning."
        ],
        [
            "So we use actually two datasets that are collected from Flickr.",
            "The first is the well known Pascal VOC 2007 data set which contains about 10,000 images of 20 classes.",
            "But if you download it from the Pascal webpage, don't have the Flickr tags, but we use the Flickr ID to go fetch on the web.",
            "The user tags on Flickr.",
            "So at the time of download there was about 95% of the images that were still 9, so we really got most of the tags.",
            "We also used a more recent multimodal information retrieval of Flickr datasets.",
            "So I will click near Flickr.",
            "It contains 25,000 images of 38 classes that are manually annotated.",
            "It contains 450 different feature tags."
        ],
        [
            "So to represent the textual features we actually going to limit the vocabulary of tags that were using.",
            "That's because if we look at this scatter plot of the frequency of the tags, we see that.",
            "So here I sorted the tag index with respect to frequency.",
            "And here I plot on the X axis the tag frequency.",
            "You see that like more than 10,000 of the tags just appear once, so we're not going to be able to exploit this information very well.",
            "So we actually restricted.",
            "Recovery to the 800 tags that appear more than four times.",
            "I to using this vocabulary we can have a binary vector representing simply the tag, presence or absence for an image, and for this vector we use a linear kernel that simply will count the number of tags that are shared between 2 images."
        ],
        [
            "Concerning the visual features, we use a collection of 15 different image representation that go from local to global descriptors and also include color and texture.",
            "We also have spatial layouts from the spatial.",
            "Matching work from last week."
        ],
        [
            "So now I'm going to move to the first learning setting, which is a supervised with the model classification.",
            "So let me."
        ],
        [
            "Say a bit more about the setting so we have manual annotation for the entire training data and the user has manually annotated images for containing given class or not.",
            "Here it's dog, so we're going to use the Flickr tags as additional feature for classification.",
            "An importantly, those tags are also available at Test time, so we can use multimodal pacifier to obtain libel prediction for the test images.",
            "And we're going to use the multiple caroling framework to combine the visual and textual kernel."
        ],
        [
            "So if we look at the mean average precision on Pascal VOC 2007, here the image price tag is our method.",
            "The multiple kernel learning and we compare those to the VM's, learn on each individual kernel and on the so I have the 20 Pascal classes and the mean on the far right.",
            "So what you can see from this graph is that it depends on the class whether the tags or the visual features are the best to do the prediction.",
            "But so on on this particular choice of classes, they are on average, the tags seem to perform a bit worse than the image, but if you combine them with the multiple kernel learning well, you have a significant improvement on the classification.",
            "And this is true for each individual class.",
            "So except for four, which are person, chair, sofa and dining table where the multiple kernel learning what the multiple classifier performs comparably to the best of the two?",
            "Individual classifier.",
            "So Interestingly, if you look at the winner of the Pascal VLC 2007, which used of course image only, he obtained a 59% of map, which is better than our image kernel, but simply worse than the combined classifier that we used, and we observed exactly very similar results for a mere Flickr data set."
        ],
        [
            "So not not well, sorry.",
            "Now that we are encouraged by this result, we can try to remove some supervision to see how well our method can perform.",
            "With less supervision."
        ],
        [
            "So there are two main differences in the what I called Milton model.",
            "Similar supervised scenario and the first is the supervised case where we have a large pool of unlabeled data and here it's like you fetch a lot of images from Flickr, so you can also get the tags from those images.",
            "But the second difference and I think it's more important it's at the tags will not be available at Test time anymore.",
            "So it's a reasonable assumption, because you can imagine that users upload their new images on Flickr.",
            "They don't have tag, they didn't tag them yet.",
            "But still you want to classify the well you want to do visual object recognition on those images.",
            "So to address this problem, we propose 3 steps."
        ],
        [
            "Learning algorithm, the first step is to learn multiple kernel learning, well multimodal classifier on the label images with their tags and then to label the unlabeled data using this combined classifier and as a first step we're going to train a visual learning classifier with the labels that are predicted by the MCL classifier.",
            "So we consider two options for this third step.",
            "The first is an SVM where we're going to use the sign of the score.",
            "Of the combined classifier to label them labeled data, an well trained, the visual only classifier.",
            "Doing this, we actually will ignore.",
            "Well, I mean if we use only the sign of the scores we ignore the confidence of the classification.",
            "So instead what he proposes to regress the combined score?",
            "Using the visual kernel only and we do that with a simple discretion that we regularize using K PCA."
        ],
        [
            "So let me summarize the five settings that we compare here.",
            "The first is the base, the baseline SVM that we trained on the visual on the visual kernel only on the label set.",
            "Then in the second method we can use this as VM to label the unlabeled data and retraining SVM.",
            "On the entire training set, we also compared to the state of the Art Co. Training by Bloom and Michelle, 98, and here you can actually code Train a visual SVM using the textual SVM.",
            "And we also compare with all two options of our three step learning algorithm.",
            "So the first is using SVM as the third step and the 2nd is to use the regression of the multiple classify."
        ],
        [
            "So here I show the mean average precision.",
            "Score for two datasets.",
            "Pass calendar for the the five methods that we compare, and I also show how the scorer evolved with the number of labeled training examples.",
            "So for the first column here I have 40, which means that I have 20 labeled images positive and 10:20 negative, and I use the rest of the training set as my pool of unlabeled data.",
            "So the first observation, as we can do from that we can draw from this plot is that re learning and SVM after using it after learning the first one is worse than the original SVM.",
            "That's because the.",
            "Well, basically the SVM will not be able to learn from itself.",
            "Now if we look at the Co training method, it does a reasonable job in improving the visual classifier using that extra one.",
            "And if we compare to our methods, well, the using an SVM as the second step is actually just only slightly better than the baseline while the regression is significantly better and especially when small amount of supervision is used and when we increase the level of sufficient.",
            "And in the paper we consider using up to half of the training set as labeled data.",
            "The difference tends to shrink."
        ],
        [
            "So now I'm going to try to remove any manual annotation that we're using, and this is possible because."
        ],
        [
            "For some of the classes we actually have the corresponding tag in the vocabulary.",
            "So for instance here for the class dog we see that actually two of our images had the tag dog.",
            "So here we used those tags and their presence and absence directly as the class label.",
            "So.",
            "Here too, to evaluate, we still want to know how good our visual classifier is, so we were not.",
            "We don't want to know how well we predict the tag dog, so we're going to evaluate on the manual annotation for dog.",
            "An we can note that we can still use the other tags as additional features to improve classification.",
            "So to be more precise, sorry, I mean let me go back here.",
            "You can see that the presence of the tag here is a relatively confident information.",
            "Usually when you have the tag present then you indeed have dog in the image, but for images where you don't have the tag, it doesn't really say a lot about the content.",
            "That's because user don't put all the possible tags on their images.",
            "So to be more precise."
        ],
        [
            "Detect present is relatively clean.",
            "You have 82% precision, but if you use the tag you only get less than 80% of the images that are relevant for your class.",
            "So using this we propose to modify slightly or approach an concerns only the second step.",
            "So first we learn our multimodal classifier on the using the tag annotation, and we're going to rank the training images that don't have the tag and remove those that have the highest combined score.",
            "Then we're going to fit the least core regulation on the remaining images, and we compared to a baseline where we only retrain the visual only SVM on the on the tag annotation so."
        ],
        [
            "So actually you have 18 classes of the mere Flickr data said that correspond to tags.",
            "And if we look at the mean average precision here, the baseline performs 38.5%.",
            "Which is significantly lower than the manual annotation, and if we look at our method, we can actually improve over this baseline as we remove more training pad negatives.",
            "So if we look at individual classes that don't show here, actually the baseline is better for four classes, while our method is better for the other 14 with up to 10% additional mean average."
        ],
        [
            "Decision.",
            "So to conclude, let me go over the free settings again, so we have considered using Flickr tags for free scenarios.",
            "The first is the fully supervised classification, the second is removing manual annotation in a supervised setting, and finally removing all manual annotation altogether in a weakly supervised learning.",
            "So for that last two methods we propose three step learning process where you train a multimodal pacifier and then.",
            "Use it to classify the unlabeled data, and finally you regress this multimodal classifier to obtain a visual only classifier and that we show an old three settings that outperforms SVM with nonlinear visual kernel and it also improves the Co training for the Super supervised setting.",
            "Thankful."
        ],
        [
            "Thank you for intention and will have it your answer, any question.",
            "Thank you very much.",
            "We could plan for a couple of questions.",
            "May could you, could you maybe use the microphone or?",
            "So I'm not sure I actually got the question, so if you could repeat, please.",
            "So.",
            "Yeah, exactly the.",
            "The setting we're using is exactly the Pascal visual object category recognition, so we just want to have the binary decision.",
            "Is there a dog or not in the image, right?",
            "So I have I have manual annotation that.",
            "It tells me if there's a dog or not and I want to.",
            "Well, the I explained that the visual feature that we're using we're using standard color histogram, bag of word histograms with features and we have a collection of 15 image representation that we combine in the RBF kernel that it's.",
            "One of my first slides.",
            "Here.",
            "So we have.",
            "We we could we compute Harris interest points and also sample points on the dense grid.",
            "Then we describe them with C features for texture and Hue features for color weaken ties them to obtain biographer representation for those four type of descriptors.",
            "So that's already 4 representation.",
            "And here we have color Instagram on precor spaces and we use 16 bits per channel.",
            "We use a dish just descriptor and for each of the histogram we also consider binding them in different regions and that's in total 15 image representation that we normalize an average.",
            "To put that in their RBF kernel.",
            "Thank you, are there more questions?",
            "So the question is, do we explicitly use the correlation between the tags to improve classification?",
            "So the question that the answer is no, we just used the.",
            "So here we still consider.",
            "The other tags as additional features, so the SVM will or the combined classifier will use it implicitly.",
            "I mean, if he if he finds that there is interesting feature to use that are different than the tags were considering, then he will use it.",
            "Through another question, very quick one.",
            "I have a question regarding so experimental protocol, especially on Pascal Pascal.",
            "Data set was collected by using text queries, you know on Flickr, which means that the classes you're interested in, you know there is a guarantee that at least one of the tag is informative of the classes.",
            "Which is not the case in general, you know so.",
            "Isn't it guaranteed that actually the text you know will always, you know, provide you with information and Pascal?",
            "Or is it the best data set you know to use to?",
            "Do you know image plus text?",
            "You know merging?",
            "OK, good question, so indeed well, it's well known that the Pascal data set has some strong bias, so even in the visual appearance of the images, but also in the tags.",
            "That's why we also had this additional data set of near Flickr.",
            "And since the observation and their experimental results are really, really similar.",
            "It kind of shows that while this has a little influence on on our method.",
            "Thank you very much.",
            "Let's think this is speaker once again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, so I'm at Yuma and I'm here to talk about our paper called Multimodal Symbolism in supervised learning for image classification.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with backup for bacon.",
                    "label": 0
                },
                {
                    "sent": "Codilla Schmidt at inria.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the motivation for this work and for using language for image classification comes from the fact that more and more frequently images come with additional textual information, and that's especially true on the web.",
                    "label": 1
                },
                {
                    "sent": "So if we look at this Wikipedia page example, we have this image of the Golden Gate Bridge and well, the text surrounding this image tells us that this suspension bridge that it's in San Francisco and we have GPS coordinates and a lot of information.",
                    "label": 0
                },
                {
                    "sent": "So this is not restricted to.",
                    "label": 0
                },
                {
                    "sent": "Online data this is also true with videos, for instance that come with script some subtitles.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we will still restrict or study two images that we find online and we're going to use.",
                    "label": 0
                },
                {
                    "sent": "The user tags that we can find on Flickr to help visual object category recognition.",
                    "label": 1
                },
                {
                    "sent": "So if you look at this example image from Flickr, well you can imagine that it's pretty hard for a visual classifier to decide he correctly that this is a bridge.",
                    "label": 0
                },
                {
                    "sent": "But if the system is allowed to look at the tags, well, well, some of the tags are not informative, but at least three of them that I highlighted here can tell us a lot about the visual content.",
                    "label": 1
                },
                {
                    "sent": "So for instance, San Francisco Golden Gate Bridge and fog.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the overview of this talk is the following.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about the datasets and the features we're using an, then I'll go over free learning scenarios that we considered in this work that go from the completely supervised setting.",
                    "label": 0
                },
                {
                    "sent": "Multimodal for multimodal classification and then I will remove some supervision to and go to the Cemetery by setting and then even remove completely human annotation in a weakly supervised learning.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we use actually two datasets that are collected from Flickr.",
                    "label": 0
                },
                {
                    "sent": "The first is the well known Pascal VOC 2007 data set which contains about 10,000 images of 20 classes.",
                    "label": 1
                },
                {
                    "sent": "But if you download it from the Pascal webpage, don't have the Flickr tags, but we use the Flickr ID to go fetch on the web.",
                    "label": 0
                },
                {
                    "sent": "The user tags on Flickr.",
                    "label": 0
                },
                {
                    "sent": "So at the time of download there was about 95% of the images that were still 9, so we really got most of the tags.",
                    "label": 0
                },
                {
                    "sent": "We also used a more recent multimodal information retrieval of Flickr datasets.",
                    "label": 0
                },
                {
                    "sent": "So I will click near Flickr.",
                    "label": 1
                },
                {
                    "sent": "It contains 25,000 images of 38 classes that are manually annotated.",
                    "label": 0
                },
                {
                    "sent": "It contains 450 different feature tags.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to represent the textual features we actually going to limit the vocabulary of tags that were using.",
                    "label": 0
                },
                {
                    "sent": "That's because if we look at this scatter plot of the frequency of the tags, we see that.",
                    "label": 0
                },
                {
                    "sent": "So here I sorted the tag index with respect to frequency.",
                    "label": 1
                },
                {
                    "sent": "And here I plot on the X axis the tag frequency.",
                    "label": 0
                },
                {
                    "sent": "You see that like more than 10,000 of the tags just appear once, so we're not going to be able to exploit this information very well.",
                    "label": 1
                },
                {
                    "sent": "So we actually restricted.",
                    "label": 0
                },
                {
                    "sent": "Recovery to the 800 tags that appear more than four times.",
                    "label": 0
                },
                {
                    "sent": "I to using this vocabulary we can have a binary vector representing simply the tag, presence or absence for an image, and for this vector we use a linear kernel that simply will count the number of tags that are shared between 2 images.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concerning the visual features, we use a collection of 15 different image representation that go from local to global descriptors and also include color and texture.",
                    "label": 0
                },
                {
                    "sent": "We also have spatial layouts from the spatial.",
                    "label": 0
                },
                {
                    "sent": "Matching work from last week.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to move to the first learning setting, which is a supervised with the model classification.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say a bit more about the setting so we have manual annotation for the entire training data and the user has manually annotated images for containing given class or not.",
                    "label": 0
                },
                {
                    "sent": "Here it's dog, so we're going to use the Flickr tags as additional feature for classification.",
                    "label": 1
                },
                {
                    "sent": "An importantly, those tags are also available at Test time, so we can use multimodal pacifier to obtain libel prediction for the test images.",
                    "label": 1
                },
                {
                    "sent": "And we're going to use the multiple caroling framework to combine the visual and textual kernel.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at the mean average precision on Pascal VOC 2007, here the image price tag is our method.",
                    "label": 1
                },
                {
                    "sent": "The multiple kernel learning and we compare those to the VM's, learn on each individual kernel and on the so I have the 20 Pascal classes and the mean on the far right.",
                    "label": 0
                },
                {
                    "sent": "So what you can see from this graph is that it depends on the class whether the tags or the visual features are the best to do the prediction.",
                    "label": 0
                },
                {
                    "sent": "But so on on this particular choice of classes, they are on average, the tags seem to perform a bit worse than the image, but if you combine them with the multiple kernel learning well, you have a significant improvement on the classification.",
                    "label": 0
                },
                {
                    "sent": "And this is true for each individual class.",
                    "label": 0
                },
                {
                    "sent": "So except for four, which are person, chair, sofa and dining table where the multiple kernel learning what the multiple classifier performs comparably to the best of the two?",
                    "label": 0
                },
                {
                    "sent": "Individual classifier.",
                    "label": 0
                },
                {
                    "sent": "So Interestingly, if you look at the winner of the Pascal VLC 2007, which used of course image only, he obtained a 59% of map, which is better than our image kernel, but simply worse than the combined classifier that we used, and we observed exactly very similar results for a mere Flickr data set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So not not well, sorry.",
                    "label": 0
                },
                {
                    "sent": "Now that we are encouraged by this result, we can try to remove some supervision to see how well our method can perform.",
                    "label": 0
                },
                {
                    "sent": "With less supervision.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are two main differences in the what I called Milton model.",
                    "label": 0
                },
                {
                    "sent": "Similar supervised scenario and the first is the supervised case where we have a large pool of unlabeled data and here it's like you fetch a lot of images from Flickr, so you can also get the tags from those images.",
                    "label": 0
                },
                {
                    "sent": "But the second difference and I think it's more important it's at the tags will not be available at Test time anymore.",
                    "label": 1
                },
                {
                    "sent": "So it's a reasonable assumption, because you can imagine that users upload their new images on Flickr.",
                    "label": 0
                },
                {
                    "sent": "They don't have tag, they didn't tag them yet.",
                    "label": 0
                },
                {
                    "sent": "But still you want to classify the well you want to do visual object recognition on those images.",
                    "label": 0
                },
                {
                    "sent": "So to address this problem, we propose 3 steps.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning algorithm, the first step is to learn multiple kernel learning, well multimodal classifier on the label images with their tags and then to label the unlabeled data using this combined classifier and as a first step we're going to train a visual learning classifier with the labels that are predicted by the MCL classifier.",
                    "label": 0
                },
                {
                    "sent": "So we consider two options for this third step.",
                    "label": 0
                },
                {
                    "sent": "The first is an SVM where we're going to use the sign of the score.",
                    "label": 0
                },
                {
                    "sent": "Of the combined classifier to label them labeled data, an well trained, the visual only classifier.",
                    "label": 0
                },
                {
                    "sent": "Doing this, we actually will ignore.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean if we use only the sign of the scores we ignore the confidence of the classification.",
                    "label": 1
                },
                {
                    "sent": "So instead what he proposes to regress the combined score?",
                    "label": 1
                },
                {
                    "sent": "Using the visual kernel only and we do that with a simple discretion that we regularize using K PCA.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me summarize the five settings that we compare here.",
                    "label": 0
                },
                {
                    "sent": "The first is the base, the baseline SVM that we trained on the visual on the visual kernel only on the label set.",
                    "label": 0
                },
                {
                    "sent": "Then in the second method we can use this as VM to label the unlabeled data and retraining SVM.",
                    "label": 0
                },
                {
                    "sent": "On the entire training set, we also compared to the state of the Art Co. Training by Bloom and Michelle, 98, and here you can actually code Train a visual SVM using the textual SVM.",
                    "label": 0
                },
                {
                    "sent": "And we also compare with all two options of our three step learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the first is using SVM as the third step and the 2nd is to use the regression of the multiple classify.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I show the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "Score for two datasets.",
                    "label": 0
                },
                {
                    "sent": "Pass calendar for the the five methods that we compare, and I also show how the scorer evolved with the number of labeled training examples.",
                    "label": 1
                },
                {
                    "sent": "So for the first column here I have 40, which means that I have 20 labeled images positive and 10:20 negative, and I use the rest of the training set as my pool of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So the first observation, as we can do from that we can draw from this plot is that re learning and SVM after using it after learning the first one is worse than the original SVM.",
                    "label": 0
                },
                {
                    "sent": "That's because the.",
                    "label": 0
                },
                {
                    "sent": "Well, basically the SVM will not be able to learn from itself.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at the Co training method, it does a reasonable job in improving the visual classifier using that extra one.",
                    "label": 0
                },
                {
                    "sent": "And if we compare to our methods, well, the using an SVM as the second step is actually just only slightly better than the baseline while the regression is significantly better and especially when small amount of supervision is used and when we increase the level of sufficient.",
                    "label": 0
                },
                {
                    "sent": "And in the paper we consider using up to half of the training set as labeled data.",
                    "label": 0
                },
                {
                    "sent": "The difference tends to shrink.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to try to remove any manual annotation that we're using, and this is possible because.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For some of the classes we actually have the corresponding tag in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So for instance here for the class dog we see that actually two of our images had the tag dog.",
                    "label": 0
                },
                {
                    "sent": "So here we used those tags and their presence and absence directly as the class label.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here too, to evaluate, we still want to know how good our visual classifier is, so we were not.",
                    "label": 0
                },
                {
                    "sent": "We don't want to know how well we predict the tag dog, so we're going to evaluate on the manual annotation for dog.",
                    "label": 0
                },
                {
                    "sent": "An we can note that we can still use the other tags as additional features to improve classification.",
                    "label": 1
                },
                {
                    "sent": "So to be more precise, sorry, I mean let me go back here.",
                    "label": 0
                },
                {
                    "sent": "You can see that the presence of the tag here is a relatively confident information.",
                    "label": 0
                },
                {
                    "sent": "Usually when you have the tag present then you indeed have dog in the image, but for images where you don't have the tag, it doesn't really say a lot about the content.",
                    "label": 0
                },
                {
                    "sent": "That's because user don't put all the possible tags on their images.",
                    "label": 0
                },
                {
                    "sent": "So to be more precise.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detect present is relatively clean.",
                    "label": 1
                },
                {
                    "sent": "You have 82% precision, but if you use the tag you only get less than 80% of the images that are relevant for your class.",
                    "label": 0
                },
                {
                    "sent": "So using this we propose to modify slightly or approach an concerns only the second step.",
                    "label": 0
                },
                {
                    "sent": "So first we learn our multimodal classifier on the using the tag annotation, and we're going to rank the training images that don't have the tag and remove those that have the highest combined score.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to fit the least core regulation on the remaining images, and we compared to a baseline where we only retrain the visual only SVM on the on the tag annotation so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually you have 18 classes of the mere Flickr data said that correspond to tags.",
                    "label": 1
                },
                {
                    "sent": "And if we look at the mean average precision here, the baseline performs 38.5%.",
                    "label": 0
                },
                {
                    "sent": "Which is significantly lower than the manual annotation, and if we look at our method, we can actually improve over this baseline as we remove more training pad negatives.",
                    "label": 0
                },
                {
                    "sent": "So if we look at individual classes that don't show here, actually the baseline is better for four classes, while our method is better for the other 14 with up to 10% additional mean average.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decision.",
                    "label": 0
                },
                {
                    "sent": "So to conclude, let me go over the free settings again, so we have considered using Flickr tags for free scenarios.",
                    "label": 1
                },
                {
                    "sent": "The first is the fully supervised classification, the second is removing manual annotation in a supervised setting, and finally removing all manual annotation altogether in a weakly supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So for that last two methods we propose three step learning process where you train a multimodal pacifier and then.",
                    "label": 0
                },
                {
                    "sent": "Use it to classify the unlabeled data, and finally you regress this multimodal classifier to obtain a visual only classifier and that we show an old three settings that outperforms SVM with nonlinear visual kernel and it also improves the Co training for the Super supervised setting.",
                    "label": 0
                },
                {
                    "sent": "Thankful.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for intention and will have it your answer, any question.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "We could plan for a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "May could you, could you maybe use the microphone or?",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure I actually got the question, so if you could repeat, please.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly the.",
                    "label": 0
                },
                {
                    "sent": "The setting we're using is exactly the Pascal visual object category recognition, so we just want to have the binary decision.",
                    "label": 0
                },
                {
                    "sent": "Is there a dog or not in the image, right?",
                    "label": 0
                },
                {
                    "sent": "So I have I have manual annotation that.",
                    "label": 0
                },
                {
                    "sent": "It tells me if there's a dog or not and I want to.",
                    "label": 0
                },
                {
                    "sent": "Well, the I explained that the visual feature that we're using we're using standard color histogram, bag of word histograms with features and we have a collection of 15 image representation that we combine in the RBF kernel that it's.",
                    "label": 0
                },
                {
                    "sent": "One of my first slides.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We we could we compute Harris interest points and also sample points on the dense grid.",
                    "label": 0
                },
                {
                    "sent": "Then we describe them with C features for texture and Hue features for color weaken ties them to obtain biographer representation for those four type of descriptors.",
                    "label": 0
                },
                {
                    "sent": "So that's already 4 representation.",
                    "label": 0
                },
                {
                    "sent": "And here we have color Instagram on precor spaces and we use 16 bits per channel.",
                    "label": 0
                },
                {
                    "sent": "We use a dish just descriptor and for each of the histogram we also consider binding them in different regions and that's in total 15 image representation that we normalize an average.",
                    "label": 0
                },
                {
                    "sent": "To put that in their RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "Thank you, are there more questions?",
                    "label": 0
                },
                {
                    "sent": "So the question is, do we explicitly use the correlation between the tags to improve classification?",
                    "label": 0
                },
                {
                    "sent": "So the question that the answer is no, we just used the.",
                    "label": 0
                },
                {
                    "sent": "So here we still consider.",
                    "label": 0
                },
                {
                    "sent": "The other tags as additional features, so the SVM will or the combined classifier will use it implicitly.",
                    "label": 0
                },
                {
                    "sent": "I mean, if he if he finds that there is interesting feature to use that are different than the tags were considering, then he will use it.",
                    "label": 0
                },
                {
                    "sent": "Through another question, very quick one.",
                    "label": 0
                },
                {
                    "sent": "I have a question regarding so experimental protocol, especially on Pascal Pascal.",
                    "label": 0
                },
                {
                    "sent": "Data set was collected by using text queries, you know on Flickr, which means that the classes you're interested in, you know there is a guarantee that at least one of the tag is informative of the classes.",
                    "label": 0
                },
                {
                    "sent": "Which is not the case in general, you know so.",
                    "label": 0
                },
                {
                    "sent": "Isn't it guaranteed that actually the text you know will always, you know, provide you with information and Pascal?",
                    "label": 0
                },
                {
                    "sent": "Or is it the best data set you know to use to?",
                    "label": 0
                },
                {
                    "sent": "Do you know image plus text?",
                    "label": 0
                },
                {
                    "sent": "You know merging?",
                    "label": 0
                },
                {
                    "sent": "OK, good question, so indeed well, it's well known that the Pascal data set has some strong bias, so even in the visual appearance of the images, but also in the tags.",
                    "label": 0
                },
                {
                    "sent": "That's why we also had this additional data set of near Flickr.",
                    "label": 0
                },
                {
                    "sent": "And since the observation and their experimental results are really, really similar.",
                    "label": 0
                },
                {
                    "sent": "It kind of shows that while this has a little influence on on our method.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Let's think this is speaker once again.",
                    "label": 0
                }
            ]
        }
    }
}