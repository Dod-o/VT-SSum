{
    "id": "vxkxwwrtxlpwx6chyhyvsp5ugl4mdhel",
    "title": "Learning Deep Boltzmann Machines",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_salakhutdinov_ldbm/",
    "segmentation": [
        [
            "See.",
            "Yes."
        ],
        [
            "So the outline of the talk I'm going to talk about both machines, so you can think of them as Markov random fields.",
            "Then I'm going to talk about learning in these deep Boltzmann machines.",
            "In particular, I'm going to talk about variational inference and MCMC, and I'm going to try to convince you that it's it's very essential to have both of these approximations in order to learn these graphical models.",
            "I'm.",
            "And then I'm going to introduce deep Boltzmann machines.",
            "What they are?",
            "I also will introduce how we can evaluate these models and that would require us to estimate partition function.",
            "The problem that a lot of folks in our community are working on, and then I'm going to show you some results."
        ],
        [
            "So.",
            "Boltzmann machines well, Boltzmann machines are just Markov random fields with hidden variables, and the question is how we can do learning in these Boltzmann machines.",
            "Here we have.",
            "We have a joint distribution over the states of visible units and visible units would be image pixels in your image for example, or words in your document.",
            "Something that you observe and we're going to have a stochastic hidden units age.",
            "This could be viewed as feature detectors now.",
            "Throughout this talk, I will primarily concentrate on binary case where we have binary visible binary he needs, and later on I'll say a few words about hybrid models where we have Gaussian or POS and visible units.",
            "And of course, inference as well as the maximum likelihood learning in these models is really hard.",
            "So this talk that the main objective of this talk is to show you how we can do parameter learning in these.",
            "Boltzmann machines OK."
        ],
        [
            "Now 1 interesting subclass of models is called restricted.",
            "Both machines has that particular form.",
            "It has a bipartite form and people like these types of models because I think of them as the simplest possible graphical models where you have within variables and the reason why is because conditional and states of the visible units, you can immediately infer the states of the hidden variables.",
            "So it's almost like you have a full observed Markov random field.",
            "Now of course, again, maximum likelihood learning in this graphical model is is hard, but at least inferring the states of the latent variables is easy.",
            "So these types of models have been successfully used in information retrieval applications as well as in collaborative filtering applications and model image patches.",
            "And such, but let's."
        ],
        [
            "Get back to the Boltzmann machines.",
            "Where we have connections between hidden variables as well as hidden to visible variables as well as the visible variables.",
            "OK, now if we look at the maximum likelihood learning and I sort of alluded it in the morning dog.",
            "The maximum likelihood learning takes that particular form.",
            "It's a different difference between two expectations.",
            "It's a expectation of the sufficient statistics driven by the data distribution and expectation of sufficient statistics driven by the model distribution.",
            "In general, in any type of undirected graphical model.",
            "You will always have.",
            "That form you're trying to match data dependent expectations with the model dependent expectations.",
            "Now the problem here is that first of all we can't really compute that expectation because we can compute the conditional distribution over the hidden variables given the states of the visible variables.",
            "Because it's exponential in the number of hidden variables, the second is that we can't really compute this expectation either.",
            "So, and this model was developed probably 20 years ago, and Terry Sonoski was in Jeff Hinton were the ones who proposed this and the initial idea was, well, we have the difference of two expectations when we just approximate them by Markov chain Monte Carlo.",
            "And you know you run a separate Markov chain for every single data point.",
            "To actually get samples from this conditional distribution so it to estimate this expectation and then you have to run an additional chain to estimate the second expectation.",
            "And that approach was proposed 20 years ago, but it doesn't really work just because you have to run so many changes, the variance of the estimators are bad and you can't learn anything meaningful."
        ],
        [
            "So.",
            "The key idea is actually very, very simple.",
            "The key idea is the following.",
            "Well, why don't we run variational inference to approximate the data driven expectation?",
            "And I'm going to say what that variational inference is.",
            "In particular, we're going to be using mean field approximation, and we're going to run what's called persistent Markov chain Monte Carlo to estimate the second expectation.",
            "I'm."
        ],
        [
            "One thing I want to point out here is that estimating the second expectation is far, far more difficult than estimating the first expectation.",
            "And the reason is the following.",
            "Imagine you're trying to model the distribution of images.",
            "Then if I condition on the states of if I condition my pixels in the image, then I expect my posterior distribution over the hidden variables to be sort of unique model.",
            "Maybe a few modes, because there might be few alternative explanations as to what's going on in the data.",
            "Where is estimating this expectation is very difficult because you have to be able to visit exponentially many modes you have since you're trying to model, for example, distribution of images.",
            "So dealing with that expectation is far more difficult than dealing with that expectation.",
            "So let's let's concentrate on trying to approximate the second expectation.",
            "There is a nice."
        ],
        [
            "This class of algorithms called stochastic approximation algorithms and in the last three years people discovered that they work surprisingly well for learning in Markov random fields.",
            "The algorithm itself was developed in 8687 by Lauren Jonas.",
            "And you know, people didn't really, I guess didn't really have computers to try this.",
            "This stochastic approximation algorithm.",
            "And then there was contrastive divergent's, but.",
            "This algorithm works remarkably well.",
            "What's the intuition?",
            "Well, suppose we have a fully connected.",
            "Binary pairwise Markov random field.",
            "So we have that expression for the probability.",
            "Well, what we're going to do is we're going to do something very simple.",
            "We're going to update the states of these variables.",
            "Using for example Gibbs sampler or any type of transition operator, that leaves P, Theta, septien, variant, and then we're going to update our parameters by replacing the expectation with respect to the model distribution by just the point estimate.",
            "OK, and and we're going to keep going.",
            "So we're going to replace our expectation with the point estimate.",
            "Run now Gibbs sampler.",
            "Estimate this expectation by the point estimate and keep going.",
            "So why why?"
        ],
        [
            "This algorithm would be working.",
            "Well, the the justification is actually very very simple.",
            "It says the following.",
            "It says well if my learning rate on these parameters is sufficiently small compared to the mixing rate of the Markov chain, then my samples will always stay at the stationary distribution.",
            "OK, this is sort of asymptotic argument, and you can actually prove almost sure convergence, that this procedure asymptotically will converge to the maximum likelihood solution.",
            "And again here I just point out the intuition here is that if I actually set my learning rate to zero, in other words, I don't update these parameters, then all I'm doing is.",
            "I'm just running a Markov chain, and which asymptotically will get to the stationary distribution.",
            "People who are familiar with contrastive divergent learning.",
            "This is very similar to contrastive divergent learning, except for in contrastive divergent learning.",
            "You start at the data you run, a few MCMC samples.",
            "You update your parameters, you go back to the data.",
            "You do a few MCMC steps and update the parameters.",
            "This procedure basically doesn't go back to the data is just keep keeps going.",
            "So it's a it's a controlled Markov chain that that keeps running, and then you changing the parameters and it still keeps running."
        ],
        [
            "So.",
            "You can.",
            "You can also sort of see the proof voice imputing convergence for these types of algorithm basically relies on this very very simple decomposition.",
            "It says well if I look at the log, likelihood or derivative of the log likelihood my parameters, then I can write the update rule in this form.",
            "It's the true gradient plus the difference between two terms.",
            "One term is the true expectation, the second term.",
            "Is your estimate of that expectation?",
            "And now the proof goes along the lines by saying, well, if your learning rate is sufficiently small, this noise term will not be dominated by.",
            "By this term or this ordinary differential equation.",
            "OK, now again you can look at this formulation.",
            "You could say, well, this thing here is actually not an unbiased estimate of this expectation, right?",
            "And it's true.",
            "In fact, this thing here.",
            "Could give you garbage.",
            "And so.",
            "But in practice it works.",
            "It works surprisingly well.",
            "So.",
            "Let me now go back to."
        ],
        [
            "Estimating the first expectation well to actually estimate the data driven expectation, we can use variational inference.",
            "And in variational inference we can effectively use Jensen's inequality and say, well, the log probability of the data is lower bounded by this expression here, which is expected complete log likelihood with respect to some approximate distribution Q plus the entropy of this distribution Q and then become the bound becomes tight if and only if.",
            "Q is is actually out true probability conditional probability.",
            "And in in in our work we're using mean field approximation, but.",
            "There are the approximations you could use.",
            "So."
        ],
        [
            "What what would what would the learning look like?",
            "Well, the learning for both machines would look as follows for each iteration of learning you're doing variational inference.",
            "You fix the parameter Theta.",
            "You maximize this lower bound with respect to your variational parameters Q.",
            "That gives you data dependent term.",
            "Data in expectation and in the second step you apply stochastic approximation algorithm to effectively update.",
            "To effectively compute the gradients of your partition function.",
            "So it's a very in fact.",
            "It's a very very simple procedure and we can train very very large scale models in few hours.",
            "So one question is, does it actually work?"
        ],
        [
            "Well, here's some.",
            "Boltzmann machine fully connected.",
            "Boltzmann machine trained on 74 visible units, the Xemnas digits and 510 units.",
            "It has about 800,000 parameters.",
            "And what I'm showing to you are the samples generated by running a Gibbs sampler for one 1000 steps from that model.",
            "So the question is, on one image, I'm showing you the actual training data and in the second image I'm showing you samples generated by the model.",
            "So can you tell me which one is which?",
            "That would be the Turing test.",
            "Yes.",
            "This would be generated by by the model, be sure.",
            "Yeah, well.",
            "So yes, that's correct.",
            "This is actually generated by the model, right?",
            "And if you look at them, you can see well this three kind of looks funny people don't don't.",
            "You would expect it to see in the training data right?",
            "Or this 5 looks funny even though if you look at this zero that can get zero confuses a lot of people.",
            "So nevertheless we can see that the model is able to learn fairly good generative model of MNIST digits."
        ],
        [
            "But one thing is that you know, we really were really interested in just learning flat.",
            "Both machines where everything is connected to everything.",
            "This is just a generalization too deep Boltzmann machines did.",
            "Both machines are special cases of both machines where I restrict connectivity between hidden variables.",
            "So I have sets of hidden variables an only connections between layers of latent variables and no connections between the hidden variables and the idea here is that, well, this is just a Markov random field with hidden variables.",
            "This is not like a deep belief network where you have hybrid sigmoid belief network and restricted Boltzmann machine.",
            "It you can also well, hopefully it will.",
            "You will be able to extract complex representations and I will show you that in fact you do.",
            "There is a there is a fast greedy initialization trick such that you can pre train this model reasonably well.",
            "One key thing that I want to emphasize here is that learning in this Boltzmann machines do not doesn't require you any labeled data, so you can really take large.",
            "Datasets and try to model the distribution over the visible units and the labeled data can be only used to just adjust your model parameters for specific tasks like classification task or information retrieval task and such.",
            "Um?"
        ],
        [
            "So here's a here's a deep Boltzmann machine, and this is, again, this is the distribution of images that it models.",
            "It has a bright .9 million parameters.",
            "An if you look at the discriminative fine tuning we get a test area of about .95% an at the time when we published this result.",
            "This was.",
            "You know one of the best results on this data set, much better than deep belief networks would get as well as support vector machine is just standard backpropagation.",
            "And then this this notion of tuning the model by by running variational inference.",
            "MCMC really helps.",
            "To both one good generative models as well as do well for for classification tasks.",
            "So the variational inference comes into the training is that if I give you this image.",
            "You have to be able to figure out what the states of those variables are, right?",
            "So this is like you can think of it as an E step.",
            "In the step, you have to figure out what the states of latent variables are, so that's where the variational inference comes in, and the MCMC just basically tries to sample from the entire model.",
            "And you can try to match the two statistics.",
            "Yes, yes.",
            "So there is a greedy pre training so you can sort of initialize parameters to reasonable values.",
            "But then you're training the whole model at once and this is in contrast to deep belief networks where people don't usually train the entire system at once.",
            "I hear it's much more efficient to train the entire system at once."
        ],
        [
            "So 11 interesting question that people are asking, is that well?",
            "What're this second level unions actually doing?",
            "Are they doing anything meaningful?",
            "And what I'm showing you here are the filters that you learn at the first layer of your model, and these are sort of filters that you learn at the second layer model so you can see that the second layer hidden unions actually.",
            "Capture more global structure in the images like that unit for example might be a very good unit for discriminating force and 9th or something like that.",
            "It's a way of combining these little primitive features into something more global.",
            "And and this is just a very ad hoc way of illustrating what the second layer hidden units are doing, and people are discovering that top level unions are trying to try to get some more complex structures to what's going on in the data.",
            "And we believe this is one of the reasons why these models do so well in discriminative tasks like objects."
        ],
        [
            "Mission.",
            "So one other thing.",
            "Is is how we can evaluate these models and you know people in deep belief.",
            "Net community typically evaluate these models discriminatively how well you can do well in terms of prediction.",
            "But what if I want to compare the Boltzmann machines to something like mixture of Bernoulli's?",
            "OK. Another probabilistic model.",
            "Well, if I if I want to compute probability of the test inputs, I actually need to compute the partition function.",
            "I need an estimate of the partition function.",
            "And you know, by looking at these samples you could say, well, these samples look better than these samples, But the problem is if only generating samples from the model is I can define a fantastic generative model and my generative model would be pick a training image and display it to you.",
            "Right, I could generate fantastic images, but that procedure I mean that model would be meaningless.",
            "So we need an estimate of the partition function.",
            "There's been a lot of work trying to estimate this partition function both in the MC community as well as people in variational communities where they're trying to upper bound the log partition function in C."
        ],
        [
            "So what you can do here is we found that methods based on sequential Monte Carlo, while kneeling style methods, works surprisingly well in terms of estimating partition function and under Defreitas was talking about these types of models in the tutorial and the intuition here is very simple.",
            "You saying that?",
            "Well, why don't I define a new distribution parameterized by parameter beta, which is a geometric average of two distributions, one pie, which is a simple distribution, something like a uniform distribution.",
            "Ann, this is your distribution, your complex distribution.",
            "So when beta is zero, you effectively starting with some simple distribution.",
            "And when beta is 1, you get to your original distribution and that allows you to estimate the partition function.",
            "So here I'm showing you 16 runs off and you'll important sampling.",
            "You start with something very simple and as this inverse temperature parameter goes from zero to one, you start seeing.",
            "Samples from your model.",
            "And this is this is related to annealing or tempering style methods.",
            "OK."
        ],
        [
            "So once we have an estimate of the partition function, which effectively gives you an approximation over this exponential sum, we can use this estimate to actually compute the lower bound on the log probability of the data.",
            "So we can use variational inference Q as well as our estimate of the log partition function to get a lower bound and one Question 1 interesting questions in my view is that are there any way other ways of estimating partition functions reliably?",
            "No, this is this is this is a real lower bound.",
            "The problem is that this is only gives you an estimate and so it's just.",
            "It's an estimate of the lower bound, so it can go either way.",
            "Mike so here's here's some numbers."
        ],
        [
            "If you look at the mixture of Bernoulli, something very simple, you get 137 knots per digit, and if you look at the both machines you get something like 84 nuts to five knots per digit.",
            "So the difference is actually very large and it's it was pleasing for us at least to see that these D balsam machines are actually doing better than just plain mixture models.",
            "Which people will say well but nevertheless trying to quantify the numbers is, I think, is important."
        ],
        [
            "Compare this to this."
        ],
        [
            "The mixture for ICAS, something that happened it and they did a lot better on the image patches, right?",
            "Yeah, that was my work, so we actually compared some of these models and like I see a type of models and in some cases I see a type of models were actually beating deep belief networks.",
            "Divorce machine so you know it's a good.",
            "It's a good strategy to have because sometimes you can build these complicated models, but they may actually not do much better than factor analysis and it's nice to quantify the numbers and see.",
            "Where they fail.",
            "Now, so far I've been talking only about binary cases, but you know."
        ],
        [
            "Can also define Gaussian Bernoulli's restricted Boltzmann machines where these visible units are Gaussian units so they can model real value pixel values.",
            "For example in images and you hidden variables are are binary variables and Max welling and Jeff.",
            "They had a paper extending this type of models to multinomial, Poisson, exponential or any kind of.",
            "Exponential family model.",
            "So you can learn in these mixed.",
            "Mixed hybrid models.",
            "11"
        ],
        [
            "Trysting thing that I would like to show you is well, what if we actually move to something a little bit more challenging than amnist and that would be images of North data.",
            "So these are datasets of animals, soldiers, planes, cars and trucks, cars and trucks.",
            "There are five object categories.",
            "There's five different objects within each category.",
            "These are different viewpoints, different light lighting, lighting, lighting conditions.",
            "So these are actually fairly big images.",
            "These are 96 by 96 images, so they have fairly high resolution images."
        ],
        [
            "This is a D Boston machine that we trained.",
            "The input is a stereo pair of two images and we've trained the debounce machine with 4004 thousand 4000 Kenyans.",
            "That was a three ladybugs machine.",
            "It has about 68 million parameters, so it's a huge market random field.",
            "And here's what it does.",
            "These are."
        ],
        [
            "Is it raining samples?",
            "And these are samples generated by the model, so you can see the model is able to capture a lot of structure in the images or a lot of interesting configurations in the images.",
            "Of course the model is not perfect because it's a bit disbalance, it tends to generate these people with guns more often.",
            "And that's just that's just the.",
            "That's just a failure of Markov chain Monte Carlo being able to visit different modes, so I think we need we need better algorithms for learning these models.",
            "One pleasing thing about this model is that if you actually try to discriminative fine tune it, you get a test error of about 7.2% and this is compared to support vector machines and logistic regression.",
            "So you see that these models really allow you to extract latent representations that carry meaningful information about the inputs.",
            "Um?"
        ],
        [
            "And one other fun thing you can do is you can do image completion, something that discriminative models can do.",
            "So what I'm showing you here is I'm showing you images test images.",
            "And for this data set, the images are not actually part of the training images.",
            "So for example in the training datasets you don't see this cowboy.",
            "OK, now if I black part of the image and try to infer the second part, these are the results that that you see, so you know it's able to figure out that there should be a lag in an arm for this guy, or that if you give it like a plane like this, it figures out that there should be another wing.",
            "Or or the interesting.",
            "This was an interesting example.",
            "This is an elk and there is no elk in the training data.",
            "There is no figure like that in the training data, but then in training data we have something that looks like a horse.",
            "And so if I give it.",
            "Just half of the stuff, basically.",
            "Fills in the head of an animal, so it really captures.",
            "This structure of these images.",
            "Um?"
        ],
        [
            "So finally one other thing I wanted to show you is you can actually not necessarily just apply to images, but you can also do dimensionality reduction.",
            "So I was I was showing you these models where I have large number of hidden variables, but in fact you can train these deep deep models where you actually go from, let's say 2000 dimensional space down to two dimensional space and you can view it as a form of dimensionality reduction.",
            "In this case this model was trained on.",
            "800,000.",
            "Documents from Reuters data set and we used a simple bag of words representation and we took 2000 most frequently used words in the datasets and you can see it's able to find this very rich structure in the data and that was done completely unsupervised way.",
            "There were no labels provided to the algorithm here.",
            "You can see the two dimensional embedding of of of.",
            "Of the topics that the both machine finds and here you can see what latent semantic analysis would do.",
            "So this is a standard procedure for semantic analysis, something that people use for information retrieval.",
            "And you know it's sort of it finds reasonable topics.",
            "It lays them out in a reasonable space.",
            "In particular, it puts European Community monitoring."
        ],
        [
            "Yes.",
            "This forecasting learning?",
            "So you.",
            "Is it?",
            "Estimate a Max.",
            "So I guess I should point out that here."
        ],
        [
            "I should point out, maybe I forgot to point out that in practice, what we actually do is we run several of these chains in parallel.",
            "So typically we run hundred chains in parallel and what you get is you take an empirical average over the samples generated by each one of those chains.",
            "So just reduce variance a little bit.",
            "One step, yeah, so exactly so.",
            "These parallel chains, what they do is imagine you have 100 chains.",
            "You get samples, you update the parameters, get samples, you have data parameters, and the samples are averaged over these chains.",
            "Now the convergence proof of the algorithm doesn't require you to run multiple chains.",
            "You can do it with one chain.",
            "Of course.",
            "The problem is that as your distribution particularly trying to model something very interesting becomes very very highly multi model.",
            "It's harder for the.",
            "Samples to move around between different modes.",
            "Really small to make sure you still send me from the model.",
            "Is that so?",
            "The conditions on the learning rate is something like 1 / T. What time if you look at the proofs, the learning rates to actually guarantee that you will converge to the maximum likelihood solution, ridiculously small.",
            "And in practice it would take you years to actually learn something like that.",
            "What happens in practice is is is people.",
            "Notice that even with large learning rates, you tend to learn reasonable models and the question is why?",
            "Why is this happening in a non aseptic case?",
            "There's been some work coming out of several groups.",
            "Particularly in statistical physics where they where they show that that may actually relate to the class of adaptive Markov chain Monte Carlo algorithms.",
            "Where?",
            "It's it's this delicate interplay between inference and learning.",
            "Because what's happening here is that.",
            "During the inference step you you're trying to visit these modes in during the learning State you update your model parameters.",
            "But what happens with stochastic approximation algorithm is that.",
            "You're you're learning, tries to encourage mixing, so in some sense you're learning actually helps you to do inference and.",
            "People propose something like herding in in dynamical systems by Mac swelling, and he's using this trick by by sort of using this idea that learning helps inference you know if you set this learning rate to be very large, then what you observe is your samples here.",
            "They jump around the space like crazy.",
            "You changing your energy landscape.",
            "In in, in these big steps and your samples walk around very fast and then helps learning.",
            "But of course to in order to get the very stable model, you still have to.",
            "Decrease the learning rate and then.",
            "Yeah.",
            "The way to get around the small learning rate is to have a large learning rate, but then having strangers at the end, you know if this procedure will converge.",
            "So say that again, have a large learning rate and then.",
            "Yeah, so so.",
            "So there's been some theoretical results showing that what you should actually do is toward the end of learning average over.",
            "Several several parameters or or average over certain window and that produces consistent.",
            "You can prove consistency in terms of convergence of this algorithm.",
            "Yes, in practice, people notice that that works fairly well as well, and there are some papers in statistics communities that.",
            "Don't have that yet.",
            "Like or.",
            "Yeah, the no, I haven't, but I think that's a good idea.",
            "One of the things you could try to do is you could try to say, well, can we actually use these samples to give us an approximate Hessian or some some 2nd order information?",
            "Because you have these samples and such and?",
            "And I think there is some work that that's attempting to do that, but I haven't.",
            "I haven't tried, but I think this is something something too.",
            "It's just the one thing that's.",
            "Sort of pushes back is these estimates are not really unbiased estimates of these expectations, so they are very noisy.",
            "So then you can get sort of the noisy 2nd order estimates, and I don't know whether that will help or hurt.",
            "Talk about more later, OK?",
            "In this case.",
            "No, no, but I think this is something to to try, yes?",
            "Yeah.",
            "Aren't these really expensive?",
            "But if you have 2000 hidden units, you have to sample each of them separately.",
            "Yeah, so that's that's sort of the trick."
        ],
        [
            "We're using with these models with these deep models is we don't have any connections between the hidden variables and that's precisely just to encourage the mixing because you know you can update all of these units in parallel.",
            "Conditional states of these units in the states of these units.",
            "But yeah.",
            "Gibbs sampling becomes particularly having these big models.",
            "Efficiency becomes becomes an issue, yes.",
            "Yes, how long?",
            "So this.",
            "Right, so this model took us."
        ],
        [
            "I think a week that rain.",
            "What was the computer?",
            "Oh, I think it was fairly fast computer.",
            "I think something like 3 point something giga Hertz or.",
            "Yeah, it's just a single computer didn't paralyze, so didn't do GPU's or anything like that even though you in principle could.",
            "I just let it run.",
            "Yeah.",
            "In your opinion, random people and are versus science.",
            "Yeah, that's it.",
            "That's a very good question.",
            "I think it's I think it's to be honest.",
            "I think it's it's a bit of science.",
            "So I mean, it's signs, but it's also art.",
            "You know one of the problems with the models that we have right now is, you know we're doing these pre training algorithms and we're using contrastive divergent.",
            "So if you start using Markov chain Monte Carlo algorithms then not as robust as you would have hoped they would be.",
            "You know you sort of have to watch out for the learning rates.",
            "In particular, the learning rates for these.",
            "For these stochastic approximation algorithms.",
            "Particularly these learning."
        ],
        [
            "It's a fairly important.",
            "So I would say that we are getting there.",
            "A lot of people are now beginning to use these models in see how they do, but we have very far from actually making them.",
            "Very reliable and very robust to just model all kinds of distributions, that's.",
            "And you'd expect that because.",
            "Increasing the number.",
            "So sort of maybe some kind of nonparametric.",
            "Something that can adapt the number of hidden units?",
            "No, I don't think there is.",
            "There is any work doing that.",
            "What people tend to do in practice they tend to use cross validation such as you train multiple systems and then you're trying to just pick one out of those.",
            "The problem with using some kind of nonparametric type of models is it's very hard to do model selection because of the partition function.",
            "So if you add more hidden units that changes the global partition function of your model.",
            "It's like trying to learn Markov random fields where the number of.",
            "Variables grows or shrinks, it's.",
            "Um?",
            "It's a bit hard, but.",
            "There's been some work, and Martin Zoomer has a work on Basean restricted Boltzmann machines where he actually tries to compute the posterior distribution over the parameters and that might give him some indication as to which parameters should be in the model or not.",
            "And Tom in kinnelon.",
            "Allen Key they also had basean conditional random fields, sort of.",
            "OK, so.",
            "We're going to return at 9:10 for a regularly scheduled program with Ericsson.",
            "So let's thank God."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of the talk I'm going to talk about both machines, so you can think of them as Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about learning in these deep Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm going to talk about variational inference and MCMC, and I'm going to try to convince you that it's it's very essential to have both of these approximations in order to learn these graphical models.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to introduce deep Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "What they are?",
                    "label": 0
                },
                {
                    "sent": "I also will introduce how we can evaluate these models and that would require us to estimate partition function.",
                    "label": 0
                },
                {
                    "sent": "The problem that a lot of folks in our community are working on, and then I'm going to show you some results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann machines well, Boltzmann machines are just Markov random fields with hidden variables, and the question is how we can do learning in these Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "We have a joint distribution over the states of visible units and visible units would be image pixels in your image for example, or words in your document.",
                    "label": 0
                },
                {
                    "sent": "Something that you observe and we're going to have a stochastic hidden units age.",
                    "label": 0
                },
                {
                    "sent": "This could be viewed as feature detectors now.",
                    "label": 1
                },
                {
                    "sent": "Throughout this talk, I will primarily concentrate on binary case where we have binary visible binary he needs, and later on I'll say a few words about hybrid models where we have Gaussian or POS and visible units.",
                    "label": 0
                },
                {
                    "sent": "And of course, inference as well as the maximum likelihood learning in these models is really hard.",
                    "label": 1
                },
                {
                    "sent": "So this talk that the main objective of this talk is to show you how we can do parameter learning in these.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann machines OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now 1 interesting subclass of models is called restricted.",
                    "label": 0
                },
                {
                    "sent": "Both machines has that particular form.",
                    "label": 0
                },
                {
                    "sent": "It has a bipartite form and people like these types of models because I think of them as the simplest possible graphical models where you have within variables and the reason why is because conditional and states of the visible units, you can immediately infer the states of the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like you have a full observed Markov random field.",
                    "label": 0
                },
                {
                    "sent": "Now of course, again, maximum likelihood learning in this graphical model is is hard, but at least inferring the states of the latent variables is easy.",
                    "label": 1
                },
                {
                    "sent": "So these types of models have been successfully used in information retrieval applications as well as in collaborative filtering applications and model image patches.",
                    "label": 0
                },
                {
                    "sent": "And such, but let's.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get back to the Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "Where we have connections between hidden variables as well as hidden to visible variables as well as the visible variables.",
                    "label": 0
                },
                {
                    "sent": "OK, now if we look at the maximum likelihood learning and I sort of alluded it in the morning dog.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood learning takes that particular form.",
                    "label": 0
                },
                {
                    "sent": "It's a different difference between two expectations.",
                    "label": 0
                },
                {
                    "sent": "It's a expectation of the sufficient statistics driven by the data distribution and expectation of sufficient statistics driven by the model distribution.",
                    "label": 0
                },
                {
                    "sent": "In general, in any type of undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "You will always have.",
                    "label": 0
                },
                {
                    "sent": "That form you're trying to match data dependent expectations with the model dependent expectations.",
                    "label": 0
                },
                {
                    "sent": "Now the problem here is that first of all we can't really compute that expectation because we can compute the conditional distribution over the hidden variables given the states of the visible variables.",
                    "label": 0
                },
                {
                    "sent": "Because it's exponential in the number of hidden variables, the second is that we can't really compute this expectation either.",
                    "label": 0
                },
                {
                    "sent": "So, and this model was developed probably 20 years ago, and Terry Sonoski was in Jeff Hinton were the ones who proposed this and the initial idea was, well, we have the difference of two expectations when we just approximate them by Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And you know you run a separate Markov chain for every single data point.",
                    "label": 1
                },
                {
                    "sent": "To actually get samples from this conditional distribution so it to estimate this expectation and then you have to run an additional chain to estimate the second expectation.",
                    "label": 0
                },
                {
                    "sent": "And that approach was proposed 20 years ago, but it doesn't really work just because you have to run so many changes, the variance of the estimators are bad and you can't learn anything meaningful.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The key idea is actually very, very simple.",
                    "label": 0
                },
                {
                    "sent": "The key idea is the following.",
                    "label": 1
                },
                {
                    "sent": "Well, why don't we run variational inference to approximate the data driven expectation?",
                    "label": 1
                },
                {
                    "sent": "And I'm going to say what that variational inference is.",
                    "label": 0
                },
                {
                    "sent": "In particular, we're going to be using mean field approximation, and we're going to run what's called persistent Markov chain Monte Carlo to estimate the second expectation.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing I want to point out here is that estimating the second expectation is far, far more difficult than estimating the first expectation.",
                    "label": 0
                },
                {
                    "sent": "And the reason is the following.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're trying to model the distribution of images.",
                    "label": 0
                },
                {
                    "sent": "Then if I condition on the states of if I condition my pixels in the image, then I expect my posterior distribution over the hidden variables to be sort of unique model.",
                    "label": 0
                },
                {
                    "sent": "Maybe a few modes, because there might be few alternative explanations as to what's going on in the data.",
                    "label": 0
                },
                {
                    "sent": "Where is estimating this expectation is very difficult because you have to be able to visit exponentially many modes you have since you're trying to model, for example, distribution of images.",
                    "label": 0
                },
                {
                    "sent": "So dealing with that expectation is far more difficult than dealing with that expectation.",
                    "label": 0
                },
                {
                    "sent": "So let's let's concentrate on trying to approximate the second expectation.",
                    "label": 0
                },
                {
                    "sent": "There is a nice.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This class of algorithms called stochastic approximation algorithms and in the last three years people discovered that they work surprisingly well for learning in Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "The algorithm itself was developed in 8687 by Lauren Jonas.",
                    "label": 0
                },
                {
                    "sent": "And you know, people didn't really, I guess didn't really have computers to try this.",
                    "label": 0
                },
                {
                    "sent": "This stochastic approximation algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then there was contrastive divergent's, but.",
                    "label": 0
                },
                {
                    "sent": "This algorithm works remarkably well.",
                    "label": 0
                },
                {
                    "sent": "What's the intuition?",
                    "label": 0
                },
                {
                    "sent": "Well, suppose we have a fully connected.",
                    "label": 0
                },
                {
                    "sent": "Binary pairwise Markov random field.",
                    "label": 0
                },
                {
                    "sent": "So we have that expression for the probability.",
                    "label": 0
                },
                {
                    "sent": "Well, what we're going to do is we're going to do something very simple.",
                    "label": 0
                },
                {
                    "sent": "We're going to update the states of these variables.",
                    "label": 0
                },
                {
                    "sent": "Using for example Gibbs sampler or any type of transition operator, that leaves P, Theta, septien, variant, and then we're going to update our parameters by replacing the expectation with respect to the model distribution by just the point estimate.",
                    "label": 1
                },
                {
                    "sent": "OK, and and we're going to keep going.",
                    "label": 0
                },
                {
                    "sent": "So we're going to replace our expectation with the point estimate.",
                    "label": 0
                },
                {
                    "sent": "Run now Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "Estimate this expectation by the point estimate and keep going.",
                    "label": 0
                },
                {
                    "sent": "So why why?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This algorithm would be working.",
                    "label": 0
                },
                {
                    "sent": "Well, the the justification is actually very very simple.",
                    "label": 0
                },
                {
                    "sent": "It says the following.",
                    "label": 0
                },
                {
                    "sent": "It says well if my learning rate on these parameters is sufficiently small compared to the mixing rate of the Markov chain, then my samples will always stay at the stationary distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, this is sort of asymptotic argument, and you can actually prove almost sure convergence, that this procedure asymptotically will converge to the maximum likelihood solution.",
                    "label": 0
                },
                {
                    "sent": "And again here I just point out the intuition here is that if I actually set my learning rate to zero, in other words, I don't update these parameters, then all I'm doing is.",
                    "label": 0
                },
                {
                    "sent": "I'm just running a Markov chain, and which asymptotically will get to the stationary distribution.",
                    "label": 0
                },
                {
                    "sent": "People who are familiar with contrastive divergent learning.",
                    "label": 0
                },
                {
                    "sent": "This is very similar to contrastive divergent learning, except for in contrastive divergent learning.",
                    "label": 0
                },
                {
                    "sent": "You start at the data you run, a few MCMC samples.",
                    "label": 0
                },
                {
                    "sent": "You update your parameters, you go back to the data.",
                    "label": 1
                },
                {
                    "sent": "You do a few MCMC steps and update the parameters.",
                    "label": 0
                },
                {
                    "sent": "This procedure basically doesn't go back to the data is just keep keeps going.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a controlled Markov chain that that keeps running, and then you changing the parameters and it still keeps running.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can also sort of see the proof voice imputing convergence for these types of algorithm basically relies on this very very simple decomposition.",
                    "label": 0
                },
                {
                    "sent": "It says well if I look at the log, likelihood or derivative of the log likelihood my parameters, then I can write the update rule in this form.",
                    "label": 0
                },
                {
                    "sent": "It's the true gradient plus the difference between two terms.",
                    "label": 0
                },
                {
                    "sent": "One term is the true expectation, the second term.",
                    "label": 0
                },
                {
                    "sent": "Is your estimate of that expectation?",
                    "label": 0
                },
                {
                    "sent": "And now the proof goes along the lines by saying, well, if your learning rate is sufficiently small, this noise term will not be dominated by.",
                    "label": 0
                },
                {
                    "sent": "By this term or this ordinary differential equation.",
                    "label": 0
                },
                {
                    "sent": "OK, now again you can look at this formulation.",
                    "label": 0
                },
                {
                    "sent": "You could say, well, this thing here is actually not an unbiased estimate of this expectation, right?",
                    "label": 0
                },
                {
                    "sent": "And it's true.",
                    "label": 0
                },
                {
                    "sent": "In fact, this thing here.",
                    "label": 0
                },
                {
                    "sent": "Could give you garbage.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "But in practice it works.",
                    "label": 0
                },
                {
                    "sent": "It works surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me now go back to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimating the first expectation well to actually estimate the data driven expectation, we can use variational inference.",
                    "label": 0
                },
                {
                    "sent": "And in variational inference we can effectively use Jensen's inequality and say, well, the log probability of the data is lower bounded by this expression here, which is expected complete log likelihood with respect to some approximate distribution Q plus the entropy of this distribution Q and then become the bound becomes tight if and only if.",
                    "label": 0
                },
                {
                    "sent": "Q is is actually out true probability conditional probability.",
                    "label": 0
                },
                {
                    "sent": "And in in in our work we're using mean field approximation, but.",
                    "label": 0
                },
                {
                    "sent": "There are the approximations you could use.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What what would what would the learning look like?",
                    "label": 0
                },
                {
                    "sent": "Well, the learning for both machines would look as follows for each iteration of learning you're doing variational inference.",
                    "label": 1
                },
                {
                    "sent": "You fix the parameter Theta.",
                    "label": 1
                },
                {
                    "sent": "You maximize this lower bound with respect to your variational parameters Q.",
                    "label": 0
                },
                {
                    "sent": "That gives you data dependent term.",
                    "label": 1
                },
                {
                    "sent": "Data in expectation and in the second step you apply stochastic approximation algorithm to effectively update.",
                    "label": 0
                },
                {
                    "sent": "To effectively compute the gradients of your partition function.",
                    "label": 0
                },
                {
                    "sent": "So it's a very in fact.",
                    "label": 0
                },
                {
                    "sent": "It's a very very simple procedure and we can train very very large scale models in few hours.",
                    "label": 0
                },
                {
                    "sent": "So one question is, does it actually work?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, here's some.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann machine fully connected.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann machine trained on 74 visible units, the Xemnas digits and 510 units.",
                    "label": 0
                },
                {
                    "sent": "It has about 800,000 parameters.",
                    "label": 0
                },
                {
                    "sent": "And what I'm showing to you are the samples generated by running a Gibbs sampler for one 1000 steps from that model.",
                    "label": 1
                },
                {
                    "sent": "So the question is, on one image, I'm showing you the actual training data and in the second image I'm showing you samples generated by the model.",
                    "label": 0
                },
                {
                    "sent": "So can you tell me which one is which?",
                    "label": 0
                },
                {
                    "sent": "That would be the Turing test.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This would be generated by by the model, be sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well.",
                    "label": 0
                },
                {
                    "sent": "So yes, that's correct.",
                    "label": 0
                },
                {
                    "sent": "This is actually generated by the model, right?",
                    "label": 0
                },
                {
                    "sent": "And if you look at them, you can see well this three kind of looks funny people don't don't.",
                    "label": 0
                },
                {
                    "sent": "You would expect it to see in the training data right?",
                    "label": 0
                },
                {
                    "sent": "Or this 5 looks funny even though if you look at this zero that can get zero confuses a lot of people.",
                    "label": 0
                },
                {
                    "sent": "So nevertheless we can see that the model is able to learn fairly good generative model of MNIST digits.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But one thing is that you know, we really were really interested in just learning flat.",
                    "label": 0
                },
                {
                    "sent": "Both machines where everything is connected to everything.",
                    "label": 0
                },
                {
                    "sent": "This is just a generalization too deep Boltzmann machines did.",
                    "label": 1
                },
                {
                    "sent": "Both machines are special cases of both machines where I restrict connectivity between hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So I have sets of hidden variables an only connections between layers of latent variables and no connections between the hidden variables and the idea here is that, well, this is just a Markov random field with hidden variables.",
                    "label": 0
                },
                {
                    "sent": "This is not like a deep belief network where you have hybrid sigmoid belief network and restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "It you can also well, hopefully it will.",
                    "label": 0
                },
                {
                    "sent": "You will be able to extract complex representations and I will show you that in fact you do.",
                    "label": 0
                },
                {
                    "sent": "There is a there is a fast greedy initialization trick such that you can pre train this model reasonably well.",
                    "label": 0
                },
                {
                    "sent": "One key thing that I want to emphasize here is that learning in this Boltzmann machines do not doesn't require you any labeled data, so you can really take large.",
                    "label": 0
                },
                {
                    "sent": "Datasets and try to model the distribution over the visible units and the labeled data can be only used to just adjust your model parameters for specific tasks like classification task or information retrieval task and such.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a here's a deep Boltzmann machine, and this is, again, this is the distribution of images that it models.",
                    "label": 0
                },
                {
                    "sent": "It has a bright .9 million parameters.",
                    "label": 1
                },
                {
                    "sent": "An if you look at the discriminative fine tuning we get a test area of about .95% an at the time when we published this result.",
                    "label": 0
                },
                {
                    "sent": "This was.",
                    "label": 0
                },
                {
                    "sent": "You know one of the best results on this data set, much better than deep belief networks would get as well as support vector machine is just standard backpropagation.",
                    "label": 0
                },
                {
                    "sent": "And then this this notion of tuning the model by by running variational inference.",
                    "label": 0
                },
                {
                    "sent": "MCMC really helps.",
                    "label": 0
                },
                {
                    "sent": "To both one good generative models as well as do well for for classification tasks.",
                    "label": 0
                },
                {
                    "sent": "So the variational inference comes into the training is that if I give you this image.",
                    "label": 0
                },
                {
                    "sent": "You have to be able to figure out what the states of those variables are, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like you can think of it as an E step.",
                    "label": 0
                },
                {
                    "sent": "In the step, you have to figure out what the states of latent variables are, so that's where the variational inference comes in, and the MCMC just basically tries to sample from the entire model.",
                    "label": 0
                },
                {
                    "sent": "And you can try to match the two statistics.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So there is a greedy pre training so you can sort of initialize parameters to reasonable values.",
                    "label": 0
                },
                {
                    "sent": "But then you're training the whole model at once and this is in contrast to deep belief networks where people don't usually train the entire system at once.",
                    "label": 0
                },
                {
                    "sent": "I hear it's much more efficient to train the entire system at once.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 11 interesting question that people are asking, is that well?",
                    "label": 0
                },
                {
                    "sent": "What're this second level unions actually doing?",
                    "label": 0
                },
                {
                    "sent": "Are they doing anything meaningful?",
                    "label": 0
                },
                {
                    "sent": "And what I'm showing you here are the filters that you learn at the first layer of your model, and these are sort of filters that you learn at the second layer model so you can see that the second layer hidden unions actually.",
                    "label": 0
                },
                {
                    "sent": "Capture more global structure in the images like that unit for example might be a very good unit for discriminating force and 9th or something like that.",
                    "label": 0
                },
                {
                    "sent": "It's a way of combining these little primitive features into something more global.",
                    "label": 0
                },
                {
                    "sent": "And and this is just a very ad hoc way of illustrating what the second layer hidden units are doing, and people are discovering that top level unions are trying to try to get some more complex structures to what's going on in the data.",
                    "label": 0
                },
                {
                    "sent": "And we believe this is one of the reasons why these models do so well in discriminative tasks like objects.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "So one other thing.",
                    "label": 0
                },
                {
                    "sent": "Is is how we can evaluate these models and you know people in deep belief.",
                    "label": 0
                },
                {
                    "sent": "Net community typically evaluate these models discriminatively how well you can do well in terms of prediction.",
                    "label": 0
                },
                {
                    "sent": "But what if I want to compare the Boltzmann machines to something like mixture of Bernoulli's?",
                    "label": 1
                },
                {
                    "sent": "OK. Another probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Well, if I if I want to compute probability of the test inputs, I actually need to compute the partition function.",
                    "label": 0
                },
                {
                    "sent": "I need an estimate of the partition function.",
                    "label": 1
                },
                {
                    "sent": "And you know, by looking at these samples you could say, well, these samples look better than these samples, But the problem is if only generating samples from the model is I can define a fantastic generative model and my generative model would be pick a training image and display it to you.",
                    "label": 0
                },
                {
                    "sent": "Right, I could generate fantastic images, but that procedure I mean that model would be meaningless.",
                    "label": 0
                },
                {
                    "sent": "So we need an estimate of the partition function.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work trying to estimate this partition function both in the MC community as well as people in variational communities where they're trying to upper bound the log partition function in C.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you can do here is we found that methods based on sequential Monte Carlo, while kneeling style methods, works surprisingly well in terms of estimating partition function and under Defreitas was talking about these types of models in the tutorial and the intuition here is very simple.",
                    "label": 0
                },
                {
                    "sent": "You saying that?",
                    "label": 0
                },
                {
                    "sent": "Well, why don't I define a new distribution parameterized by parameter beta, which is a geometric average of two distributions, one pie, which is a simple distribution, something like a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is your distribution, your complex distribution.",
                    "label": 0
                },
                {
                    "sent": "So when beta is zero, you effectively starting with some simple distribution.",
                    "label": 0
                },
                {
                    "sent": "And when beta is 1, you get to your original distribution and that allows you to estimate the partition function.",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing you 16 runs off and you'll important sampling.",
                    "label": 0
                },
                {
                    "sent": "You start with something very simple and as this inverse temperature parameter goes from zero to one, you start seeing.",
                    "label": 0
                },
                {
                    "sent": "Samples from your model.",
                    "label": 0
                },
                {
                    "sent": "And this is this is related to annealing or tempering style methods.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have an estimate of the partition function, which effectively gives you an approximation over this exponential sum, we can use this estimate to actually compute the lower bound on the log probability of the data.",
                    "label": 1
                },
                {
                    "sent": "So we can use variational inference Q as well as our estimate of the log partition function to get a lower bound and one Question 1 interesting questions in my view is that are there any way other ways of estimating partition functions reliably?",
                    "label": 1
                },
                {
                    "sent": "No, this is this is this is a real lower bound.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this is only gives you an estimate and so it's just.",
                    "label": 0
                },
                {
                    "sent": "It's an estimate of the lower bound, so it can go either way.",
                    "label": 0
                },
                {
                    "sent": "Mike so here's here's some numbers.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at the mixture of Bernoulli, something very simple, you get 137 knots per digit, and if you look at the both machines you get something like 84 nuts to five knots per digit.",
                    "label": 1
                },
                {
                    "sent": "So the difference is actually very large and it's it was pleasing for us at least to see that these D balsam machines are actually doing better than just plain mixture models.",
                    "label": 0
                },
                {
                    "sent": "Which people will say well but nevertheless trying to quantify the numbers is, I think, is important.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare this to this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mixture for ICAS, something that happened it and they did a lot better on the image patches, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that was my work, so we actually compared some of these models and like I see a type of models and in some cases I see a type of models were actually beating deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "Divorce machine so you know it's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good strategy to have because sometimes you can build these complicated models, but they may actually not do much better than factor analysis and it's nice to quantify the numbers and see.",
                    "label": 0
                },
                {
                    "sent": "Where they fail.",
                    "label": 0
                },
                {
                    "sent": "Now, so far I've been talking only about binary cases, but you know.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can also define Gaussian Bernoulli's restricted Boltzmann machines where these visible units are Gaussian units so they can model real value pixel values.",
                    "label": 0
                },
                {
                    "sent": "For example in images and you hidden variables are are binary variables and Max welling and Jeff.",
                    "label": 0
                },
                {
                    "sent": "They had a paper extending this type of models to multinomial, Poisson, exponential or any kind of.",
                    "label": 1
                },
                {
                    "sent": "Exponential family model.",
                    "label": 0
                },
                {
                    "sent": "So you can learn in these mixed.",
                    "label": 0
                },
                {
                    "sent": "Mixed hybrid models.",
                    "label": 0
                },
                {
                    "sent": "11",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trysting thing that I would like to show you is well, what if we actually move to something a little bit more challenging than amnist and that would be images of North data.",
                    "label": 0
                },
                {
                    "sent": "So these are datasets of animals, soldiers, planes, cars and trucks, cars and trucks.",
                    "label": 0
                },
                {
                    "sent": "There are five object categories.",
                    "label": 1
                },
                {
                    "sent": "There's five different objects within each category.",
                    "label": 1
                },
                {
                    "sent": "These are different viewpoints, different light lighting, lighting, lighting conditions.",
                    "label": 0
                },
                {
                    "sent": "So these are actually fairly big images.",
                    "label": 0
                },
                {
                    "sent": "These are 96 by 96 images, so they have fairly high resolution images.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a D Boston machine that we trained.",
                    "label": 0
                },
                {
                    "sent": "The input is a stereo pair of two images and we've trained the debounce machine with 4004 thousand 4000 Kenyans.",
                    "label": 0
                },
                {
                    "sent": "That was a three ladybugs machine.",
                    "label": 0
                },
                {
                    "sent": "It has about 68 million parameters, so it's a huge market random field.",
                    "label": 1
                },
                {
                    "sent": "And here's what it does.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it raining samples?",
                    "label": 0
                },
                {
                    "sent": "And these are samples generated by the model, so you can see the model is able to capture a lot of structure in the images or a lot of interesting configurations in the images.",
                    "label": 0
                },
                {
                    "sent": "Of course the model is not perfect because it's a bit disbalance, it tends to generate these people with guns more often.",
                    "label": 0
                },
                {
                    "sent": "And that's just that's just the.",
                    "label": 0
                },
                {
                    "sent": "That's just a failure of Markov chain Monte Carlo being able to visit different modes, so I think we need we need better algorithms for learning these models.",
                    "label": 0
                },
                {
                    "sent": "One pleasing thing about this model is that if you actually try to discriminative fine tune it, you get a test error of about 7.2% and this is compared to support vector machines and logistic regression.",
                    "label": 1
                },
                {
                    "sent": "So you see that these models really allow you to extract latent representations that carry meaningful information about the inputs.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one other fun thing you can do is you can do image completion, something that discriminative models can do.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you here is I'm showing you images test images.",
                    "label": 0
                },
                {
                    "sent": "And for this data set, the images are not actually part of the training images.",
                    "label": 0
                },
                {
                    "sent": "So for example in the training datasets you don't see this cowboy.",
                    "label": 0
                },
                {
                    "sent": "OK, now if I black part of the image and try to infer the second part, these are the results that that you see, so you know it's able to figure out that there should be a lag in an arm for this guy, or that if you give it like a plane like this, it figures out that there should be another wing.",
                    "label": 0
                },
                {
                    "sent": "Or or the interesting.",
                    "label": 0
                },
                {
                    "sent": "This was an interesting example.",
                    "label": 0
                },
                {
                    "sent": "This is an elk and there is no elk in the training data.",
                    "label": 0
                },
                {
                    "sent": "There is no figure like that in the training data, but then in training data we have something that looks like a horse.",
                    "label": 0
                },
                {
                    "sent": "And so if I give it.",
                    "label": 0
                },
                {
                    "sent": "Just half of the stuff, basically.",
                    "label": 0
                },
                {
                    "sent": "Fills in the head of an animal, so it really captures.",
                    "label": 0
                },
                {
                    "sent": "This structure of these images.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally one other thing I wanted to show you is you can actually not necessarily just apply to images, but you can also do dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So I was I was showing you these models where I have large number of hidden variables, but in fact you can train these deep deep models where you actually go from, let's say 2000 dimensional space down to two dimensional space and you can view it as a form of dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "In this case this model was trained on.",
                    "label": 0
                },
                {
                    "sent": "800,000.",
                    "label": 0
                },
                {
                    "sent": "Documents from Reuters data set and we used a simple bag of words representation and we took 2000 most frequently used words in the datasets and you can see it's able to find this very rich structure in the data and that was done completely unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "There were no labels provided to the algorithm here.",
                    "label": 0
                },
                {
                    "sent": "You can see the two dimensional embedding of of of.",
                    "label": 0
                },
                {
                    "sent": "Of the topics that the both machine finds and here you can see what latent semantic analysis would do.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard procedure for semantic analysis, something that people use for information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And you know it's sort of it finds reasonable topics.",
                    "label": 0
                },
                {
                    "sent": "It lays them out in a reasonable space.",
                    "label": 0
                },
                {
                    "sent": "In particular, it puts European Community monitoring.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This forecasting learning?",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Estimate a Max.",
                    "label": 0
                },
                {
                    "sent": "So I guess I should point out that here.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should point out, maybe I forgot to point out that in practice, what we actually do is we run several of these chains in parallel.",
                    "label": 0
                },
                {
                    "sent": "So typically we run hundred chains in parallel and what you get is you take an empirical average over the samples generated by each one of those chains.",
                    "label": 0
                },
                {
                    "sent": "So just reduce variance a little bit.",
                    "label": 0
                },
                {
                    "sent": "One step, yeah, so exactly so.",
                    "label": 0
                },
                {
                    "sent": "These parallel chains, what they do is imagine you have 100 chains.",
                    "label": 0
                },
                {
                    "sent": "You get samples, you update the parameters, get samples, you have data parameters, and the samples are averaged over these chains.",
                    "label": 0
                },
                {
                    "sent": "Now the convergence proof of the algorithm doesn't require you to run multiple chains.",
                    "label": 0
                },
                {
                    "sent": "You can do it with one chain.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "The problem is that as your distribution particularly trying to model something very interesting becomes very very highly multi model.",
                    "label": 0
                },
                {
                    "sent": "It's harder for the.",
                    "label": 0
                },
                {
                    "sent": "Samples to move around between different modes.",
                    "label": 0
                },
                {
                    "sent": "Really small to make sure you still send me from the model.",
                    "label": 0
                },
                {
                    "sent": "Is that so?",
                    "label": 0
                },
                {
                    "sent": "The conditions on the learning rate is something like 1 / T. What time if you look at the proofs, the learning rates to actually guarantee that you will converge to the maximum likelihood solution, ridiculously small.",
                    "label": 0
                },
                {
                    "sent": "And in practice it would take you years to actually learn something like that.",
                    "label": 0
                },
                {
                    "sent": "What happens in practice is is is people.",
                    "label": 0
                },
                {
                    "sent": "Notice that even with large learning rates, you tend to learn reasonable models and the question is why?",
                    "label": 0
                },
                {
                    "sent": "Why is this happening in a non aseptic case?",
                    "label": 0
                },
                {
                    "sent": "There's been some work coming out of several groups.",
                    "label": 0
                },
                {
                    "sent": "Particularly in statistical physics where they where they show that that may actually relate to the class of adaptive Markov chain Monte Carlo algorithms.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "It's it's this delicate interplay between inference and learning.",
                    "label": 0
                },
                {
                    "sent": "Because what's happening here is that.",
                    "label": 0
                },
                {
                    "sent": "During the inference step you you're trying to visit these modes in during the learning State you update your model parameters.",
                    "label": 0
                },
                {
                    "sent": "But what happens with stochastic approximation algorithm is that.",
                    "label": 0
                },
                {
                    "sent": "You're you're learning, tries to encourage mixing, so in some sense you're learning actually helps you to do inference and.",
                    "label": 0
                },
                {
                    "sent": "People propose something like herding in in dynamical systems by Mac swelling, and he's using this trick by by sort of using this idea that learning helps inference you know if you set this learning rate to be very large, then what you observe is your samples here.",
                    "label": 0
                },
                {
                    "sent": "They jump around the space like crazy.",
                    "label": 0
                },
                {
                    "sent": "You changing your energy landscape.",
                    "label": 0
                },
                {
                    "sent": "In in, in these big steps and your samples walk around very fast and then helps learning.",
                    "label": 0
                },
                {
                    "sent": "But of course to in order to get the very stable model, you still have to.",
                    "label": 0
                },
                {
                    "sent": "Decrease the learning rate and then.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The way to get around the small learning rate is to have a large learning rate, but then having strangers at the end, you know if this procedure will converge.",
                    "label": 0
                },
                {
                    "sent": "So say that again, have a large learning rate and then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "So there's been some theoretical results showing that what you should actually do is toward the end of learning average over.",
                    "label": 0
                },
                {
                    "sent": "Several several parameters or or average over certain window and that produces consistent.",
                    "label": 0
                },
                {
                    "sent": "You can prove consistency in terms of convergence of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yes, in practice, people notice that that works fairly well as well, and there are some papers in statistics communities that.",
                    "label": 0
                },
                {
                    "sent": "Don't have that yet.",
                    "label": 0
                },
                {
                    "sent": "Like or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the no, I haven't, but I think that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "One of the things you could try to do is you could try to say, well, can we actually use these samples to give us an approximate Hessian or some some 2nd order information?",
                    "label": 0
                },
                {
                    "sent": "Because you have these samples and such and?",
                    "label": 0
                },
                {
                    "sent": "And I think there is some work that that's attempting to do that, but I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried, but I think this is something something too.",
                    "label": 0
                },
                {
                    "sent": "It's just the one thing that's.",
                    "label": 0
                },
                {
                    "sent": "Sort of pushes back is these estimates are not really unbiased estimates of these expectations, so they are very noisy.",
                    "label": 0
                },
                {
                    "sent": "So then you can get sort of the noisy 2nd order estimates, and I don't know whether that will help or hurt.",
                    "label": 0
                },
                {
                    "sent": "Talk about more later, OK?",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "No, no, but I think this is something to to try, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Aren't these really expensive?",
                    "label": 0
                },
                {
                    "sent": "But if you have 2000 hidden units, you have to sample each of them separately.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's that's sort of the trick.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're using with these models with these deep models is we don't have any connections between the hidden variables and that's precisely just to encourage the mixing because you know you can update all of these units in parallel.",
                    "label": 0
                },
                {
                    "sent": "Conditional states of these units in the states of these units.",
                    "label": 0
                },
                {
                    "sent": "But yeah.",
                    "label": 0
                },
                {
                    "sent": "Gibbs sampling becomes particularly having these big models.",
                    "label": 0
                },
                {
                    "sent": "Efficiency becomes becomes an issue, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, how long?",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Right, so this model took us.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think a week that rain.",
                    "label": 0
                },
                {
                    "sent": "What was the computer?",
                    "label": 0
                },
                {
                    "sent": "Oh, I think it was fairly fast computer.",
                    "label": 0
                },
                {
                    "sent": "I think something like 3 point something giga Hertz or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just a single computer didn't paralyze, so didn't do GPU's or anything like that even though you in principle could.",
                    "label": 0
                },
                {
                    "sent": "I just let it run.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "In your opinion, random people and are versus science.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I think it's I think it's to be honest.",
                    "label": 0
                },
                {
                    "sent": "I think it's it's a bit of science.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's signs, but it's also art.",
                    "label": 0
                },
                {
                    "sent": "You know one of the problems with the models that we have right now is, you know we're doing these pre training algorithms and we're using contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "So if you start using Markov chain Monte Carlo algorithms then not as robust as you would have hoped they would be.",
                    "label": 0
                },
                {
                    "sent": "You know you sort of have to watch out for the learning rates.",
                    "label": 0
                },
                {
                    "sent": "In particular, the learning rates for these.",
                    "label": 0
                },
                {
                    "sent": "For these stochastic approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "Particularly these learning.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a fairly important.",
                    "label": 0
                },
                {
                    "sent": "So I would say that we are getting there.",
                    "label": 0
                },
                {
                    "sent": "A lot of people are now beginning to use these models in see how they do, but we have very far from actually making them.",
                    "label": 0
                },
                {
                    "sent": "Very reliable and very robust to just model all kinds of distributions, that's.",
                    "label": 0
                },
                {
                    "sent": "And you'd expect that because.",
                    "label": 0
                },
                {
                    "sent": "Increasing the number.",
                    "label": 0
                },
                {
                    "sent": "So sort of maybe some kind of nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Something that can adapt the number of hidden units?",
                    "label": 0
                },
                {
                    "sent": "No, I don't think there is.",
                    "label": 0
                },
                {
                    "sent": "There is any work doing that.",
                    "label": 0
                },
                {
                    "sent": "What people tend to do in practice they tend to use cross validation such as you train multiple systems and then you're trying to just pick one out of those.",
                    "label": 0
                },
                {
                    "sent": "The problem with using some kind of nonparametric type of models is it's very hard to do model selection because of the partition function.",
                    "label": 0
                },
                {
                    "sent": "So if you add more hidden units that changes the global partition function of your model.",
                    "label": 0
                },
                {
                    "sent": "It's like trying to learn Markov random fields where the number of.",
                    "label": 0
                },
                {
                    "sent": "Variables grows or shrinks, it's.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's a bit hard, but.",
                    "label": 0
                },
                {
                    "sent": "There's been some work, and Martin Zoomer has a work on Basean restricted Boltzmann machines where he actually tries to compute the posterior distribution over the parameters and that might give him some indication as to which parameters should be in the model or not.",
                    "label": 0
                },
                {
                    "sent": "And Tom in kinnelon.",
                    "label": 0
                },
                {
                    "sent": "Allen Key they also had basean conditional random fields, sort of.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're going to return at 9:10 for a regularly scheduled program with Ericsson.",
                    "label": 0
                },
                {
                    "sent": "So let's thank God.",
                    "label": 0
                }
            ]
        }
    }
}