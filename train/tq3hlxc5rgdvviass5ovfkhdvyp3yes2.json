{
    "id": "tq3hlxc5rgdvviass5ovfkhdvyp3yes2",
    "title": "Bayesian models of human inductive learning",
    "info": {
        "author": [
            "Joshua B. Tenenbaum, Center for Future Civic Media, Massachusetts Institute of Technology, MIT"
        ],
        "published": "June 22, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Social Sciences->Psychology->Developmental Psychology",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_tenenbaum_bmhi/",
    "segmentation": [
        [
            "Welcome and.",
            "I'm very pleased to introduce Josh Tenenbaum, our invited speaker this morning.",
            "Josh studied.",
            "Physics is an undergraduate at Yale and then came to the Department of Raven Cognitive Sciences to do his PhD.",
            "Where he studied computational models of human categorisation.",
            "And generalization and.",
            "From MIT judgment onto effective position at Stanford, he was a sample for a few years, but I guess he missed life on the East Coast and MIT and all that.",
            "So he came back to MIT where he's.",
            "Active member in the Brain Cognitive science Department.",
            "Josh is really a pioneer of.",
            "I think it's really exciting new era in cognitive science, which is taking computational models.",
            "And statistical models very seriously.",
            "And there is a whole cadre of young researchers who are pushing cognitive science in this direction.",
            "And Josh is made many contributions to this area in categorization, generalization, causal inference, and just a variety of other important areas in computational cognitive science, which is really probably the best label for what he does.",
            "But being so close to the machine learning community and using so many of the common ideas from information theory and statistics to inform computational models of cognition, Josh is also made important.",
            "Contributions for machine learning.",
            "So it's great to have him a nice email.",
            "So for example, his work on Isomap for nonlinear dimensionality reduction was a very well known paper that was published in science.",
            "So.",
            "I'm just thrilled to have Josh here.",
            "It's always really fun to hear him talk and tell us about computational models of human cognition, so that's all.",
            "Thanks, Josh for making it here.",
            "Thanks.",
            "OK, thank you.",
            "Can you hear me?",
            "Yep, thank you very much for having me here.",
            "I think this is my first time at ICL.",
            "I've read of course many ICL papers, but I don't think I've actually been to the conference before, so it's great to be here."
        ],
        [
            "Let me just start off by acknowledging a bunch of lab members, including one former lab member, Tom Griffiths, who's now on the faculty Berkeley.",
            "I just want to all of these people have made some contribution to the research.",
            "I'm going to talk about here.",
            "I want to highlight the Charles camp over there in Red because most of the good ideas and hard work in what I'm going to talk about come from Charles's thesis.",
            "He's just about to graduate, and then he'll be starting a faculty position at CMU.",
            "So if you like these ideas, you might look him up."
        ],
        [
            "Now I I don't.",
            "I don't think I need to tell this audience about the probabilistic revolution in machine learning and AI more generally across pretty much all subfields of AI, maybe most prominently in machine learning, but including vision, robotics, natural language, expert systems, reasoning, planning the fields been revolutionized by the adoption of sophisticated mathematical techniques that give principled, and in some cases sufficient and certainly effective methods for dealing with uncertainty, in particular for.",
            "Trying to make inductive inferences from very, very ill posed problems from very ambiguous data, but if you look at you know I would say what if you sort of did an informal survey of the leading practitioners at the state of the art of probabilistic AI and machine learning.",
            "Most of them would say that they don't think there's any necessary connection between the math of these probabilistic methods and the way the human brain actually learns.",
            "And increasingly as statistical machine learning has become more mathematically sophisticated, at least up until a few years ago, there's been less and less of a connection between the field of machine learning in the field of human learning.",
            "To me, that's too bad because of someone who sits between these fields as open was saying.",
            "I think these fields have a lot to learn from each other.",
            "I hope I'm not the only one I wanted to start here with the quote that Zubin wrote me over email when he was inviting me to speak and we were talking about topics.",
            "He said many people in machine learning get into the field because they are interested in how humans learn rather than how convex functions are optimized and how we can get machines to be more like humans.",
            "So I hope that view is representative of at least much of the audience here and I hope.",
            "Will be some interesting insights to be to be had."
        ],
        [
            "From this interdisciplinary discussion.",
            "Now the particular problems that we're working on here or what we like to call everyday inductive leaps.",
            "These are problems where sort of essential core problems of human learning, where people seem to learn so much about the world from such.",
            "Apparently such limited evidence.",
            "Classic examples that I've been studying for awhile, as even mentioned, are basically problems of concept, learning and categorization.",
            "So to take a paradigmatic example, consider a child learning their first words like this is sort of a rough reconstruction of my daughter.",
            "Learning the word horse when she was a year and a half old, we went out to the country.",
            "For Thanksgiving, western Massachusetts Ann from just a few experiences like this, seeing a horse paired with an excited label from one of her parents horse, she was able to grasp this concept and pick out if not perfectly, almost perfectly, other horses.",
            "You know she might be confused about the occasional cow, or or camel, but more or less she got this concept from just one or a few examples.",
            "If you want to think about it just with a simple mathematical abstraction, there's some you know.",
            "Basically infinite set of objects out there in the world.",
            "It really is infinite if you think of all the horses you could draw, and even of course this concept is going to generalize to drawn horses as well.",
            "So there's this infinite set of all objects you could perceive, and there's an infinite subset of that which you could say is the subset of horses and somehow a child learning a word like this is able to grasp more or less the boundaries of that set from just one or a few.",
            "Randomly chosen points within the set.",
            "That's a stunning achievement."
        ],
        [
            "We want to understand how it could work in case you forgot what it's like to be a child learning a word, let me give you an example of a sort of a demo of experiment we've done in our lab where we tried to put human adult in this same kind of situation.",
            "So you're on the planet zoo.",
            "These are some of the guys, Ubi and objects, and we're going to give you a few examples of various words in the Gazoo being language.",
            "So let's show you a few tufas here now, even I think, I think if I'd only just giving you an example and certainly giving you 3 examples here, though, you've never seen the word two before or seen most of these kinds of objects.",
            "Before you have a pretty good idea of what things are two phase in which aren't, so let's try this out for the audience participation part of the lecture.",
            "Just yell out.",
            "Is that a 2 for yes or no?",
            "Little more uncertainty there.",
            "That's good because our model predicts that all right.",
            "So you get the idea."
        ],
        [
            "The problem of generalizing from a few examples isn't just about learning words and concepts, although that's one of the main places where it comes up, but it also comes up and we've studied it in other contexts, such as learning causal relations, relations between cause and effect.",
            "What cognitive scientists called theory of mind learning about the internal mental states, beliefs, goals, plans of other agents from observing their behavior or, for example, learning intuitive learning intuitive theories that govern our psychological roller, social world, social rules and conventions.",
            "Intuitive physics.",
            "All of these are versions of this kind of problem, just to give one other example, which will come back to time permitting at the end, learning cause and effect.",
            "So we all learn in our most basic statistics classes that you can't infer necessarily causation from observing the correlation right.",
            "There could be confounders and hidden variables and things like that directionality isn't so clear with recent developments in causal Bayes Nets.",
            "Arguably there are some cases now where you can infer causation from correlation, and there are modern kinds of techniques for controlled experiments which allow you to do this under intervention.",
            "But think about the case of a child learning the causal relation or or inferences, we make all the time, where often just one or a few examples.",
            "The right kind of pattern of suspicious coincidence can strongly suggest the causal relation when again from just one or a few data points.",
            "You couldn't even computer reliable correlation, right?",
            "So the problem isn't going from correlations to causation, but rather observe or inferring causal structure.",
            "When you, when you have so little data, you can't buy at least conventional statistical means, even computer reliable correlation."
        ],
        [
            "So how can you solve this problem?",
            "Well, in some sense the answer is pretty simple and we all know what it is right?",
            "If you're learning a lot from a little data, it must be because in some sense you already knew a lot.",
            "You must have some strong prior knowledge or some inductive bias.",
            "This is a lesson that the field of machine learning has learned and taught quite well, right?",
            "Pretty much any paradigm for machine learning has some version of the idea that there's no free lunch that if you have no constraints on your hypothesis space, you can't learn anything interesting that goes beyond data.",
            "And the bigger the in principle, hypothesis, space and the less data you have, the more constraints, the stronger the inductive bias you're going to need.",
            "But of course that isn't really satisfying, because if that's all there was to it, then we already have human like machine learning systems which."
        ],
        [
            "We don't for the most part, so really this is just the opening of a number of questions which we want to ask exactly how is it that abstract, background knowledge, inductive bias, guides, learning from sparse examples?",
            "In the cases that I've talked about?",
            "What form does knowledge take?",
            "What is its content and representational form across different tasks and domains?",
            "This is a hard problem and one that a lot of cognitive science is devoted to try to study because people most the kinds of inductive biases, implicit knowledge people have that are constraining their generalizations.",
            "A lot of it is quite implicit and unconscious.",
            "You can't just ask somebody, so what's your inductive bias exactly here?",
            "But that's often what we're most interested in is explaining how they can do these tasks.",
            "So we need techniques that can elicit and formally represent the kinds of prior knowledge people bring to these tasks.",
            "Maybe the most interesting kinds of learning questions are how is this abstract prior knowledge itself learned or acquired?",
            "Now, of course, inductive bias doesn't have to be learned in principle, it doesn't have to be.",
            "And if you look kind of classically in machine learning as well as in cognitive science, I think that sort of default assumption is that it's not like inductive biases are these hardwired constraints we designed by hand into our learning algorithm in machine learning, or the people who've most forcefully advocated for inductive biases and cognitive science.",
            "People like Chomsky and lists Pelkey and others in the nativist camp, saying that there are just the mind comes into the world through genetic specification to have very strong constraints on what it can learn and what it can expect.",
            "And that's why we're able to learn so much from so little.",
            "But I think it's clear that in many cases, inductive biases in human learning have to be learned.",
            "There are many reasons for this.",
            "One is, as I'll show you in a minute.",
            "There are some very compelling examples of cases where human children at a certain age, maybe age 2.",
            "Have some very important inductive bias, but but slightly before that they don't have it.",
            "And there's evidence that they can actually be taught.",
            "These inductive bias is another important reason to focus on learning is that we can learn about new domains that we didn't have access to in our evolutionary time.",
            "We can.",
            "We can learn chemistry and then acquire new inductive biases for making inductive inferences about new chemical compounds, or to take a more intuitive example, you know we live in a very complex social system and there are different societies follow different kinds of rules.",
            "Many of which didn't have any precursor in in our evolution existence.",
            "We didn't have the same kind of population density and size of cultures and complexity of cultures.",
            "Yet when you grow up in this culture, you're able to learn lots of implicit knowledge of how your culture works, and then you use that to make very rapid and for the most part, reasonable inferences from very sparse data about new people you meet in new situations that you find yourself in.",
            "And this brings us to the last question, which I think is particularly interesting from a machine learning point of view, as well as a human learning one, which is, it seems like there's a tension between these two aspects of human learning.",
            "On the one hand, and really, these are distinctive aspects of human learning which you don't see necessarily so much in other natural animal learning systems.",
            "On the one hand, we have these very powerful abstract constraints, inductive biases that allow us to learn so much from so little.",
            "On the other hand, it seems like we can learn new ones.",
            "We're very flexible in the inductive biases we can.",
            "We can come up with when you go to a new culture.",
            "You can pick up how that culture works.",
            "You can actually learn things in school, and it seems like these might be in conflict, right?",
            "If we have strong constraints which limit the hypothesis we can entertain, then how is it that we can then not only entertain hypothesis which aren't part aren't governed by those constraints, But actually sort of throw out those constraints and come up with a new set of constraints for a new situation we find ourselves in the goal in this work is to come up with a computational framework for addressing these questions.",
            "And these are hard questions, you know.",
            "We're just beginning to make progress, but I think we are starting to make at least some progress on these questions and hopefully."
        ],
        [
            "Will be useful insights for machine learning.",
            "Now the approach that we're working with builds on a bunch of ideas which are probably an aggregate familiar to this community, but we might be using them in some slightly different ways because we're working on slightly different more human motivated problems.",
            "On the most basic technical ideas, Bayesian inference as you could guess from the title of the talk and in a sense, these technical ideas are each going to address these four big questions I had here.",
            "So how does background knowledge guide learning from sparse data?",
            "Well, if you are Bayesian priors are some sense derived from this abstract background experience, so that's a clear and powerful and totally general purpose way that background knowledge can guide learning.",
            "But of course that doesn't really tell you very much anymore than just pointing to inductive bias.",
            "We need more, more powerful, richer kinds of techniques, so one is to address the question of the form of knowledge, right?",
            "If you just look at a very simple Bayesian analysis, what is a prior?",
            "It's a set of numbers, or it's a simple parametric statistical distribution, but that isn't knowledge, right?",
            "And we want to understand how rich knowledge representations can generate priors for Bayesian inference, so that requires us to look at more structured probabilistic models.",
            "For example, probabilities defined over graphs or richer things like grammars.",
            "Relational schemas and so on.",
            "If we want to address the question of how his background, knowledge and inductive bias itself learned well for that, we've been using hierarchical Bayesian models where you have multiple levels of probabilistic representation with safe above the level of the conventional prior, you have some kind of hyperparameters a prior on the prior and by looking at learning in those hierarchical models, we might be able to explain where peoples inductive biases come from.",
            "By combining these techniques right, we can we can.",
            "Learn inductive bias which are not just numbers of parameters, but actual structured knowledge.",
            "And Lastly, we want to understand this this tradeoff and maybe seeming paradox between constraint and flexibility.",
            "For that we've been working with nonparametric Bayesian techniques where the complexity of the model isn't determined in advance but is able to grow as licensed as new data come in.",
            "So new data come in and when the when appropriate, the model can make itself in the sentence, make itself more complex as needed.",
            "But but but maintains the ability to have strong constraints for data that respect previously learned."
        ],
        [
            "Knowledge.",
            "So the outline for the talk is will be to go through three case studies in modeling human inductive learning, and we'll see how much exactly time there is for all three of those.",
            "The last one will be somewhat abbreviated.",
            "The first case study is on word learning, and the model that I'm going to talk about here.",
            "Is in some sense a very, very simple hierarchical Bayesian model.",
            "It's probably trivial to many of you with one sort of little twist at the end, which isn't so trivial, but it's worth going through is just to warm up for the basic ideas, and also because it allows us to address and really to talk about one of the most interesting phenomena of how humans learn induct."
        ],
        [
            "Biases in childhood and the phenomena is what's called the shape bias in Word learning.",
            "This is a phenomenon that was studied by a number of developmental psychologist starting in the mid to late 80s, and here's an example of it.",
            "You take a 2 year old child who's learning to speak English and you show them something like this and you say this is attacks and then you show them three other things here and you say, can you show me the other decks?",
            "Which one do you think they're going to choose?",
            "Yeah, they'll choose the first one, the one that has the same shape as opposed to, say, one that has the same color but not the same shape, or one that has the same texture.",
            "This is a sort of abstract rendering of texture.",
            "They'll choose the one that has the same shape.",
            "Mount 2 interesting things about this.",
            "First of all, while two year old children in English will reliably do this, if you go back a few months before that, they won't.",
            "So 20 month olds will be at chance.",
            "There will be just as likely to give you any anything that matches on one of these feature dimensions.",
            "So something interesting is going on there.",
            "And the other point is at the shape bias is, it seems at least to be very useful.",
            "Kind of inductive constraint.",
            "Arguably the majority of early words are basically labels for categories of objects, and arguably the most reliable queue.",
            "Two Object category membership is some notion of shared shape.",
            "All balls are basically ball shaped.",
            "All bottles are in some sense ball shape, although they might vary in color and material.",
            "So where does this shape bias come from?",
            "Well, one possibility at that.",
            "Some people have proposed is that it actually derives from some kind of innate knowledge that then only somehow becomes manifest around this age, but there's some pretty compelling evidence that it is in fact learn."
        ],
        [
            "It's a kind of abstraction from experience, so Linda Smith and colleagues have done a series of really nice studies looking at this, and here's one example of one of these studies.",
            "So they took 17 month olds.",
            "Kids were a year and a half.",
            "Basically who don't don't already show the shape bias, and they gave them a kind of a training training curriculum that took place over 8 sessions once per week, so over two months each session was pretty short, only 20 minutes long.",
            "And what did they do in these sessions?",
            "Well, they took these.",
            "Basically these these.",
            "Eight novel objects, four pairs of objects which have within each pair basically the same shape, but they differ in color and texture and other features, and they gave these two 2 objects in the same category the same label, and played with them in a way that highlighted the labels.",
            "So they would say, oh, here's your web.",
            "And here's my web.",
            "Let's play with our web.",
            "So can you.",
            "Can you show me your web?",
            "Oh here's my web, OK?",
            "That sort of thing, and they did that with the same 8 objects every every session.",
            "Once per week.",
            "There was also a control group which I'll come back to in a second who had the same objects but just didn't get the words.",
            "So they're just like, oh, let's play with these toys.",
            "Here's here's here's mine exactly, the same thing, but no label.",
            "And what happened after 8 weeks of training?",
            "The 19 month olds now had had acquire the shape bias, so now you can show them a new thing.",
            "This is a Dax and show them these other ones again matching in different dimensions and they'll reliably pick out the one that matches in shape.",
            "So what's going on here?",
            "Well, Smith, another psychologist characterizes as a kind of learned attentional bias that children have learned to pay attention to the dimension of shape when they're learning words, but in modern machine learning parlance, we might call this transfer learning right somehow.",
            "By learning these words.",
            "For these categories, children have picked up some kind of abstraction which now transfers to another word in another object category, which has nothing in common at a very concrete level, right?",
            "This shape is totally different from those shapes, and it's a totally different word, but yet there's some transferable knowledge."
        ],
        [
            "So, so how does this work?",
            "Well, one other relevant phenomenon, which is that here's the real interesting transfer, which is transferred to the children's real world vocabulary.",
            "So the this the scientists tracked for each child what words they know in English.",
            "So not just made up words for made up objects, but you can get parents to basically report in a somewhat noisy, but on average, reliable way what words their children know and you can see how many words they know at.",
            "17 months when they started at 19 months when they finished an and they separated out the mean number of object names from other words, the children know.",
            "You can see that at the start, about half the words are actually object names now.",
            "Well, what they found was that the kids who had gone through this training and learned the shape bias after two months had a dramatically accelerated rate of real world word learning compared to the control group.",
            "Remember, the controller had the same kind of experience, except they just didn't get word labels and what happened over two months, the control group learned about 10 or 15 new object labels, but the training group with the word labels learned about 40.",
            "So like three or four times more really.",
            "Wopping dramatic effect, and it's quite specific because you can see that in terms of other words that they learned there wasn't really any difference, so it's not like the training kids are smarter or that they had acquired sort of a general ability to learn words.",
            "No, it seems like they learn something specific about how to learn object labels.",
            "This is, I think, one of the most striking results in developmental psychology because it's an actual useful intervention that you can do with kids and it works, and they've replicated people.",
            "Didn't believe this when they first did this.",
            "There's not many results in developmental psychology.",
            "This robust but they are.",
            "It's been replicated and it seems to be real, so the puzzle here computationally is it seems very clear that the shape bias is a powerful inductive constraint, because look how useful it is having learned it.",
            "Yet not only does the shape bias allow you to learn new words from very few examples, but it itself seems to be learnable from very few examples, right?",
            "Just basically four example categories, and that's quite striking, right?",
            "If we if we think of we're used to thinking of inductive biases or abstract knowledge, is something that takes a much longer time and a lot of experience.",
            "No, you can learn this effectively from just basically 4 examples of categories that follow it with two examples and objects each.",
            "So how can we explain this?",
            "Well, I'll just show you a simple kind of hierarchical Bayesian model which is based on the idea of learning about feature variability.",
            "The intuition which I think is the same as the psychologist intuition is basically that the dimension of shape for nameable object categories has a different profile of variability than other dimensions.",
            "Shape varies a lot across different object categories, but not very much within object category.",
            "It's the distinctive feature that picks out these early nameable object categories and the ones that most matter to infants, whereas other features like size and texture and color vary a lot more across objects, but particularly a lot more."
        ],
        [
            "In categories.",
            "So how do we capture this idea in a hierarchical Bayesian model?",
            "Now to many of you, this will probably be again trivial except for a little twist at the end, but I thought it's good to just warm up with a simple example.",
            "So I'll just.",
            "I'll just motivate this or I'll introduce the model using sort of a textbook case of learning about bags of marbles.",
            "So let's say here you have a bunch of marbles of different colors.",
            "And you have a bunch of bags that somehow have these marbles distributed in the bags.",
            "Let's take a bag and choose a Marvel out of it so that we get a green marble here, let's see.",
            "So what can we say about the other marbles in that bag?",
            "What do we think the next one is going to look like?",
            "Well, not a whole lot.",
            "There's not a whole lot we can say."
        ],
        [
            "Contrast that with the case where we look inside the other bags and find this.",
            "So across all these bags we find different colors of marbles, but it seems like most bags are mostly homogeneous.",
            "Then for this bag we can feel pretty confident in guessing.",
            "Assuming these bags are in some sense of the same type that the other marbles we're going to try out our green here, even though we haven't actually seen any bags of green marbles, despite what you might think, this is not in fact green on my screen.",
            "So this is again, there's some abstract knowledge about about how variable this feature is across."
        ],
        [
            "Bags within bags which you can capture intuitively in a kind of two level hierarchical representation so you can see what you've learned about each of the bags which you've inspected extensively.",
            "You can learn, you know that say this bag is mostly red, and this ones mostly yellow and so on.",
            "But you also learn something more abstract, which is about the dimension of color.",
            "In this world, that color varies across bags, but not much within bags or in other words, the variance of color is mostly due to between bag variation rather than within Bagration.",
            "And having learned that now, if this new bag which the green one has been drawn from in some sense, is the same kind of bag, then it's a good guess, although although we can't be certain, that bag is also mostly green and."
        ],
        [
            "It draws from that will be green, so the way we capture this in a hierarchical Bayesian model is with what's called Dirichlet multinomial model.",
            "It's just a multidimensional sort of multi way extension of the beta binomial model.",
            "If you're familiar with that one where we've just replaced these qualitative descriptions with parameters of a statistical model, so we characterize each bag by a multinomial distribution over the colors you expect to see when drawing from it, and we have Theta parameter to describe that multinomial.",
            "And then we assume that that if all of these bags are of the same type and there's some kind of common prior in this case, it's a Dirichlet distribution which parameterized by Alpha and beta, which we assume it provides the same prior for all of these multinomial parameters across bags.",
            "Now there's nothing special.",
            "I mean, there's no particular reason we have to use a D richly distribution in somewhat limited in what it can capture, but it's just enough to capture what we want.",
            "For this example, it's mostly convenient because it's a conjugate prior.",
            "For these parameters, and so we can integrate out these parameters."
        ],
        [
            "It just makes inference easier.",
            "What these parameters mean right?",
            "Is Beta is a vector of probabilities.",
            "Which captures which expresses the proportion of colors we expect to see across the whole population, and Alpha is a scalar that determines how much.",
            "Variability we expect to see within class versus between classes.",
            "So if Alpha is very large then we expect that individual bags are pretty variable in the same way the whole population is, whereas Alpha is very small.",
            "Then we then we expect the variability is is mostly between bags and each bag is pretty."
        ],
        [
            "Homogeneous and the idea here is we're going to put some prior very weak prior over the the second level of the model, and we're going to observe the data at the bottom.",
            "And then we're going to fill in what's in between and we're going to do inference at at both levels of representation here to simultaneously infer what each bag is like, as well as the parameters that describe what bags in general are like that can be used to transfer to new cases, and just to graphically illustrate what's going on there.",
            "Let's say to capturing the knowledge that this bag is mostly red means here I'm just drawing a simplex for three colors.",
            "Of course this should really be higher dimensional simplex, where each corner corresponds to a bag of a pure color.",
            "If if we thought the bag was pretty mixed, then Theta would be in the middle of the simplex.",
            "But if we think the bag is mostly red, than Theta is towards the red corner, it may not be exactly at the red quarter, even with this uniform data, because the prior mites."
        ],
        [
            "Little bit for a bag that's mostly yellow.",
            "We're inferring that Theta is towards the yellow corner of the simplex.",
            "For knowledge at this level.",
            "What we're inferring is, is now the distribution over the simplex, which is the prior for the quantities.",
            "I was just showing and the Dirichlet distribution is capable of reflecting different kinds of priors you have.",
            "You can have sort of unimodal distributions like that, which expect that each bag is pretty variable in the same way that the population is, but in this case, what you have is a distribution like this, where Alpha is low where most of the mass is out at the corners of the simplex, which reflects the idea that we expect most bags are.",
            "Pretty homogeneous and the variability in the population comes between bags."
        ],
        [
            "So how do we apply this to the shape bias case well?",
            "Just a very simple analysis will represent each of these eight objects with several dimensions, including shape, texture, color and size, and what we have here is that.",
            "The category labels just refer to the words here and shape is constant within category, but the others vary across within category.",
            "And if we the simplest way to apply this model is to assume that each dimension is generated from an independent Dirichlet multinomial model, the same structure but just different parameters, and then what we learn is that shape varies across categories but not within categories.",
            "In other words, shape is just behaves just like color did here.",
            "But these other dimensions of texture, color and size as you.",
            "As you might guess."
        ],
        [
            "Don't very.",
            "And if we if we then go and apply this to the case of the of the.",
            "You know there was actually tested the sort of transfer phase, so now we have a new object in a new category #5 here.",
            "A new shape, new texture, a new color.",
            "So on what's important in this simulation, and of course any model has got to do something like this for this task is that the we've assumed there's more possible shapes, textures, colors and so on that are actually observed during training.",
            "There may be a lot more, but the key thing about this kind of hierarchical Bayesian model is it can learn expectations even for the values of dimensions that haven't been seen.",
            "So even for example, for a shape that has never been seen before, paired with the category label and.",
            "Then we have these three test items.",
            "This one here corresponds to having the same shape as the tax, but a different new texture and color.",
            "This one has a different shape and a different color, but the same texture and so on.",
            "And what the model tells us is that this one is much more likely to have come from the same category that this one came from.",
            "Then these two so relatively.",
            "This is the best choice for Dex, so even this very simple model is capable of capturing this kind of abstract."
        ],
        [
            "Transfer that that children seem to show.",
            "Now this model is is very simple and it's easy to come up with limitations.",
            "For example, assuming that we have a single dimension of shape, maybe seems like a limitation and there's lots of extensions which you can go through, which I'm not going to go through here.",
            "'cause I'm going to shift to a different topic.",
            "We can work with more distributed representations of shape, for example, and get the same kind of effect.",
            "I'll just talk about one extension because it's again corresponds to an interesting developmental milestone, and it has a nice machine learning correlate.",
            "Which is an interesting kind of selective transfer depending on knowledge of ontological kinds, so.",
            "The shape bias that we saw was a phenomenon that kicks in around age 2 by age three.",
            "Children know about the shape bias, but they also know about other kinds of biases, for example, and they restrict them appropriately to certain large scale categories, so they apply the shape bias to solid object categories like ball book toothbrush.",
            "But for non solid substances like for example toothpaste, they apply a material by so you can introduce a new weird gel non solid substance or weird powdery thing.",
            "And they will not care about the shape of that entity, but they will care about its material properties in generalizing a new word.",
            "Again, that's it."
        ],
        [
            "Correct bias, so it takes a little bit longer to learn.",
            "So how can we model this kind of selective transfer?",
            "Well, you might say you know.",
            "Suppose we we know the ontological kind, whether something is a solid object or a non solid substance.",
            "That's this Z variable here.",
            "Then we can just learn a separate Dirichlet multinomial model for each.",
            "Each ontological category.",
            "Just adding another level to the Bayesian hierarchy.",
            "So here capturing variability and features for solid objects.",
            "Knowing that, say all objects all things in this category are solid, but shape is variable across objects, although not within category and other features are variable within category as well as across categories.",
            "And then at this lower level learning specifics about what words refer to, it shapes and then we will learn something corresponding over here on the material side.",
            "But of course this itself is already oversimplified.",
            "Because the data don't necessarily come labeled this way, we don't Opry Ori know what the right ontological kinds are, so this is chicken and egg problem.",
            "We don't know how to partition the object categories into these different parts of the of the hierarchical struck."
        ],
        [
            "So our solution to this is just to add inference at this higher level using a non parametric prior over this partition.",
            "In particular we're using a Chinese restaurant process prior, so we're saying that.",
            "Basically.",
            "Each object category belongs to some higher level ontological kind.",
            "That's the Z vector, but we don't know how many ontological kinds there are or which object categories in which ontological kind.",
            "But we can do inference over that.",
            "At the same time as we do inference about these two levels of parameters, and in fact, for these simple toy datasets recover the right kind of structure here.",
            "So this is kind of nice.",
            "It's certainly related to CRP mixtures, but instead of clustering objects were clustering categories of objects, and while the objects in a conventional clustering model the objects.",
            "Have something in common?",
            "Here are the categories of objects.",
            "Don't really have anything concrete in common.",
            "In fact, their most distinctive feature which is the shape of that objects in the category is totally different for every category.",
            "But what they have in common is that they draw on the same kind of prior.",
            "So it's this kind of cluster innocence clustering that you get at an abstract level by combining a nonparametric cluster prior with a hierarchical Bayesian model is a powerful way to do selective transfer, and in fact very recently.",
            "Some other researchers at MIT, Dan Ryan Leslie cabling have defined a related model for a selective transfer task in an applied setting where you are trying to learn.",
            "It's a Caillou project where you're trying to learn whether a user will accept a meeting invitation and they learn a naive Bayes classifier for each user, but then they define a CRP prior over the parameters of those naive Bayes classifiers and they're able to sort of improve transfer by by figuring out just exactly which users should be should be used to track."
        ],
        [
            "To which other new users so this kind of idea could have?",
            "I think a lot of implications.",
            "Other applications in machine learning.",
            "Now what I want to do next is turn to another setting which is in some sense more knowledge rich in the in the previous setting.",
            "It was nice because it it was a very simple, elegant mathematical model of a very important developmental phenomenon, but there wasn't much interesting structured knowledge representation going on which I was arguing is so crucial for human learning."
        ],
        [
            "So to get that, we're going to switch to a different task, which is another one of these tasks.",
            "Of generalizing from a few examples, it's a task that many psychologists have studied really since the mid 70s, so there's a lot of data here that you can model, and our models can get much more quantitative.",
            "So here's the kind of task it's in the form of evaluating the strength of an inductive argument.",
            "But again, it's like generalizing a new concept from a few examples.",
            "So these are the premises.",
            "Gorillas have.",
            "T9 hormones, seals have T9 hormones, squirrels have T9 hormones, and you're asked how likely is it that the conclusion is true given these premises, how likely is it that horses have T9 hormones?",
            "Given that these do so to compare this with, say, the word learning case.",
            "There's a new concept which is having teen hormones and you get a few examples of things in that concept and you're asked to judge the probability that something else is in the concept.",
            "So it's the same kind of task.",
            "But it's it's in some sense, easier to study different kinds of knowledge that can be brought to bear on this task.",
            "Now psychologists will do things like compare how people rate the strength of arguments or compare pairs of arguments and say which is stronger, which is a better generalization and say if we compare these three people typically think this is a stronger generalization that knowing that gorillas, seals and squirrels have some arbitrary biological property provides better evidence for thinking that horses do then for thinking that flies do, or and also better evidence for thinking that horses do.",
            "Then, if you were told that.",
            "Say this property was true of gorillas, chimps, monkeys, and baboons, in which case you might think it's just something specific to primates, and they explain this with more or less informal notions of simile."
        ],
        [
            "Pretty and typicality, but we want to get a bit more formal.",
            "Well, a lot more formal, and so how are we going to do this?",
            "Well, here's a way to render the computational problem abstractly.",
            "There's a bunch of objects in the domain that we know something about, and what we know about this domain will start off with our most concrete level of knowledge will just be various features we know about these things.",
            "So for example, again, psychologists have collected a lot of data on what features people will will know for different animals, so there's a in this case here.",
            "We're going to use a matrix of object feature judgments that has 85 features and 50 animals and a bunch of subjects rated how how much each feature is associated with each animal.",
            "And these features are just sort of basic observable, anatomical, behavioral, ecological properties.",
            "The kind of things that child walking around the zoo or reading kids books about animals might learn.",
            "So for example, the features that are strongly associated with elephant are things like being grey, hairless, tough skin, big bulbous body shape, long legs.",
            "Tasks walking, being slow, living in the jungle, that kind of thing being strong.",
            "So there's this matrix of sort of known facts about these animals, and there's some new property like say having T9 hormones that is less observable that we're interested in.",
            "If you think it's a little bit weird to be reasoning about having T9 hormones.",
            "Think of a more naturalistic task where your reasoning about whether something is good to eat or whether it's poisonous, or whether it wants to eat you.",
            "These are very ecologically important reasoning tasks, which again, we typically get very sparse data on.",
            "You want to be able to infer if some which things are going to be poisonous from as little data as possible.",
            "But here psychologists have used these so-called blank predicates to be as in some sense.",
            "Knowledge free as possible, except that the fact that they drawn some more abstract knowledge about biological species and properties, hence something like having T9 hormones and the idea is we just observe that new property for a couple of objects and we want to know how likely it is.",
            "Another things have this property.",
            "So somehow we need to innocence bridge from a lot of features that we can observe more densely to some new features which are less densely observed.",
            "Again, you can think of this as a kind of transfer learning problem.",
            "But now where these objects instead of being independent draws from a bag are actually related in some interesting way to each other.",
            "And you could also think of this as a kind of semi supervised learning problem, and that there's a few labeled examples and a lot of unlabeled data, and we want to learn some inductive bias to generalize the new property as well as we can from all the unlabeled data.",
            "And I think the techniques that we're developing here would have applications for semi supervised learning, and in fact an earlier version of this, I think in this Jerry Juice Semi supervised learning tutorial.",
            "Yesterday he talked about an early version of this with a tree."
        ],
        [
            "Just prior, so here's the idea of of the model.",
            "It's again a hierarchical Bayesian model where the lowest level, what we're called, the level of the data, is the stuff I already told you about features you can observe for these animals and new features that you want to generalize from sparse data.",
            "And then we're going to.",
            "We're going to try to address that problem with two levels of higher level background knowledge.",
            "The most immediate level up what we call the structure.",
            "Is just some kind of relational system over these object categories, which which we can use to interpret the data we see.",
            "So for example, a taxonomic tree over animals which we might expect that this some.",
            "You know, this new feature is somehow going to be for example smooth with respect to this taxonomic tree.",
            "Things nearby in the tree are likely to have the same values of these features, and we're going to.",
            "We're going by defining a prior on the data given the structure of probabilistic model of what data we expect to see given a certain say tree structure.",
            "We'll be able to use this to give us appropriate inductive biases for generalizing this new property for figuring out which other things have this property, and we can also work backwards from all the observed features that we see to infer this tree structure.",
            "Of course we have to have some kind of higher level knowledge.",
            "For example, we have to know we're looking for a tree in order to infer a tree like this, so will also specify what we call the level of structural form which specifies the kind of relational system we're learning with, and will even extend the hierarchical Bayesian model to make inferences at this level.",
            "To be able to infer that, say, one domain has a tree structure, another domain has a different kind of structure, and that's going to be important, because not every domain or every set of properties is tree structure."
        ],
        [
            "Alright, so.",
            "I'll just start and work up from the bottom here.",
            "How do we define the probabilistic model of the data given structure?",
            "Well, the intuition is like I said, is that we want a prior which in a sense is going to be flexible.",
            "It's going to give any way of labeling the objects.",
            "Here I'm just labeling them black or grey.",
            "Any way of labeling objects some prior probability, but ones which are smooth with respect to the graph structure should have higher prior probability.",
            "This captures the basic idea of generalization by similarity."
        ],
        [
            "Where the structure determines similarity and to do this, we're going to use build on some nice work of Zoo, Lafferty and Ghahremani, which probably most people here are familiar with, so I won't really go into detail on it, but the basic idea is to define a Gaussian field, a set of over the graph, a set of continuous real valued quantity quantities at each node of the graph, reflecting sort of how much that feature is associated with that point of the graph and then.",
            "We use the structure of the graph to parameterise the covariance of this Gaussian field.",
            "So how feature associations vary between as you go across the graph.",
            "The idea the intuition is that points which are closer in the graph should have higher covariance.",
            "You can think of that also is like saying that this feature sort of varies like a like a smooth random walk or a diffusion process over the graph.",
            "So for example, here's 1 sample from this Gaussian field.",
            "Here's another one, the math behind it, as again, many of you are probably familiar with is just.",
            "We take the regularised version of the graph Laplacian and invert that to give the covariance of this Gaussian process."
        ],
        [
            "Or the kernel if you like.",
            "And then if we want to turn this into a distribution on feature extensions or ways of labeling objects according to whether they have a binary feature or not, then we can just threshold this."
        ],
        [
            "Tenuous Association.",
            "Now.",
            "Now, in order to use this distribution, we want to do two things.",
            "We want to understand how we can learn the right structured representation of the domain and how we can use it to make inferences from very few examples of a new property.",
            "And the answer to those two questions is basically the same.",
            "So to infer the right tree structure, we assume that each of the observed features are generated from this Gaussian process over that over the tree, and then we just find the tree that best fits the observed features.",
            "So you can think of it as a kind of Bayesian hierarchical clustering.",
            "I mean, yet another Bayesian hierarchical clustering.",
            "We find the tree that you know on average, makes the observed features most likely under that smooth process."
        ],
        [
            "But the method isn't just restricted to trees.",
            "We can use other kinds of inductive biases.",
            "So, for example, here's a tree.",
            "Here's the tree that the best tree that was found for this 50 animal."
        ],
        [
            "80 feature set.",
            "But we can also try a different kind of inductive bias if you like a different kind of structure.",
            "For example, we could search for the best 2 dimensional embedding, where now again we have some way of determining covariance based on proximity in this 2 dimensional space.",
            "And you can think of it more or less as a limiting case of the graph based approach when you take when you take a mesh and take the mesh size to zero and we can see well which of these kinds of both of these are structured representations of the."
        ],
        [
            "Main that makes sense of the observed feature matrix.",
            "They both do a kind of compression or dimensionality reduction.",
            "We can ask which provide the best inductive bias for generalizing new properties.",
            "So that is we now take assume that this new property.",
            "Is the complete extension of this new property which we don't observe right?",
            "We only observe a few samples.",
            "The complete extension is a random draw from that same Gaussian process over the graph, and we're going to use that as a prior for how to fill in.",
            "The UN observed."
        ],
        [
            "Values.",
            "And here's the result on 2 classic datasets.",
            "So what what I'm plotting here are the two datasets.",
            "Are the two columns here and each point data point corresponds to a different inductive generalization, a different one of these arguments.",
            "The ones over here are have horses as their conclusion.",
            "So basically were, and they very very the objects which are the examples.",
            "So for example, this point is is the argument given that cows and elephants have a property.",
            "How likely is it that horses have the same property?",
            "Another points here correspond to different sets of examples over here.",
            "The conclusion is a general one.",
            "The question is, how likely is it that all mammals have a property given 3 examples of things which have that property and the X axis are the model predictions and the Y axis are human judgments.",
            "So what you see here is a case where you have a model that's highly correlated with peoples judgments.",
            "Generalizations at the model thinks are strong like this one are also ones that people think are strong and once, and they also agree on which ones are weak and.",
            "These two plots here correspond to the predictions of this Gaussian process defined over a tree structure.",
            "The ones here correspond to the predictions of the Gaussian process defined over 2 dimensional space and what you can see is a substantial difference that restructured model predicts people's judgments significantly better than the two dimensional one learned from the."
        ],
        [
            "Same feature matrix.",
            "Now we can.",
            "We can explore other kinds of inductive biases.",
            "In a sense, what what?",
            "I would argue here is that the tree structured model is the correct or more more more of the correct inductive bias for biological species and properties, whereas the two dimensional space isn't really the right one.",
            "If you think about evolution and mutation and selection, you're going to get structures of species and distributions of properties that more or less have a tree distribution, and people in some sense it seems, might be picking up on that.",
            "But we can try other kinds of priors.",
            "For example, what we call the rock covariance model, where there's no dimensionality reduction, there's no compression, we just take that raw feature matrix of 80 features and we just take take that, take that as the kernel basically.",
            "So we take take the inner products in that high dimensional feature space, and what you can see there is that model also doesn't do very well.",
            "I would say this is basically because that model doesn't have enough inductive bias, it has some inductive bias because the features were not completely random, they were meaningful.",
            "But it doesn't have the.",
            "The stronger inductive bias you get from compressing down to a tree structure, we can take another kind of model, which is what we call the strict tree model where you take the same tree used here, but instead of having that smoothness process the Gaussian field.",
            "We assume a simpler hypothesis space of just cuts of the tree, so each each feature has to strictly cut the tree at some point and you can see that model doesn't do well either, but in the opposite way it's bias is too strong.",
            "It gives too many arguments.",
            "The same strength like you see these vertical stripes, whereas people are distinguishing between more and less representative sets of examples.",
            "So I think this is pretty good evidence that when people are making their generalizations, they're using an inductive bias which is well tuned to this.",
            "This particular kind of domain.",
            "And it's sort of not too strong and not too weak."
        ],
        [
            "But just just right.",
            "Now of course, it's part of that argument would be we have to say that you know, if people are really using an inductive bias which is tuned to the domain, then there should be other domains which are not naturally tree structure, in which people would be using something else for.",
            "So here's an.",
            "I mean that we've looked at many different kinds of tasks in domains.",
            "Here's an example of one which we think.",
            "It should clearly suggest more of a 2 dimensional model.",
            "It's a kind of a geographic inference task.",
            "We said to people subjects given that a certain kind of Native American artifact has been found in sites near City X.",
            "How likely is the same artifact to be found near City Y and then we used various American cities?",
            "People would be familiar with and we can build the best 2 dimensional.",
            "Representation of peoples mental space.",
            "But by getting judgments of distances between cities and also the best tree structure model.",
            "And here we can see the two dimensional space does much better people predicting people's judgments again as we would hope if people are using a prior, that's the right one.",
            "And given this truck."
        ],
        [
            "Sure of the relevant part of the real world.",
            "So this brings us to the last part of this case study, which is how do we?",
            "How do we make this higher level inference?",
            "How do we figure out from data about features and properties and objects in some domain, not only what's the right structure of a particular form, like the right tree, but the right kind of structure that a certain domain is tree structured but some other domain is, say chain structured or spatially."
        ],
        [
            "Structured or other kinds of things just to motivate this a little bit.",
            "It's as as as an important problem in both cognitive development as well as the history of science.",
            "Consider example.",
            "Like this, we might think of biological species as following some kind of a tree structure, but and the idea that they would follow some kind of a wonder."
        ],
        [
            "Chain kind of silly, but exactly this was the dominant mode in Western European thought.",
            "Say roughly between in the Middle Ages up until about 1700 the work of Linnaeus and colleagues, the so-called great chain of being, which puts plants and rocks down at the bottom and got up at the top end and animals up to men and even Angels in between."
        ],
        [
            "So there is there some kind of structural form discovery going from, say the Middle Ages in the early modern period to Linnaeus is natural system up to Darwin, where this tree structured model gradually emerges as the dominant way to think about the natural world.",
            "To take another example from history of Science, Linnaeus and other contemporaries discovered the periodic structure of the chemical elements are more abstract structural form then then just the particular relations between elements, but they could use that then to fill in.",
            "New properties of new elements or to infer elements which had never even been seen before.",
            "The same kind of abstract learning about sexual form is critical in children's cognitive development.",
            "So, for example, learning that category labels are not just a discrete partition like we assumed in the first part of the talk, but that they actually have some kind of hierarchical structure.",
            "That's key for learning about real human categories, or, for example, learning that temporal categories like the seasons are the days of the week off and have a cyclical structure or learning about transitive.",
            "Comparative relation so learning that, for example, you know a relation like bigger than or better than or taller than, but all of those can be represented as an as a one as values along A1 dimensional order.",
            "Children go through discrete conceptual discoveries when prior to some age they don't have this abstract knowledge of the structural form of some domain and then later on they do so.",
            "We want to know."
        ],
        [
            "How they can do it now?",
            "There's lots of models that we're all familiar with for doing unsupervised structure learning, but for the most part they assume that you know the right structural form and what we don't have is what seems to be going on in human cognitive development.",
            "In science.",
            "Some kind of a universal structure learner.",
            "If you like this is I mean admittedly ambitious, but something which replaces all these different algorithms which learn just a structure of a particular form with a method that's able to find the right form of structure as well as the best structure of."
        ],
        [
            "Particular form that's best suited for a particular domain, so I'll just tell you since time is almost up just a little bit about our initial work on this problem.",
            "The first step here is to give some kind of hypothesis space of different structural forms that we might be able to learn with either thinking about what might be going on for a human child or scientific discovery setting, and we're going to work with different structural forms that can be described by simple generating processes using what are called context free graph grammars.",
            "These are.",
            "So called node replacement grammars, where the idea is that these are these are we have a hypothesis of rules for growing structures, graphs, basically by taking a note and it's in and out links and replacing it with two other nodes connected somehow and specifying how the in and out links of the seed node go get assigned to the new nodes and just by taking these simple rules, simple generative processes for graph structures we can grow.",
            "Many of the forms of structure that show up in basic models for unsupervised learning.",
            "So flat clusters or partitions, dominance orders, chains, rings, hierarchies, trees with latent nodes by taking cross products of these production rules we can take we can grow higher, more complex structures, like by taking a cross product of 2 Chainz we can get a 2 dimensional space or chain in a ring.",
            "Give us a Taurus sorry cylinder."
        ],
        [
            "Can get a Taurus by taking two rings and so on, and then the idea is to use these graph grammars to specify a hypothesis space of structural forms and do inference at all levels of this hierarchy.",
            "I don't have time to tell you all the mathematical details or actually any of them, but the basic idea is that, well, the probabilistic model for this level is the same one I told you about the smoothness model of you at all, and at this level here there's a fairly simple kind of Bayesian Occam's razor going on, where this graph specifies a prior over possible.",
            "Sorry, this grammar specifies a prior over possible graphs.",
            "And there's there's going to be a preference for grammars that generate fewer graphs with fewer nodes and fewer links because they have fewer alternatives.",
            "Just the usual Bayesian Occam's razor.",
            "And then we're going to do joint inference at both of these level."
        ],
        [
            "So the hierarchy.",
            "So here are a few results on just various kinds of psychologically plausible real world datasets.",
            "So here's a here's a result for the animals domain taking the same kind of data I showed you before.",
            "It's a different data set, but again, you start off with a matrix at this level.",
            "Here a matrix of animals and features.",
            "And what you learn is that you learn here we're showing the best tree.",
            "But what this represents is that the best structure found for this data set is a tree, as opposed to a chain or or a cylinder or two dimensional space.",
            "And so on.",
            "And you can see that it's learning a reasonable, psychologically reasonable tree structure taxonomy, but it's nice to know that this model is capable of recovering what we think is the actual right structure of the domain.",
            "Of course, it's not required to learn a tree structure, and here we have a different kind of domain.",
            "These are this is a domain of political opinions.",
            "Supreme Court votes.",
            "For all the judges who served in the Rehnquist courts, this is sort of 80s and early 90s.",
            "And what we what the model recovers is a chain of 1 dimensional structure, roughly corresponding to the liberal conservative spectrum with Thurgood Marshall and Brandon over here on the left, Scalia and Thomas over there.",
            "On the right, you know, that's our familiar left, right liberal conservative spectrum, but not, but the models made a higher level abstract discovery of structural form, namely, that this one dimensional spectrum is the right way.",
            "To organize this domain or the best, most compact way of thinking about it logically, it could have been a tree structure, or it could have been a 2 dimensional space like this is a 2 dimensional space recovered from face images varying and a racial and gender dimension.",
            "They were these were realistically synthesized to actually have these two latent dimensions.",
            "But the model here is able to recover that as well as say, to recover this some cylinder of latitude and longitude for cities and."
        ],
        [
            "Other circle.",
            "Since I'm basically out of time right, I will skip the nice stuff about relational data, but we can do that."
        ],
        [
            "Same kinds of things with relational data and even study how people are able to learn structural 4."
        ],
        [
            "On relations.",
            "I'll just make one last point, because it's just relevant for the general conclusion.",
            "I want to make, which is what happens if we look at how these structural forms develop.",
            "As we observe more and more data.",
            "So here we're back to the animals domain and we're going to see what have we learned when we just take 5 features, 20 features, or the full data set of 100 or so features, and what you can see is something kind of interesting here, but with with just five features, we don't.",
            "We learn a more simple structural form, we just learn a set of flat clusters.",
            "They're reasonable clusters.",
            "They're sort of large scale, superordinate categories like birds and insects and carnivorous mammals and herbivore mammals and aquatic creatures and so on, but we don't have the full hierarchical structure.",
            "Now when you add in a few more features now the model realizes that the extra complexity of that restructured taxonomy is licensed, and it doesn't for the best structural form is a tree, but it doesn't have the right tree, so you can see over here it has a tree, has a node with Dolphins, seals, whales and Penguins.",
            "And we know as adults that Penguins don't belong with Dolphins, seals and whales.",
            "Rather they belong something like over here.",
            "So this is what the tree you learn when you have the full data set, where now there's a distinct subtree for the aquatic mammals and Penguins are over here with the other birds.",
            "Although they're sort of closer to the fish and the aquatic mammals and what's what you see going on here is a phenomenon that we call the blessing of abstraction.",
            "It's kind of kind of overly dramatic name for what I think is a basic but really important phenomenon.",
            "Which is that often learning abstract knowledge is easier than learning concrete knowledge.",
            "The concrete knowledge here is the specific structure in the abstract knowledge is the structural form, and you often find when you work with hierarchical Bayesian models that because that less data is required to converge on the right, learns knowledge at the higher level than at the lower levels, and in this case that means the structural form, then the, then the.",
            "Particular structure, and I think that's an important feature of cognitive development as well as science, where when we first learning about a domain we kind of get the big picture and sort of the general way of organizing things.",
            "And then it might take much more observation to fill in the details.",
            "Indeed, that's exactly what happened in scientific biology, and that's exactly what happens in children's cognitive development, where it was relatively early, say in the 1700s, where we figured out that some kind of tree structure is the right tree structure was the right form, but it took a long time and a lot more data to converge on."
        ],
        [
            "We thought was the right tree.",
            "And this is this is really a sort of a different perspective on what is maybe the most classic debate in cognitive science about the nature of learning, what's called the nativist versus the empiricist.",
            "And you see the same kind of debate in AI.",
            "But I'll just conclude by talking about the debate in cognitive science.",
            "It's long recognized that some kind of abstractions is important, but you have these two camps, the nativists who say that there is explicit knowledge of say.",
            "Structural form or we have explicit abstract knowledge, but that is not learned.",
            "Chomsky being the most famous example.",
            "And then you have the so called empiricist camp, which says, well, we can learn abstract knowledge, but it's not really explicit.",
            "We just sort of, it just comes out kind of implicitly from overlaying a lot of concrete experience.",
            "This is sort of the classic connectionist approach as well as traditional.",
            "I would say traditional structure learning and probabilistic graphical models where you're learning some graphical representation of the structure of a set of variables.",
            "But you're not learning.",
            "Some kind of higher level inductive bias or abstract constraint on the kind of structure that's there, and what I've tried to argue here is that both of these things are necessary.",
            "We really do need explicit structural forms 'cause you could see from modeling peoples ability to generalize from very few examples.",
            "You need that the strong inductive bias, but you also need the flexibility that comes from being able to learn and what we were trying to work towards here is really a third way or a new approach to thinking about cognitive development as well as how you might, say, engineer the development of."
        ],
        [
            "Official system, so I'll skip my little thing on causal learning.",
            "And just sum up.",
            "I've given you 2 case studies on how we can model human inductive learning by combining a few basic ideas.",
            "The idea of Bayesian inference over hierarchies of flexibly structured representations and and what in each of these cases we've looked at multiple levels of knowledge and tried to ask some of the big questions about cognitive science.",
            "What's the form and content of knowledge at these different levels of abstraction?",
            "How does that knowledge guide learning?",
            "And how might that knowledge itself be acquired in a way that combines both strong inductive biases but also flexibility?",
            "And there's sort of these bigger picture lessons, which I hope you know.",
            "Certainly in cognitive science, these are important, but I hope they will also pay off for people interested in machine learning and AI.",
            "This third way of thinking about development, the idea that in some sense, inductive biases, abstract powerful abstractions can be learned very quickly, sometimes in a kind of a top down way, rather than having to be wired in or learned very gradually and implicitly.",
            "And maybe the most general lesson, which I think many people in machine learning are already have already learned and taught others, including me that we need to go beyond some of the classic dichotomy's of thinking about intelligence.",
            "Dichotomy's of, say you know, domain specific hardwired approaches versus domain general learning based approaches or statistics versus structured representations and ask really more integrated questions.",
            "For example, how does?",
            "How could domain general learning mechanisms build domain specific knowledge representations?",
            "Or how could statistical learning?",
            "Lead to structured knowledge representations that provide the inductive biases we need to get around in the world.",
            "I think there's a bright future in cognitive science as well as an AI in exploring these ideas, thanks.",
            "Like what are you thinking about exactly?",
            "Yep.",
            "Yeah, I mean.",
            "In some sense.",
            "We're mostly, you know.",
            "Our first step is to try to get it right, and then we'll worry about fooling the models.",
            "So, for example, one of the ways we tested this structural form discovery was to generate synthetic datasets by taking features distributed over different kinds of graph structures and showing we could recover that.",
            "We haven't yet gone to the stage of saying, OK, well, what kinds of things are going to fool the model into thinking that there's this kind of structural form, when in fact there's this other kind.",
            "I guess you could say in some sense that was what was going on here.",
            "Right, you could say when when we just have a small amount of data, the model gets fooled into thinking the world is qualitatively simpler than it really is, that there's actually just a set of flat clusters rather than a more complex tree structured hierarchy, and I guess we could say in some sense children are fooled in the same way that children's early.",
            "Categories are follow what's called the principle of mutual exclusivity.",
            "Developmental psychologists have named it this because they assume that something can just be in one nameable category rather than, say, a whole hierarchical taxonomy.",
            "I don't know if that's exactly what we're looking at.",
            "In general, I do think that's a promising approach that any kind of inductive bias is going to lead to some kind of illusions of inference, right?",
            "Like optical illusions and vision, which can show you that inductive bias at work and.",
            "By studying that both in a computational model as well as in an actual human learner, you can start to see if the forms of inductive bias that you're putting into your model correspond to those that the human might have.",
            "That's right, Joe.",
            "Yeah.",
            "Yeah.",
            "Select models.",
            "Well, yeah, that's a really good question.",
            "It's very interesting that you know in the history of AI we often see.",
            "Those two things they don't always line up well.",
            "They often line up in intriguing ways, right?",
            "Like, for example, you know a Bayes net might be if you try to decide which of two bayonet structures is the best way of representing some set of statistics.",
            "If there's really some kind of causal structure there, a Bayes net which respects the right causal order will tend to be simpler model.",
            "It will be more productive and it will tend to be more efficient because there will be fewer loops in it and there's lots of other examples of that.",
            "Trees are both good inductive biases, but they're also computationally efficient.",
            "But they don't have to line up.",
            "We've mostly focused on what in cognitive science in the tradition of David Marr.",
            "Is called the kind of computational theory level.",
            "Like really looking at the abstract constraints on inference rather than the processes.",
            "But there's a whole other approach you could follow by, for example, measuring reaction time or number of trials to learn things which really push the human processing engine to its capacity limits, or approve that, and then try to see you try to try to.",
            "Look at that other side of things, but we, but we haven't looked at that.",
            "Although you know my, my guess is that, well, I guess I'm just I'm sort of in a long range, but backburner sort of way obsessed with this interesting correspondence between computational efficiency and kind of inductive efficiency, and I'd like to understand more better how those lineup.",
            "Well David yeah.",
            "Yeah.",
            "Turns out not.",
            "Tell you something about.",
            "What is cognitive level?",
            "You mean evolution of cognition?",
            "Yeah, I mean, right, it's it's.",
            "At this level of abstraction that we're modeling here, you know.",
            "It's not.",
            "I mean, this could just as well apply to evolution of cognition as it could to learning, but I don't think, well, I don't think that's that's the main thing we're studying here.",
            "So in one of the slides I had to skip at the end we've actually been looking at how people learn structural forms like this in the lab, and people can infer different kinds of structural forms from relatively short experience of observing objects and properties and relations, and then use those inductive biases to generalize to new things with only sparse experience, so they can learn in a relatively short time scale of minutes.",
            "These kinds of things if you if you sort of prepare their mind and give them the right sort of data.",
            "I think also just I don't think the debate between nativism and or you know nature versus nurture.",
            "It's not just about the time scale.",
            "In that, in the traditional approaches of both the sort of nature and nurture approaches, in both cases there's this, I think, implicit assumption that the more abstract knowledge is going to take longer to learn.",
            "I mean, both of them are really long timescales, right?",
            "So evolution is a long time scale, but the kind of gradual, many, many, many examples.",
            "Many trials sort of learning that you get, say in connectionism or in sort of typical Bayes net structure learning, where you need a lot of data to really learn anything relevant about the structure.",
            "Both of those are long and what's?",
            "Things that's most striking to me as I tried to emphasize throughout the talk, is this blessing of abstraction.",
            "The fact that these powerful abstractions can be learned relatively quickly, and that really seems to require a totally different approach than either of those two classic approaches.",
            "Rich bad science.",
            "So you look at the data.",
            "Yep.",
            "That well.",
            "Yeah, I mean, I think science is an example of what I was talking about about the fact that on the one hand we have strong biases, but we also have to keep an open mind in a sense.",
            "So I feel like I've been trying to do that, but of course you also have to do things like write papers and market your work and give talks and so that lends a certain rhetorical bias.",
            "I tried to show for example in one of these.",
            "Things back here.",
            "And this slide only represents a small portion of some of the work we've done in taking these more quantitatively rich domains where you can get people to make lots of different inductive judgments about a set of objects.",
            "We've we've explored many, many different kinds of models.",
            "It's all broadly within a kind of Bayesian approach, but but for example, we've been.",
            "We've been at pains to explore qualitatively more or less kinds of structure.",
            "So, for example, this raw covariance thing.",
            "It's essentially basically just to kind of learning feature relevance, I mean.",
            "In a way that's almost sort of hand coded in because there's a set of features that people judged as the relevant ones for biology, and essentially what you can see is that that's not sufficiently strong inductive bias.",
            "The you know in some sense you could see the kind of either of these more structured approaches, like learning a tree, structured representation, or ultra low dimensional space as a more sophisticated kind of learning feature relevance where basically it's filtered by the structural constraint.",
            "So when you learn that restructured model, what essentially you've learned, I think I would say it's kind of.",
            "It's it's.",
            "It's rather similar to the idea of learning that just the features which best fit the tree structure are the relevant ones for guiding generalization, and I think if you just filtered out the features which are best explained by the tree and then just use the raw covariance kernel on those, it would be very similar.",
            "Well, except that there wouldn't be enough of them, but it would.",
            "It would if you had a huge set of features and you just filtered out the best ones.",
            "That would be very similar, but partly what I've tried to show here by comparing, say, the different kinds of structured representations is that it isn't enough just to say you somehow filter out the right ones, because at least in this case, there's a qualitative difference between using the tree structure to constrain if you like, which features are relevant versus using other kinds of structures, and for other kinds of domains, it comes out differently.",
            "So I think you know I try to keep an open mind about these things and I can only fit so much into talk, but but I agree that we should.",
            "We should be considering both richer and less rich representations.",
            "I've been led to some of these richer ones by the need to try to explain how we can learn so much from so little.",
            "But I think we we should constantly be checking our inductive bias as you said.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome and.",
                    "label": 0
                },
                {
                    "sent": "I'm very pleased to introduce Josh Tenenbaum, our invited speaker this morning.",
                    "label": 0
                },
                {
                    "sent": "Josh studied.",
                    "label": 0
                },
                {
                    "sent": "Physics is an undergraduate at Yale and then came to the Department of Raven Cognitive Sciences to do his PhD.",
                    "label": 1
                },
                {
                    "sent": "Where he studied computational models of human categorisation.",
                    "label": 1
                },
                {
                    "sent": "And generalization and.",
                    "label": 0
                },
                {
                    "sent": "From MIT judgment onto effective position at Stanford, he was a sample for a few years, but I guess he missed life on the East Coast and MIT and all that.",
                    "label": 0
                },
                {
                    "sent": "So he came back to MIT where he's.",
                    "label": 0
                },
                {
                    "sent": "Active member in the Brain Cognitive science Department.",
                    "label": 0
                },
                {
                    "sent": "Josh is really a pioneer of.",
                    "label": 0
                },
                {
                    "sent": "I think it's really exciting new era in cognitive science, which is taking computational models.",
                    "label": 0
                },
                {
                    "sent": "And statistical models very seriously.",
                    "label": 0
                },
                {
                    "sent": "And there is a whole cadre of young researchers who are pushing cognitive science in this direction.",
                    "label": 0
                },
                {
                    "sent": "And Josh is made many contributions to this area in categorization, generalization, causal inference, and just a variety of other important areas in computational cognitive science, which is really probably the best label for what he does.",
                    "label": 0
                },
                {
                    "sent": "But being so close to the machine learning community and using so many of the common ideas from information theory and statistics to inform computational models of cognition, Josh is also made important.",
                    "label": 0
                },
                {
                    "sent": "Contributions for machine learning.",
                    "label": 0
                },
                {
                    "sent": "So it's great to have him a nice email.",
                    "label": 0
                },
                {
                    "sent": "So for example, his work on Isomap for nonlinear dimensionality reduction was a very well known paper that was published in science.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm just thrilled to have Josh here.",
                    "label": 0
                },
                {
                    "sent": "It's always really fun to hear him talk and tell us about computational models of human cognition, so that's all.",
                    "label": 0
                },
                {
                    "sent": "Thanks, Josh for making it here.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me?",
                    "label": 0
                },
                {
                    "sent": "Yep, thank you very much for having me here.",
                    "label": 0
                },
                {
                    "sent": "I think this is my first time at ICL.",
                    "label": 0
                },
                {
                    "sent": "I've read of course many ICL papers, but I don't think I've actually been to the conference before, so it's great to be here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just start off by acknowledging a bunch of lab members, including one former lab member, Tom Griffiths, who's now on the faculty Berkeley.",
                    "label": 1
                },
                {
                    "sent": "I just want to all of these people have made some contribution to the research.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "I want to highlight the Charles camp over there in Red because most of the good ideas and hard work in what I'm going to talk about come from Charles's thesis.",
                    "label": 0
                },
                {
                    "sent": "He's just about to graduate, and then he'll be starting a faculty position at CMU.",
                    "label": 0
                },
                {
                    "sent": "So if you like these ideas, you might look him up.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think I need to tell this audience about the probabilistic revolution in machine learning and AI more generally across pretty much all subfields of AI, maybe most prominently in machine learning, but including vision, robotics, natural language, expert systems, reasoning, planning the fields been revolutionized by the adoption of sophisticated mathematical techniques that give principled, and in some cases sufficient and certainly effective methods for dealing with uncertainty, in particular for.",
                    "label": 0
                },
                {
                    "sent": "Trying to make inductive inferences from very, very ill posed problems from very ambiguous data, but if you look at you know I would say what if you sort of did an informal survey of the leading practitioners at the state of the art of probabilistic AI and machine learning.",
                    "label": 0
                },
                {
                    "sent": "Most of them would say that they don't think there's any necessary connection between the math of these probabilistic methods and the way the human brain actually learns.",
                    "label": 0
                },
                {
                    "sent": "And increasingly as statistical machine learning has become more mathematically sophisticated, at least up until a few years ago, there's been less and less of a connection between the field of machine learning in the field of human learning.",
                    "label": 0
                },
                {
                    "sent": "To me, that's too bad because of someone who sits between these fields as open was saying.",
                    "label": 0
                },
                {
                    "sent": "I think these fields have a lot to learn from each other.",
                    "label": 0
                },
                {
                    "sent": "I hope I'm not the only one I wanted to start here with the quote that Zubin wrote me over email when he was inviting me to speak and we were talking about topics.",
                    "label": 0
                },
                {
                    "sent": "He said many people in machine learning get into the field because they are interested in how humans learn rather than how convex functions are optimized and how we can get machines to be more like humans.",
                    "label": 1
                },
                {
                    "sent": "So I hope that view is representative of at least much of the audience here and I hope.",
                    "label": 0
                },
                {
                    "sent": "Will be some interesting insights to be to be had.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From this interdisciplinary discussion.",
                    "label": 0
                },
                {
                    "sent": "Now the particular problems that we're working on here or what we like to call everyday inductive leaps.",
                    "label": 1
                },
                {
                    "sent": "These are problems where sort of essential core problems of human learning, where people seem to learn so much about the world from such.",
                    "label": 1
                },
                {
                    "sent": "Apparently such limited evidence.",
                    "label": 0
                },
                {
                    "sent": "Classic examples that I've been studying for awhile, as even mentioned, are basically problems of concept, learning and categorization.",
                    "label": 0
                },
                {
                    "sent": "So to take a paradigmatic example, consider a child learning their first words like this is sort of a rough reconstruction of my daughter.",
                    "label": 0
                },
                {
                    "sent": "Learning the word horse when she was a year and a half old, we went out to the country.",
                    "label": 0
                },
                {
                    "sent": "For Thanksgiving, western Massachusetts Ann from just a few experiences like this, seeing a horse paired with an excited label from one of her parents horse, she was able to grasp this concept and pick out if not perfectly, almost perfectly, other horses.",
                    "label": 0
                },
                {
                    "sent": "You know she might be confused about the occasional cow, or or camel, but more or less she got this concept from just one or a few examples.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about it just with a simple mathematical abstraction, there's some you know.",
                    "label": 0
                },
                {
                    "sent": "Basically infinite set of objects out there in the world.",
                    "label": 0
                },
                {
                    "sent": "It really is infinite if you think of all the horses you could draw, and even of course this concept is going to generalize to drawn horses as well.",
                    "label": 0
                },
                {
                    "sent": "So there's this infinite set of all objects you could perceive, and there's an infinite subset of that which you could say is the subset of horses and somehow a child learning a word like this is able to grasp more or less the boundaries of that set from just one or a few.",
                    "label": 0
                },
                {
                    "sent": "Randomly chosen points within the set.",
                    "label": 0
                },
                {
                    "sent": "That's a stunning achievement.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to understand how it could work in case you forgot what it's like to be a child learning a word, let me give you an example of a sort of a demo of experiment we've done in our lab where we tried to put human adult in this same kind of situation.",
                    "label": 0
                },
                {
                    "sent": "So you're on the planet zoo.",
                    "label": 0
                },
                {
                    "sent": "These are some of the guys, Ubi and objects, and we're going to give you a few examples of various words in the Gazoo being language.",
                    "label": 0
                },
                {
                    "sent": "So let's show you a few tufas here now, even I think, I think if I'd only just giving you an example and certainly giving you 3 examples here, though, you've never seen the word two before or seen most of these kinds of objects.",
                    "label": 0
                },
                {
                    "sent": "Before you have a pretty good idea of what things are two phase in which aren't, so let's try this out for the audience participation part of the lecture.",
                    "label": 0
                },
                {
                    "sent": "Just yell out.",
                    "label": 0
                },
                {
                    "sent": "Is that a 2 for yes or no?",
                    "label": 0
                },
                {
                    "sent": "Little more uncertainty there.",
                    "label": 0
                },
                {
                    "sent": "That's good because our model predicts that all right.",
                    "label": 0
                },
                {
                    "sent": "So you get the idea.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of generalizing from a few examples isn't just about learning words and concepts, although that's one of the main places where it comes up, but it also comes up and we've studied it in other contexts, such as learning causal relations, relations between cause and effect.",
                    "label": 0
                },
                {
                    "sent": "What cognitive scientists called theory of mind learning about the internal mental states, beliefs, goals, plans of other agents from observing their behavior or, for example, learning intuitive learning intuitive theories that govern our psychological roller, social world, social rules and conventions.",
                    "label": 1
                },
                {
                    "sent": "Intuitive physics.",
                    "label": 0
                },
                {
                    "sent": "All of these are versions of this kind of problem, just to give one other example, which will come back to time permitting at the end, learning cause and effect.",
                    "label": 0
                },
                {
                    "sent": "So we all learn in our most basic statistics classes that you can't infer necessarily causation from observing the correlation right.",
                    "label": 0
                },
                {
                    "sent": "There could be confounders and hidden variables and things like that directionality isn't so clear with recent developments in causal Bayes Nets.",
                    "label": 0
                },
                {
                    "sent": "Arguably there are some cases now where you can infer causation from correlation, and there are modern kinds of techniques for controlled experiments which allow you to do this under intervention.",
                    "label": 0
                },
                {
                    "sent": "But think about the case of a child learning the causal relation or or inferences, we make all the time, where often just one or a few examples.",
                    "label": 0
                },
                {
                    "sent": "The right kind of pattern of suspicious coincidence can strongly suggest the causal relation when again from just one or a few data points.",
                    "label": 0
                },
                {
                    "sent": "You couldn't even computer reliable correlation, right?",
                    "label": 0
                },
                {
                    "sent": "So the problem isn't going from correlations to causation, but rather observe or inferring causal structure.",
                    "label": 0
                },
                {
                    "sent": "When you, when you have so little data, you can't buy at least conventional statistical means, even computer reliable correlation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can you solve this problem?",
                    "label": 0
                },
                {
                    "sent": "Well, in some sense the answer is pretty simple and we all know what it is right?",
                    "label": 0
                },
                {
                    "sent": "If you're learning a lot from a little data, it must be because in some sense you already knew a lot.",
                    "label": 0
                },
                {
                    "sent": "You must have some strong prior knowledge or some inductive bias.",
                    "label": 1
                },
                {
                    "sent": "This is a lesson that the field of machine learning has learned and taught quite well, right?",
                    "label": 0
                },
                {
                    "sent": "Pretty much any paradigm for machine learning has some version of the idea that there's no free lunch that if you have no constraints on your hypothesis space, you can't learn anything interesting that goes beyond data.",
                    "label": 0
                },
                {
                    "sent": "And the bigger the in principle, hypothesis, space and the less data you have, the more constraints, the stronger the inductive bias you're going to need.",
                    "label": 0
                },
                {
                    "sent": "But of course that isn't really satisfying, because if that's all there was to it, then we already have human like machine learning systems which.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We don't for the most part, so really this is just the opening of a number of questions which we want to ask exactly how is it that abstract, background knowledge, inductive bias, guides, learning from sparse examples?",
                    "label": 1
                },
                {
                    "sent": "In the cases that I've talked about?",
                    "label": 1
                },
                {
                    "sent": "What form does knowledge take?",
                    "label": 0
                },
                {
                    "sent": "What is its content and representational form across different tasks and domains?",
                    "label": 0
                },
                {
                    "sent": "This is a hard problem and one that a lot of cognitive science is devoted to try to study because people most the kinds of inductive biases, implicit knowledge people have that are constraining their generalizations.",
                    "label": 0
                },
                {
                    "sent": "A lot of it is quite implicit and unconscious.",
                    "label": 0
                },
                {
                    "sent": "You can't just ask somebody, so what's your inductive bias exactly here?",
                    "label": 0
                },
                {
                    "sent": "But that's often what we're most interested in is explaining how they can do these tasks.",
                    "label": 0
                },
                {
                    "sent": "So we need techniques that can elicit and formally represent the kinds of prior knowledge people bring to these tasks.",
                    "label": 0
                },
                {
                    "sent": "Maybe the most interesting kinds of learning questions are how is this abstract prior knowledge itself learned or acquired?",
                    "label": 0
                },
                {
                    "sent": "Now, of course, inductive bias doesn't have to be learned in principle, it doesn't have to be.",
                    "label": 0
                },
                {
                    "sent": "And if you look kind of classically in machine learning as well as in cognitive science, I think that sort of default assumption is that it's not like inductive biases are these hardwired constraints we designed by hand into our learning algorithm in machine learning, or the people who've most forcefully advocated for inductive biases and cognitive science.",
                    "label": 0
                },
                {
                    "sent": "People like Chomsky and lists Pelkey and others in the nativist camp, saying that there are just the mind comes into the world through genetic specification to have very strong constraints on what it can learn and what it can expect.",
                    "label": 0
                },
                {
                    "sent": "And that's why we're able to learn so much from so little.",
                    "label": 0
                },
                {
                    "sent": "But I think it's clear that in many cases, inductive biases in human learning have to be learned.",
                    "label": 0
                },
                {
                    "sent": "There are many reasons for this.",
                    "label": 0
                },
                {
                    "sent": "One is, as I'll show you in a minute.",
                    "label": 0
                },
                {
                    "sent": "There are some very compelling examples of cases where human children at a certain age, maybe age 2.",
                    "label": 0
                },
                {
                    "sent": "Have some very important inductive bias, but but slightly before that they don't have it.",
                    "label": 0
                },
                {
                    "sent": "And there's evidence that they can actually be taught.",
                    "label": 0
                },
                {
                    "sent": "These inductive bias is another important reason to focus on learning is that we can learn about new domains that we didn't have access to in our evolutionary time.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can learn chemistry and then acquire new inductive biases for making inductive inferences about new chemical compounds, or to take a more intuitive example, you know we live in a very complex social system and there are different societies follow different kinds of rules.",
                    "label": 0
                },
                {
                    "sent": "Many of which didn't have any precursor in in our evolution existence.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the same kind of population density and size of cultures and complexity of cultures.",
                    "label": 0
                },
                {
                    "sent": "Yet when you grow up in this culture, you're able to learn lots of implicit knowledge of how your culture works, and then you use that to make very rapid and for the most part, reasonable inferences from very sparse data about new people you meet in new situations that you find yourself in.",
                    "label": 0
                },
                {
                    "sent": "And this brings us to the last question, which I think is particularly interesting from a machine learning point of view, as well as a human learning one, which is, it seems like there's a tension between these two aspects of human learning.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, and really, these are distinctive aspects of human learning which you don't see necessarily so much in other natural animal learning systems.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, we have these very powerful abstract constraints, inductive biases that allow us to learn so much from so little.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it seems like we can learn new ones.",
                    "label": 0
                },
                {
                    "sent": "We're very flexible in the inductive biases we can.",
                    "label": 0
                },
                {
                    "sent": "We can come up with when you go to a new culture.",
                    "label": 0
                },
                {
                    "sent": "You can pick up how that culture works.",
                    "label": 0
                },
                {
                    "sent": "You can actually learn things in school, and it seems like these might be in conflict, right?",
                    "label": 0
                },
                {
                    "sent": "If we have strong constraints which limit the hypothesis we can entertain, then how is it that we can then not only entertain hypothesis which aren't part aren't governed by those constraints, But actually sort of throw out those constraints and come up with a new set of constraints for a new situation we find ourselves in the goal in this work is to come up with a computational framework for addressing these questions.",
                    "label": 0
                },
                {
                    "sent": "And these are hard questions, you know.",
                    "label": 0
                },
                {
                    "sent": "We're just beginning to make progress, but I think we are starting to make at least some progress on these questions and hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be useful insights for machine learning.",
                    "label": 0
                },
                {
                    "sent": "Now the approach that we're working with builds on a bunch of ideas which are probably an aggregate familiar to this community, but we might be using them in some slightly different ways because we're working on slightly different more human motivated problems.",
                    "label": 0
                },
                {
                    "sent": "On the most basic technical ideas, Bayesian inference as you could guess from the title of the talk and in a sense, these technical ideas are each going to address these four big questions I had here.",
                    "label": 0
                },
                {
                    "sent": "So how does background knowledge guide learning from sparse data?",
                    "label": 1
                },
                {
                    "sent": "Well, if you are Bayesian priors are some sense derived from this abstract background experience, so that's a clear and powerful and totally general purpose way that background knowledge can guide learning.",
                    "label": 0
                },
                {
                    "sent": "But of course that doesn't really tell you very much anymore than just pointing to inductive bias.",
                    "label": 0
                },
                {
                    "sent": "We need more, more powerful, richer kinds of techniques, so one is to address the question of the form of knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "If you just look at a very simple Bayesian analysis, what is a prior?",
                    "label": 0
                },
                {
                    "sent": "It's a set of numbers, or it's a simple parametric statistical distribution, but that isn't knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "And we want to understand how rich knowledge representations can generate priors for Bayesian inference, so that requires us to look at more structured probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "For example, probabilities defined over graphs or richer things like grammars.",
                    "label": 1
                },
                {
                    "sent": "Relational schemas and so on.",
                    "label": 0
                },
                {
                    "sent": "If we want to address the question of how his background, knowledge and inductive bias itself learned well for that, we've been using hierarchical Bayesian models where you have multiple levels of probabilistic representation with safe above the level of the conventional prior, you have some kind of hyperparameters a prior on the prior and by looking at learning in those hierarchical models, we might be able to explain where peoples inductive biases come from.",
                    "label": 0
                },
                {
                    "sent": "By combining these techniques right, we can we can.",
                    "label": 0
                },
                {
                    "sent": "Learn inductive bias which are not just numbers of parameters, but actual structured knowledge.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, we want to understand this this tradeoff and maybe seeming paradox between constraint and flexibility.",
                    "label": 0
                },
                {
                    "sent": "For that we've been working with nonparametric Bayesian techniques where the complexity of the model isn't determined in advance but is able to grow as licensed as new data come in.",
                    "label": 0
                },
                {
                    "sent": "So new data come in and when the when appropriate, the model can make itself in the sentence, make itself more complex as needed.",
                    "label": 0
                },
                {
                    "sent": "But but but maintains the ability to have strong constraints for data that respect previously learned.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the outline for the talk is will be to go through three case studies in modeling human inductive learning, and we'll see how much exactly time there is for all three of those.",
                    "label": 1
                },
                {
                    "sent": "The last one will be somewhat abbreviated.",
                    "label": 0
                },
                {
                    "sent": "The first case study is on word learning, and the model that I'm going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "Is in some sense a very, very simple hierarchical Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "It's probably trivial to many of you with one sort of little twist at the end, which isn't so trivial, but it's worth going through is just to warm up for the basic ideas, and also because it allows us to address and really to talk about one of the most interesting phenomena of how humans learn induct.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Biases in childhood and the phenomena is what's called the shape bias in Word learning.",
                    "label": 1
                },
                {
                    "sent": "This is a phenomenon that was studied by a number of developmental psychologist starting in the mid to late 80s, and here's an example of it.",
                    "label": 0
                },
                {
                    "sent": "You take a 2 year old child who's learning to speak English and you show them something like this and you say this is attacks and then you show them three other things here and you say, can you show me the other decks?",
                    "label": 0
                },
                {
                    "sent": "Which one do you think they're going to choose?",
                    "label": 0
                },
                {
                    "sent": "Yeah, they'll choose the first one, the one that has the same shape as opposed to, say, one that has the same color but not the same shape, or one that has the same texture.",
                    "label": 1
                },
                {
                    "sent": "This is a sort of abstract rendering of texture.",
                    "label": 0
                },
                {
                    "sent": "They'll choose the one that has the same shape.",
                    "label": 0
                },
                {
                    "sent": "Mount 2 interesting things about this.",
                    "label": 0
                },
                {
                    "sent": "First of all, while two year old children in English will reliably do this, if you go back a few months before that, they won't.",
                    "label": 0
                },
                {
                    "sent": "So 20 month olds will be at chance.",
                    "label": 0
                },
                {
                    "sent": "There will be just as likely to give you any anything that matches on one of these feature dimensions.",
                    "label": 0
                },
                {
                    "sent": "So something interesting is going on there.",
                    "label": 1
                },
                {
                    "sent": "And the other point is at the shape bias is, it seems at least to be very useful.",
                    "label": 0
                },
                {
                    "sent": "Kind of inductive constraint.",
                    "label": 1
                },
                {
                    "sent": "Arguably the majority of early words are basically labels for categories of objects, and arguably the most reliable queue.",
                    "label": 0
                },
                {
                    "sent": "Two Object category membership is some notion of shared shape.",
                    "label": 0
                },
                {
                    "sent": "All balls are basically ball shaped.",
                    "label": 0
                },
                {
                    "sent": "All bottles are in some sense ball shape, although they might vary in color and material.",
                    "label": 0
                },
                {
                    "sent": "So where does this shape bias come from?",
                    "label": 0
                },
                {
                    "sent": "Well, one possibility at that.",
                    "label": 0
                },
                {
                    "sent": "Some people have proposed is that it actually derives from some kind of innate knowledge that then only somehow becomes manifest around this age, but there's some pretty compelling evidence that it is in fact learn.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a kind of abstraction from experience, so Linda Smith and colleagues have done a series of really nice studies looking at this, and here's one example of one of these studies.",
                    "label": 0
                },
                {
                    "sent": "So they took 17 month olds.",
                    "label": 0
                },
                {
                    "sent": "Kids were a year and a half.",
                    "label": 0
                },
                {
                    "sent": "Basically who don't don't already show the shape bias, and they gave them a kind of a training training curriculum that took place over 8 sessions once per week, so over two months each session was pretty short, only 20 minutes long.",
                    "label": 0
                },
                {
                    "sent": "And what did they do in these sessions?",
                    "label": 0
                },
                {
                    "sent": "Well, they took these.",
                    "label": 0
                },
                {
                    "sent": "Basically these these.",
                    "label": 0
                },
                {
                    "sent": "Eight novel objects, four pairs of objects which have within each pair basically the same shape, but they differ in color and texture and other features, and they gave these two 2 objects in the same category the same label, and played with them in a way that highlighted the labels.",
                    "label": 0
                },
                {
                    "sent": "So they would say, oh, here's your web.",
                    "label": 0
                },
                {
                    "sent": "And here's my web.",
                    "label": 0
                },
                {
                    "sent": "Let's play with our web.",
                    "label": 0
                },
                {
                    "sent": "So can you.",
                    "label": 0
                },
                {
                    "sent": "Can you show me your web?",
                    "label": 0
                },
                {
                    "sent": "Oh here's my web, OK?",
                    "label": 0
                },
                {
                    "sent": "That sort of thing, and they did that with the same 8 objects every every session.",
                    "label": 0
                },
                {
                    "sent": "Once per week.",
                    "label": 0
                },
                {
                    "sent": "There was also a control group which I'll come back to in a second who had the same objects but just didn't get the words.",
                    "label": 0
                },
                {
                    "sent": "So they're just like, oh, let's play with these toys.",
                    "label": 0
                },
                {
                    "sent": "Here's here's here's mine exactly, the same thing, but no label.",
                    "label": 0
                },
                {
                    "sent": "And what happened after 8 weeks of training?",
                    "label": 1
                },
                {
                    "sent": "The 19 month olds now had had acquire the shape bias, so now you can show them a new thing.",
                    "label": 1
                },
                {
                    "sent": "This is a Dax and show them these other ones again matching in different dimensions and they'll reliably pick out the one that matches in shape.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Well, Smith, another psychologist characterizes as a kind of learned attentional bias that children have learned to pay attention to the dimension of shape when they're learning words, but in modern machine learning parlance, we might call this transfer learning right somehow.",
                    "label": 0
                },
                {
                    "sent": "By learning these words.",
                    "label": 0
                },
                {
                    "sent": "For these categories, children have picked up some kind of abstraction which now transfers to another word in another object category, which has nothing in common at a very concrete level, right?",
                    "label": 0
                },
                {
                    "sent": "This shape is totally different from those shapes, and it's a totally different word, but yet there's some transferable knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so how does this work?",
                    "label": 0
                },
                {
                    "sent": "Well, one other relevant phenomenon, which is that here's the real interesting transfer, which is transferred to the children's real world vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So the this the scientists tracked for each child what words they know in English.",
                    "label": 0
                },
                {
                    "sent": "So not just made up words for made up objects, but you can get parents to basically report in a somewhat noisy, but on average, reliable way what words their children know and you can see how many words they know at.",
                    "label": 0
                },
                {
                    "sent": "17 months when they started at 19 months when they finished an and they separated out the mean number of object names from other words, the children know.",
                    "label": 0
                },
                {
                    "sent": "You can see that at the start, about half the words are actually object names now.",
                    "label": 0
                },
                {
                    "sent": "Well, what they found was that the kids who had gone through this training and learned the shape bias after two months had a dramatically accelerated rate of real world word learning compared to the control group.",
                    "label": 0
                },
                {
                    "sent": "Remember, the controller had the same kind of experience, except they just didn't get word labels and what happened over two months, the control group learned about 10 or 15 new object labels, but the training group with the word labels learned about 40.",
                    "label": 0
                },
                {
                    "sent": "So like three or four times more really.",
                    "label": 0
                },
                {
                    "sent": "Wopping dramatic effect, and it's quite specific because you can see that in terms of other words that they learned there wasn't really any difference, so it's not like the training kids are smarter or that they had acquired sort of a general ability to learn words.",
                    "label": 0
                },
                {
                    "sent": "No, it seems like they learn something specific about how to learn object labels.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, one of the most striking results in developmental psychology because it's an actual useful intervention that you can do with kids and it works, and they've replicated people.",
                    "label": 0
                },
                {
                    "sent": "Didn't believe this when they first did this.",
                    "label": 0
                },
                {
                    "sent": "There's not many results in developmental psychology.",
                    "label": 0
                },
                {
                    "sent": "This robust but they are.",
                    "label": 0
                },
                {
                    "sent": "It's been replicated and it seems to be real, so the puzzle here computationally is it seems very clear that the shape bias is a powerful inductive constraint, because look how useful it is having learned it.",
                    "label": 0
                },
                {
                    "sent": "Yet not only does the shape bias allow you to learn new words from very few examples, but it itself seems to be learnable from very few examples, right?",
                    "label": 0
                },
                {
                    "sent": "Just basically four example categories, and that's quite striking, right?",
                    "label": 0
                },
                {
                    "sent": "If we if we think of we're used to thinking of inductive biases or abstract knowledge, is something that takes a much longer time and a lot of experience.",
                    "label": 0
                },
                {
                    "sent": "No, you can learn this effectively from just basically 4 examples of categories that follow it with two examples and objects each.",
                    "label": 0
                },
                {
                    "sent": "So how can we explain this?",
                    "label": 0
                },
                {
                    "sent": "Well, I'll just show you a simple kind of hierarchical Bayesian model which is based on the idea of learning about feature variability.",
                    "label": 0
                },
                {
                    "sent": "The intuition which I think is the same as the psychologist intuition is basically that the dimension of shape for nameable object categories has a different profile of variability than other dimensions.",
                    "label": 0
                },
                {
                    "sent": "Shape varies a lot across different object categories, but not very much within object category.",
                    "label": 0
                },
                {
                    "sent": "It's the distinctive feature that picks out these early nameable object categories and the ones that most matter to infants, whereas other features like size and texture and color vary a lot more across objects, but particularly a lot more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In categories.",
                    "label": 0
                },
                {
                    "sent": "So how do we capture this idea in a hierarchical Bayesian model?",
                    "label": 0
                },
                {
                    "sent": "Now to many of you, this will probably be again trivial except for a little twist at the end, but I thought it's good to just warm up with a simple example.",
                    "label": 0
                },
                {
                    "sent": "So I'll just.",
                    "label": 0
                },
                {
                    "sent": "I'll just motivate this or I'll introduce the model using sort of a textbook case of learning about bags of marbles.",
                    "label": 0
                },
                {
                    "sent": "So let's say here you have a bunch of marbles of different colors.",
                    "label": 1
                },
                {
                    "sent": "And you have a bunch of bags that somehow have these marbles distributed in the bags.",
                    "label": 0
                },
                {
                    "sent": "Let's take a bag and choose a Marvel out of it so that we get a green marble here, let's see.",
                    "label": 0
                },
                {
                    "sent": "So what can we say about the other marbles in that bag?",
                    "label": 0
                },
                {
                    "sent": "What do we think the next one is going to look like?",
                    "label": 0
                },
                {
                    "sent": "Well, not a whole lot.",
                    "label": 0
                },
                {
                    "sent": "There's not a whole lot we can say.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contrast that with the case where we look inside the other bags and find this.",
                    "label": 0
                },
                {
                    "sent": "So across all these bags we find different colors of marbles, but it seems like most bags are mostly homogeneous.",
                    "label": 1
                },
                {
                    "sent": "Then for this bag we can feel pretty confident in guessing.",
                    "label": 0
                },
                {
                    "sent": "Assuming these bags are in some sense of the same type that the other marbles we're going to try out our green here, even though we haven't actually seen any bags of green marbles, despite what you might think, this is not in fact green on my screen.",
                    "label": 0
                },
                {
                    "sent": "So this is again, there's some abstract knowledge about about how variable this feature is across.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bags within bags which you can capture intuitively in a kind of two level hierarchical representation so you can see what you've learned about each of the bags which you've inspected extensively.",
                    "label": 0
                },
                {
                    "sent": "You can learn, you know that say this bag is mostly red, and this ones mostly yellow and so on.",
                    "label": 0
                },
                {
                    "sent": "But you also learn something more abstract, which is about the dimension of color.",
                    "label": 0
                },
                {
                    "sent": "In this world, that color varies across bags, but not much within bags or in other words, the variance of color is mostly due to between bag variation rather than within Bagration.",
                    "label": 1
                },
                {
                    "sent": "And having learned that now, if this new bag which the green one has been drawn from in some sense, is the same kind of bag, then it's a good guess, although although we can't be certain, that bag is also mostly green and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It draws from that will be green, so the way we capture this in a hierarchical Bayesian model is with what's called Dirichlet multinomial model.",
                    "label": 1
                },
                {
                    "sent": "It's just a multidimensional sort of multi way extension of the beta binomial model.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with that one where we've just replaced these qualitative descriptions with parameters of a statistical model, so we characterize each bag by a multinomial distribution over the colors you expect to see when drawing from it, and we have Theta parameter to describe that multinomial.",
                    "label": 0
                },
                {
                    "sent": "And then we assume that that if all of these bags are of the same type and there's some kind of common prior in this case, it's a Dirichlet distribution which parameterized by Alpha and beta, which we assume it provides the same prior for all of these multinomial parameters across bags.",
                    "label": 0
                },
                {
                    "sent": "Now there's nothing special.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no particular reason we have to use a D richly distribution in somewhat limited in what it can capture, but it's just enough to capture what we want.",
                    "label": 0
                },
                {
                    "sent": "For this example, it's mostly convenient because it's a conjugate prior.",
                    "label": 0
                },
                {
                    "sent": "For these parameters, and so we can integrate out these parameters.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It just makes inference easier.",
                    "label": 0
                },
                {
                    "sent": "What these parameters mean right?",
                    "label": 0
                },
                {
                    "sent": "Is Beta is a vector of probabilities.",
                    "label": 0
                },
                {
                    "sent": "Which captures which expresses the proportion of colors we expect to see across the whole population, and Alpha is a scalar that determines how much.",
                    "label": 0
                },
                {
                    "sent": "Variability we expect to see within class versus between classes.",
                    "label": 0
                },
                {
                    "sent": "So if Alpha is very large then we expect that individual bags are pretty variable in the same way the whole population is, whereas Alpha is very small.",
                    "label": 0
                },
                {
                    "sent": "Then we then we expect the variability is is mostly between bags and each bag is pretty.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Homogeneous and the idea here is we're going to put some prior very weak prior over the the second level of the model, and we're going to observe the data at the bottom.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to fill in what's in between and we're going to do inference at at both levels of representation here to simultaneously infer what each bag is like, as well as the parameters that describe what bags in general are like that can be used to transfer to new cases, and just to graphically illustrate what's going on there.",
                    "label": 1
                },
                {
                    "sent": "Let's say to capturing the knowledge that this bag is mostly red means here I'm just drawing a simplex for three colors.",
                    "label": 0
                },
                {
                    "sent": "Of course this should really be higher dimensional simplex, where each corner corresponds to a bag of a pure color.",
                    "label": 0
                },
                {
                    "sent": "If if we thought the bag was pretty mixed, then Theta would be in the middle of the simplex.",
                    "label": 0
                },
                {
                    "sent": "But if we think the bag is mostly red, than Theta is towards the red corner, it may not be exactly at the red quarter, even with this uniform data, because the prior mites.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit for a bag that's mostly yellow.",
                    "label": 0
                },
                {
                    "sent": "We're inferring that Theta is towards the yellow corner of the simplex.",
                    "label": 0
                },
                {
                    "sent": "For knowledge at this level.",
                    "label": 0
                },
                {
                    "sent": "What we're inferring is, is now the distribution over the simplex, which is the prior for the quantities.",
                    "label": 0
                },
                {
                    "sent": "I was just showing and the Dirichlet distribution is capable of reflecting different kinds of priors you have.",
                    "label": 0
                },
                {
                    "sent": "You can have sort of unimodal distributions like that, which expect that each bag is pretty variable in the same way that the population is, but in this case, what you have is a distribution like this, where Alpha is low where most of the mass is out at the corners of the simplex, which reflects the idea that we expect most bags are.",
                    "label": 0
                },
                {
                    "sent": "Pretty homogeneous and the variability in the population comes between bags.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we apply this to the shape bias case well?",
                    "label": 1
                },
                {
                    "sent": "Just a very simple analysis will represent each of these eight objects with several dimensions, including shape, texture, color and size, and what we have here is that.",
                    "label": 0
                },
                {
                    "sent": "The category labels just refer to the words here and shape is constant within category, but the others vary across within category.",
                    "label": 0
                },
                {
                    "sent": "And if we the simplest way to apply this model is to assume that each dimension is generated from an independent Dirichlet multinomial model, the same structure but just different parameters, and then what we learn is that shape varies across categories but not within categories.",
                    "label": 1
                },
                {
                    "sent": "In other words, shape is just behaves just like color did here.",
                    "label": 0
                },
                {
                    "sent": "But these other dimensions of texture, color and size as you.",
                    "label": 0
                },
                {
                    "sent": "As you might guess.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Don't very.",
                    "label": 0
                },
                {
                    "sent": "And if we if we then go and apply this to the case of the of the.",
                    "label": 0
                },
                {
                    "sent": "You know there was actually tested the sort of transfer phase, so now we have a new object in a new category #5 here.",
                    "label": 0
                },
                {
                    "sent": "A new shape, new texture, a new color.",
                    "label": 0
                },
                {
                    "sent": "So on what's important in this simulation, and of course any model has got to do something like this for this task is that the we've assumed there's more possible shapes, textures, colors and so on that are actually observed during training.",
                    "label": 0
                },
                {
                    "sent": "There may be a lot more, but the key thing about this kind of hierarchical Bayesian model is it can learn expectations even for the values of dimensions that haven't been seen.",
                    "label": 0
                },
                {
                    "sent": "So even for example, for a shape that has never been seen before, paired with the category label and.",
                    "label": 0
                },
                {
                    "sent": "Then we have these three test items.",
                    "label": 0
                },
                {
                    "sent": "This one here corresponds to having the same shape as the tax, but a different new texture and color.",
                    "label": 0
                },
                {
                    "sent": "This one has a different shape and a different color, but the same texture and so on.",
                    "label": 0
                },
                {
                    "sent": "And what the model tells us is that this one is much more likely to have come from the same category that this one came from.",
                    "label": 0
                },
                {
                    "sent": "Then these two so relatively.",
                    "label": 0
                },
                {
                    "sent": "This is the best choice for Dex, so even this very simple model is capable of capturing this kind of abstract.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Transfer that that children seem to show.",
                    "label": 0
                },
                {
                    "sent": "Now this model is is very simple and it's easy to come up with limitations.",
                    "label": 0
                },
                {
                    "sent": "For example, assuming that we have a single dimension of shape, maybe seems like a limitation and there's lots of extensions which you can go through, which I'm not going to go through here.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm going to shift to a different topic.",
                    "label": 0
                },
                {
                    "sent": "We can work with more distributed representations of shape, for example, and get the same kind of effect.",
                    "label": 0
                },
                {
                    "sent": "I'll just talk about one extension because it's again corresponds to an interesting developmental milestone, and it has a nice machine learning correlate.",
                    "label": 0
                },
                {
                    "sent": "Which is an interesting kind of selective transfer depending on knowledge of ontological kinds, so.",
                    "label": 1
                },
                {
                    "sent": "The shape bias that we saw was a phenomenon that kicks in around age 2 by age three.",
                    "label": 1
                },
                {
                    "sent": "Children know about the shape bias, but they also know about other kinds of biases, for example, and they restrict them appropriately to certain large scale categories, so they apply the shape bias to solid object categories like ball book toothbrush.",
                    "label": 1
                },
                {
                    "sent": "But for non solid substances like for example toothpaste, they apply a material by so you can introduce a new weird gel non solid substance or weird powdery thing.",
                    "label": 0
                },
                {
                    "sent": "And they will not care about the shape of that entity, but they will care about its material properties in generalizing a new word.",
                    "label": 0
                },
                {
                    "sent": "Again, that's it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct bias, so it takes a little bit longer to learn.",
                    "label": 0
                },
                {
                    "sent": "So how can we model this kind of selective transfer?",
                    "label": 1
                },
                {
                    "sent": "Well, you might say you know.",
                    "label": 0
                },
                {
                    "sent": "Suppose we we know the ontological kind, whether something is a solid object or a non solid substance.",
                    "label": 1
                },
                {
                    "sent": "That's this Z variable here.",
                    "label": 0
                },
                {
                    "sent": "Then we can just learn a separate Dirichlet multinomial model for each.",
                    "label": 1
                },
                {
                    "sent": "Each ontological category.",
                    "label": 0
                },
                {
                    "sent": "Just adding another level to the Bayesian hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So here capturing variability and features for solid objects.",
                    "label": 0
                },
                {
                    "sent": "Knowing that, say all objects all things in this category are solid, but shape is variable across objects, although not within category and other features are variable within category as well as across categories.",
                    "label": 0
                },
                {
                    "sent": "And then at this lower level learning specifics about what words refer to, it shapes and then we will learn something corresponding over here on the material side.",
                    "label": 0
                },
                {
                    "sent": "But of course this itself is already oversimplified.",
                    "label": 0
                },
                {
                    "sent": "Because the data don't necessarily come labeled this way, we don't Opry Ori know what the right ontological kinds are, so this is chicken and egg problem.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to partition the object categories into these different parts of the of the hierarchical struck.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our solution to this is just to add inference at this higher level using a non parametric prior over this partition.",
                    "label": 1
                },
                {
                    "sent": "In particular we're using a Chinese restaurant process prior, so we're saying that.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "Each object category belongs to some higher level ontological kind.",
                    "label": 1
                },
                {
                    "sent": "That's the Z vector, but we don't know how many ontological kinds there are or which object categories in which ontological kind.",
                    "label": 0
                },
                {
                    "sent": "But we can do inference over that.",
                    "label": 0
                },
                {
                    "sent": "At the same time as we do inference about these two levels of parameters, and in fact, for these simple toy datasets recover the right kind of structure here.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of nice.",
                    "label": 0
                },
                {
                    "sent": "It's certainly related to CRP mixtures, but instead of clustering objects were clustering categories of objects, and while the objects in a conventional clustering model the objects.",
                    "label": 0
                },
                {
                    "sent": "Have something in common?",
                    "label": 0
                },
                {
                    "sent": "Here are the categories of objects.",
                    "label": 0
                },
                {
                    "sent": "Don't really have anything concrete in common.",
                    "label": 0
                },
                {
                    "sent": "In fact, their most distinctive feature which is the shape of that objects in the category is totally different for every category.",
                    "label": 0
                },
                {
                    "sent": "But what they have in common is that they draw on the same kind of prior.",
                    "label": 0
                },
                {
                    "sent": "So it's this kind of cluster innocence clustering that you get at an abstract level by combining a nonparametric cluster prior with a hierarchical Bayesian model is a powerful way to do selective transfer, and in fact very recently.",
                    "label": 0
                },
                {
                    "sent": "Some other researchers at MIT, Dan Ryan Leslie cabling have defined a related model for a selective transfer task in an applied setting where you are trying to learn.",
                    "label": 0
                },
                {
                    "sent": "It's a Caillou project where you're trying to learn whether a user will accept a meeting invitation and they learn a naive Bayes classifier for each user, but then they define a CRP prior over the parameters of those naive Bayes classifiers and they're able to sort of improve transfer by by figuring out just exactly which users should be should be used to track.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To which other new users so this kind of idea could have?",
                    "label": 0
                },
                {
                    "sent": "I think a lot of implications.",
                    "label": 0
                },
                {
                    "sent": "Other applications in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Now what I want to do next is turn to another setting which is in some sense more knowledge rich in the in the previous setting.",
                    "label": 0
                },
                {
                    "sent": "It was nice because it it was a very simple, elegant mathematical model of a very important developmental phenomenon, but there wasn't much interesting structured knowledge representation going on which I was arguing is so crucial for human learning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to get that, we're going to switch to a different task, which is another one of these tasks.",
                    "label": 0
                },
                {
                    "sent": "Of generalizing from a few examples, it's a task that many psychologists have studied really since the mid 70s, so there's a lot of data here that you can model, and our models can get much more quantitative.",
                    "label": 0
                },
                {
                    "sent": "So here's the kind of task it's in the form of evaluating the strength of an inductive argument.",
                    "label": 0
                },
                {
                    "sent": "But again, it's like generalizing a new concept from a few examples.",
                    "label": 0
                },
                {
                    "sent": "So these are the premises.",
                    "label": 0
                },
                {
                    "sent": "Gorillas have.",
                    "label": 0
                },
                {
                    "sent": "T9 hormones, seals have T9 hormones, squirrels have T9 hormones, and you're asked how likely is it that the conclusion is true given these premises, how likely is it that horses have T9 hormones?",
                    "label": 1
                },
                {
                    "sent": "Given that these do so to compare this with, say, the word learning case.",
                    "label": 0
                },
                {
                    "sent": "There's a new concept which is having teen hormones and you get a few examples of things in that concept and you're asked to judge the probability that something else is in the concept.",
                    "label": 0
                },
                {
                    "sent": "So it's the same kind of task.",
                    "label": 0
                },
                {
                    "sent": "But it's it's in some sense, easier to study different kinds of knowledge that can be brought to bear on this task.",
                    "label": 0
                },
                {
                    "sent": "Now psychologists will do things like compare how people rate the strength of arguments or compare pairs of arguments and say which is stronger, which is a better generalization and say if we compare these three people typically think this is a stronger generalization that knowing that gorillas, seals and squirrels have some arbitrary biological property provides better evidence for thinking that horses do then for thinking that flies do, or and also better evidence for thinking that horses do.",
                    "label": 0
                },
                {
                    "sent": "Then, if you were told that.",
                    "label": 0
                },
                {
                    "sent": "Say this property was true of gorillas, chimps, monkeys, and baboons, in which case you might think it's just something specific to primates, and they explain this with more or less informal notions of simile.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretty and typicality, but we want to get a bit more formal.",
                    "label": 0
                },
                {
                    "sent": "Well, a lot more formal, and so how are we going to do this?",
                    "label": 0
                },
                {
                    "sent": "Well, here's a way to render the computational problem abstractly.",
                    "label": 1
                },
                {
                    "sent": "There's a bunch of objects in the domain that we know something about, and what we know about this domain will start off with our most concrete level of knowledge will just be various features we know about these things.",
                    "label": 0
                },
                {
                    "sent": "So for example, again, psychologists have collected a lot of data on what features people will will know for different animals, so there's a in this case here.",
                    "label": 1
                },
                {
                    "sent": "We're going to use a matrix of object feature judgments that has 85 features and 50 animals and a bunch of subjects rated how how much each feature is associated with each animal.",
                    "label": 0
                },
                {
                    "sent": "And these features are just sort of basic observable, anatomical, behavioral, ecological properties.",
                    "label": 0
                },
                {
                    "sent": "The kind of things that child walking around the zoo or reading kids books about animals might learn.",
                    "label": 0
                },
                {
                    "sent": "So for example, the features that are strongly associated with elephant are things like being grey, hairless, tough skin, big bulbous body shape, long legs.",
                    "label": 0
                },
                {
                    "sent": "Tasks walking, being slow, living in the jungle, that kind of thing being strong.",
                    "label": 0
                },
                {
                    "sent": "So there's this matrix of sort of known facts about these animals, and there's some new property like say having T9 hormones that is less observable that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "If you think it's a little bit weird to be reasoning about having T9 hormones.",
                    "label": 0
                },
                {
                    "sent": "Think of a more naturalistic task where your reasoning about whether something is good to eat or whether it's poisonous, or whether it wants to eat you.",
                    "label": 0
                },
                {
                    "sent": "These are very ecologically important reasoning tasks, which again, we typically get very sparse data on.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to infer if some which things are going to be poisonous from as little data as possible.",
                    "label": 0
                },
                {
                    "sent": "But here psychologists have used these so-called blank predicates to be as in some sense.",
                    "label": 0
                },
                {
                    "sent": "Knowledge free as possible, except that the fact that they drawn some more abstract knowledge about biological species and properties, hence something like having T9 hormones and the idea is we just observe that new property for a couple of objects and we want to know how likely it is.",
                    "label": 0
                },
                {
                    "sent": "Another things have this property.",
                    "label": 1
                },
                {
                    "sent": "So somehow we need to innocence bridge from a lot of features that we can observe more densely to some new features which are less densely observed.",
                    "label": 0
                },
                {
                    "sent": "Again, you can think of this as a kind of transfer learning problem.",
                    "label": 0
                },
                {
                    "sent": "But now where these objects instead of being independent draws from a bag are actually related in some interesting way to each other.",
                    "label": 0
                },
                {
                    "sent": "And you could also think of this as a kind of semi supervised learning problem, and that there's a few labeled examples and a lot of unlabeled data, and we want to learn some inductive bias to generalize the new property as well as we can from all the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And I think the techniques that we're developing here would have applications for semi supervised learning, and in fact an earlier version of this, I think in this Jerry Juice Semi supervised learning tutorial.",
                    "label": 0
                },
                {
                    "sent": "Yesterday he talked about an early version of this with a tree.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just prior, so here's the idea of of the model.",
                    "label": 0
                },
                {
                    "sent": "It's again a hierarchical Bayesian model where the lowest level, what we're called, the level of the data, is the stuff I already told you about features you can observe for these animals and new features that you want to generalize from sparse data.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to address that problem with two levels of higher level background knowledge.",
                    "label": 0
                },
                {
                    "sent": "The most immediate level up what we call the structure.",
                    "label": 0
                },
                {
                    "sent": "Is just some kind of relational system over these object categories, which which we can use to interpret the data we see.",
                    "label": 0
                },
                {
                    "sent": "So for example, a taxonomic tree over animals which we might expect that this some.",
                    "label": 0
                },
                {
                    "sent": "You know, this new feature is somehow going to be for example smooth with respect to this taxonomic tree.",
                    "label": 0
                },
                {
                    "sent": "Things nearby in the tree are likely to have the same values of these features, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going by defining a prior on the data given the structure of probabilistic model of what data we expect to see given a certain say tree structure.",
                    "label": 0
                },
                {
                    "sent": "We'll be able to use this to give us appropriate inductive biases for generalizing this new property for figuring out which other things have this property, and we can also work backwards from all the observed features that we see to infer this tree structure.",
                    "label": 0
                },
                {
                    "sent": "Of course we have to have some kind of higher level knowledge.",
                    "label": 0
                },
                {
                    "sent": "For example, we have to know we're looking for a tree in order to infer a tree like this, so will also specify what we call the level of structural form which specifies the kind of relational system we're learning with, and will even extend the hierarchical Bayesian model to make inferences at this level.",
                    "label": 0
                },
                {
                    "sent": "To be able to infer that, say, one domain has a tree structure, another domain has a different kind of structure, and that's going to be important, because not every domain or every set of properties is tree structure.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I'll just start and work up from the bottom here.",
                    "label": 0
                },
                {
                    "sent": "How do we define the probabilistic model of the data given structure?",
                    "label": 0
                },
                {
                    "sent": "Well, the intuition is like I said, is that we want a prior which in a sense is going to be flexible.",
                    "label": 0
                },
                {
                    "sent": "It's going to give any way of labeling the objects.",
                    "label": 0
                },
                {
                    "sent": "Here I'm just labeling them black or grey.",
                    "label": 0
                },
                {
                    "sent": "Any way of labeling objects some prior probability, but ones which are smooth with respect to the graph structure should have higher prior probability.",
                    "label": 0
                },
                {
                    "sent": "This captures the basic idea of generalization by similarity.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where the structure determines similarity and to do this, we're going to use build on some nice work of Zoo, Lafferty and Ghahremani, which probably most people here are familiar with, so I won't really go into detail on it, but the basic idea is to define a Gaussian field, a set of over the graph, a set of continuous real valued quantity quantities at each node of the graph, reflecting sort of how much that feature is associated with that point of the graph and then.",
                    "label": 0
                },
                {
                    "sent": "We use the structure of the graph to parameterise the covariance of this Gaussian field.",
                    "label": 1
                },
                {
                    "sent": "So how feature associations vary between as you go across the graph.",
                    "label": 0
                },
                {
                    "sent": "The idea the intuition is that points which are closer in the graph should have higher covariance.",
                    "label": 0
                },
                {
                    "sent": "You can think of that also is like saying that this feature sort of varies like a like a smooth random walk or a diffusion process over the graph.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's 1 sample from this Gaussian field.",
                    "label": 0
                },
                {
                    "sent": "Here's another one, the math behind it, as again, many of you are probably familiar with is just.",
                    "label": 1
                },
                {
                    "sent": "We take the regularised version of the graph Laplacian and invert that to give the covariance of this Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the kernel if you like.",
                    "label": 0
                },
                {
                    "sent": "And then if we want to turn this into a distribution on feature extensions or ways of labeling objects according to whether they have a binary feature or not, then we can just threshold this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tenuous Association.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Now, in order to use this distribution, we want to do two things.",
                    "label": 0
                },
                {
                    "sent": "We want to understand how we can learn the right structured representation of the domain and how we can use it to make inferences from very few examples of a new property.",
                    "label": 0
                },
                {
                    "sent": "And the answer to those two questions is basically the same.",
                    "label": 0
                },
                {
                    "sent": "So to infer the right tree structure, we assume that each of the observed features are generated from this Gaussian process over that over the tree, and then we just find the tree that best fits the observed features.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as a kind of Bayesian hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "I mean, yet another Bayesian hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "We find the tree that you know on average, makes the observed features most likely under that smooth process.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the method isn't just restricted to trees.",
                    "label": 0
                },
                {
                    "sent": "We can use other kinds of inductive biases.",
                    "label": 0
                },
                {
                    "sent": "So, for example, here's a tree.",
                    "label": 0
                },
                {
                    "sent": "Here's the tree that the best tree that was found for this 50 animal.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "80 feature set.",
                    "label": 0
                },
                {
                    "sent": "But we can also try a different kind of inductive bias if you like a different kind of structure.",
                    "label": 0
                },
                {
                    "sent": "For example, we could search for the best 2 dimensional embedding, where now again we have some way of determining covariance based on proximity in this 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And you can think of it more or less as a limiting case of the graph based approach when you take when you take a mesh and take the mesh size to zero and we can see well which of these kinds of both of these are structured representations of the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Main that makes sense of the observed feature matrix.",
                    "label": 0
                },
                {
                    "sent": "They both do a kind of compression or dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "We can ask which provide the best inductive bias for generalizing new properties.",
                    "label": 0
                },
                {
                    "sent": "So that is we now take assume that this new property.",
                    "label": 0
                },
                {
                    "sent": "Is the complete extension of this new property which we don't observe right?",
                    "label": 0
                },
                {
                    "sent": "We only observe a few samples.",
                    "label": 0
                },
                {
                    "sent": "The complete extension is a random draw from that same Gaussian process over the graph, and we're going to use that as a prior for how to fill in.",
                    "label": 0
                },
                {
                    "sent": "The UN observed.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "And here's the result on 2 classic datasets.",
                    "label": 0
                },
                {
                    "sent": "So what what I'm plotting here are the two datasets.",
                    "label": 0
                },
                {
                    "sent": "Are the two columns here and each point data point corresponds to a different inductive generalization, a different one of these arguments.",
                    "label": 0
                },
                {
                    "sent": "The ones over here are have horses as their conclusion.",
                    "label": 0
                },
                {
                    "sent": "So basically were, and they very very the objects which are the examples.",
                    "label": 0
                },
                {
                    "sent": "So for example, this point is is the argument given that cows and elephants have a property.",
                    "label": 1
                },
                {
                    "sent": "How likely is it that horses have the same property?",
                    "label": 1
                },
                {
                    "sent": "Another points here correspond to different sets of examples over here.",
                    "label": 0
                },
                {
                    "sent": "The conclusion is a general one.",
                    "label": 0
                },
                {
                    "sent": "The question is, how likely is it that all mammals have a property given 3 examples of things which have that property and the X axis are the model predictions and the Y axis are human judgments.",
                    "label": 1
                },
                {
                    "sent": "So what you see here is a case where you have a model that's highly correlated with peoples judgments.",
                    "label": 0
                },
                {
                    "sent": "Generalizations at the model thinks are strong like this one are also ones that people think are strong and once, and they also agree on which ones are weak and.",
                    "label": 0
                },
                {
                    "sent": "These two plots here correspond to the predictions of this Gaussian process defined over a tree structure.",
                    "label": 0
                },
                {
                    "sent": "The ones here correspond to the predictions of the Gaussian process defined over 2 dimensional space and what you can see is a substantial difference that restructured model predicts people's judgments significantly better than the two dimensional one learned from the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same feature matrix.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "We can explore other kinds of inductive biases.",
                    "label": 0
                },
                {
                    "sent": "In a sense, what what?",
                    "label": 0
                },
                {
                    "sent": "I would argue here is that the tree structured model is the correct or more more more of the correct inductive bias for biological species and properties, whereas the two dimensional space isn't really the right one.",
                    "label": 0
                },
                {
                    "sent": "If you think about evolution and mutation and selection, you're going to get structures of species and distributions of properties that more or less have a tree distribution, and people in some sense it seems, might be picking up on that.",
                    "label": 0
                },
                {
                    "sent": "But we can try other kinds of priors.",
                    "label": 0
                },
                {
                    "sent": "For example, what we call the rock covariance model, where there's no dimensionality reduction, there's no compression, we just take that raw feature matrix of 80 features and we just take take that, take that as the kernel basically.",
                    "label": 0
                },
                {
                    "sent": "So we take take the inner products in that high dimensional feature space, and what you can see there is that model also doesn't do very well.",
                    "label": 0
                },
                {
                    "sent": "I would say this is basically because that model doesn't have enough inductive bias, it has some inductive bias because the features were not completely random, they were meaningful.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't have the.",
                    "label": 0
                },
                {
                    "sent": "The stronger inductive bias you get from compressing down to a tree structure, we can take another kind of model, which is what we call the strict tree model where you take the same tree used here, but instead of having that smoothness process the Gaussian field.",
                    "label": 0
                },
                {
                    "sent": "We assume a simpler hypothesis space of just cuts of the tree, so each each feature has to strictly cut the tree at some point and you can see that model doesn't do well either, but in the opposite way it's bias is too strong.",
                    "label": 0
                },
                {
                    "sent": "It gives too many arguments.",
                    "label": 0
                },
                {
                    "sent": "The same strength like you see these vertical stripes, whereas people are distinguishing between more and less representative sets of examples.",
                    "label": 0
                },
                {
                    "sent": "So I think this is pretty good evidence that when people are making their generalizations, they're using an inductive bias which is well tuned to this.",
                    "label": 0
                },
                {
                    "sent": "This particular kind of domain.",
                    "label": 0
                },
                {
                    "sent": "And it's sort of not too strong and not too weak.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But just just right.",
                    "label": 0
                },
                {
                    "sent": "Now of course, it's part of that argument would be we have to say that you know, if people are really using an inductive bias which is tuned to the domain, then there should be other domains which are not naturally tree structure, in which people would be using something else for.",
                    "label": 0
                },
                {
                    "sent": "So here's an.",
                    "label": 0
                },
                {
                    "sent": "I mean that we've looked at many different kinds of tasks in domains.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of one which we think.",
                    "label": 0
                },
                {
                    "sent": "It should clearly suggest more of a 2 dimensional model.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of a geographic inference task.",
                    "label": 0
                },
                {
                    "sent": "We said to people subjects given that a certain kind of Native American artifact has been found in sites near City X.",
                    "label": 1
                },
                {
                    "sent": "How likely is the same artifact to be found near City Y and then we used various American cities?",
                    "label": 0
                },
                {
                    "sent": "People would be familiar with and we can build the best 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Representation of peoples mental space.",
                    "label": 0
                },
                {
                    "sent": "But by getting judgments of distances between cities and also the best tree structure model.",
                    "label": 0
                },
                {
                    "sent": "And here we can see the two dimensional space does much better people predicting people's judgments again as we would hope if people are using a prior, that's the right one.",
                    "label": 0
                },
                {
                    "sent": "And given this truck.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure of the relevant part of the real world.",
                    "label": 0
                },
                {
                    "sent": "So this brings us to the last part of this case study, which is how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we make this higher level inference?",
                    "label": 0
                },
                {
                    "sent": "How do we figure out from data about features and properties and objects in some domain, not only what's the right structure of a particular form, like the right tree, but the right kind of structure that a certain domain is tree structured but some other domain is, say chain structured or spatially.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structured or other kinds of things just to motivate this a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's as as as an important problem in both cognitive development as well as the history of science.",
                    "label": 0
                },
                {
                    "sent": "Consider example.",
                    "label": 0
                },
                {
                    "sent": "Like this, we might think of biological species as following some kind of a tree structure, but and the idea that they would follow some kind of a wonder.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chain kind of silly, but exactly this was the dominant mode in Western European thought.",
                    "label": 0
                },
                {
                    "sent": "Say roughly between in the Middle Ages up until about 1700 the work of Linnaeus and colleagues, the so-called great chain of being, which puts plants and rocks down at the bottom and got up at the top end and animals up to men and even Angels in between.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is there some kind of structural form discovery going from, say the Middle Ages in the early modern period to Linnaeus is natural system up to Darwin, where this tree structured model gradually emerges as the dominant way to think about the natural world.",
                    "label": 0
                },
                {
                    "sent": "To take another example from history of Science, Linnaeus and other contemporaries discovered the periodic structure of the chemical elements are more abstract structural form then then just the particular relations between elements, but they could use that then to fill in.",
                    "label": 1
                },
                {
                    "sent": "New properties of new elements or to infer elements which had never even been seen before.",
                    "label": 1
                },
                {
                    "sent": "The same kind of abstract learning about sexual form is critical in children's cognitive development.",
                    "label": 1
                },
                {
                    "sent": "So, for example, learning that category labels are not just a discrete partition like we assumed in the first part of the talk, but that they actually have some kind of hierarchical structure.",
                    "label": 0
                },
                {
                    "sent": "That's key for learning about real human categories, or, for example, learning that temporal categories like the seasons are the days of the week off and have a cyclical structure or learning about transitive.",
                    "label": 1
                },
                {
                    "sent": "Comparative relation so learning that, for example, you know a relation like bigger than or better than or taller than, but all of those can be represented as an as a one as values along A1 dimensional order.",
                    "label": 0
                },
                {
                    "sent": "Children go through discrete conceptual discoveries when prior to some age they don't have this abstract knowledge of the structural form of some domain and then later on they do so.",
                    "label": 0
                },
                {
                    "sent": "We want to know.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How they can do it now?",
                    "label": 0
                },
                {
                    "sent": "There's lots of models that we're all familiar with for doing unsupervised structure learning, but for the most part they assume that you know the right structural form and what we don't have is what seems to be going on in human cognitive development.",
                    "label": 0
                },
                {
                    "sent": "In science.",
                    "label": 0
                },
                {
                    "sent": "Some kind of a universal structure learner.",
                    "label": 1
                },
                {
                    "sent": "If you like this is I mean admittedly ambitious, but something which replaces all these different algorithms which learn just a structure of a particular form with a method that's able to find the right form of structure as well as the best structure of.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Particular form that's best suited for a particular domain, so I'll just tell you since time is almost up just a little bit about our initial work on this problem.",
                    "label": 0
                },
                {
                    "sent": "The first step here is to give some kind of hypothesis space of different structural forms that we might be able to learn with either thinking about what might be going on for a human child or scientific discovery setting, and we're going to work with different structural forms that can be described by simple generating processes using what are called context free graph grammars.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "So called node replacement grammars, where the idea is that these are these are we have a hypothesis of rules for growing structures, graphs, basically by taking a note and it's in and out links and replacing it with two other nodes connected somehow and specifying how the in and out links of the seed node go get assigned to the new nodes and just by taking these simple rules, simple generative processes for graph structures we can grow.",
                    "label": 0
                },
                {
                    "sent": "Many of the forms of structure that show up in basic models for unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So flat clusters or partitions, dominance orders, chains, rings, hierarchies, trees with latent nodes by taking cross products of these production rules we can take we can grow higher, more complex structures, like by taking a cross product of 2 Chainz we can get a 2 dimensional space or chain in a ring.",
                    "label": 0
                },
                {
                    "sent": "Give us a Taurus sorry cylinder.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can get a Taurus by taking two rings and so on, and then the idea is to use these graph grammars to specify a hypothesis space of structural forms and do inference at all levels of this hierarchy.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to tell you all the mathematical details or actually any of them, but the basic idea is that, well, the probabilistic model for this level is the same one I told you about the smoothness model of you at all, and at this level here there's a fairly simple kind of Bayesian Occam's razor going on, where this graph specifies a prior over possible.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this grammar specifies a prior over possible graphs.",
                    "label": 0
                },
                {
                    "sent": "And there's there's going to be a preference for grammars that generate fewer graphs with fewer nodes and fewer links because they have fewer alternatives.",
                    "label": 0
                },
                {
                    "sent": "Just the usual Bayesian Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do joint inference at both of these level.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So here are a few results on just various kinds of psychologically plausible real world datasets.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a result for the animals domain taking the same kind of data I showed you before.",
                    "label": 0
                },
                {
                    "sent": "It's a different data set, but again, you start off with a matrix at this level.",
                    "label": 0
                },
                {
                    "sent": "Here a matrix of animals and features.",
                    "label": 0
                },
                {
                    "sent": "And what you learn is that you learn here we're showing the best tree.",
                    "label": 0
                },
                {
                    "sent": "But what this represents is that the best structure found for this data set is a tree, as opposed to a chain or or a cylinder or two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And you can see that it's learning a reasonable, psychologically reasonable tree structure taxonomy, but it's nice to know that this model is capable of recovering what we think is the actual right structure of the domain.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's not required to learn a tree structure, and here we have a different kind of domain.",
                    "label": 0
                },
                {
                    "sent": "These are this is a domain of political opinions.",
                    "label": 0
                },
                {
                    "sent": "Supreme Court votes.",
                    "label": 0
                },
                {
                    "sent": "For all the judges who served in the Rehnquist courts, this is sort of 80s and early 90s.",
                    "label": 0
                },
                {
                    "sent": "And what we what the model recovers is a chain of 1 dimensional structure, roughly corresponding to the liberal conservative spectrum with Thurgood Marshall and Brandon over here on the left, Scalia and Thomas over there.",
                    "label": 0
                },
                {
                    "sent": "On the right, you know, that's our familiar left, right liberal conservative spectrum, but not, but the models made a higher level abstract discovery of structural form, namely, that this one dimensional spectrum is the right way.",
                    "label": 0
                },
                {
                    "sent": "To organize this domain or the best, most compact way of thinking about it logically, it could have been a tree structure, or it could have been a 2 dimensional space like this is a 2 dimensional space recovered from face images varying and a racial and gender dimension.",
                    "label": 0
                },
                {
                    "sent": "They were these were realistically synthesized to actually have these two latent dimensions.",
                    "label": 0
                },
                {
                    "sent": "But the model here is able to recover that as well as say, to recover this some cylinder of latitude and longitude for cities and.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other circle.",
                    "label": 0
                },
                {
                    "sent": "Since I'm basically out of time right, I will skip the nice stuff about relational data, but we can do that.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same kinds of things with relational data and even study how people are able to learn structural 4.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On relations.",
                    "label": 0
                },
                {
                    "sent": "I'll just make one last point, because it's just relevant for the general conclusion.",
                    "label": 0
                },
                {
                    "sent": "I want to make, which is what happens if we look at how these structural forms develop.",
                    "label": 0
                },
                {
                    "sent": "As we observe more and more data.",
                    "label": 1
                },
                {
                    "sent": "So here we're back to the animals domain and we're going to see what have we learned when we just take 5 features, 20 features, or the full data set of 100 or so features, and what you can see is something kind of interesting here, but with with just five features, we don't.",
                    "label": 0
                },
                {
                    "sent": "We learn a more simple structural form, we just learn a set of flat clusters.",
                    "label": 0
                },
                {
                    "sent": "They're reasonable clusters.",
                    "label": 0
                },
                {
                    "sent": "They're sort of large scale, superordinate categories like birds and insects and carnivorous mammals and herbivore mammals and aquatic creatures and so on, but we don't have the full hierarchical structure.",
                    "label": 0
                },
                {
                    "sent": "Now when you add in a few more features now the model realizes that the extra complexity of that restructured taxonomy is licensed, and it doesn't for the best structural form is a tree, but it doesn't have the right tree, so you can see over here it has a tree, has a node with Dolphins, seals, whales and Penguins.",
                    "label": 0
                },
                {
                    "sent": "And we know as adults that Penguins don't belong with Dolphins, seals and whales.",
                    "label": 0
                },
                {
                    "sent": "Rather they belong something like over here.",
                    "label": 0
                },
                {
                    "sent": "So this is what the tree you learn when you have the full data set, where now there's a distinct subtree for the aquatic mammals and Penguins are over here with the other birds.",
                    "label": 1
                },
                {
                    "sent": "Although they're sort of closer to the fish and the aquatic mammals and what's what you see going on here is a phenomenon that we call the blessing of abstraction.",
                    "label": 0
                },
                {
                    "sent": "It's kind of kind of overly dramatic name for what I think is a basic but really important phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Which is that often learning abstract knowledge is easier than learning concrete knowledge.",
                    "label": 0
                },
                {
                    "sent": "The concrete knowledge here is the specific structure in the abstract knowledge is the structural form, and you often find when you work with hierarchical Bayesian models that because that less data is required to converge on the right, learns knowledge at the higher level than at the lower levels, and in this case that means the structural form, then the, then the.",
                    "label": 0
                },
                {
                    "sent": "Particular structure, and I think that's an important feature of cognitive development as well as science, where when we first learning about a domain we kind of get the big picture and sort of the general way of organizing things.",
                    "label": 0
                },
                {
                    "sent": "And then it might take much more observation to fill in the details.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that's exactly what happened in scientific biology, and that's exactly what happens in children's cognitive development, where it was relatively early, say in the 1700s, where we figured out that some kind of tree structure is the right tree structure was the right form, but it took a long time and a lot more data to converge on.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We thought was the right tree.",
                    "label": 0
                },
                {
                    "sent": "And this is this is really a sort of a different perspective on what is maybe the most classic debate in cognitive science about the nature of learning, what's called the nativist versus the empiricist.",
                    "label": 0
                },
                {
                    "sent": "And you see the same kind of debate in AI.",
                    "label": 0
                },
                {
                    "sent": "But I'll just conclude by talking about the debate in cognitive science.",
                    "label": 0
                },
                {
                    "sent": "It's long recognized that some kind of abstractions is important, but you have these two camps, the nativists who say that there is explicit knowledge of say.",
                    "label": 1
                },
                {
                    "sent": "Structural form or we have explicit abstract knowledge, but that is not learned.",
                    "label": 0
                },
                {
                    "sent": "Chomsky being the most famous example.",
                    "label": 0
                },
                {
                    "sent": "And then you have the so called empiricist camp, which says, well, we can learn abstract knowledge, but it's not really explicit.",
                    "label": 0
                },
                {
                    "sent": "We just sort of, it just comes out kind of implicitly from overlaying a lot of concrete experience.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the classic connectionist approach as well as traditional.",
                    "label": 0
                },
                {
                    "sent": "I would say traditional structure learning and probabilistic graphical models where you're learning some graphical representation of the structure of a set of variables.",
                    "label": 1
                },
                {
                    "sent": "But you're not learning.",
                    "label": 0
                },
                {
                    "sent": "Some kind of higher level inductive bias or abstract constraint on the kind of structure that's there, and what I've tried to argue here is that both of these things are necessary.",
                    "label": 0
                },
                {
                    "sent": "We really do need explicit structural forms 'cause you could see from modeling peoples ability to generalize from very few examples.",
                    "label": 0
                },
                {
                    "sent": "You need that the strong inductive bias, but you also need the flexibility that comes from being able to learn and what we were trying to work towards here is really a third way or a new approach to thinking about cognitive development as well as how you might, say, engineer the development of.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Official system, so I'll skip my little thing on causal learning.",
                    "label": 0
                },
                {
                    "sent": "And just sum up.",
                    "label": 0
                },
                {
                    "sent": "I've given you 2 case studies on how we can model human inductive learning by combining a few basic ideas.",
                    "label": 0
                },
                {
                    "sent": "The idea of Bayesian inference over hierarchies of flexibly structured representations and and what in each of these cases we've looked at multiple levels of knowledge and tried to ask some of the big questions about cognitive science.",
                    "label": 1
                },
                {
                    "sent": "What's the form and content of knowledge at these different levels of abstraction?",
                    "label": 0
                },
                {
                    "sent": "How does that knowledge guide learning?",
                    "label": 0
                },
                {
                    "sent": "And how might that knowledge itself be acquired in a way that combines both strong inductive biases but also flexibility?",
                    "label": 0
                },
                {
                    "sent": "And there's sort of these bigger picture lessons, which I hope you know.",
                    "label": 0
                },
                {
                    "sent": "Certainly in cognitive science, these are important, but I hope they will also pay off for people interested in machine learning and AI.",
                    "label": 0
                },
                {
                    "sent": "This third way of thinking about development, the idea that in some sense, inductive biases, abstract powerful abstractions can be learned very quickly, sometimes in a kind of a top down way, rather than having to be wired in or learned very gradually and implicitly.",
                    "label": 0
                },
                {
                    "sent": "And maybe the most general lesson, which I think many people in machine learning are already have already learned and taught others, including me that we need to go beyond some of the classic dichotomy's of thinking about intelligence.",
                    "label": 0
                },
                {
                    "sent": "Dichotomy's of, say you know, domain specific hardwired approaches versus domain general learning based approaches or statistics versus structured representations and ask really more integrated questions.",
                    "label": 0
                },
                {
                    "sent": "For example, how does?",
                    "label": 0
                },
                {
                    "sent": "How could domain general learning mechanisms build domain specific knowledge representations?",
                    "label": 0
                },
                {
                    "sent": "Or how could statistical learning?",
                    "label": 0
                },
                {
                    "sent": "Lead to structured knowledge representations that provide the inductive biases we need to get around in the world.",
                    "label": 0
                },
                {
                    "sent": "I think there's a bright future in cognitive science as well as an AI in exploring these ideas, thanks.",
                    "label": 0
                },
                {
                    "sent": "Like what are you thinking about exactly?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "In some sense.",
                    "label": 0
                },
                {
                    "sent": "We're mostly, you know.",
                    "label": 0
                },
                {
                    "sent": "Our first step is to try to get it right, and then we'll worry about fooling the models.",
                    "label": 0
                },
                {
                    "sent": "So, for example, one of the ways we tested this structural form discovery was to generate synthetic datasets by taking features distributed over different kinds of graph structures and showing we could recover that.",
                    "label": 0
                },
                {
                    "sent": "We haven't yet gone to the stage of saying, OK, well, what kinds of things are going to fool the model into thinking that there's this kind of structural form, when in fact there's this other kind.",
                    "label": 0
                },
                {
                    "sent": "I guess you could say in some sense that was what was going on here.",
                    "label": 0
                },
                {
                    "sent": "Right, you could say when when we just have a small amount of data, the model gets fooled into thinking the world is qualitatively simpler than it really is, that there's actually just a set of flat clusters rather than a more complex tree structured hierarchy, and I guess we could say in some sense children are fooled in the same way that children's early.",
                    "label": 0
                },
                {
                    "sent": "Categories are follow what's called the principle of mutual exclusivity.",
                    "label": 0
                },
                {
                    "sent": "Developmental psychologists have named it this because they assume that something can just be in one nameable category rather than, say, a whole hierarchical taxonomy.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's exactly what we're looking at.",
                    "label": 0
                },
                {
                    "sent": "In general, I do think that's a promising approach that any kind of inductive bias is going to lead to some kind of illusions of inference, right?",
                    "label": 0
                },
                {
                    "sent": "Like optical illusions and vision, which can show you that inductive bias at work and.",
                    "label": 0
                },
                {
                    "sent": "By studying that both in a computational model as well as in an actual human learner, you can start to see if the forms of inductive bias that you're putting into your model correspond to those that the human might have.",
                    "label": 0
                },
                {
                    "sent": "That's right, Joe.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Select models.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, that's a really good question.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting that you know in the history of AI we often see.",
                    "label": 0
                },
                {
                    "sent": "Those two things they don't always line up well.",
                    "label": 0
                },
                {
                    "sent": "They often line up in intriguing ways, right?",
                    "label": 0
                },
                {
                    "sent": "Like, for example, you know a Bayes net might be if you try to decide which of two bayonet structures is the best way of representing some set of statistics.",
                    "label": 0
                },
                {
                    "sent": "If there's really some kind of causal structure there, a Bayes net which respects the right causal order will tend to be simpler model.",
                    "label": 0
                },
                {
                    "sent": "It will be more productive and it will tend to be more efficient because there will be fewer loops in it and there's lots of other examples of that.",
                    "label": 0
                },
                {
                    "sent": "Trees are both good inductive biases, but they're also computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "But they don't have to line up.",
                    "label": 0
                },
                {
                    "sent": "We've mostly focused on what in cognitive science in the tradition of David Marr.",
                    "label": 0
                },
                {
                    "sent": "Is called the kind of computational theory level.",
                    "label": 0
                },
                {
                    "sent": "Like really looking at the abstract constraints on inference rather than the processes.",
                    "label": 0
                },
                {
                    "sent": "But there's a whole other approach you could follow by, for example, measuring reaction time or number of trials to learn things which really push the human processing engine to its capacity limits, or approve that, and then try to see you try to try to.",
                    "label": 0
                },
                {
                    "sent": "Look at that other side of things, but we, but we haven't looked at that.",
                    "label": 0
                },
                {
                    "sent": "Although you know my, my guess is that, well, I guess I'm just I'm sort of in a long range, but backburner sort of way obsessed with this interesting correspondence between computational efficiency and kind of inductive efficiency, and I'd like to understand more better how those lineup.",
                    "label": 0
                },
                {
                    "sent": "Well David yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Turns out not.",
                    "label": 0
                },
                {
                    "sent": "Tell you something about.",
                    "label": 0
                },
                {
                    "sent": "What is cognitive level?",
                    "label": 0
                },
                {
                    "sent": "You mean evolution of cognition?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, right, it's it's.",
                    "label": 0
                },
                {
                    "sent": "At this level of abstraction that we're modeling here, you know.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "I mean, this could just as well apply to evolution of cognition as it could to learning, but I don't think, well, I don't think that's that's the main thing we're studying here.",
                    "label": 0
                },
                {
                    "sent": "So in one of the slides I had to skip at the end we've actually been looking at how people learn structural forms like this in the lab, and people can infer different kinds of structural forms from relatively short experience of observing objects and properties and relations, and then use those inductive biases to generalize to new things with only sparse experience, so they can learn in a relatively short time scale of minutes.",
                    "label": 0
                },
                {
                    "sent": "These kinds of things if you if you sort of prepare their mind and give them the right sort of data.",
                    "label": 0
                },
                {
                    "sent": "I think also just I don't think the debate between nativism and or you know nature versus nurture.",
                    "label": 0
                },
                {
                    "sent": "It's not just about the time scale.",
                    "label": 0
                },
                {
                    "sent": "In that, in the traditional approaches of both the sort of nature and nurture approaches, in both cases there's this, I think, implicit assumption that the more abstract knowledge is going to take longer to learn.",
                    "label": 0
                },
                {
                    "sent": "I mean, both of them are really long timescales, right?",
                    "label": 0
                },
                {
                    "sent": "So evolution is a long time scale, but the kind of gradual, many, many, many examples.",
                    "label": 0
                },
                {
                    "sent": "Many trials sort of learning that you get, say in connectionism or in sort of typical Bayes net structure learning, where you need a lot of data to really learn anything relevant about the structure.",
                    "label": 0
                },
                {
                    "sent": "Both of those are long and what's?",
                    "label": 0
                },
                {
                    "sent": "Things that's most striking to me as I tried to emphasize throughout the talk, is this blessing of abstraction.",
                    "label": 0
                },
                {
                    "sent": "The fact that these powerful abstractions can be learned relatively quickly, and that really seems to require a totally different approach than either of those two classic approaches.",
                    "label": 0
                },
                {
                    "sent": "Rich bad science.",
                    "label": 0
                },
                {
                    "sent": "So you look at the data.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "That well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I think science is an example of what I was talking about about the fact that on the one hand we have strong biases, but we also have to keep an open mind in a sense.",
                    "label": 0
                },
                {
                    "sent": "So I feel like I've been trying to do that, but of course you also have to do things like write papers and market your work and give talks and so that lends a certain rhetorical bias.",
                    "label": 0
                },
                {
                    "sent": "I tried to show for example in one of these.",
                    "label": 0
                },
                {
                    "sent": "Things back here.",
                    "label": 0
                },
                {
                    "sent": "And this slide only represents a small portion of some of the work we've done in taking these more quantitatively rich domains where you can get people to make lots of different inductive judgments about a set of objects.",
                    "label": 0
                },
                {
                    "sent": "We've we've explored many, many different kinds of models.",
                    "label": 0
                },
                {
                    "sent": "It's all broadly within a kind of Bayesian approach, but but for example, we've been.",
                    "label": 0
                },
                {
                    "sent": "We've been at pains to explore qualitatively more or less kinds of structure.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this raw covariance thing.",
                    "label": 0
                },
                {
                    "sent": "It's essentially basically just to kind of learning feature relevance, I mean.",
                    "label": 0
                },
                {
                    "sent": "In a way that's almost sort of hand coded in because there's a set of features that people judged as the relevant ones for biology, and essentially what you can see is that that's not sufficiently strong inductive bias.",
                    "label": 0
                },
                {
                    "sent": "The you know in some sense you could see the kind of either of these more structured approaches, like learning a tree, structured representation, or ultra low dimensional space as a more sophisticated kind of learning feature relevance where basically it's filtered by the structural constraint.",
                    "label": 0
                },
                {
                    "sent": "So when you learn that restructured model, what essentially you've learned, I think I would say it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's it's.",
                    "label": 0
                },
                {
                    "sent": "It's rather similar to the idea of learning that just the features which best fit the tree structure are the relevant ones for guiding generalization, and I think if you just filtered out the features which are best explained by the tree and then just use the raw covariance kernel on those, it would be very similar.",
                    "label": 0
                },
                {
                    "sent": "Well, except that there wouldn't be enough of them, but it would.",
                    "label": 0
                },
                {
                    "sent": "It would if you had a huge set of features and you just filtered out the best ones.",
                    "label": 0
                },
                {
                    "sent": "That would be very similar, but partly what I've tried to show here by comparing, say, the different kinds of structured representations is that it isn't enough just to say you somehow filter out the right ones, because at least in this case, there's a qualitative difference between using the tree structure to constrain if you like, which features are relevant versus using other kinds of structures, and for other kinds of domains, it comes out differently.",
                    "label": 0
                },
                {
                    "sent": "So I think you know I try to keep an open mind about these things and I can only fit so much into talk, but but I agree that we should.",
                    "label": 0
                },
                {
                    "sent": "We should be considering both richer and less rich representations.",
                    "label": 0
                },
                {
                    "sent": "I've been led to some of these richer ones by the need to try to explain how we can learn so much from so little.",
                    "label": 0
                },
                {
                    "sent": "But I think we we should constantly be checking our inductive bias as you said.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}