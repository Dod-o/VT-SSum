{
    "id": "ooudmybvykgghf6abvylu66ayofgk2dn",
    "title": "What Helps Where - And Why? Semantic Relatedness for Knowledge Transfer",
    "info": {
        "author": [
            "Marcus Rohrbach, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Computer Vision->Object Recognition"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_rohrbach_whw/",
    "segmentation": [
        [
            "Hello everyone, let me welcome you to my talk and the talk we just heard.",
            "Consider transferring knowledge from noisy Flickr tags to unlabeled images, and we try to transfer knowledge between image classes with the help of semantic relatedness from language specifically."
        ],
        [
            "We are interested in classifying unseen image classes.",
            "So for example, we like to decide if there's a giant panda on this image, but without having seen any child pandas before.",
            "And actually recently there have been approaches using attributes to tackle this problem, and we had two prominent directions presented at last year CPR who use attributes and the one is by far hardly at all who describe describe unknown classes using attributes for example animal.",
            "Fallout and normal would be description for polar bear.",
            "The other approach was by Lampert at all and they describe and a crew classes by attributes so they would describe the attribute polar bear by white and poor.",
            "So.",
            "This approach is still requires supervision to enable knowledge transfer and S in the form of attribute labels or in the form of objectclass attribute associations.",
            "And what we try to do is to replace the supervision by.",
            "I'm using semantic relatedness."
        ],
        [
            "Remind from language specifically, we query word Net, Flickr, Yahoo and Wikipedia to then achieve a fully unsupervised knowledge transfer.",
            "I like to detail out."
        ],
        [
            "The model of lampitt we're using again, so we have a set of training classes which are known and we have a set of attributes where we associate all the non training classes before.",
            "So for example there may shun has spots and is wide, but a Dalmatian obviously does not live in the ocean.",
            "And then we also have these unknown classes.",
            "Um?",
            "Here, shown here and they are associated to the attribute classifiers we trained.",
            "And this way we can transfer the knowledge from the known training classes over the attribute classifiers who the unseen test classes.",
            "Now what remains are the class attribute associations shown as Sudan, and so far they have been provided by human experts who decided if a certain Association existed or not and what we suggest this."
        ],
        [
            "To replace the supervision via mining, semantic relatedness from language.",
            "Now we addition to this first model.",
            "We also use a second model and."
        ],
        [
            "Here."
        ],
        [
            "And we directly train a classifier for each of these separate classes.",
            "So we train the classifier formation for polar bear and for killer whale.",
            "And we also say that each of these with each of these unseen test classes, with the most similar one so.",
            "For example, steel would be associated with killer whale and we do not only follow most."
        ],
        [
            "First, most similar, but also for the five most similar classes we associate."
        ],
        [
            "So now that we have seen the two models, we're going to look into detail at the semantic relatedness measures reuse, and we chose a for each of these resources we choose the measure which is considered being state of the art in the language community.",
            "So we start off with word net."
        ],
        [
            "And what it is is actually quite a widely used resource in computer vision, and it's an ontology which.",
            "Has the most abstract term at the top entity and then goes down and her small concrete terms at the bottom."
        ],
        [
            "And the idea of the Lynn measure we are using is that we associate that those terms which appear closely in the trees, such as horse and elephant, are more related than those terms appearing further apart, such as horse and car.",
            "In this toy example.",
            "So there was."
        ],
        [
            "First example, the 2nd in our first measure and resource and the second resource is Wikipedia give used the explicit semantic analysis and we use the frequencies over articles.",
            "So if we have horse and elephant which we want to compare with computer frequency of appearing in all articles such as Farm who Van Task.",
            "And."
        ],
        [
            "Given these vectors or frequencies, we can compare them using the cosine, which then gives us the semantic relatedness."
        ],
        [
            "The next rest as we look at this, the World Wide Web probably largest resource in the world, which fairly accessible.",
            "And here we use hit count.",
            "So we look at how frequently this horse an elephant, appear together in the web and more the hired a hit count them a likely they are similar and we do normalize this by using the dice coefficient.",
            "The."
        ],
        [
            "4th Measure we're using is image search and here we hope that we get more visually relevant information and compared to web search which also is has quite a lot of incidental core occurrences and because these terms hopefully refer to the same images or poison elephant actually have to be on the image at least.",
            "Hopefully we also hope to have terms referring to each other.",
            "So for it's an elephant.",
            "Being similar.",
            "And again we use hit count and the dice coefficient."
        ],
        [
            "So now that we have seen the two models were using and different semantic relatedness measures to mind their resources, we can now evaluate first the attribute based model and on the direct similarity based model."
        ],
        [
            "At first, look at the experimental setup.",
            "So we used the Animus attribute data set, which was introduced by Lamictal at last year's VPR, which consists of 40 training and disjoint dentist joined test classes, and for computational feasibility, downsampled the data set or the training data set to 92 images per class.",
            "And we always compare to the provided manual associations to 85 given attributes.",
            "And for image classification we use a standard setup.",
            "We use SVM, resistor, Chrome, intersection, kernel and use the provided image features which consists of Fitch 5th, an color histogram for example.",
            "In total is 6 different features.",
            "And we always report the mean overall 10 plus classes for the area under the RC curve, which we use as performance measure."
        ],
        [
            "Let's first look at the performance of the supervised approach.",
            "We achieve nearly 80% mean area under the RC curve, which is similar to performance reported by prior work.",
            "And now if you look at the."
        ],
        [
            ".",
            "Class attributes associations.",
            "We see that we get quite far, but there's a drop in performance, but we still think that this result is quite encouraging as we do not have any supervision we had before for providing the associations from classes to attributes.",
            "So this performance drop can be explained 4 by 1 fact that we for clearing the associations we use only appreciation.",
            "So for example you see operation a child but the human experts were given full length descriptions.",
            "For example having a high degree of physical coordination.",
            "OK, now let's look at today."
        ],
        [
            "Different measures how they compare best is clearly image search in Wikipedia and image shirt is based on image related text and for Wikipedia we found that it's actually quite a robust resource as it's based on definition texts.",
            "Then next follows is Yahoo Rap, where there's quite a big drop."
        ],
        [
            "And we found that it's a very noisy resource, so we have a lot of incidental core occurrences in the World Wide Web."
        ],
        [
            "Written, it does not have any incidental core occurrences, but the measure we are using path lengths.",
            "Also, it's a standard measure used for word.",
            "Net is actually a poor indicator for class attribute associations, and it's actually quite obvious since we look at this example here.",
            "The attributes and the terms of the object terms tend to be.",
            "It will be there in very different parts of the tree, so the path is not really a good measure anymore.",
            "So if you would need to decide if horses or elephants have a task, we couldn't decide becausw the task the past links is the same.",
            "In this toy example here.",
            "OK, so far the did you."
        ],
        [
            "Although provided attributes with the attribute provider with the data set, but defining such an attribute set is actually quite a tedious task if we move to new domain.",
            "So what we as a check?"
        ],
        [
            "Access to use the word net."
        ],
        [
            "To mind is attributes and specifically their impart relations encoded words for net which you use to mine part attributes.",
            "So parts are for example, lag of a cow and having these part terms, we can then use the things we used before so we can use semantic relatedness to associate to known classes and unseen test classes.",
            "We also learn a classifier again as before, and now that we have."
        ],
        [
            "Only part attributes.",
            "We can use a special measure which uses Hallinan patterns which are provided in language.",
            "So for example, you have cause lag or lack of a cow, and they specifically denote part whole associations.",
            "So lag is a part of cow.",
            "And here again we used the dice coefficient and with the measure we hope to reduce the incident occur occurrences which normally appear in normal web search."
        ],
        [
            "OK, the results are actually quite encouraging.",
            "We see that Yahoo HoloLens, this newly introduced measure, actually performs close to the manual attributes shown in blue and.",
            "This can be explained because it's tailored to this part attributes."
        ],
        [
            "But apart from this we see that there is quite a drop from the manual attributes shown in blue to the mind attributes shown in cream.",
            "And that can be explained by two facts first, then we choose diversity of attributes and the specialized terms we are using which we mined from word NET.",
            "And they're not that of all covered in image search in Wikipedia."
        ],
        [
            "OK, now we evaluated direct similarity."
        ],
        [
            "This model and here we also donate do not need any supervision to.",
            "Because there are no attributes and remind these most most similar classes."
        ],
        [
            "So the performance is shown here in Orange and actually we achieve performance which is on par with the manual supervised approach shown in black apart from word net business a bit below.",
            "And there are also clearly above the performance on the attribute based model.",
            "The reason for this is that the five most related classes tend to be quite reliable and also very similar across the different methods.",
            "The last thing I want to look at is how does the attribute and direct similarity based?"
        ],
        [
            "Compare and so far we always looked at this joint training and test set, test test classes, but the question is what happens if the classifiers to decide against known classes.",
            "For this we extend to test that and add images from the known classes as negatives and we hope to get a more realistic setting in this way.",
            "Here what we see so far is the result we had with the old setting.",
            "And now when we add.",
            "Images from the from the known classes we see there's a clear drop in performance for the direct similarity approach shown in orange, while the actually based models actually improve performance.",
            "Is this?"
        ],
        [
            "I come to come to the conclusion of my talk so."
        ],
        [
            "So we fully replaced supervision with semantic relatedness and direct similarity.",
            "Approach showed better performance that attribute based approach and actually achieved performance which is on par with the supervised approach."
        ],
        [
            "However, the attribute based approach generalizes.",
            "Better general is better."
        ],
        [
            "And if you compare the different semantic relatedness measures, we see that Yahoo images overall best and the Hallinan patterns which we introduced providing an improvement but are limited to part attributes.",
            "Word net provides rather poor performance, especially for object attribute associations."
        ],
        [
            "If you are interested in computing these measures for your own set of classes, we provide software at our web page, and now I hope are successfully transferred all the knowledge to you.",
            "Thank you.",
            "Yes.",
            "Brother or something?",
            "So the question was, if we're crawling the web for what?",
            "Yes.",
            "Yeah.",
            "Oh OK, so the question was if we always update the semantic relatedness and and we don't, we just did it one time.",
            "We crawled the web for example for this hit count based measures we crawled the web and ask for this semantic relatedness.",
            "And if we do it now again then there will be different results there and we did actually notice then if we called it several times the performance slightly vary.",
            "But the general thing is still the same, so that's at least how we our experience.",
            "But there might actually be changes, especially if we go to this different.",
            "Search engine, you might get slightly different results.",
            "I just wanted to make a comment.",
            "I've seen a lot of people using word net and I don't know how many are aware that the word net was manually developed right over a period of years.",
            "So there are some inconsistencies in the data, particularly with respect to symmetry of relations.",
            "And when you get to do meta concepts like as Beyond so hypernyms or hyponyms, you're going to have problems.",
            "So just to be aware of that for using word net.",
            "There are alternatives available, but you might want to just think about this fact when you use Wordnet as a resource.",
            "Yeah, I agree.",
            "Other questions the other questions.",
            "OK, it's not.",
            "Let's take this people off again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, let me welcome you to my talk and the talk we just heard.",
                    "label": 0
                },
                {
                    "sent": "Consider transferring knowledge from noisy Flickr tags to unlabeled images, and we try to transfer knowledge between image classes with the help of semantic relatedness from language specifically.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are interested in classifying unseen image classes.",
                    "label": 0
                },
                {
                    "sent": "So for example, we like to decide if there's a giant panda on this image, but without having seen any child pandas before.",
                    "label": 1
                },
                {
                    "sent": "And actually recently there have been approaches using attributes to tackle this problem, and we had two prominent directions presented at last year CPR who use attributes and the one is by far hardly at all who describe describe unknown classes using attributes for example animal.",
                    "label": 1
                },
                {
                    "sent": "Fallout and normal would be description for polar bear.",
                    "label": 1
                },
                {
                    "sent": "The other approach was by Lampert at all and they describe and a crew classes by attributes so they would describe the attribute polar bear by white and poor.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "This approach is still requires supervision to enable knowledge transfer and S in the form of attribute labels or in the form of objectclass attribute associations.",
                    "label": 0
                },
                {
                    "sent": "And what we try to do is to replace the supervision by.",
                    "label": 0
                },
                {
                    "sent": "I'm using semantic relatedness.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remind from language specifically, we query word Net, Flickr, Yahoo and Wikipedia to then achieve a fully unsupervised knowledge transfer.",
                    "label": 0
                },
                {
                    "sent": "I like to detail out.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model of lampitt we're using again, so we have a set of training classes which are known and we have a set of attributes where we associate all the non training classes before.",
                    "label": 0
                },
                {
                    "sent": "So for example there may shun has spots and is wide, but a Dalmatian obviously does not live in the ocean.",
                    "label": 0
                },
                {
                    "sent": "And then we also have these unknown classes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here, shown here and they are associated to the attribute classifiers we trained.",
                    "label": 0
                },
                {
                    "sent": "And this way we can transfer the knowledge from the known training classes over the attribute classifiers who the unseen test classes.",
                    "label": 1
                },
                {
                    "sent": "Now what remains are the class attribute associations shown as Sudan, and so far they have been provided by human experts who decided if a certain Association existed or not and what we suggest this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To replace the supervision via mining, semantic relatedness from language.",
                    "label": 1
                },
                {
                    "sent": "Now we addition to this first model.",
                    "label": 0
                },
                {
                    "sent": "We also use a second model and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we directly train a classifier for each of these separate classes.",
                    "label": 0
                },
                {
                    "sent": "So we train the classifier formation for polar bear and for killer whale.",
                    "label": 1
                },
                {
                    "sent": "And we also say that each of these with each of these unseen test classes, with the most similar one so.",
                    "label": 0
                },
                {
                    "sent": "For example, steel would be associated with killer whale and we do not only follow most.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, most similar, but also for the five most similar classes we associate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have seen the two models, we're going to look into detail at the semantic relatedness measures reuse, and we chose a for each of these resources we choose the measure which is considered being state of the art in the language community.",
                    "label": 0
                },
                {
                    "sent": "So we start off with word net.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what it is is actually quite a widely used resource in computer vision, and it's an ontology which.",
                    "label": 0
                },
                {
                    "sent": "Has the most abstract term at the top entity and then goes down and her small concrete terms at the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea of the Lynn measure we are using is that we associate that those terms which appear closely in the trees, such as horse and elephant, are more related than those terms appearing further apart, such as horse and car.",
                    "label": 0
                },
                {
                    "sent": "In this toy example.",
                    "label": 0
                },
                {
                    "sent": "So there was.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First example, the 2nd in our first measure and resource and the second resource is Wikipedia give used the explicit semantic analysis and we use the frequencies over articles.",
                    "label": 0
                },
                {
                    "sent": "So if we have horse and elephant which we want to compare with computer frequency of appearing in all articles such as Farm who Van Task.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given these vectors or frequencies, we can compare them using the cosine, which then gives us the semantic relatedness.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next rest as we look at this, the World Wide Web probably largest resource in the world, which fairly accessible.",
                    "label": 0
                },
                {
                    "sent": "And here we use hit count.",
                    "label": 0
                },
                {
                    "sent": "So we look at how frequently this horse an elephant, appear together in the web and more the hired a hit count them a likely they are similar and we do normalize this by using the dice coefficient.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "4th Measure we're using is image search and here we hope that we get more visually relevant information and compared to web search which also is has quite a lot of incidental core occurrences and because these terms hopefully refer to the same images or poison elephant actually have to be on the image at least.",
                    "label": 1
                },
                {
                    "sent": "Hopefully we also hope to have terms referring to each other.",
                    "label": 0
                },
                {
                    "sent": "So for it's an elephant.",
                    "label": 0
                },
                {
                    "sent": "Being similar.",
                    "label": 0
                },
                {
                    "sent": "And again we use hit count and the dice coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have seen the two models were using and different semantic relatedness measures to mind their resources, we can now evaluate first the attribute based model and on the direct similarity based model.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At first, look at the experimental setup.",
                    "label": 1
                },
                {
                    "sent": "So we used the Animus attribute data set, which was introduced by Lamictal at last year's VPR, which consists of 40 training and disjoint dentist joined test classes, and for computational feasibility, downsampled the data set or the training data set to 92 images per class.",
                    "label": 1
                },
                {
                    "sent": "And we always compare to the provided manual associations to 85 given attributes.",
                    "label": 0
                },
                {
                    "sent": "And for image classification we use a standard setup.",
                    "label": 0
                },
                {
                    "sent": "We use SVM, resistor, Chrome, intersection, kernel and use the provided image features which consists of Fitch 5th, an color histogram for example.",
                    "label": 0
                },
                {
                    "sent": "In total is 6 different features.",
                    "label": 0
                },
                {
                    "sent": "And we always report the mean overall 10 plus classes for the area under the RC curve, which we use as performance measure.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's first look at the performance of the supervised approach.",
                    "label": 1
                },
                {
                    "sent": "We achieve nearly 80% mean area under the RC curve, which is similar to performance reported by prior work.",
                    "label": 0
                },
                {
                    "sent": "And now if you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ".",
                    "label": 0
                },
                {
                    "sent": "Class attributes associations.",
                    "label": 0
                },
                {
                    "sent": "We see that we get quite far, but there's a drop in performance, but we still think that this result is quite encouraging as we do not have any supervision we had before for providing the associations from classes to attributes.",
                    "label": 0
                },
                {
                    "sent": "So this performance drop can be explained 4 by 1 fact that we for clearing the associations we use only appreciation.",
                    "label": 0
                },
                {
                    "sent": "So for example you see operation a child but the human experts were given full length descriptions.",
                    "label": 0
                },
                {
                    "sent": "For example having a high degree of physical coordination.",
                    "label": 1
                },
                {
                    "sent": "OK, now let's look at today.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different measures how they compare best is clearly image search in Wikipedia and image shirt is based on image related text and for Wikipedia we found that it's actually quite a robust resource as it's based on definition texts.",
                    "label": 0
                },
                {
                    "sent": "Then next follows is Yahoo Rap, where there's quite a big drop.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we found that it's a very noisy resource, so we have a lot of incidental core occurrences in the World Wide Web.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Written, it does not have any incidental core occurrences, but the measure we are using path lengths.",
                    "label": 0
                },
                {
                    "sent": "Also, it's a standard measure used for word.",
                    "label": 0
                },
                {
                    "sent": "Net is actually a poor indicator for class attribute associations, and it's actually quite obvious since we look at this example here.",
                    "label": 0
                },
                {
                    "sent": "The attributes and the terms of the object terms tend to be.",
                    "label": 0
                },
                {
                    "sent": "It will be there in very different parts of the tree, so the path is not really a good measure anymore.",
                    "label": 0
                },
                {
                    "sent": "So if you would need to decide if horses or elephants have a task, we couldn't decide becausw the task the past links is the same.",
                    "label": 0
                },
                {
                    "sent": "In this toy example here.",
                    "label": 0
                },
                {
                    "sent": "OK, so far the did you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Although provided attributes with the attribute provider with the data set, but defining such an attribute set is actually quite a tedious task if we move to new domain.",
                    "label": 0
                },
                {
                    "sent": "So what we as a check?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Access to use the word net.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To mind is attributes and specifically their impart relations encoded words for net which you use to mine part attributes.",
                    "label": 1
                },
                {
                    "sent": "So parts are for example, lag of a cow and having these part terms, we can then use the things we used before so we can use semantic relatedness to associate to known classes and unseen test classes.",
                    "label": 1
                },
                {
                    "sent": "We also learn a classifier again as before, and now that we have.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only part attributes.",
                    "label": 0
                },
                {
                    "sent": "We can use a special measure which uses Hallinan patterns which are provided in language.",
                    "label": 0
                },
                {
                    "sent": "So for example, you have cause lag or lack of a cow, and they specifically denote part whole associations.",
                    "label": 1
                },
                {
                    "sent": "So lag is a part of cow.",
                    "label": 1
                },
                {
                    "sent": "And here again we used the dice coefficient and with the measure we hope to reduce the incident occur occurrences which normally appear in normal web search.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the results are actually quite encouraging.",
                    "label": 0
                },
                {
                    "sent": "We see that Yahoo HoloLens, this newly introduced measure, actually performs close to the manual attributes shown in blue and.",
                    "label": 1
                },
                {
                    "sent": "This can be explained because it's tailored to this part attributes.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But apart from this we see that there is quite a drop from the manual attributes shown in blue to the mind attributes shown in cream.",
                    "label": 1
                },
                {
                    "sent": "And that can be explained by two facts first, then we choose diversity of attributes and the specialized terms we are using which we mined from word NET.",
                    "label": 0
                },
                {
                    "sent": "And they're not that of all covered in image search in Wikipedia.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we evaluated direct similarity.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model and here we also donate do not need any supervision to.",
                    "label": 0
                },
                {
                    "sent": "Because there are no attributes and remind these most most similar classes.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the performance is shown here in Orange and actually we achieve performance which is on par with the manual supervised approach shown in black apart from word net business a bit below.",
                    "label": 1
                },
                {
                    "sent": "And there are also clearly above the performance on the attribute based model.",
                    "label": 1
                },
                {
                    "sent": "The reason for this is that the five most related classes tend to be quite reliable and also very similar across the different methods.",
                    "label": 1
                },
                {
                    "sent": "The last thing I want to look at is how does the attribute and direct similarity based?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compare and so far we always looked at this joint training and test set, test test classes, but the question is what happens if the classifiers to decide against known classes.",
                    "label": 0
                },
                {
                    "sent": "For this we extend to test that and add images from the known classes as negatives and we hope to get a more realistic setting in this way.",
                    "label": 1
                },
                {
                    "sent": "Here what we see so far is the result we had with the old setting.",
                    "label": 0
                },
                {
                    "sent": "And now when we add.",
                    "label": 1
                },
                {
                    "sent": "Images from the from the known classes we see there's a clear drop in performance for the direct similarity approach shown in orange, while the actually based models actually improve performance.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I come to come to the conclusion of my talk so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we fully replaced supervision with semantic relatedness and direct similarity.",
                    "label": 0
                },
                {
                    "sent": "Approach showed better performance that attribute based approach and actually achieved performance which is on par with the supervised approach.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the attribute based approach generalizes.",
                    "label": 0
                },
                {
                    "sent": "Better general is better.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you compare the different semantic relatedness measures, we see that Yahoo images overall best and the Hallinan patterns which we introduced providing an improvement but are limited to part attributes.",
                    "label": 0
                },
                {
                    "sent": "Word net provides rather poor performance, especially for object attribute associations.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you are interested in computing these measures for your own set of classes, we provide software at our web page, and now I hope are successfully transferred all the knowledge to you.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Brother or something?",
                    "label": 0
                },
                {
                    "sent": "So the question was, if we're crawling the web for what?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so the question was if we always update the semantic relatedness and and we don't, we just did it one time.",
                    "label": 1
                },
                {
                    "sent": "We crawled the web for example for this hit count based measures we crawled the web and ask for this semantic relatedness.",
                    "label": 0
                },
                {
                    "sent": "And if we do it now again then there will be different results there and we did actually notice then if we called it several times the performance slightly vary.",
                    "label": 0
                },
                {
                    "sent": "But the general thing is still the same, so that's at least how we our experience.",
                    "label": 0
                },
                {
                    "sent": "But there might actually be changes, especially if we go to this different.",
                    "label": 0
                },
                {
                    "sent": "Search engine, you might get slightly different results.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to make a comment.",
                    "label": 0
                },
                {
                    "sent": "I've seen a lot of people using word net and I don't know how many are aware that the word net was manually developed right over a period of years.",
                    "label": 0
                },
                {
                    "sent": "So there are some inconsistencies in the data, particularly with respect to symmetry of relations.",
                    "label": 0
                },
                {
                    "sent": "And when you get to do meta concepts like as Beyond so hypernyms or hyponyms, you're going to have problems.",
                    "label": 0
                },
                {
                    "sent": "So just to be aware of that for using word net.",
                    "label": 0
                },
                {
                    "sent": "There are alternatives available, but you might want to just think about this fact when you use Wordnet as a resource.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "Other questions the other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not.",
                    "label": 1
                },
                {
                    "sent": "Let's take this people off again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}