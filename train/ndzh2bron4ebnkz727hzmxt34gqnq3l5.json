{
    "id": "ndzh2bron4ebnkz727hzmxt34gqnq3l5",
    "title": "Using Partial Reference Alignments to Align Ontologies",
    "info": {
        "author": [
            "Patrick Lambrix, Division for Databases and Information Techniques, Linkoping University"
        ],
        "published": "July 28, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Semantic Web->Ontologies"
        ]
    },
    "url": "http://videolectures.net/eswc09_lambrix_upra/",
    "segmentation": [
        [
            "OK, so thank you.",
            "My name is Patrick Lambrix and this is work together with my student Yucheng.",
            "Before I started.",
            "Also want to thank Christian Malika for running or algorithms on one of the datasets.",
            "So we're talking about using partial reference alignments to align ontologies.",
            "Now for this crowds, and considering that you have already heard to talk."
        ],
        [
            "About ontology alignment.",
            "Where the first part of introduction is going to be very fast.",
            "We know that there are many ontologies, many of them have overlapping information.",
            "You want to use them together.",
            "Well, then you need to know the entology ratio."
        ],
        [
            "Ships so given to ontology."
        ],
        [
            "Determining their correspondences between the terms and the different ontologies we call ontology alignments."
        ],
        [
            "There we go.",
            "Now, many of today's systems actually follow this kind of framework as input.",
            "You have a number of ontologies.",
            "These can be pre processed and then given to matures.",
            "Much as usually compute similarity values between terms in different domains between terms in the different ontologies.",
            "And then they can use external knowledge like instances, general dictionary or domain knowledge.",
            "The results of these measures can be combined in filtered in different ways so that we get mapping suggestions.",
            "Then in phase two, usually a user can look at these mapping suggestions and accept or reject them.",
            "An acceptance and rejection of.",
            "Suggestions can influence new iteration in this process.",
            "The end result is an alignment or a set of mappings."
        ],
        [
            "Now recently a new situation has occurred.",
            "First of all, a number of portals have appeared where mappings are actually stored mappings between ontologies.",
            "Then as I just told you, several systems actually allow the acceptance and rejection of suggestions to influence the next step in iteration.",
            "And then last year in the ontology Alignment Evaluation Initiative, the anatomy track introduced a new task task for where the idea was that you already got some of the correct mappings, and could you then?",
            "Give better results or give higher quality results for the ontology alignment based on this.",
            "So common in all these cases that you already have some of the correct mappings and they are given or they have been obtained in some way and a subset of all the correct mappings.",
            "We called in a partial reference."
        ],
        [
            "So our research problem that we looked at is can we use these partial reference alignments to obtain higher quality suggestions in the ontology alignments?",
            "And we are."
        ],
        [
            "Refined this question to can we look at the different components in our ontology framework and use the partial reference alignments in these components?",
            "And we have basically looked at the preprocessing step of matching step in the filter step."
        ],
        [
            "So the remainder of my talk I will first start with some background tell you about the baseline systems that we've used in our test cases.",
            "Then show you the test cases, then it will go onto.",
            "Actually talking about algorithms and saying whether they're any good.",
            "And then we'll have some conclusion answer."
        ],
        [
            "Work.",
            "So let us."
        ],
        [
            "Start with the baseline systems.",
            "So the first systems called some bullets system for aligning and merging biomedical ontologies.",
            "By the way, Sam was also a Swedish word.",
            "It means the person you live together with, so hopefully it's somebody you like.",
            "And that's of course the feeling that we want you to have about our system.",
            "To my students, on the other hand, I say, is the person you spend a lot of time with, so.",
            "This is hopefully what you do with this system as well.",
            "So somebody implements a number of measures.",
            "We have some string based measures, string bass patches using word net, some matches using domain knowledge.",
            "Instance based measures and also structure based matches and results of these matches can be combined using a way to some combination and somebody uses single threshold filtering.",
            "That means that pairs of concepts or dissimilarity value higher or equal to the threshold are given to the user as mapping suggestions."
        ],
        [
            "Then in the second phase, the user can look at these.",
            "Here is an example of a small test case where.",
            "Item.",
            "Suggest that nasal cavity epithelium is the same as nasal mucosa and the string based measure taking into account synonym should have found this one.",
            "So the user can then say whether these concepts are equivalent or the one is a sub counts of the other.",
            "Vice versa can also reject them.",
            "We also see that there is a manual alignment mode, so which can be interleaved with this suggestion mode."
        ],
        [
            "Then somebody DF is similar to summer, but it uses a more advanced threshold filtering approach and.",
            "We introduced this to deal with the following observation.",
            "When you use a single threshold, usually the higher the threshold you have.",
            "The more of your suggestions will be correct, but at the same time you don't find as many correct suggestions."
        ],
        [
            "So what we proposed was to use two thresholds.",
            "First we have the upper thresholds and pairs of concepts with similarity values above this higher threshold they will be given to the user's suggestions.",
            "Pairs of concepts with similarity value lower than the lower threshold.",
            "They will be rejected, but in between we will look at the.",
            "At the pairs of concepts we will check what are reasonable prospectus structure of the ontologies and mapping suggestions from step one."
        ],
        [
            "So here's an example.",
            "So assume we have two ontologies.",
            "We have already calculated their similarity values.",
            "We have an upper threshold and a lower threshold, so these guys are going to be retained as suggestions and the ones below the lower threshold are going to be discarded.",
            "But these were going to look at by.",
            "We're going to filter them based on the structure of the ontology.",
            "So what is that idea?",
            "Well, if you know that you have a mapping suggestion, a prime and this is an equivalence mapping and you have a suggestion.",
            "Our mapping BB prime.",
            "This is also equivalent mapping.",
            "Then if you know that a isabi, then in the other entology want a prime to be a B prime.",
            "So for instance, if we have this as a mapping to be.",
            "Then this is an equivalence.",
            "Then you weren't only sub concepts of two to be mapped to sub concepts of B.",
            "So what we do is actually for a suggestion to be we divide on the two ontologies into subclasses or sub councils of two and rest and the other ontology sub counts observed B interest.",
            "When you do this for all of the mapping suggestions above, the upper threshold, you actually partition the two ontologies.",
            "What we do then is we only retain suggestions for which the concepts belong to corresponding map of groups.",
            "So for instance 5 and E will be retained because both are in the Red Group, which actually are the sub counts of this two and B and 5C will not be retained.",
            "Becausw 5 is in the Red Group and C as in the Blue Group.",
            "Now this only works if the suggestions above the upper threshold actually satisfy this property."
        ],
        [
            "So in this example, where 5C is also above the upper threshold, we see that five is a 2, but she is not to be.",
            "So what we do in this case is we try to find a subset of the suggestions above the upper threshold which followed this property.",
            "And then based on this consistent group as we call it, we partitioned ontology.",
            "So what do we do then?",
            "Well, we only allow.",
            "And we do the same for four as before, so we only allow suggestions where the.",
            "The concepts belong to corresponding map."
        ],
        [
            "Groups so the actual baseline systems that we used in our sample and somebody TF as they are for the ontology Alignment Evaluation Initiative last year, which means that we actually removed phase two as there are no users.",
            "An as there are no users, we also changed our algorithms a little bit so that.",
            "A term only appears in one mapping suggestion.",
            "In the full sample system, a term can actually appear in several suggestions and user can make a choice there.",
            "The metrics that we use are termed turn WN, which is essentially a combination of several string matching algorithms and that uses word NET as a dictionary.",
            "And then we have another measure which uses unified Medical language system as a domain knowledge dictionary.",
            "The combination that we use in this system is maximum based strategy and the filters are then single and double threshold filter."
        ],
        [
            "The test cases that we use are five smaller cases that we used in previous work.",
            "Behavior and defense are parts of genealogy and signal ontology, nose, ear and eye or parts of the adult mouse anatomy and medical subject headings.",
            "And then the larger test case that we use is actually the anatomy data set from the ontology Alignment Evaluation Initiative.",
            "Here you see the number of concepts in the first ontology, number of concepts in the second one.",
            "The number of correct mappings and these are the P arrays that we use, so the subset of the correct mappings."
        ],
        [
            "We evaluated the OR algorithms using precision, so it tells you of the suggestions that you felt.",
            "How many were correct.",
            "Recall how many of the correct mappings do you actually find an F measure is a mean of precision recall?",
            "We also looked at recall PRA, which is actually a call with respect to the non given part of the reference alignment.",
            "They're interested to talk about, well, actually not mentioned this one, 'cause results range from really zero to 1.",
            "So let us look."
        ],
        [
            "It's a different algorithm."
        ],
        [
            "So first we see that we have someone, somebody, DF which do not have any preprocessing.",
            "They have these two measures, maximum combination algorithm and single and double threshold filter.",
            "We do not change the combination algorithm, but we have two algorithms which particularly look at preprocessing, one at matching and three at filtering."
        ],
        [
            "So let us look how we can use the."
        ],
        [
            "Arrays in the preprocessing step.",
            "So the intuition is now we have these correct mappings.",
            "We have the structure of the ontology.",
            "Let's use this structural to partition the ontology in at the.",
            "Let's use the correct mappings to partition ontology into map of the groups."
        ],
        [
            "So the first method is.",
            "The idea is very similar to the partitioning step in the double threshold filtering.",
            "So first of all.",
            "We have given a PRA.",
            "We find that consistent group in the PRA.",
            "And then we partition the ontologies into mapping mapping."
        ],
        [
            "So for instance this.",
            "And what do we do then?",
            "Well then, we essentially reduce the search space for the for the matching algorithms, so we'll only allow.",
            "Pairs of concepts that belong to corresponding map of the groups.",
            "So you don't look at all the pairs anymore.",
            "Now what we also note."
        ],
        [
            "It was that in many cases the structure of the original ontology is actually not complete.",
            "For instance.",
            "If we see here where we have these two ontologies and we have the correct mappings 6D, these are equivalent and nine G or equivalent.",
            "So we also know that G is a deontology too, so ascent actually 9 should also be a 610.",
            "Now this is the case that appears in real situations, for instance in the data set for anatomy track we found that are about 90 missing is a relations based on this reasoning.",
            "So what we do here is, well, first, we fix the ontology.",
            "We add.",
            "These missing is relations."
        ],
        [
            "And then we use the previous approach.",
            "So based on the fixed ontologies we partition into my public groups."
        ],
        [
            "OK, so how good are these algorithms?",
            "Well, for low thresholds or no conclusive results, and the reason is mainly that low thresholds in this case means you're very flexible string matching.",
            "But for thresholds, open six and oh point 8 we find that using this PRA almost gives you higher or equal precision than the baseline system.",
            "The first approach also gives you a higher or equal recall and that's entrance.",
            "Interesting because you actually reduce the search space.",
            "And then the second algorithm where you fix the ontologies actually gives you equal or lower recall.",
            "And that's a bit surprising.",
            "The main reason is.",
            "Well, I admit it, we did something stupid.",
            "Now we knew that we were going to."
        ],
        [
            "Do something stupid and this is this is the.",
            "Um?",
            "In one of these test cases where they are in the test cases where they recall actually is lower, we used mesh.",
            "We know that the structural relation in mesh actually covers both is and part of.",
            "And of course, our algorithms treated structure relation as ISM.",
            "This means that you get the strange situation that if you map nose with nose than enmesh, you will have links between the parts of nose and nose itself, and then of course you're going to fix the other ontology by introducing is relations between nose and its parts.",
            "It was stupid, but it shows that you should be careful when you use these things."
        ],
        [
            "So then we were thinking, how can we use Peery PRA and metrics?"
        ],
        [
            "And we had an observation, especially from the anatomy case that many of the PRA mappings have some kind of similar linguistic pattern.",
            "So for instance, in this example you have 4 words and that you have the 1st letter of the other word with the number and this becomes this.",
            "This is the same here and at these examples you actually added this or move to space, and in this example you actually change the order of the words.",
            "So what we then thought was let's run these mappings through different linguistic measures.",
            "We used here at the distance N gram and linguistic matter that looked at word separately, and this gives you a similarity, linguistic similarity vector and then the idea is that if you take these mappings which share this similar pattern, you're actually going to be similar in their linguistic similarity factors."
        ],
        [
            "So this was used in a mature.",
            "And the intuition is that mapping suggestions which share a similar linguistic similarity vector to.",
            "Mappings in the PRA.",
            "There are more likely to be correct suggestions, so essentially what we did was we computed the specters for every PRA mapping and then for all the suggestions that we have for each of the PRA mappings that were linguistically close to them, we augmented the similarity value."
        ],
        [
            "So for the small data sets, this didn't give anything and the reason was that the correct suggestion mappings already had high similarity value, so we didn't help raising it an for the ones that we missed or Sambo missed, then there was actually no linguistic pattern there.",
            "However, for the anatomy data set, what we found was that you got lower or equal precision, but recall increased for high threshold and decreased for load thresholds.",
            "So essentially what it meant was that you actually do find some new correct mappings.",
            "But for low thresholds you also find some new wrong mappings.",
            "It means that you again your similarity value is string matching based and you allow very flex."
        ],
        [
            "String matching.",
            "So how can we use the P?"
        ],
        [
            "Ray in the filter step.",
            "The first approach is very, very simple.",
            "You have correct mappings, so you should add them to the final result.",
            "And Moreover, if you have things which are inconsistent with these mappings, throw that away.",
            "The second approach is using double threshold filtering, but now with PRA.",
            "So you use a consistent group within the partial reference alignment.",
            "Instead of using the mapping suggestions which are above the higher threshold."
        ],
        [
            "And then we also have the linguistic similarity vector based ID where what we did was we clustered all the suggestions according to this linguistic similarity vector.",
            "Then we assign each PRA mapping to its.",
            "Right?"
        ],
        [
            "Cluster.",
            "Then we calculated the center actually and then we only kept the suggestions which were close to this center."
        ],
        [
            "So what do we know about this?",
            "Well, the first approach always gives you higher precision and recall, so this is what you always have to do.",
            "The.",
            "Linguistic approach gives you always equal or higher precision.",
            "Is kind of interesting.",
            "But it always gave you also equal or lower recalled ensemble, and that's also quite natural, because some of the correct suggestions will be filtered out because they don't share any linguistic pattern with PRA MIM."
        ],
        [
            "Then for the double threshold filtering approach using the PRA, you always get higher or equal recall than not using the PRA.",
            "For lower threshold open six, you always got higher precision also and threshold open for you almost always got higher precision.",
            "There were two datasets for which this was not the case, and then we analyze that we saw that the reason is probably because the consistent group that we had for the PRA was very small, while the consistent group when you had all the suggestions above the threshold was much bigger."
        ],
        [
            "We also looked briefly at the influence of the size of the PRA."
        ],
        [
            "So what we did here was we took the anatomy data set.",
            "We took the PRA that was given and then we divided this word in half.",
            "What we saw was that for the larger PRA, to recall always gets higher.",
            "But one big reason is of course that you start with a lot more correct mappings.",
            "When you do preprocessing and linguistic based mature for low thresholds, precision good, lower for high threshold precision got higher and then for the prefiltering strategies the precision always goes equal or higher."
        ],
        [
            "So what were the lessons learned from this study?",
            "Well, first of all, using the PRA and preprocessing means that you actually reduce the search space, so you get fewer suggestions, which in most cases gives you an improvement in precision and in most cases in some cases even an improvement in recall.",
            "The linguistic pattern matching I would use it somehow only to find new suggestions, maybe as a iteration step on its own.",
            "For the filtering approaches, always use this simple approach where you implants the results.",
            "And then the other filter approaches work well when you can actually trust the structure of their ontologies.",
            "So that is related to this structure, relation really is is a relation and not combination of isn't part of for instance.",
            "I also should note that when you look at the results in the paper, you will find that there is actually not such a big difference between the results from this PRA based algorithms and the baseline system.",
            "Someone somebody TF.",
            "The reasons are probably that someone somewhere that you have already do well on the test cases.",
            "For instance, they were the two best performing systems in anatomy track last year.",
            "So any improvement is actually valuable.",
            "Also, the way that the organizers have developed the partial reference alignment for the anatomy track means that every new correct mapping you find is actually nontrivial, which means it cannot be found by a reasonably simple string matcher."
        ],
        [
            "So what are some ideas for future work while improving current strategies?",
            "Just another ontologies.",
            "For instance, we may want to look at different kinds of patterns, not just linguistic patterns.",
            "Investigate combinations and interactions of the strategies, specially because several of the strategies in different phases are actually based on the same ID.",
            "So maybe if you do the pattern.",
            "We will make the pattern matcher.",
            "You don't need to do the pattern filtering.",
            "And then the main thing that for future work is that we want to develop an iterative ontology alignment framework where at the use of.",
            "Partial reference alignment is going to be an important component.",
            "Thank you.",
            "Any question?",
            "Yep.",
            "Thank you very much for this interesting talk.",
            "It was quite interesting to see in which stages this partial reference alignment could be used, but I missed one thing or one.",
            "One more thing would have been interesting to see how it could be used to increase the performance in with respect to the speed of the system, because especially in this preprocessing step there might be some some room to use to make use of this too, because you don't have to look at all pairs we have any.",
            "Experiments done with this.",
            "No, we didn't do anymore experience and actually this is the only step word can reduce the performance, right?",
            "Because in the other, once you actually just add processing, on the other hand, the processing that was added was quite little compared to the performance of the baseline systems.",
            "Thank you.",
            "Other questions.",
            "I have a question about the way you extract actually the alignments.",
            "OK basically you've got similarity measures, so you would have something like a metrics of similarity.",
            "Similarity between every pair of stuff and so when you do, for instance the preprocessing, do you do it on specially extracted or on the wall metrics?",
            "Do you apply some stable marriage algorithm or in order so we do it on the whole matrix?",
            "I'm.",
            "So I was wondering about this.",
            "There's this notion of of mappable submods, right?",
            "So it seems to me like some very weak form of trying to include reasoning in this in the things that you do, expect that.",
            "If you would use more formal reasoning like in the previous presentation that this mapping would help more that.",
            "Just this partial reference mapping would help more than your case.",
            "It could be the question is where it will not be too strong for the real cases.",
            "For instance, one of the other systems in the ontology Alignment inflation initiative last year actually used this for finding mappings, and as there are quite some links missing, this actually turned out to be too strong for them.",
            "So it may be, but in real cases when the ontologies are not so well defined and nicely looking, it may be too strong.",
            "Other questions.",
            "Thank you for the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "My name is Patrick Lambrix and this is work together with my student Yucheng.",
                    "label": 0
                },
                {
                    "sent": "Before I started.",
                    "label": 0
                },
                {
                    "sent": "Also want to thank Christian Malika for running or algorithms on one of the datasets.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about using partial reference alignments to align ontologies.",
                    "label": 1
                },
                {
                    "sent": "Now for this crowds, and considering that you have already heard to talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About ontology alignment.",
                    "label": 0
                },
                {
                    "sent": "Where the first part of introduction is going to be very fast.",
                    "label": 0
                },
                {
                    "sent": "We know that there are many ontologies, many of them have overlapping information.",
                    "label": 1
                },
                {
                    "sent": "You want to use them together.",
                    "label": 1
                },
                {
                    "sent": "Well, then you need to know the entology ratio.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ships so given to ontology.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Determining their correspondences between the terms and the different ontologies we call ontology alignments.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "Now, many of today's systems actually follow this kind of framework as input.",
                    "label": 0
                },
                {
                    "sent": "You have a number of ontologies.",
                    "label": 0
                },
                {
                    "sent": "These can be pre processed and then given to matures.",
                    "label": 0
                },
                {
                    "sent": "Much as usually compute similarity values between terms in different domains between terms in the different ontologies.",
                    "label": 0
                },
                {
                    "sent": "And then they can use external knowledge like instances, general dictionary or domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "The results of these measures can be combined in filtered in different ways so that we get mapping suggestions.",
                    "label": 0
                },
                {
                    "sent": "Then in phase two, usually a user can look at these mapping suggestions and accept or reject them.",
                    "label": 0
                },
                {
                    "sent": "An acceptance and rejection of.",
                    "label": 0
                },
                {
                    "sent": "Suggestions can influence new iteration in this process.",
                    "label": 0
                },
                {
                    "sent": "The end result is an alignment or a set of mappings.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now recently a new situation has occurred.",
                    "label": 0
                },
                {
                    "sent": "First of all, a number of portals have appeared where mappings are actually stored mappings between ontologies.",
                    "label": 0
                },
                {
                    "sent": "Then as I just told you, several systems actually allow the acceptance and rejection of suggestions to influence the next step in iteration.",
                    "label": 0
                },
                {
                    "sent": "And then last year in the ontology Alignment Evaluation Initiative, the anatomy track introduced a new task task for where the idea was that you already got some of the correct mappings, and could you then?",
                    "label": 0
                },
                {
                    "sent": "Give better results or give higher quality results for the ontology alignment based on this.",
                    "label": 0
                },
                {
                    "sent": "So common in all these cases that you already have some of the correct mappings and they are given or they have been obtained in some way and a subset of all the correct mappings.",
                    "label": 1
                },
                {
                    "sent": "We called in a partial reference.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our research problem that we looked at is can we use these partial reference alignments to obtain higher quality suggestions in the ontology alignments?",
                    "label": 0
                },
                {
                    "sent": "And we are.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Refined this question to can we look at the different components in our ontology framework and use the partial reference alignments in these components?",
                    "label": 0
                },
                {
                    "sent": "And we have basically looked at the preprocessing step of matching step in the filter step.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the remainder of my talk I will first start with some background tell you about the baseline systems that we've used in our test cases.",
                    "label": 0
                },
                {
                    "sent": "Then show you the test cases, then it will go onto.",
                    "label": 1
                },
                {
                    "sent": "Actually talking about algorithms and saying whether they're any good.",
                    "label": 1
                },
                {
                    "sent": "And then we'll have some conclusion answer.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So let us.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with the baseline systems.",
                    "label": 0
                },
                {
                    "sent": "So the first systems called some bullets system for aligning and merging biomedical ontologies.",
                    "label": 1
                },
                {
                    "sent": "By the way, Sam was also a Swedish word.",
                    "label": 0
                },
                {
                    "sent": "It means the person you live together with, so hopefully it's somebody you like.",
                    "label": 0
                },
                {
                    "sent": "And that's of course the feeling that we want you to have about our system.",
                    "label": 0
                },
                {
                    "sent": "To my students, on the other hand, I say, is the person you spend a lot of time with, so.",
                    "label": 0
                },
                {
                    "sent": "This is hopefully what you do with this system as well.",
                    "label": 0
                },
                {
                    "sent": "So somebody implements a number of measures.",
                    "label": 0
                },
                {
                    "sent": "We have some string based measures, string bass patches using word net, some matches using domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "Instance based measures and also structure based matches and results of these matches can be combined using a way to some combination and somebody uses single threshold filtering.",
                    "label": 0
                },
                {
                    "sent": "That means that pairs of concepts or dissimilarity value higher or equal to the threshold are given to the user as mapping suggestions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then in the second phase, the user can look at these.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of a small test case where.",
                    "label": 0
                },
                {
                    "sent": "Item.",
                    "label": 0
                },
                {
                    "sent": "Suggest that nasal cavity epithelium is the same as nasal mucosa and the string based measure taking into account synonym should have found this one.",
                    "label": 0
                },
                {
                    "sent": "So the user can then say whether these concepts are equivalent or the one is a sub counts of the other.",
                    "label": 0
                },
                {
                    "sent": "Vice versa can also reject them.",
                    "label": 0
                },
                {
                    "sent": "We also see that there is a manual alignment mode, so which can be interleaved with this suggestion mode.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then somebody DF is similar to summer, but it uses a more advanced threshold filtering approach and.",
                    "label": 0
                },
                {
                    "sent": "We introduced this to deal with the following observation.",
                    "label": 0
                },
                {
                    "sent": "When you use a single threshold, usually the higher the threshold you have.",
                    "label": 1
                },
                {
                    "sent": "The more of your suggestions will be correct, but at the same time you don't find as many correct suggestions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we proposed was to use two thresholds.",
                    "label": 1
                },
                {
                    "sent": "First we have the upper thresholds and pairs of concepts with similarity values above this higher threshold they will be given to the user's suggestions.",
                    "label": 0
                },
                {
                    "sent": "Pairs of concepts with similarity value lower than the lower threshold.",
                    "label": 1
                },
                {
                    "sent": "They will be rejected, but in between we will look at the.",
                    "label": 1
                },
                {
                    "sent": "At the pairs of concepts we will check what are reasonable prospectus structure of the ontologies and mapping suggestions from step one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "So assume we have two ontologies.",
                    "label": 0
                },
                {
                    "sent": "We have already calculated their similarity values.",
                    "label": 1
                },
                {
                    "sent": "We have an upper threshold and a lower threshold, so these guys are going to be retained as suggestions and the ones below the lower threshold are going to be discarded.",
                    "label": 0
                },
                {
                    "sent": "But these were going to look at by.",
                    "label": 0
                },
                {
                    "sent": "We're going to filter them based on the structure of the ontology.",
                    "label": 0
                },
                {
                    "sent": "So what is that idea?",
                    "label": 0
                },
                {
                    "sent": "Well, if you know that you have a mapping suggestion, a prime and this is an equivalence mapping and you have a suggestion.",
                    "label": 0
                },
                {
                    "sent": "Our mapping BB prime.",
                    "label": 0
                },
                {
                    "sent": "This is also equivalent mapping.",
                    "label": 0
                },
                {
                    "sent": "Then if you know that a isabi, then in the other entology want a prime to be a B prime.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we have this as a mapping to be.",
                    "label": 0
                },
                {
                    "sent": "Then this is an equivalence.",
                    "label": 0
                },
                {
                    "sent": "Then you weren't only sub concepts of two to be mapped to sub concepts of B.",
                    "label": 0
                },
                {
                    "sent": "So what we do is actually for a suggestion to be we divide on the two ontologies into subclasses or sub councils of two and rest and the other ontology sub counts observed B interest.",
                    "label": 0
                },
                {
                    "sent": "When you do this for all of the mapping suggestions above, the upper threshold, you actually partition the two ontologies.",
                    "label": 1
                },
                {
                    "sent": "What we do then is we only retain suggestions for which the concepts belong to corresponding map of groups.",
                    "label": 0
                },
                {
                    "sent": "So for instance 5 and E will be retained because both are in the Red Group, which actually are the sub counts of this two and B and 5C will not be retained.",
                    "label": 0
                },
                {
                    "sent": "Becausw 5 is in the Red Group and C as in the Blue Group.",
                    "label": 0
                },
                {
                    "sent": "Now this only works if the suggestions above the upper threshold actually satisfy this property.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this example, where 5C is also above the upper threshold, we see that five is a 2, but she is not to be.",
                    "label": 0
                },
                {
                    "sent": "So what we do in this case is we try to find a subset of the suggestions above the upper threshold which followed this property.",
                    "label": 1
                },
                {
                    "sent": "And then based on this consistent group as we call it, we partitioned ontology.",
                    "label": 0
                },
                {
                    "sent": "So what do we do then?",
                    "label": 0
                },
                {
                    "sent": "Well, we only allow.",
                    "label": 0
                },
                {
                    "sent": "And we do the same for four as before, so we only allow suggestions where the.",
                    "label": 0
                },
                {
                    "sent": "The concepts belong to corresponding map.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Groups so the actual baseline systems that we used in our sample and somebody TF as they are for the ontology Alignment Evaluation Initiative last year, which means that we actually removed phase two as there are no users.",
                    "label": 0
                },
                {
                    "sent": "An as there are no users, we also changed our algorithms a little bit so that.",
                    "label": 0
                },
                {
                    "sent": "A term only appears in one mapping suggestion.",
                    "label": 1
                },
                {
                    "sent": "In the full sample system, a term can actually appear in several suggestions and user can make a choice there.",
                    "label": 0
                },
                {
                    "sent": "The metrics that we use are termed turn WN, which is essentially a combination of several string matching algorithms and that uses word NET as a dictionary.",
                    "label": 0
                },
                {
                    "sent": "And then we have another measure which uses unified Medical language system as a domain knowledge dictionary.",
                    "label": 0
                },
                {
                    "sent": "The combination that we use in this system is maximum based strategy and the filters are then single and double threshold filter.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The test cases that we use are five smaller cases that we used in previous work.",
                    "label": 0
                },
                {
                    "sent": "Behavior and defense are parts of genealogy and signal ontology, nose, ear and eye or parts of the adult mouse anatomy and medical subject headings.",
                    "label": 1
                },
                {
                    "sent": "And then the larger test case that we use is actually the anatomy data set from the ontology Alignment Evaluation Initiative.",
                    "label": 0
                },
                {
                    "sent": "Here you see the number of concepts in the first ontology, number of concepts in the second one.",
                    "label": 0
                },
                {
                    "sent": "The number of correct mappings and these are the P arrays that we use, so the subset of the correct mappings.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated the OR algorithms using precision, so it tells you of the suggestions that you felt.",
                    "label": 0
                },
                {
                    "sent": "How many were correct.",
                    "label": 0
                },
                {
                    "sent": "Recall how many of the correct mappings do you actually find an F measure is a mean of precision recall?",
                    "label": 1
                },
                {
                    "sent": "We also looked at recall PRA, which is actually a call with respect to the non given part of the reference alignment.",
                    "label": 0
                },
                {
                    "sent": "They're interested to talk about, well, actually not mentioned this one, 'cause results range from really zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So let us look.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a different algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we see that we have someone, somebody, DF which do not have any preprocessing.",
                    "label": 0
                },
                {
                    "sent": "They have these two measures, maximum combination algorithm and single and double threshold filter.",
                    "label": 0
                },
                {
                    "sent": "We do not change the combination algorithm, but we have two algorithms which particularly look at preprocessing, one at matching and three at filtering.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us look how we can use the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arrays in the preprocessing step.",
                    "label": 1
                },
                {
                    "sent": "So the intuition is now we have these correct mappings.",
                    "label": 0
                },
                {
                    "sent": "We have the structure of the ontology.",
                    "label": 1
                },
                {
                    "sent": "Let's use this structural to partition the ontology in at the.",
                    "label": 0
                },
                {
                    "sent": "Let's use the correct mappings to partition ontology into map of the groups.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first method is.",
                    "label": 0
                },
                {
                    "sent": "The idea is very similar to the partitioning step in the double threshold filtering.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "We have given a PRA.",
                    "label": 0
                },
                {
                    "sent": "We find that consistent group in the PRA.",
                    "label": 1
                },
                {
                    "sent": "And then we partition the ontologies into mapping mapping.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance this.",
                    "label": 0
                },
                {
                    "sent": "And what do we do then?",
                    "label": 0
                },
                {
                    "sent": "Well then, we essentially reduce the search space for the for the matching algorithms, so we'll only allow.",
                    "label": 0
                },
                {
                    "sent": "Pairs of concepts that belong to corresponding map of the groups.",
                    "label": 0
                },
                {
                    "sent": "So you don't look at all the pairs anymore.",
                    "label": 0
                },
                {
                    "sent": "Now what we also note.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It was that in many cases the structure of the original ontology is actually not complete.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "If we see here where we have these two ontologies and we have the correct mappings 6D, these are equivalent and nine G or equivalent.",
                    "label": 0
                },
                {
                    "sent": "So we also know that G is a deontology too, so ascent actually 9 should also be a 610.",
                    "label": 0
                },
                {
                    "sent": "Now this is the case that appears in real situations, for instance in the data set for anatomy track we found that are about 90 missing is a relations based on this reasoning.",
                    "label": 1
                },
                {
                    "sent": "So what we do here is, well, first, we fix the ontology.",
                    "label": 1
                },
                {
                    "sent": "We add.",
                    "label": 0
                },
                {
                    "sent": "These missing is relations.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we use the previous approach.",
                    "label": 0
                },
                {
                    "sent": "So based on the fixed ontologies we partition into my public groups.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how good are these algorithms?",
                    "label": 0
                },
                {
                    "sent": "Well, for low thresholds or no conclusive results, and the reason is mainly that low thresholds in this case means you're very flexible string matching.",
                    "label": 1
                },
                {
                    "sent": "But for thresholds, open six and oh point 8 we find that using this PRA almost gives you higher or equal precision than the baseline system.",
                    "label": 0
                },
                {
                    "sent": "The first approach also gives you a higher or equal recall and that's entrance.",
                    "label": 0
                },
                {
                    "sent": "Interesting because you actually reduce the search space.",
                    "label": 0
                },
                {
                    "sent": "And then the second algorithm where you fix the ontologies actually gives you equal or lower recall.",
                    "label": 1
                },
                {
                    "sent": "And that's a bit surprising.",
                    "label": 0
                },
                {
                    "sent": "The main reason is.",
                    "label": 0
                },
                {
                    "sent": "Well, I admit it, we did something stupid.",
                    "label": 0
                },
                {
                    "sent": "Now we knew that we were going to.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do something stupid and this is this is the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In one of these test cases where they are in the test cases where they recall actually is lower, we used mesh.",
                    "label": 0
                },
                {
                    "sent": "We know that the structural relation in mesh actually covers both is and part of.",
                    "label": 1
                },
                {
                    "sent": "And of course, our algorithms treated structure relation as ISM.",
                    "label": 0
                },
                {
                    "sent": "This means that you get the strange situation that if you map nose with nose than enmesh, you will have links between the parts of nose and nose itself, and then of course you're going to fix the other ontology by introducing is relations between nose and its parts.",
                    "label": 1
                },
                {
                    "sent": "It was stupid, but it shows that you should be careful when you use these things.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we were thinking, how can we use Peery PRA and metrics?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we had an observation, especially from the anatomy case that many of the PRA mappings have some kind of similar linguistic pattern.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in this example you have 4 words and that you have the 1st letter of the other word with the number and this becomes this.",
                    "label": 0
                },
                {
                    "sent": "This is the same here and at these examples you actually added this or move to space, and in this example you actually change the order of the words.",
                    "label": 0
                },
                {
                    "sent": "So what we then thought was let's run these mappings through different linguistic measures.",
                    "label": 0
                },
                {
                    "sent": "We used here at the distance N gram and linguistic matter that looked at word separately, and this gives you a similarity, linguistic similarity vector and then the idea is that if you take these mappings which share this similar pattern, you're actually going to be similar in their linguistic similarity factors.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was used in a mature.",
                    "label": 0
                },
                {
                    "sent": "And the intuition is that mapping suggestions which share a similar linguistic similarity vector to.",
                    "label": 0
                },
                {
                    "sent": "Mappings in the PRA.",
                    "label": 0
                },
                {
                    "sent": "There are more likely to be correct suggestions, so essentially what we did was we computed the specters for every PRA mapping and then for all the suggestions that we have for each of the PRA mappings that were linguistically close to them, we augmented the similarity value.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the small data sets, this didn't give anything and the reason was that the correct suggestion mappings already had high similarity value, so we didn't help raising it an for the ones that we missed or Sambo missed, then there was actually no linguistic pattern there.",
                    "label": 1
                },
                {
                    "sent": "However, for the anatomy data set, what we found was that you got lower or equal precision, but recall increased for high threshold and decreased for load thresholds.",
                    "label": 1
                },
                {
                    "sent": "So essentially what it meant was that you actually do find some new correct mappings.",
                    "label": 0
                },
                {
                    "sent": "But for low thresholds you also find some new wrong mappings.",
                    "label": 0
                },
                {
                    "sent": "It means that you again your similarity value is string matching based and you allow very flex.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "String matching.",
                    "label": 0
                },
                {
                    "sent": "So how can we use the P?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ray in the filter step.",
                    "label": 1
                },
                {
                    "sent": "The first approach is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "You have correct mappings, so you should add them to the final result.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, if you have things which are inconsistent with these mappings, throw that away.",
                    "label": 0
                },
                {
                    "sent": "The second approach is using double threshold filtering, but now with PRA.",
                    "label": 0
                },
                {
                    "sent": "So you use a consistent group within the partial reference alignment.",
                    "label": 0
                },
                {
                    "sent": "Instead of using the mapping suggestions which are above the higher threshold.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we also have the linguistic similarity vector based ID where what we did was we clustered all the suggestions according to this linguistic similarity vector.",
                    "label": 0
                },
                {
                    "sent": "Then we assign each PRA mapping to its.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cluster.",
                    "label": 0
                },
                {
                    "sent": "Then we calculated the center actually and then we only kept the suggestions which were close to this center.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we know about this?",
                    "label": 0
                },
                {
                    "sent": "Well, the first approach always gives you higher precision and recall, so this is what you always have to do.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Linguistic approach gives you always equal or higher precision.",
                    "label": 1
                },
                {
                    "sent": "Is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "But it always gave you also equal or lower recalled ensemble, and that's also quite natural, because some of the correct suggestions will be filtered out because they don't share any linguistic pattern with PRA MIM.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then for the double threshold filtering approach using the PRA, you always get higher or equal recall than not using the PRA.",
                    "label": 0
                },
                {
                    "sent": "For lower threshold open six, you always got higher precision also and threshold open for you almost always got higher precision.",
                    "label": 1
                },
                {
                    "sent": "There were two datasets for which this was not the case, and then we analyze that we saw that the reason is probably because the consistent group that we had for the PRA was very small, while the consistent group when you had all the suggestions above the threshold was much bigger.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also looked briefly at the influence of the size of the PRA.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did here was we took the anatomy data set.",
                    "label": 0
                },
                {
                    "sent": "We took the PRA that was given and then we divided this word in half.",
                    "label": 0
                },
                {
                    "sent": "What we saw was that for the larger PRA, to recall always gets higher.",
                    "label": 1
                },
                {
                    "sent": "But one big reason is of course that you start with a lot more correct mappings.",
                    "label": 0
                },
                {
                    "sent": "When you do preprocessing and linguistic based mature for low thresholds, precision good, lower for high threshold precision got higher and then for the prefiltering strategies the precision always goes equal or higher.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what were the lessons learned from this study?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, using the PRA and preprocessing means that you actually reduce the search space, so you get fewer suggestions, which in most cases gives you an improvement in precision and in most cases in some cases even an improvement in recall.",
                    "label": 1
                },
                {
                    "sent": "The linguistic pattern matching I would use it somehow only to find new suggestions, maybe as a iteration step on its own.",
                    "label": 0
                },
                {
                    "sent": "For the filtering approaches, always use this simple approach where you implants the results.",
                    "label": 1
                },
                {
                    "sent": "And then the other filter approaches work well when you can actually trust the structure of their ontologies.",
                    "label": 0
                },
                {
                    "sent": "So that is related to this structure, relation really is is a relation and not combination of isn't part of for instance.",
                    "label": 1
                },
                {
                    "sent": "I also should note that when you look at the results in the paper, you will find that there is actually not such a big difference between the results from this PRA based algorithms and the baseline system.",
                    "label": 0
                },
                {
                    "sent": "Someone somebody TF.",
                    "label": 0
                },
                {
                    "sent": "The reasons are probably that someone somewhere that you have already do well on the test cases.",
                    "label": 0
                },
                {
                    "sent": "For instance, they were the two best performing systems in anatomy track last year.",
                    "label": 0
                },
                {
                    "sent": "So any improvement is actually valuable.",
                    "label": 0
                },
                {
                    "sent": "Also, the way that the organizers have developed the partial reference alignment for the anatomy track means that every new correct mapping you find is actually nontrivial, which means it cannot be found by a reasonably simple string matcher.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are some ideas for future work while improving current strategies?",
                    "label": 0
                },
                {
                    "sent": "Just another ontologies.",
                    "label": 0
                },
                {
                    "sent": "For instance, we may want to look at different kinds of patterns, not just linguistic patterns.",
                    "label": 0
                },
                {
                    "sent": "Investigate combinations and interactions of the strategies, specially because several of the strategies in different phases are actually based on the same ID.",
                    "label": 1
                },
                {
                    "sent": "So maybe if you do the pattern.",
                    "label": 0
                },
                {
                    "sent": "We will make the pattern matcher.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do the pattern filtering.",
                    "label": 0
                },
                {
                    "sent": "And then the main thing that for future work is that we want to develop an iterative ontology alignment framework where at the use of.",
                    "label": 1
                },
                {
                    "sent": "Partial reference alignment is going to be an important component.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Any question?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for this interesting talk.",
                    "label": 0
                },
                {
                    "sent": "It was quite interesting to see in which stages this partial reference alignment could be used, but I missed one thing or one.",
                    "label": 0
                },
                {
                    "sent": "One more thing would have been interesting to see how it could be used to increase the performance in with respect to the speed of the system, because especially in this preprocessing step there might be some some room to use to make use of this too, because you don't have to look at all pairs we have any.",
                    "label": 0
                },
                {
                    "sent": "Experiments done with this.",
                    "label": 0
                },
                {
                    "sent": "No, we didn't do anymore experience and actually this is the only step word can reduce the performance, right?",
                    "label": 0
                },
                {
                    "sent": "Because in the other, once you actually just add processing, on the other hand, the processing that was added was quite little compared to the performance of the baseline systems.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "I have a question about the way you extract actually the alignments.",
                    "label": 0
                },
                {
                    "sent": "OK basically you've got similarity measures, so you would have something like a metrics of similarity.",
                    "label": 0
                },
                {
                    "sent": "Similarity between every pair of stuff and so when you do, for instance the preprocessing, do you do it on specially extracted or on the wall metrics?",
                    "label": 0
                },
                {
                    "sent": "Do you apply some stable marriage algorithm or in order so we do it on the whole matrix?",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering about this.",
                    "label": 0
                },
                {
                    "sent": "There's this notion of of mappable submods, right?",
                    "label": 0
                },
                {
                    "sent": "So it seems to me like some very weak form of trying to include reasoning in this in the things that you do, expect that.",
                    "label": 0
                },
                {
                    "sent": "If you would use more formal reasoning like in the previous presentation that this mapping would help more that.",
                    "label": 0
                },
                {
                    "sent": "Just this partial reference mapping would help more than your case.",
                    "label": 0
                },
                {
                    "sent": "It could be the question is where it will not be too strong for the real cases.",
                    "label": 0
                },
                {
                    "sent": "For instance, one of the other systems in the ontology Alignment inflation initiative last year actually used this for finding mappings, and as there are quite some links missing, this actually turned out to be too strong for them.",
                    "label": 0
                },
                {
                    "sent": "So it may be, but in real cases when the ontologies are not so well defined and nicely looking, it may be too strong.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the speaker.",
                    "label": 0
                }
            ]
        }
    }
}