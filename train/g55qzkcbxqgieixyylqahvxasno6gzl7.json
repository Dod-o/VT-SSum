{
    "id": "g55qzkcbxqgieixyylqahvxasno6gzl7",
    "title": "A Close Look to Margin Complexity and Related Parameters",
    "info": {
        "author": [
            "Michael Kallweit, Ruhr University Bochum"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_kallweit_look/",
    "segmentation": [
        [
            "OK, for the next 20 minutes I will give you a close look into margin complexity and related parameters and their relation to each other.",
            "This is a joint work with Huntsman.",
            "OK."
        ],
        [
            "So first of all, I want to remind you of the setting of margin optimization problems and give you an overview of our results."
        ],
        [
            "So assume we're in a binary classification task, and as usual we can denote the true value with minus one in the 4th way was plus one.",
            "So every concept class can be represented by & matrix, so every column is now one of these Boolean functions and the rows corresponds to the instances.",
            "And if he"
        ],
        [
            "So every linear separable concept classes admit linear arrangements which are just embedding into homogeneous half spaces.",
            "I know the concepts are represented by hyperplanes and our instances as vectors point in this space."
        ],
        [
            "And in the context of large margin classification, we're interested in lower and upper bounds for the margin, which are linear classifier can achieve.",
            "So the goal here is to achieve a margin as large as possible, because this leads to small generalization errors."
        ],
        [
            "So I want to go into detail what is the linear arrangement so we are given a real valued matrix of size M * N and the linear arrangement consists of vectors.",
            "You want to UN and we want to VM who's orchidia norm is bounded by one and they live in this, say D dimensional space.",
            "Now the UR the points and we vectors are the normal vectors of this hyperplanes and we have a linear arrangement if the matrix product.",
            "Of these vectors, seniors.",
            "Matrices reveal the same sign pattern is or matrix A.",
            "So we classify everything correct, so the points lie in the correct half of this.",
            "For correct side of this hyperplane."
        ],
        [
            "So for each combination of vector and.",
            "Hyperplane and Victor.",
            "We can look at the distance to each other.",
            "It's called just margin of despair and well known complexity measure of margin complexity, which is the biggest minimal.",
            "Margin.",
            "So the minimum overall individual margin para meters best linear arrangement can achieve, so it's just the inverse of it.",
            "And."
        ],
        [
            "One can think of many other margin para meters.",
            "So for example we can just add up all of the individual margins which give us the total margin.",
            "And as pointed out by linear, let other gamma two norm and it's dual gamma two stars closely related to margin complexity.",
            "So the gamma two star norm is exactly the best total margin linear classifier can achieve.",
            "And so now we have just in some of these individual margin.",
            "It's very false.",
            "It's very interesting to look at some sort of averaging process here, so we weight the individual margin."
        ],
        [
            "The following way.",
            "So now I define the Y average margin complexity for stochastic matrix.",
            "Why this is just the?",
            "Entries sum up to one, so it's just the.",
            "Best the best.",
            "Total sum of this individual model of the matrix Y circle A.",
            "This is just the Integrys multiplication, so we get here this.",
            "Factors?",
            "If so, just the entries.",
            "So the entry why?",
            "IIJ here.",
            "So we do some waiting on the entries of a an.",
            "For example, if Y is just the.",
            "Uniform distribution we get the."
        ],
        [
            "Unknown average margin complexity and another important example in my talk will be when why is rank one matrix build up from 2 probability vectors P&Q and I call this the rank one average margin complexity.",
            "So what did we do with all this stuff?"
        ],
        [
            "We proved some very nice man Maxi rims for this margin perimeters.",
            "So for example, the margin complexity we deal with minimal margin we can achieve is exactly the Why average margin for worst case we chose and why distribution?",
            "And a similar result is true for this rank.",
            "Some worst case rank, one average margin complexity."
        ],
        [
            "We did some work on the classical foster bound, two which, so the classic were false around give gives us, among other things, lower bound for the average margin complexity in terms of the spectral norm of the matrix.",
            "So just the biggest singular value.",
            "And we complemented with a new upper bound now in terms of the trace norm of a matrix.",
            "And we also give for this rank one average margin complexity.",
            "We give also a lower bound which looks similar to the classical bound here, so also in terms of the biggest singular value."
        ],
        [
            "We showed some connections to complexity measures in statistics query models, so we heard in this session many about the correlational testicle query model answer statistic we remodel and we can show that this.",
            "Yeah, worst case, why average margin complexity is polynomial related to the SSQ dimension and this worst case rank one average margin complexity is polynomial related to the SQ dimension."
        ],
        [
            "So now I want to show you one of our main maxi rooms in detail."
        ],
        [
            "So again, this margin complexity.",
            "I remind you this works with minimum margin is.",
            "Equals the worst case.",
            "Why average margin complexity?",
            "So this statement is true.",
            "That is equivalent to.",
            "The that the best minimal margin equals the minimum overall wise of the best Y average margin we can achieve."
        ],
        [
            "So the proof sketch reads very easy, and so it's a real proof.",
            "So at first we express both of this margin optimization problems.",
            "Standards immediately programs so similar.",
            "If you need programming is just a special case on convex optimization where we optimize over the cone of simple definite matrices and then as a second step we just compare, recompute the duals of these two programs.",
            "And exploit the nice duality theory which is there for SDP's.",
            "So what do we need to express?",
            "Those program's source optimization problem problems as SDP?"
        ],
        [
            "Please.",
            "Yeah, we need a fact which was also exploited or used by linear at all.",
            "Before that we can express semidefinite matrix X as a product of.",
            "A matrix W and its transpose.",
            "So by the Cholesky decomposition or.",
            "For example, and the columns of this W can be seen as the vectors of arrangement.",
            "So for we can.",
            "Express every condition on the linear arrangement into a semi definite program.",
            "For example, the condition here that the entries on the main diagonal of XR1 is just the statement that so kleidion Norma vectors of you and we are one.",
            "Now let's take a look at the."
        ],
        [
            "Dual fonts.",
            "So we can exploit.",
            "Like I said before, they do energy theory here.",
            "For example, we can fulfill some Slater constraint qualifications that is just we have some strictly feasible points in the primal and dual form.",
            "And so strong duality hold that is the optimal value of the primal and the dual equal.",
            "So as you already notice, this both programs look almost the same."
        ],
        [
            "So the dual variables here are the small wise.",
            "So here's the objective functions equal also the same definite constraint on this block matrix.",
            "But they also."
        ],
        [
            "Huge difference here.",
            "Difference here.",
            "This capital Y in the first program.",
            "So this capital Y consists just of these two indices.",
            "This module variables so the part of the optimization process in the program and in the second one this Y comes from outside, so it's not part of this optimization process here.",
            "So if we want that this is equal something with here we just have to.",
            "Put a minimum of all wise here in front of this, and this is exactly the statement I wanted to show you."
        ],
        [
            "So our work did reveal some more insights, so this later constraint qualification also gives us that there exists really maximal of minimizers for the prime lens dual respectively.",
            "And for now, just let us denote the optimal margin, so the best minimal margin we can achieve optimal value of this primal to dual.",
            "Is, star and now?",
            "Given an optimal arrangement where this can be achieved, we look at exactly the payouts where this minimal margin."
        ],
        [
            "Can be achieved.",
            "So this is K. Of a given this optimal arrangement, so we just pick the ones where the optimal margin is really realized.",
            "And now we built the intersection overall.",
            "Optimal arrangements so in this set of indices we just call for.",
            "Now the hard part of the Matrix and why I called the hard part."
        ],
        [
            "What will become clear now be cause every minimizer why the star of this maximum of this best?",
            "Why average margin is centered on this hard part there is.",
            "For every index pair which is not in this hard part, the corresponding entry of this Y matrix is 0.",
            "This leads to the following result.",
            "Nice result that if we want to compute this best minimal margin.",
            "So basically the margin complexity we can just focus on this hard part.",
            "So on this matrix.",
            "So we can ignore all the other things, so this this hard Parrot plays a similar role for our linear arrangement as support vectors in support vector optimization, and I think this is a very nice result.",
            "OK."
        ],
        [
            "Like I promised, I will give you a some connections to the."
        ],
        [
            "Korean models we heard before 1st of into the correlational testicle query model.",
            "So in the third last talk we heard this is equivalent to the.",
            "Solvability of Valeant's availability framework of radiant and the corresponding."
        ],
        [
            "Complexity measure here with the SSQ dimension.",
            "So what is it?",
            "We have a collection of vectors and they are universally correlated with some matrix.",
            "If we for every probability vector and every column of the matrix we choose, we find some vector in our collection.",
            "With this at least the correlation of 1 / D and the CCS Q dimension is just the minimal amount of.",
            "Vectors that form such a University correlate."
        ],
        [
            "So what did we prove here?",
            "We prove that the margin complexity is upper bounded by the SSQ dimension.",
            "To the power of 1.5 and the SSQ dimension itself is upper bounded by essentially the logarithm of the size of the matrix and its margin complexity squared.",
            "So I want to show you."
        ],
        [
            "Cool.",
            "A bit of this proof.",
            "First part of the lemma.",
            "Yeah, we just take some University correlated set so correlated with this matrix A and we want to find some relation to the margin complexity now by Armin Maxi ring this equals.",
            "Because the magic complexity is the same as the Why average margin complexity for worst case Y.",
            "So we can pick this why we know it exists and we can use this.",
            "Why?",
            "And this University correlated sets to build up a universal and linear arrangement, and which one can easily verify that the.",
            "The Why average margin complexity via rich margin of this fulfills our property we needed here."
        ],
        [
            "The other, the second part of the lemma, sold the other direction.",
            "Now we take an optimal arrangement.",
            "And via the application of the Johnson Lindenstrauss lemma, so a technique called Brandon projections.",
            "We transform this arrangement into another one, which is at most at least half as optimal as.",
            "The original concerning the margin and has now a specific dimension here, so the dimension of the linear, so the vectors of the linear arrangement.",
            "And once again, one can easily verify that.",
            "You can build the universal correlated vectors out of these vectors from this new arrangement.",
            "So just easy calculations."
        ],
        [
            "What our results for the statistical query model.",
            "So."
        ],
        [
            "So that in the statistical query model we heard before.",
            "Characterizing the weak learnability the week learning characterizing complexity measure icsa Askew dimension.",
            "So now here we are dependent on a some probability vector which gives us here and the inner product.",
            "So they have just the.",
            "Maximal number of columns which are almost orthogonal."
        ],
        [
            "And here our results.",
            "Are there some kind of worst case rank?",
            "One average margin complexity is upper bounded by.",
            "Essentially this Q dimension to the power of three halves and the SQ dimension itself is upper bounded by two times this.",
            "Worst case rank One average margin complexity.",
            "So this holds for every probability vector P. So we also can choose PN."
        ],
        [
            "Worst case fashion and get the following conclusion.",
            "Here is the my the polynomial relation between the worst case rank, one margin complexity in siskyou dimension and this allows us among other things too in."
        ],
        [
            "Proof of previous result from share stuff, which is that.",
            "Rescue dimension of the transpose of a matrix is upper bounded by the Q dimension to the power of 4."
        ],
        [
            "Now we can improve it slightly.",
            "Now we have now a polynomial of degree three, but also smaller constant here."
        ],
        [
            "So let me summarize the results I showed you today."
        ],
        [
            "We have some nice min Max theorem for matching complexity measures.",
            "We have support vectors for linear arrangements and we have some connections to test if who statistically re learning models and let me elaborate on this last part."
        ],
        [
            "But more.",
            "So in the dual setting we're always confronted with the problem of maximizing the total margin of a matrix Y circle, a soul winterized multiplication.",
            "So this nature of this why is very crucial.",
            "Anne."
        ],
        [
            "And.",
            "So for example.",
            "If Y is just this rank one matrix so.",
            "Build up from the product of two probability vectors and Q is under other controller of an adversary.",
            "So we maximize over Q, which means we choose the target function.",
            "In the worst case fashion this is.",
            "As we saw related to the weak learning under fixed distribution in the SQ model.",
            "If in addition this P is under control of adversary, so we also maximize over P. Which means we choose the domain distribution in the worst case fashion.",
            "This corresponds to weak learning under hardest but still fixed distribution in the SQ model.",
            "And if this why is arbitrary, so we don't know anything about it can be a full rank opposed to the case where here is rank one.",
            "The related complexity measure is the worst case Y average margin complexity, which is just the margin complexity.",
            "As I showed before, this corresponds to weak learning in the distribution independent SSQ model.",
            "There's still work to do.",
            "So.",
            "Our results for the SQL Learning model here.",
            "Out for just fix this trip."
        ],
        [
            "Lucian and one open problem and the natural question is, is there margin parimeter characterizing distribution independent SQ learning and can this distribution independent as Q learning be separated from SQ learning with respect to the hardest fixed distribution?",
            "Another interesting question is what can maybe we can find para meters for other soft margin optimization problem so hard margin where this minimal margin and we softer?",
            "Now by this why ever reaching?",
            "That's typical case where also, when you have this hinge loss, so we have some sort of clip margin as which poses slick variables in this support vector optimization problem.",
            "OK.",
            "I'm done here."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for the next 20 minutes I will give you a close look into margin complexity and related parameters and their relation to each other.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with Huntsman.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, I want to remind you of the setting of margin optimization problems and give you an overview of our results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So assume we're in a binary classification task, and as usual we can denote the true value with minus one in the 4th way was plus one.",
                    "label": 0
                },
                {
                    "sent": "So every concept class can be represented by & matrix, so every column is now one of these Boolean functions and the rows corresponds to the instances.",
                    "label": 0
                },
                {
                    "sent": "And if he",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So every linear separable concept classes admit linear arrangements which are just embedding into homogeneous half spaces.",
                    "label": 0
                },
                {
                    "sent": "I know the concepts are represented by hyperplanes and our instances as vectors point in this space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the context of large margin classification, we're interested in lower and upper bounds for the margin, which are linear classifier can achieve.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is to achieve a margin as large as possible, because this leads to small generalization errors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to go into detail what is the linear arrangement so we are given a real valued matrix of size M * N and the linear arrangement consists of vectors.",
                    "label": 0
                },
                {
                    "sent": "You want to UN and we want to VM who's orchidia norm is bounded by one and they live in this, say D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Now the UR the points and we vectors are the normal vectors of this hyperplanes and we have a linear arrangement if the matrix product.",
                    "label": 0
                },
                {
                    "sent": "Of these vectors, seniors.",
                    "label": 0
                },
                {
                    "sent": "Matrices reveal the same sign pattern is or matrix A.",
                    "label": 0
                },
                {
                    "sent": "So we classify everything correct, so the points lie in the correct half of this.",
                    "label": 0
                },
                {
                    "sent": "For correct side of this hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for each combination of vector and.",
                    "label": 0
                },
                {
                    "sent": "Hyperplane and Victor.",
                    "label": 0
                },
                {
                    "sent": "We can look at the distance to each other.",
                    "label": 0
                },
                {
                    "sent": "It's called just margin of despair and well known complexity measure of margin complexity, which is the biggest minimal.",
                    "label": 0
                },
                {
                    "sent": "Margin.",
                    "label": 0
                },
                {
                    "sent": "So the minimum overall individual margin para meters best linear arrangement can achieve, so it's just the inverse of it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One can think of many other margin para meters.",
                    "label": 0
                },
                {
                    "sent": "So for example we can just add up all of the individual margins which give us the total margin.",
                    "label": 0
                },
                {
                    "sent": "And as pointed out by linear, let other gamma two norm and it's dual gamma two stars closely related to margin complexity.",
                    "label": 1
                },
                {
                    "sent": "So the gamma two star norm is exactly the best total margin linear classifier can achieve.",
                    "label": 0
                },
                {
                    "sent": "And so now we have just in some of these individual margin.",
                    "label": 0
                },
                {
                    "sent": "It's very false.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting to look at some sort of averaging process here, so we weight the individual margin.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following way.",
                    "label": 0
                },
                {
                    "sent": "So now I define the Y average margin complexity for stochastic matrix.",
                    "label": 1
                },
                {
                    "sent": "Why this is just the?",
                    "label": 0
                },
                {
                    "sent": "Entries sum up to one, so it's just the.",
                    "label": 0
                },
                {
                    "sent": "Best the best.",
                    "label": 0
                },
                {
                    "sent": "Total sum of this individual model of the matrix Y circle A.",
                    "label": 0
                },
                {
                    "sent": "This is just the Integrys multiplication, so we get here this.",
                    "label": 0
                },
                {
                    "sent": "Factors?",
                    "label": 0
                },
                {
                    "sent": "If so, just the entries.",
                    "label": 0
                },
                {
                    "sent": "So the entry why?",
                    "label": 0
                },
                {
                    "sent": "IIJ here.",
                    "label": 0
                },
                {
                    "sent": "So we do some waiting on the entries of a an.",
                    "label": 0
                },
                {
                    "sent": "For example, if Y is just the.",
                    "label": 0
                },
                {
                    "sent": "Uniform distribution we get the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unknown average margin complexity and another important example in my talk will be when why is rank one matrix build up from 2 probability vectors P&Q and I call this the rank one average margin complexity.",
                    "label": 0
                },
                {
                    "sent": "So what did we do with all this stuff?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We proved some very nice man Maxi rims for this margin perimeters.",
                    "label": 0
                },
                {
                    "sent": "So for example, the margin complexity we deal with minimal margin we can achieve is exactly the Why average margin for worst case we chose and why distribution?",
                    "label": 0
                },
                {
                    "sent": "And a similar result is true for this rank.",
                    "label": 0
                },
                {
                    "sent": "Some worst case rank, one average margin complexity.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did some work on the classical foster bound, two which, so the classic were false around give gives us, among other things, lower bound for the average margin complexity in terms of the spectral norm of the matrix.",
                    "label": 1
                },
                {
                    "sent": "So just the biggest singular value.",
                    "label": 1
                },
                {
                    "sent": "And we complemented with a new upper bound now in terms of the trace norm of a matrix.",
                    "label": 0
                },
                {
                    "sent": "And we also give for this rank one average margin complexity.",
                    "label": 0
                },
                {
                    "sent": "We give also a lower bound which looks similar to the classical bound here, so also in terms of the biggest singular value.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We showed some connections to complexity measures in statistics query models, so we heard in this session many about the correlational testicle query model answer statistic we remodel and we can show that this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, worst case, why average margin complexity is polynomial related to the SSQ dimension and this worst case rank one average margin complexity is polynomial related to the SQ dimension.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I want to show you one of our main maxi rooms in detail.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, this margin complexity.",
                    "label": 0
                },
                {
                    "sent": "I remind you this works with minimum margin is.",
                    "label": 0
                },
                {
                    "sent": "Equals the worst case.",
                    "label": 0
                },
                {
                    "sent": "Why average margin complexity?",
                    "label": 0
                },
                {
                    "sent": "So this statement is true.",
                    "label": 0
                },
                {
                    "sent": "That is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "The that the best minimal margin equals the minimum overall wise of the best Y average margin we can achieve.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the proof sketch reads very easy, and so it's a real proof.",
                    "label": 0
                },
                {
                    "sent": "So at first we express both of this margin optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Standards immediately programs so similar.",
                    "label": 0
                },
                {
                    "sent": "If you need programming is just a special case on convex optimization where we optimize over the cone of simple definite matrices and then as a second step we just compare, recompute the duals of these two programs.",
                    "label": 0
                },
                {
                    "sent": "And exploit the nice duality theory which is there for SDP's.",
                    "label": 0
                },
                {
                    "sent": "So what do we need to express?",
                    "label": 0
                },
                {
                    "sent": "Those program's source optimization problem problems as SDP?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we need a fact which was also exploited or used by linear at all.",
                    "label": 0
                },
                {
                    "sent": "Before that we can express semidefinite matrix X as a product of.",
                    "label": 0
                },
                {
                    "sent": "A matrix W and its transpose.",
                    "label": 0
                },
                {
                    "sent": "So by the Cholesky decomposition or.",
                    "label": 0
                },
                {
                    "sent": "For example, and the columns of this W can be seen as the vectors of arrangement.",
                    "label": 0
                },
                {
                    "sent": "So for we can.",
                    "label": 0
                },
                {
                    "sent": "Express every condition on the linear arrangement into a semi definite program.",
                    "label": 0
                },
                {
                    "sent": "For example, the condition here that the entries on the main diagonal of XR1 is just the statement that so kleidion Norma vectors of you and we are one.",
                    "label": 0
                },
                {
                    "sent": "Now let's take a look at the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dual fonts.",
                    "label": 0
                },
                {
                    "sent": "So we can exploit.",
                    "label": 0
                },
                {
                    "sent": "Like I said before, they do energy theory here.",
                    "label": 0
                },
                {
                    "sent": "For example, we can fulfill some Slater constraint qualifications that is just we have some strictly feasible points in the primal and dual form.",
                    "label": 0
                },
                {
                    "sent": "And so strong duality hold that is the optimal value of the primal and the dual equal.",
                    "label": 0
                },
                {
                    "sent": "So as you already notice, this both programs look almost the same.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the dual variables here are the small wise.",
                    "label": 0
                },
                {
                    "sent": "So here's the objective functions equal also the same definite constraint on this block matrix.",
                    "label": 0
                },
                {
                    "sent": "But they also.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Huge difference here.",
                    "label": 0
                },
                {
                    "sent": "Difference here.",
                    "label": 0
                },
                {
                    "sent": "This capital Y in the first program.",
                    "label": 0
                },
                {
                    "sent": "So this capital Y consists just of these two indices.",
                    "label": 0
                },
                {
                    "sent": "This module variables so the part of the optimization process in the program and in the second one this Y comes from outside, so it's not part of this optimization process here.",
                    "label": 0
                },
                {
                    "sent": "So if we want that this is equal something with here we just have to.",
                    "label": 0
                },
                {
                    "sent": "Put a minimum of all wise here in front of this, and this is exactly the statement I wanted to show you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our work did reveal some more insights, so this later constraint qualification also gives us that there exists really maximal of minimizers for the prime lens dual respectively.",
                    "label": 0
                },
                {
                    "sent": "And for now, just let us denote the optimal margin, so the best minimal margin we can achieve optimal value of this primal to dual.",
                    "label": 0
                },
                {
                    "sent": "Is, star and now?",
                    "label": 0
                },
                {
                    "sent": "Given an optimal arrangement where this can be achieved, we look at exactly the payouts where this minimal margin.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be achieved.",
                    "label": 0
                },
                {
                    "sent": "So this is K. Of a given this optimal arrangement, so we just pick the ones where the optimal margin is really realized.",
                    "label": 0
                },
                {
                    "sent": "And now we built the intersection overall.",
                    "label": 0
                },
                {
                    "sent": "Optimal arrangements so in this set of indices we just call for.",
                    "label": 0
                },
                {
                    "sent": "Now the hard part of the Matrix and why I called the hard part.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What will become clear now be cause every minimizer why the star of this maximum of this best?",
                    "label": 1
                },
                {
                    "sent": "Why average margin is centered on this hard part there is.",
                    "label": 1
                },
                {
                    "sent": "For every index pair which is not in this hard part, the corresponding entry of this Y matrix is 0.",
                    "label": 0
                },
                {
                    "sent": "This leads to the following result.",
                    "label": 0
                },
                {
                    "sent": "Nice result that if we want to compute this best minimal margin.",
                    "label": 1
                },
                {
                    "sent": "So basically the margin complexity we can just focus on this hard part.",
                    "label": 0
                },
                {
                    "sent": "So on this matrix.",
                    "label": 0
                },
                {
                    "sent": "So we can ignore all the other things, so this this hard Parrot plays a similar role for our linear arrangement as support vectors in support vector optimization, and I think this is a very nice result.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I promised, I will give you a some connections to the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Korean models we heard before 1st of into the correlational testicle query model.",
                    "label": 0
                },
                {
                    "sent": "So in the third last talk we heard this is equivalent to the.",
                    "label": 0
                },
                {
                    "sent": "Solvability of Valeant's availability framework of radiant and the corresponding.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Complexity measure here with the SSQ dimension.",
                    "label": 0
                },
                {
                    "sent": "So what is it?",
                    "label": 0
                },
                {
                    "sent": "We have a collection of vectors and they are universally correlated with some matrix.",
                    "label": 1
                },
                {
                    "sent": "If we for every probability vector and every column of the matrix we choose, we find some vector in our collection.",
                    "label": 0
                },
                {
                    "sent": "With this at least the correlation of 1 / D and the CCS Q dimension is just the minimal amount of.",
                    "label": 1
                },
                {
                    "sent": "Vectors that form such a University correlate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what did we prove here?",
                    "label": 0
                },
                {
                    "sent": "We prove that the margin complexity is upper bounded by the SSQ dimension.",
                    "label": 0
                },
                {
                    "sent": "To the power of 1.5 and the SSQ dimension itself is upper bounded by essentially the logarithm of the size of the matrix and its margin complexity squared.",
                    "label": 0
                },
                {
                    "sent": "So I want to show you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool.",
                    "label": 0
                },
                {
                    "sent": "A bit of this proof.",
                    "label": 0
                },
                {
                    "sent": "First part of the lemma.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we just take some University correlated set so correlated with this matrix A and we want to find some relation to the margin complexity now by Armin Maxi ring this equals.",
                    "label": 0
                },
                {
                    "sent": "Because the magic complexity is the same as the Why average margin complexity for worst case Y.",
                    "label": 0
                },
                {
                    "sent": "So we can pick this why we know it exists and we can use this.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "And this University correlated sets to build up a universal and linear arrangement, and which one can easily verify that the.",
                    "label": 0
                },
                {
                    "sent": "The Why average margin complexity via rich margin of this fulfills our property we needed here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other, the second part of the lemma, sold the other direction.",
                    "label": 0
                },
                {
                    "sent": "Now we take an optimal arrangement.",
                    "label": 1
                },
                {
                    "sent": "And via the application of the Johnson Lindenstrauss lemma, so a technique called Brandon projections.",
                    "label": 0
                },
                {
                    "sent": "We transform this arrangement into another one, which is at most at least half as optimal as.",
                    "label": 0
                },
                {
                    "sent": "The original concerning the margin and has now a specific dimension here, so the dimension of the linear, so the vectors of the linear arrangement.",
                    "label": 0
                },
                {
                    "sent": "And once again, one can easily verify that.",
                    "label": 1
                },
                {
                    "sent": "You can build the universal correlated vectors out of these vectors from this new arrangement.",
                    "label": 0
                },
                {
                    "sent": "So just easy calculations.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What our results for the statistical query model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that in the statistical query model we heard before.",
                    "label": 1
                },
                {
                    "sent": "Characterizing the weak learnability the week learning characterizing complexity measure icsa Askew dimension.",
                    "label": 1
                },
                {
                    "sent": "So now here we are dependent on a some probability vector which gives us here and the inner product.",
                    "label": 0
                },
                {
                    "sent": "So they have just the.",
                    "label": 0
                },
                {
                    "sent": "Maximal number of columns which are almost orthogonal.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here our results.",
                    "label": 0
                },
                {
                    "sent": "Are there some kind of worst case rank?",
                    "label": 0
                },
                {
                    "sent": "One average margin complexity is upper bounded by.",
                    "label": 0
                },
                {
                    "sent": "Essentially this Q dimension to the power of three halves and the SQ dimension itself is upper bounded by two times this.",
                    "label": 0
                },
                {
                    "sent": "Worst case rank One average margin complexity.",
                    "label": 0
                },
                {
                    "sent": "So this holds for every probability vector P. So we also can choose PN.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worst case fashion and get the following conclusion.",
                    "label": 0
                },
                {
                    "sent": "Here is the my the polynomial relation between the worst case rank, one margin complexity in siskyou dimension and this allows us among other things too in.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proof of previous result from share stuff, which is that.",
                    "label": 0
                },
                {
                    "sent": "Rescue dimension of the transpose of a matrix is upper bounded by the Q dimension to the power of 4.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can improve it slightly.",
                    "label": 0
                },
                {
                    "sent": "Now we have now a polynomial of degree three, but also smaller constant here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me summarize the results I showed you today.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have some nice min Max theorem for matching complexity measures.",
                    "label": 0
                },
                {
                    "sent": "We have support vectors for linear arrangements and we have some connections to test if who statistically re learning models and let me elaborate on this last part.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But more.",
                    "label": 0
                },
                {
                    "sent": "So in the dual setting we're always confronted with the problem of maximizing the total margin of a matrix Y circle, a soul winterized multiplication.",
                    "label": 1
                },
                {
                    "sent": "So this nature of this why is very crucial.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If Y is just this rank one matrix so.",
                    "label": 1
                },
                {
                    "sent": "Build up from the product of two probability vectors and Q is under other controller of an adversary.",
                    "label": 0
                },
                {
                    "sent": "So we maximize over Q, which means we choose the target function.",
                    "label": 0
                },
                {
                    "sent": "In the worst case fashion this is.",
                    "label": 0
                },
                {
                    "sent": "As we saw related to the weak learning under fixed distribution in the SQ model.",
                    "label": 1
                },
                {
                    "sent": "If in addition this P is under control of adversary, so we also maximize over P. Which means we choose the domain distribution in the worst case fashion.",
                    "label": 1
                },
                {
                    "sent": "This corresponds to weak learning under hardest but still fixed distribution in the SQ model.",
                    "label": 0
                },
                {
                    "sent": "And if this why is arbitrary, so we don't know anything about it can be a full rank opposed to the case where here is rank one.",
                    "label": 0
                },
                {
                    "sent": "The related complexity measure is the worst case Y average margin complexity, which is just the margin complexity.",
                    "label": 0
                },
                {
                    "sent": "As I showed before, this corresponds to weak learning in the distribution independent SSQ model.",
                    "label": 0
                },
                {
                    "sent": "There's still work to do.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our results for the SQL Learning model here.",
                    "label": 0
                },
                {
                    "sent": "Out for just fix this trip.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lucian and one open problem and the natural question is, is there margin parimeter characterizing distribution independent SQ learning and can this distribution independent as Q learning be separated from SQ learning with respect to the hardest fixed distribution?",
                    "label": 1
                },
                {
                    "sent": "Another interesting question is what can maybe we can find para meters for other soft margin optimization problem so hard margin where this minimal margin and we softer?",
                    "label": 0
                },
                {
                    "sent": "Now by this why ever reaching?",
                    "label": 0
                },
                {
                    "sent": "That's typical case where also, when you have this hinge loss, so we have some sort of clip margin as which poses slick variables in this support vector optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm done here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}