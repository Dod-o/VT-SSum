{
    "id": "iurznxny3fnday6odsbsapsh2sd7qnx5",
    "title": "Blockwise Coordinate Descent Procedures for the Multi-Task Lasso with Applications to Neural Semantic Basis Discovery",
    "info": {
        "author": [
            "Mark Palatucci, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Image Analysis",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_palatucci_bcdp/",
    "segmentation": [
        [
            "OK, great so this is joint work with Han, Lu and Jonesy."
        ],
        [
            "OK, so this talk is going to be divided into 2 parts.",
            "The first is to talk briefly about sparsity since you guys got a great overview in the last section, I'm not going to spend too much time here.",
            "I'm going to talk about a multi task version of the lasso that you just saw an I'm going to discuss a new technique for solving it that's highly scalable.",
            "And the second part of the talk I want to talk about a problem in cognitive neuroscience where the goal is we want to predict a person's brain activity in response to some stimulus, and we want to show how we can formulate this as a multi task lasso and then use that to learn good features and build a good predictor so."
        ],
        [
            "Quickly I'm just going to breeze through this.",
            "Once again we have some features.",
            "Maybe there's a model that we learn we want to make some prediction an only some of those features are relevant, so we want to have a model that can learn the relevant features and ignore the rest."
        ],
        [
            "You can achieve that through regularization, just like we saw in the last talk, so we're going to basically have some sort of objective function here.",
            "We're having squared loss with response variable, some features, and then we have this L1 penalty that we saw.",
            "And that's just the lasso, so.",
            "So."
        ],
        [
            "You could imagine doing that for one task, but you could also imagine if you have multiple tasks where each has a set of features.",
            "You might believe that these tasks share some sort of common features, even though the.",
            "The data might be be different.",
            "You might believe that they share some sort of same sparsity pattern.",
            "OK, so."
        ],
        [
            "As a special case of that, you could imagine using that all these related tasks share.",
            "The same features, in which case you're just trying to discover then.",
            "Common features that are useful for predicting all these tasks simultaneously, and this was originally, I think proposed are is called the simultaneous lasso by turlach back in about five years ago, 2004."
        ],
        [
            "So.",
            "We have a regular lasso model an.",
            "The multi task version of this is very similar, so once again we have a response variable.",
            "We have some parameters that we're learning but now are square losses not just over 1 task, but we see we're summing over K tasks OK and now are our penalty term instead of the normal L1 which is the sum of the absolute values of the parameters are.",
            "Are norm here something called the L1L Infinity?",
            "So it's a it's a matrix norm and it's what this is saying is that I'm going to penalize the sum of the Max value of across tasks, so let me actually give you some intuition about what's going on here in the lasso penalty.",
            "We're learning a coefficient vector.",
            "Some parameters beta an."
        ],
        [
            "We impose.",
            "This penalty and that allows us to learn a sparse solution so the most elements are zero and we only want the relevant variables to be non zero in the multi task version of this.",
            "OK, we want to learn something that we're learning a matrix of coefficients, so you see we have.",
            "J features here and then we have K tasks.",
            "And we want to learn something that's row sparse and what I mean by row sparse is that most of the elements most of the roads are just going to contain all zeros OK and then a few of the Rose such as these two right will be allowed to have allow be allowed to be non zero and they can have their own value.",
            "For that for that task.",
            "OK."
        ],
        [
            "So.",
            "There are lots of ways to solve the single task lasso, Lars, Interior Point, Primal, dual gradient projection, coordinate descent.",
            "Just just saw another method.",
            "Stochastic version and for the multi task Lasso, Turlock, the work I mentioned before for the simultaneous lasso, he proposed an interior point method.",
            "I think she'll be speaking Adriana was speaking next about gradient projection method, and here we're going to talk about a coordinate descent method for this problem.",
            "And you guys are very lucky because you know a little more than a year ago we didn't have any of this work and this was actually kind of a hard thing to solve.",
            "And now you have two methods, so that's good."
        ],
        [
            "Ace so.",
            "So what's the best method, best method?",
            "I guess the previous talk stole my Thunder little bit on this, but there's been a lot of work in the last year.",
            "I think you mentioned the same exact same exact papers.",
            "The freedmen work wound Lang an dookie an.",
            "So.",
            "Even though coordinates, it's actually very old.",
            "It's only very recently that we found out how good it is.",
            "So in some sense it's kind of like this.",
            "Susan Boyle of optimizations algorithms where it's like it's old, it's unsophisticated, and we didn't know how good it was until very recent."
        ],
        [
            "So so why?",
            "Why is it so good?",
            "And it really it really boil, at least for the last owner and it boils down to three reasons.",
            "So the first is that I'm glad you gave the over station.",
            "The overview of coordinate sense.",
            "So I have to do it, but.",
            "Each iteration of the coordinate descent for the lasso can be solved in a closed form using soft thresholding operator.",
            "OK, an A second reason is that if the solution is actually very sparse, then we can simply kind of check for these irrelevant variables and we don't have to actually compute the updates that allows us to take advantage of the sparsity, and that's a good thing.",
            "An there's a lot of computational tricks such as warm start covariance.",
            "Pre computation and these adaptive greedy updates I'm not going to go into the details of those, but the baseline for a lot of the tricks is that often you want to compute.",
            "You want to solve this objective function for many different values of the penalty term that Lambda value that we saw before OK, an these tricks allow you to kind of re use a lot of that computation for different lambdas.",
            "OK, so the question we want to ask is can we develop a coordinate descent procedure for the multi task lasso that has sort of similar close form update for each iteration?",
            "An short answer is yes we can.",
            "An in the."
        ],
        [
            "Simple in the single task class.",
            "So we have this soft thresholding operator here.",
            "That's the close form that handles the subdifferential conditions an in the multi task lasso.",
            "The main technical result of this paper is that we can generalize soft thresholding to multiple tasks using something that's called a winterization operator.",
            "An this is the form here and I don't want to get in to the technical details of it.",
            "Right now, but the key point is that even though it is kind of hard to derive this, it's actually very, very easy to implement.",
            "And it leads to kind of this very easy."
        ],
        [
            "Procedures, so those are the two kind of takeaway points I want to.",
            "I want you to take get for this section, so the first is we can solve the multi task lasso using this closed form winsorization operator.",
            "It is very easy to implement and if you look at the algorithm in the paper you can follow it very easily.",
            "So and then that leads to coordinate descent and the good news there is that we can now take advantage of a lot of the same computational tricks for the single task lasso."
        ],
        [
            "OK, so that's the end of the first section.",
            "And now I want to talk about an application of this to cognitive neuroscience.",
            "So once again, our goal is to predict the brains activity.",
            "OK, and we're going to use the lasso to actually learn good features to do that so."
        ],
        [
            "Let me tell you about this neural activity prediction is proposed originally by colleagues Tom Mitchell Ann.",
            "The goal here is we want to take a word.",
            "We want to have some sort of computational model that given any word in English, I can put that word into the model and I can get a prediction about what the brain is actually going to do, how it's going to respond.",
            "So in the original Mitchell work, the way they represented words was actually using feature vectors that were essentially Co occurrence statistics over the Google trillion word corpus.",
            "So.",
            "For example.",
            "Over the web you'll find that the word Apple Co occurs very often with the word eat OK and taste, but it doesn't Co occur a lot with the word ride, so you see a lot of weight here and not a lot of wait there for the word airplane.",
            "It doesn't Co occur a lot with the word E or taste, so you see little weight and then but if you do ride an airplane so that's why you see a lot of weight there, so using this kind of encoding, their model had 25, it was the Co occurrence statistics with 25.",
            "Kind of sensory motor words that were handcrafted based on theories in cognitive neuroscience."
        ],
        [
            "So.",
            "Our question is, can we actually train a model to automatically discover the features rather than relying on this handcrafted stuff so?",
            "We have maybe a large encoding."
        ],
        [
            "Of Apple Ann.",
            "Here we're going to model this as a multi task lasso problem where you imagine the brain is really a 3 dimensional image of neural activity in all these different locations and those locations are called voxels.",
            "So what we do is we model the the activity.",
            "As a task, so each voxel is a task in this multi last multi test lesson.",
            "OK so."
        ],
        [
            "The data that we've collected it's fMRI images of the neural activity for 60 different words, and there's five exemplars from each category.",
            "So, as a quick example of that.",
            "This picture in the upper right hand corner.",
            "Someone gets into an fMRI scanner and this is what we show them.",
            "It's the word dog and then."
        ],
        [
            "Picture little picture of a dog little line drawing.",
            "And we've done this for body parts, animals, tools and buildings.",
            "Bunch of different categories here.",
            "OK so."
        ],
        [
            "We're going to treat that as our response that normal X videos are response variables and then our design.",
            "Matrix is now going to be cooccurrence, not with 25 words, but with 5000 most frequent words in English OK.",
            "So the experiment we're going to do is, uh, leave, too."
        ],
        [
            "To leave out two cross validation.",
            "So what we're going to do is going to train on 58 of the words are going to the words out.",
            "We're going to run our multi task lasso to select features out of that 5000 that we think are good and relevant, and then we're going to take those features and we're going to plug it into the same model as Mitchell, because what we want to do is evaluate.",
            "Are we selecting better features and?",
            "We don't want to compare necessarily different models, so.",
            "And then we're going to apply the prediction to two held out words and then will label the held out words using a cosine similarity.",
            "And then we repeat this for 60 choose two 1700 iterations.",
            "So as one example of this.",
            "Will leave out the words."
        ],
        [
            "Salary will leave out the word airplane an we put the feature vectors into our model that we've learned.",
            "Get these two predicted images and then we show the model.",
            "The two true images without the labels.",
            "And then we say you know do a cosine similarity to pick what are the labels and then that's.",
            "One test.",
            "OK so."
        ],
        [
            "Some results if we pick features randomly.",
            "OK, just grab a 25 words, train a model.",
            "And then do that a bunch of times.",
            "This is the accuracy that we get.",
            "This this black bar here.",
            "If we use the handcrafted features from.",
            "The domain experts that use these sensorimotor verbs, then we get about 77% accuracy, and then if we learn good features using the multi task lasso and we adjust our regularization parameter to give us kind of different size sets we get.",
            "Significantly better accuracy about 8087% so.",
            "And we've also noticed that as we increase the number of basis functions, our performance can go up, so repeated this then for 9 different human participants."
        ],
        [
            "Then the main things to get out of this graph are so one.",
            "In the majority of the subjects we actually.",
            "Beat the handcrafted features.",
            "The second thing is that.",
            "Um?",
            "We're doing that without actually making very many assumptions at all, so we don't have in this model.",
            "We're not assuming anything necessarily about neuroscience an the downside of that is that we're learning the features from the data, so if the data are poor.",
            "Then we can get in trouble and we have a scenario here with our subject #4 where the handcrafted features end up doing better and anyone who's done any work with fMRI studies know that often?",
            "You know, if someone gets in the scanner and they jiggle their head around a little bit, you can get very very noisy data and it's hard to actually do anything with that.",
            "So that's our theory here about why this guy is doing so poorly."
        ],
        [
            "So.",
            "One thing that's nice about this kind of model is that.",
            "Besides building a good predictor, you can actually then go back and interpret it.",
            "So what we've done is looked at the top 25 features that are selected by the multi task lasso.",
            "Now these features here are the cooccurrence words in some sense.",
            "So.",
            "We see that the model actually learned a feature tools, and that's actually really interesting because one of the categories of objects we showed people was tools we have screwdriver and hammer in there, and it learned that we had a vehicles category.",
            "So you see these these words here.",
            "These basis words.",
            "We had animals.",
            "So you see dog, Kitchen items, foods and body parts.",
            "It's easy to see where these features are coming from.",
            "Some of them.",
            "It's it's not exactly clear.",
            "Where they came from, but it is coming from the web so.",
            "So one thing you can ask is how does an individual feature contribute to neural activation so?"
        ],
        [
            "If you think about every single feature is going to add a little bit of activation to each region of the brain.",
            "We're learning weights.",
            "OK, so since each task is.",
            "A voxel in the brain.",
            "There's geometry.",
            "We can actually look at the learned weights as a actual."
        ],
        [
            "Image Ann this is what we find.",
            "So if we analyze the weights that we've learned on the tools feature OK and we plot it according to the geometry, we can see that the tools feature is putting a lot of weight in this superior temporal sulcus which is believed to be associated with perception biological motion.",
            "You also see activity here in the postcentral gyrus, which is believed associated with pre motor planning.",
            "So this is just to give you an idea of how you could use a model like this to do interpretation.",
            "OK, so the takeaway points are.",
            "The multi task lasso."
        ],
        [
            "It learns these common features across related tasks with this close form winsorization operator we can scale.",
            "We can have it.",
            "We have our method that can scale to thousands of features and tasks.",
            "It allows us to build these interpretable models and on this neural prediction task without.",
            "A lot of assumptions we can perform better than these kind of handcrafted features by the domain experts.",
            "So with that I encourage you to see the paper an will open it up for actually.",
            "Yeah, thanks to Tom Mitchell as well."
        ],
        [
            "As Keck Foundation, NSF, an Intel and Google for their support, thank you."
        ],
        [
            "Yes please.",
            "For.",
            "OK, that is going to work.",
            "Yeah, so it's a good question, so there's some theory from saying in 2001 that shows conditions that coordinate descent when it will converge.",
            "An usually depends.",
            "Depends on the loss function and it depends on whether or not you can decompose the penalty term.",
            "So in this model it actually fits the form.",
            "Of the same work, so based on that result you could say that.",
            "This will then converge because it fits that that form.",
            "Condition.",
            "So.",
            "It has to do with basically being able to decompose the penalty, and then there's a certain class of loss functions, of which I think L2 loss.",
            "Is one of them.",
            "So basically, if you have L2 loss, an indecomposable penalty, then it will work.",
            "So.",
            "Brain experiments you use seven patients or seven test subjects and you train them.",
            "Understanding is different models.",
            "Independent models for this subject.",
            "I was just wondering what kind of performance you get if you pull all those different subjects as extra multitask learning problems.",
            "So train the whole program modified that.",
            "Yeah, so that's actually really a great point.",
            "'cause what you would hope then is that if you did something like that that you might be able to avoid this problem of.",
            "The subject for outlier.",
            "So we actually did that and there's a result in the paper.",
            "Where we combine the nine subjects to learn the features and what we found is that it performed slightly better on two of the basis sets and slightly worse on the smaller basis set.",
            "So that's actually an interesting area for future work.",
            "I don't think it's.",
            "I was really hoping it was going to give you a big win, but at least in the result in the paper.",
            "It's not.",
            "It didn't show that much of a difference.",
            "Why do you think the different subjects are different?",
            "In what kind of basis functions one or you know, that's a good question an.",
            "You know, some subjects are certainly a lot noisier than others, so four was bad or subject nine was bad, so it's not clear to me at how how much having a few bad guys in there can really hurt you.",
            "So.",
            "They come from denoising image models of.",
            "Can you comment on whether this quarter decent methods day?",
            "Models wears glasses and all that.",
            "I'm not sure I understand.",
            "Coupling is very strong in the model.",
            "So your question is will it will it still?",
            "Is this sort of problems with discord and dissent would be faster?",
            "Which are the sort of problems where it would be just?",
            "Oh.",
            "Um?",
            "Well, one of the main one of the main advantages of this sort of coordinate descent is you can really take advantage of having that sparsity, so you lose that OK. We're still.",
            "Order of K log K in terms of the number of scalability in terms of the number of tasks so.",
            "And it would also it would scale in terms of the number of features very well too.",
            "So beyond that, I'm not really sure.",
            "I haven't looked at other problems yet to know enough to I guess answer.",
            "Answer That question and any sort of detail."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, great so this is joint work with Han, Lu and Jonesy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this talk is going to be divided into 2 parts.",
                    "label": 0
                },
                {
                    "sent": "The first is to talk briefly about sparsity since you guys got a great overview in the last section, I'm not going to spend too much time here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about a multi task version of the lasso that you just saw an I'm going to discuss a new technique for solving it that's highly scalable.",
                    "label": 1
                },
                {
                    "sent": "And the second part of the talk I want to talk about a problem in cognitive neuroscience where the goal is we want to predict a person's brain activity in response to some stimulus, and we want to show how we can formulate this as a multi task lasso and then use that to learn good features and build a good predictor so.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly I'm just going to breeze through this.",
                    "label": 0
                },
                {
                    "sent": "Once again we have some features.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a model that we learn we want to make some prediction an only some of those features are relevant, so we want to have a model that can learn the relevant features and ignore the rest.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can achieve that through regularization, just like we saw in the last talk, so we're going to basically have some sort of objective function here.",
                    "label": 1
                },
                {
                    "sent": "We're having squared loss with response variable, some features, and then we have this L1 penalty that we saw.",
                    "label": 1
                },
                {
                    "sent": "And that's just the lasso, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could imagine doing that for one task, but you could also imagine if you have multiple tasks where each has a set of features.",
                    "label": 0
                },
                {
                    "sent": "You might believe that these tasks share some sort of common features, even though the.",
                    "label": 0
                },
                {
                    "sent": "The data might be be different.",
                    "label": 0
                },
                {
                    "sent": "You might believe that they share some sort of same sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a special case of that, you could imagine using that all these related tasks share.",
                    "label": 0
                },
                {
                    "sent": "The same features, in which case you're just trying to discover then.",
                    "label": 0
                },
                {
                    "sent": "Common features that are useful for predicting all these tasks simultaneously, and this was originally, I think proposed are is called the simultaneous lasso by turlach back in about five years ago, 2004.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have a regular lasso model an.",
                    "label": 1
                },
                {
                    "sent": "The multi task version of this is very similar, so once again we have a response variable.",
                    "label": 1
                },
                {
                    "sent": "We have some parameters that we're learning but now are square losses not just over 1 task, but we see we're summing over K tasks OK and now are our penalty term instead of the normal L1 which is the sum of the absolute values of the parameters are.",
                    "label": 0
                },
                {
                    "sent": "Are norm here something called the L1L Infinity?",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a matrix norm and it's what this is saying is that I'm going to penalize the sum of the Max value of across tasks, so let me actually give you some intuition about what's going on here in the lasso penalty.",
                    "label": 0
                },
                {
                    "sent": "We're learning a coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "Some parameters beta an.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We impose.",
                    "label": 0
                },
                {
                    "sent": "This penalty and that allows us to learn a sparse solution so the most elements are zero and we only want the relevant variables to be non zero in the multi task version of this.",
                    "label": 1
                },
                {
                    "sent": "OK, we want to learn something that we're learning a matrix of coefficients, so you see we have.",
                    "label": 0
                },
                {
                    "sent": "J features here and then we have K tasks.",
                    "label": 0
                },
                {
                    "sent": "And we want to learn something that's row sparse and what I mean by row sparse is that most of the elements most of the roads are just going to contain all zeros OK and then a few of the Rose such as these two right will be allowed to have allow be allowed to be non zero and they can have their own value.",
                    "label": 0
                },
                {
                    "sent": "For that for that task.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are lots of ways to solve the single task lasso, Lars, Interior Point, Primal, dual gradient projection, coordinate descent.",
                    "label": 1
                },
                {
                    "sent": "Just just saw another method.",
                    "label": 0
                },
                {
                    "sent": "Stochastic version and for the multi task Lasso, Turlock, the work I mentioned before for the simultaneous lasso, he proposed an interior point method.",
                    "label": 0
                },
                {
                    "sent": "I think she'll be speaking Adriana was speaking next about gradient projection method, and here we're going to talk about a coordinate descent method for this problem.",
                    "label": 0
                },
                {
                    "sent": "And you guys are very lucky because you know a little more than a year ago we didn't have any of this work and this was actually kind of a hard thing to solve.",
                    "label": 0
                },
                {
                    "sent": "And now you have two methods, so that's good.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ace so.",
                    "label": 0
                },
                {
                    "sent": "So what's the best method, best method?",
                    "label": 1
                },
                {
                    "sent": "I guess the previous talk stole my Thunder little bit on this, but there's been a lot of work in the last year.",
                    "label": 0
                },
                {
                    "sent": "I think you mentioned the same exact same exact papers.",
                    "label": 0
                },
                {
                    "sent": "The freedmen work wound Lang an dookie an.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Even though coordinates, it's actually very old.",
                    "label": 0
                },
                {
                    "sent": "It's only very recently that we found out how good it is.",
                    "label": 0
                },
                {
                    "sent": "So in some sense it's kind of like this.",
                    "label": 0
                },
                {
                    "sent": "Susan Boyle of optimizations algorithms where it's like it's old, it's unsophisticated, and we didn't know how good it was until very recent.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so why?",
                    "label": 0
                },
                {
                    "sent": "Why is it so good?",
                    "label": 1
                },
                {
                    "sent": "And it really it really boil, at least for the last owner and it boils down to three reasons.",
                    "label": 0
                },
                {
                    "sent": "So the first is that I'm glad you gave the over station.",
                    "label": 0
                },
                {
                    "sent": "The overview of coordinate sense.",
                    "label": 0
                },
                {
                    "sent": "So I have to do it, but.",
                    "label": 0
                },
                {
                    "sent": "Each iteration of the coordinate descent for the lasso can be solved in a closed form using soft thresholding operator.",
                    "label": 1
                },
                {
                    "sent": "OK, an A second reason is that if the solution is actually very sparse, then we can simply kind of check for these irrelevant variables and we don't have to actually compute the updates that allows us to take advantage of the sparsity, and that's a good thing.",
                    "label": 0
                },
                {
                    "sent": "An there's a lot of computational tricks such as warm start covariance.",
                    "label": 0
                },
                {
                    "sent": "Pre computation and these adaptive greedy updates I'm not going to go into the details of those, but the baseline for a lot of the tricks is that often you want to compute.",
                    "label": 0
                },
                {
                    "sent": "You want to solve this objective function for many different values of the penalty term that Lambda value that we saw before OK, an these tricks allow you to kind of re use a lot of that computation for different lambdas.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question we want to ask is can we develop a coordinate descent procedure for the multi task lasso that has sort of similar close form update for each iteration?",
                    "label": 1
                },
                {
                    "sent": "An short answer is yes we can.",
                    "label": 0
                },
                {
                    "sent": "An in the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple in the single task class.",
                    "label": 0
                },
                {
                    "sent": "So we have this soft thresholding operator here.",
                    "label": 0
                },
                {
                    "sent": "That's the close form that handles the subdifferential conditions an in the multi task lasso.",
                    "label": 0
                },
                {
                    "sent": "The main technical result of this paper is that we can generalize soft thresholding to multiple tasks using something that's called a winterization operator.",
                    "label": 1
                },
                {
                    "sent": "An this is the form here and I don't want to get in to the technical details of it.",
                    "label": 0
                },
                {
                    "sent": "Right now, but the key point is that even though it is kind of hard to derive this, it's actually very, very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "And it leads to kind of this very easy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Procedures, so those are the two kind of takeaway points I want to.",
                    "label": 0
                },
                {
                    "sent": "I want you to take get for this section, so the first is we can solve the multi task lasso using this closed form winsorization operator.",
                    "label": 1
                },
                {
                    "sent": "It is very easy to implement and if you look at the algorithm in the paper you can follow it very easily.",
                    "label": 0
                },
                {
                    "sent": "So and then that leads to coordinate descent and the good news there is that we can now take advantage of a lot of the same computational tricks for the single task lasso.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the end of the first section.",
                    "label": 0
                },
                {
                    "sent": "And now I want to talk about an application of this to cognitive neuroscience.",
                    "label": 1
                },
                {
                    "sent": "So once again, our goal is to predict the brains activity.",
                    "label": 1
                },
                {
                    "sent": "OK, and we're going to use the lasso to actually learn good features to do that so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me tell you about this neural activity prediction is proposed originally by colleagues Tom Mitchell Ann.",
                    "label": 1
                },
                {
                    "sent": "The goal here is we want to take a word.",
                    "label": 0
                },
                {
                    "sent": "We want to have some sort of computational model that given any word in English, I can put that word into the model and I can get a prediction about what the brain is actually going to do, how it's going to respond.",
                    "label": 0
                },
                {
                    "sent": "So in the original Mitchell work, the way they represented words was actually using feature vectors that were essentially Co occurrence statistics over the Google trillion word corpus.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Over the web you'll find that the word Apple Co occurs very often with the word eat OK and taste, but it doesn't Co occur a lot with the word ride, so you see a lot of weight here and not a lot of wait there for the word airplane.",
                    "label": 0
                },
                {
                    "sent": "It doesn't Co occur a lot with the word E or taste, so you see little weight and then but if you do ride an airplane so that's why you see a lot of weight there, so using this kind of encoding, their model had 25, it was the Co occurrence statistics with 25.",
                    "label": 0
                },
                {
                    "sent": "Kind of sensory motor words that were handcrafted based on theories in cognitive neuroscience.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our question is, can we actually train a model to automatically discover the features rather than relying on this handcrafted stuff so?",
                    "label": 1
                },
                {
                    "sent": "We have maybe a large encoding.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of Apple Ann.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to model this as a multi task lasso problem where you imagine the brain is really a 3 dimensional image of neural activity in all these different locations and those locations are called voxels.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we model the the activity.",
                    "label": 1
                },
                {
                    "sent": "As a task, so each voxel is a task in this multi last multi test lesson.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The data that we've collected it's fMRI images of the neural activity for 60 different words, and there's five exemplars from each category.",
                    "label": 1
                },
                {
                    "sent": "So, as a quick example of that.",
                    "label": 0
                },
                {
                    "sent": "This picture in the upper right hand corner.",
                    "label": 0
                },
                {
                    "sent": "Someone gets into an fMRI scanner and this is what we show them.",
                    "label": 0
                },
                {
                    "sent": "It's the word dog and then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture little picture of a dog little line drawing.",
                    "label": 0
                },
                {
                    "sent": "And we've done this for body parts, animals, tools and buildings.",
                    "label": 0
                },
                {
                    "sent": "Bunch of different categories here.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to treat that as our response that normal X videos are response variables and then our design.",
                    "label": 0
                },
                {
                    "sent": "Matrix is now going to be cooccurrence, not with 25 words, but with 5000 most frequent words in English OK.",
                    "label": 1
                },
                {
                    "sent": "So the experiment we're going to do is, uh, leave, too.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To leave out two cross validation.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is going to train on 58 of the words are going to the words out.",
                    "label": 1
                },
                {
                    "sent": "We're going to run our multi task lasso to select features out of that 5000 that we think are good and relevant, and then we're going to take those features and we're going to plug it into the same model as Mitchell, because what we want to do is evaluate.",
                    "label": 1
                },
                {
                    "sent": "Are we selecting better features and?",
                    "label": 0
                },
                {
                    "sent": "We don't want to compare necessarily different models, so.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to apply the prediction to two held out words and then will label the held out words using a cosine similarity.",
                    "label": 1
                },
                {
                    "sent": "And then we repeat this for 60 choose two 1700 iterations.",
                    "label": 0
                },
                {
                    "sent": "So as one example of this.",
                    "label": 0
                },
                {
                    "sent": "Will leave out the words.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Salary will leave out the word airplane an we put the feature vectors into our model that we've learned.",
                    "label": 0
                },
                {
                    "sent": "Get these two predicted images and then we show the model.",
                    "label": 0
                },
                {
                    "sent": "The two true images without the labels.",
                    "label": 0
                },
                {
                    "sent": "And then we say you know do a cosine similarity to pick what are the labels and then that's.",
                    "label": 0
                },
                {
                    "sent": "One test.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some results if we pick features randomly.",
                    "label": 0
                },
                {
                    "sent": "OK, just grab a 25 words, train a model.",
                    "label": 0
                },
                {
                    "sent": "And then do that a bunch of times.",
                    "label": 0
                },
                {
                    "sent": "This is the accuracy that we get.",
                    "label": 0
                },
                {
                    "sent": "This this black bar here.",
                    "label": 0
                },
                {
                    "sent": "If we use the handcrafted features from.",
                    "label": 1
                },
                {
                    "sent": "The domain experts that use these sensorimotor verbs, then we get about 77% accuracy, and then if we learn good features using the multi task lasso and we adjust our regularization parameter to give us kind of different size sets we get.",
                    "label": 0
                },
                {
                    "sent": "Significantly better accuracy about 8087% so.",
                    "label": 0
                },
                {
                    "sent": "And we've also noticed that as we increase the number of basis functions, our performance can go up, so repeated this then for 9 different human participants.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the main things to get out of this graph are so one.",
                    "label": 0
                },
                {
                    "sent": "In the majority of the subjects we actually.",
                    "label": 0
                },
                {
                    "sent": "Beat the handcrafted features.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We're doing that without actually making very many assumptions at all, so we don't have in this model.",
                    "label": 0
                },
                {
                    "sent": "We're not assuming anything necessarily about neuroscience an the downside of that is that we're learning the features from the data, so if the data are poor.",
                    "label": 0
                },
                {
                    "sent": "Then we can get in trouble and we have a scenario here with our subject #4 where the handcrafted features end up doing better and anyone who's done any work with fMRI studies know that often?",
                    "label": 0
                },
                {
                    "sent": "You know, if someone gets in the scanner and they jiggle their head around a little bit, you can get very very noisy data and it's hard to actually do anything with that.",
                    "label": 0
                },
                {
                    "sent": "So that's our theory here about why this guy is doing so poorly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One thing that's nice about this kind of model is that.",
                    "label": 0
                },
                {
                    "sent": "Besides building a good predictor, you can actually then go back and interpret it.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is looked at the top 25 features that are selected by the multi task lasso.",
                    "label": 0
                },
                {
                    "sent": "Now these features here are the cooccurrence words in some sense.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We see that the model actually learned a feature tools, and that's actually really interesting because one of the categories of objects we showed people was tools we have screwdriver and hammer in there, and it learned that we had a vehicles category.",
                    "label": 0
                },
                {
                    "sent": "So you see these these words here.",
                    "label": 0
                },
                {
                    "sent": "These basis words.",
                    "label": 0
                },
                {
                    "sent": "We had animals.",
                    "label": 0
                },
                {
                    "sent": "So you see dog, Kitchen items, foods and body parts.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see where these features are coming from.",
                    "label": 0
                },
                {
                    "sent": "Some of them.",
                    "label": 0
                },
                {
                    "sent": "It's it's not exactly clear.",
                    "label": 0
                },
                {
                    "sent": "Where they came from, but it is coming from the web so.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can ask is how does an individual feature contribute to neural activation so?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you think about every single feature is going to add a little bit of activation to each region of the brain.",
                    "label": 0
                },
                {
                    "sent": "We're learning weights.",
                    "label": 0
                },
                {
                    "sent": "OK, so since each task is.",
                    "label": 0
                },
                {
                    "sent": "A voxel in the brain.",
                    "label": 0
                },
                {
                    "sent": "There's geometry.",
                    "label": 0
                },
                {
                    "sent": "We can actually look at the learned weights as a actual.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Image Ann this is what we find.",
                    "label": 0
                },
                {
                    "sent": "So if we analyze the weights that we've learned on the tools feature OK and we plot it according to the geometry, we can see that the tools feature is putting a lot of weight in this superior temporal sulcus which is believed to be associated with perception biological motion.",
                    "label": 1
                },
                {
                    "sent": "You also see activity here in the postcentral gyrus, which is believed associated with pre motor planning.",
                    "label": 0
                },
                {
                    "sent": "So this is just to give you an idea of how you could use a model like this to do interpretation.",
                    "label": 0
                },
                {
                    "sent": "OK, so the takeaway points are.",
                    "label": 0
                },
                {
                    "sent": "The multi task lasso.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It learns these common features across related tasks with this close form winsorization operator we can scale.",
                    "label": 1
                },
                {
                    "sent": "We can have it.",
                    "label": 0
                },
                {
                    "sent": "We have our method that can scale to thousands of features and tasks.",
                    "label": 1
                },
                {
                    "sent": "It allows us to build these interpretable models and on this neural prediction task without.",
                    "label": 1
                },
                {
                    "sent": "A lot of assumptions we can perform better than these kind of handcrafted features by the domain experts.",
                    "label": 0
                },
                {
                    "sent": "So with that I encourage you to see the paper an will open it up for actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks to Tom Mitchell as well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As Keck Foundation, NSF, an Intel and Google for their support, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "OK, that is going to work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's a good question, so there's some theory from saying in 2001 that shows conditions that coordinate descent when it will converge.",
                    "label": 0
                },
                {
                    "sent": "An usually depends.",
                    "label": 0
                },
                {
                    "sent": "Depends on the loss function and it depends on whether or not you can decompose the penalty term.",
                    "label": 0
                },
                {
                    "sent": "So in this model it actually fits the form.",
                    "label": 0
                },
                {
                    "sent": "Of the same work, so based on that result you could say that.",
                    "label": 0
                },
                {
                    "sent": "This will then converge because it fits that that form.",
                    "label": 0
                },
                {
                    "sent": "Condition.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It has to do with basically being able to decompose the penalty, and then there's a certain class of loss functions, of which I think L2 loss.",
                    "label": 0
                },
                {
                    "sent": "Is one of them.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you have L2 loss, an indecomposable penalty, then it will work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Brain experiments you use seven patients or seven test subjects and you train them.",
                    "label": 0
                },
                {
                    "sent": "Understanding is different models.",
                    "label": 0
                },
                {
                    "sent": "Independent models for this subject.",
                    "label": 0
                },
                {
                    "sent": "I was just wondering what kind of performance you get if you pull all those different subjects as extra multitask learning problems.",
                    "label": 0
                },
                {
                    "sent": "So train the whole program modified that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's actually really a great point.",
                    "label": 0
                },
                {
                    "sent": "'cause what you would hope then is that if you did something like that that you might be able to avoid this problem of.",
                    "label": 0
                },
                {
                    "sent": "The subject for outlier.",
                    "label": 0
                },
                {
                    "sent": "So we actually did that and there's a result in the paper.",
                    "label": 0
                },
                {
                    "sent": "Where we combine the nine subjects to learn the features and what we found is that it performed slightly better on two of the basis sets and slightly worse on the smaller basis set.",
                    "label": 0
                },
                {
                    "sent": "So that's actually an interesting area for future work.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's.",
                    "label": 0
                },
                {
                    "sent": "I was really hoping it was going to give you a big win, but at least in the result in the paper.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It didn't show that much of a difference.",
                    "label": 0
                },
                {
                    "sent": "Why do you think the different subjects are different?",
                    "label": 0
                },
                {
                    "sent": "In what kind of basis functions one or you know, that's a good question an.",
                    "label": 0
                },
                {
                    "sent": "You know, some subjects are certainly a lot noisier than others, so four was bad or subject nine was bad, so it's not clear to me at how how much having a few bad guys in there can really hurt you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "They come from denoising image models of.",
                    "label": 0
                },
                {
                    "sent": "Can you comment on whether this quarter decent methods day?",
                    "label": 0
                },
                {
                    "sent": "Models wears glasses and all that.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand.",
                    "label": 0
                },
                {
                    "sent": "Coupling is very strong in the model.",
                    "label": 0
                },
                {
                    "sent": "So your question is will it will it still?",
                    "label": 0
                },
                {
                    "sent": "Is this sort of problems with discord and dissent would be faster?",
                    "label": 0
                },
                {
                    "sent": "Which are the sort of problems where it would be just?",
                    "label": 0
                },
                {
                    "sent": "Oh.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, one of the main one of the main advantages of this sort of coordinate descent is you can really take advantage of having that sparsity, so you lose that OK. We're still.",
                    "label": 0
                },
                {
                    "sent": "Order of K log K in terms of the number of scalability in terms of the number of tasks so.",
                    "label": 0
                },
                {
                    "sent": "And it would also it would scale in terms of the number of features very well too.",
                    "label": 0
                },
                {
                    "sent": "So beyond that, I'm not really sure.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at other problems yet to know enough to I guess answer.",
                    "label": 0
                },
                {
                    "sent": "Answer That question and any sort of detail.",
                    "label": 0
                }
            ]
        }
    }
}