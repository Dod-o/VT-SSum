{
    "id": "b7wbfp27ovcgqseioe3ahajlgtogy64q",
    "title": "Critical behavior in networks of real neurons",
    "info": {
        "author": [
            "Ga\u0161per Tka\u010dik, Institute of Science and Technology Austria (IST Austria)"
        ],
        "published": "April 29, 2014",
        "recorded": "April 2014",
        "category": [
            "Top->Physics",
            "Top->Medicine->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/kolokviji_tkacik_real_neurons/",
    "segmentation": [
        [
            "It's my pleasure to introduce, introduce Professor Gaspard Kacik.",
            "He got his Bachelor of Science in Physics from University of Ljubljana in 2001 and then a PhD in physics from Princeton University.",
            "Six years later he was a post doctoral fellow at Princeton University in 2007 and then moved to University of Pennsylvania for another postdoc.",
            "Currently he is assistant professor at the Institute of Science and Technology, Austria.",
            "That's since 2011.",
            "His main research topics are bio physics and neuroscience, especially gene regulatory networks, neural coding.",
            "Natural scenes.",
            "Within the context of efficient coding hypothesis and collect the dynamics.",
            "He's coauthor of 34 papers, seven of which have been published in Proceedings of National Academy of Sciences of the United States of America has also engaged in some editorial, an organization work.",
            "His editor of physical biology, and he organized the conference sensory Coding and natural Environment in Vienna for his work.",
            "He's received many grants and awards, and today he's going to tell us.",
            "Something about critical behavior in networks of real neurons.",
            "Below happy to be out.",
            "There is some super while a premature as the emotions developed.",
            "It's down at the low.",
            "Car beer at here picazo.",
            "Because of it, I put the monster, but not a not a touch interaction with physical and Bellagio, the knee, Vietnam Premier, some Utada, blog post, dollar operation, yeah Physically Patch statistic receiver or audia in Annalisa.",
            "Numerical analysts predict Conan Alicia and work the solve.",
            "Leave Oprah, Sonia physical illness and negotiations.",
            "Which aspect of physical and mental pressure is to put in voted to trigger the Serb mature paradigma?",
            "Bernie Houghton Co. Life Sciences.",
            "Resuming Staticky Harnish today to open the Mucha.",
            "Nikki Otega load Sheet notices is present at yeah.",
            "OK, so maybe now it's a good time to for me to switch into English before going on.",
            "I'd like to acknowledge our collaborators olyvia marred area modern Michael Berry, where the experimental colleagues on this work, Princeton University and Olivia has moved to Paris now anthurium, Orion.",
            "Bill Bialek, our theoretical collaborators Bill also used to be my PhD advisor.",
            "So what we're going to be talking about today is."
        ],
        [
            "This that's actually not just a schematic diagram, it's a real piece of data.",
            "This is.",
            "Visualization of a neural code time in this picture goes this way, every single row, for instance, this one represents the activity of 1 neuron.",
            "Where you see this a box of nothing happening silences so neuron produces no activity.",
            "And then this this sharp lines represent neural spikes.",
            "So unitary events that are used in the nervous system to transmit information.",
            "I'll say more about the details of the experimental system, but what I want to convey is that this is the kind of data that you can get.",
            "From various nervous tissues, this is the data that encodes the information, for instance, of visual information that goes from your eye to the brain and the questions we would like to ask."
        ],
        [
            "This sort of data are as follows.",
            "First of all, can we make sense of this?",
            "If this is really a, can we make sense of this joint patterns of activity?",
            "So these are many neurons across time and we would like to somehow understand these patterns of activity.",
            "Really, at the single spike level, without averaging, without computing, firing rates, etc.",
            "If we really understood, really think of this as a code that conveys information, maybe even encodes a stimulus from the outside world.",
            "Then how can we figure out what this code is?",
            "Can we ultimately decode this code?",
            "So looking at the spikes, can we say what the stimulus was that induced this spikes?",
            "You know basic questions about the code are.",
            "Is this code independent?",
            "By what I mean is that every neuron is is conveying its own bit of information about the stimulus, like a CCD camera.",
            "Every pixel is conveying an independent piece of information about what's going on in the outside world, in a little part of the Sky.",
            "Or is it in some sense combinatorial, which means that to understand what this spike of neuron 37 means, I actually need to know what the neuron 35 did at the same time, right?",
            "So it is not somehow?",
            "You know I cannot break up the code in sort of individual lines that I can understand one by one.",
            "And Lastly, I won't say much about this, but it's actually a very interesting historical remark, so there has been a lot of theoretical work on neural coding specifically coming from statistical mechanics, community Hopfield networks, associative memory, etc etc.",
            "These are all very very nice theoretical ideas, But the problem has always been how to connect those ideas to data, right?",
            "'cause they are statements about collective activity of many neurons, and yet in every single experiment you don't measure from all the neurons at the same time.",
            "How can we ever test whether a piece of neural tissue is actually at, say, implementing one of these theoretical ideas or not?",
            "Some of the work that we are doing is trying to make this connection."
        ],
        [
            "So the overview of the talk is as follows.",
            "I'll present the data that we're so this is a data driven research will present the data that we work on.",
            "Then I will introduce this concept of sort of inverse statistical physics models.",
            "Maximum entropy models for these joint activity patterns of many neurons.",
            "I'll argue that these models are good descriptions of the data and then we study the behavior of this inferred models and really compare with some predictions with some signatures that we find in the data."
        ],
        [
            "So let me let me now introduce the retina.",
            "That's the system that we are going to study.",
            "So writing as the little piece of nervous tissue in your eye that transduces light coming in to excite photo receptors here so it transduces light into electrical spiking activity of these output layer.",
            "And once called the retinal ganglion cells.",
            "These are the things that send the lines to the brain, right?",
            "So everything that you know about the visual world comes from the joint activity patterns of this layer and those are the spikes that you saw on the 1st slide.",
            "Right, so it's a stereo type piece of planar tissue.",
            "That's what make you know what makes it nice for experiment.",
            "You can take the retina out of the eye of the animal and put it down on the microelectrode recording array.",
            "I'll show it in the next slide, and then you can display any light stimulus you want to this device.",
            "You can repeat it many times, exactly the same stimulus you can keep it alive for several hours, Meanwhile recording the output activities of these neurons.",
            "And so it's one of the.",
            "It's one of the paradigmatic systems in which, in neuroscience you study the input output relation.",
            "So how input stimuli are encoded in the output activity patterns of the neurons, with the advantage that the input is perfectly controlled because it slide, right?",
            "So you can use just your screen to project any sort of stimulus you want onto this retina."
        ],
        [
            "So and in particular in neural coding, it's one instance where we really ask about populations.",
            "What I try to motivate in the beginning.",
            "So how do many of these neurons together represent the stimulus, not how do they do it?",
            "One run at the time?",
            "You might say, wait, well, you know we know stuff about the retina, so isn't it just like a camera?",
            "So biological camera with lots of pixels, right?",
            "Tiling the Sky, each pixel?",
            "It's photo?"
        ],
        [
            "Receptor reporting on the local at light intensity level let me present this straw man argument that we are going to kill afterwards.",
            "So what has been known and what is the textbook is that each one of those cells, the cells that we record from retinal ganglion cells, what it is doing is it's literally.",
            "I mean that's the dogma.",
            "It's taking an input light intensity pattern in a little part of the Sky.",
            "It's linearly filtering it with the.",
            "Thing that looks like this, which is called the center surround.",
            "So basically it takes light intensity here and subtracts away a light intensity in the little bit of a surround, right?",
            "That's what is responding to.",
            "And then you know this linearly filtered result is what it communicates.",
            "We know how this is implemented anatomically in the retina, more or less theory tells us why the retina should do something like that.",
            "That's becausw, natural scenes are highly correlated across large distances, so if there is bright here next door, there will be a bright pixel as well.",
            "You don't want to communicate redundant information, so you only report contrast changes, right?",
            "That's like a derivative filter if you want in space, right?",
            "The center minus the surround.",
            "This theory predicts that the retinal cells should do that, and it makes a prediction that the outputs therefore of these neurons should be as decorrelated or as independent as possible.",
            "Alright, so that's called the efficient coding hypothesis.",
            "We can build very good mathematical models that can predict the behavior of every single neuron to the slide stimulus, and then you just need to combine this.",
            "I'll skip that, just need to combine this with the additional fact that each of these neurons pays attention to a little part of the Sky.",
            "And together they tile the visual space, right?",
            "So there is many in arranging this local mosaic like in your CCD camera and so given that we know how to model one, we would assume, well we know how to model all of them, right?",
            "So we can predict the response of the retina to an arbitrary input stimulus, let's say."
        ],
        [
            "So we should know everything that there is to know about the retina and this whole thing makes a prediction that the spike trains the outgoing spike transform.",
            "The retina should be as decorrelated as possible because they're decorrelating natural scenes through this optimal linear filter here."
        ],
        [
            "So that's that's the picture I want you to have in mind and will challenge some of these things in the picture.",
            "But before I go on, I want to show you how you study neural population.",
            "So that's experimental colleagues that published the method in Journal of Neuroscience.",
            "So that is a microelectrode array.",
            "It's a fabricated device with 252 electrodes.",
            "Those are these little things here.",
            "The wires going out there space about.",
            "30 Micron.",
            "And if you take retina of the animals that we study in this case was salamander and put it down on this array, the key feature is that these arrays so dense that in the retina is a planar tissue that in a little Patch of the retina the stuff that sits above the array you actually record every single or close to every single output neurons, right?",
            "So you really record from a dense Patch.",
            "You don't miss the signals that come out.",
            "You don't record from the full retina because it's much bigger than the stuff that sitting.",
            "Above the array, but you record from everything that is above the array and by the virtue of how the retina is designed, the neurons that review record.",
            "From here they are all Co located and they all in code apart of the stimulus space apart of the visual scene.",
            "That's also in a connected region right?",
            "So that connected region of the stimulus space is fully encoded by the population you record from.",
            "And that's a very rare case in neuroscience that you get the full mapping between inputs and outputs.",
            "Traditionally, if you record from somewhere in the cortex, you stick in the electrodes.",
            "Record from one 1000 neurons, but that will be 100 out of a tiny fraction of the total number, which could be millions right or thousands, right?",
            "So you have to do with this sparse sampling of the network.",
            "But here we actually get essentially all of the neurons."
        ],
        [
            "To see how this.",
            "Normal activity looks like what I'm going to present is.",
            "I'm going to play for you, a movie that's a movie that was actually shown to the salamander retina fish swimming around in the fish tank.",
            "Every neuron that you record from pays attention to a little part of the visual space.",
            "Here, for one, we're on this neuron denoted by red lips is paying attention to what is happening in this part of the picture, so there is.",
            "There is a standard ways to when you do a recording to figure out which part of the Sky the neuron is responsive to.",
            "So this one neuron is responsive to the part here, but we."
        ],
        [
            "Records from several 100 neurons.",
            "These are all their so called receptive fields.",
            "So the neurons on my array are densely.",
            "Reporting and what's happening in this part of the visual space, right?"
        ],
        [
            "So to animate this, to give you an impression of the actual dynamics of the activity.",
            "What I'm going to do now is actually play a short clip of the movie and every time the neuron that looks at this part of the Sky is making a spike, this red ellipse will blink on and off, so we will see how this looks like."
        ],
        [
            "Alright, so here is my remember neurons look here.",
            "There is a spontaneous activity but nothing much happens because nothing changes.",
            "But now fish will come right so fish comes it has this high contrast stripes and you see so I hope that when the fish was swimming this way it excited this burst of activity of various neurons right?",
            "And then you can if you want you can repeat the same stimulus many times over or you can display synthetic stimuli, not naturalistic one, just dots bars.",
            "You know something statistically well defined if you want to.",
            "What we'll do is we'll try to analyze this response of the retina to natural natural scenes, which is what the right now involved."
        ],
        [
            "Code.",
            "So here is your natural stimulus.",
            "And this is what you get from the experiment.",
            "So roster time going this way, various distinct neurons recorded going this way and to analyze this data, we're going to reply."
        ],
        [
            "Then the state of the retina in little time bins of duration.",
            "I'll been the time into 20 milliseconds.",
            "I'm happy to explain why 20.",
            "If somebody asks later on within these 20 millisecond time bins the joint activity pattern of the retina is therefore represented by this pattern of silences and spikes, and 20 milliseconds is such a time scale that within a single bin of 20 milliseconds the neurons almost never spike twice.",
            "So basically we have a binary representation of the retinal activity with zeros and ones.",
            "Alright, and that vector W of binary activity is one of the vectors in the long roster, and what we want to understand the very basic question is like."
        ],
        [
            "You are trying to understand the foreign language.",
            "If you want is the statistics of this activity patterns right?",
            "So when I display a movie The retina and maybe repeat the same movie many times, the retina produces this binary words and I'd like to have a model of the probability distribution over all these binary words.",
            "So which words happen often?",
            "Which words are never happen?",
            "Are they organized in some sort of clusters?",
            "And so on."
        ],
        [
            "Some of that is the major problem with.",
            "That is of course the curse of dimensionality, right?",
            "So if I have 100 neurons emitting 100 bit binary words, there is at least in principle two to two to the 100 possible combinations and I cannot get to this distribution by sampling right?",
            "I'm always in the totally undersampled limit, so something has to be done about that.",
            "So what can?"
        ],
        [
            "Be done about this is one way to deal with this are the maximum entropy model, so that will be a dense slide by it's the most of kind of most formal slide, and once we are done with this, hopefully things get get to be easy, so maximum entropy method is a method for approximating probability distributions.",
            "Even high dimensional ones, so I'll give you now step by step.",
            "Summary of how you can build such models and this is over generic frameworks not useful just for this retina.",
            "So what do you do?",
            "So consider so this.",
            "So consider the Sigma one to Sigma N are the states of my neurons neuron one or this can be binary as in our case so these are 01 variables and what we start with is you choose L functions.",
            "Arbitrary number of functions over the state of these retina, so over this 100 bit.",
            "If it's 100 neuron, 100 binary words, and you choose these functions such that you can rely abli, estimate their empirical averages across the final data set that you get in the experiment, right?",
            "I'll give you examples very soon.",
            "Right, so you choose the functions you estimate the average values of these functions across the code words that you record in the data, and now you look for a distribution over all possible activity patterns that fulfills two conditions.",
            "So the first condition is that this distribution will exactly match the measured constraints.",
            "This measured average values in the data and the second condition is that apart from matching this constraint exactly, this distribution is going to be as random as possible, which means.",
            "Maximum entropy, so no assumptions, but for the explicit, explicitly matching exactly a certain list of constraints that you measure, you can show that this maximum entropy distributions, much like in statistical physics, have a Boltzmann like form.",
            "So this is normalization exponent of a sum.",
            "And in this sum there is one term for every function that you want to constrain and in front of that function is a LaGrange multiplier.",
            "If you want a number that you have to tune in such a way.",
            "That averages of these L functions over the distribution matched the ones in the data.",
            "For physics physicist in the audience, of course the Bolts you can view a Boltzmann distribution of statistical mechanics precisely.",
            "The maximum entropy distribution with the constraint on mean energy and LaGrange multiplier is 1 / K BT right?",
            "Um?",
            "So.",
            "Numerically finding these LaGrange multipliers is hard, but I won't talk about this problem at all, so now the whole thing of modeling reduces to choosing what kind of stuff do we want to constrain in the data, and there is no prescription for that, so that is led by intuition measurements and sort of iterative work of science.",
            "But let me give you a few basic ideas right so you can constrain for these L functions.",
            "You can just constrain the mean value of every neuron.",
            "The mean firing rate.",
            "How many spikes on average does each neuron make?",
            "And if you do that, the model will factorize.",
            "So the most random model consistent with the average firing rate of every neuron is a model of independent neurons, each ticking, along with each with its own firing rate.",
            "That is reproducing what you measure in the data.",
            "So simple, trivial model.",
            "The next most more complicated models you say I want probability distribution that will reproduce the average firing rate of every neuron, but also it will reproduce the measured pairwise correlation between every pair of neurons.",
            "OK, now that is already and I'll show you in the next life that's already highly nontrivial.",
            "If you look at what kind of Hamiltonian, what kind of probability distribution is consistent with these two constraints, it will be exactly or Ising Ising model from statistical physics without any symmetries so densely connected.",
            "JJ between every pair of neurons and knowledge edges can be different.",
            "You can also constrain various other things.",
            "How many neurons spike simultaneously?",
            "That's the cold so cold case by constraint and you can mix and match these constraints so we can make a model consistent with average firing rates of everyone with the covariance of every pair and with this case statistics which I will introduce also later, which is we just means you know how many times out of 100 neurons are, all of them quiet.",
            "Any one of them fires 2 of them fire, etc.",
            "So you can do all of these things.",
            "You know?",
            "Why am I telling you that, right?"
        ],
        [
            "I'm telling you that maybe?",
            "Maybe I skip this slide, sorry I'll get back to you if you have specific questions."
        ],
        [
            "Why am I telling you that I'm telling you that becausw in neural coding there has been long for a long time that there has been this sort of problem and the problem has been as follows.",
            "If you take any pair of neurons and that's holds true for many neural recordings, not just the retina, you measure the correlation between any pair of neurons in the retina or everywhere else.",
            "What you're going to find is that these correlations between pairs of neurons.",
            "Here is a histogram of this between different recorded pairs are all extremely small.",
            "Alright, so this is correlation coefficient between minus one and one the typical value 0.03 or so.",
            "They're statistically significant because you have enough data to say that, but they're very small.",
            "Write an when people in neuroscience measure this.",
            "They wave their hands and basically said well, these correlations are so small that perhaps we can just neglect them, and these are independent neurons, and if they are independent, that's actually consistent with what we think.",
            "Let's say in the retina, how the retina should work.",
            "It should be called the inputs, and so the output should be independent.",
            "That even fits the theory, so that's great, right?",
            "But how do you actually check whether this correlations, despite them, all of them being smaller, important or not while you do the following, you build a maximum entropy distribution that is consistent with either just average firing rates in neglect correlations, or you build a model that's consistent both with average firing rates and the pairwise correlations, and you compare them to data, right?",
            "So let me show you how this looks like.",
            "So this is from a public, not our paper.",
            "This one published paper in 2006.",
            "So what is this?",
            "Is the analysis of only 10 neurons, right?",
            "So 10 neurons can produce 2 two to the power of 1024.",
            "Different combinations of spiking and silence.",
            "Here the new rule #1 fires in normal #9 for everyone else is silent for every one of these 1024 configurations there is actually enough data.",
            "In a typical experiment to estimate its probability empirically by counting.",
            "That's on this axis here, right?",
            "So that's a particular pattern whose empirical probability is this much right, and so on.",
            "And now I can ask, how well are these probabilities predicted by models that only neglect correlations and only pay attention to the mean pirate firing rate, which is the blue thing.",
            "So equality line would be here.",
            "So a perfect model, this model predictions?",
            "This is data, so stuff that's predicted well lies on this line.",
            "If you neglect this small correlations, you get the blue dots and the blue dots are terrible in predicting the activity patterns of 10 neurons.",
            "Alright, so this is this is order of magnitude apart.",
            "This is a log scale right?",
            "If however, instead you include the pairwise correlation, so we now have a Maxent model where you tune where you reproduce exactly the mean firing rate of every neuron and the covariance in between every pair of neurons.",
            "You get this Ising like model and you fit it to data.",
            "Then you get the prediction of the activity patterns, which are these red dots here that are lining up almost perfectly on the diagonal.",
            "And here is a small sampling limit because you're counting.",
            "Empirically, right?",
            "So this spread comes from small data size.",
            "So the conclusion was once, despite the fact that every pairwise correlation is small, there is many pairwise correlations together even at the population of 10 neurons they predict.",
            "Collective effects that are much sort of departing from independence by a lot, alright?",
            "And the question we had is how does this scale you know, 10 neurons was an experimental constraint at that time, so you measure more neurons 100 and so on.",
            "How far away?",
            "How important are the collective effects?"
        ],
        [
            "A large population.",
            "So this is what I'm going to present you to you now.",
            "So what we do is we again we have the recording that you saw.",
            "We have a fish movie natural movie clip that we repeat many times 300 times.",
            "Repetition of 22nd Movie Clip record from 100 to 160 very good neurons discretizing this small time bins and now we make this sort of Ising like pairwise models that fit means and covariances and we also make the show the K pairwise models where we add this additional global constraint.",
            "So.",
            "These are the pairwise models.",
            "So basically what you do is you in the raw data.",
            "You now measure the covariance matrix by between 100 neurons, so it's 100 by 100 matrix.",
            "You measure them in firing rate, here sorted of all the 100 neurons.",
            "That's again the distribution of covariance is consistent with what has been reported previously and now from this you compute the Maxent interactions JJ and the magnetic fields hi.",
            "Just to connect with what you probably know better, this is the exact opposite in how the traditional statistical mechanics goes right in the traditional statistical mechanics you assume some form for the interaction and some type of external field, and you're computing the correlations out and the means right?",
            "So this is turning the problem on its head and solving the inverse problem right?",
            "And what you might notice immediately is that if you look at this infer JJ matrix, it has things of both signs about equal proportion.",
            "There is many frustrated triangles in here.",
            "And it's sort of relatively dense, so implying a relatively old tall connectivity functional connectivity.",
            "Promising that perhaps the statistical mechanics models might be interesting if this reminds you of spin glasses, perhaps right with the frustration and so on.",
            "So.",
            "You ask, well, you can ask how well do these models do?",
            "You can mean I won't say anything about technical about how you construct them, but you can check that you are doing a good job of reconstructing how well do they do in predicting something nontrivial."
        ],
        [
            "The thing is, they fail very quickly.",
            "They worked for 10 neurons for a large population.",
            "They don't work any longer, but they fail in a very systematic way that I'm trying to illustrate here.",
            "So this is for a group of 10 neurons where they should work as reported before and they do for 14 runs in the hundred neurons an what's plotted on this plot is the probability that out of 10 neurons, let's say 5 neurons spike in the same little 20 millisecond time bin.",
            "So data is shown in red the maximum.",
            "Ising, like pairwise model, is showing black and for 10 neurons these lines are extremely close to each other.",
            "But you go here 200 neurons.",
            "There is both a deviation in the tail, it's very strong.",
            "There is also actually a deviation here.",
            "This is a log scale, so this is the probability of zero neurons firing, so it's silence, complete silence, every neuron being silent, and there is a large difference right here between the two models where the factor of three between the prediction and the data and these are the most well sampled patterns, right?",
            "They happen most often, as you see here, neurons is being silent, so pairwise models cannot capture this global activity pattern well.",
            "And you know you can.",
            "There is many ways you can try to fix this, and perhaps the most obvious way to fix it, which is what we are going to do and then explore the consequences, is to make a Maxent model that's now consistent with means and pairwise correlations as before, but constrained the model.",
            "Also by this global potential to exactly reproduce also this global activity, right?",
            "So it's like if you want, it's like a global modulatory effect.",
            "Overall, neurons in forcing your model to reproduce in the data.",
            "How many times a neuron spike together or not?",
            "So, and this is what we call the K pairwise K pairwise model, and that's the reconstructed potential that will do the job for the specific data case shown here.",
            "So alright, so this.",
            "Augmented Ising models.",
            "If you want are what we are studying now so well, how well do they do in predicting something knew about the data that was not put into the."
        ],
        [
            "All by hand.",
            "So here is 1 example.",
            "I just saw one statistic so we can measure the three point correlation between triplets of neurons.",
            "So take any three neurons and measure their correlation.",
            "Ask the model to predict that same correlation and for this K pairwise improved models the matches in the red shown in this red red points is not perfect, but it's definitely a very good match.",
            "And it turns out that this match, so the capers models can predict 3 point correlations without without significant bias, and they can do that across.",
            "So this is the error in predicting 3 point correlation across different sizes of the network.",
            "So for zero neurons, 5000 and so on, and this doesn't doesn't grow, which is interesting 'cause you have a model that has more and more 3 point correlations, and it doesn't have the same number of parameters mean the number of parameters grows more slowly, yet you do a good job of predicting this report correlations.",
            "So you can check many other statistics which I'm not going to.",
            "Do in this talk.",
            "Rather ask what do we learn from these models about the neural code?",
            "OK, I won't discuss this so."
        ],
        [
            "First thing that is an interesting suggestion coming from these models concerns that pattern of interaction with frustration, right?",
            "So if that reminds you of the spin class what this could mean is that.",
            "So your model is defined by an energy function, right?",
            "It's a Boltzmann like thing with a Hamiltonian in the exponent, and that energy function and energy overall patterns could exhibit many local minima if you want because of frustration and there has been an idea in neural coding existing for quite awhile that what is actually important for coding is not the detailed microscopic pattern of activity over all these neurons.",
            "After all, neurons are noisy, so if you repeat the same stimulus again, a neuron might be silent or might be spiking.",
            "With some probability right, you won't get exactly the same patterns, so it cannot be that exact.",
            "Microscopic patterns encode the stimulus, but what else could it be?",
            "Well, one suggestion was that what it could be is it's not a microscopic pattern that's important, but what basin of attraction?",
            "If you want so?",
            "If this is my energy function, there is a particular pattern sitting here, and there is a lot of other microscopic states that all belong to the same basic energy basin in this landscape.",
            "That what matters for stimulus encoding is simply what base in your in, not the detailed microscopic state.",
            "So that would be the microscopic pattern that has the locally lowest energy.",
            "And all of these other patterns.",
            "You can just go downhill by flipping the spins until you hit this bottom bottom thing.",
            "So can we identify such basins in the energy functions of the models we reconstructed?",
            "Indeed, we can.",
            "Here is the number of the basins we find as the size of the neural network grows larger, so this is for models for 40 neurons, 8000.",
            "20 This is the number of such metastable states that we identify.",
            "So for the largest network, we find several hundreds of such basins if you want.",
            "This is just to give you an example how patterns belonging to this basis look like.",
            "So what's shown here?",
            "These are neurons.",
            "These are now collected microscopic patterns that all belong to base in number one.",
            "I just call it number one, it's the base, and we're actually no neuron is active, so it's the silent state.",
            "What you see is most stuff being silent here and there.",
            "There is a little spike.",
            "These are all microscopic states belonging to another base in that basin is defined by a pattern here that has this guy active.",
            "This guy active this, this and this active an indeed all microscopic patterns belonging to this basin have these neurons.",
            "Active, but here and there there is other spikes alright.",
            "So basically this.",
            "This paradigm is now partitioning the space of two to the N patterns into basins of attraction.",
            "And what one is saying is that perhaps the only thing that matters is that you know these guys code for something and all of these guys code for something else.",
            "So all of the patterns belonging to this basis right?"
        ],
        [
            "The patterns cluster if you order them by which basically they go in the similarity matrix, you know this place is nice clustering.",
            "I'll skip that.",
            "Maybe I pay attention to this plot here.",
            "So what is shown here is now.",
            "Going to the data, taking the actual microscopic state of the retina at every point in time and assigning every microscopic state to each own basin of attraction.",
            "Alright, and then 'cause I repeat the stimulus.",
            "It's exactly the same stimulus repeated 300 times.",
            "I can ask for every time point in the stimulus in what besen across repeats, what the probability of the retina being in the blue base it right or in the Yellow basin?",
            "Or something like that.",
            "And what you see is this picture where the retina here is in.",
            "This is blue is the silent base, and so nothing happens.",
            "And then rather reproducibly across repeats, it transitions into this light blue thing, and it goes into the green thing and again a green into red and so on, right?",
            "And what's important here is that the model didn't assume anything about the repeated stimulus statistics or anything, right?",
            "So basically what emerges from this analysis is that across stimulus repeats, the retina is state is well described instead of by these two to the end bit microscopic pattern.",
            "Simply, in what base in it is in and how it transitions from basin to basin, right?",
            "So this is not.",
            "This is not evidence that the retina is using this basic code, but it is a strong suggestion.",
            "That's an interesting line of research and.",
            "This has been suggested in theoretical models in neuroscience, but it was never their theory that was hard to connect to data, right?",
            "So this is now the same type of model that was used in this theoretical work, but since it's inferred from data, you can try to make statements that maybe some of these ideas actually make sense of sort of global states of activity coding for the stimulus."
        ],
        [
            "What we can also do is we can compute the entropy of these code, right?",
            "So if you know a little bit about information theory, the entropy of these code words, they come from the retina puts an upper limit on how much information you can push through the retina, right?",
            "We can compute this in various ways.",
            "And I'm just showing this here, so what's shown here is the entropy per neuron in red in red dots as the number of us, my size of the neural network grows.",
            "What you see is this interesting thing that you might not be.",
            "You know it's a regime where it's not that we're not at home in statistical physics where where the entropy per neuron is actually still decreasing.",
            "It's not extensive as I take more and more neurons.",
            "That's big, cause the system is strongly coupled and neurons have wires leading between.",
            "Each other, they're not just nearest neighbors, and when I take 120 neurons, I'm not yet in the regime where I'm out of the real circuit.",
            "Connectivity alright.",
            "If you extrapolate you hit extensivity about about 2 to 300 neurons, which is actually consistent with the known Physiology in the salamander.",
            "This is the distance.",
            "This is the Patch of the retina that's densely connected.",
            "If you go out of it then the wires don't reach any longer across those distances, right?",
            "So that's the thing here, and these absolute numbers actually have a meaning for people who do neural coding in putting a limit on how many bits per second you can squeeze through the retina."
        ],
        [
            "One of the last part results is now going back to this introduction.",
            "You will remember that the dominant thought in neural coding, especially in the retina, has been this notion of decorrelation right?",
            "I said there is a picture of what the retina does.",
            "It filters the stimulus and it does it in such a way that the outputs of the neurons are as decorrelated as possible.",
            "Alright, so clearly the results up to now have demonstrated the neurons are not independent.",
            "I mean, you know the independence is a very bad model, so the correlation remains, but can you explicitly demonstrate this correlated nature of activity?",
            "And you can do that now that you have a model for the joint activity of all the neurons by doing the following experiment, so you pretend you don't know the state of neuron one, but you know the states of neuron from the data.",
            "You know what the neurons do?",
            "You know two to the 100 are doing and you can use your model to predict what the neuron one should be doing because it's coupled to everyone, right?",
            "So of course, in the data you know what the neuron one is doing, and so since I have many repeats of the same movie, this is the firing rate.",
            "So this is the probability of the neuron spiking at every instant in time.",
            "It does this crazy thing of quiet spiking and so on, so that's the data from my neuron number one, and now I can ask, can I predict this neuron number 1 from 9 other neighbors in the network so from a network of a total size of 10?",
            "I don't do a very good job.",
            "If I take 20 neurons, I do a little bit better, but still actually quite crappy.",
            "But once I go up to the network of 120 neurons.",
            "I can predict the activity of this withheld neuron with very good cross correlation.",
            "If you want right.",
            "I can do this across many neurons and maybe this is the summary plot here.",
            "When I go up to 120 neural networks, on average, I can predict the activity of.",
            "An arbitrary neuron that I exclude with the cross relation about 80% so 80%.",
            "Just this is knowing nothing about the stimulus.",
            "This is just knowing on average what other guys are doing.",
            "And this is telling me something about what my neuron is doing.",
            "Alright, so this is an explicit demonstration that these networks are very, very far from producing independent outputs.",
            "Actually outputs are so dependent you can predict the activity of one guy very well from the activity of the rest.",
            "Now you know what doesn't fit right?",
            "What doesn't fit with the existing theories that existing theory is true in the limit of low noise in your in your network.",
            "So if there were zero noise in the normal response, the neurons should indeed be correlate.",
            "However, if each element is noisy, you don't want the correlation.",
            "You don't want every neuron to say something independent of the others, because if that moron makes a mistake, there is no way to correct that mistake, right?",
            "Just a mistake that propagates the brain.",
            "So it's very much like an engineer.",
            "It codes when you design a code to pass through a noisy channel.",
            "What you do is you put in artificial redundancy in the form of error correcting bits or so on, right?",
            "That's redundant information because it helps you correct the errors.",
            "So in this case you can demonstrate that if you know what the other neurons are doing, you can predict what any withheld neuron is doing fairly well, right?",
            "And this turns out if you delve into the theory a bit more deeply, this is the optimal kind of thing you would do if your network units are not noise free.",
            "If they're noisy, right?",
            "So you should keep some amount of redundancy in the network too.",
            "To error correct."
        ],
        [
            "Alright.",
            "OK, and now I'll try to go towards this hint of criticality.",
            "I tried to finish in about 10 minutes.",
            "So we have this.",
            "So we have the data.",
            "We have models of the joint activity of the neurons that we think are are very good.",
            "I'd like to emphasize that this has not been done right, the model.",
            "So this detailed level 400 plus neurons have not been actually constructed, so generative models have not been constructed until very recently.",
            "What we would like to ask now is whether these distributions that we have constructed in some ways special or in particular, are they close to critical.",
            "And that turns out to be much more complicated than you would think, right?",
            "So if you have a real physical system, a thermodynamic system, then the claim is let's say it's close to the critical point.",
            "Well, what kind of analysis do we do?",
            "Let's say you can look at the behavior of correlation functions right?",
            "Do they diverge when you go close to the critical point?",
            "Well, here it's hard cause our system is all too all connected.",
            "So the notion of distance doesn't doesn't really fit at this scale.",
            "The other thing that you could do, let's say in the in the normal system, is.",
            "You could make perturbation to some external parameter that couples to the you know something.",
            "So you basically try to manipulate the order parameter of the system, right?",
            "You change magnetic field or so you look at the susceptibility in our case.",
            "And in general, in the case of inverse models, the problem is even how to identify relevant order parameters?",
            "This is very specific construction, right?",
            "It's not clear what these are, right.",
            "What we can do in?",
            "I'll try to show is you can right there is a mess in if you go back to mathematical physics there is a strong link between statistical.",
            "So basically statistical physics is a statement about probability distributions.",
            "Right?",
            "And then of course you can kind of.",
            "Implement external knobs that you twinkle and so on, but actually criticality can be discussed without doing the without.",
            "Doing that and I'll try to show some of these things here."
        ],
        [
            "So the first thing that we do is we compute the microcanonical entropy of our distribution.",
            "That's that's actually very simple.",
            "We have a model that can you give me a microscopic state and the model predicts the energy of that state, right?",
            "So the only thing I need to do is I need to count how many states there are with the given energy E and the log of that.",
            "If you want, is the microcanonical entropy SOV.",
            "And then I can look for this quantity to identify whether there is something like a critical point in my system, right?",
            "So the second derivative of this entropy with respect to the energy?",
            "I can construct this curve for the using very recent.",
            "Very nice sampling techniques like one Clan downsampling.",
            "You can take him all that we have an.",
            "You can construct the curve of entropy versus energy, so energy per neuron, entropy per neuron here and these curves are constructed for blue groups of 20 neurons.",
            "Groups of 40 neurons etc etc up to the group of 100 out of groups of 120 neurons in the red which are here.",
            "And what's interesting is that there is a very natural extrapolation towards higher and higher groups, so you see that when I increase the size of my network, I go this way right.",
            "And you can just make this formal.",
            "You can extrapolate to infinite size of the network using this construction.",
            "And the extrapolated points for the entropy versus energy are these black things that go like that.",
            "And what you see is in this low energy region.",
            "They all line up apart from this guy here on a straight line.",
            "Which is interesting 'cause the straight line were actually quality of entropy and energy means that this condition here is actually satisfied, not to the given point, but the whole low energy range, right?",
            "So this suggests a very.",
            "Critical behavior in which in which this curve is flat across this whole low energy range of patterns, and the interesting thing is that here you might object while it's a statement about the model.",
            "It's not really statement.",
            "I mean we went this complicated way.",
            "We took the data, we constructed Maxon model, now we studied the model or the model does that that.",
            "But you know what happens in real data.",
            "The nice thing is you can reproduce exactly this thing directly from the data, right?",
            "You can take patterns.",
            "You can sample some of them actually very well, not for a small fraction of them.",
            "But you can and then the energy of those is just the log probability that you sample and you can ask how many of them there are.",
            "So this is now the same type of thing constructed from raw data alone for different size of the networks.",
            "And of course, because the data is fine at your running into a sampling sampling limit, the higher the size of the network, the smaller part of this curve you can see because of the sampling limit.",
            "So here is 420 neurons, but still you can take this extrapolation to angos large and you see exactly the same thing.",
            "Extrapolated points in the data line up on the equality line exactly as they do in the model, right?",
            "So this suggests that so, so we don't think that's an artifact of fitting a Maxent model.",
            "You can redo this directly from the data, and it suggests a very peculiar critical behavior where this thing is zero across actually a whole range of you, not just at the any given point.",
            "That is equivalent to a if you know what the zipf law is.",
            "If you plot the zip flop for these patterns, both from the model and the data, they will fall on a very nice minus one log log slope.",
            "I won't say more in detail, but if you have a question you can ask."
        ],
        [
            "About that, so that was one signature of criticality.",
            "I'll present another construction.",
            "To examine the kinds of models we have.",
            "So remember, I inferred from data model of the following form, right?",
            "There was this Hamiltonian which matched the constraints in the data, but there was no notion in maximum entropy model.",
            "There is no notion of temperature like in thermodynamics.",
            "However, mathematically I can introduce this temperature parameter and view it simply as a scaling parameter of the Hamiltonian, right?",
            "So for now, look at this as a mathematical trick if you want.",
            "This generates a family of distributions parameterized by this parameter.",
            "And for each such distribution, for each value of TI, can compute if you want to hit capacity like I do in statistical physics.",
            "And this is what I see.",
            "So this is the parameter T that I tune T equal 1 is the real model inferred from the data right?",
            "Equal 1 is exactly the stuff that I learned from the data going away from one.",
            "Are these mathematical constructions that don't match any data and these are heat capacity curves for 20 neurons, 4020, forty, 80 and 120 neurons.",
            "And what you see is that the larger the size of the network the speak of heat capacity is actually first of all, it's as I'll show it.",
            "Bing faster than linear with North, but importantly is moving towards the equal 1.",
            "So this means that the larger in the limit when I'm taking a large network of neurons, the peak of this, the peak of the heat capacity which indicates some incipient.",
            "If you want criticality will lie exactly on the line where my models describing the data are right.",
            "So if I do the same thing but normalize heat capacity per neuron, you actually see that this quantity per neuron is growing faster than linear.",
            "Right, so it's not intensive right?",
            "And the P goes closer to."
        ],
        [
            "On which you can, you can quantify.",
            "So this is how the peak is approaching one.",
            "This is how quickly the heat capacity per neuron is growing.",
            "This is another signature if you want of of emerging criticality in this system."
        ],
        [
            "And you can do other constructions which interest of time.",
            "I will skip.",
            "Happy to discuss it if there is any."
        ],
        [
            "Question there is other checks that you can do different sort of analysis and maybe just."
        ],
        [
            "Include on, you know with a few words so.",
            "You know?",
            "Critical so alright, so the distribution of code words coming from the retina is special in a way that looks like a critical system in statistical physics.",
            "You know why is this good?",
            "We don't know.",
            "There is suggestions that maybe this is not a fact about threatening at all.",
            "Maybe it has something to do with our fitting of a model, right?",
            "That was a suggestion by Martha Marcy and Master Mateo this week and this week and check to show that this is not the case according to their own specifications.",
            "There has been another suggestion by recent by David Robillard, Eminem, Pankaj Mehta, that perhaps the criticality is somehow an automatic consequence of the fact that you are taking a statistical physics model that looks like an equilibrium thing and you're applying it to describe the behavior of a driven system.",
            "The retina is driven system, right stimulus is driving it all the time.",
            "The answer to this is maybe there is some truth to it, but actually it's not so clear cut as this publication was suggesting.",
            "Because we have some tuning knobs in the retina, unfortunate there are not as obvious ones as in physics, temperature and so on, but we can change the stimulus ensembles for instance from more to less correlated and so on.",
            "And they don't exactly match what this is supposed to be doing.",
            "This suggestion is supposed to be saying.",
            "So we're not sure about this point.",
            "We don't think it fully explains it, but there is some truth to this.",
            "Perhaps now, maybe criticality has a functional significance.",
            "So what could that be?",
            "First of all.",
            "The being having a code that sits at the peak of heat capacity.",
            "What is heat capacity?",
            "Heat capacity?",
            "Just a variance in lock probability?",
            "Alright, I mean just the quality right?",
            "So it means that if you are there you are maximizing in your distribution.",
            "The variance of log probabilities, which means that in your code book, if you want in your in your dictionary you have code words that come with the widest possible range of probabilities, right from something that happens very often to something that happens extremely rarely, right?",
            "So if you want to encode.",
            "Outside stimuli which people are thinking are happening, you know the stuff in the outside world is happening with the structures that have these long tails.",
            "If you want to encode them fast, meaning something happens and you pick up a code, work from code book for code Word from the dictionary and throw it out, then you have a code that matches that broad range of stuff happening outside.",
            "That's actually the inverse of what an optimal code like a zip compression algorithm would do.",
            "Right zip compression algorithm would have stuff that he has a huge range of probability putting together into blocks in a smart way such that what comes out is.",
            "Of equal probability, but that takes time, right?",
            "You need to block code right?",
            "So it has a delay, which is not something very smart for the retina.",
            "There is stores that this is a side effect of the fact that in the retina we have very long time scale dynamics of adaptation and so on.",
            "It could be a side effect of maximizing other things such as the system wanting to maximize information transmission, and of course this is what we are currently studying, which is, you know, forget what is the reason for the mechanistic reason for this distribution.",
            "But that's the code words that go into the brain.",
            "So the brain is sampling is getting signals that come from this distribution.",
            "And the brain remember, has to learn it right while it grows up.",
            "The brain doesn't have a code book that says whenever I get 101010, that's a dog, right?",
            "It has to infer this right now.",
            "There could be distributions of code words coming from the retina into the brain that are presumably rich in information, but impossible to learn, which is what you would get if you take what the writing assessment put it through.",
            "A mathematical encryption algorithm, right?",
            "You know the brain can learn, can look at this code, works for 10 years and not make any sense of this, right?",
            "So we're actually asking, we think an interesting mathematical question, which is what is the mathematical inverse of encryption schemes?",
            "What are the most obvious schemes that you can have, given that the code words can be noisy, right?",
            "If there are noises not a problem, but if there are noisy you should.",
            "There should be some correlation, but you should be very quick and the brain should be very quick.",
            "And figuring out what it is because you only get some finite amount of samples and you have to wire the brain up correctly."
        ],
        [
            "Alright.",
            "So I want it distorts as you see there is still a lot of open space here for research.",
            "I would like to conclude.",
            "I hope I could show that this inverse statistical mechanics using maximum entropy is actually a very powerful tool for studying.",
            "Systems in biology, neuroscience where we observe the states of many elements at the same time and the data in that direction is now exploding.",
            "It used to be very hard to obtain simultaneous recordings of the states of many genes.",
            "Many neurons, many animals and so on at the same time.",
            "Now we have it.",
            "But now the question is what we do with it, right?",
            "There is all these data, all this money that's going to flow into various brain projects, some of which are smart, some of which are frankly not.",
            "But you know, on the experimental side, the push will be there at some point.",
            "We will have joint recordings of 100,000 neurons right of what they do together in a zebrafish.",
            "So, well, you know, how do you extract what the brain is doing from those things?",
            "One suggestion is this sort of methodology is not the only one.",
            "Of course, what we found concretely on the retina are some interesting coding properties.",
            "That is that despite small correlations that you measure the behavior of the neurons is strongly collective.",
            "It leads to these metastable states of activity.",
            "The entropy still scaling non extensively.",
            "We think that this redundancy, so the fact that these are not independent can be used for error correction.",
            "There is some stuff about coincidence probability I didn't manage to talk about.",
            "In particular, we also find that these output code word and samples are close to critical in various measures, some of which I didn't and end up explaining.",
            "One thing I maybe I skip this so the crucial question for us is, is this system the retina tuned to or dynamically adapting to some sort of a critical point, or is this result somehow generic?",
            "And what I think is sorry, and what I think is nice.",
            "Just last concluding thought is that this question can be experimentally addressed, right?",
            "An let me give you a. Alright, forget about it.",
            "Let me give you a one sentence summary how you have your retina sitting on the array exposed to some stimulus, like this one.",
            "A natural stimulus.",
            "Right now, many repeats.",
            "You do exactly the same analysis here and now you suddenly switch the stimulus to some very different statistic, right noise?",
            "Something that has very different different stimulus structure.",
            "Immediately after the switch, the retina is not adapted to the to the new stimulus, and so the thinking is that if you construct the model immediately after the switch, it will actually shift away from critical for some onsan on some time scale, while the retina is re adapting itself to the new stimulus, it will slide back to the critical, right?",
            "That's an experiment you can do, and we're actually doing right, and if you observe this.",
            "Move right of being in adaptive state, which is close to critical.",
            "Once you perturb the retina by changing its stimulus, quickly goes away and then it returns back to critical.",
            "We would also feel much more strongly that you know there is a real meaning to this tuning to a critical point, right and somehow beneficial for coding, so that's what was taking place in the lab right now.",
            "Alright, so with this I'd like to conclude and maybe we have few minutes for questions.",
            "Thank you.",
            "Thank you very much for a nice talk and I would be very quickly question and starting from the from the end when you say the system somehow see it's always at a critical point.",
            "And evidently there are criticality in there, are there are attempts to model dynamics because actually this is dynamical driven system and nonlinear and so on and correlated.",
            "Yes, like in in the sense of self organization and this kind of things.",
            "Where are actually?",
            "You should not just know dynamics.",
            "Which of neurons will also the model interactions, right?",
            "I mean, you didn't mention that point of view going on the on the other level and that brings me to.",
            "Your second part over you did describe the model.",
            "When you get the model from interpreting data, why always you're talking about full graph in terms of of network?",
            "OK, all to all, in this dynamical model you have to actually put exactly how do they interact and is it full graph structure reliable in this sense?",
            "OK, alright, so regarding the dynamics, that's a great question, was not part of this talk because of time constraints, but.",
            "So you can.",
            "You can try to model dynamics in various ways.",
            "You can actually model it in exactly the same framework, right?",
            "So we have now modeled.",
            "You know, we have imposed constraints within a single time, been right on the statistics of what the neurons do together.",
            "But of course nothing prevents you from doing this across time bins.",
            "So you say you know when a time T neuron I spikes, what's the likelihood that the T + 1 than orange spikes, right?",
            "And that will give you directional pairwise couplings across the time bins.",
            "This has been done by us and by others.",
            "And of course it improves so it improves stuff in terms of the match to data etc etc.",
            "It's just harder to do.",
            "I mean technically it's harder to do.",
            "Yeah, and so in the next question relates then to, you know we reconstruct the full connectivity graph right?",
            "And how does that fit with?",
            "That's a hard right, so that's a hard question.",
            "Becausw, we are modeling the output distribution of the retina, which is a consequence of two things.",
            "One is the Y receiver in the actual retina, and the other is the fact that the stimulus itself, which we don't explicitly model at all, is driving certain units strongly in parallel, right?",
            "So, at the output, you cannot know whether these two units are connected, because there is a wire going in between them, or because the same stimulus is doing this to both units, right?",
            "You can again extend this framework to actually disentangle these two things by putting in explicitly the stimulus and factoring out whatever can be explained by the common stimulus, and then what remains is supposedly just the so-called noise correlations.",
            "The wires in the retina, so this can also be done.",
            "Unfortunately, that is technically so difficult, we can only do it for our network of several 10s, where maybe 20 neurons, and then it's much less clear.",
            "This sort of scaling that we have to wait for the new experiment to 200 plus.",
            "Which would like to do in that setup is for now out of reach.",
            "From what we can do, right?",
            "So that we this would be great, right?",
            "If we did large network both with stimulus and the.",
            "I think we're getting there, but it will take some time.",
            "This will be just the question which is not related so much to actual evidence I had, but it just came to my mind.",
            "Is there any photo receptors on the ganglion or or brain site?",
            "Not that I know of.",
            "No, I didn't know about that.",
            "But what about holography?",
            "How does that refer to what you're talking about holography in the in the holographic images of something which is so so, so?",
            "I mean, there are two meanings right there.",
            "Is the holography as as as of people invoke it in or science as you can reconstruct?",
            "I mean that somehow information is encoded dispersedly across many parts of the brain.",
            "Or you mean the actual holography?",
            "How you produce by interference will produce the images.",
            "Well, I don't.",
            "I don't think there is any any direct link from this to that, right?",
            "I mean, that's at least not not an obvious one.",
            "I have a car hardware question.",
            "Could you show the sensor at this electrodes that you had showed?",
            "Yeah.",
            "Yeah, I must mention that Interestingly enough, the other group that's yeah.",
            "Or maybe you should go ahead with the question.",
            "I mean, yeah sorry right now, let me understand.",
            "So what you're saying is that you're on the electrode site.",
            "You cover the whole exit from the retina, so that basically, if I understood correctly, basically each output from the retina is covered by at least one electrode.",
            "The I have to correct it there so.",
            "The retina is this big.",
            "You cover a little piece of it, but within that little piece your statement is correct.",
            "Every neuron is seen by many elect by about 9 electrodes in this case.",
            "So how do you disentangle this before you start analyzing this code?",
            "I think, yeah, that's one of the hardest problems in data processing that neuroscience has, which is the so-called spike sorting problem.",
            "So what this means is when a neuron sitting on top of the array makes a spike, you see the echo of that spike on about 6:00 to 9:00 electrodes.",
            "And of course, each electrode listens into more than one neuron, right?",
            "So what the way you do this is you make use of the fact that the spikes that neurons make are extremely stereotyped in shape, and so normal number one is making a spike of a specific shape that you see on the given electrode.",
            "And that's because you know, alright, so the normal itself makes this pipe, but then the signal actually propagates across some distance in some medium to this electrode.",
            "So every time these neuron spikes, it makes you record the shape that slightly different from when these other neuron spikes.",
            "But talks to the same electrode, right?",
            "So now you have a problem where at each point in time there is a voltage trace on your electrode and you want to represent that voltage observed voltage trace as a combination as a linear sum of voltage signals coming from putatively.",
            "Whether this guys pack this guy, this guy or this guys, we have a computer problem right for every point in time there is, you know either none of them the best expressions, nobody fired.",
            "Then you just have noise or this guy fire.",
            "Then you see it's echo and so on.",
            "Right, so so it's a rather computationally intensive inverse problem where you go through all the voltage signals and.",
            "Basically, you do if you want a maximum likelihood fitting at every point in time.",
            "What is the most likely combination of neurons that spike to give rise to the electronic voltage signals?",
            "I see on the on the array?",
            "And the really hard part is when these things overlap, right?",
            "So neurons like to spike in synchrony.",
            "So that means that the exactly those combinatorial events that we are studying you have to get correct, right?",
            "'cause otherwise you get an interesting comment I had is that the second group that manufactured disarray was.",
            "This was a known collaboration with high energy experimental physicists.",
            "Actually.",
            "So Littke was experimental physicist at I think at certain.",
            "Actually, who then went to neuroscience to design one of these things, right?",
            "Kind of an interesting link.",
            "Daniel dennett in his book on Free will.",
            "Poses a, in my opinion, the first realistic model of.",
            "Forming some decision in the neural system from scratch.",
            "Because all the all such models that we read about from classical psychology were always implying some little man in a box somewhere.",
            "It really is hard to to to formulate a model for an autonomic decision to come out of the system.",
            "Well, he he saw the kind of.",
            "Growth of some, some avatars or whatever.",
            "He says that that do some darbinyan battle and one comes out and he's the bearer of the decision I see.",
            "There is some connection in in in your model with this flat regions with many local minima.",
            "This is a kind of.",
            "Physical basis which could.",
            "Somehow leads to this.",
            "Then it's avatars yes.",
            "So so to give a comment, I think the link comes from the fact that.",
            "So this this, if they exist, right?",
            "These basins of attraction in this energy landscape, of course, is not.",
            "I mean, this is not some random thing, right?",
            "The thinking is as follows, right?",
            "You have a system that has both evolved, which implies a certain type of wiring in the system.",
            "But it has also learned like the brain learns by exposure to natural stimuli to the kind of environment that it grows in.",
            "So it is.",
            "It has been exposed to and it has evolved in response to certain statistics.",
            "And that's the link to studying the natural statistics, those statistics.",
            "Shape.",
            "This landscape, the bottoms of the landscape.",
            "Our hypothesis about what could have possibly happened.",
            "You know the stuff that could have possibly happened outside and now the computation in this physical senses then viewed as some ambiguous stimulus comes, and what the stimulus that does it takes.",
            "This landscape of possibilities of prior landscape of possibilities and tilted by evidence in this or that way, to put the system into this or that attractor right?",
            "And it basically I mean we know that the outside world is very high dimensional if you look at images, right?",
            "This is a.",
            "My number of pixels on its high dimension is noisy, but actually the number of possible things happening outside.",
            "They still much less than the full dimension of all the pixels and light intensities and so on, right?",
            "So you want to somehow have a system that says, well, when I see this snapshot of photons, you know that's a dog, right?",
            "And that comes at me now simplifying, but that's what you want the system to do, right?",
            "And on the longtime skeleton evolution is shaping this landscape and then the short term dynamics is putting your ball in the correct in the correct attractor.",
            "So that in that case there is this link.",
            "The interesting thing is that this link has now neuroscience data driven things from neuroscience have started to converge with people working in machine learning, right?",
            "And people in machine learning try to figure out well, given some very rich data set in an unsupervised way.",
            "So when you don't have an answer, you don't have a classification problem, we just get this very rich data set that you want to model.",
            "How do you do it right?",
            "And if there is anyone from machine learning in this Community, of course inverse Ising models are the Boltzmann machines of machine learning, right?",
            "So they call it differently.",
            "There is different tools, but.",
            "Actually, it's exactly the same thing, right?",
            "Seems to me that there are no further questions, so let's thank prosodic again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's my pleasure to introduce, introduce Professor Gaspard Kacik.",
                    "label": 0
                },
                {
                    "sent": "He got his Bachelor of Science in Physics from University of Ljubljana in 2001 and then a PhD in physics from Princeton University.",
                    "label": 0
                },
                {
                    "sent": "Six years later he was a post doctoral fellow at Princeton University in 2007 and then moved to University of Pennsylvania for another postdoc.",
                    "label": 0
                },
                {
                    "sent": "Currently he is assistant professor at the Institute of Science and Technology, Austria.",
                    "label": 0
                },
                {
                    "sent": "That's since 2011.",
                    "label": 0
                },
                {
                    "sent": "His main research topics are bio physics and neuroscience, especially gene regulatory networks, neural coding.",
                    "label": 0
                },
                {
                    "sent": "Natural scenes.",
                    "label": 0
                },
                {
                    "sent": "Within the context of efficient coding hypothesis and collect the dynamics.",
                    "label": 0
                },
                {
                    "sent": "He's coauthor of 34 papers, seven of which have been published in Proceedings of National Academy of Sciences of the United States of America has also engaged in some editorial, an organization work.",
                    "label": 0
                },
                {
                    "sent": "His editor of physical biology, and he organized the conference sensory Coding and natural Environment in Vienna for his work.",
                    "label": 0
                },
                {
                    "sent": "He's received many grants and awards, and today he's going to tell us.",
                    "label": 0
                },
                {
                    "sent": "Something about critical behavior in networks of real neurons.",
                    "label": 1
                },
                {
                    "sent": "Below happy to be out.",
                    "label": 0
                },
                {
                    "sent": "There is some super while a premature as the emotions developed.",
                    "label": 0
                },
                {
                    "sent": "It's down at the low.",
                    "label": 0
                },
                {
                    "sent": "Car beer at here picazo.",
                    "label": 0
                },
                {
                    "sent": "Because of it, I put the monster, but not a not a touch interaction with physical and Bellagio, the knee, Vietnam Premier, some Utada, blog post, dollar operation, yeah Physically Patch statistic receiver or audia in Annalisa.",
                    "label": 0
                },
                {
                    "sent": "Numerical analysts predict Conan Alicia and work the solve.",
                    "label": 0
                },
                {
                    "sent": "Leave Oprah, Sonia physical illness and negotiations.",
                    "label": 0
                },
                {
                    "sent": "Which aspect of physical and mental pressure is to put in voted to trigger the Serb mature paradigma?",
                    "label": 0
                },
                {
                    "sent": "Bernie Houghton Co. Life Sciences.",
                    "label": 0
                },
                {
                    "sent": "Resuming Staticky Harnish today to open the Mucha.",
                    "label": 0
                },
                {
                    "sent": "Nikki Otega load Sheet notices is present at yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe now it's a good time to for me to switch into English before going on.",
                    "label": 0
                },
                {
                    "sent": "I'd like to acknowledge our collaborators olyvia marred area modern Michael Berry, where the experimental colleagues on this work, Princeton University and Olivia has moved to Paris now anthurium, Orion.",
                    "label": 0
                },
                {
                    "sent": "Bill Bialek, our theoretical collaborators Bill also used to be my PhD advisor.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to be talking about today is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This that's actually not just a schematic diagram, it's a real piece of data.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Visualization of a neural code time in this picture goes this way, every single row, for instance, this one represents the activity of 1 neuron.",
                    "label": 0
                },
                {
                    "sent": "Where you see this a box of nothing happening silences so neuron produces no activity.",
                    "label": 0
                },
                {
                    "sent": "And then this this sharp lines represent neural spikes.",
                    "label": 0
                },
                {
                    "sent": "So unitary events that are used in the nervous system to transmit information.",
                    "label": 0
                },
                {
                    "sent": "I'll say more about the details of the experimental system, but what I want to convey is that this is the kind of data that you can get.",
                    "label": 0
                },
                {
                    "sent": "From various nervous tissues, this is the data that encodes the information, for instance, of visual information that goes from your eye to the brain and the questions we would like to ask.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This sort of data are as follows.",
                    "label": 0
                },
                {
                    "sent": "First of all, can we make sense of this?",
                    "label": 1
                },
                {
                    "sent": "If this is really a, can we make sense of this joint patterns of activity?",
                    "label": 0
                },
                {
                    "sent": "So these are many neurons across time and we would like to somehow understand these patterns of activity.",
                    "label": 0
                },
                {
                    "sent": "Really, at the single spike level, without averaging, without computing, firing rates, etc.",
                    "label": 0
                },
                {
                    "sent": "If we really understood, really think of this as a code that conveys information, maybe even encodes a stimulus from the outside world.",
                    "label": 1
                },
                {
                    "sent": "Then how can we figure out what this code is?",
                    "label": 0
                },
                {
                    "sent": "Can we ultimately decode this code?",
                    "label": 1
                },
                {
                    "sent": "So looking at the spikes, can we say what the stimulus was that induced this spikes?",
                    "label": 0
                },
                {
                    "sent": "You know basic questions about the code are.",
                    "label": 0
                },
                {
                    "sent": "Is this code independent?",
                    "label": 0
                },
                {
                    "sent": "By what I mean is that every neuron is is conveying its own bit of information about the stimulus, like a CCD camera.",
                    "label": 0
                },
                {
                    "sent": "Every pixel is conveying an independent piece of information about what's going on in the outside world, in a little part of the Sky.",
                    "label": 0
                },
                {
                    "sent": "Or is it in some sense combinatorial, which means that to understand what this spike of neuron 37 means, I actually need to know what the neuron 35 did at the same time, right?",
                    "label": 0
                },
                {
                    "sent": "So it is not somehow?",
                    "label": 0
                },
                {
                    "sent": "You know I cannot break up the code in sort of individual lines that I can understand one by one.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I won't say much about this, but it's actually a very interesting historical remark, so there has been a lot of theoretical work on neural coding specifically coming from statistical mechanics, community Hopfield networks, associative memory, etc etc.",
                    "label": 0
                },
                {
                    "sent": "These are all very very nice theoretical ideas, But the problem has always been how to connect those ideas to data, right?",
                    "label": 1
                },
                {
                    "sent": "'cause they are statements about collective activity of many neurons, and yet in every single experiment you don't measure from all the neurons at the same time.",
                    "label": 0
                },
                {
                    "sent": "How can we ever test whether a piece of neural tissue is actually at, say, implementing one of these theoretical ideas or not?",
                    "label": 0
                },
                {
                    "sent": "Some of the work that we are doing is trying to make this connection.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the overview of the talk is as follows.",
                    "label": 0
                },
                {
                    "sent": "I'll present the data that we're so this is a data driven research will present the data that we work on.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce this concept of sort of inverse statistical physics models.",
                    "label": 1
                },
                {
                    "sent": "Maximum entropy models for these joint activity patterns of many neurons.",
                    "label": 1
                },
                {
                    "sent": "I'll argue that these models are good descriptions of the data and then we study the behavior of this inferred models and really compare with some predictions with some signatures that we find in the data.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me let me now introduce the retina.",
                    "label": 0
                },
                {
                    "sent": "That's the system that we are going to study.",
                    "label": 0
                },
                {
                    "sent": "So writing as the little piece of nervous tissue in your eye that transduces light coming in to excite photo receptors here so it transduces light into electrical spiking activity of these output layer.",
                    "label": 0
                },
                {
                    "sent": "And once called the retinal ganglion cells.",
                    "label": 0
                },
                {
                    "sent": "These are the things that send the lines to the brain, right?",
                    "label": 0
                },
                {
                    "sent": "So everything that you know about the visual world comes from the joint activity patterns of this layer and those are the spikes that you saw on the 1st slide.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's a stereo type piece of planar tissue.",
                    "label": 1
                },
                {
                    "sent": "That's what make you know what makes it nice for experiment.",
                    "label": 0
                },
                {
                    "sent": "You can take the retina out of the eye of the animal and put it down on the microelectrode recording array.",
                    "label": 0
                },
                {
                    "sent": "I'll show it in the next slide, and then you can display any light stimulus you want to this device.",
                    "label": 0
                },
                {
                    "sent": "You can repeat it many times, exactly the same stimulus you can keep it alive for several hours, Meanwhile recording the output activities of these neurons.",
                    "label": 0
                },
                {
                    "sent": "And so it's one of the.",
                    "label": 0
                },
                {
                    "sent": "It's one of the paradigmatic systems in which, in neuroscience you study the input output relation.",
                    "label": 1
                },
                {
                    "sent": "So how input stimuli are encoded in the output activity patterns of the neurons, with the advantage that the input is perfectly controlled because it slide, right?",
                    "label": 0
                },
                {
                    "sent": "So you can use just your screen to project any sort of stimulus you want onto this retina.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and in particular in neural coding, it's one instance where we really ask about populations.",
                    "label": 0
                },
                {
                    "sent": "What I try to motivate in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So how do many of these neurons together represent the stimulus, not how do they do it?",
                    "label": 0
                },
                {
                    "sent": "One run at the time?",
                    "label": 0
                },
                {
                    "sent": "You might say, wait, well, you know we know stuff about the retina, so isn't it just like a camera?",
                    "label": 0
                },
                {
                    "sent": "So biological camera with lots of pixels, right?",
                    "label": 0
                },
                {
                    "sent": "Tiling the Sky, each pixel?",
                    "label": 0
                },
                {
                    "sent": "It's photo?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Receptor reporting on the local at light intensity level let me present this straw man argument that we are going to kill afterwards.",
                    "label": 0
                },
                {
                    "sent": "So what has been known and what is the textbook is that each one of those cells, the cells that we record from retinal ganglion cells, what it is doing is it's literally.",
                    "label": 0
                },
                {
                    "sent": "I mean that's the dogma.",
                    "label": 0
                },
                {
                    "sent": "It's taking an input light intensity pattern in a little part of the Sky.",
                    "label": 0
                },
                {
                    "sent": "It's linearly filtering it with the.",
                    "label": 0
                },
                {
                    "sent": "Thing that looks like this, which is called the center surround.",
                    "label": 0
                },
                {
                    "sent": "So basically it takes light intensity here and subtracts away a light intensity in the little bit of a surround, right?",
                    "label": 0
                },
                {
                    "sent": "That's what is responding to.",
                    "label": 0
                },
                {
                    "sent": "And then you know this linearly filtered result is what it communicates.",
                    "label": 0
                },
                {
                    "sent": "We know how this is implemented anatomically in the retina, more or less theory tells us why the retina should do something like that.",
                    "label": 1
                },
                {
                    "sent": "That's becausw, natural scenes are highly correlated across large distances, so if there is bright here next door, there will be a bright pixel as well.",
                    "label": 0
                },
                {
                    "sent": "You don't want to communicate redundant information, so you only report contrast changes, right?",
                    "label": 0
                },
                {
                    "sent": "That's like a derivative filter if you want in space, right?",
                    "label": 0
                },
                {
                    "sent": "The center minus the surround.",
                    "label": 0
                },
                {
                    "sent": "This theory predicts that the retinal cells should do that, and it makes a prediction that the outputs therefore of these neurons should be as decorrelated or as independent as possible.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's called the efficient coding hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We can build very good mathematical models that can predict the behavior of every single neuron to the slide stimulus, and then you just need to combine this.",
                    "label": 0
                },
                {
                    "sent": "I'll skip that, just need to combine this with the additional fact that each of these neurons pays attention to a little part of the Sky.",
                    "label": 0
                },
                {
                    "sent": "And together they tile the visual space, right?",
                    "label": 0
                },
                {
                    "sent": "So there is many in arranging this local mosaic like in your CCD camera and so given that we know how to model one, we would assume, well we know how to model all of them, right?",
                    "label": 0
                },
                {
                    "sent": "So we can predict the response of the retina to an arbitrary input stimulus, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we should know everything that there is to know about the retina and this whole thing makes a prediction that the spike trains the outgoing spike transform.",
                    "label": 0
                },
                {
                    "sent": "The retina should be as decorrelated as possible because they're decorrelating natural scenes through this optimal linear filter here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's the picture I want you to have in mind and will challenge some of these things in the picture.",
                    "label": 0
                },
                {
                    "sent": "But before I go on, I want to show you how you study neural population.",
                    "label": 1
                },
                {
                    "sent": "So that's experimental colleagues that published the method in Journal of Neuroscience.",
                    "label": 0
                },
                {
                    "sent": "So that is a microelectrode array.",
                    "label": 0
                },
                {
                    "sent": "It's a fabricated device with 252 electrodes.",
                    "label": 0
                },
                {
                    "sent": "Those are these little things here.",
                    "label": 0
                },
                {
                    "sent": "The wires going out there space about.",
                    "label": 0
                },
                {
                    "sent": "30 Micron.",
                    "label": 0
                },
                {
                    "sent": "And if you take retina of the animals that we study in this case was salamander and put it down on this array, the key feature is that these arrays so dense that in the retina is a planar tissue that in a little Patch of the retina the stuff that sits above the array you actually record every single or close to every single output neurons, right?",
                    "label": 0
                },
                {
                    "sent": "So you really record from a dense Patch.",
                    "label": 1
                },
                {
                    "sent": "You don't miss the signals that come out.",
                    "label": 0
                },
                {
                    "sent": "You don't record from the full retina because it's much bigger than the stuff that sitting.",
                    "label": 0
                },
                {
                    "sent": "Above the array, but you record from everything that is above the array and by the virtue of how the retina is designed, the neurons that review record.",
                    "label": 0
                },
                {
                    "sent": "From here they are all Co located and they all in code apart of the stimulus space apart of the visual scene.",
                    "label": 1
                },
                {
                    "sent": "That's also in a connected region right?",
                    "label": 1
                },
                {
                    "sent": "So that connected region of the stimulus space is fully encoded by the population you record from.",
                    "label": 0
                },
                {
                    "sent": "And that's a very rare case in neuroscience that you get the full mapping between inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "Traditionally, if you record from somewhere in the cortex, you stick in the electrodes.",
                    "label": 1
                },
                {
                    "sent": "Record from one 1000 neurons, but that will be 100 out of a tiny fraction of the total number, which could be millions right or thousands, right?",
                    "label": 0
                },
                {
                    "sent": "So you have to do with this sparse sampling of the network.",
                    "label": 0
                },
                {
                    "sent": "But here we actually get essentially all of the neurons.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see how this.",
                    "label": 0
                },
                {
                    "sent": "Normal activity looks like what I'm going to present is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to play for you, a movie that's a movie that was actually shown to the salamander retina fish swimming around in the fish tank.",
                    "label": 0
                },
                {
                    "sent": "Every neuron that you record from pays attention to a little part of the visual space.",
                    "label": 0
                },
                {
                    "sent": "Here, for one, we're on this neuron denoted by red lips is paying attention to what is happening in this part of the picture, so there is.",
                    "label": 0
                },
                {
                    "sent": "There is a standard ways to when you do a recording to figure out which part of the Sky the neuron is responsive to.",
                    "label": 0
                },
                {
                    "sent": "So this one neuron is responsive to the part here, but we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Records from several 100 neurons.",
                    "label": 0
                },
                {
                    "sent": "These are all their so called receptive fields.",
                    "label": 0
                },
                {
                    "sent": "So the neurons on my array are densely.",
                    "label": 0
                },
                {
                    "sent": "Reporting and what's happening in this part of the visual space, right?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to animate this, to give you an impression of the actual dynamics of the activity.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do now is actually play a short clip of the movie and every time the neuron that looks at this part of the Sky is making a spike, this red ellipse will blink on and off, so we will see how this looks like.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here is my remember neurons look here.",
                    "label": 0
                },
                {
                    "sent": "There is a spontaneous activity but nothing much happens because nothing changes.",
                    "label": 0
                },
                {
                    "sent": "But now fish will come right so fish comes it has this high contrast stripes and you see so I hope that when the fish was swimming this way it excited this burst of activity of various neurons right?",
                    "label": 0
                },
                {
                    "sent": "And then you can if you want you can repeat the same stimulus many times over or you can display synthetic stimuli, not naturalistic one, just dots bars.",
                    "label": 0
                },
                {
                    "sent": "You know something statistically well defined if you want to.",
                    "label": 0
                },
                {
                    "sent": "What we'll do is we'll try to analyze this response of the retina to natural natural scenes, which is what the right now involved.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Code.",
                    "label": 0
                },
                {
                    "sent": "So here is your natural stimulus.",
                    "label": 1
                },
                {
                    "sent": "And this is what you get from the experiment.",
                    "label": 0
                },
                {
                    "sent": "So roster time going this way, various distinct neurons recorded going this way and to analyze this data, we're going to reply.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the state of the retina in little time bins of duration.",
                    "label": 1
                },
                {
                    "sent": "I'll been the time into 20 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to explain why 20.",
                    "label": 0
                },
                {
                    "sent": "If somebody asks later on within these 20 millisecond time bins the joint activity pattern of the retina is therefore represented by this pattern of silences and spikes, and 20 milliseconds is such a time scale that within a single bin of 20 milliseconds the neurons almost never spike twice.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a binary representation of the retinal activity with zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "Alright, and that vector W of binary activity is one of the vectors in the long roster, and what we want to understand the very basic question is like.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You are trying to understand the foreign language.",
                    "label": 0
                },
                {
                    "sent": "If you want is the statistics of this activity patterns right?",
                    "label": 0
                },
                {
                    "sent": "So when I display a movie The retina and maybe repeat the same movie many times, the retina produces this binary words and I'd like to have a model of the probability distribution over all these binary words.",
                    "label": 0
                },
                {
                    "sent": "So which words happen often?",
                    "label": 0
                },
                {
                    "sent": "Which words are never happen?",
                    "label": 0
                },
                {
                    "sent": "Are they organized in some sort of clusters?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of that is the major problem with.",
                    "label": 1
                },
                {
                    "sent": "That is of course the curse of dimensionality, right?",
                    "label": 1
                },
                {
                    "sent": "So if I have 100 neurons emitting 100 bit binary words, there is at least in principle two to two to the 100 possible combinations and I cannot get to this distribution by sampling right?",
                    "label": 0
                },
                {
                    "sent": "I'm always in the totally undersampled limit, so something has to be done about that.",
                    "label": 0
                },
                {
                    "sent": "So what can?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be done about this is one way to deal with this are the maximum entropy model, so that will be a dense slide by it's the most of kind of most formal slide, and once we are done with this, hopefully things get get to be easy, so maximum entropy method is a method for approximating probability distributions.",
                    "label": 0
                },
                {
                    "sent": "Even high dimensional ones, so I'll give you now step by step.",
                    "label": 0
                },
                {
                    "sent": "Summary of how you can build such models and this is over generic frameworks not useful just for this retina.",
                    "label": 0
                },
                {
                    "sent": "So what do you do?",
                    "label": 0
                },
                {
                    "sent": "So consider so this.",
                    "label": 0
                },
                {
                    "sent": "So consider the Sigma one to Sigma N are the states of my neurons neuron one or this can be binary as in our case so these are 01 variables and what we start with is you choose L functions.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary number of functions over the state of these retina, so over this 100 bit.",
                    "label": 0
                },
                {
                    "sent": "If it's 100 neuron, 100 binary words, and you choose these functions such that you can rely abli, estimate their empirical averages across the final data set that you get in the experiment, right?",
                    "label": 0
                },
                {
                    "sent": "I'll give you examples very soon.",
                    "label": 0
                },
                {
                    "sent": "Right, so you choose the functions you estimate the average values of these functions across the code words that you record in the data, and now you look for a distribution over all possible activity patterns that fulfills two conditions.",
                    "label": 0
                },
                {
                    "sent": "So the first condition is that this distribution will exactly match the measured constraints.",
                    "label": 0
                },
                {
                    "sent": "This measured average values in the data and the second condition is that apart from matching this constraint exactly, this distribution is going to be as random as possible, which means.",
                    "label": 0
                },
                {
                    "sent": "Maximum entropy, so no assumptions, but for the explicit, explicitly matching exactly a certain list of constraints that you measure, you can show that this maximum entropy distributions, much like in statistical physics, have a Boltzmann like form.",
                    "label": 0
                },
                {
                    "sent": "So this is normalization exponent of a sum.",
                    "label": 0
                },
                {
                    "sent": "And in this sum there is one term for every function that you want to constrain and in front of that function is a LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "If you want a number that you have to tune in such a way.",
                    "label": 0
                },
                {
                    "sent": "That averages of these L functions over the distribution matched the ones in the data.",
                    "label": 0
                },
                {
                    "sent": "For physics physicist in the audience, of course the Bolts you can view a Boltzmann distribution of statistical mechanics precisely.",
                    "label": 0
                },
                {
                    "sent": "The maximum entropy distribution with the constraint on mean energy and LaGrange multiplier is 1 / K BT right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Numerically finding these LaGrange multipliers is hard, but I won't talk about this problem at all, so now the whole thing of modeling reduces to choosing what kind of stuff do we want to constrain in the data, and there is no prescription for that, so that is led by intuition measurements and sort of iterative work of science.",
                    "label": 0
                },
                {
                    "sent": "But let me give you a few basic ideas right so you can constrain for these L functions.",
                    "label": 0
                },
                {
                    "sent": "You can just constrain the mean value of every neuron.",
                    "label": 0
                },
                {
                    "sent": "The mean firing rate.",
                    "label": 0
                },
                {
                    "sent": "How many spikes on average does each neuron make?",
                    "label": 0
                },
                {
                    "sent": "And if you do that, the model will factorize.",
                    "label": 0
                },
                {
                    "sent": "So the most random model consistent with the average firing rate of every neuron is a model of independent neurons, each ticking, along with each with its own firing rate.",
                    "label": 0
                },
                {
                    "sent": "That is reproducing what you measure in the data.",
                    "label": 0
                },
                {
                    "sent": "So simple, trivial model.",
                    "label": 0
                },
                {
                    "sent": "The next most more complicated models you say I want probability distribution that will reproduce the average firing rate of every neuron, but also it will reproduce the measured pairwise correlation between every pair of neurons.",
                    "label": 0
                },
                {
                    "sent": "OK, now that is already and I'll show you in the next life that's already highly nontrivial.",
                    "label": 0
                },
                {
                    "sent": "If you look at what kind of Hamiltonian, what kind of probability distribution is consistent with these two constraints, it will be exactly or Ising Ising model from statistical physics without any symmetries so densely connected.",
                    "label": 0
                },
                {
                    "sent": "JJ between every pair of neurons and knowledge edges can be different.",
                    "label": 0
                },
                {
                    "sent": "You can also constrain various other things.",
                    "label": 0
                },
                {
                    "sent": "How many neurons spike simultaneously?",
                    "label": 0
                },
                {
                    "sent": "That's the cold so cold case by constraint and you can mix and match these constraints so we can make a model consistent with average firing rates of everyone with the covariance of every pair and with this case statistics which I will introduce also later, which is we just means you know how many times out of 100 neurons are, all of them quiet.",
                    "label": 0
                },
                {
                    "sent": "Any one of them fires 2 of them fire, etc.",
                    "label": 0
                },
                {
                    "sent": "So you can do all of these things.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Why am I telling you that, right?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm telling you that maybe?",
                    "label": 0
                },
                {
                    "sent": "Maybe I skip this slide, sorry I'll get back to you if you have specific questions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why am I telling you that I'm telling you that becausw in neural coding there has been long for a long time that there has been this sort of problem and the problem has been as follows.",
                    "label": 0
                },
                {
                    "sent": "If you take any pair of neurons and that's holds true for many neural recordings, not just the retina, you measure the correlation between any pair of neurons in the retina or everywhere else.",
                    "label": 0
                },
                {
                    "sent": "What you're going to find is that these correlations between pairs of neurons.",
                    "label": 0
                },
                {
                    "sent": "Here is a histogram of this between different recorded pairs are all extremely small.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is correlation coefficient between minus one and one the typical value 0.03 or so.",
                    "label": 0
                },
                {
                    "sent": "They're statistically significant because you have enough data to say that, but they're very small.",
                    "label": 0
                },
                {
                    "sent": "Write an when people in neuroscience measure this.",
                    "label": 0
                },
                {
                    "sent": "They wave their hands and basically said well, these correlations are so small that perhaps we can just neglect them, and these are independent neurons, and if they are independent, that's actually consistent with what we think.",
                    "label": 0
                },
                {
                    "sent": "Let's say in the retina, how the retina should work.",
                    "label": 1
                },
                {
                    "sent": "It should be called the inputs, and so the output should be independent.",
                    "label": 0
                },
                {
                    "sent": "That even fits the theory, so that's great, right?",
                    "label": 0
                },
                {
                    "sent": "But how do you actually check whether this correlations, despite them, all of them being smaller, important or not while you do the following, you build a maximum entropy distribution that is consistent with either just average firing rates in neglect correlations, or you build a model that's consistent both with average firing rates and the pairwise correlations, and you compare them to data, right?",
                    "label": 0
                },
                {
                    "sent": "So let me show you how this looks like.",
                    "label": 0
                },
                {
                    "sent": "So this is from a public, not our paper.",
                    "label": 0
                },
                {
                    "sent": "This one published paper in 2006.",
                    "label": 0
                },
                {
                    "sent": "So what is this?",
                    "label": 0
                },
                {
                    "sent": "Is the analysis of only 10 neurons, right?",
                    "label": 0
                },
                {
                    "sent": "So 10 neurons can produce 2 two to the power of 1024.",
                    "label": 0
                },
                {
                    "sent": "Different combinations of spiking and silence.",
                    "label": 0
                },
                {
                    "sent": "Here the new rule #1 fires in normal #9 for everyone else is silent for every one of these 1024 configurations there is actually enough data.",
                    "label": 0
                },
                {
                    "sent": "In a typical experiment to estimate its probability empirically by counting.",
                    "label": 0
                },
                {
                    "sent": "That's on this axis here, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a particular pattern whose empirical probability is this much right, and so on.",
                    "label": 0
                },
                {
                    "sent": "And now I can ask, how well are these probabilities predicted by models that only neglect correlations and only pay attention to the mean pirate firing rate, which is the blue thing.",
                    "label": 0
                },
                {
                    "sent": "So equality line would be here.",
                    "label": 0
                },
                {
                    "sent": "So a perfect model, this model predictions?",
                    "label": 0
                },
                {
                    "sent": "This is data, so stuff that's predicted well lies on this line.",
                    "label": 0
                },
                {
                    "sent": "If you neglect this small correlations, you get the blue dots and the blue dots are terrible in predicting the activity patterns of 10 neurons.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is this is order of magnitude apart.",
                    "label": 0
                },
                {
                    "sent": "This is a log scale right?",
                    "label": 0
                },
                {
                    "sent": "If however, instead you include the pairwise correlation, so we now have a Maxent model where you tune where you reproduce exactly the mean firing rate of every neuron and the covariance in between every pair of neurons.",
                    "label": 1
                },
                {
                    "sent": "You get this Ising like model and you fit it to data.",
                    "label": 0
                },
                {
                    "sent": "Then you get the prediction of the activity patterns, which are these red dots here that are lining up almost perfectly on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And here is a small sampling limit because you're counting.",
                    "label": 0
                },
                {
                    "sent": "Empirically, right?",
                    "label": 0
                },
                {
                    "sent": "So this spread comes from small data size.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion was once, despite the fact that every pairwise correlation is small, there is many pairwise correlations together even at the population of 10 neurons they predict.",
                    "label": 0
                },
                {
                    "sent": "Collective effects that are much sort of departing from independence by a lot, alright?",
                    "label": 0
                },
                {
                    "sent": "And the question we had is how does this scale you know, 10 neurons was an experimental constraint at that time, so you measure more neurons 100 and so on.",
                    "label": 0
                },
                {
                    "sent": "How far away?",
                    "label": 0
                },
                {
                    "sent": "How important are the collective effects?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A large population.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm going to present you to you now.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we again we have the recording that you saw.",
                    "label": 0
                },
                {
                    "sent": "We have a fish movie natural movie clip that we repeat many times 300 times.",
                    "label": 0
                },
                {
                    "sent": "Repetition of 22nd Movie Clip record from 100 to 160 very good neurons discretizing this small time bins and now we make this sort of Ising like pairwise models that fit means and covariances and we also make the show the K pairwise models where we add this additional global constraint.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These are the pairwise models.",
                    "label": 0
                },
                {
                    "sent": "So basically what you do is you in the raw data.",
                    "label": 0
                },
                {
                    "sent": "You now measure the covariance matrix by between 100 neurons, so it's 100 by 100 matrix.",
                    "label": 0
                },
                {
                    "sent": "You measure them in firing rate, here sorted of all the 100 neurons.",
                    "label": 0
                },
                {
                    "sent": "That's again the distribution of covariance is consistent with what has been reported previously and now from this you compute the Maxent interactions JJ and the magnetic fields hi.",
                    "label": 0
                },
                {
                    "sent": "Just to connect with what you probably know better, this is the exact opposite in how the traditional statistical mechanics goes right in the traditional statistical mechanics you assume some form for the interaction and some type of external field, and you're computing the correlations out and the means right?",
                    "label": 0
                },
                {
                    "sent": "So this is turning the problem on its head and solving the inverse problem right?",
                    "label": 0
                },
                {
                    "sent": "And what you might notice immediately is that if you look at this infer JJ matrix, it has things of both signs about equal proportion.",
                    "label": 0
                },
                {
                    "sent": "There is many frustrated triangles in here.",
                    "label": 0
                },
                {
                    "sent": "And it's sort of relatively dense, so implying a relatively old tall connectivity functional connectivity.",
                    "label": 0
                },
                {
                    "sent": "Promising that perhaps the statistical mechanics models might be interesting if this reminds you of spin glasses, perhaps right with the frustration and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You ask, well, you can ask how well do these models do?",
                    "label": 0
                },
                {
                    "sent": "You can mean I won't say anything about technical about how you construct them, but you can check that you are doing a good job of reconstructing how well do they do in predicting something nontrivial.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thing is, they fail very quickly.",
                    "label": 0
                },
                {
                    "sent": "They worked for 10 neurons for a large population.",
                    "label": 0
                },
                {
                    "sent": "They don't work any longer, but they fail in a very systematic way that I'm trying to illustrate here.",
                    "label": 0
                },
                {
                    "sent": "So this is for a group of 10 neurons where they should work as reported before and they do for 14 runs in the hundred neurons an what's plotted on this plot is the probability that out of 10 neurons, let's say 5 neurons spike in the same little 20 millisecond time bin.",
                    "label": 0
                },
                {
                    "sent": "So data is shown in red the maximum.",
                    "label": 0
                },
                {
                    "sent": "Ising, like pairwise model, is showing black and for 10 neurons these lines are extremely close to each other.",
                    "label": 0
                },
                {
                    "sent": "But you go here 200 neurons.",
                    "label": 0
                },
                {
                    "sent": "There is both a deviation in the tail, it's very strong.",
                    "label": 0
                },
                {
                    "sent": "There is also actually a deviation here.",
                    "label": 0
                },
                {
                    "sent": "This is a log scale, so this is the probability of zero neurons firing, so it's silence, complete silence, every neuron being silent, and there is a large difference right here between the two models where the factor of three between the prediction and the data and these are the most well sampled patterns, right?",
                    "label": 0
                },
                {
                    "sent": "They happen most often, as you see here, neurons is being silent, so pairwise models cannot capture this global activity pattern well.",
                    "label": 0
                },
                {
                    "sent": "And you know you can.",
                    "label": 0
                },
                {
                    "sent": "There is many ways you can try to fix this, and perhaps the most obvious way to fix it, which is what we are going to do and then explore the consequences, is to make a Maxent model that's now consistent with means and pairwise correlations as before, but constrained the model.",
                    "label": 0
                },
                {
                    "sent": "Also by this global potential to exactly reproduce also this global activity, right?",
                    "label": 0
                },
                {
                    "sent": "So it's like if you want, it's like a global modulatory effect.",
                    "label": 0
                },
                {
                    "sent": "Overall, neurons in forcing your model to reproduce in the data.",
                    "label": 0
                },
                {
                    "sent": "How many times a neuron spike together or not?",
                    "label": 0
                },
                {
                    "sent": "So, and this is what we call the K pairwise K pairwise model, and that's the reconstructed potential that will do the job for the specific data case shown here.",
                    "label": 0
                },
                {
                    "sent": "So alright, so this.",
                    "label": 0
                },
                {
                    "sent": "Augmented Ising models.",
                    "label": 0
                },
                {
                    "sent": "If you want are what we are studying now so well, how well do they do in predicting something knew about the data that was not put into the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All by hand.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 example.",
                    "label": 0
                },
                {
                    "sent": "I just saw one statistic so we can measure the three point correlation between triplets of neurons.",
                    "label": 0
                },
                {
                    "sent": "So take any three neurons and measure their correlation.",
                    "label": 0
                },
                {
                    "sent": "Ask the model to predict that same correlation and for this K pairwise improved models the matches in the red shown in this red red points is not perfect, but it's definitely a very good match.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this match, so the capers models can predict 3 point correlations without without significant bias, and they can do that across.",
                    "label": 0
                },
                {
                    "sent": "So this is the error in predicting 3 point correlation across different sizes of the network.",
                    "label": 0
                },
                {
                    "sent": "So for zero neurons, 5000 and so on, and this doesn't doesn't grow, which is interesting 'cause you have a model that has more and more 3 point correlations, and it doesn't have the same number of parameters mean the number of parameters grows more slowly, yet you do a good job of predicting this report correlations.",
                    "label": 0
                },
                {
                    "sent": "So you can check many other statistics which I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "Do in this talk.",
                    "label": 0
                },
                {
                    "sent": "Rather ask what do we learn from these models about the neural code?",
                    "label": 0
                },
                {
                    "sent": "OK, I won't discuss this so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing that is an interesting suggestion coming from these models concerns that pattern of interaction with frustration, right?",
                    "label": 0
                },
                {
                    "sent": "So if that reminds you of the spin class what this could mean is that.",
                    "label": 0
                },
                {
                    "sent": "So your model is defined by an energy function, right?",
                    "label": 0
                },
                {
                    "sent": "It's a Boltzmann like thing with a Hamiltonian in the exponent, and that energy function and energy overall patterns could exhibit many local minima if you want because of frustration and there has been an idea in neural coding existing for quite awhile that what is actually important for coding is not the detailed microscopic pattern of activity over all these neurons.",
                    "label": 0
                },
                {
                    "sent": "After all, neurons are noisy, so if you repeat the same stimulus again, a neuron might be silent or might be spiking.",
                    "label": 0
                },
                {
                    "sent": "With some probability right, you won't get exactly the same patterns, so it cannot be that exact.",
                    "label": 0
                },
                {
                    "sent": "Microscopic patterns encode the stimulus, but what else could it be?",
                    "label": 0
                },
                {
                    "sent": "Well, one suggestion was that what it could be is it's not a microscopic pattern that's important, but what basin of attraction?",
                    "label": 0
                },
                {
                    "sent": "If you want so?",
                    "label": 0
                },
                {
                    "sent": "If this is my energy function, there is a particular pattern sitting here, and there is a lot of other microscopic states that all belong to the same basic energy basin in this landscape.",
                    "label": 0
                },
                {
                    "sent": "That what matters for stimulus encoding is simply what base in your in, not the detailed microscopic state.",
                    "label": 0
                },
                {
                    "sent": "So that would be the microscopic pattern that has the locally lowest energy.",
                    "label": 0
                },
                {
                    "sent": "And all of these other patterns.",
                    "label": 0
                },
                {
                    "sent": "You can just go downhill by flipping the spins until you hit this bottom bottom thing.",
                    "label": 0
                },
                {
                    "sent": "So can we identify such basins in the energy functions of the models we reconstructed?",
                    "label": 0
                },
                {
                    "sent": "Indeed, we can.",
                    "label": 0
                },
                {
                    "sent": "Here is the number of the basins we find as the size of the neural network grows larger, so this is for models for 40 neurons, 8000.",
                    "label": 0
                },
                {
                    "sent": "20 This is the number of such metastable states that we identify.",
                    "label": 0
                },
                {
                    "sent": "So for the largest network, we find several hundreds of such basins if you want.",
                    "label": 0
                },
                {
                    "sent": "This is just to give you an example how patterns belonging to this basis look like.",
                    "label": 0
                },
                {
                    "sent": "So what's shown here?",
                    "label": 0
                },
                {
                    "sent": "These are neurons.",
                    "label": 0
                },
                {
                    "sent": "These are now collected microscopic patterns that all belong to base in number one.",
                    "label": 0
                },
                {
                    "sent": "I just call it number one, it's the base, and we're actually no neuron is active, so it's the silent state.",
                    "label": 0
                },
                {
                    "sent": "What you see is most stuff being silent here and there.",
                    "label": 0
                },
                {
                    "sent": "There is a little spike.",
                    "label": 0
                },
                {
                    "sent": "These are all microscopic states belonging to another base in that basin is defined by a pattern here that has this guy active.",
                    "label": 0
                },
                {
                    "sent": "This guy active this, this and this active an indeed all microscopic patterns belonging to this basin have these neurons.",
                    "label": 0
                },
                {
                    "sent": "Active, but here and there there is other spikes alright.",
                    "label": 0
                },
                {
                    "sent": "So basically this.",
                    "label": 0
                },
                {
                    "sent": "This paradigm is now partitioning the space of two to the N patterns into basins of attraction.",
                    "label": 0
                },
                {
                    "sent": "And what one is saying is that perhaps the only thing that matters is that you know these guys code for something and all of these guys code for something else.",
                    "label": 0
                },
                {
                    "sent": "So all of the patterns belonging to this basis right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The patterns cluster if you order them by which basically they go in the similarity matrix, you know this place is nice clustering.",
                    "label": 0
                },
                {
                    "sent": "I'll skip that.",
                    "label": 0
                },
                {
                    "sent": "Maybe I pay attention to this plot here.",
                    "label": 0
                },
                {
                    "sent": "So what is shown here is now.",
                    "label": 0
                },
                {
                    "sent": "Going to the data, taking the actual microscopic state of the retina at every point in time and assigning every microscopic state to each own basin of attraction.",
                    "label": 1
                },
                {
                    "sent": "Alright, and then 'cause I repeat the stimulus.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same stimulus repeated 300 times.",
                    "label": 0
                },
                {
                    "sent": "I can ask for every time point in the stimulus in what besen across repeats, what the probability of the retina being in the blue base it right or in the Yellow basin?",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 1
                },
                {
                    "sent": "And what you see is this picture where the retina here is in.",
                    "label": 0
                },
                {
                    "sent": "This is blue is the silent base, and so nothing happens.",
                    "label": 0
                },
                {
                    "sent": "And then rather reproducibly across repeats, it transitions into this light blue thing, and it goes into the green thing and again a green into red and so on, right?",
                    "label": 0
                },
                {
                    "sent": "And what's important here is that the model didn't assume anything about the repeated stimulus statistics or anything, right?",
                    "label": 0
                },
                {
                    "sent": "So basically what emerges from this analysis is that across stimulus repeats, the retina is state is well described instead of by these two to the end bit microscopic pattern.",
                    "label": 0
                },
                {
                    "sent": "Simply, in what base in it is in and how it transitions from basin to basin, right?",
                    "label": 0
                },
                {
                    "sent": "So this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not evidence that the retina is using this basic code, but it is a strong suggestion.",
                    "label": 0
                },
                {
                    "sent": "That's an interesting line of research and.",
                    "label": 0
                },
                {
                    "sent": "This has been suggested in theoretical models in neuroscience, but it was never their theory that was hard to connect to data, right?",
                    "label": 0
                },
                {
                    "sent": "So this is now the same type of model that was used in this theoretical work, but since it's inferred from data, you can try to make statements that maybe some of these ideas actually make sense of sort of global states of activity coding for the stimulus.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we can also do is we can compute the entropy of these code, right?",
                    "label": 0
                },
                {
                    "sent": "So if you know a little bit about information theory, the entropy of these code words, they come from the retina puts an upper limit on how much information you can push through the retina, right?",
                    "label": 0
                },
                {
                    "sent": "We can compute this in various ways.",
                    "label": 0
                },
                {
                    "sent": "And I'm just showing this here, so what's shown here is the entropy per neuron in red in red dots as the number of us, my size of the neural network grows.",
                    "label": 1
                },
                {
                    "sent": "What you see is this interesting thing that you might not be.",
                    "label": 0
                },
                {
                    "sent": "You know it's a regime where it's not that we're not at home in statistical physics where where the entropy per neuron is actually still decreasing.",
                    "label": 0
                },
                {
                    "sent": "It's not extensive as I take more and more neurons.",
                    "label": 0
                },
                {
                    "sent": "That's big, cause the system is strongly coupled and neurons have wires leading between.",
                    "label": 0
                },
                {
                    "sent": "Each other, they're not just nearest neighbors, and when I take 120 neurons, I'm not yet in the regime where I'm out of the real circuit.",
                    "label": 0
                },
                {
                    "sent": "Connectivity alright.",
                    "label": 0
                },
                {
                    "sent": "If you extrapolate you hit extensivity about about 2 to 300 neurons, which is actually consistent with the known Physiology in the salamander.",
                    "label": 0
                },
                {
                    "sent": "This is the distance.",
                    "label": 1
                },
                {
                    "sent": "This is the Patch of the retina that's densely connected.",
                    "label": 0
                },
                {
                    "sent": "If you go out of it then the wires don't reach any longer across those distances, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the thing here, and these absolute numbers actually have a meaning for people who do neural coding in putting a limit on how many bits per second you can squeeze through the retina.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the last part results is now going back to this introduction.",
                    "label": 0
                },
                {
                    "sent": "You will remember that the dominant thought in neural coding, especially in the retina, has been this notion of decorrelation right?",
                    "label": 0
                },
                {
                    "sent": "I said there is a picture of what the retina does.",
                    "label": 0
                },
                {
                    "sent": "It filters the stimulus and it does it in such a way that the outputs of the neurons are as decorrelated as possible.",
                    "label": 0
                },
                {
                    "sent": "Alright, so clearly the results up to now have demonstrated the neurons are not independent.",
                    "label": 1
                },
                {
                    "sent": "I mean, you know the independence is a very bad model, so the correlation remains, but can you explicitly demonstrate this correlated nature of activity?",
                    "label": 0
                },
                {
                    "sent": "And you can do that now that you have a model for the joint activity of all the neurons by doing the following experiment, so you pretend you don't know the state of neuron one, but you know the states of neuron from the data.",
                    "label": 0
                },
                {
                    "sent": "You know what the neurons do?",
                    "label": 0
                },
                {
                    "sent": "You know two to the 100 are doing and you can use your model to predict what the neuron one should be doing because it's coupled to everyone, right?",
                    "label": 0
                },
                {
                    "sent": "So of course, in the data you know what the neuron one is doing, and so since I have many repeats of the same movie, this is the firing rate.",
                    "label": 1
                },
                {
                    "sent": "So this is the probability of the neuron spiking at every instant in time.",
                    "label": 0
                },
                {
                    "sent": "It does this crazy thing of quiet spiking and so on, so that's the data from my neuron number one, and now I can ask, can I predict this neuron number 1 from 9 other neighbors in the network so from a network of a total size of 10?",
                    "label": 0
                },
                {
                    "sent": "I don't do a very good job.",
                    "label": 0
                },
                {
                    "sent": "If I take 20 neurons, I do a little bit better, but still actually quite crappy.",
                    "label": 1
                },
                {
                    "sent": "But once I go up to the network of 120 neurons.",
                    "label": 0
                },
                {
                    "sent": "I can predict the activity of this withheld neuron with very good cross correlation.",
                    "label": 0
                },
                {
                    "sent": "If you want right.",
                    "label": 0
                },
                {
                    "sent": "I can do this across many neurons and maybe this is the summary plot here.",
                    "label": 0
                },
                {
                    "sent": "When I go up to 120 neural networks, on average, I can predict the activity of.",
                    "label": 0
                },
                {
                    "sent": "An arbitrary neuron that I exclude with the cross relation about 80% so 80%.",
                    "label": 0
                },
                {
                    "sent": "Just this is knowing nothing about the stimulus.",
                    "label": 1
                },
                {
                    "sent": "This is just knowing on average what other guys are doing.",
                    "label": 0
                },
                {
                    "sent": "And this is telling me something about what my neuron is doing.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is an explicit demonstration that these networks are very, very far from producing independent outputs.",
                    "label": 0
                },
                {
                    "sent": "Actually outputs are so dependent you can predict the activity of one guy very well from the activity of the rest.",
                    "label": 0
                },
                {
                    "sent": "Now you know what doesn't fit right?",
                    "label": 0
                },
                {
                    "sent": "What doesn't fit with the existing theories that existing theory is true in the limit of low noise in your in your network.",
                    "label": 1
                },
                {
                    "sent": "So if there were zero noise in the normal response, the neurons should indeed be correlate.",
                    "label": 0
                },
                {
                    "sent": "However, if each element is noisy, you don't want the correlation.",
                    "label": 0
                },
                {
                    "sent": "You don't want every neuron to say something independent of the others, because if that moron makes a mistake, there is no way to correct that mistake, right?",
                    "label": 0
                },
                {
                    "sent": "Just a mistake that propagates the brain.",
                    "label": 0
                },
                {
                    "sent": "So it's very much like an engineer.",
                    "label": 0
                },
                {
                    "sent": "It codes when you design a code to pass through a noisy channel.",
                    "label": 0
                },
                {
                    "sent": "What you do is you put in artificial redundancy in the form of error correcting bits or so on, right?",
                    "label": 0
                },
                {
                    "sent": "That's redundant information because it helps you correct the errors.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can demonstrate that if you know what the other neurons are doing, you can predict what any withheld neuron is doing fairly well, right?",
                    "label": 0
                },
                {
                    "sent": "And this turns out if you delve into the theory a bit more deeply, this is the optimal kind of thing you would do if your network units are not noise free.",
                    "label": 0
                },
                {
                    "sent": "If they're noisy, right?",
                    "label": 0
                },
                {
                    "sent": "So you should keep some amount of redundancy in the network too.",
                    "label": 0
                },
                {
                    "sent": "To error correct.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK, and now I'll try to go towards this hint of criticality.",
                    "label": 0
                },
                {
                    "sent": "I tried to finish in about 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "So we have the data.",
                    "label": 0
                },
                {
                    "sent": "We have models of the joint activity of the neurons that we think are are very good.",
                    "label": 0
                },
                {
                    "sent": "I'd like to emphasize that this has not been done right, the model.",
                    "label": 0
                },
                {
                    "sent": "So this detailed level 400 plus neurons have not been actually constructed, so generative models have not been constructed until very recently.",
                    "label": 0
                },
                {
                    "sent": "What we would like to ask now is whether these distributions that we have constructed in some ways special or in particular, are they close to critical.",
                    "label": 0
                },
                {
                    "sent": "And that turns out to be much more complicated than you would think, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a real physical system, a thermodynamic system, then the claim is let's say it's close to the critical point.",
                    "label": 0
                },
                {
                    "sent": "Well, what kind of analysis do we do?",
                    "label": 0
                },
                {
                    "sent": "Let's say you can look at the behavior of correlation functions right?",
                    "label": 0
                },
                {
                    "sent": "Do they diverge when you go close to the critical point?",
                    "label": 0
                },
                {
                    "sent": "Well, here it's hard cause our system is all too all connected.",
                    "label": 0
                },
                {
                    "sent": "So the notion of distance doesn't doesn't really fit at this scale.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you could do, let's say in the in the normal system, is.",
                    "label": 0
                },
                {
                    "sent": "You could make perturbation to some external parameter that couples to the you know something.",
                    "label": 0
                },
                {
                    "sent": "So you basically try to manipulate the order parameter of the system, right?",
                    "label": 0
                },
                {
                    "sent": "You change magnetic field or so you look at the susceptibility in our case.",
                    "label": 0
                },
                {
                    "sent": "And in general, in the case of inverse models, the problem is even how to identify relevant order parameters?",
                    "label": 0
                },
                {
                    "sent": "This is very specific construction, right?",
                    "label": 0
                },
                {
                    "sent": "It's not clear what these are, right.",
                    "label": 0
                },
                {
                    "sent": "What we can do in?",
                    "label": 0
                },
                {
                    "sent": "I'll try to show is you can right there is a mess in if you go back to mathematical physics there is a strong link between statistical.",
                    "label": 0
                },
                {
                    "sent": "So basically statistical physics is a statement about probability distributions.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And then of course you can kind of.",
                    "label": 0
                },
                {
                    "sent": "Implement external knobs that you twinkle and so on, but actually criticality can be discussed without doing the without.",
                    "label": 0
                },
                {
                    "sent": "Doing that and I'll try to show some of these things here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing that we do is we compute the microcanonical entropy of our distribution.",
                    "label": 0
                },
                {
                    "sent": "That's that's actually very simple.",
                    "label": 0
                },
                {
                    "sent": "We have a model that can you give me a microscopic state and the model predicts the energy of that state, right?",
                    "label": 0
                },
                {
                    "sent": "So the only thing I need to do is I need to count how many states there are with the given energy E and the log of that.",
                    "label": 0
                },
                {
                    "sent": "If you want, is the microcanonical entropy SOV.",
                    "label": 0
                },
                {
                    "sent": "And then I can look for this quantity to identify whether there is something like a critical point in my system, right?",
                    "label": 0
                },
                {
                    "sent": "So the second derivative of this entropy with respect to the energy?",
                    "label": 0
                },
                {
                    "sent": "I can construct this curve for the using very recent.",
                    "label": 0
                },
                {
                    "sent": "Very nice sampling techniques like one Clan downsampling.",
                    "label": 0
                },
                {
                    "sent": "You can take him all that we have an.",
                    "label": 0
                },
                {
                    "sent": "You can construct the curve of entropy versus energy, so energy per neuron, entropy per neuron here and these curves are constructed for blue groups of 20 neurons.",
                    "label": 0
                },
                {
                    "sent": "Groups of 40 neurons etc etc up to the group of 100 out of groups of 120 neurons in the red which are here.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting is that there is a very natural extrapolation towards higher and higher groups, so you see that when I increase the size of my network, I go this way right.",
                    "label": 0
                },
                {
                    "sent": "And you can just make this formal.",
                    "label": 0
                },
                {
                    "sent": "You can extrapolate to infinite size of the network using this construction.",
                    "label": 0
                },
                {
                    "sent": "And the extrapolated points for the entropy versus energy are these black things that go like that.",
                    "label": 0
                },
                {
                    "sent": "And what you see is in this low energy region.",
                    "label": 0
                },
                {
                    "sent": "They all line up apart from this guy here on a straight line.",
                    "label": 0
                },
                {
                    "sent": "Which is interesting 'cause the straight line were actually quality of entropy and energy means that this condition here is actually satisfied, not to the given point, but the whole low energy range, right?",
                    "label": 0
                },
                {
                    "sent": "So this suggests a very.",
                    "label": 0
                },
                {
                    "sent": "Critical behavior in which in which this curve is flat across this whole low energy range of patterns, and the interesting thing is that here you might object while it's a statement about the model.",
                    "label": 1
                },
                {
                    "sent": "It's not really statement.",
                    "label": 0
                },
                {
                    "sent": "I mean we went this complicated way.",
                    "label": 0
                },
                {
                    "sent": "We took the data, we constructed Maxon model, now we studied the model or the model does that that.",
                    "label": 0
                },
                {
                    "sent": "But you know what happens in real data.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is you can reproduce exactly this thing directly from the data, right?",
                    "label": 0
                },
                {
                    "sent": "You can take patterns.",
                    "label": 0
                },
                {
                    "sent": "You can sample some of them actually very well, not for a small fraction of them.",
                    "label": 0
                },
                {
                    "sent": "But you can and then the energy of those is just the log probability that you sample and you can ask how many of them there are.",
                    "label": 0
                },
                {
                    "sent": "So this is now the same type of thing constructed from raw data alone for different size of the networks.",
                    "label": 0
                },
                {
                    "sent": "And of course, because the data is fine at your running into a sampling sampling limit, the higher the size of the network, the smaller part of this curve you can see because of the sampling limit.",
                    "label": 0
                },
                {
                    "sent": "So here is 420 neurons, but still you can take this extrapolation to angos large and you see exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Extrapolated points in the data line up on the equality line exactly as they do in the model, right?",
                    "label": 0
                },
                {
                    "sent": "So this suggests that so, so we don't think that's an artifact of fitting a Maxent model.",
                    "label": 0
                },
                {
                    "sent": "You can redo this directly from the data, and it suggests a very peculiar critical behavior where this thing is zero across actually a whole range of you, not just at the any given point.",
                    "label": 0
                },
                {
                    "sent": "That is equivalent to a if you know what the zipf law is.",
                    "label": 0
                },
                {
                    "sent": "If you plot the zip flop for these patterns, both from the model and the data, they will fall on a very nice minus one log log slope.",
                    "label": 0
                },
                {
                    "sent": "I won't say more in detail, but if you have a question you can ask.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About that, so that was one signature of criticality.",
                    "label": 1
                },
                {
                    "sent": "I'll present another construction.",
                    "label": 0
                },
                {
                    "sent": "To examine the kinds of models we have.",
                    "label": 1
                },
                {
                    "sent": "So remember, I inferred from data model of the following form, right?",
                    "label": 1
                },
                {
                    "sent": "There was this Hamiltonian which matched the constraints in the data, but there was no notion in maximum entropy model.",
                    "label": 1
                },
                {
                    "sent": "There is no notion of temperature like in thermodynamics.",
                    "label": 0
                },
                {
                    "sent": "However, mathematically I can introduce this temperature parameter and view it simply as a scaling parameter of the Hamiltonian, right?",
                    "label": 1
                },
                {
                    "sent": "So for now, look at this as a mathematical trick if you want.",
                    "label": 1
                },
                {
                    "sent": "This generates a family of distributions parameterized by this parameter.",
                    "label": 0
                },
                {
                    "sent": "And for each such distribution, for each value of TI, can compute if you want to hit capacity like I do in statistical physics.",
                    "label": 1
                },
                {
                    "sent": "And this is what I see.",
                    "label": 0
                },
                {
                    "sent": "So this is the parameter T that I tune T equal 1 is the real model inferred from the data right?",
                    "label": 0
                },
                {
                    "sent": "Equal 1 is exactly the stuff that I learned from the data going away from one.",
                    "label": 0
                },
                {
                    "sent": "Are these mathematical constructions that don't match any data and these are heat capacity curves for 20 neurons, 4020, forty, 80 and 120 neurons.",
                    "label": 1
                },
                {
                    "sent": "And what you see is that the larger the size of the network the speak of heat capacity is actually first of all, it's as I'll show it.",
                    "label": 0
                },
                {
                    "sent": "Bing faster than linear with North, but importantly is moving towards the equal 1.",
                    "label": 0
                },
                {
                    "sent": "So this means that the larger in the limit when I'm taking a large network of neurons, the peak of this, the peak of the heat capacity which indicates some incipient.",
                    "label": 1
                },
                {
                    "sent": "If you want criticality will lie exactly on the line where my models describing the data are right.",
                    "label": 0
                },
                {
                    "sent": "So if I do the same thing but normalize heat capacity per neuron, you actually see that this quantity per neuron is growing faster than linear.",
                    "label": 1
                },
                {
                    "sent": "Right, so it's not intensive right?",
                    "label": 0
                },
                {
                    "sent": "And the P goes closer to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On which you can, you can quantify.",
                    "label": 0
                },
                {
                    "sent": "So this is how the peak is approaching one.",
                    "label": 0
                },
                {
                    "sent": "This is how quickly the heat capacity per neuron is growing.",
                    "label": 0
                },
                {
                    "sent": "This is another signature if you want of of emerging criticality in this system.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can do other constructions which interest of time.",
                    "label": 0
                },
                {
                    "sent": "I will skip.",
                    "label": 0
                },
                {
                    "sent": "Happy to discuss it if there is any.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question there is other checks that you can do different sort of analysis and maybe just.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Include on, you know with a few words so.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Critical so alright, so the distribution of code words coming from the retina is special in a way that looks like a critical system in statistical physics.",
                    "label": 0
                },
                {
                    "sent": "You know why is this good?",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "There is suggestions that maybe this is not a fact about threatening at all.",
                    "label": 0
                },
                {
                    "sent": "Maybe it has something to do with our fitting of a model, right?",
                    "label": 0
                },
                {
                    "sent": "That was a suggestion by Martha Marcy and Master Mateo this week and this week and check to show that this is not the case according to their own specifications.",
                    "label": 0
                },
                {
                    "sent": "There has been another suggestion by recent by David Robillard, Eminem, Pankaj Mehta, that perhaps the criticality is somehow an automatic consequence of the fact that you are taking a statistical physics model that looks like an equilibrium thing and you're applying it to describe the behavior of a driven system.",
                    "label": 0
                },
                {
                    "sent": "The retina is driven system, right stimulus is driving it all the time.",
                    "label": 0
                },
                {
                    "sent": "The answer to this is maybe there is some truth to it, but actually it's not so clear cut as this publication was suggesting.",
                    "label": 0
                },
                {
                    "sent": "Because we have some tuning knobs in the retina, unfortunate there are not as obvious ones as in physics, temperature and so on, but we can change the stimulus ensembles for instance from more to less correlated and so on.",
                    "label": 0
                },
                {
                    "sent": "And they don't exactly match what this is supposed to be doing.",
                    "label": 0
                },
                {
                    "sent": "This suggestion is supposed to be saying.",
                    "label": 0
                },
                {
                    "sent": "So we're not sure about this point.",
                    "label": 0
                },
                {
                    "sent": "We don't think it fully explains it, but there is some truth to this.",
                    "label": 0
                },
                {
                    "sent": "Perhaps now, maybe criticality has a functional significance.",
                    "label": 0
                },
                {
                    "sent": "So what could that be?",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "The being having a code that sits at the peak of heat capacity.",
                    "label": 0
                },
                {
                    "sent": "What is heat capacity?",
                    "label": 0
                },
                {
                    "sent": "Heat capacity?",
                    "label": 0
                },
                {
                    "sent": "Just a variance in lock probability?",
                    "label": 0
                },
                {
                    "sent": "Alright, I mean just the quality right?",
                    "label": 0
                },
                {
                    "sent": "So it means that if you are there you are maximizing in your distribution.",
                    "label": 0
                },
                {
                    "sent": "The variance of log probabilities, which means that in your code book, if you want in your in your dictionary you have code words that come with the widest possible range of probabilities, right from something that happens very often to something that happens extremely rarely, right?",
                    "label": 0
                },
                {
                    "sent": "So if you want to encode.",
                    "label": 0
                },
                {
                    "sent": "Outside stimuli which people are thinking are happening, you know the stuff in the outside world is happening with the structures that have these long tails.",
                    "label": 0
                },
                {
                    "sent": "If you want to encode them fast, meaning something happens and you pick up a code, work from code book for code Word from the dictionary and throw it out, then you have a code that matches that broad range of stuff happening outside.",
                    "label": 0
                },
                {
                    "sent": "That's actually the inverse of what an optimal code like a zip compression algorithm would do.",
                    "label": 0
                },
                {
                    "sent": "Right zip compression algorithm would have stuff that he has a huge range of probability putting together into blocks in a smart way such that what comes out is.",
                    "label": 0
                },
                {
                    "sent": "Of equal probability, but that takes time, right?",
                    "label": 0
                },
                {
                    "sent": "You need to block code right?",
                    "label": 0
                },
                {
                    "sent": "So it has a delay, which is not something very smart for the retina.",
                    "label": 0
                },
                {
                    "sent": "There is stores that this is a side effect of the fact that in the retina we have very long time scale dynamics of adaptation and so on.",
                    "label": 0
                },
                {
                    "sent": "It could be a side effect of maximizing other things such as the system wanting to maximize information transmission, and of course this is what we are currently studying, which is, you know, forget what is the reason for the mechanistic reason for this distribution.",
                    "label": 0
                },
                {
                    "sent": "But that's the code words that go into the brain.",
                    "label": 0
                },
                {
                    "sent": "So the brain is sampling is getting signals that come from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And the brain remember, has to learn it right while it grows up.",
                    "label": 0
                },
                {
                    "sent": "The brain doesn't have a code book that says whenever I get 101010, that's a dog, right?",
                    "label": 0
                },
                {
                    "sent": "It has to infer this right now.",
                    "label": 0
                },
                {
                    "sent": "There could be distributions of code words coming from the retina into the brain that are presumably rich in information, but impossible to learn, which is what you would get if you take what the writing assessment put it through.",
                    "label": 0
                },
                {
                    "sent": "A mathematical encryption algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "You know the brain can learn, can look at this code, works for 10 years and not make any sense of this, right?",
                    "label": 0
                },
                {
                    "sent": "So we're actually asking, we think an interesting mathematical question, which is what is the mathematical inverse of encryption schemes?",
                    "label": 0
                },
                {
                    "sent": "What are the most obvious schemes that you can have, given that the code words can be noisy, right?",
                    "label": 0
                },
                {
                    "sent": "If there are noises not a problem, but if there are noisy you should.",
                    "label": 0
                },
                {
                    "sent": "There should be some correlation, but you should be very quick and the brain should be very quick.",
                    "label": 0
                },
                {
                    "sent": "And figuring out what it is because you only get some finite amount of samples and you have to wire the brain up correctly.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I want it distorts as you see there is still a lot of open space here for research.",
                    "label": 0
                },
                {
                    "sent": "I would like to conclude.",
                    "label": 0
                },
                {
                    "sent": "I hope I could show that this inverse statistical mechanics using maximum entropy is actually a very powerful tool for studying.",
                    "label": 0
                },
                {
                    "sent": "Systems in biology, neuroscience where we observe the states of many elements at the same time and the data in that direction is now exploding.",
                    "label": 0
                },
                {
                    "sent": "It used to be very hard to obtain simultaneous recordings of the states of many genes.",
                    "label": 0
                },
                {
                    "sent": "Many neurons, many animals and so on at the same time.",
                    "label": 0
                },
                {
                    "sent": "Now we have it.",
                    "label": 0
                },
                {
                    "sent": "But now the question is what we do with it, right?",
                    "label": 0
                },
                {
                    "sent": "There is all these data, all this money that's going to flow into various brain projects, some of which are smart, some of which are frankly not.",
                    "label": 0
                },
                {
                    "sent": "But you know, on the experimental side, the push will be there at some point.",
                    "label": 0
                },
                {
                    "sent": "We will have joint recordings of 100,000 neurons right of what they do together in a zebrafish.",
                    "label": 0
                },
                {
                    "sent": "So, well, you know, how do you extract what the brain is doing from those things?",
                    "label": 0
                },
                {
                    "sent": "One suggestion is this sort of methodology is not the only one.",
                    "label": 0
                },
                {
                    "sent": "Of course, what we found concretely on the retina are some interesting coding properties.",
                    "label": 0
                },
                {
                    "sent": "That is that despite small correlations that you measure the behavior of the neurons is strongly collective.",
                    "label": 0
                },
                {
                    "sent": "It leads to these metastable states of activity.",
                    "label": 0
                },
                {
                    "sent": "The entropy still scaling non extensively.",
                    "label": 0
                },
                {
                    "sent": "We think that this redundancy, so the fact that these are not independent can be used for error correction.",
                    "label": 0
                },
                {
                    "sent": "There is some stuff about coincidence probability I didn't manage to talk about.",
                    "label": 0
                },
                {
                    "sent": "In particular, we also find that these output code word and samples are close to critical in various measures, some of which I didn't and end up explaining.",
                    "label": 0
                },
                {
                    "sent": "One thing I maybe I skip this so the crucial question for us is, is this system the retina tuned to or dynamically adapting to some sort of a critical point, or is this result somehow generic?",
                    "label": 0
                },
                {
                    "sent": "And what I think is sorry, and what I think is nice.",
                    "label": 1
                },
                {
                    "sent": "Just last concluding thought is that this question can be experimentally addressed, right?",
                    "label": 0
                },
                {
                    "sent": "An let me give you a. Alright, forget about it.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a one sentence summary how you have your retina sitting on the array exposed to some stimulus, like this one.",
                    "label": 0
                },
                {
                    "sent": "A natural stimulus.",
                    "label": 0
                },
                {
                    "sent": "Right now, many repeats.",
                    "label": 0
                },
                {
                    "sent": "You do exactly the same analysis here and now you suddenly switch the stimulus to some very different statistic, right noise?",
                    "label": 0
                },
                {
                    "sent": "Something that has very different different stimulus structure.",
                    "label": 0
                },
                {
                    "sent": "Immediately after the switch, the retina is not adapted to the to the new stimulus, and so the thinking is that if you construct the model immediately after the switch, it will actually shift away from critical for some onsan on some time scale, while the retina is re adapting itself to the new stimulus, it will slide back to the critical, right?",
                    "label": 0
                },
                {
                    "sent": "That's an experiment you can do, and we're actually doing right, and if you observe this.",
                    "label": 0
                },
                {
                    "sent": "Move right of being in adaptive state, which is close to critical.",
                    "label": 0
                },
                {
                    "sent": "Once you perturb the retina by changing its stimulus, quickly goes away and then it returns back to critical.",
                    "label": 0
                },
                {
                    "sent": "We would also feel much more strongly that you know there is a real meaning to this tuning to a critical point, right and somehow beneficial for coding, so that's what was taking place in the lab right now.",
                    "label": 0
                },
                {
                    "sent": "Alright, so with this I'd like to conclude and maybe we have few minutes for questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for a nice talk and I would be very quickly question and starting from the from the end when you say the system somehow see it's always at a critical point.",
                    "label": 1
                },
                {
                    "sent": "And evidently there are criticality in there, are there are attempts to model dynamics because actually this is dynamical driven system and nonlinear and so on and correlated.",
                    "label": 0
                },
                {
                    "sent": "Yes, like in in the sense of self organization and this kind of things.",
                    "label": 0
                },
                {
                    "sent": "Where are actually?",
                    "label": 0
                },
                {
                    "sent": "You should not just know dynamics.",
                    "label": 0
                },
                {
                    "sent": "Which of neurons will also the model interactions, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, you didn't mention that point of view going on the on the other level and that brings me to.",
                    "label": 0
                },
                {
                    "sent": "Your second part over you did describe the model.",
                    "label": 0
                },
                {
                    "sent": "When you get the model from interpreting data, why always you're talking about full graph in terms of of network?",
                    "label": 0
                },
                {
                    "sent": "OK, all to all, in this dynamical model you have to actually put exactly how do they interact and is it full graph structure reliable in this sense?",
                    "label": 0
                },
                {
                    "sent": "OK, alright, so regarding the dynamics, that's a great question, was not part of this talk because of time constraints, but.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can try to model dynamics in various ways.",
                    "label": 0
                },
                {
                    "sent": "You can actually model it in exactly the same framework, right?",
                    "label": 0
                },
                {
                    "sent": "So we have now modeled.",
                    "label": 0
                },
                {
                    "sent": "You know, we have imposed constraints within a single time, been right on the statistics of what the neurons do together.",
                    "label": 0
                },
                {
                    "sent": "But of course nothing prevents you from doing this across time bins.",
                    "label": 0
                },
                {
                    "sent": "So you say you know when a time T neuron I spikes, what's the likelihood that the T + 1 than orange spikes, right?",
                    "label": 0
                },
                {
                    "sent": "And that will give you directional pairwise couplings across the time bins.",
                    "label": 0
                },
                {
                    "sent": "This has been done by us and by others.",
                    "label": 0
                },
                {
                    "sent": "And of course it improves so it improves stuff in terms of the match to data etc etc.",
                    "label": 0
                },
                {
                    "sent": "It's just harder to do.",
                    "label": 0
                },
                {
                    "sent": "I mean technically it's harder to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so in the next question relates then to, you know we reconstruct the full connectivity graph right?",
                    "label": 0
                },
                {
                    "sent": "And how does that fit with?",
                    "label": 0
                },
                {
                    "sent": "That's a hard right, so that's a hard question.",
                    "label": 1
                },
                {
                    "sent": "Becausw, we are modeling the output distribution of the retina, which is a consequence of two things.",
                    "label": 0
                },
                {
                    "sent": "One is the Y receiver in the actual retina, and the other is the fact that the stimulus itself, which we don't explicitly model at all, is driving certain units strongly in parallel, right?",
                    "label": 0
                },
                {
                    "sent": "So, at the output, you cannot know whether these two units are connected, because there is a wire going in between them, or because the same stimulus is doing this to both units, right?",
                    "label": 0
                },
                {
                    "sent": "You can again extend this framework to actually disentangle these two things by putting in explicitly the stimulus and factoring out whatever can be explained by the common stimulus, and then what remains is supposedly just the so-called noise correlations.",
                    "label": 0
                },
                {
                    "sent": "The wires in the retina, so this can also be done.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that is technically so difficult, we can only do it for our network of several 10s, where maybe 20 neurons, and then it's much less clear.",
                    "label": 0
                },
                {
                    "sent": "This sort of scaling that we have to wait for the new experiment to 200 plus.",
                    "label": 0
                },
                {
                    "sent": "Which would like to do in that setup is for now out of reach.",
                    "label": 0
                },
                {
                    "sent": "From what we can do, right?",
                    "label": 0
                },
                {
                    "sent": "So that we this would be great, right?",
                    "label": 0
                },
                {
                    "sent": "If we did large network both with stimulus and the.",
                    "label": 0
                },
                {
                    "sent": "I think we're getting there, but it will take some time.",
                    "label": 0
                },
                {
                    "sent": "This will be just the question which is not related so much to actual evidence I had, but it just came to my mind.",
                    "label": 0
                },
                {
                    "sent": "Is there any photo receptors on the ganglion or or brain site?",
                    "label": 0
                },
                {
                    "sent": "Not that I know of.",
                    "label": 0
                },
                {
                    "sent": "No, I didn't know about that.",
                    "label": 0
                },
                {
                    "sent": "But what about holography?",
                    "label": 0
                },
                {
                    "sent": "How does that refer to what you're talking about holography in the in the holographic images of something which is so so, so?",
                    "label": 0
                },
                {
                    "sent": "I mean, there are two meanings right there.",
                    "label": 0
                },
                {
                    "sent": "Is the holography as as as of people invoke it in or science as you can reconstruct?",
                    "label": 0
                },
                {
                    "sent": "I mean that somehow information is encoded dispersedly across many parts of the brain.",
                    "label": 0
                },
                {
                    "sent": "Or you mean the actual holography?",
                    "label": 0
                },
                {
                    "sent": "How you produce by interference will produce the images.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think there is any any direct link from this to that, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's at least not not an obvious one.",
                    "label": 0
                },
                {
                    "sent": "I have a car hardware question.",
                    "label": 0
                },
                {
                    "sent": "Could you show the sensor at this electrodes that you had showed?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I must mention that Interestingly enough, the other group that's yeah.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you should go ahead with the question.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah sorry right now, let me understand.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying is that you're on the electrode site.",
                    "label": 0
                },
                {
                    "sent": "You cover the whole exit from the retina, so that basically, if I understood correctly, basically each output from the retina is covered by at least one electrode.",
                    "label": 0
                },
                {
                    "sent": "The I have to correct it there so.",
                    "label": 0
                },
                {
                    "sent": "The retina is this big.",
                    "label": 0
                },
                {
                    "sent": "You cover a little piece of it, but within that little piece your statement is correct.",
                    "label": 0
                },
                {
                    "sent": "Every neuron is seen by many elect by about 9 electrodes in this case.",
                    "label": 0
                },
                {
                    "sent": "So how do you disentangle this before you start analyzing this code?",
                    "label": 0
                },
                {
                    "sent": "I think, yeah, that's one of the hardest problems in data processing that neuroscience has, which is the so-called spike sorting problem.",
                    "label": 0
                },
                {
                    "sent": "So what this means is when a neuron sitting on top of the array makes a spike, you see the echo of that spike on about 6:00 to 9:00 electrodes.",
                    "label": 0
                },
                {
                    "sent": "And of course, each electrode listens into more than one neuron, right?",
                    "label": 0
                },
                {
                    "sent": "So what the way you do this is you make use of the fact that the spikes that neurons make are extremely stereotyped in shape, and so normal number one is making a spike of a specific shape that you see on the given electrode.",
                    "label": 0
                },
                {
                    "sent": "And that's because you know, alright, so the normal itself makes this pipe, but then the signal actually propagates across some distance in some medium to this electrode.",
                    "label": 0
                },
                {
                    "sent": "So every time these neuron spikes, it makes you record the shape that slightly different from when these other neuron spikes.",
                    "label": 0
                },
                {
                    "sent": "But talks to the same electrode, right?",
                    "label": 0
                },
                {
                    "sent": "So now you have a problem where at each point in time there is a voltage trace on your electrode and you want to represent that voltage observed voltage trace as a combination as a linear sum of voltage signals coming from putatively.",
                    "label": 0
                },
                {
                    "sent": "Whether this guys pack this guy, this guy or this guys, we have a computer problem right for every point in time there is, you know either none of them the best expressions, nobody fired.",
                    "label": 0
                },
                {
                    "sent": "Then you just have noise or this guy fire.",
                    "label": 0
                },
                {
                    "sent": "Then you see it's echo and so on.",
                    "label": 0
                },
                {
                    "sent": "Right, so so it's a rather computationally intensive inverse problem where you go through all the voltage signals and.",
                    "label": 0
                },
                {
                    "sent": "Basically, you do if you want a maximum likelihood fitting at every point in time.",
                    "label": 0
                },
                {
                    "sent": "What is the most likely combination of neurons that spike to give rise to the electronic voltage signals?",
                    "label": 0
                },
                {
                    "sent": "I see on the on the array?",
                    "label": 0
                },
                {
                    "sent": "And the really hard part is when these things overlap, right?",
                    "label": 0
                },
                {
                    "sent": "So neurons like to spike in synchrony.",
                    "label": 0
                },
                {
                    "sent": "So that means that the exactly those combinatorial events that we are studying you have to get correct, right?",
                    "label": 0
                },
                {
                    "sent": "'cause otherwise you get an interesting comment I had is that the second group that manufactured disarray was.",
                    "label": 0
                },
                {
                    "sent": "This was a known collaboration with high energy experimental physicists.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So Littke was experimental physicist at I think at certain.",
                    "label": 0
                },
                {
                    "sent": "Actually, who then went to neuroscience to design one of these things, right?",
                    "label": 0
                },
                {
                    "sent": "Kind of an interesting link.",
                    "label": 0
                },
                {
                    "sent": "Daniel dennett in his book on Free will.",
                    "label": 0
                },
                {
                    "sent": "Poses a, in my opinion, the first realistic model of.",
                    "label": 0
                },
                {
                    "sent": "Forming some decision in the neural system from scratch.",
                    "label": 0
                },
                {
                    "sent": "Because all the all such models that we read about from classical psychology were always implying some little man in a box somewhere.",
                    "label": 0
                },
                {
                    "sent": "It really is hard to to to formulate a model for an autonomic decision to come out of the system.",
                    "label": 0
                },
                {
                    "sent": "Well, he he saw the kind of.",
                    "label": 0
                },
                {
                    "sent": "Growth of some, some avatars or whatever.",
                    "label": 0
                },
                {
                    "sent": "He says that that do some darbinyan battle and one comes out and he's the bearer of the decision I see.",
                    "label": 0
                },
                {
                    "sent": "There is some connection in in in your model with this flat regions with many local minima.",
                    "label": 0
                },
                {
                    "sent": "This is a kind of.",
                    "label": 0
                },
                {
                    "sent": "Physical basis which could.",
                    "label": 0
                },
                {
                    "sent": "Somehow leads to this.",
                    "label": 0
                },
                {
                    "sent": "Then it's avatars yes.",
                    "label": 0
                },
                {
                    "sent": "So so to give a comment, I think the link comes from the fact that.",
                    "label": 0
                },
                {
                    "sent": "So this this, if they exist, right?",
                    "label": 0
                },
                {
                    "sent": "These basins of attraction in this energy landscape, of course, is not.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is not some random thing, right?",
                    "label": 0
                },
                {
                    "sent": "The thinking is as follows, right?",
                    "label": 0
                },
                {
                    "sent": "You have a system that has both evolved, which implies a certain type of wiring in the system.",
                    "label": 0
                },
                {
                    "sent": "But it has also learned like the brain learns by exposure to natural stimuli to the kind of environment that it grows in.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "It has been exposed to and it has evolved in response to certain statistics.",
                    "label": 0
                },
                {
                    "sent": "And that's the link to studying the natural statistics, those statistics.",
                    "label": 0
                },
                {
                    "sent": "Shape.",
                    "label": 0
                },
                {
                    "sent": "This landscape, the bottoms of the landscape.",
                    "label": 0
                },
                {
                    "sent": "Our hypothesis about what could have possibly happened.",
                    "label": 0
                },
                {
                    "sent": "You know the stuff that could have possibly happened outside and now the computation in this physical senses then viewed as some ambiguous stimulus comes, and what the stimulus that does it takes.",
                    "label": 0
                },
                {
                    "sent": "This landscape of possibilities of prior landscape of possibilities and tilted by evidence in this or that way, to put the system into this or that attractor right?",
                    "label": 0
                },
                {
                    "sent": "And it basically I mean we know that the outside world is very high dimensional if you look at images, right?",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "My number of pixels on its high dimension is noisy, but actually the number of possible things happening outside.",
                    "label": 0
                },
                {
                    "sent": "They still much less than the full dimension of all the pixels and light intensities and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So you want to somehow have a system that says, well, when I see this snapshot of photons, you know that's a dog, right?",
                    "label": 0
                },
                {
                    "sent": "And that comes at me now simplifying, but that's what you want the system to do, right?",
                    "label": 0
                },
                {
                    "sent": "And on the longtime skeleton evolution is shaping this landscape and then the short term dynamics is putting your ball in the correct in the correct attractor.",
                    "label": 0
                },
                {
                    "sent": "So that in that case there is this link.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that this link has now neuroscience data driven things from neuroscience have started to converge with people working in machine learning, right?",
                    "label": 0
                },
                {
                    "sent": "And people in machine learning try to figure out well, given some very rich data set in an unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "So when you don't have an answer, you don't have a classification problem, we just get this very rich data set that you want to model.",
                    "label": 0
                },
                {
                    "sent": "How do you do it right?",
                    "label": 0
                },
                {
                    "sent": "And if there is anyone from machine learning in this Community, of course inverse Ising models are the Boltzmann machines of machine learning, right?",
                    "label": 0
                },
                {
                    "sent": "So they call it differently.",
                    "label": 0
                },
                {
                    "sent": "There is different tools, but.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's exactly the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "Seems to me that there are no further questions, so let's thank prosodic again.",
                    "label": 0
                }
            ]
        }
    }
}