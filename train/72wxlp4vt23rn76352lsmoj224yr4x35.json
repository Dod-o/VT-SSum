{
    "id": "72wxlp4vt23rn76352lsmoj224yr4x35",
    "title": "Learning All Optimal Policies with Multiple Criteria",
    "info": {
        "author": [
            "Leon Barrett, UC Berkeley"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_barrett_lao/",
    "segmentation": [
        [
            "So hi, I'm Leon Barrett.",
            "I did this work at UC Berkeley with Doctor Sweeney Ryan."
        ],
        [
            "So multi criteria learning is where you take ordinary rewards and split them into components.",
            "You combine the components into single reward scalar with some sort of preference vector, some sort of weights and as your preference changes the optimal policy changes too because the combined reward changes.",
            "So existing techniques.",
            "Keep a summary so when your when your weight change when your preferences change, you don't have to start at square one.",
            "You can Start learning based on some existing information, but you still have to relearn.",
            "I have an algorithm that solves for all the optimal policies for all the preferences all at once.",
            "That means you can change your policy and your preferences online without any re learning and you can get a view of policy space which lets you.",
            "Choose the behaviors you want rather than just tweaking preferences."
        ],
        [
            "I'm sure by this point you are.",
            "Quite comfortable with reinforcement learning, but let me say what I think you need to know for this talk which is.",
            "The important thing is that you have these summaries Q&V and these summaries are sufficient for to calculate the policy.",
            "You summarize the expected reward.",
            "Looking forward from this from a particular state or particular action.",
            "You have the bellman recurrence that allows you to define those summaries in terms of the reward and you can solve those in any number of ways.",
            "Q Learning value iteration, whatever."
        ],
        [
            "Ha.",
            "And.",
            "When you define an environment, you one issue is you have to state the relative rewards right.",
            "You have to say you know.",
            "You get one unit of reward for acquiring gold and 10 units of reward for acquiring gems or whatever and.",
            "Are there arbitrary choices?",
            "Maybe you twiddle them to get some behavior, but I say make it explicit you.",
            "You say that you got a reward vector.",
            "You got one.",
            "This is 1 unit of.",
            "Gold reward this is one year of gem reward.",
            "This is 1 unit of being attacked.",
            "Penalty right and your weight and then you combine them by having a weighted sum so this your weight vector would be 110 negative 50 and that standard for multicriteria enforcement learning."
        ],
        [
            "But now that we have a."
        ],
        [
            "Now that we have these reward vectors that, then we can look at this and."
        ],
        [
            "Metric space, right?",
            "So this is.",
            "Let's look at the 1D case first.",
            "This is the rewards for one value function, right for one state, or for one action.",
            "Each point represents the value for a different policy, right?",
            "So the optimal policy is of course, the one that gives the highest reward, but in general there are all these policies in the."
        ],
        [
            "Higher dimensional case.",
            "You get each policy gives an expected reward.",
            "In more dimensions.",
            "So this corresponds to a policy that gives this amount of reward the expected discounted reward one, and this amount of.",
            "Expected discounted reward too, right?",
            "And.",
            "So you have this cloud of possible rewards."
        ],
        [
            "And if you know your weight vector well, you just want to take the point that's farthest along that weight vector, right?",
            "You can compress this down to the 1D case by projecting onto the line defined by the weight vector by taking the dot product and then you just need the maximum.",
            "But as your preferences change, then your weight vector sweeps and different policies become optimal and you can see that some of these policies are never going to be optimal.",
            "There dominated under no weight vectors where you ever choose."
        ],
        [
            "That policy, So what we actually want are the set of policies that are farthest in any given direction in any direction.",
            "That's the convex Hull and we can throw away everything else in the middle.",
            "So in ordinary reinforcement learning, you keep as your summary as your value function, you keep a maximum.",
            "But what I'm saying you should do instead is keep the convex Hull, which is equivalent to Maxima in all directions.",
            "So if we're keeping the and, then we can actually define the bellman if we define operations on convex hulls.",
            "We can make the Bellman equations operate on convex hulls dere."
        ],
        [
            "Convex hulls with a circle over just as you denote vectors with an arrow, and so these are this ordinary Bellman equations, right?",
            "So that the value of a state is usually the maximum.",
            "Over the actions of the Q values for that for the actions from that state right, you take them.",
            "You want the best action.",
            "Well in pulse.",
            "In the case of hulls you want to take the whole overall those points, right?",
            "Similarly, you can multiply a whole by a scalar by multiplying all the points by a scalar, right?",
            "You can scale the points.",
            "You can translate all the points and expectation is a little harder, but it's basically scaling.",
            "In addition, you're adding holes.",
            "So instead of backing up Maxima, instead of keeping just one number for each value function, you keep a whole foreach value function, and so you back up these holes and combine them.",
            "I.",
            "Just like normal."
        ],
        [
            "Ah, so once you've got these value function, what these holes?",
            "How do you suppose you have a weight vector?",
            "How do you actually get the policy?",
            "Well, the policy corresponds to choosing the best action, so we just have to calculate the values of the actions.",
            "So all we have to do is take a maximum over the points on the Hull.",
            "Our of and that gives us the value of each action, so we're just taking this Mac.",
            "It turns out that you can prove that the resulting policies are identical.",
            "If you've done reinforcement learning with that weight vector from the very beginning.",
            "There's a proof in the paper, but the intuition is pretty simple.",
            "The point has its farthest in that direction has to be on the whole, because that's the definition of a convex alright, it contains all the points in in farthest in that direction, and that because that holds at each step recursively, it must hold true at every iteration.",
            "So."
        ],
        [
            "You can you've got these Bellman equations and you can still solve them through any method you like in the paper.",
            "I prove it works for value iteration, but in general Q learning or any off policy method will work.",
            "It has to be off policy because you're learning more than one policy."
        ],
        [
            "So.",
            "Just as an example, here's a resource collection world, right?",
            "So you have an agent that moves around deterministically in this grid world, and it wants to collect one or both resources and bring them back to the home base.",
            "But as a complication, there are some enemies that work stochastically.",
            "You might pass safely through an enemy, or you might have your resources taken away and be sent home.",
            "Right, so you can see that there will be some different optimal policies you might want if all you value is gold into the enemy.",
            "But if you do care about the enemy might go around, or maybe a lot."
        ],
        [
            "Both right?",
            "And indeed in reward space.",
            "You get a set of optimal rewards that looks like this, right?",
            "So that if all you care about is gold, then you'll choose this."
        ],
        [
            "Path this light blue path, which goes straight through the enemy to the Golden back, whereas if you care about the enemy also then you'll go around.",
            "You'll notice that I don't highlight these policies.",
            "These points over here.",
            "That's because there they correspond to policies that are only optimal if you value being attacked by the enemy, right?",
            "So there's a policy over here that's just go and be attacked.",
            "You can actually eliminate those during the computation, but I chose to eliminate them afterwards that you can see the whole thing."
        ],
        [
            "And of course there's a dual of this reward space that is policy space.",
            "Each of these points, that's potential reward corresponds to a region that's a potential or the region preference space in which you would choose that policy.",
            "That gives that reward.",
            "I.",
            "And again, you can see you know if you prefer gold, then you choose that one, and as you prefer different things than your the policy select will change."
        ],
        [
            "The price you pay is complexity.",
            "It's exponential in the dimension of the problem.",
            "As you have more and more reward components, the complexity of your convex hulls grows exponentially.",
            "But it's only polynomial in the number of optimal policies.",
            "And especially for the low dimensional cases, for 2D, it's actually linear.",
            "You can merge two convex hulls in linear time.",
            "So you're pretty much paying no overhead, you just get all the optimal policies in the time it would take to compute them.",
            "And there are some tricks you can play to reduce this complexity.",
            "You can constrain your weight space right.",
            "You don't have to keep, you know you don't really want to compute the policies that are masochistic that only like going and getting attacked by the enemy, right?",
            "Also there are.",
            "You can use witnesses reinforcement learning.",
            "You want your policy, your value functions to converge, and that basically means you're computing the same convex hulls over and over again.",
            "Ah, so instead of doing that, we can keep up a set of proofs that indicate that prove that.",
            "The the points are still, you know, the convex Hull structure is still correct and we don't have to rebuild the whole, we just move the points a little bit and as long as those proofs check out which we can check in polynomial time, we're fine only in the case where that fails we have to rebuild the whole."
        ],
        [
            "Interestingly.",
            "This is actually identical to a particular to this palm DP.",
            "If your reward is a mixture, you have a mixture of rewards.",
            "Those mixture components are your reward components and the variable that defines that mixture.",
            "It's distribution.",
            "Gives you a some convex convex combination of rewards and so its distribution is exactly like your.",
            "Weight vector, your preferences, and so ordinary palm DP methods will actually give you this same algorithm, but more importantly, this shows exactly how you can extend.",
            "This method to arbitrary palm DPS."
        ],
        [
            "So again, what's this good for?",
            "Well, you get all the optimal policies all at once.",
            "You can change your fault.",
            "Then you can change your weights on the fly and instead of tuning your preferences to get your desired behavior right, instead of tuning your preferences so that your helicopter doesn't take the safe path and just sit there doesn't take the risky path and crash, but does something interesting in between.",
            "Instead of tweaking things, you can just look into policy space and choose the behavior you want."
        ],
        [
            "And there are some extensions, although I describe how you can combine it with Palm DP's I haven't actually done it.",
            "There's an interesting inverse problem that is suppose you have an optimal agent and you see it perform some actions.",
            "Then you can determine what it's possible weights might be.",
            "It basically comes down to a Intersect a set intersection problem in weight space.",
            "Also.",
            "Humans don't seem to have exponential discounting behavior.",
            "We value close things more than you would expect, and far things less than you would expect.",
            "That's about equivalent to having a that can be approximated by having a fast falloff reward function and a slow falloff reward function so the you can actually have a vector of discounting rates so that your rewards get discounted differently, and this will give you all the optimal policies.",
            "As the shape of your.",
            "Reward discounting changes."
        ],
        [
            "And I'd like to thank, in particular my Guinea pigs who made it into the paper as a motivating example.",
            "Self, thank you.",
            "Question.",
            "Transitions you know you talked about different discounts.",
            "You do work for different rewards.",
            "Is it something considering is do you think it's possible?",
            "Space mixture of different transition models that you would solve all together.",
            "I haven't thought about that at all.",
            "I had thought of the I had sort of been thinking of the preferences as being something in the agent's head.",
            "You know what it cares about and not something about the world itself but.",
            "I have to think about it and we can just talk offline.",
            "I have two questions actually.",
            "First one, I'm surprised you didn't talk about greater optimality, and I'm wondering why you maintain the whole convex Hull.",
            "Would it be more natural to just maintain the creator front and then you wouldn't have to throw away these masochistic well?",
            "So just wondering how you think this approach would compare to a policy search approach where you could use many multi objective optimization methods.",
            "Just search for the best policy.",
            "OK, so first I actually do mention that you can constrain your weight preference space.",
            "That is, you can say I only want Paul.",
            "I only want to worry about policies that are optimal in the you know in positive rewards.",
            "That's right, and yes, the positive paradeau front.",
            "And the second thing was.",
            "Can you put a lot of I got a little distracted.",
            "Can you say this one?",
            "Where you take a single given a single preference, you do some learning and get a new policy.",
            "So that we mean no.",
            "Optimally, definition again, find it.",
            "OK, so yes.",
            "I mean, if you're going to search if you're going to look for another if you're going to take another wait and look for another optimal policy.",
            "Then you're essentially going to be sampling in weight space.",
            "This gives you.",
            "Again, this gets all the policies guaranteed, and you want you don't have to do any sampling.",
            "It only does work proportional to the number of policies you got.",
            "There are other ways to build your front.",
            "There are other ways to compute it, right?",
            "You can inflate a convex whole similarly right?",
            "You can take a vector and keep growing in different keep growing perpendicular to the face facets.",
            "And thus inflate your preato front, inflate your convex Hull, but.",
            "This is complicated when you have so many when you're trying to keep the convex things for each.",
            "For each state in each action.",
            "So there are different ways to build it, but this one is.",
            "Apple.",
            "Have a quick question, nobody else.",
            "Go ahead.",
            "How complicated your confession.",
            "So.",
            "I I don't know, it will certainly depend on the structure of your problem.",
            "I don't know if there are any provable results for all Palm DP's in general, but convex holes in general can become can have exponentially many facets.",
            "So there may be something there I don't know.",
            "And is the number of optimal policies.",
            "I. Um?",
            "You know, I believe you're right so that so that depending on the definition of your environment, you could have.",
            "Your you could have each state.",
            "As you change your weight vector, you could have each state change.",
            "D times so you get an exponential number of optimal policies, and so I guess this won't work for some domains.",
            "But, but again, for the you'll get an interesting set of policies for only even the 2D case, right?",
            "Even in 2D you'll get interesting tradeoffs between.",
            "Different results and it's linear into."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hi, I'm Leon Barrett.",
                    "label": 0
                },
                {
                    "sent": "I did this work at UC Berkeley with Doctor Sweeney Ryan.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So multi criteria learning is where you take ordinary rewards and split them into components.",
                    "label": 0
                },
                {
                    "sent": "You combine the components into single reward scalar with some sort of preference vector, some sort of weights and as your preference changes the optimal policy changes too because the combined reward changes.",
                    "label": 0
                },
                {
                    "sent": "So existing techniques.",
                    "label": 0
                },
                {
                    "sent": "Keep a summary so when your when your weight change when your preferences change, you don't have to start at square one.",
                    "label": 0
                },
                {
                    "sent": "You can Start learning based on some existing information, but you still have to relearn.",
                    "label": 0
                },
                {
                    "sent": "I have an algorithm that solves for all the optimal policies for all the preferences all at once.",
                    "label": 1
                },
                {
                    "sent": "That means you can change your policy and your preferences online without any re learning and you can get a view of policy space which lets you.",
                    "label": 0
                },
                {
                    "sent": "Choose the behaviors you want rather than just tweaking preferences.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sure by this point you are.",
                    "label": 0
                },
                {
                    "sent": "Quite comfortable with reinforcement learning, but let me say what I think you need to know for this talk which is.",
                    "label": 0
                },
                {
                    "sent": "The important thing is that you have these summaries Q&V and these summaries are sufficient for to calculate the policy.",
                    "label": 0
                },
                {
                    "sent": "You summarize the expected reward.",
                    "label": 0
                },
                {
                    "sent": "Looking forward from this from a particular state or particular action.",
                    "label": 0
                },
                {
                    "sent": "You have the bellman recurrence that allows you to define those summaries in terms of the reward and you can solve those in any number of ways.",
                    "label": 0
                },
                {
                    "sent": "Q Learning value iteration, whatever.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When you define an environment, you one issue is you have to state the relative rewards right.",
                    "label": 0
                },
                {
                    "sent": "You have to say you know.",
                    "label": 0
                },
                {
                    "sent": "You get one unit of reward for acquiring gold and 10 units of reward for acquiring gems or whatever and.",
                    "label": 0
                },
                {
                    "sent": "Are there arbitrary choices?",
                    "label": 0
                },
                {
                    "sent": "Maybe you twiddle them to get some behavior, but I say make it explicit you.",
                    "label": 0
                },
                {
                    "sent": "You say that you got a reward vector.",
                    "label": 0
                },
                {
                    "sent": "You got one.",
                    "label": 0
                },
                {
                    "sent": "This is 1 unit of.",
                    "label": 0
                },
                {
                    "sent": "Gold reward this is one year of gem reward.",
                    "label": 0
                },
                {
                    "sent": "This is 1 unit of being attacked.",
                    "label": 0
                },
                {
                    "sent": "Penalty right and your weight and then you combine them by having a weighted sum so this your weight vector would be 110 negative 50 and that standard for multicriteria enforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now that we have a.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we have these reward vectors that, then we can look at this and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metric space, right?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the 1D case first.",
                    "label": 0
                },
                {
                    "sent": "This is the rewards for one value function, right for one state, or for one action.",
                    "label": 1
                },
                {
                    "sent": "Each point represents the value for a different policy, right?",
                    "label": 0
                },
                {
                    "sent": "So the optimal policy is of course, the one that gives the highest reward, but in general there are all these policies in the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Higher dimensional case.",
                    "label": 0
                },
                {
                    "sent": "You get each policy gives an expected reward.",
                    "label": 1
                },
                {
                    "sent": "In more dimensions.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds to a policy that gives this amount of reward the expected discounted reward one, and this amount of.",
                    "label": 0
                },
                {
                    "sent": "Expected discounted reward too, right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So you have this cloud of possible rewards.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you know your weight vector well, you just want to take the point that's farthest along that weight vector, right?",
                    "label": 0
                },
                {
                    "sent": "You can compress this down to the 1D case by projecting onto the line defined by the weight vector by taking the dot product and then you just need the maximum.",
                    "label": 0
                },
                {
                    "sent": "But as your preferences change, then your weight vector sweeps and different policies become optimal and you can see that some of these policies are never going to be optimal.",
                    "label": 0
                },
                {
                    "sent": "There dominated under no weight vectors where you ever choose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That policy, So what we actually want are the set of policies that are farthest in any given direction in any direction.",
                    "label": 0
                },
                {
                    "sent": "That's the convex Hull and we can throw away everything else in the middle.",
                    "label": 0
                },
                {
                    "sent": "So in ordinary reinforcement learning, you keep as your summary as your value function, you keep a maximum.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying you should do instead is keep the convex Hull, which is equivalent to Maxima in all directions.",
                    "label": 0
                },
                {
                    "sent": "So if we're keeping the and, then we can actually define the bellman if we define operations on convex hulls.",
                    "label": 0
                },
                {
                    "sent": "We can make the Bellman equations operate on convex hulls dere.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convex hulls with a circle over just as you denote vectors with an arrow, and so these are this ordinary Bellman equations, right?",
                    "label": 0
                },
                {
                    "sent": "So that the value of a state is usually the maximum.",
                    "label": 0
                },
                {
                    "sent": "Over the actions of the Q values for that for the actions from that state right, you take them.",
                    "label": 0
                },
                {
                    "sent": "You want the best action.",
                    "label": 0
                },
                {
                    "sent": "Well in pulse.",
                    "label": 0
                },
                {
                    "sent": "In the case of hulls you want to take the whole overall those points, right?",
                    "label": 0
                },
                {
                    "sent": "Similarly, you can multiply a whole by a scalar by multiplying all the points by a scalar, right?",
                    "label": 0
                },
                {
                    "sent": "You can scale the points.",
                    "label": 0
                },
                {
                    "sent": "You can translate all the points and expectation is a little harder, but it's basically scaling.",
                    "label": 0
                },
                {
                    "sent": "In addition, you're adding holes.",
                    "label": 0
                },
                {
                    "sent": "So instead of backing up Maxima, instead of keeping just one number for each value function, you keep a whole foreach value function, and so you back up these holes and combine them.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Just like normal.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ah, so once you've got these value function, what these holes?",
                    "label": 0
                },
                {
                    "sent": "How do you suppose you have a weight vector?",
                    "label": 0
                },
                {
                    "sent": "How do you actually get the policy?",
                    "label": 0
                },
                {
                    "sent": "Well, the policy corresponds to choosing the best action, so we just have to calculate the values of the actions.",
                    "label": 0
                },
                {
                    "sent": "So all we have to do is take a maximum over the points on the Hull.",
                    "label": 0
                },
                {
                    "sent": "Our of and that gives us the value of each action, so we're just taking this Mac.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can prove that the resulting policies are identical.",
                    "label": 0
                },
                {
                    "sent": "If you've done reinforcement learning with that weight vector from the very beginning.",
                    "label": 0
                },
                {
                    "sent": "There's a proof in the paper, but the intuition is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "The point has its farthest in that direction has to be on the whole, because that's the definition of a convex alright, it contains all the points in in farthest in that direction, and that because that holds at each step recursively, it must hold true at every iteration.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can you've got these Bellman equations and you can still solve them through any method you like in the paper.",
                    "label": 0
                },
                {
                    "sent": "I prove it works for value iteration, but in general Q learning or any off policy method will work.",
                    "label": 0
                },
                {
                    "sent": "It has to be off policy because you're learning more than one policy.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just as an example, here's a resource collection world, right?",
                    "label": 0
                },
                {
                    "sent": "So you have an agent that moves around deterministically in this grid world, and it wants to collect one or both resources and bring them back to the home base.",
                    "label": 0
                },
                {
                    "sent": "But as a complication, there are some enemies that work stochastically.",
                    "label": 0
                },
                {
                    "sent": "You might pass safely through an enemy, or you might have your resources taken away and be sent home.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can see that there will be some different optimal policies you might want if all you value is gold into the enemy.",
                    "label": 0
                },
                {
                    "sent": "But if you do care about the enemy might go around, or maybe a lot.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both right?",
                    "label": 0
                },
                {
                    "sent": "And indeed in reward space.",
                    "label": 1
                },
                {
                    "sent": "You get a set of optimal rewards that looks like this, right?",
                    "label": 0
                },
                {
                    "sent": "So that if all you care about is gold, then you'll choose this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Path this light blue path, which goes straight through the enemy to the Golden back, whereas if you care about the enemy also then you'll go around.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that I don't highlight these policies.",
                    "label": 0
                },
                {
                    "sent": "These points over here.",
                    "label": 0
                },
                {
                    "sent": "That's because there they correspond to policies that are only optimal if you value being attacked by the enemy, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a policy over here that's just go and be attacked.",
                    "label": 0
                },
                {
                    "sent": "You can actually eliminate those during the computation, but I chose to eliminate them afterwards that you can see the whole thing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course there's a dual of this reward space that is policy space.",
                    "label": 1
                },
                {
                    "sent": "Each of these points, that's potential reward corresponds to a region that's a potential or the region preference space in which you would choose that policy.",
                    "label": 0
                },
                {
                    "sent": "That gives that reward.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And again, you can see you know if you prefer gold, then you choose that one, and as you prefer different things than your the policy select will change.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The price you pay is complexity.",
                    "label": 0
                },
                {
                    "sent": "It's exponential in the dimension of the problem.",
                    "label": 0
                },
                {
                    "sent": "As you have more and more reward components, the complexity of your convex hulls grows exponentially.",
                    "label": 0
                },
                {
                    "sent": "But it's only polynomial in the number of optimal policies.",
                    "label": 0
                },
                {
                    "sent": "And especially for the low dimensional cases, for 2D, it's actually linear.",
                    "label": 1
                },
                {
                    "sent": "You can merge two convex hulls in linear time.",
                    "label": 0
                },
                {
                    "sent": "So you're pretty much paying no overhead, you just get all the optimal policies in the time it would take to compute them.",
                    "label": 0
                },
                {
                    "sent": "And there are some tricks you can play to reduce this complexity.",
                    "label": 0
                },
                {
                    "sent": "You can constrain your weight space right.",
                    "label": 0
                },
                {
                    "sent": "You don't have to keep, you know you don't really want to compute the policies that are masochistic that only like going and getting attacked by the enemy, right?",
                    "label": 0
                },
                {
                    "sent": "Also there are.",
                    "label": 0
                },
                {
                    "sent": "You can use witnesses reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "You want your policy, your value functions to converge, and that basically means you're computing the same convex hulls over and over again.",
                    "label": 0
                },
                {
                    "sent": "Ah, so instead of doing that, we can keep up a set of proofs that indicate that prove that.",
                    "label": 0
                },
                {
                    "sent": "The the points are still, you know, the convex Hull structure is still correct and we don't have to rebuild the whole, we just move the points a little bit and as long as those proofs check out which we can check in polynomial time, we're fine only in the case where that fails we have to rebuild the whole.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interestingly.",
                    "label": 0
                },
                {
                    "sent": "This is actually identical to a particular to this palm DP.",
                    "label": 0
                },
                {
                    "sent": "If your reward is a mixture, you have a mixture of rewards.",
                    "label": 0
                },
                {
                    "sent": "Those mixture components are your reward components and the variable that defines that mixture.",
                    "label": 0
                },
                {
                    "sent": "It's distribution.",
                    "label": 0
                },
                {
                    "sent": "Gives you a some convex convex combination of rewards and so its distribution is exactly like your.",
                    "label": 0
                },
                {
                    "sent": "Weight vector, your preferences, and so ordinary palm DP methods will actually give you this same algorithm, but more importantly, this shows exactly how you can extend.",
                    "label": 0
                },
                {
                    "sent": "This method to arbitrary palm DPS.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, what's this good for?",
                    "label": 0
                },
                {
                    "sent": "Well, you get all the optimal policies all at once.",
                    "label": 1
                },
                {
                    "sent": "You can change your fault.",
                    "label": 0
                },
                {
                    "sent": "Then you can change your weights on the fly and instead of tuning your preferences to get your desired behavior right, instead of tuning your preferences so that your helicopter doesn't take the safe path and just sit there doesn't take the risky path and crash, but does something interesting in between.",
                    "label": 1
                },
                {
                    "sent": "Instead of tweaking things, you can just look into policy space and choose the behavior you want.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are some extensions, although I describe how you can combine it with Palm DP's I haven't actually done it.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting inverse problem that is suppose you have an optimal agent and you see it perform some actions.",
                    "label": 0
                },
                {
                    "sent": "Then you can determine what it's possible weights might be.",
                    "label": 0
                },
                {
                    "sent": "It basically comes down to a Intersect a set intersection problem in weight space.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Humans don't seem to have exponential discounting behavior.",
                    "label": 0
                },
                {
                    "sent": "We value close things more than you would expect, and far things less than you would expect.",
                    "label": 0
                },
                {
                    "sent": "That's about equivalent to having a that can be approximated by having a fast falloff reward function and a slow falloff reward function so the you can actually have a vector of discounting rates so that your rewards get discounted differently, and this will give you all the optimal policies.",
                    "label": 0
                },
                {
                    "sent": "As the shape of your.",
                    "label": 0
                },
                {
                    "sent": "Reward discounting changes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'd like to thank, in particular my Guinea pigs who made it into the paper as a motivating example.",
                    "label": 1
                },
                {
                    "sent": "Self, thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Transitions you know you talked about different discounts.",
                    "label": 0
                },
                {
                    "sent": "You do work for different rewards.",
                    "label": 0
                },
                {
                    "sent": "Is it something considering is do you think it's possible?",
                    "label": 0
                },
                {
                    "sent": "Space mixture of different transition models that you would solve all together.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about that at all.",
                    "label": 0
                },
                {
                    "sent": "I had thought of the I had sort of been thinking of the preferences as being something in the agent's head.",
                    "label": 0
                },
                {
                    "sent": "You know what it cares about and not something about the world itself but.",
                    "label": 0
                },
                {
                    "sent": "I have to think about it and we can just talk offline.",
                    "label": 0
                },
                {
                    "sent": "I have two questions actually.",
                    "label": 0
                },
                {
                    "sent": "First one, I'm surprised you didn't talk about greater optimality, and I'm wondering why you maintain the whole convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Would it be more natural to just maintain the creator front and then you wouldn't have to throw away these masochistic well?",
                    "label": 0
                },
                {
                    "sent": "So just wondering how you think this approach would compare to a policy search approach where you could use many multi objective optimization methods.",
                    "label": 0
                },
                {
                    "sent": "Just search for the best policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so first I actually do mention that you can constrain your weight preference space.",
                    "label": 0
                },
                {
                    "sent": "That is, you can say I only want Paul.",
                    "label": 0
                },
                {
                    "sent": "I only want to worry about policies that are optimal in the you know in positive rewards.",
                    "label": 0
                },
                {
                    "sent": "That's right, and yes, the positive paradeau front.",
                    "label": 0
                },
                {
                    "sent": "And the second thing was.",
                    "label": 0
                },
                {
                    "sent": "Can you put a lot of I got a little distracted.",
                    "label": 0
                },
                {
                    "sent": "Can you say this one?",
                    "label": 0
                },
                {
                    "sent": "Where you take a single given a single preference, you do some learning and get a new policy.",
                    "label": 0
                },
                {
                    "sent": "So that we mean no.",
                    "label": 0
                },
                {
                    "sent": "Optimally, definition again, find it.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're going to search if you're going to look for another if you're going to take another wait and look for another optimal policy.",
                    "label": 0
                },
                {
                    "sent": "Then you're essentially going to be sampling in weight space.",
                    "label": 0
                },
                {
                    "sent": "This gives you.",
                    "label": 0
                },
                {
                    "sent": "Again, this gets all the policies guaranteed, and you want you don't have to do any sampling.",
                    "label": 0
                },
                {
                    "sent": "It only does work proportional to the number of policies you got.",
                    "label": 0
                },
                {
                    "sent": "There are other ways to build your front.",
                    "label": 0
                },
                {
                    "sent": "There are other ways to compute it, right?",
                    "label": 0
                },
                {
                    "sent": "You can inflate a convex whole similarly right?",
                    "label": 0
                },
                {
                    "sent": "You can take a vector and keep growing in different keep growing perpendicular to the face facets.",
                    "label": 0
                },
                {
                    "sent": "And thus inflate your preato front, inflate your convex Hull, but.",
                    "label": 0
                },
                {
                    "sent": "This is complicated when you have so many when you're trying to keep the convex things for each.",
                    "label": 0
                },
                {
                    "sent": "For each state in each action.",
                    "label": 0
                },
                {
                    "sent": "So there are different ways to build it, but this one is.",
                    "label": 0
                },
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "Have a quick question, nobody else.",
                    "label": 0
                },
                {
                    "sent": "Go ahead.",
                    "label": 0
                },
                {
                    "sent": "How complicated your confession.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I I don't know, it will certainly depend on the structure of your problem.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there are any provable results for all Palm DP's in general, but convex holes in general can become can have exponentially many facets.",
                    "label": 0
                },
                {
                    "sent": "So there may be something there I don't know.",
                    "label": 0
                },
                {
                    "sent": "And is the number of optimal policies.",
                    "label": 0
                },
                {
                    "sent": "I. Um?",
                    "label": 0
                },
                {
                    "sent": "You know, I believe you're right so that so that depending on the definition of your environment, you could have.",
                    "label": 0
                },
                {
                    "sent": "Your you could have each state.",
                    "label": 0
                },
                {
                    "sent": "As you change your weight vector, you could have each state change.",
                    "label": 0
                },
                {
                    "sent": "D times so you get an exponential number of optimal policies, and so I guess this won't work for some domains.",
                    "label": 0
                },
                {
                    "sent": "But, but again, for the you'll get an interesting set of policies for only even the 2D case, right?",
                    "label": 0
                },
                {
                    "sent": "Even in 2D you'll get interesting tradeoffs between.",
                    "label": 0
                },
                {
                    "sent": "Different results and it's linear into.",
                    "label": 0
                }
            ]
        }
    }
}