{
    "id": "ur7xydehwyhszpqcvanlxw2rhbrghxes",
    "title": "Optimization Algorithms in Support Vector Machines",
    "info": {
        "author": [
            "Stephen J. Wright, Computer Sciences Department, University of Wisconsin-Madison"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlss09us_wright_oasvm/",
    "segmentation": [
        [
            "From the University of Wisconsin.",
            "Thank you very much brother and thanks to the organizers also for inviting me to this terrific event.",
            "I'm going to be giving a bit of an optimization perspective on the area of support vector machines.",
            "Of course, I'm kind of an outsider in this area, but I want to recognize that."
        ],
        [
            "In the last 1215 years, people in this community have made tremendous contributions in.",
            "In using optimization methods and in developing new optimization methods to solve the many, many problems that arise in machine learning computational learning.",
            "And I want to review some of those with an emphasis, particularly on sort of the classical classic SVM classification type problems, and I'll talk a little bit at the end about some newer developments in optimization that are.",
            "I think particularly relevant to certain formulations of SVM problems, and I'll talk about how some of them are being applied in those problems, but a lot of the talk is kind of is sort of review of what's being done and.",
            "And kind of representing things to some extent from from from an optimization perspective, putting things into an optimization frame."
        ],
        [
            "Work.",
            "OK, so the first thing I want to notice that problems in this area are really hard.",
            "Not in the sense that they've got incredible nonlinearity or non convexity, but in the sense that they are often very large, you've gotta deal often with a kernel matrix that's very large and possibly all condition and just the expense of evaluating a function.",
            "Just if you want to evaluate a loss function over all the data that could be intractable or very difficult if you use all the data at once.",
            "To repeat what I said a moment ago, this community is really made excellent use of optimization technology.",
            "They've adapted fundamental algorithms in very interesting ways that exploit the structure of the problem and kind of customized to the requirements of the applications.",
            "However, this community is always throwing up new formulations that kind of thinking up new ways to to formulate problems, and they're throwing up whole new classes of problems, like manifold learning and so on.",
            "They will present new challenges and in some cases much harder problem.",
            "Then we had to deal with before.",
            "For example, semi supervised learning as you know, depending on which formulation you use, it can be a common tutorial problem with 01 variables it can be nonconvex.",
            "You can really need to use global optimization techniques to find decent answers and then Lastly."
        ],
        [
            "I point out there are some new strands of work in optimization that might be useful in this area, so I want to start off kind of at a more abstract level, and just to point out that machine learning is 1 instance of a whole class of a whole set of new paradigm and optimization that's capturing a lot of interest, and that's the area of sparser regularised optimization.",
            "Anas, I'll give a number of examples to point out that these kinds of problems are happening all over the place, so I mean traditional work in optimization.",
            "People have assumed that the data that formulation is exact.",
            "And let you want an exact answer.",
            "You know that was kind of the name of the game, but it's being realized that in many, many applications we're much happier with an approximate answer.",
            "That's in some sense simple.",
            "Simple manifested in, you know ways like sparsity.",
            "So what are some reasons for this?",
            "Well, they differ of course according to the application, but but some reasons are that simple solutions are easy to implement or actuator in some cases they are easy to understand.",
            "If you've got uncertain data.",
            "If you're not certain about the data in the formulation, there's no point in solving the problem exactly.",
            "That's another reason.",
            "And then there's this sort of the Occam's razor thing of if you're trying to explain some phenomenon in this case by solving and.",
            "An optimization problem.",
            "The simplest explanation, that kind of the simplest explanation is the most sort of plausible one in some sense.",
            "So these ground rules kind of changed the whole decision making process about what algorithm you should use or what approaches should use, and they can change it in pretty radical ways.",
            "For instance, if you were trying to solve the problem exactly, you might go for some kind of Newton like method using higher order.",
            "His steps and higher information if you only want an exact solution, you might decide that some kind of approximate 1st order method is good."
        ],
        [
            "Tough.",
            "OK, regularize formulations many problems can be stated in this kind of form.",
            "A combination of two terms that you're trying to minimize.",
            "The first term is some kind of model term or data fitting term or loss term, and the second term is some kind of regularization.",
            "And often there's some single parameter.",
            "That balance is the two off and you want to try and minimize that combination.",
            "And sometimes you don't know what that parameter is in advance.",
            "You have to do some kind of search.",
            "Or maybe you're interested in finding approximate solutions for a whole range of values of Lambda OK, and then often if you're talking about sparse or simple solutions, this regularization term is often non smooth and for various reasons non smoothness is needed to kind of induce sparsity in the solution.",
            "For reasons that I won't explain.",
            "But you can't explain it.",
            "Sometimes in terms of."
        ],
        [
            "Duality is useful, so here's one example that's getting a tremendous amount of play in the applied math and statistical communities, and that's compressed sensing.",
            "And there are many formulations of this.",
            "This is one of the popular ones.",
            "The idea is that you've got a matrix A, which is a combination of some kind of basis matrix and maybe some sort of sensing matrix.",
            "You've got an unknown signal X.",
            "You've got some observations B and you're trying to find a sparse X that is an X with relatively few nonzeros.",
            "That explains the observations.",
            "You know, sort of our priority that that that is that B is explicable by a sparse signal.",
            "You have some information or some reason to believe that there's a sparse explanation for B.",
            "Now, under certain assumptions on a, this came up a little in the previous talk going by the name of restricted isometry or incoherence type properties in coherence between the observations and the basis, you can show that this formulation actually will give you the right answer or something close to the right answer for an appropriate choice of Lambda.",
            "OK, and this has the form that I described earlier are fitting term plus Lambda times some kind of regularization."
        ],
        [
            "Turn.",
            "Another example, it will return to later in the talk is denoising image.",
            "So if you've got a rectangular domain and some kind of image data F, you want to recover a denoised version of that image that also has a fairly low measure of total variation norm.",
            "So the total variation norm as you know it's suitable for recovering images that are sort of cartoon like where you have large areas where not much is happening, and then a few edges OK, so the total variation norm is the spatial grading of you.",
            "The norm of that integrated over the domain.",
            "OK, so again that has that same kind of form that I mentioned earlier, and as I said, I'll return."
        ],
        [
            "This problem later on, here's a much more nitty gritty kind of area that you get sparse optimization problems arising in.",
            "This is radiotherapy for cancer, so you may know that one way, one way of treating cancer is by by setting people up on an X Ray producing device, and then shooting X Rays into their body kind of training it on the tumor, which is which is inside their body, and the idea is that you use it a number of different angles.",
            "You fire the beams in from a number of different angles.",
            "So is to kind of focus them all on the tumor, but not affect too much of the surrounding tissue.",
            "So so you sort of want to spread the radiation out among the health around the healthy tissue, but have it all kind of focus on the tumor.",
            "Now, if you want to, you can overfit that problem.",
            "You can have a solution where you where you where you shoot the beam in from 25 or 30 different angles.",
            "The problem with that is it's very difficult to implement and for all sorts of reasons it's not a good idea to have the patient lying on the table for two hours.",
            "And trying to keep them still while they keep moving the head of the machine around so they much prefer solutions where you've only got three or four or five different angles.",
            "OK, so in a sense that's a sparse solution 'cause you're trying to pick a very small number of angles from among a huge range of possibilities.",
            "Not only angles in this case, but also you get to shape the beam and you get to decide how long to expose it."
        ],
        [
            "An another example matrix completion that I'm sure some of you here know about.",
            "This is a problem where you've got an unknown matrix X and you're allowed to observe certain elements of the matrix or certain linear functions of elements of the matrix.",
            "Linear combinations, and again those observations are in the vector B and you're trying to find in some sense the lowest rank or the simplest matrix ax that explains the observations.",
            "And it turns out that here in appropriate regularization term, one that's been proposed as this nuclear norm.",
            "Which is the sum of singular values of X.",
            "If you're dealing with a symmetric positive definite X, then you get the nuclear norm corresponds to the trace, which is also the sum of the eigenvalues.",
            "So you can use that instead.",
            "So I'll just point out I won't say anything more about this problem except that quite a few people are working on it, but I'll just point out that the methods that have been proposed.",
            "A very similar to compressed sensing methods, except that the linear algebra is a lot more complicated and the relationship between the two is like if you know about interior point methods for linear programming and for semidefinite programming.",
            "The analogy between those two interior point methods for those two problems is very similar to the analogy between algorithms for compressed sensing algorithms for matrix completion."
        ],
        [
            "OK, so how do you go about solving these regularised formulations well?",
            "The algorithm approaches tend to be very problem specific, and so you can only go so far with kind of abstracting approaches and, and you know, generalizing across a whole bunch of different applications.",
            "But you can do something with that.",
            "There are algorithms that work well in one class of problems that you can kind of abstract to other problems in this general area.",
            "Sparse optimization, you can make some general observations.",
            "One is that duality is often a key to getting a formulation that's useful, and that's certainly true in machine learning.",
            "Another one is that, as I mentioned earlier, you often want to solve for a range of regularization parameters, and often you have more than one parameter.",
            "You might have a number of knobs you can turn 'cause you might have multiple regularization terms, each of with each of which has its own parameter.",
            "Another sort of general observation is that often you know you've got a choice between a method that has fast asymptotic convergence where each step is relatively expensive, like an interior point method.",
            "And then there's another kind of method where each step is relatively cheap at some kind of 1st order or subgradient type method.",
            "But you need a lot of steps OK, and which of these two classes is more appropriate?",
            "Very much depends on the application and what."
        ],
        [
            "You need what kind of solution you're looking for.",
            "OK, so now I want to say spend some time talking about basic SVM and this.",
            "This really has a bit more of a review or tutorial type flavor, 'cause I'm sure that many of you have very familiar with this problem, so I'll be going over these slides fairly quickly.",
            "So I want to talk about the basic classification problem where we have feature vectors in RN.",
            "We've got an addon, we've got binary labels and we're trying to find a linear classifier that's given by a vector WNRN and some intercept B, so that we've got a linear classification function F of X, and the idea is that we learn WMB from the data and then we use.",
            "If someone comes along with a new feature vector, we can use F to classify or predict a label for that for that vector.",
            "OK, so one of the very popular ways of setting this problem up is is to use the so called hinge loss function.",
            "Where the in this case the fitting term is really the second term there.",
            "This nonce or this piecewise linear term, which is the Max.",
            "Basically you incur a penalty if XI if the feature excise on the wrong side of the hyperplane.",
            "Otherwise if it's on the right, so it is sufficiently far on the right side that second term is 0.",
            "So we sum up all the penalties from one to N, and then we add on this regularization term, which in this case is a smooth term.",
            "It's a two norm of W. OK, for reasons that this gives us the maximum margin separating hyperplane in the case we got separable data.",
            "And so on.",
            "But this is this formulation is proved to be very popular in this.",
            "See in this case plays the role of the regularization parameter.",
            "So as it's written here, this is an unconstrained piecewise quadratic function.",
            "We can write it as a convex quadratic program just by introducing some extra extra variables to take the place of each of the terms in that summation, and then introduce some linear constraints.",
            "OK, so we can turn it into a standard quadratic program, very easy."
        ],
        [
            "Easily.",
            "We can go from that standard quadratic program to it dual by just using.",
            "You know the Wolf dual from quadratic convex quadratic optimization and we can try to do like this and this is a form again that I'm sure that many of you are familiar with now the unknowns you can think over them as being LaGrange multipliers for each term in the loss function.",
            "Bounded between Zero, the offers that matter between zero and C. Because we've got an intercept, we've got an extra linear constraint.",
            "Y transpose offer equals zero, and then we've got this quadratic objective.",
            "OK, in each each term in the passion of the objective is an inner product of two of the feature vectors.",
            "OK, with the labels included as well.",
            "And you can write down KKT or Karush Kuhn Tucker conditions that relate to primal and dual solutions.",
            "And you can express the classifier.",
            "In fact, in terms of the dual solution."
        ],
        [
            "Now, one of the motivations for going to the dual formulation is that you can use the so called kernel trick.",
            "I'm sure no one calls it that anymore, but that's what it was called.",
            "10 years or so ago where you can say that, OK, I don't really need to do the classification in the original feature space.",
            "I can.",
            "I can virtually project into a high dimensional space given by this mapping fee, and I can think of each term in the kernel as being the inner product of 2 feature vectors mapped into the high dimensional space, OK?",
            "And if you do that, you can then define the classifier in the same way, But then you can make the observation that you never really actually learn need to know fee.",
            "You can just say let me replace the entries in the kernel by some positive definite function.",
            "OK, and then think of and.",
            "Then there's this theorem that says that there is it to go with each positive definite function.",
            "There is a fee.",
            "OK, roughly speaking alright, so."
        ],
        [
            "Never actually have to deal with the.",
            "We can do everything in terms of the kernel function K, and as you know there are a few popular kernels.",
            "The one that seems to be used for the two that seem to be used all the time are the first to the linear kernel and the Gaussian or radial basis function kernel.",
            "And there are a few others that are that are out there.",
            "And actually, there's been a few talks on learning kernels in this."
        ],
        [
            "In this conference.",
            "OK, so how do you go about solving these?",
            "Both of these are fairly classic optimization problems.",
            "Are convex quadratic programs, but there's no magic bullet algorithm because there are problems of many types of feature vectors can be very short or very long.",
            "There can be a huge amount of data, or they can be not so much.",
            "Your parameters in the defining the kernel matrix might make it relatively well conditioned, or it can be really terribly conditioned.",
            "So all these, and then there's this question about how accurately do you really need to solve this problem.",
            "Given that this you know this function that I've written down is just an empirical measure of risk, innocence and your aim, not really as to solve that empirical problem exactly, but to find a good classifier ultimately.",
            "OK, so all these questions kind of play."
        ],
        [
            "Say into what kind of algorithm you need and they kind of motivate the need for maybe a whole family of a whole bunch of algorithms.",
            "The whole smorgasbord of algorithms to choose from.",
            "OK, so here's the deal.",
            "Formulation.",
            "The thing that I would say makes this most tricky, besides its size is the is the fact that the Hessian is dense and or condition, so one of the first things people trade with some kind of interior point method back in the 90s.",
            "When interior point methods were kind of all the rage and you know I've tried that on this problem, and one of the things that hurt you is is the fact that the Hessian is ill conditioned.",
            "So you end up having to solve at each interior point iteration.",
            "If you do a naive implementation, you end up having to solve a very ill conditioned linear system.",
            "Any other thing about this is that if you insist on including an intercept term, then you've got this linear constraint, which is really a nuisance.",
            "You know dealing with bound constraints is easy, but when you intersect about constraint with the hype."
        ],
        [
            "Klein had becomes a bit of a pain.",
            "OK so here is 1 possible approach is that you can just throw you forget about the Intercept.",
            "Let's just look for a classifier that that doesn't have an intercept and then we just got a nice relatively nice bound constrained QP.",
            "And people have proposed various things for this.",
            "Here is a proposal of actually very recent, one of a coordinate descent method where you just pick an element of Alpha and take some kind of a step in that.",
            "Fix all the others.",
            "OK, you can cycle through the alphas or you can maybe pick them at random, but the point is to take a step in in and in a direction Alpha.",
            "You don't need to know very much about the kernel.",
            "You certainly don't need to have it all evaluated and stored.",
            "OK so each step of this method is pretty cheap at storage requirements are pretty modest, but it probably needs an awful lot of steps."
        ],
        [
            "Maybe maybe it's that's OK. Maybe you can find an ear and ear solution fairly efficiently.",
            "Here's another fairly recent proposal, actually from some people in the optimization community, and that is to leave the Intercept term in there.",
            "But again, just use methods from gradient projection.",
            "In other words, make each step some step along the negative gradient direction, but but each time you take a step, you have to project it back onto this feasible set and the feasible set is now the intersection of the box with the hyperplane.",
            "Now that projection is not trivial anymore, but it's also not too bad.",
            "You can implement it fairly efficiently, kind of as a subproblem.",
            "And some of the interesting issues are how do you choose the step length?",
            "Well, there's a.",
            "There's a technique due to buzz Lionboy, and it's not about 20 years old that has come back into fashion a little bit in the last five years or so, and the reason it's it was kind of very non mainstream is that it's typically non monotone that you often take steps that increase the objective function and this goes against the DNA of a lot of people in optimization who are used to having dissent type methods that are guaranteed to give you an improvement at every iteration.",
            "But you can show that there there's reason to think that non monotone methods by sort of sacrificing short-term improvement in the objective.",
            "They can actually get better long term performance over a whole range of iterations.",
            "And the basic idea of a buzz lowboy and step length is is in some sense it imitates the 2nd order term.",
            "If this were a Newton type method you would have the Hessian inverse times the gradient as the search direction, but he replaced the Hessian inverse with just a scalar and as long as the scalar is somewhere in the spectrum of.",
            "Of the Hessian or the Hessian inverse, and this is not an unreasonable thing to do.",
            "You can think of the scalar as being a very crude approximation to the inverse."
        ],
        [
            "Session.",
            "Decomposition strategies well.",
            "Again, if you're working in the dual space, the size of the problem is equal to the number of data points, and that can be huge.",
            "So so going right back to the 90s, people have have sort of proposed methods that just work with a few components at a time.",
            "Maybe only two components, maybe 10 or maybe 100.",
            "Maybe some kind of a sliding window where you change the.",
            "You may change the set of components you're working with at every iteration, or you may just change it periodically when you think you've kind of exhausted the current set so.",
            "All kinds of methods can be embedded in a decomposition framework and I mentioned some of them here.",
            "Gradient projection methods, even interior point methods."
        ],
        [
            "OK yeah, and live SVM.",
            "I think this is another one that's kind of.",
            "Embedded in A is sort of a decomposition framework, but you use some second or information to to to solve the low dimensional QP at each edge for each subproblem.",
            "So there are all sorts of other ideas you can bring in, like shrinking.",
            "You can sort of figure out which components of Alpha seem to be really at zero and or at their upper bound.",
            "And once you've decided that there at one of the bounds, you don't really have to worry about them too much anymore, except that you might want to check in later on to check if they really do belong there.",
            "Caching is another important kind of implementation level issue that if you evaluate some elements of the kernel, and often that's quite expensive to do, you would like to keep them around for a while in case you need them again on.",
            "Not too far down the road in a future iteration, so figuring out strategies to do that is.",
            "It is also sort of a non trivial issue.",
            "OK, so decomposition methods work OK.",
            "The problem is that in the sparse setting, particularly using something like a Gaussian kernel.",
            "The solutions Alpha, the dual solutions are often not all that sparse.",
            "There are many examples where they're not particularly sparse, and so that kind of limits how good a decomposition framework can really be, or even a shrinking type framework."
        ],
        [
            "OK, so some some other methods I'll just throw up here.",
            "Active set type methods.",
            "These are these are methods that actually do sort of pivots, and there's sometimes useful if you want to solve for an entire range of values of the of the.",
            "Of the regularization parameter, they also might be useful if you want to build up a solution incrementally, so there's a recent paper by Sheinberg where she sort of keeps adding data and then keeps figuring out how that changes the solution of the QP.",
            "So you saw the QP for a certain amount of data.",
            "Then you toss in a new data point that expands the dimension of the QP by one.",
            "You take whatever action.",
            "It's usually a fairly simple pivot is needed to adapt the solution to the new data point and continue from there.",
            "So that can be quite expensive to really if you if you're dealing with very large amounts of data.",
            "Another way of dealing with that intercept constraint away transfers Alpha constraint is to turn the problem into a min Max problem.",
            "That's another thing that's recently been proposed, and they do sort of an active set method on."
        ],
        [
            "That again, I've got a bunch of references at the end, so if you're interested in looking at.",
            "Looking at any of these, you can.",
            "I'll give you the slides later on.",
            "OK, active set methods in general, what you can say about them?",
            "As I've already said, they're good if you've got kind of you introducing the data incrementally.",
            "They're also good if you want to trace an entire solution path.",
            "'cause often when you change the value of see, you just need a few pivots of the active step method to recover the value to recover the updated values."
        ],
        [
            "In that context.",
            "They may not be completely unreasonable.",
            "OK, interior point methods I mentioned earlier that these had been tried and there's a paper in 2001 and I'm sure there are some early attempts earlier attempts, but the key, the most expensive operation.",
            "Interior Point method is to solve a linear system in which the kernel plus you've got the kernel plus a diagonal matrix.",
            "And if you literally do this, of course it really restricts the size of the problems.",
            "You can solve another problem as I said is that is the problem of conditioning that the kernel may not be well conditioned.",
            "So you can do things like precondition this and you apply some kind of iterative technique and that's that in some circumstances is not so bad, but here's a thought.",
            "What if you just replace the kernel by a low rank approximation?",
            "And do that, and that's something that we tried.",
            "Last year, just to use to take the kernel of play some kind of iterative method like an Arnoldi type method.",
            "Identify maybe the top five or ten eigenvalues.",
            "You can even there's even a command in Matlab to do this.",
            "OK I guess.",
            "Or you could use some kind of sampling method like.",
            "Mahoney may be talking about that right now, and the other talk I don't know, but in other words, to do some sort of pre preliminary computation to figure out what the leading eigenspace of K is."
        ],
        [
            "And then just substitute VV transpose for K in the dual SVM.",
            "OK and solve that problem.",
            "So how good a job does that do?",
            "Well, it turns out that if you do that, you get by making a change of substitution of variables.",
            "Gamma is V transpose Alpha.",
            "You actually get a very simple quadratic program, and if you hand it to seaplex it solves very very quickly.",
            "OK, so this is very efficient.",
            "The offer that you recover from this may not be unique, but who cares?",
            "The classifier you get out of it is well defined and how well does it do?",
            "Well, our experience wasn't so great.",
            "We believe other people have done something like this and have had better experience, and I think the trick.",
            "We're told that the trick is in choosing the.",
            "Parameters of the kernel to make sure that the full kernel itself would be a decent separator, but we were kind of surprised to find that even if we captured most of the matrix in the VV transpose and the part we were ignoring was relatively insignificant, that somehow the quality of the classifier sort of degraded fairly noticeably OK."
        ],
        [
            "OK, so let me talk a little bit about solving the primal.",
            "So this is going back to that.",
            "Original formulation where we were kind of restrict ourselves to classify as in the original linear space.",
            "No kernels for the moment, although there have been some attempts to extend these these these techniques to kernel spaces as well.",
            "So what we have here again is this last term added to a regularization term, which is the two norm term and.",
            "OK, so the."
        ],
        [
            "So there's been quite a bit of excitement in this area lately.",
            "2 main approaches.",
            "One is a kind of a cutting plane approach.",
            "So in the cutting plane approach, what you do is you look at their loss term and you realize that it's a piecewise linear function, because each of those hinge loss functions is just a two piece linear function.",
            "When you add them up.",
            "Of course you get a lot of kinks, but watch what you aim to do is to find a lower bounding piecewise linear approximation by constructing sort of hyperplanes that that are lower bounds to that loss function.",
            "And hopefully you just build it will build up the quality of your lower bounding approximation in the region that you're interested in, so you won't have to deal with the you know with the approximation far from the solution point.",
            "So this idea is kind of a classic idea in.",
            "In optimization, and it's been tried in a lot of different contexts, including stochastic optimization, integer programming, and so on.",
            "In this context, it turns out that it's particularly suitable because that's upgrading is very easy to calculate.",
            "If I give you a W, you can easily give me back a subgradient which will, from which I can construct a lower bounding hyperplanes and there been a number of papers.",
            "SPM Purf is 1 implementation of this.",
            "OK, this is another implementation.",
            "They add some line search strategy there, and they had some.",
            "Other strategy for figuring out where to evaluate the new cutting plane and so on, and you can prove various."
        ],
        [
            "Convergence and complexity results.",
            "We've tried some various enhancements of this approach, motivated by what people do in stochastic programming, namely by breaking the piecewise linear term up into chunks or bundles and generating a separate cut for each one.",
            "This gives you kind of a richer.",
            "Lower bounding piecewise linear function and thereby converging in few iterations.",
            "Although each iteration is more expensive, strategies for deleting cuts that you haven't used much recently, so that you don't get more and more cuts being generated without limit, and different heuristics for deciding how you add a cut after you take a step that that is not successful, it doesn't produce an improvement in the function, and there's probably a lot of other ideas that can be tried in this context motivated by."
        ],
        [
            "Other areas of have optimization.",
            "Here's another approach that again has been around for awhile along by two is one person that's advocated this approach, and more recently the Pegasus code of people here at TTI, is a very nice implementation of this idea.",
            "The idea is that you treat that second that summation over all the data instead of instead of getting a subgradient of that whole thing, you just sample from that some.",
            "So you pick either one term from that some or just a few terms and your computer subgradient from that.",
            "And then you take some kind of a step in that direction along that subgradient.",
            "OK, and the step could either decrease like at iteration K. It could be like 1 / K or it could decrease like 1 / sqrt K. And it turns out you can prove things about the subgradients defined there at the bottom of the at the bottom of the slide if.",
            "If you happen to pick a term in the somewhere that's already correctly classified, then there's no contribution from the second term and the subgradient is just the subgradient.",
            "The regularization term, which is W. If you do pick a term that's incorrectly classified, then it gets a little bit more complicated."
        ],
        [
            "But the idea is you don't do any kind of a line, so if you just sort of blindly take this step in that direction and you can prove quite a few interesting things about these kinds of schemes, and typically what you can prove is some kind of convergence in expectation, so you can prove that if you do the sampling uniformly that the objective value of the point that you're at is converging to the optimal objective value in expectation at some rate, like 1, /, K or 1 / sqrt K. And I mentioned a few variants of that there OK, But the interesting thing about this is this is a whole stream of research and optimization that's very closely related to what's been happening in the machine learning Community in this area, and probably the most interesting paper to look at if you're not familiar with this is a paper by nemerofsky."
        ],
        [
            "There's just appeared in some Journal of optimization where he basically talks about it exactly.",
            "These kinds of approaches in a slightly different setting, and the and the conclusion of the results that he proves there almost almost identical to the results you're seeing.",
            "The machine learning literature.",
            "So we seem to have an instance of, like you know, very imaginative, interesting class of methods being generated, sort of independently by different groups.",
            "One of the points that Nemerofsky makes it it's worthwhile even reading just the first couple of sections of his paper, is that a method that you step size is like 1 / K only can work well if you have some guess of the modulus of convexity.",
            "Now in this problem we do, we know that that first term, the one the 1/2 W squared term, we know what the modulus of convexity is, the curve it is sort of a lower bounding curvature.",
            "And if you know that there's a sensible way to choose the step length and get.",
            "1 /, K convergence of the objective and expectation.",
            "If you don't know that or if or if there is no strong convexity, then he shows that you're better off using 1 / sqrt K rule.",
            "In some sense that's more robust.",
            "That gives slower convergence and expectation, but it's less sensitive to getting the getting the estimate of of.",
            "The convexity parameter wrong.",
            "You can pay a high price if you if you give it the wrong value for the convexity parameter or 1 / K rule can give you really terrible results.",
            "That's not so much an issue in this formulation.",
            "'cause we do know what that is."
        ],
        [
            "OK, I want to say a little bit now about moving away now to some extent from the classic machine learning formulation.",
            "I want to talk about some.",
            "Approaches that have been tossed around in other applications of sparse optimization recently.",
            "That you can apply to problems in machine learning.",
            "So let me bring up again this problem of total variation denoising and I'll remind you that the primal formulation is the thing I've given in the first formula.",
            "Which is you've got a sum of a fitting term, which is you minus?",
            "FF is the data.",
            "And then you've got you're adding on a total variation norm.",
            "Now it turns out that you can write if you use the appropriate Sobolev space, you can write a dual of that in the continuous infinite dimensional setting and the dual looks like this.",
            "It's a minimum.",
            "It's a maximization over function W, which is a mapping from.",
            "Omega 2 Two are two I think.",
            "And it's basically a quadratic function, and W gotta divergance of W term in the second Euclidean norm there and you want to maximize that over W."
        ],
        [
            "OK, now you can discretize all this very easily.",
            "If Omega isn't is a rectangular domain, you can just introduce a regular rectangular grid, replace the derivative type operators with finite difference operators, and you can get finite or discretized analogs of the primal and dual.",
            "So there's the first one is the discretization of the primal, and that's pretty straightforward.",
            "The Matrix AL is kind of the matrix that forms the gradient of the finite difference gradient of the.",
            "See terms and then we have to sum over every point in the discretization because that's the the sum.",
            "Takes the place of the integral operator OK. And then when we discretize that, we get an analogue of the of the continuous dual.",
            "Sorry when we take the dual of that thing we get an analog of the continuous dual and that's a problem where the variable that we're minimizing over I just change the sign to make the Max into a men.",
            "We have to minimize over a set which consists of Cartesian product of bulls in two dimension little circles in 2 dimensional space.",
            "So as a Cartesian product of any of these circles.",
            "OK, so this turns out to be.",
            "The dual turns out to be particularly nice, is just a quadratic objective convex quadratic objective with a constraint set that's not a box, but it's the next best thing.",
            "It's a, it's a separable Cartesian product of circles.",
            "And so one thing that we tried quite recently, Tony Chan and his student means you and I was just doing fairly naive projected gradient on that dual formulation where we had to take.",
            "We're taking steps in the negative gradient direction and projecting back on to X and all kinds of variants of that trying to use powerslide born step links and trying to introduce some curvature and so on.",
            "And what we found is maybe not unexpectedly is that if you just want a fairly rough solution or even a moderate accuracy solution, these this first order type approach beats the pants off of 2nd order approach, where you actually do some kind of interior point method which was.",
            "Something that there was that a lot of people were doing to solve these problems.",
            "Another thing we're doing that people were doing was taking that first order formulation and applying a second order cone programming formulation.",
            "Again, that's an interior point method.",
            "Each iteration is expensive.",
            "If you want a high accuracy solution, it's a good idea.",
            "If you just want a solution that looks good in the eyeball norm, it's kind of overkill."
        ],
        [
            "OK, but here's here's the interesting thing.",
            "Ming Zhu went off and messed around with this a little bit more and came up with an even better approach, and that was rather than just doing gradient descent in the dual.",
            "He came over this primal dual algorithm that just took alternating steps.",
            "1st order steps in the primal space in the dual space.",
            "So the first thing you did was to write down a saddle point formulation of this problem.",
            "He he wrote the Primal dual together as a min Max.",
            "Problem well and this is what it looks like.",
            "It's a very simple min Max problem.",
            "And then the approaches that you just take steps in X.",
            "The dual variable projected gradient steps.",
            "You take a step length along the gradient of L with respect to X project back onto the feasible set.",
            "And then you then you fix X and then you take a steepest descent step in V. OK and you just keep alternating between those two.",
            "And surprisingly found that from a lot of problems.",
            "This was incredibly fast.",
            "Even at finding high, even in funding high accuracy solutions.",
            "But another interesting thing was the choice of step links is kind of non intuitive.",
            "Normally in any kind of 1st order gradient descent method you choose a sequence of step links that you either get from align search or is somehow decreasing monotonically with K. The steps get shorter and shorter, but instead what he did was to take the step links in the dual space he made them longer and longer as.",
            "The iterations went on OK. Now this doesn't mean that you're moving further 'cause this projection back on the X tends to bring you back and tends to make the step actually quite short.",
            "But the interesting thing was he was sort of moving further and further out in along the dual gradient direction.",
            "The step length in the primal space stayed was decreasing as it turned out.",
            "So what's the explanation for this?",
            "Well, I don't think there's yet any analysis that kind of explains us.",
            "We kind of last fall.",
            "We thought about this for a lot of different angles using trying to bring a lot of the literature to bear, but.",
            "Haven't yet found a really satisfying explanation for why this works.",
            "Someone else might have done it in the meantime, but if."
        ],
        [
            "So I'm really interested to know what it would be.",
            "But the reason I mentioned this is that I wanted to show a problem that an SVM type problem where this was applicable and this is something that.",
            "That saying did recently my students saying who I mentioned on the on the 1st slide.",
            "So he went back and found this semiparametric SVM regression problem in the literature from about 10 years ago.",
            "Paper smaller and scholkopf and someone else and the idea here is that rather than doing purely nonparametric estimation, you first of all you're doing regression with some kind of epsilon insensitive margin.",
            "Of course you can do other things as well, but the key point here is that the function you're using is not just.",
            "The classifier is not just nonparametric.",
            "You also get to bring in some basis functions.",
            "These side JS and so you can have a parametric part in the.",
            "In H as well, and the idea is to learn both the nonparametric and parametric part are simultaneously OK.",
            "So after a bit of manipulation you can write that primal formulation of that regression problem.",
            "As it has a dual, it looks a lot like the dual that I was talking about earlier in the talk, except that instead of having just a single equality constraint, Now we've got K equality constraints where K is a number of parametric functions."
        ],
        [
            "Bring in.",
            "Then you can apply."
        ],
        [
            "A primal dual type method to that you can write it as a minmax problem.",
            "Sean there with that sort of Lagrangian and you can take alternating steps in the primal variables Alpha and the dual variables ADA.",
            "And you can introduce some curvature information and do a few other enhancements and you end up with something that actually works pretty well.",
            "There was another algorithm recently by Scholkopf and and keen slow, where they did something.",
            "Somewhat similar, but there they sort of made more of an effort to solve the primal space problem, and then would occasionally take a step in the dual space or the other way around.",
            "They would do a lot of iterations in the Alpha and then occasionally adjust the ADA and we just did this thing of taking sort of 1 alternating steps in the two, and it seemed to be faster.",
            "An eye."
        ],
        [
            "A picture there.",
            "Yeah, I just had a picture here.",
            "You probably can't see it too well, but it's just to show on a toy problem.",
            "This was from that original paper.",
            "Smaller on a toy problem.",
            "We compare the we have a true function that we're trying to recover, which is given by the blue line.",
            "The solid blue line.",
            "We take a bunch of samples on the interval from zero to 10.",
            "I've got noise in them and then we try fitting a nonparametric estimate, a semiparametric estimate, and a parametric estimate using three basis functions.",
            "And the semi parametric estimate in this instance does a little bit better job at following the wiggles of the true function."
        ],
        [
            "So."
        ],
        [
            "Seems to be some reason to think that this might be interesting.",
            "OK, I've got a little time left, so I'll talk about some alternative formulations.",
            "You can turn the classification problem into a purely linear problem.",
            "If you replace that two norm term with one norm term.",
            "And various people have done this.",
            "I'm not sure how satisfying the statistical justification is, but it's certainly nice to have a linear program 'cause there's all kinds of technology to do that.",
            "If you just blindly apply a linear programming solver, however, you you may not do that well.",
            "So you have to use sort of specialized solvers on this problem, but one possible advantage of this approach is that it might produce sparse weight vectors, so you might sort of.",
            "This might do feet be doing feature selection at the same time."
        ],
        [
            "Time has doing.",
            "Classification, in fact you could add that W two term back in there and do something like the elastic net where you've got a two norm regularization, Anna one norm regularization as well as a loss function.",
            "And has anyone tried this in SVM?",
            "I'd be surprised if someone hadn't tried something like this.",
            "Hey, you have two knobs to turn and I'm I'm just wondering if this has any interest in doing classification and sort of feature selection at the same time."
        ],
        [
            "OK, I thought the last thing that I would talk about.",
            "Is actually comes from work that I've done with Rob Nowak and Maria Figueredo on compressive sensing.",
            "We have an algorithm for that which actually generalizes to that fairly general statement of regularize optimization that I mentioned earlier.",
            "So let me bring back the general problem of F plus.",
            "Lambda times are and.",
            "The idea of this parser algorithm is that you you assume that the regularization term is non smooth back but simple in some sense like.",
            "The one norm OK?",
            "Or maybe a group regularizer sum of Infinity norms, or a sum of two norms?",
            "So the idea is that you.",
            "At each iteration you form a simple model of the F part, which is which is the smooth part.",
            "And in particular model we use is, we just linearize around the current iterate XC by taking the exact gradient of F and then we add on a quadratic term which is like a tune on penalty.",
            "Which is the first term at with the 101 / 2 Alpha K?",
            "Is the coefficient OK?",
            "You can think of this in a number of ways and then you leave the art term exactly as it is OK. You can think of this in a number of different ways.",
            "You can think of it as a linearisation of the F. With some kind of a trust region so that that first term the quadratic term is kind of the penalty term for staying within a some kind of two norm ball of where you currently are.",
            "Or you can think of that quadratic term as being some sort of crude approximation to the real 2nd order term, which would have the Hessian of F in it.",
            "So again, this is like the buzz lowborn motivation where you can think of the one over Alpha as being in some sense an approximation to the Hessian of F. OK, so there are many ways to choose the one over Alpha you can try.",
            "Increasing and decreasing it to make sure you get dissent every iteration you can do a non monotonic.",
            "You can settle for a non monotonic strategy and so on.",
            "And this is basically all sparse.",
            "There is really nothing to it.",
            "It's as simple as you know.",
            "I'm not really covering very much up here.",
            "The other comment on makers that this kind of approach is that it is good if you want to solve for a range of alphas, because if you get a solution of this regularizer problem for one value of Alpha, you can then decrease Alpha slightly and use your previous solution as a starting point for the next version, so you can do some kind of continuation strategy."
        ],
        [
            "And that tends to be quite effective, and as I said, our can be one or more sum of two norms or some Infinity norms.",
            "What you really need to make this work is."
        ],
        [
            "You need this subproblem to be easy to solve.",
            "And if our is just a one norm, you can find the solution of that subproblem in closed form and just in operations.",
            "So each subproblem is very cheap and the same is true.",
            "Is it far is sum of two norms?",
            "Or you know some sort of group separable regularizer.",
            "But if R is more complicated like a TV norm, then it's not so easy.",
            "It's not much easier to solve the subproblem then to solve the original problem.",
            "So in that."
        ],
        [
            "Guys, this approach is not as interesting."
        ],
        [
            "OK, I'll just mention that we applied this approach.",
            "I said we we use it for classic L2L1 compressed sensing.",
            "Well, there's also some joint work with Grace that she mentioned in her talk, Esther Grace Wahba and his student were young Hershey and some other collaborators.",
            "Where we do this for regularised logistic regression.",
            "So here it's the same problem except the fitting term in this case is just is just.",
            "It is just a logistic regression function and we do exactly this approach."
        ],
        [
            "We do a number of enhancements like we tried taking if we think we have identified the non zero terms of X or W or whatever the variable name is.",
            "We kind of fix them and take a Newton step reduce Newton step in those components and often in these logistic regression problems you're interested in problems where there are only a few non zeros at the solution.",
            "So it makes sense to evaluate a little piece of the Hessian and to sort of taken enhance 2nd order step in that very tiny subspace.",
            "Then we also use continuation.",
            "We sort of.",
            "So for a bunch of different land values and we end up with something that seems to be quite powerful."
        ],
        [
            "And solving very large instances of this problem, so I'm done at this point I've got a bunch of references."
        ],
        [
            "These are papers that I mentioned during the talk and you can't read them from where you are, but I'll be happy to give you the PDF if you want to chase up any of these slides, so I'm done.",
            "So any questions?",
            "Thanks.",
            "For power right?",
            "So.",
            "So sorry, I didn't quite catch that.",
            "Right direction for off measures and machine learning people to make problems more feasible for real world problems.",
            "It.",
            "Yeah, yeah, it's a kind of a general question.",
            "Well, I think I guess the answer is that people should just talk to each other more.",
            "So meetings like this are good because it's always good for, you know optimization.",
            "People are by nature our community is very interested in doing applications, but unless we get a lot of interaction with the other side we might be working on things that are really not very interesting in machine learning.",
            "On the other hand, what we have to offer as we do have potentially some tools that with the appropriate customization and adaptations might.",
            "Turn out to be useful for problems that you want to you actually want to solve.",
            "Yeah.",
            "Curious.",
            "In some sense, one might hope that as machine learning person you could sort of frame produce a mathematical formulation and sort of go off to you know one of the servers got resolved.",
            "Stop dictation programs and download the right one and feed your mathematical formulation and machine learning without writing software.",
            "Special purpose pizza.",
            "The program for every problem, well, I think it's true that if your problem isn't too big then you can certainly do what you said.",
            "You can just in fact, that should be the first cut.",
            "If you've got some prototype data you know you come up with a new formulation you want to test it out as long as it's not too big then there's you can just call off the shelf software and you can get an answer at least.",
            "But I guess my point is that if your if your concern is really for performance and if you've got a really big data set and.",
            "And you really do want to solve it in real time and not wait forever.",
            "That that approach may not be enough that you may have to look a little deeper and and figure out which of these you know.",
            "Variety of approaches is actually applicable to the problem you want to solve, but I think for prototyping what you're suggesting is certainly OK. What does Monica, Group or researcher?",
            "Mounted data genetic data out solution.",
            "Statistician engineer said something problems is testicle.",
            "Subject matter collaborators.",
            "Movie theater.",
            "Optimizing.",
            "That sounds simple solution.",
            "More wine Africa.",
            "A lot of data sets have special structure that motivate people power.",
            "So.",
            "Here.",
            "Right?",
            "Yeah, that's a good comment.",
            "You know the jewels SVF is kind of quadratic optimization.",
            "We can stop by interior point method.",
            "In order to get direction.",
            "As we haven't, they cost us very large, so the new system will be huge.",
            "Air condition.",
            "Exactly.",
            "After we could measure might like we walk into writing messages, open your system or use it in your appointments over.",
            "Yeah, that's something I just touched on lightly, but that was one of the first things though was tried in the 90s, was in Interior Point solver, and the reason for the reasons you mentioned, it turned out not to be very good.",
            "So the game, the name of the game, tried it turned out to be to try to find good preconditioners like real life type preconditions in conjunction with it approaches.",
            "And there's this paper of fine and shine Book from 2001 where they do something like that.",
            "But again, I you know I wouldn't claim it.",
            "That's that that approach might be good for certain problems with certain dimensions and so on.",
            "I wouldn't claim as a panacea for all problems, and that there might be some of the other methods I've mentioned might be better for other.",
            "For other sort of datasets.",
            "Faster would be slower than Treski direct master.",
            "Yeah, again again, it's it's going to be somewhat problem dependent, so if you do it salesky direct method, that's going to be ordering cubed right where N is for each iteration, where is the size of the problem, and even then it might not work, because if the matrix you're factoring might be really ill conditioned and so the choice he might sort of breakdown at some point.",
            "So there are tricks to fix that up.",
            "You know you can regularize it and get some kind of inexact step, but but it's very expensive and so.",
            "The bottom line is that I think to make it this interior method at all.",
            "Useful for a reasonable sized datasets.",
            "You probably have to do some kind of preconditioned iterative method like you said, Anne and I think it's just going to be the only thing you can really do rather than actual ASCII type method.",
            "That's my guess.",
            "I guess we can thank the speaker and close the session.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the University of Wisconsin.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much brother and thanks to the organizers also for inviting me to this terrific event.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be giving a bit of an optimization perspective on the area of support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Of course, I'm kind of an outsider in this area, but I want to recognize that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the last 1215 years, people in this community have made tremendous contributions in.",
                    "label": 0
                },
                {
                    "sent": "In using optimization methods and in developing new optimization methods to solve the many, many problems that arise in machine learning computational learning.",
                    "label": 0
                },
                {
                    "sent": "And I want to review some of those with an emphasis, particularly on sort of the classical classic SVM classification type problems, and I'll talk a little bit at the end about some newer developments in optimization that are.",
                    "label": 0
                },
                {
                    "sent": "I think particularly relevant to certain formulations of SVM problems, and I'll talk about how some of them are being applied in those problems, but a lot of the talk is kind of is sort of review of what's being done and.",
                    "label": 0
                },
                {
                    "sent": "And kind of representing things to some extent from from from an optimization perspective, putting things into an optimization frame.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing I want to notice that problems in this area are really hard.",
                    "label": 0
                },
                {
                    "sent": "Not in the sense that they've got incredible nonlinearity or non convexity, but in the sense that they are often very large, you've gotta deal often with a kernel matrix that's very large and possibly all condition and just the expense of evaluating a function.",
                    "label": 0
                },
                {
                    "sent": "Just if you want to evaluate a loss function over all the data that could be intractable or very difficult if you use all the data at once.",
                    "label": 0
                },
                {
                    "sent": "To repeat what I said a moment ago, this community is really made excellent use of optimization technology.",
                    "label": 1
                },
                {
                    "sent": "They've adapted fundamental algorithms in very interesting ways that exploit the structure of the problem and kind of customized to the requirements of the applications.",
                    "label": 1
                },
                {
                    "sent": "However, this community is always throwing up new formulations that kind of thinking up new ways to to formulate problems, and they're throwing up whole new classes of problems, like manifold learning and so on.",
                    "label": 0
                },
                {
                    "sent": "They will present new challenges and in some cases much harder problem.",
                    "label": 1
                },
                {
                    "sent": "Then we had to deal with before.",
                    "label": 0
                },
                {
                    "sent": "For example, semi supervised learning as you know, depending on which formulation you use, it can be a common tutorial problem with 01 variables it can be nonconvex.",
                    "label": 0
                },
                {
                    "sent": "You can really need to use global optimization techniques to find decent answers and then Lastly.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I point out there are some new strands of work in optimization that might be useful in this area, so I want to start off kind of at a more abstract level, and just to point out that machine learning is 1 instance of a whole class of a whole set of new paradigm and optimization that's capturing a lot of interest, and that's the area of sparser regularised optimization.",
                    "label": 0
                },
                {
                    "sent": "Anas, I'll give a number of examples to point out that these kinds of problems are happening all over the place, so I mean traditional work in optimization.",
                    "label": 0
                },
                {
                    "sent": "People have assumed that the data that formulation is exact.",
                    "label": 0
                },
                {
                    "sent": "And let you want an exact answer.",
                    "label": 0
                },
                {
                    "sent": "You know that was kind of the name of the game, but it's being realized that in many, many applications we're much happier with an approximate answer.",
                    "label": 1
                },
                {
                    "sent": "That's in some sense simple.",
                    "label": 0
                },
                {
                    "sent": "Simple manifested in, you know ways like sparsity.",
                    "label": 0
                },
                {
                    "sent": "So what are some reasons for this?",
                    "label": 1
                },
                {
                    "sent": "Well, they differ of course according to the application, but but some reasons are that simple solutions are easy to implement or actuator in some cases they are easy to understand.",
                    "label": 0
                },
                {
                    "sent": "If you've got uncertain data.",
                    "label": 1
                },
                {
                    "sent": "If you're not certain about the data in the formulation, there's no point in solving the problem exactly.",
                    "label": 0
                },
                {
                    "sent": "That's another reason.",
                    "label": 0
                },
                {
                    "sent": "And then there's this sort of the Occam's razor thing of if you're trying to explain some phenomenon in this case by solving and.",
                    "label": 0
                },
                {
                    "sent": "An optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The simplest explanation, that kind of the simplest explanation is the most sort of plausible one in some sense.",
                    "label": 0
                },
                {
                    "sent": "So these ground rules kind of changed the whole decision making process about what algorithm you should use or what approaches should use, and they can change it in pretty radical ways.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you were trying to solve the problem exactly, you might go for some kind of Newton like method using higher order.",
                    "label": 0
                },
                {
                    "sent": "His steps and higher information if you only want an exact solution, you might decide that some kind of approximate 1st order method is good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tough.",
                    "label": 0
                },
                {
                    "sent": "OK, regularize formulations many problems can be stated in this kind of form.",
                    "label": 1
                },
                {
                    "sent": "A combination of two terms that you're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The first term is some kind of model term or data fitting term or loss term, and the second term is some kind of regularization.",
                    "label": 1
                },
                {
                    "sent": "And often there's some single parameter.",
                    "label": 0
                },
                {
                    "sent": "That balance is the two off and you want to try and minimize that combination.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you don't know what that parameter is in advance.",
                    "label": 0
                },
                {
                    "sent": "You have to do some kind of search.",
                    "label": 1
                },
                {
                    "sent": "Or maybe you're interested in finding approximate solutions for a whole range of values of Lambda OK, and then often if you're talking about sparse or simple solutions, this regularization term is often non smooth and for various reasons non smoothness is needed to kind of induce sparsity in the solution.",
                    "label": 0
                },
                {
                    "sent": "For reasons that I won't explain.",
                    "label": 0
                },
                {
                    "sent": "But you can't explain it.",
                    "label": 0
                },
                {
                    "sent": "Sometimes in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Duality is useful, so here's one example that's getting a tremendous amount of play in the applied math and statistical communities, and that's compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "And there are many formulations of this.",
                    "label": 0
                },
                {
                    "sent": "This is one of the popular ones.",
                    "label": 1
                },
                {
                    "sent": "The idea is that you've got a matrix A, which is a combination of some kind of basis matrix and maybe some sort of sensing matrix.",
                    "label": 1
                },
                {
                    "sent": "You've got an unknown signal X.",
                    "label": 0
                },
                {
                    "sent": "You've got some observations B and you're trying to find a sparse X that is an X with relatively few nonzeros.",
                    "label": 1
                },
                {
                    "sent": "That explains the observations.",
                    "label": 0
                },
                {
                    "sent": "You know, sort of our priority that that that is that B is explicable by a sparse signal.",
                    "label": 0
                },
                {
                    "sent": "You have some information or some reason to believe that there's a sparse explanation for B.",
                    "label": 0
                },
                {
                    "sent": "Now, under certain assumptions on a, this came up a little in the previous talk going by the name of restricted isometry or incoherence type properties in coherence between the observations and the basis, you can show that this formulation actually will give you the right answer or something close to the right answer for an appropriate choice of Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, and this has the form that I described earlier are fitting term plus Lambda times some kind of regularization.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn.",
                    "label": 0
                },
                {
                    "sent": "Another example, it will return to later in the talk is denoising image.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a rectangular domain and some kind of image data F, you want to recover a denoised version of that image that also has a fairly low measure of total variation norm.",
                    "label": 0
                },
                {
                    "sent": "So the total variation norm as you know it's suitable for recovering images that are sort of cartoon like where you have large areas where not much is happening, and then a few edges OK, so the total variation norm is the spatial grading of you.",
                    "label": 0
                },
                {
                    "sent": "The norm of that integrated over the domain.",
                    "label": 0
                },
                {
                    "sent": "OK, so again that has that same kind of form that I mentioned earlier, and as I said, I'll return.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem later on, here's a much more nitty gritty kind of area that you get sparse optimization problems arising in.",
                    "label": 0
                },
                {
                    "sent": "This is radiotherapy for cancer, so you may know that one way, one way of treating cancer is by by setting people up on an X Ray producing device, and then shooting X Rays into their body kind of training it on the tumor, which is which is inside their body, and the idea is that you use it a number of different angles.",
                    "label": 0
                },
                {
                    "sent": "You fire the beams in from a number of different angles.",
                    "label": 0
                },
                {
                    "sent": "So is to kind of focus them all on the tumor, but not affect too much of the surrounding tissue.",
                    "label": 0
                },
                {
                    "sent": "So so you sort of want to spread the radiation out among the health around the healthy tissue, but have it all kind of focus on the tumor.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to, you can overfit that problem.",
                    "label": 0
                },
                {
                    "sent": "You can have a solution where you where you where you shoot the beam in from 25 or 30 different angles.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is it's very difficult to implement and for all sorts of reasons it's not a good idea to have the patient lying on the table for two hours.",
                    "label": 0
                },
                {
                    "sent": "And trying to keep them still while they keep moving the head of the machine around so they much prefer solutions where you've only got three or four or five different angles.",
                    "label": 0
                },
                {
                    "sent": "OK, so in a sense that's a sparse solution 'cause you're trying to pick a very small number of angles from among a huge range of possibilities.",
                    "label": 0
                },
                {
                    "sent": "Not only angles in this case, but also you get to shape the beam and you get to decide how long to expose it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An another example matrix completion that I'm sure some of you here know about.",
                    "label": 1
                },
                {
                    "sent": "This is a problem where you've got an unknown matrix X and you're allowed to observe certain elements of the matrix or certain linear functions of elements of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Linear combinations, and again those observations are in the vector B and you're trying to find in some sense the lowest rank or the simplest matrix ax that explains the observations.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that here in appropriate regularization term, one that's been proposed as this nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "Which is the sum of singular values of X.",
                    "label": 1
                },
                {
                    "sent": "If you're dealing with a symmetric positive definite X, then you get the nuclear norm corresponds to the trace, which is also the sum of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So you can use that instead.",
                    "label": 0
                },
                {
                    "sent": "So I'll just point out I won't say anything more about this problem except that quite a few people are working on it, but I'll just point out that the methods that have been proposed.",
                    "label": 1
                },
                {
                    "sent": "A very similar to compressed sensing methods, except that the linear algebra is a lot more complicated and the relationship between the two is like if you know about interior point methods for linear programming and for semidefinite programming.",
                    "label": 0
                },
                {
                    "sent": "The analogy between those two interior point methods for those two problems is very similar to the analogy between algorithms for compressed sensing algorithms for matrix completion.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do you go about solving these regularised formulations well?",
                    "label": 0
                },
                {
                    "sent": "The algorithm approaches tend to be very problem specific, and so you can only go so far with kind of abstracting approaches and, and you know, generalizing across a whole bunch of different applications.",
                    "label": 0
                },
                {
                    "sent": "But you can do something with that.",
                    "label": 0
                },
                {
                    "sent": "There are algorithms that work well in one class of problems that you can kind of abstract to other problems in this general area.",
                    "label": 0
                },
                {
                    "sent": "Sparse optimization, you can make some general observations.",
                    "label": 0
                },
                {
                    "sent": "One is that duality is often a key to getting a formulation that's useful, and that's certainly true in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Another one is that, as I mentioned earlier, you often want to solve for a range of regularization parameters, and often you have more than one parameter.",
                    "label": 1
                },
                {
                    "sent": "You might have a number of knobs you can turn 'cause you might have multiple regularization terms, each of with each of which has its own parameter.",
                    "label": 1
                },
                {
                    "sent": "Another sort of general observation is that often you know you've got a choice between a method that has fast asymptotic convergence where each step is relatively expensive, like an interior point method.",
                    "label": 0
                },
                {
                    "sent": "And then there's another kind of method where each step is relatively cheap at some kind of 1st order or subgradient type method.",
                    "label": 0
                },
                {
                    "sent": "But you need a lot of steps OK, and which of these two classes is more appropriate?",
                    "label": 0
                },
                {
                    "sent": "Very much depends on the application and what.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You need what kind of solution you're looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I want to say spend some time talking about basic SVM and this.",
                    "label": 0
                },
                {
                    "sent": "This really has a bit more of a review or tutorial type flavor, 'cause I'm sure that many of you have very familiar with this problem, so I'll be going over these slides fairly quickly.",
                    "label": 0
                },
                {
                    "sent": "So I want to talk about the basic classification problem where we have feature vectors in RN.",
                    "label": 1
                },
                {
                    "sent": "We've got an addon, we've got binary labels and we're trying to find a linear classifier that's given by a vector WNRN and some intercept B, so that we've got a linear classification function F of X, and the idea is that we learn WMB from the data and then we use.",
                    "label": 1
                },
                {
                    "sent": "If someone comes along with a new feature vector, we can use F to classify or predict a label for that for that vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the very popular ways of setting this problem up is is to use the so called hinge loss function.",
                    "label": 0
                },
                {
                    "sent": "Where the in this case the fitting term is really the second term there.",
                    "label": 0
                },
                {
                    "sent": "This nonce or this piecewise linear term, which is the Max.",
                    "label": 0
                },
                {
                    "sent": "Basically you incur a penalty if XI if the feature excise on the wrong side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Otherwise if it's on the right, so it is sufficiently far on the right side that second term is 0.",
                    "label": 0
                },
                {
                    "sent": "So we sum up all the penalties from one to N, and then we add on this regularization term, which in this case is a smooth term.",
                    "label": 0
                },
                {
                    "sent": "It's a two norm of W. OK, for reasons that this gives us the maximum margin separating hyperplane in the case we got separable data.",
                    "label": 1
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "But this is this formulation is proved to be very popular in this.",
                    "label": 1
                },
                {
                    "sent": "See in this case plays the role of the regularization parameter.",
                    "label": 1
                },
                {
                    "sent": "So as it's written here, this is an unconstrained piecewise quadratic function.",
                    "label": 0
                },
                {
                    "sent": "We can write it as a convex quadratic program just by introducing some extra extra variables to take the place of each of the terms in that summation, and then introduce some linear constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can turn it into a standard quadratic program, very easy.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Easily.",
                    "label": 0
                },
                {
                    "sent": "We can go from that standard quadratic program to it dual by just using.",
                    "label": 0
                },
                {
                    "sent": "You know the Wolf dual from quadratic convex quadratic optimization and we can try to do like this and this is a form again that I'm sure that many of you are familiar with now the unknowns you can think over them as being LaGrange multipliers for each term in the loss function.",
                    "label": 0
                },
                {
                    "sent": "Bounded between Zero, the offers that matter between zero and C. Because we've got an intercept, we've got an extra linear constraint.",
                    "label": 0
                },
                {
                    "sent": "Y transpose offer equals zero, and then we've got this quadratic objective.",
                    "label": 0
                },
                {
                    "sent": "OK, in each each term in the passion of the objective is an inner product of two of the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, with the labels included as well.",
                    "label": 0
                },
                {
                    "sent": "And you can write down KKT or Karush Kuhn Tucker conditions that relate to primal and dual solutions.",
                    "label": 1
                },
                {
                    "sent": "And you can express the classifier.",
                    "label": 0
                },
                {
                    "sent": "In fact, in terms of the dual solution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, one of the motivations for going to the dual formulation is that you can use the so called kernel trick.",
                    "label": 1
                },
                {
                    "sent": "I'm sure no one calls it that anymore, but that's what it was called.",
                    "label": 0
                },
                {
                    "sent": "10 years or so ago where you can say that, OK, I don't really need to do the classification in the original feature space.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "I can virtually project into a high dimensional space given by this mapping fee, and I can think of each term in the kernel as being the inner product of 2 feature vectors mapped into the high dimensional space, OK?",
                    "label": 1
                },
                {
                    "sent": "And if you do that, you can then define the classifier in the same way, But then you can make the observation that you never really actually learn need to know fee.",
                    "label": 0
                },
                {
                    "sent": "You can just say let me replace the entries in the kernel by some positive definite function.",
                    "label": 0
                },
                {
                    "sent": "OK, and then think of and.",
                    "label": 0
                },
                {
                    "sent": "Then there's this theorem that says that there is it to go with each positive definite function.",
                    "label": 0
                },
                {
                    "sent": "There is a fee.",
                    "label": 0
                },
                {
                    "sent": "OK, roughly speaking alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never actually have to deal with the.",
                    "label": 0
                },
                {
                    "sent": "We can do everything in terms of the kernel function K, and as you know there are a few popular kernels.",
                    "label": 0
                },
                {
                    "sent": "The one that seems to be used for the two that seem to be used all the time are the first to the linear kernel and the Gaussian or radial basis function kernel.",
                    "label": 0
                },
                {
                    "sent": "And there are a few others that are that are out there.",
                    "label": 0
                },
                {
                    "sent": "And actually, there's been a few talks on learning kernels in this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this conference.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do you go about solving these?",
                    "label": 0
                },
                {
                    "sent": "Both of these are fairly classic optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Are convex quadratic programs, but there's no magic bullet algorithm because there are problems of many types of feature vectors can be very short or very long.",
                    "label": 0
                },
                {
                    "sent": "There can be a huge amount of data, or they can be not so much.",
                    "label": 0
                },
                {
                    "sent": "Your parameters in the defining the kernel matrix might make it relatively well conditioned, or it can be really terribly conditioned.",
                    "label": 0
                },
                {
                    "sent": "So all these, and then there's this question about how accurately do you really need to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Given that this you know this function that I've written down is just an empirical measure of risk, innocence and your aim, not really as to solve that empirical problem exactly, but to find a good classifier ultimately.",
                    "label": 0
                },
                {
                    "sent": "OK, so all these questions kind of play.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say into what kind of algorithm you need and they kind of motivate the need for maybe a whole family of a whole bunch of algorithms.",
                    "label": 0
                },
                {
                    "sent": "The whole smorgasbord of algorithms to choose from.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the deal.",
                    "label": 0
                },
                {
                    "sent": "Formulation.",
                    "label": 0
                },
                {
                    "sent": "The thing that I would say makes this most tricky, besides its size is the is the fact that the Hessian is dense and or condition, so one of the first things people trade with some kind of interior point method back in the 90s.",
                    "label": 0
                },
                {
                    "sent": "When interior point methods were kind of all the rage and you know I've tried that on this problem, and one of the things that hurt you is is the fact that the Hessian is ill conditioned.",
                    "label": 0
                },
                {
                    "sent": "So you end up having to solve at each interior point iteration.",
                    "label": 0
                },
                {
                    "sent": "If you do a naive implementation, you end up having to solve a very ill conditioned linear system.",
                    "label": 1
                },
                {
                    "sent": "Any other thing about this is that if you insist on including an intercept term, then you've got this linear constraint, which is really a nuisance.",
                    "label": 1
                },
                {
                    "sent": "You know dealing with bound constraints is easy, but when you intersect about constraint with the hype.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Klein had becomes a bit of a pain.",
                    "label": 0
                },
                {
                    "sent": "OK so here is 1 possible approach is that you can just throw you forget about the Intercept.",
                    "label": 1
                },
                {
                    "sent": "Let's just look for a classifier that that doesn't have an intercept and then we just got a nice relatively nice bound constrained QP.",
                    "label": 0
                },
                {
                    "sent": "And people have proposed various things for this.",
                    "label": 0
                },
                {
                    "sent": "Here is a proposal of actually very recent, one of a coordinate descent method where you just pick an element of Alpha and take some kind of a step in that.",
                    "label": 0
                },
                {
                    "sent": "Fix all the others.",
                    "label": 0
                },
                {
                    "sent": "OK, you can cycle through the alphas or you can maybe pick them at random, but the point is to take a step in in and in a direction Alpha.",
                    "label": 1
                },
                {
                    "sent": "You don't need to know very much about the kernel.",
                    "label": 0
                },
                {
                    "sent": "You certainly don't need to have it all evaluated and stored.",
                    "label": 0
                },
                {
                    "sent": "OK so each step of this method is pretty cheap at storage requirements are pretty modest, but it probably needs an awful lot of steps.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe maybe it's that's OK. Maybe you can find an ear and ear solution fairly efficiently.",
                    "label": 0
                },
                {
                    "sent": "Here's another fairly recent proposal, actually from some people in the optimization community, and that is to leave the Intercept term in there.",
                    "label": 0
                },
                {
                    "sent": "But again, just use methods from gradient projection.",
                    "label": 0
                },
                {
                    "sent": "In other words, make each step some step along the negative gradient direction, but but each time you take a step, you have to project it back onto this feasible set and the feasible set is now the intersection of the box with the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Now that projection is not trivial anymore, but it's also not too bad.",
                    "label": 0
                },
                {
                    "sent": "You can implement it fairly efficiently, kind of as a subproblem.",
                    "label": 0
                },
                {
                    "sent": "And some of the interesting issues are how do you choose the step length?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a technique due to buzz Lionboy, and it's not about 20 years old that has come back into fashion a little bit in the last five years or so, and the reason it's it was kind of very non mainstream is that it's typically non monotone that you often take steps that increase the objective function and this goes against the DNA of a lot of people in optimization who are used to having dissent type methods that are guaranteed to give you an improvement at every iteration.",
                    "label": 0
                },
                {
                    "sent": "But you can show that there there's reason to think that non monotone methods by sort of sacrificing short-term improvement in the objective.",
                    "label": 0
                },
                {
                    "sent": "They can actually get better long term performance over a whole range of iterations.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea of a buzz lowboy and step length is is in some sense it imitates the 2nd order term.",
                    "label": 0
                },
                {
                    "sent": "If this were a Newton type method you would have the Hessian inverse times the gradient as the search direction, but he replaced the Hessian inverse with just a scalar and as long as the scalar is somewhere in the spectrum of.",
                    "label": 0
                },
                {
                    "sent": "Of the Hessian or the Hessian inverse, and this is not an unreasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "You can think of the scalar as being a very crude approximation to the inverse.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Session.",
                    "label": 0
                },
                {
                    "sent": "Decomposition strategies well.",
                    "label": 0
                },
                {
                    "sent": "Again, if you're working in the dual space, the size of the problem is equal to the number of data points, and that can be huge.",
                    "label": 0
                },
                {
                    "sent": "So so going right back to the 90s, people have have sort of proposed methods that just work with a few components at a time.",
                    "label": 0
                },
                {
                    "sent": "Maybe only two components, maybe 10 or maybe 100.",
                    "label": 0
                },
                {
                    "sent": "Maybe some kind of a sliding window where you change the.",
                    "label": 0
                },
                {
                    "sent": "You may change the set of components you're working with at every iteration, or you may just change it periodically when you think you've kind of exhausted the current set so.",
                    "label": 0
                },
                {
                    "sent": "All kinds of methods can be embedded in a decomposition framework and I mentioned some of them here.",
                    "label": 0
                },
                {
                    "sent": "Gradient projection methods, even interior point methods.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK yeah, and live SVM.",
                    "label": 0
                },
                {
                    "sent": "I think this is another one that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Embedded in A is sort of a decomposition framework, but you use some second or information to to to solve the low dimensional QP at each edge for each subproblem.",
                    "label": 0
                },
                {
                    "sent": "So there are all sorts of other ideas you can bring in, like shrinking.",
                    "label": 0
                },
                {
                    "sent": "You can sort of figure out which components of Alpha seem to be really at zero and or at their upper bound.",
                    "label": 1
                },
                {
                    "sent": "And once you've decided that there at one of the bounds, you don't really have to worry about them too much anymore, except that you might want to check in later on to check if they really do belong there.",
                    "label": 0
                },
                {
                    "sent": "Caching is another important kind of implementation level issue that if you evaluate some elements of the kernel, and often that's quite expensive to do, you would like to keep them around for a while in case you need them again on.",
                    "label": 0
                },
                {
                    "sent": "Not too far down the road in a future iteration, so figuring out strategies to do that is.",
                    "label": 0
                },
                {
                    "sent": "It is also sort of a non trivial issue.",
                    "label": 0
                },
                {
                    "sent": "OK, so decomposition methods work OK.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in the sparse setting, particularly using something like a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "The solutions Alpha, the dual solutions are often not all that sparse.",
                    "label": 1
                },
                {
                    "sent": "There are many examples where they're not particularly sparse, and so that kind of limits how good a decomposition framework can really be, or even a shrinking type framework.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so some some other methods I'll just throw up here.",
                    "label": 0
                },
                {
                    "sent": "Active set type methods.",
                    "label": 0
                },
                {
                    "sent": "These are these are methods that actually do sort of pivots, and there's sometimes useful if you want to solve for an entire range of values of the of the.",
                    "label": 0
                },
                {
                    "sent": "Of the regularization parameter, they also might be useful if you want to build up a solution incrementally, so there's a recent paper by Sheinberg where she sort of keeps adding data and then keeps figuring out how that changes the solution of the QP.",
                    "label": 0
                },
                {
                    "sent": "So you saw the QP for a certain amount of data.",
                    "label": 0
                },
                {
                    "sent": "Then you toss in a new data point that expands the dimension of the QP by one.",
                    "label": 0
                },
                {
                    "sent": "You take whatever action.",
                    "label": 0
                },
                {
                    "sent": "It's usually a fairly simple pivot is needed to adapt the solution to the new data point and continue from there.",
                    "label": 0
                },
                {
                    "sent": "So that can be quite expensive to really if you if you're dealing with very large amounts of data.",
                    "label": 0
                },
                {
                    "sent": "Another way of dealing with that intercept constraint away transfers Alpha constraint is to turn the problem into a min Max problem.",
                    "label": 0
                },
                {
                    "sent": "That's another thing that's recently been proposed, and they do sort of an active set method on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That again, I've got a bunch of references at the end, so if you're interested in looking at.",
                    "label": 0
                },
                {
                    "sent": "Looking at any of these, you can.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the slides later on.",
                    "label": 0
                },
                {
                    "sent": "OK, active set methods in general, what you can say about them?",
                    "label": 1
                },
                {
                    "sent": "As I've already said, they're good if you've got kind of you introducing the data incrementally.",
                    "label": 1
                },
                {
                    "sent": "They're also good if you want to trace an entire solution path.",
                    "label": 0
                },
                {
                    "sent": "'cause often when you change the value of see, you just need a few pivots of the active step method to recover the value to recover the updated values.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that context.",
                    "label": 0
                },
                {
                    "sent": "They may not be completely unreasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, interior point methods I mentioned earlier that these had been tried and there's a paper in 2001 and I'm sure there are some early attempts earlier attempts, but the key, the most expensive operation.",
                    "label": 0
                },
                {
                    "sent": "Interior Point method is to solve a linear system in which the kernel plus you've got the kernel plus a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "And if you literally do this, of course it really restricts the size of the problems.",
                    "label": 0
                },
                {
                    "sent": "You can solve another problem as I said is that is the problem of conditioning that the kernel may not be well conditioned.",
                    "label": 0
                },
                {
                    "sent": "So you can do things like precondition this and you apply some kind of iterative technique and that's that in some circumstances is not so bad, but here's a thought.",
                    "label": 0
                },
                {
                    "sent": "What if you just replace the kernel by a low rank approximation?",
                    "label": 0
                },
                {
                    "sent": "And do that, and that's something that we tried.",
                    "label": 0
                },
                {
                    "sent": "Last year, just to use to take the kernel of play some kind of iterative method like an Arnoldi type method.",
                    "label": 0
                },
                {
                    "sent": "Identify maybe the top five or ten eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "You can even there's even a command in Matlab to do this.",
                    "label": 1
                },
                {
                    "sent": "OK I guess.",
                    "label": 0
                },
                {
                    "sent": "Or you could use some kind of sampling method like.",
                    "label": 1
                },
                {
                    "sent": "Mahoney may be talking about that right now, and the other talk I don't know, but in other words, to do some sort of pre preliminary computation to figure out what the leading eigenspace of K is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then just substitute VV transpose for K in the dual SVM.",
                    "label": 0
                },
                {
                    "sent": "OK and solve that problem.",
                    "label": 0
                },
                {
                    "sent": "So how good a job does that do?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that if you do that, you get by making a change of substitution of variables.",
                    "label": 0
                },
                {
                    "sent": "Gamma is V transpose Alpha.",
                    "label": 0
                },
                {
                    "sent": "You actually get a very simple quadratic program, and if you hand it to seaplex it solves very very quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very efficient.",
                    "label": 0
                },
                {
                    "sent": "The offer that you recover from this may not be unique, but who cares?",
                    "label": 0
                },
                {
                    "sent": "The classifier you get out of it is well defined and how well does it do?",
                    "label": 0
                },
                {
                    "sent": "Well, our experience wasn't so great.",
                    "label": 0
                },
                {
                    "sent": "We believe other people have done something like this and have had better experience, and I think the trick.",
                    "label": 0
                },
                {
                    "sent": "We're told that the trick is in choosing the.",
                    "label": 0
                },
                {
                    "sent": "Parameters of the kernel to make sure that the full kernel itself would be a decent separator, but we were kind of surprised to find that even if we captured most of the matrix in the VV transpose and the part we were ignoring was relatively insignificant, that somehow the quality of the classifier sort of degraded fairly noticeably OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk a little bit about solving the primal.",
                    "label": 0
                },
                {
                    "sent": "So this is going back to that.",
                    "label": 0
                },
                {
                    "sent": "Original formulation where we were kind of restrict ourselves to classify as in the original linear space.",
                    "label": 0
                },
                {
                    "sent": "No kernels for the moment, although there have been some attempts to extend these these these techniques to kernel spaces as well.",
                    "label": 0
                },
                {
                    "sent": "So what we have here again is this last term added to a regularization term, which is the two norm term and.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been quite a bit of excitement in this area lately.",
                    "label": 0
                },
                {
                    "sent": "2 main approaches.",
                    "label": 0
                },
                {
                    "sent": "One is a kind of a cutting plane approach.",
                    "label": 1
                },
                {
                    "sent": "So in the cutting plane approach, what you do is you look at their loss term and you realize that it's a piecewise linear function, because each of those hinge loss functions is just a two piece linear function.",
                    "label": 1
                },
                {
                    "sent": "When you add them up.",
                    "label": 0
                },
                {
                    "sent": "Of course you get a lot of kinks, but watch what you aim to do is to find a lower bounding piecewise linear approximation by constructing sort of hyperplanes that that are lower bounds to that loss function.",
                    "label": 0
                },
                {
                    "sent": "And hopefully you just build it will build up the quality of your lower bounding approximation in the region that you're interested in, so you won't have to deal with the you know with the approximation far from the solution point.",
                    "label": 0
                },
                {
                    "sent": "So this idea is kind of a classic idea in.",
                    "label": 0
                },
                {
                    "sent": "In optimization, and it's been tried in a lot of different contexts, including stochastic optimization, integer programming, and so on.",
                    "label": 1
                },
                {
                    "sent": "In this context, it turns out that it's particularly suitable because that's upgrading is very easy to calculate.",
                    "label": 0
                },
                {
                    "sent": "If I give you a W, you can easily give me back a subgradient which will, from which I can construct a lower bounding hyperplanes and there been a number of papers.",
                    "label": 1
                },
                {
                    "sent": "SPM Purf is 1 implementation of this.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another implementation.",
                    "label": 0
                },
                {
                    "sent": "They add some line search strategy there, and they had some.",
                    "label": 0
                },
                {
                    "sent": "Other strategy for figuring out where to evaluate the new cutting plane and so on, and you can prove various.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convergence and complexity results.",
                    "label": 0
                },
                {
                    "sent": "We've tried some various enhancements of this approach, motivated by what people do in stochastic programming, namely by breaking the piecewise linear term up into chunks or bundles and generating a separate cut for each one.",
                    "label": 0
                },
                {
                    "sent": "This gives you kind of a richer.",
                    "label": 1
                },
                {
                    "sent": "Lower bounding piecewise linear function and thereby converging in few iterations.",
                    "label": 0
                },
                {
                    "sent": "Although each iteration is more expensive, strategies for deleting cuts that you haven't used much recently, so that you don't get more and more cuts being generated without limit, and different heuristics for deciding how you add a cut after you take a step that that is not successful, it doesn't produce an improvement in the function, and there's probably a lot of other ideas that can be tried in this context motivated by.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other areas of have optimization.",
                    "label": 0
                },
                {
                    "sent": "Here's another approach that again has been around for awhile along by two is one person that's advocated this approach, and more recently the Pegasus code of people here at TTI, is a very nice implementation of this idea.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you treat that second that summation over all the data instead of instead of getting a subgradient of that whole thing, you just sample from that some.",
                    "label": 0
                },
                {
                    "sent": "So you pick either one term from that some or just a few terms and your computer subgradient from that.",
                    "label": 0
                },
                {
                    "sent": "And then you take some kind of a step in that direction along that subgradient.",
                    "label": 0
                },
                {
                    "sent": "OK, and the step could either decrease like at iteration K. It could be like 1 / K or it could decrease like 1 / sqrt K. And it turns out you can prove things about the subgradients defined there at the bottom of the at the bottom of the slide if.",
                    "label": 0
                },
                {
                    "sent": "If you happen to pick a term in the somewhere that's already correctly classified, then there's no contribution from the second term and the subgradient is just the subgradient.",
                    "label": 0
                },
                {
                    "sent": "The regularization term, which is W. If you do pick a term that's incorrectly classified, then it gets a little bit more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the idea is you don't do any kind of a line, so if you just sort of blindly take this step in that direction and you can prove quite a few interesting things about these kinds of schemes, and typically what you can prove is some kind of convergence in expectation, so you can prove that if you do the sampling uniformly that the objective value of the point that you're at is converging to the optimal objective value in expectation at some rate, like 1, /, K or 1 / sqrt K. And I mentioned a few variants of that there OK, But the interesting thing about this is this is a whole stream of research and optimization that's very closely related to what's been happening in the machine learning Community in this area, and probably the most interesting paper to look at if you're not familiar with this is a paper by nemerofsky.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's just appeared in some Journal of optimization where he basically talks about it exactly.",
                    "label": 0
                },
                {
                    "sent": "These kinds of approaches in a slightly different setting, and the and the conclusion of the results that he proves there almost almost identical to the results you're seeing.",
                    "label": 0
                },
                {
                    "sent": "The machine learning literature.",
                    "label": 0
                },
                {
                    "sent": "So we seem to have an instance of, like you know, very imaginative, interesting class of methods being generated, sort of independently by different groups.",
                    "label": 0
                },
                {
                    "sent": "One of the points that Nemerofsky makes it it's worthwhile even reading just the first couple of sections of his paper, is that a method that you step size is like 1 / K only can work well if you have some guess of the modulus of convexity.",
                    "label": 0
                },
                {
                    "sent": "Now in this problem we do, we know that that first term, the one the 1/2 W squared term, we know what the modulus of convexity is, the curve it is sort of a lower bounding curvature.",
                    "label": 0
                },
                {
                    "sent": "And if you know that there's a sensible way to choose the step length and get.",
                    "label": 0
                },
                {
                    "sent": "1 /, K convergence of the objective and expectation.",
                    "label": 0
                },
                {
                    "sent": "If you don't know that or if or if there is no strong convexity, then he shows that you're better off using 1 / sqrt K rule.",
                    "label": 0
                },
                {
                    "sent": "In some sense that's more robust.",
                    "label": 0
                },
                {
                    "sent": "That gives slower convergence and expectation, but it's less sensitive to getting the getting the estimate of of.",
                    "label": 0
                },
                {
                    "sent": "The convexity parameter wrong.",
                    "label": 0
                },
                {
                    "sent": "You can pay a high price if you if you give it the wrong value for the convexity parameter or 1 / K rule can give you really terrible results.",
                    "label": 0
                },
                {
                    "sent": "That's not so much an issue in this formulation.",
                    "label": 0
                },
                {
                    "sent": "'cause we do know what that is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I want to say a little bit now about moving away now to some extent from the classic machine learning formulation.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about some.",
                    "label": 0
                },
                {
                    "sent": "Approaches that have been tossed around in other applications of sparse optimization recently.",
                    "label": 0
                },
                {
                    "sent": "That you can apply to problems in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So let me bring up again this problem of total variation denoising and I'll remind you that the primal formulation is the thing I've given in the first formula.",
                    "label": 0
                },
                {
                    "sent": "Which is you've got a sum of a fitting term, which is you minus?",
                    "label": 0
                },
                {
                    "sent": "FF is the data.",
                    "label": 0
                },
                {
                    "sent": "And then you've got you're adding on a total variation norm.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that you can write if you use the appropriate Sobolev space, you can write a dual of that in the continuous infinite dimensional setting and the dual looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's a minimum.",
                    "label": 0
                },
                {
                    "sent": "It's a maximization over function W, which is a mapping from.",
                    "label": 0
                },
                {
                    "sent": "Omega 2 Two are two I think.",
                    "label": 0
                },
                {
                    "sent": "And it's basically a quadratic function, and W gotta divergance of W term in the second Euclidean norm there and you want to maximize that over W.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now you can discretize all this very easily.",
                    "label": 0
                },
                {
                    "sent": "If Omega isn't is a rectangular domain, you can just introduce a regular rectangular grid, replace the derivative type operators with finite difference operators, and you can get finite or discretized analogs of the primal and dual.",
                    "label": 0
                },
                {
                    "sent": "So there's the first one is the discretization of the primal, and that's pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "The Matrix AL is kind of the matrix that forms the gradient of the finite difference gradient of the.",
                    "label": 0
                },
                {
                    "sent": "See terms and then we have to sum over every point in the discretization because that's the the sum.",
                    "label": 0
                },
                {
                    "sent": "Takes the place of the integral operator OK. And then when we discretize that, we get an analogue of the of the continuous dual.",
                    "label": 0
                },
                {
                    "sent": "Sorry when we take the dual of that thing we get an analog of the continuous dual and that's a problem where the variable that we're minimizing over I just change the sign to make the Max into a men.",
                    "label": 0
                },
                {
                    "sent": "We have to minimize over a set which consists of Cartesian product of bulls in two dimension little circles in 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So as a Cartesian product of any of these circles.",
                    "label": 0
                },
                {
                    "sent": "OK, so this turns out to be.",
                    "label": 0
                },
                {
                    "sent": "The dual turns out to be particularly nice, is just a quadratic objective convex quadratic objective with a constraint set that's not a box, but it's the next best thing.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a separable Cartesian product of circles.",
                    "label": 0
                },
                {
                    "sent": "And so one thing that we tried quite recently, Tony Chan and his student means you and I was just doing fairly naive projected gradient on that dual formulation where we had to take.",
                    "label": 0
                },
                {
                    "sent": "We're taking steps in the negative gradient direction and projecting back on to X and all kinds of variants of that trying to use powerslide born step links and trying to introduce some curvature and so on.",
                    "label": 0
                },
                {
                    "sent": "And what we found is maybe not unexpectedly is that if you just want a fairly rough solution or even a moderate accuracy solution, these this first order type approach beats the pants off of 2nd order approach, where you actually do some kind of interior point method which was.",
                    "label": 0
                },
                {
                    "sent": "Something that there was that a lot of people were doing to solve these problems.",
                    "label": 0
                },
                {
                    "sent": "Another thing we're doing that people were doing was taking that first order formulation and applying a second order cone programming formulation.",
                    "label": 0
                },
                {
                    "sent": "Again, that's an interior point method.",
                    "label": 0
                },
                {
                    "sent": "Each iteration is expensive.",
                    "label": 0
                },
                {
                    "sent": "If you want a high accuracy solution, it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "If you just want a solution that looks good in the eyeball norm, it's kind of overkill.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but here's here's the interesting thing.",
                    "label": 0
                },
                {
                    "sent": "Ming Zhu went off and messed around with this a little bit more and came up with an even better approach, and that was rather than just doing gradient descent in the dual.",
                    "label": 0
                },
                {
                    "sent": "He came over this primal dual algorithm that just took alternating steps.",
                    "label": 0
                },
                {
                    "sent": "1st order steps in the primal space in the dual space.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you did was to write down a saddle point formulation of this problem.",
                    "label": 1
                },
                {
                    "sent": "He he wrote the Primal dual together as a min Max.",
                    "label": 0
                },
                {
                    "sent": "Problem well and this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple min Max problem.",
                    "label": 0
                },
                {
                    "sent": "And then the approaches that you just take steps in X.",
                    "label": 0
                },
                {
                    "sent": "The dual variable projected gradient steps.",
                    "label": 0
                },
                {
                    "sent": "You take a step length along the gradient of L with respect to X project back onto the feasible set.",
                    "label": 0
                },
                {
                    "sent": "And then you then you fix X and then you take a steepest descent step in V. OK and you just keep alternating between those two.",
                    "label": 1
                },
                {
                    "sent": "And surprisingly found that from a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "This was incredibly fast.",
                    "label": 1
                },
                {
                    "sent": "Even at finding high, even in funding high accuracy solutions.",
                    "label": 0
                },
                {
                    "sent": "But another interesting thing was the choice of step links is kind of non intuitive.",
                    "label": 0
                },
                {
                    "sent": "Normally in any kind of 1st order gradient descent method you choose a sequence of step links that you either get from align search or is somehow decreasing monotonically with K. The steps get shorter and shorter, but instead what he did was to take the step links in the dual space he made them longer and longer as.",
                    "label": 0
                },
                {
                    "sent": "The iterations went on OK. Now this doesn't mean that you're moving further 'cause this projection back on the X tends to bring you back and tends to make the step actually quite short.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing was he was sort of moving further and further out in along the dual gradient direction.",
                    "label": 0
                },
                {
                    "sent": "The step length in the primal space stayed was decreasing as it turned out.",
                    "label": 0
                },
                {
                    "sent": "So what's the explanation for this?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't think there's yet any analysis that kind of explains us.",
                    "label": 0
                },
                {
                    "sent": "We kind of last fall.",
                    "label": 0
                },
                {
                    "sent": "We thought about this for a lot of different angles using trying to bring a lot of the literature to bear, but.",
                    "label": 0
                },
                {
                    "sent": "Haven't yet found a really satisfying explanation for why this works.",
                    "label": 0
                },
                {
                    "sent": "Someone else might have done it in the meantime, but if.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm really interested to know what it would be.",
                    "label": 0
                },
                {
                    "sent": "But the reason I mentioned this is that I wanted to show a problem that an SVM type problem where this was applicable and this is something that.",
                    "label": 0
                },
                {
                    "sent": "That saying did recently my students saying who I mentioned on the on the 1st slide.",
                    "label": 0
                },
                {
                    "sent": "So he went back and found this semiparametric SVM regression problem in the literature from about 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Paper smaller and scholkopf and someone else and the idea here is that rather than doing purely nonparametric estimation, you first of all you're doing regression with some kind of epsilon insensitive margin.",
                    "label": 0
                },
                {
                    "sent": "Of course you can do other things as well, but the key point here is that the function you're using is not just.",
                    "label": 0
                },
                {
                    "sent": "The classifier is not just nonparametric.",
                    "label": 0
                },
                {
                    "sent": "You also get to bring in some basis functions.",
                    "label": 0
                },
                {
                    "sent": "These side JS and so you can have a parametric part in the.",
                    "label": 0
                },
                {
                    "sent": "In H as well, and the idea is to learn both the nonparametric and parametric part are simultaneously OK.",
                    "label": 0
                },
                {
                    "sent": "So after a bit of manipulation you can write that primal formulation of that regression problem.",
                    "label": 0
                },
                {
                    "sent": "As it has a dual, it looks a lot like the dual that I was talking about earlier in the talk, except that instead of having just a single equality constraint, Now we've got K equality constraints where K is a number of parametric functions.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring in.",
                    "label": 0
                },
                {
                    "sent": "Then you can apply.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A primal dual type method to that you can write it as a minmax problem.",
                    "label": 1
                },
                {
                    "sent": "Sean there with that sort of Lagrangian and you can take alternating steps in the primal variables Alpha and the dual variables ADA.",
                    "label": 1
                },
                {
                    "sent": "And you can introduce some curvature information and do a few other enhancements and you end up with something that actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "There was another algorithm recently by Scholkopf and and keen slow, where they did something.",
                    "label": 0
                },
                {
                    "sent": "Somewhat similar, but there they sort of made more of an effort to solve the primal space problem, and then would occasionally take a step in the dual space or the other way around.",
                    "label": 0
                },
                {
                    "sent": "They would do a lot of iterations in the Alpha and then occasionally adjust the ADA and we just did this thing of taking sort of 1 alternating steps in the two, and it seemed to be faster.",
                    "label": 0
                },
                {
                    "sent": "An eye.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A picture there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I just had a picture here.",
                    "label": 0
                },
                {
                    "sent": "You probably can't see it too well, but it's just to show on a toy problem.",
                    "label": 0
                },
                {
                    "sent": "This was from that original paper.",
                    "label": 0
                },
                {
                    "sent": "Smaller on a toy problem.",
                    "label": 0
                },
                {
                    "sent": "We compare the we have a true function that we're trying to recover, which is given by the blue line.",
                    "label": 0
                },
                {
                    "sent": "The solid blue line.",
                    "label": 0
                },
                {
                    "sent": "We take a bunch of samples on the interval from zero to 10.",
                    "label": 0
                },
                {
                    "sent": "I've got noise in them and then we try fitting a nonparametric estimate, a semiparametric estimate, and a parametric estimate using three basis functions.",
                    "label": 0
                },
                {
                    "sent": "And the semi parametric estimate in this instance does a little bit better job at following the wiggles of the true function.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seems to be some reason to think that this might be interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, I've got a little time left, so I'll talk about some alternative formulations.",
                    "label": 1
                },
                {
                    "sent": "You can turn the classification problem into a purely linear problem.",
                    "label": 0
                },
                {
                    "sent": "If you replace that two norm term with one norm term.",
                    "label": 0
                },
                {
                    "sent": "And various people have done this.",
                    "label": 1
                },
                {
                    "sent": "I'm not sure how satisfying the statistical justification is, but it's certainly nice to have a linear program 'cause there's all kinds of technology to do that.",
                    "label": 0
                },
                {
                    "sent": "If you just blindly apply a linear programming solver, however, you you may not do that well.",
                    "label": 1
                },
                {
                    "sent": "So you have to use sort of specialized solvers on this problem, but one possible advantage of this approach is that it might produce sparse weight vectors, so you might sort of.",
                    "label": 0
                },
                {
                    "sent": "This might do feet be doing feature selection at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time has doing.",
                    "label": 0
                },
                {
                    "sent": "Classification, in fact you could add that W two term back in there and do something like the elastic net where you've got a two norm regularization, Anna one norm regularization as well as a loss function.",
                    "label": 0
                },
                {
                    "sent": "And has anyone tried this in SVM?",
                    "label": 0
                },
                {
                    "sent": "I'd be surprised if someone hadn't tried something like this.",
                    "label": 0
                },
                {
                    "sent": "Hey, you have two knobs to turn and I'm I'm just wondering if this has any interest in doing classification and sort of feature selection at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I thought the last thing that I would talk about.",
                    "label": 0
                },
                {
                    "sent": "Is actually comes from work that I've done with Rob Nowak and Maria Figueredo on compressive sensing.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm for that which actually generalizes to that fairly general statement of regularize optimization that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So let me bring back the general problem of F plus.",
                    "label": 1
                },
                {
                    "sent": "Lambda times are and.",
                    "label": 0
                },
                {
                    "sent": "The idea of this parser algorithm is that you you assume that the regularization term is non smooth back but simple in some sense like.",
                    "label": 0
                },
                {
                    "sent": "The one norm OK?",
                    "label": 0
                },
                {
                    "sent": "Or maybe a group regularizer sum of Infinity norms, or a sum of two norms?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you.",
                    "label": 0
                },
                {
                    "sent": "At each iteration you form a simple model of the F part, which is which is the smooth part.",
                    "label": 1
                },
                {
                    "sent": "And in particular model we use is, we just linearize around the current iterate XC by taking the exact gradient of F and then we add on a quadratic term which is like a tune on penalty.",
                    "label": 1
                },
                {
                    "sent": "Which is the first term at with the 101 / 2 Alpha K?",
                    "label": 0
                },
                {
                    "sent": "Is the coefficient OK?",
                    "label": 0
                },
                {
                    "sent": "You can think of this in a number of ways and then you leave the art term exactly as it is OK. You can think of this in a number of different ways.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a linearisation of the F. With some kind of a trust region so that that first term the quadratic term is kind of the penalty term for staying within a some kind of two norm ball of where you currently are.",
                    "label": 0
                },
                {
                    "sent": "Or you can think of that quadratic term as being some sort of crude approximation to the real 2nd order term, which would have the Hessian of F in it.",
                    "label": 0
                },
                {
                    "sent": "So again, this is like the buzz lowborn motivation where you can think of the one over Alpha as being in some sense an approximation to the Hessian of F. OK, so there are many ways to choose the one over Alpha you can try.",
                    "label": 0
                },
                {
                    "sent": "Increasing and decreasing it to make sure you get dissent every iteration you can do a non monotonic.",
                    "label": 0
                },
                {
                    "sent": "You can settle for a non monotonic strategy and so on.",
                    "label": 0
                },
                {
                    "sent": "And this is basically all sparse.",
                    "label": 0
                },
                {
                    "sent": "There is really nothing to it.",
                    "label": 0
                },
                {
                    "sent": "It's as simple as you know.",
                    "label": 0
                },
                {
                    "sent": "I'm not really covering very much up here.",
                    "label": 0
                },
                {
                    "sent": "The other comment on makers that this kind of approach is that it is good if you want to solve for a range of alphas, because if you get a solution of this regularizer problem for one value of Alpha, you can then decrease Alpha slightly and use your previous solution as a starting point for the next version, so you can do some kind of continuation strategy.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that tends to be quite effective, and as I said, our can be one or more sum of two norms or some Infinity norms.",
                    "label": 0
                },
                {
                    "sent": "What you really need to make this work is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You need this subproblem to be easy to solve.",
                    "label": 0
                },
                {
                    "sent": "And if our is just a one norm, you can find the solution of that subproblem in closed form and just in operations.",
                    "label": 0
                },
                {
                    "sent": "So each subproblem is very cheap and the same is true.",
                    "label": 0
                },
                {
                    "sent": "Is it far is sum of two norms?",
                    "label": 0
                },
                {
                    "sent": "Or you know some sort of group separable regularizer.",
                    "label": 0
                },
                {
                    "sent": "But if R is more complicated like a TV norm, then it's not so easy.",
                    "label": 0
                },
                {
                    "sent": "It's not much easier to solve the subproblem then to solve the original problem.",
                    "label": 0
                },
                {
                    "sent": "So in that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, this approach is not as interesting.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'll just mention that we applied this approach.",
                    "label": 0
                },
                {
                    "sent": "I said we we use it for classic L2L1 compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "Well, there's also some joint work with Grace that she mentioned in her talk, Esther Grace Wahba and his student were young Hershey and some other collaborators.",
                    "label": 0
                },
                {
                    "sent": "Where we do this for regularised logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So here it's the same problem except the fitting term in this case is just is just.",
                    "label": 0
                },
                {
                    "sent": "It is just a logistic regression function and we do exactly this approach.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do a number of enhancements like we tried taking if we think we have identified the non zero terms of X or W or whatever the variable name is.",
                    "label": 0
                },
                {
                    "sent": "We kind of fix them and take a Newton step reduce Newton step in those components and often in these logistic regression problems you're interested in problems where there are only a few non zeros at the solution.",
                    "label": 0
                },
                {
                    "sent": "So it makes sense to evaluate a little piece of the Hessian and to sort of taken enhance 2nd order step in that very tiny subspace.",
                    "label": 0
                },
                {
                    "sent": "Then we also use continuation.",
                    "label": 0
                },
                {
                    "sent": "We sort of.",
                    "label": 0
                },
                {
                    "sent": "So for a bunch of different land values and we end up with something that seems to be quite powerful.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And solving very large instances of this problem, so I'm done at this point I've got a bunch of references.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are papers that I mentioned during the talk and you can't read them from where you are, but I'll be happy to give you the PDF if you want to chase up any of these slides, so I'm done.",
                    "label": 0
                },
                {
                    "sent": "So any questions?",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "For power right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So sorry, I didn't quite catch that.",
                    "label": 0
                },
                {
                    "sent": "Right direction for off measures and machine learning people to make problems more feasible for real world problems.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, it's a kind of a general question.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I guess the answer is that people should just talk to each other more.",
                    "label": 0
                },
                {
                    "sent": "So meetings like this are good because it's always good for, you know optimization.",
                    "label": 0
                },
                {
                    "sent": "People are by nature our community is very interested in doing applications, but unless we get a lot of interaction with the other side we might be working on things that are really not very interesting in machine learning.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, what we have to offer as we do have potentially some tools that with the appropriate customization and adaptations might.",
                    "label": 0
                },
                {
                    "sent": "Turn out to be useful for problems that you want to you actually want to solve.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Curious.",
                    "label": 0
                },
                {
                    "sent": "In some sense, one might hope that as machine learning person you could sort of frame produce a mathematical formulation and sort of go off to you know one of the servers got resolved.",
                    "label": 0
                },
                {
                    "sent": "Stop dictation programs and download the right one and feed your mathematical formulation and machine learning without writing software.",
                    "label": 0
                },
                {
                    "sent": "Special purpose pizza.",
                    "label": 0
                },
                {
                    "sent": "The program for every problem, well, I think it's true that if your problem isn't too big then you can certainly do what you said.",
                    "label": 0
                },
                {
                    "sent": "You can just in fact, that should be the first cut.",
                    "label": 0
                },
                {
                    "sent": "If you've got some prototype data you know you come up with a new formulation you want to test it out as long as it's not too big then there's you can just call off the shelf software and you can get an answer at least.",
                    "label": 0
                },
                {
                    "sent": "But I guess my point is that if your if your concern is really for performance and if you've got a really big data set and.",
                    "label": 0
                },
                {
                    "sent": "And you really do want to solve it in real time and not wait forever.",
                    "label": 0
                },
                {
                    "sent": "That that approach may not be enough that you may have to look a little deeper and and figure out which of these you know.",
                    "label": 0
                },
                {
                    "sent": "Variety of approaches is actually applicable to the problem you want to solve, but I think for prototyping what you're suggesting is certainly OK. What does Monica, Group or researcher?",
                    "label": 0
                },
                {
                    "sent": "Mounted data genetic data out solution.",
                    "label": 0
                },
                {
                    "sent": "Statistician engineer said something problems is testicle.",
                    "label": 0
                },
                {
                    "sent": "Subject matter collaborators.",
                    "label": 0
                },
                {
                    "sent": "Movie theater.",
                    "label": 0
                },
                {
                    "sent": "Optimizing.",
                    "label": 0
                },
                {
                    "sent": "That sounds simple solution.",
                    "label": 0
                },
                {
                    "sent": "More wine Africa.",
                    "label": 0
                },
                {
                    "sent": "A lot of data sets have special structure that motivate people power.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good comment.",
                    "label": 0
                },
                {
                    "sent": "You know the jewels SVF is kind of quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "We can stop by interior point method.",
                    "label": 0
                },
                {
                    "sent": "In order to get direction.",
                    "label": 0
                },
                {
                    "sent": "As we haven't, they cost us very large, so the new system will be huge.",
                    "label": 0
                },
                {
                    "sent": "Air condition.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "After we could measure might like we walk into writing messages, open your system or use it in your appointments over.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's something I just touched on lightly, but that was one of the first things though was tried in the 90s, was in Interior Point solver, and the reason for the reasons you mentioned, it turned out not to be very good.",
                    "label": 0
                },
                {
                    "sent": "So the game, the name of the game, tried it turned out to be to try to find good preconditioners like real life type preconditions in conjunction with it approaches.",
                    "label": 0
                },
                {
                    "sent": "And there's this paper of fine and shine Book from 2001 where they do something like that.",
                    "label": 0
                },
                {
                    "sent": "But again, I you know I wouldn't claim it.",
                    "label": 0
                },
                {
                    "sent": "That's that that approach might be good for certain problems with certain dimensions and so on.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't claim as a panacea for all problems, and that there might be some of the other methods I've mentioned might be better for other.",
                    "label": 0
                },
                {
                    "sent": "For other sort of datasets.",
                    "label": 0
                },
                {
                    "sent": "Faster would be slower than Treski direct master.",
                    "label": 0
                },
                {
                    "sent": "Yeah, again again, it's it's going to be somewhat problem dependent, so if you do it salesky direct method, that's going to be ordering cubed right where N is for each iteration, where is the size of the problem, and even then it might not work, because if the matrix you're factoring might be really ill conditioned and so the choice he might sort of breakdown at some point.",
                    "label": 0
                },
                {
                    "sent": "So there are tricks to fix that up.",
                    "label": 0
                },
                {
                    "sent": "You know you can regularize it and get some kind of inexact step, but but it's very expensive and so.",
                    "label": 0
                },
                {
                    "sent": "The bottom line is that I think to make it this interior method at all.",
                    "label": 0
                },
                {
                    "sent": "Useful for a reasonable sized datasets.",
                    "label": 0
                },
                {
                    "sent": "You probably have to do some kind of preconditioned iterative method like you said, Anne and I think it's just going to be the only thing you can really do rather than actual ASCII type method.",
                    "label": 0
                },
                {
                    "sent": "That's my guess.",
                    "label": 0
                },
                {
                    "sent": "I guess we can thank the speaker and close the session.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}