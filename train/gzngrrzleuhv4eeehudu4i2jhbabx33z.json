{
    "id": "gzngrrzleuhv4eeehudu4i2jhbabx33z",
    "title": "Rare Category Detection for Spatial Data",
    "info": {
        "author": [
            "Jingrui He, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 15, 2009",
        "recorded": "November 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_he_rcd/",
    "segmentation": [
        [
            "I'm getting ready from machine learning Department.",
            "Thanks for coming and today I'll be talking about very category detection.",
            "This is joint work with my advisor, Jaime Carbonell.",
            "So first of all, what is right category detection given a data set, we typically start the novel.",
            "That is, we don't have any labeled examples.",
            "The data set often consists of exam."
        ],
        [
            "Post from both the majority classes which occupies the vast majority of the datasets in the minority classes which have only a few examples.",
            "OK. For instance, in the left figure, the blue dots forming the X shape represent the majority class in the red dots formula, 4 characters represent the minority classes.",
            "Furthermore, we have access to a labeling article which is able to give us the class label of any example with a fixed cost.",
            "Finally.",
            "Our goal is to discover those minority classes with the least labor requests.",
            "Here's how rare category detection works.",
            "We picked an example as the labeling article for his class label.",
            "If it's from the majority class, we put it aside and continue.",
            "Otherwise we output this data point together with this class label.",
            "We keep on doing this until examples from all the minority classes have been discovered.",
            "Yeah."
        ],
        [
            "So very category detection is different from OK question, how do you know when all the classes have been discovered?",
            "OK, so if you have prior information about the number of classes then you can use that as the stopping criterion.",
            "Otherwise you could use some sort of labeling budget to determine when to stop.",
            "If you don't know how many classes there are your data set.",
            "Yeah.",
            "OK. Where category detection is different from outlier detection.",
            "For example, in the left figure, the four characters represent the four minority classes, whereas in the right figure the three Red Stars are three outliers.",
            "OK, in a more principled way, each minority class consists of a group of points.",
            "They are typically clustered in the feature space, and they are often non separable from the majority classes, whereas in outlier detection.",
            "Each outlier is a single data point.",
            "The outliers are typically scattered in the feature space in their office, separable from the normal points."
        ],
        [
            "Rare category detection is also different from traditional active learning.",
            "OK, to be specific, in rare category detection we typically start the novel that is initially we don't have any labeled examples an our goal is to discover those minority classes with only a few label requests or the least number of Labor requests.",
            "On the other hand, in traditional active learning, initially we have labeled examples from all the classes OK.",
            "In our goal is to improve the performance of the current classifier with the least label requests.",
            "OK."
        ],
        [
            "The rare category detection has a lot of applications, for example financial fraud detection.",
            "Where the vast majority of the financial transactions are legitimate and only a small number may be fraudulent.",
            "Network intrusion detection where malicious network activities are difficult are hidden amount.",
            "Huge volumes of routine network traffic.",
            "Astronomy only where only 0.001% of the objects in Sky survey images are truly beyond the scope of current science and may lead to new discoveries.",
            "And finally.",
            "Spam image detection where the near duplicates spam images are difficult to discover from the large number of non spam images."
        ],
        [
            "Here's the big picture for rare category detection.",
            "Given unbalanced, unlabeled data set, our ultimate goal is to construct a classifier so that we can predict the class labels of future ansin.",
            "Examples.",
            "OK to do this, we first perform their category detection in order to discover labeled examples from all the classes.",
            "OK, then, given those labeled examples, we can then perform learning in the unbalanced settings in order to construct the final classifier.",
            "On the other hand.",
            "Real World data comes in various types such as spatial, relational, temporal and so on.",
            "For spatial data, if there are irrelevant features in the data representation, we can first perform feature selection so that the minority classes form very compact clusters in the selected feature subspace.",
            "OK, in this talk I'll focus on rare category detection for spatial data."
        ],
        [
            "Here's the outline of my talk.",
            "I've already covered problem definition.",
            "Next, I'll briefly go through related work, followed by the main part red category detection for spatial data, and then conclude.",
            "So up until now, only a few methods have been proposed for very category detection.",
            "For example in Palo."
        ],
        [
            "And more.",
            "2004 the authors assume the mixture model to fit the data.",
            "An selected examples according to different criteria.",
            "In fine Ms Word 2006, the authors proposed a generic consistency algorithm, an improved upper bounds and lower bounds for this algorithm in some specific situations.",
            "Furthermore, some researchers have applied the outlier detection algorithms for very category detection.",
            "For example, in Papadimitriou ET al.",
            "2003, the authors proposed the lossy algorithm to detect groups of outliers.",
            "OK, all these methods are based on the assumption that the majority classes and the minority classes are separable or near separable.",
            "OK, these three figures illustrate different separability conditions.",
            "The blue dots correspond to the majority class in the red.",
            "The red axis correspond to the minority class.",
            "From left to right, separable near separable non separable.",
            "OK, the last case is the most difficult and yet most common in real applications, so we want to address this situation."
        ],
        [
            "Next, let's come to the main part, where category detection for spatial data first."
        ],
        [
            "The prior dependent methods.",
            "OK. Before diving into the details, let's first look at some notations.",
            "Suppose that we are given a set South of N unlabeled examples, which come from M distinct classes.",
            "For the sake of simplicity, here, we assume that there is only one majority class which corresponds to Y equal 1 and all the other classes are minority classes with prior P2 up to PM.",
            "OK, notice that the prior P1 of the majority class is much larger than any.",
            "Then a prior.",
            "If any minority class.",
            "OK, again our goal is to find at least one example from each minority class by requesting only a few labels, OK?"
        ],
        [
            "Here are two basic assumptions we make about rare category detection.",
            "The first one is the distribution of the majority class is sufficiently smooth.",
            "And the second one is examples from the minority classes form compact clusters in the feature space.",
            "Here I give an example where these assumptions are satisfied.",
            "Here we show a underlying distribution of a 1 dimensional synthetic data set.",
            "The majority class has a Gaussian distribution with large berries and the minority classes correspond to two lower variance peaks.",
            "OK."
        ],
        [
            "Generally speaking, the prior independent methods for rare category detection are based on nearest neighbors.",
            "They essentially perform local density differential sampling and intuition is to select the examples according to the change in local density.",
            "Now let's come to adult part how are?"
        ],
        [
            "This work?",
            "For the binary case, our method is named and be.",
            "It stands for nearest neighbor.",
            "These things for detection and B stands for binary.",
            "This is really not a good name, but will stick to it.",
            "So in this case we only have one minority class and I'll first go through the algorithm very briefly and then look into each step in more detail.",
            "OK, so in NDB we first calculate the class specific radius R prime OK. Then for each data point within our data set we find all of his neighbors within distance R prime OK which belong to the set an exhibi are prime.",
            "Then we set answer by to be the number of examples within this set.",
            "Next we calculate the score for each data point, which is the maximum difference between answer by an insect J wherein some J corresponds to the other data points within distance.",
            "Tee times are prime affected by.",
            "Here T is the iteration index, L explain later.",
            "So the example with the maximum score is presented to the labeling Oracle.",
            "If it's from the minority class, we stop an output this data point.",
            "Otherwise, we increase the value of T the iteration index by one and repeat.",
            "We keep on doing this until we have found an example from the minority class."
        ],
        [
            "Now let's look at each step in more detail.",
            "In MTB, the class specific radius is calculated as follows.",
            "Given the prior P2 of the minority class, we first calculate the number of classes.",
            "The number of examples from that class N * P two and assign it to K. Then for each data point.",
            "We calculate its distance from the case nearest neighbor which is denoted are super case survive.",
            "Finally, the class specific radius is the minimum value of our super cases by.",
            "OK. To better explain how NTP works, I'll use one part of the previous synthetic data set as a running example.",
            "OK, so."
        ],
        [
            "Around each data point we draw a hyper bowl with radius R prime.",
            "The data points within this hyperball belong to the set an annex by our prime, where XY is a staggering point.",
            "OK answer by is the number of examples in this set, which is roughly in proportion to local density."
        ],
        [
            "Next, to calculate the score for the ice data point XY which all another hyper ball with radius tee times are prime centering at exhibi and compare the value of instabuy with end sub J, whereas object corresponds to the other points within this green hyperball.",
            "The score of the idea to point is just the maximum difference between answer by and then subject.",
            "So intuitively, esobi measures the change in local density.",
            "That is, if there is sudden increase in the density at extra by its core would be very large.",
            "OK.",
            "So on the other hand, system minority classes are tightly clustered in the feature space.",
            "There tends to be sudden increase in the density at the boundary of those minority classes.",
            "Therefore, by querying the data point with the maximum score, we have a high probability of finding one example from the minority class.",
            "OK."
        ],
        [
            "If the current selected example is not from the minority class, we increase the value of T by one.",
            "Remember that T is the iteration index and enlarge the green hyperballs accordingly.",
            "OK, next we recalculate the score for each data point and select the data point with the maximum score to be labeled by the article.",
            "And we keep on doing this until we have found an example from the minority class."
        ],
        [
            "So this NDP method is pretty straightforward.",
            "Why does it work?",
            "From a theoretical perspective, we can show that under certain conditions with a high probability after a few iterations steps, our method queries at least one example whose probability of coming from the minority class is at least the third.",
            "OK, so intuitively.",
            "Since the score azzabi measures the change in the local density around exit by the selected example, with the maximum score has a high probability of coming from the minority class.",
            "Furthermore as we increase the iteration index, the selected examples will gradually move from the boundary to the interior of the minority class, as shown in this figure.",
            "OK."
        ],
        [
            "Now let's talk to the more general case where we have more than one minority classes.",
            "So remember that in this case the prior P1 of the majority class is much larger than the prior of any minority class.",
            "In this case, we have the following simple algorithm named Alice.",
            "Which is the abbreviation for active learning for initial class exploration.",
            "It works as follows for each minority class, see if we haven't found any examples from that class.",
            "Run and be with prior P Super C, That is, we trade Class C as the minority class and all the other classes together as a majority class.",
            "Otherwise, we proceed to the next minority class.",
            "So similar as before, we can show that under certain conditions."
        ],
        [
            "With a high probability in each outer loop, with Alice after a few iterations steps in and be Alice queries at least one example whose probability of coming from one minority class is at least the third.",
            "OK, so although this Alex algorithm has sound theoretical proof but."
        ],
        [
            "As a severe problem that is, once the minority class has been discovered, this algorithm may repeatedly select examples from the same minority class now sways the labeling effort.",
            "OK, to address this problem, we have modified Alice to produce the analysis algorithm which incorporates relevance feedback.",
            "OK, so for example, in the previous synthetic data set.",
            "Once we have labeled an example, this green point, any unlabeled examples within the class specific radius distance of this example will be precluded from selection in the future.",
            "In this way we save the labeling effort for the discovery of new minority classes.",
            "OK."
        ],
        [
            "To demonstrate the performance of an Debian Alice, let's first look at two synthetic datasets.",
            "In the two figures, the blue dots again represent the majority class and the red dots correspond to the minority classes.",
            "In the left figure there are 1000 examples from the majority class and only 10 examples from the minority class.",
            "So in order to discover this minority class using NDB, we need to label 101 examples on average, whereas using using random sampling.",
            "Sorry, using random sampling, we need to label 101.",
            "Example is an average, whereas using NDB we only need to label 3 examples marked as green access here.",
            "This right figure shows the synthetic data set we've been using throughout this talk.",
            "There are 3000 examples from the majority class and only 79 examples from the smallest minority class.",
            "In order to discover all the minority classes using random sampling, we need to label 83 examples on average, whereas using malice we only need to label 5 examples marked as green axis.",
            "OK."
        ],
        [
            "We also did experiments on the following two real datasets, abalone and shuttle.",
            "The abalone data set consists of more than 4000 examples described by 7 dimensional features.",
            "They come from 20 classes.",
            "The proportion of the largest class is more than 16% and the proportion of the smallest class is about .34%.",
            "In the shadow data set, we have about 400 four 1500 examples described by 9 dimensional features.",
            "They come from 7 classes.",
            "The proportion of the largest class is more than 75% in the proportion of the smallest class is about 13%.",
            "The reason for choosing these two datasets is because they're both very unbalanced, very skewed."
        ],
        [
            "OK, so besides random sampling we also compare with interleave, which is the prior best method in the literature.",
            "OK, least two figures show the results on abalone an shadow respectively.",
            "The X axis is the number of selected examples and the Y axis is the number of classes discovered.",
            "OK, so the ideal curve would be one that increases very sharply, meaning that it discovers all the classes with the least label requests.",
            "In terms of that, we can see that the performance of malice is much better than the other two.",
            "OK, to be specific with the abalone data set to discover all the classes, malice needs about 125 label requests.",
            "Interleave needs about 280, whereas random sampling needs about 480 on average.",
            "OK, with the shuttle data set to discover all the classes, malice needs about 87 label requests.",
            "Interleave needs about 140.",
            "Whereas random sampling needs more than 500 labor requests.",
            "Yeah.",
            "Yeah yeah is the average of 100 runs?",
            "Yeah yes.",
            "Is it blue?",
            "Oh sorry, yeah, this blue should be interleaving this green should be random sampling.",
            "I'm sorry bout this.",
            "Yeah any other questions?",
            "OK."
        ],
        [
            "Furthermore, remember that in TV and malice those two algorithms need the priors of the minority classes as input.",
            "Next we test the robustness of malice with respect to modest misestimation of the class priors.",
            "OK. To do this, we add and subtract 5 percent, 10% and 20% from the true priors and compare with the results on the last slide.",
            "So the red curve corresponds to reading malice with the correct prior and the blue curves correspond to ready malice with reduced the priors finally.",
            "The green curves correspond to reading malice with increased priors.",
            "OK, as we can see, malice is quite robust to small perturbations in the class priors.",
            "For example, with the abalone data set, if we subtract 10% from the true priors, only one more label request is needed in order to discover all the classes.",
            "OK."
        ],
        [
            "So much for the prior dependent methods.",
            "Now let's come to the prior free method for rare category detection."
        ],
        [
            "Generally speaking, the prior free method for rare category detection is based on density estimation.",
            "It makes use of the special design exponential families.",
            "And the intuition again is to select the examples according to the change in local density.",
            "The major difference between this method and the previous NZB Alice malice algorithms is that we don't need any prior information about the data set as input."
        ],
        [
            "OK, so before introducing our method, let's first look at the specially designed exponential families proposed by Efron and tipsy.",
            "Ronnie in 1996.",
            "So the special design exponential families are a favorable compromise between parametric and nonparametric density estimation.",
            "So in these families the estimated density has the following form where G sub zero.",
            "Is a carrier density.",
            "Dataset 0 is a normalizing parameter which makes this estimated density integrate one.",
            "Beta one is a people one vector parameter vector and finally TF X.",
            "Is a people one vector of sufficient statistics?",
            "OK."
        ],
        [
            "The name of the proposed algorithm is named Cedar, which is the abbreviation for semiparametric density estimation based very category detection.",
            "OK, so it's either.",
            "We use kernel density estimator with the Gaussian kernel as the carrier density and we set the sufficient statistics T effects to be X1 squared up to XD squared where X one is the first component of XXD is the last component of X, and so on.",
            "So in this way the parameter vector beta one is D by 1, right?",
            "OK, so today coupled the estimation of different parameters.",
            "We first decompose beta 0.",
            "This constant into a sum of D items, beta one to beta 0D OK. Then we relax this equality constraint and make each individual beta zero J satisfy the following constraint where.",
            "Peter JOI indicated attendance of this parameter on XJ and Beta.",
            "J One is a JS component of beta one parameter vector.",
            "OK.",
            "So instead are the most imp."
        ],
        [
            "And component is parameter estimation.",
            "We can show that the maximum likelihood estimate of beta one and beta zero satisfies the following condition.",
            "That is, the sum of the J component squared is equal to a complex weighted combination of E Super Jason by XJ squared, where E super JE super aggressive.",
            "I'd XJ squared is the expected value of X squared with this back to the distribution based on XIJ.",
            "A lot of math here.",
            "OK, so at the first glance this equation is very difficult to solve.",
            "So to simplify, we replace beta J1 with the function of BJ where BJ is a positive parameter."
        ],
        [
            "So in this way this equation can be approximately solved as follows.",
            "That is, for the Jays feature component, the estimated BG value BJ hat has this form, where B is the square of the bandwidth for the Jays component, C is the average of the JS component squared.",
            "an A is a complex weighted combination of zij squared.",
            "For rare category detection.",
            "In most of the cases, the estimated BG value bij het is less than or equal to 1.",
            "In this way, the estimated beta J1 value is less than or equal to 0."
        ],
        [
            "Once we have estimated all the BJ values, the estimated density has the following form.",
            "Based on which we can calculate the change in local density for each data point.",
            "Note that for each data point, if we pick a different direction, the changing local density would be different, right?",
            "OK, so it's either we manage the change in local density along the gradient which gives us the maximum change in local density.",
            "And we set the score of each data point to be the norm of the gradient which has this form, where these are by X.",
            "Is the contribution of the I Theta point to the estimated density at X?",
            "OK.",
            "Finally, the data point with the maximum score is again presented to the labeling article, and we keep on labeling those examples until we have found we have discovered all the minority classes.",
            "OK."
        ],
        [
            "So to demonstrate the performance of Theta again as located 2 synthetic datasets first in the left figure it shows the underlying distribution of 1 dimensional synthetic data set.",
            "As I have mentioned before, the majority class has a Gaussian distribution with large variance and the minority classes correspond to the two lower various pigs.",
            "The Red Stars represent the two first selected examples.",
            "By Seder we can see that they are both from the regions where the density changes the most.",
            "In the right figure, again, the blue dots represent the majority class and the black dots represent the minority classes.",
            "We can see that there are all of different shapes.",
            "So for this data set, if we use random sampling in order to discover all the classes we need to label 50 examples on average, whereas using Seder we only need to label 6 examples marked as red stars here.",
            "And finally, we also did experiments on the."
        ],
        [
            "Calling 5 real datasets.",
            "In this table, the second column is the number of number of examples in the data set.",
            "The third column is the dimensionality of the feature space.",
            "The 4th Column is the number of classes.",
            "The 5th Column is the proportion of the largest class and the last column is the proportion of the smallest class.",
            "According to the proportion of the smallest class.",
            "The first 2 datasets are referred to as moderately skewed, whereas the last three datasets are referred to as extremely skewed.",
            "So let's look at the performance on the model really skewed data set first.",
            "That is actually an glass which correspond to those two figures respectively.",
            "Besides malice random sampling an interleave, we also compare with kernel, that is, to use kernel density estimator to estimate the density and also to get the scores.",
            "OK, so from these results we can see that."
        ],
        [
            "If a data set is moderately skewed, the performance of Cedar.",
            "Which correspond to the red curve is better than or can parable with malice the blue curve, although malice is given the additional information about the number of classes, the proportion of different classes, and so on.",
            "OK, finally.",
            "The extreme."
        ],
        [
            "Skewed datasets page blocks Apollonian shuttle.",
            "OK, so from these results we have the following observations, first fader, random sampling and kernel are all prior free.",
            "They don't need any prior information about the data set as input, and yet theater is much better than the other two.",
            "OK. 2nd, If the minority classes are not separable from the majority classes, the performance of interleave which correspond to the black curve is not as good as theater.",
            "OK.",
            "So although interleaves given the additional information about the number of classes in the data set.",
            "And finally, for an extremely skewed data set, although the performance of theater is not as good as malice in some real applications, it is sometimes difficult to get all the information needed by malice, including the number of classes in a data set, the proportion of different classes, and so on."
        ],
        [
            "Finally, to conclude, in this talk I have introduced rare category detection.",
            "It's a big challenge in machine learning, but lack of effective methods.",
            "So I have first introduced the prior independent methods.",
            "They are based on nearest neighbors and they essentially perform local density differential sampling.",
            "Then I introduced the prior free method which is based on density estimation.",
            "It makes use of the specially designed exponential families.",
            "The intuition behind all these methods is to select the examples from the regions where the density changes the most.",
            "As for the question of which algorithm to use in a real application, well it depends on how much very information we have about this data set, such as the number of classes, the proportion of different classes, and so on.",
            "If we have all that information, we should use them aliste algorithm.",
            "Otherwise we should use the theater algorithm in order to guarantee a good performance.",
            "He any situation."
        ],
        [
            "That's all, thank you.",
            "OK, any questions?"
        ],
        [
            "Yes.",
            "Colonel, sure.",
            "So remember that insider?",
            "We use the specialty."
        ],
        [
            "An exponential families, with some modifications to estimate the density and also to get the score for each data point.",
            "The score is the norm of the gradient at that data point, right?",
            "So in principle we can use other density estimator to estimate the density and also to get the score right.",
            "This kernel is just to use the Gaussian to use the kernel density estimator with the Gaussian kernel to estimate the density and to calculate the score for each data point, yeah?",
            "Yes, did you try your your method with high dimensional datasets now, so as you can see.",
            "The datasets I've used so far are not very high dimensional.",
            "The higher dimensionality is 10, so there might be some problem."
        ],
        [
            "With very high dimensional data set, but for those datasets we can first perform feature selection to yet to lower down the dimensionality of the feature space.",
            "Yeah, that's a possibility.",
            "Yeah, yes.",
            "Category sparser than the majority class, because in your all your example.",
            "You mean?",
            "So if we compare the density of the majority class in minority class in the support region, if the density of the minority class larger than the majority class.",
            "That's the case for the synthetic data set.",
            "1 dimensional synthetic data set, but that's not necessarily true.",
            "Here I deliberately enlarge the font."
        ],
        [
            "Of each, each of the points here.",
            "Yeah, so they look larger and denser in here.",
            "Yeah, yeah yeah but that's yeah, but for this case the density of the minority class is larger than the majority class in the support region for the minority class, yeah?",
            "Any other questions, yeah?",
            "Early theorems you have probably have 1/3.",
            "Why that?",
            "Get up the likelihood probability 1/3 of it.",
            "That's a good question.",
            "That's the result from some geometry."
        ],
        [
            "Calculation, yeah, and."
        ],
        [
            "Now I cannot think of any clear reason why is 1/3.",
            "Yeah, that is definitely better than random sampling, especially for the extremely skewed datasets.",
            "Yeah.",
            "Any other questions?"
        ],
        [
            "Yes.",
            "The breaker clusters apart, so first of all I should say that for higher dimensional for very high dimensional data set is not a good idea to use existing dimensionality reduction techniques because you know for the skewed data set the minority classes examples from those minority classes are overwhelmed by the examples from the majority class.",
            "So if you use the usual dimensionality reduction techniques then the selected features.",
            "Or the lower dimensional space would be relevant to the majority class instead of the minority class, so we should avoid using those techniques on the skewed datasets and we should use some special techniques to deal with this problem, yeah?",
            "Yes.",
            "Right now there is no special techniques for dimensionality reduction for the imbalanced data set, but I'm working on it now.",
            "Yeah, any suggestions?",
            "Comments are welcome, yeah?",
            "Other questions.",
            "But could you give us a little bit more detail about the interleave algorithm?",
            "And why do you think your algorithm is performing outperforming?",
            "So as I have mentioned, interleave algorithm, assume the mixture model to fit data, right?",
            "So the might not.",
            "Minority class also corresponds to a mixture component in the mixture model, right?",
            "So if the minority class is not separable from the majority class, it's very difficult to identify this minority class using a individual mixture component, right?",
            "Yeah, that's probably the reason why it's not performing as well as the proposed algorithms, yeah?",
            "Other questions.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm getting ready from machine learning Department.",
                    "label": 1
                },
                {
                    "sent": "Thanks for coming and today I'll be talking about very category detection.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my advisor, Jaime Carbonell.",
                    "label": 1
                },
                {
                    "sent": "So first of all, what is right category detection given a data set, we typically start the novel.",
                    "label": 0
                },
                {
                    "sent": "That is, we don't have any labeled examples.",
                    "label": 0
                },
                {
                    "sent": "The data set often consists of exam.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Post from both the majority classes which occupies the vast majority of the datasets in the minority classes which have only a few examples.",
                    "label": 1
                },
                {
                    "sent": "OK. For instance, in the left figure, the blue dots forming the X shape represent the majority class in the red dots formula, 4 characters represent the minority classes.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we have access to a labeling article which is able to give us the class label of any example with a fixed cost.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 1
                },
                {
                    "sent": "Our goal is to discover those minority classes with the least labor requests.",
                    "label": 0
                },
                {
                    "sent": "Here's how rare category detection works.",
                    "label": 1
                },
                {
                    "sent": "We picked an example as the labeling article for his class label.",
                    "label": 0
                },
                {
                    "sent": "If it's from the majority class, we put it aside and continue.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we output this data point together with this class label.",
                    "label": 0
                },
                {
                    "sent": "We keep on doing this until examples from all the minority classes have been discovered.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very category detection is different from OK question, how do you know when all the classes have been discovered?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have prior information about the number of classes then you can use that as the stopping criterion.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you could use some sort of labeling budget to determine when to stop.",
                    "label": 0
                },
                {
                    "sent": "If you don't know how many classes there are your data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. Where category detection is different from outlier detection.",
                    "label": 0
                },
                {
                    "sent": "For example, in the left figure, the four characters represent the four minority classes, whereas in the right figure the three Red Stars are three outliers.",
                    "label": 0
                },
                {
                    "sent": "OK, in a more principled way, each minority class consists of a group of points.",
                    "label": 1
                },
                {
                    "sent": "They are typically clustered in the feature space, and they are often non separable from the majority classes, whereas in outlier detection.",
                    "label": 1
                },
                {
                    "sent": "Each outlier is a single data point.",
                    "label": 0
                },
                {
                    "sent": "The outliers are typically scattered in the feature space in their office, separable from the normal points.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rare category detection is also different from traditional active learning.",
                    "label": 1
                },
                {
                    "sent": "OK, to be specific, in rare category detection we typically start the novel that is initially we don't have any labeled examples an our goal is to discover those minority classes with only a few label requests or the least number of Labor requests.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in traditional active learning, initially we have labeled examples from all the classes OK.",
                    "label": 0
                },
                {
                    "sent": "In our goal is to improve the performance of the current classifier with the least label requests.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rare category detection has a lot of applications, for example financial fraud detection.",
                    "label": 0
                },
                {
                    "sent": "Where the vast majority of the financial transactions are legitimate and only a small number may be fraudulent.",
                    "label": 0
                },
                {
                    "sent": "Network intrusion detection where malicious network activities are difficult are hidden amount.",
                    "label": 1
                },
                {
                    "sent": "Huge volumes of routine network traffic.",
                    "label": 0
                },
                {
                    "sent": "Astronomy only where only 0.001% of the objects in Sky survey images are truly beyond the scope of current science and may lead to new discoveries.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                },
                {
                    "sent": "Spam image detection where the near duplicates spam images are difficult to discover from the large number of non spam images.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the big picture for rare category detection.",
                    "label": 1
                },
                {
                    "sent": "Given unbalanced, unlabeled data set, our ultimate goal is to construct a classifier so that we can predict the class labels of future ansin.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "OK to do this, we first perform their category detection in order to discover labeled examples from all the classes.",
                    "label": 0
                },
                {
                    "sent": "OK, then, given those labeled examples, we can then perform learning in the unbalanced settings in order to construct the final classifier.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 1
                },
                {
                    "sent": "Real World data comes in various types such as spatial, relational, temporal and so on.",
                    "label": 0
                },
                {
                    "sent": "For spatial data, if there are irrelevant features in the data representation, we can first perform feature selection so that the minority classes form very compact clusters in the selected feature subspace.",
                    "label": 0
                },
                {
                    "sent": "OK, in this talk I'll focus on rare category detection for spatial data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I've already covered problem definition.",
                    "label": 1
                },
                {
                    "sent": "Next, I'll briefly go through related work, followed by the main part red category detection for spatial data, and then conclude.",
                    "label": 1
                },
                {
                    "sent": "So up until now, only a few methods have been proposed for very category detection.",
                    "label": 0
                },
                {
                    "sent": "For example in Palo.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more.",
                    "label": 0
                },
                {
                    "sent": "2004 the authors assume the mixture model to fit the data.",
                    "label": 1
                },
                {
                    "sent": "An selected examples according to different criteria.",
                    "label": 0
                },
                {
                    "sent": "In fine Ms Word 2006, the authors proposed a generic consistency algorithm, an improved upper bounds and lower bounds for this algorithm in some specific situations.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, some researchers have applied the outlier detection algorithms for very category detection.",
                    "label": 1
                },
                {
                    "sent": "For example, in Papadimitriou ET al.",
                    "label": 0
                },
                {
                    "sent": "2003, the authors proposed the lossy algorithm to detect groups of outliers.",
                    "label": 0
                },
                {
                    "sent": "OK, all these methods are based on the assumption that the majority classes and the minority classes are separable or near separable.",
                    "label": 0
                },
                {
                    "sent": "OK, these three figures illustrate different separability conditions.",
                    "label": 0
                },
                {
                    "sent": "The blue dots correspond to the majority class in the red.",
                    "label": 0
                },
                {
                    "sent": "The red axis correspond to the minority class.",
                    "label": 0
                },
                {
                    "sent": "From left to right, separable near separable non separable.",
                    "label": 0
                },
                {
                    "sent": "OK, the last case is the most difficult and yet most common in real applications, so we want to address this situation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, let's come to the main part, where category detection for spatial data first.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The prior dependent methods.",
                    "label": 0
                },
                {
                    "sent": "OK. Before diving into the details, let's first look at some notations.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we are given a set South of N unlabeled examples, which come from M distinct classes.",
                    "label": 0
                },
                {
                    "sent": "For the sake of simplicity, here, we assume that there is only one majority class which corresponds to Y equal 1 and all the other classes are minority classes with prior P2 up to PM.",
                    "label": 0
                },
                {
                    "sent": "OK, notice that the prior P1 of the majority class is much larger than any.",
                    "label": 0
                },
                {
                    "sent": "Then a prior.",
                    "label": 0
                },
                {
                    "sent": "If any minority class.",
                    "label": 0
                },
                {
                    "sent": "OK, again our goal is to find at least one example from each minority class by requesting only a few labels, OK?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are two basic assumptions we make about rare category detection.",
                    "label": 0
                },
                {
                    "sent": "The first one is the distribution of the majority class is sufficiently smooth.",
                    "label": 1
                },
                {
                    "sent": "And the second one is examples from the minority classes form compact clusters in the feature space.",
                    "label": 1
                },
                {
                    "sent": "Here I give an example where these assumptions are satisfied.",
                    "label": 0
                },
                {
                    "sent": "Here we show a underlying distribution of a 1 dimensional synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "The majority class has a Gaussian distribution with large berries and the minority classes correspond to two lower variance peaks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generally speaking, the prior independent methods for rare category detection are based on nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "They essentially perform local density differential sampling and intuition is to select the examples according to the change in local density.",
                    "label": 1
                },
                {
                    "sent": "Now let's come to adult part how are?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This work?",
                    "label": 0
                },
                {
                    "sent": "For the binary case, our method is named and be.",
                    "label": 0
                },
                {
                    "sent": "It stands for nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "These things for detection and B stands for binary.",
                    "label": 0
                },
                {
                    "sent": "This is really not a good name, but will stick to it.",
                    "label": 0
                },
                {
                    "sent": "So in this case we only have one minority class and I'll first go through the algorithm very briefly and then look into each step in more detail.",
                    "label": 0
                },
                {
                    "sent": "OK, so in NDB we first calculate the class specific radius R prime OK. Then for each data point within our data set we find all of his neighbors within distance R prime OK which belong to the set an exhibi are prime.",
                    "label": 0
                },
                {
                    "sent": "Then we set answer by to be the number of examples within this set.",
                    "label": 0
                },
                {
                    "sent": "Next we calculate the score for each data point, which is the maximum difference between answer by an insect J wherein some J corresponds to the other data points within distance.",
                    "label": 0
                },
                {
                    "sent": "Tee times are prime affected by.",
                    "label": 0
                },
                {
                    "sent": "Here T is the iteration index, L explain later.",
                    "label": 0
                },
                {
                    "sent": "So the example with the maximum score is presented to the labeling Oracle.",
                    "label": 0
                },
                {
                    "sent": "If it's from the minority class, we stop an output this data point.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we increase the value of T the iteration index by one and repeat.",
                    "label": 0
                },
                {
                    "sent": "We keep on doing this until we have found an example from the minority class.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's look at each step in more detail.",
                    "label": 0
                },
                {
                    "sent": "In MTB, the class specific radius is calculated as follows.",
                    "label": 0
                },
                {
                    "sent": "Given the prior P2 of the minority class, we first calculate the number of classes.",
                    "label": 1
                },
                {
                    "sent": "The number of examples from that class N * P two and assign it to K. Then for each data point.",
                    "label": 1
                },
                {
                    "sent": "We calculate its distance from the case nearest neighbor which is denoted are super case survive.",
                    "label": 0
                },
                {
                    "sent": "Finally, the class specific radius is the minimum value of our super cases by.",
                    "label": 0
                },
                {
                    "sent": "OK. To better explain how NTP works, I'll use one part of the previous synthetic data set as a running example.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around each data point we draw a hyper bowl with radius R prime.",
                    "label": 0
                },
                {
                    "sent": "The data points within this hyperball belong to the set an annex by our prime, where XY is a staggering point.",
                    "label": 0
                },
                {
                    "sent": "OK answer by is the number of examples in this set, which is roughly in proportion to local density.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, to calculate the score for the ice data point XY which all another hyper ball with radius tee times are prime centering at exhibi and compare the value of instabuy with end sub J, whereas object corresponds to the other points within this green hyperball.",
                    "label": 0
                },
                {
                    "sent": "The score of the idea to point is just the maximum difference between answer by and then subject.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, esobi measures the change in local density.",
                    "label": 0
                },
                {
                    "sent": "That is, if there is sudden increase in the density at extra by its core would be very large.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, system minority classes are tightly clustered in the feature space.",
                    "label": 0
                },
                {
                    "sent": "There tends to be sudden increase in the density at the boundary of those minority classes.",
                    "label": 0
                },
                {
                    "sent": "Therefore, by querying the data point with the maximum score, we have a high probability of finding one example from the minority class.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the current selected example is not from the minority class, we increase the value of T by one.",
                    "label": 0
                },
                {
                    "sent": "Remember that T is the iteration index and enlarge the green hyperballs accordingly.",
                    "label": 0
                },
                {
                    "sent": "OK, next we recalculate the score for each data point and select the data point with the maximum score to be labeled by the article.",
                    "label": 0
                },
                {
                    "sent": "And we keep on doing this until we have found an example from the minority class.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this NDP method is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "Why does it work?",
                    "label": 0
                },
                {
                    "sent": "From a theoretical perspective, we can show that under certain conditions with a high probability after a few iterations steps, our method queries at least one example whose probability of coming from the minority class is at least the third.",
                    "label": 1
                },
                {
                    "sent": "OK, so intuitively.",
                    "label": 0
                },
                {
                    "sent": "Since the score azzabi measures the change in the local density around exit by the selected example, with the maximum score has a high probability of coming from the minority class.",
                    "label": 0
                },
                {
                    "sent": "Furthermore as we increase the iteration index, the selected examples will gradually move from the boundary to the interior of the minority class, as shown in this figure.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's talk to the more general case where we have more than one minority classes.",
                    "label": 0
                },
                {
                    "sent": "So remember that in this case the prior P1 of the majority class is much larger than the prior of any minority class.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have the following simple algorithm named Alice.",
                    "label": 1
                },
                {
                    "sent": "Which is the abbreviation for active learning for initial class exploration.",
                    "label": 0
                },
                {
                    "sent": "It works as follows for each minority class, see if we haven't found any examples from that class.",
                    "label": 1
                },
                {
                    "sent": "Run and be with prior P Super C, That is, we trade Class C as the minority class and all the other classes together as a majority class.",
                    "label": 1
                },
                {
                    "sent": "Otherwise, we proceed to the next minority class.",
                    "label": 0
                },
                {
                    "sent": "So similar as before, we can show that under certain conditions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a high probability in each outer loop, with Alice after a few iterations steps in and be Alice queries at least one example whose probability of coming from one minority class is at least the third.",
                    "label": 0
                },
                {
                    "sent": "OK, so although this Alex algorithm has sound theoretical proof but.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a severe problem that is, once the minority class has been discovered, this algorithm may repeatedly select examples from the same minority class now sways the labeling effort.",
                    "label": 1
                },
                {
                    "sent": "OK, to address this problem, we have modified Alice to produce the analysis algorithm which incorporates relevance feedback.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, in the previous synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "Once we have labeled an example, this green point, any unlabeled examples within the class specific radius distance of this example will be precluded from selection in the future.",
                    "label": 0
                },
                {
                    "sent": "In this way we save the labeling effort for the discovery of new minority classes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To demonstrate the performance of an Debian Alice, let's first look at two synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "In the two figures, the blue dots again represent the majority class and the red dots correspond to the minority classes.",
                    "label": 0
                },
                {
                    "sent": "In the left figure there are 1000 examples from the majority class and only 10 examples from the minority class.",
                    "label": 0
                },
                {
                    "sent": "So in order to discover this minority class using NDB, we need to label 101 examples on average, whereas using using random sampling.",
                    "label": 0
                },
                {
                    "sent": "Sorry, using random sampling, we need to label 101.",
                    "label": 0
                },
                {
                    "sent": "Example is an average, whereas using NDB we only need to label 3 examples marked as green access here.",
                    "label": 0
                },
                {
                    "sent": "This right figure shows the synthetic data set we've been using throughout this talk.",
                    "label": 0
                },
                {
                    "sent": "There are 3000 examples from the majority class and only 79 examples from the smallest minority class.",
                    "label": 0
                },
                {
                    "sent": "In order to discover all the minority classes using random sampling, we need to label 83 examples on average, whereas using malice we only need to label 5 examples marked as green axis.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also did experiments on the following two real datasets, abalone and shuttle.",
                    "label": 0
                },
                {
                    "sent": "The abalone data set consists of more than 4000 examples described by 7 dimensional features.",
                    "label": 1
                },
                {
                    "sent": "They come from 20 classes.",
                    "label": 0
                },
                {
                    "sent": "The proportion of the largest class is more than 16% and the proportion of the smallest class is about .34%.",
                    "label": 1
                },
                {
                    "sent": "In the shadow data set, we have about 400 four 1500 examples described by 9 dimensional features.",
                    "label": 0
                },
                {
                    "sent": "They come from 7 classes.",
                    "label": 0
                },
                {
                    "sent": "The proportion of the largest class is more than 75% in the proportion of the smallest class is about 13%.",
                    "label": 0
                },
                {
                    "sent": "The reason for choosing these two datasets is because they're both very unbalanced, very skewed.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so besides random sampling we also compare with interleave, which is the prior best method in the literature.",
                    "label": 0
                },
                {
                    "sent": "OK, least two figures show the results on abalone an shadow respectively.",
                    "label": 1
                },
                {
                    "sent": "The X axis is the number of selected examples and the Y axis is the number of classes discovered.",
                    "label": 0
                },
                {
                    "sent": "OK, so the ideal curve would be one that increases very sharply, meaning that it discovers all the classes with the least label requests.",
                    "label": 0
                },
                {
                    "sent": "In terms of that, we can see that the performance of malice is much better than the other two.",
                    "label": 0
                },
                {
                    "sent": "OK, to be specific with the abalone data set to discover all the classes, malice needs about 125 label requests.",
                    "label": 1
                },
                {
                    "sent": "Interleave needs about 280, whereas random sampling needs about 480 on average.",
                    "label": 0
                },
                {
                    "sent": "OK, with the shuttle data set to discover all the classes, malice needs about 87 label requests.",
                    "label": 0
                },
                {
                    "sent": "Interleave needs about 140.",
                    "label": 0
                },
                {
                    "sent": "Whereas random sampling needs more than 500 labor requests.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah is the average of 100 runs?",
                    "label": 0
                },
                {
                    "sent": "Yeah yes.",
                    "label": 0
                },
                {
                    "sent": "Is it blue?",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, yeah, this blue should be interleaving this green should be random sampling.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry bout this.",
                    "label": 0
                },
                {
                    "sent": "Yeah any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, remember that in TV and malice those two algorithms need the priors of the minority classes as input.",
                    "label": 0
                },
                {
                    "sent": "Next we test the robustness of malice with respect to modest misestimation of the class priors.",
                    "label": 0
                },
                {
                    "sent": "OK. To do this, we add and subtract 5 percent, 10% and 20% from the true priors and compare with the results on the last slide.",
                    "label": 0
                },
                {
                    "sent": "So the red curve corresponds to reading malice with the correct prior and the blue curves correspond to ready malice with reduced the priors finally.",
                    "label": 0
                },
                {
                    "sent": "The green curves correspond to reading malice with increased priors.",
                    "label": 0
                },
                {
                    "sent": "OK, as we can see, malice is quite robust to small perturbations in the class priors.",
                    "label": 0
                },
                {
                    "sent": "For example, with the abalone data set, if we subtract 10% from the true priors, only one more label request is needed in order to discover all the classes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much for the prior dependent methods.",
                    "label": 0
                },
                {
                    "sent": "Now let's come to the prior free method for rare category detection.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generally speaking, the prior free method for rare category detection is based on density estimation.",
                    "label": 0
                },
                {
                    "sent": "It makes use of the special design exponential families.",
                    "label": 1
                },
                {
                    "sent": "And the intuition again is to select the examples according to the change in local density.",
                    "label": 1
                },
                {
                    "sent": "The major difference between this method and the previous NZB Alice malice algorithms is that we don't need any prior information about the data set as input.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so before introducing our method, let's first look at the specially designed exponential families proposed by Efron and tipsy.",
                    "label": 0
                },
                {
                    "sent": "Ronnie in 1996.",
                    "label": 0
                },
                {
                    "sent": "So the special design exponential families are a favorable compromise between parametric and nonparametric density estimation.",
                    "label": 1
                },
                {
                    "sent": "So in these families the estimated density has the following form where G sub zero.",
                    "label": 0
                },
                {
                    "sent": "Is a carrier density.",
                    "label": 1
                },
                {
                    "sent": "Dataset 0 is a normalizing parameter which makes this estimated density integrate one.",
                    "label": 0
                },
                {
                    "sent": "Beta one is a people one vector parameter vector and finally TF X.",
                    "label": 1
                },
                {
                    "sent": "Is a people one vector of sufficient statistics?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The name of the proposed algorithm is named Cedar, which is the abbreviation for semiparametric density estimation based very category detection.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's either.",
                    "label": 0
                },
                {
                    "sent": "We use kernel density estimator with the Gaussian kernel as the carrier density and we set the sufficient statistics T effects to be X1 squared up to XD squared where X one is the first component of XXD is the last component of X, and so on.",
                    "label": 1
                },
                {
                    "sent": "So in this way the parameter vector beta one is D by 1, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so today coupled the estimation of different parameters.",
                    "label": 1
                },
                {
                    "sent": "We first decompose beta 0.",
                    "label": 0
                },
                {
                    "sent": "This constant into a sum of D items, beta one to beta 0D OK. Then we relax this equality constraint and make each individual beta zero J satisfy the following constraint where.",
                    "label": 0
                },
                {
                    "sent": "Peter JOI indicated attendance of this parameter on XJ and Beta.",
                    "label": 0
                },
                {
                    "sent": "J One is a JS component of beta one parameter vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So instead are the most imp.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And component is parameter estimation.",
                    "label": 1
                },
                {
                    "sent": "We can show that the maximum likelihood estimate of beta one and beta zero satisfies the following condition.",
                    "label": 1
                },
                {
                    "sent": "That is, the sum of the J component squared is equal to a complex weighted combination of E Super Jason by XJ squared, where E super JE super aggressive.",
                    "label": 0
                },
                {
                    "sent": "I'd XJ squared is the expected value of X squared with this back to the distribution based on XIJ.",
                    "label": 0
                },
                {
                    "sent": "A lot of math here.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the first glance this equation is very difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "So to simplify, we replace beta J1 with the function of BJ where BJ is a positive parameter.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this way this equation can be approximately solved as follows.",
                    "label": 0
                },
                {
                    "sent": "That is, for the Jays feature component, the estimated BG value BJ hat has this form, where B is the square of the bandwidth for the Jays component, C is the average of the JS component squared.",
                    "label": 0
                },
                {
                    "sent": "an A is a complex weighted combination of zij squared.",
                    "label": 0
                },
                {
                    "sent": "For rare category detection.",
                    "label": 0
                },
                {
                    "sent": "In most of the cases, the estimated BG value bij het is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "In this way, the estimated beta J1 value is less than or equal to 0.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we have estimated all the BJ values, the estimated density has the following form.",
                    "label": 0
                },
                {
                    "sent": "Based on which we can calculate the change in local density for each data point.",
                    "label": 0
                },
                {
                    "sent": "Note that for each data point, if we pick a different direction, the changing local density would be different, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so it's either we manage the change in local density along the gradient which gives us the maximum change in local density.",
                    "label": 0
                },
                {
                    "sent": "And we set the score of each data point to be the norm of the gradient which has this form, where these are by X.",
                    "label": 1
                },
                {
                    "sent": "Is the contribution of the I Theta point to the estimated density at X?",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Finally, the data point with the maximum score is again presented to the labeling article, and we keep on labeling those examples until we have found we have discovered all the minority classes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to demonstrate the performance of Theta again as located 2 synthetic datasets first in the left figure it shows the underlying distribution of 1 dimensional synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "As I have mentioned before, the majority class has a Gaussian distribution with large variance and the minority classes correspond to the two lower various pigs.",
                    "label": 0
                },
                {
                    "sent": "The Red Stars represent the two first selected examples.",
                    "label": 0
                },
                {
                    "sent": "By Seder we can see that they are both from the regions where the density changes the most.",
                    "label": 0
                },
                {
                    "sent": "In the right figure, again, the blue dots represent the majority class and the black dots represent the minority classes.",
                    "label": 0
                },
                {
                    "sent": "We can see that there are all of different shapes.",
                    "label": 0
                },
                {
                    "sent": "So for this data set, if we use random sampling in order to discover all the classes we need to label 50 examples on average, whereas using Seder we only need to label 6 examples marked as red stars here.",
                    "label": 0
                },
                {
                    "sent": "And finally, we also did experiments on the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Calling 5 real datasets.",
                    "label": 0
                },
                {
                    "sent": "In this table, the second column is the number of number of examples in the data set.",
                    "label": 0
                },
                {
                    "sent": "The third column is the dimensionality of the feature space.",
                    "label": 0
                },
                {
                    "sent": "The 4th Column is the number of classes.",
                    "label": 0
                },
                {
                    "sent": "The 5th Column is the proportion of the largest class and the last column is the proportion of the smallest class.",
                    "label": 1
                },
                {
                    "sent": "According to the proportion of the smallest class.",
                    "label": 1
                },
                {
                    "sent": "The first 2 datasets are referred to as moderately skewed, whereas the last three datasets are referred to as extremely skewed.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the performance on the model really skewed data set first.",
                    "label": 0
                },
                {
                    "sent": "That is actually an glass which correspond to those two figures respectively.",
                    "label": 0
                },
                {
                    "sent": "Besides malice random sampling an interleave, we also compare with kernel, that is, to use kernel density estimator to estimate the density and also to get the scores.",
                    "label": 0
                },
                {
                    "sent": "OK, so from these results we can see that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If a data set is moderately skewed, the performance of Cedar.",
                    "label": 1
                },
                {
                    "sent": "Which correspond to the red curve is better than or can parable with malice the blue curve, although malice is given the additional information about the number of classes, the proportion of different classes, and so on.",
                    "label": 1
                },
                {
                    "sent": "OK, finally.",
                    "label": 0
                },
                {
                    "sent": "The extreme.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skewed datasets page blocks Apollonian shuttle.",
                    "label": 1
                },
                {
                    "sent": "OK, so from these results we have the following observations, first fader, random sampling and kernel are all prior free.",
                    "label": 0
                },
                {
                    "sent": "They don't need any prior information about the data set as input, and yet theater is much better than the other two.",
                    "label": 0
                },
                {
                    "sent": "OK. 2nd, If the minority classes are not separable from the majority classes, the performance of interleave which correspond to the black curve is not as good as theater.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So although interleaves given the additional information about the number of classes in the data set.",
                    "label": 1
                },
                {
                    "sent": "And finally, for an extremely skewed data set, although the performance of theater is not as good as malice in some real applications, it is sometimes difficult to get all the information needed by malice, including the number of classes in a data set, the proportion of different classes, and so on.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, to conclude, in this talk I have introduced rare category detection.",
                    "label": 1
                },
                {
                    "sent": "It's a big challenge in machine learning, but lack of effective methods.",
                    "label": 1
                },
                {
                    "sent": "So I have first introduced the prior independent methods.",
                    "label": 1
                },
                {
                    "sent": "They are based on nearest neighbors and they essentially perform local density differential sampling.",
                    "label": 0
                },
                {
                    "sent": "Then I introduced the prior free method which is based on density estimation.",
                    "label": 1
                },
                {
                    "sent": "It makes use of the specially designed exponential families.",
                    "label": 0
                },
                {
                    "sent": "The intuition behind all these methods is to select the examples from the regions where the density changes the most.",
                    "label": 0
                },
                {
                    "sent": "As for the question of which algorithm to use in a real application, well it depends on how much very information we have about this data set, such as the number of classes, the proportion of different classes, and so on.",
                    "label": 0
                },
                {
                    "sent": "If we have all that information, we should use them aliste algorithm.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we should use the theater algorithm in order to guarantee a good performance.",
                    "label": 0
                },
                {
                    "sent": "He any situation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Colonel, sure.",
                    "label": 0
                },
                {
                    "sent": "So remember that insider?",
                    "label": 0
                },
                {
                    "sent": "We use the specialty.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An exponential families, with some modifications to estimate the density and also to get the score for each data point.",
                    "label": 0
                },
                {
                    "sent": "The score is the norm of the gradient at that data point, right?",
                    "label": 0
                },
                {
                    "sent": "So in principle we can use other density estimator to estimate the density and also to get the score right.",
                    "label": 0
                },
                {
                    "sent": "This kernel is just to use the Gaussian to use the kernel density estimator with the Gaussian kernel to estimate the density and to calculate the score for each data point, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, did you try your your method with high dimensional datasets now, so as you can see.",
                    "label": 0
                },
                {
                    "sent": "The datasets I've used so far are not very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "The higher dimensionality is 10, so there might be some problem.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With very high dimensional data set, but for those datasets we can first perform feature selection to yet to lower down the dimensionality of the feature space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a possibility.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes.",
                    "label": 0
                },
                {
                    "sent": "Category sparser than the majority class, because in your all your example.",
                    "label": 0
                },
                {
                    "sent": "You mean?",
                    "label": 0
                },
                {
                    "sent": "So if we compare the density of the majority class in minority class in the support region, if the density of the minority class larger than the majority class.",
                    "label": 0
                },
                {
                    "sent": "That's the case for the synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional synthetic data set, but that's not necessarily true.",
                    "label": 0
                },
                {
                    "sent": "Here I deliberately enlarge the font.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of each, each of the points here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they look larger and denser in here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah but that's yeah, but for this case the density of the minority class is larger than the majority class in the support region for the minority class, yeah?",
                    "label": 1
                },
                {
                    "sent": "Any other questions, yeah?",
                    "label": 0
                },
                {
                    "sent": "Early theorems you have probably have 1/3.",
                    "label": 0
                },
                {
                    "sent": "Why that?",
                    "label": 0
                },
                {
                    "sent": "Get up the likelihood probability 1/3 of it.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "That's the result from some geometry.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculation, yeah, and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I cannot think of any clear reason why is 1/3.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that is definitely better than random sampling, especially for the extremely skewed datasets.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The breaker clusters apart, so first of all I should say that for higher dimensional for very high dimensional data set is not a good idea to use existing dimensionality reduction techniques because you know for the skewed data set the minority classes examples from those minority classes are overwhelmed by the examples from the majority class.",
                    "label": 0
                },
                {
                    "sent": "So if you use the usual dimensionality reduction techniques then the selected features.",
                    "label": 0
                },
                {
                    "sent": "Or the lower dimensional space would be relevant to the majority class instead of the minority class, so we should avoid using those techniques on the skewed datasets and we should use some special techniques to deal with this problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right now there is no special techniques for dimensionality reduction for the imbalanced data set, but I'm working on it now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, any suggestions?",
                    "label": 0
                },
                {
                    "sent": "Comments are welcome, yeah?",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "But could you give us a little bit more detail about the interleave algorithm?",
                    "label": 0
                },
                {
                    "sent": "And why do you think your algorithm is performing outperforming?",
                    "label": 0
                },
                {
                    "sent": "So as I have mentioned, interleave algorithm, assume the mixture model to fit data, right?",
                    "label": 0
                },
                {
                    "sent": "So the might not.",
                    "label": 0
                },
                {
                    "sent": "Minority class also corresponds to a mixture component in the mixture model, right?",
                    "label": 0
                },
                {
                    "sent": "So if the minority class is not separable from the majority class, it's very difficult to identify this minority class using a individual mixture component, right?",
                    "label": 1
                },
                {
                    "sent": "Yeah, that's probably the reason why it's not performing as well as the proposed algorithms, yeah?",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}