{
    "id": "huqdvrgf5s7usm5dlwdttqcfppmhqlaa",
    "title": "Bayesian Multiple Instance Learning: Automatic Feature Selection and Inductive Transfer",
    "info": {
        "author": [
            "Vikas Raykar, Department of Computer Science, University of Maryland"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Instance-based Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_raykar_bmi/",
    "segmentation": [
        [
            "In this work will present multiple instance learning algorithm which does both classifier design and feature selection jointly in a Bayesian paradigm.",
            "And this is joint work with biology Jimbo rothenberger, all scientists at Siemens Medical Solutions."
        ],
        [
            "So this is the brief outline of the talk.",
            "I'll spend quite a little bit time on what is multiple instance learning.",
            "The second part will introduce the proposed algorithm.",
            "And in this end will say, how do we do feature selection and also an extension to multi task learning."
        ],
        [
            "OK, we'll start with simple basics.",
            "We have binary classification problem.",
            "We are given a region in a mammogram and we want to predict whether it is cancer one or not."
        ],
        [
            "For example, in text categorization, we're given a text, and we want to predict whether it belongs to a particular topic, one or zero."
        ],
        [
            "In general, for a binary classifier, we want to predict whether the label is zero or one."
        ],
        [
            "Rick.",
            "So in this talk I'll be concerned mainly with the linear binary classifier and the classes."
        ],
        [
            "Virus of the following form.",
            "So why is equal to 1 if W transpose X is greater than Theta zero?",
            "If it's less than Theta, X is my feature vector and W is."
        ],
        [
            "Weight vector an the theater determines the threshold of the classifier and you as you sleep data you get."
        ],
        [
            "Auto ceco So what does training imply?",
            "So we are given a set of examples.",
            "Zion why and we want to choose the weight vector W."
        ],
        [
            "Well, let us see now how we acquire labels for the training data in a typical single instance learning scenario.",
            "Every example XI has a label why belonging to 0."
        ],
        [
            "No one.",
            "Whereas in multiple instance learning a group of examples which we call it as a bag share a common label."
        ],
        [
            "This is clear from this diagram where every exam instance on the left has one label, whereas on the right hand side in the mild scenario, a group of examples has one.",
            "They share a common."
        ],
        [
            "Able.",
            "So this has to be a well framework for certain kind of problems and our motivation mainly comes from computer aided diagnosis."
        ],
        [
            "So in a typical computer diagnosis, this is a digital mammogram.",
            "There is something called masses or lesions which are known to be precursors of cancer.",
            "So a computer aided diagnosis system takes a digital mammogram, and it marks the location of the lesions for example.",
            "So, in order to train such a system, we show a mammogram to radiologist.",
            "He looks at it and he marks what is your lesion.",
            "For example, the red circle you see is what is marked by the radiologist.",
            "Now we want to train.",
            "This is all our training data.",
            "So how does the system work?",
            "So we have an image processing algorithm which essentially marks a lot of suspicious regions.",
            "For example, all the blue contours you see are marked as possibly cancerous regions by the image processing algorithm, and at each of these regions we compute a set of features an based on these features we want to predict whether it's cancerous or not.",
            "So why does this?",
            "Why is this a mild problem, it's becausw.",
            "Generally, the candidate generation step produces a lot of regions which are spatially adjacent to each other.",
            "For example, for this red circle you see that there are three or four blue candidates which are overlapping with it.",
            "So when we approach show the result to a radiologist, we are fine.",
            "If you find at least one if at least one of the candidates is detected as positive, then we are OK."
        ],
        [
            "I.",
            "A similar kind of happens in even pulmonary embolism protection, where there are a lot of candidates which point to an underlying pulmonary emboli, and we just want to find why at least."
        ],
        [
            "Correct?",
            "With this motivation will come to our notion of bags.",
            "So a bag contains many instances.",
            "All the instances in the bag share the."
        ],
        [
            "Name label a bag is labeled positive if it contains at least one instance an in our application.",
            "Elisian is detected if at least one of the candidate which overlaps with it is detected.",
            "So this definition of a positive bag is exactly mirrors over."
        ],
        [
            "Objective.",
            "And a negative bag means all instances in the bag are negative."
        ],
        [
            "Right?",
            "I just keep this.",
            "This is just a slide to show how the decision surface varies.",
            "If you take in a single instance and I'm I'll scenario which we've already seen in the previous."
        ],
        [
            "Who talks?",
            "So we come to the proposed."
        ],
        [
            "Rhythm.",
            "OK, so essentially I will show you that the algorithm which we proposed can be considered as the multiple instance version of the relevance vector machine, so we call it mirp VM.",
            "So these are the following key features.",
            "The single instance model is a simple logistic regression classifier and we modify it to handle them while case.",
            "The second thing is both feature selection and classifier design is jointly done innovation paradigm.",
            "The third thing is very easy to extend to multiple task learning.",
            "And it's extremely fast to implement.",
            "Extremely fast to implement and also running.",
            "It's easy to use and there are absolutely no tuning parameters."
        ],
        [
            "OK, so this is just a notation X."
        ],
        [
            "The feature vector and bold X is your bag, so a bag contains many instead."
        ],
        [
            "Instances and the label of the bags wondered why?"
        ],
        [
            "So we have training data which essentially has N bags.",
            "So now I'm not using the notion of instances.",
            "X is a BF, so there are N bags.",
            "Each bag contains K instances and all of them share the same way."
        ],
        [
            "Why I?",
            "So our classifier is of the following form.",
            "It's a simple linear class."
        ],
        [
            "Fire W transpose X."
        ],
        [
            "OK, now we need to have a probabilistic model.",
            "OK, so this is a simple logistic regression model.",
            "What is the probability that?",
            "Y is equal to 1 given X.",
            "This is for a single instance, is modeled as a sigmoid on W transpose 6.",
            "So we look at how do we model?",
            "We want to.",
            "Adapt this to the multiple instance learning scenario."
        ],
        [
            "So we defined a bag as positive if it contains at least one positive instance.",
            "So what is the probability that, given a bag with all the features that its label is 1, there's nothing but one minus probability that all instances are negative?",
            "So that is 1 minus product of what is the probability that each instance is negative and that is essentially 1 minus probability that it is positive.",
            "So we have a simple expression.",
            "It is 1 minus product of 1 minus sigmoids."
        ],
        [
            "So what is the net negative bag?",
            "Whatever is not positive negative, so essentially 1 minus.",
            "This should be your probability that it is Y is equal to 0 given X.",
            "So what now we have you said nice model which is.",
            "In W and what is the probability so our task is to estimate."
        ],
        [
            "W. We can start with standard maximum likelihood estimator which maximizes the log likelihood."
        ],
        [
            "This model you can simply write the log likelihood as the binomial likelihood, which is summation of these bags where P is essentially the probability that the bag is positive."
        ],
        [
            "DML estimator in general can.",
            "Exhibit C."
        ],
        [
            "Overfitting.",
            "So we can use a map estimator which uses a prior on W and then tries to find the best way."
        ],
        [
            "RW.",
            "So this is our prior, so we put a zero mean Gaussian within certain.",
            "So every weight vector in W as a certain prior which is 0 mean which are priority says that is saying this and then for that there is this particular variance parameter.",
            "So we call Alpha I as one by variant.",
            "So it is the precise."
        ],
        [
            "Parameter.",
            "And if you put this prior.",
            "And we get the log likelihood and we have a simple."
        ],
        [
            "Estimator so you find the W which maximizes L of W. And we can write the gradient for this expression and the Haitian, and you can use a simple Newton's method.",
            "To compute the best W. So up till now, what I'm saying is, if you knew the hyperparameters, think."
        ],
        [
            "So each weight each term in the prior has a certain variance, so this is called hyperparameter of the prior.",
            "So you."
        ],
        [
            "You know the hyperparameters.",
            "I can compute the."
        ],
        [
            "Maximum likelihood is the map estimated, but the whole question is how do we get this Alpha ice?"
        ],
        [
            "OK, so this comes to exactly to the feature selection."
        ],
        [
            "So we impose the prior of the form, which is a normal with zero mean.",
            "And certain.",
            "With the hyperparameters, which are just the inverse variances for each of the features."
        ],
        [
            "So if you know the hyperparameters, we can compute the map estimate."
        ],
        [
            "So solution.",
            "Note that as Alpha K tends to Infinity.",
            "That means your variance for that hyperparameter is going to 0, so the prior is essentially peaking at 0."
        ],
        [
            "Since your posterior is likelihood into prior year, regardless of the training data, the posterior on WK will be also sharply concentrated at 0."
        ],
        [
            "Hence that feature will not affect your classification result and it can be essentially removed via feature selection.",
            "So by putting the."
        ],
        [
            "Kind of prior what we have done is the discrete optimization problem.",
            "Of corresponding to feature selection, whether a feature should be included or not has been transferred to a."
        ],
        [
            "Continuous optimization over hyperparameters, so the feature selection is equal into.",
            "Now estimate these hyperparameters."
        ],
        [
            "So this is the approach we use is known as the type to marginal likelihood approach for."
        ],
        [
            "So essentially what we're trying to say is what best?",
            "What are the hyperparameters which best describes the?"
        ],
        [
            "Load data.",
            "So essentially it is probability of data given the hyperparameters you essentially integrated over all the model W. And this is not easy to compute, so we need to use an approximation to it.",
            "So we can use an approximation by the Taylor series expansion around our given map estimate."
        ],
        [
            "So you can see in the paper that this would be the exact approx."
        ],
        [
            "Mission to it.",
            "An A simple update rule can be obtained by writing the first derivative of it to 0."
        ],
        [
            "So what we have is the algorithm looks like this.",
            "First we choose a certain set of hyperparameters.",
            "Given certain hyperparameters, find the best map estimate.",
            "But we find that as and we keep repeating this.",
            "So we doing some iterations, we find that.",
            "The hyperparameters for several features will tend to Infinity and it will cause some numerical problems, essentially saying that feature is not important.",
            "So we just remove those features and keep repeating."
        ],
        [
            "Ages.",
            "So let me define now.",
            "So now we have described an algorithm which can both do classifier design and feature selection jointly."
        ],
        [
            "So here are some of the four datasets I use.",
            "Mask one must do the elephant and Tiger, but just like to have a look at the number of features Moscone and must have 166 features and elephant and Tiger are just 230 feet."
        ],
        [
            "And we compare the following six methods.",
            "MI RVM is our proposed method.",
            "MI is just a proposed method without feature selection.",
            "And our VM is proposed method without a mile.",
            "So these are two components and MI LR is a variant of logistic regression.",
            "And I compared with two variants of 1 variant of SVM and boosting."
        ],
        [
            "Mild versions of it.",
            "So the procedure is we do a 10 fold stratified cross validation.",
            "And we plot also the orosi curves and the area under the Orosi codes and the true positive rate is essentially computed in a bag."
        ],
        [
            "Well.",
            "So this shows the area under the Orosi codes for the different methods.",
            "So the two things which forget following observe.",
            "First thing is the proposed method, MI, RVM, and RVM are clearly better.",
            "Among all the other methods.",
            "Surprising observation is that for some datasets just a single instance learning RVM is much better than multiple instance RVM.",
            "And the third observation is feature selection definitely helps.",
            "If you run Myr, VM is better than MI.",
            "That is without feature selection."
        ],
        [
            "So these are some of the auto see curves here.",
            "Essentially Moscow and all of them almost doing very similar."
        ],
        [
            "Performance.",
            "For the mask too.",
            "The multiple instance learning and RVM or almost similar."
        ],
        [
            "Must Rea my RPM is the most for the Tiger.",
            "Nyr game is the most dominant."
        ],
        [
            "Algorithm and for the elephant RVM is supposedly the best algorithm of all the other multiple instancing."
        ],
        [
            "Well, the.",
            "One the key contribution I would say here is you need to see since it does feature selection, we'll compare what are the features selected by our algorithm.",
            "Right, so I compare the feature selected by our VM and the features selected by multiple instance PBM and the other algorithm which does feature selection is boosting.",
            "So this is my boost.",
            "So if you compare it, for example, if you see that.",
            "Tiger data set.",
            "Of out of 230 features with my RVM only with just 19 features, we're doing almost as good as the RVM, so you can think of Eminem.",
            "IRBM are very results are the autosys look pretty much similar, but the features are less than half of the RVM we're seeing if you do feature selection in multiple instance setting, we get much sparser models."
        ],
        [
            "So this is I mean the reason we, at least in our company.",
            "We mainly deal with CAD applications and this algorithm has been one."
        ],
        [
            "As."
        ],
        [
            "Has been one of the most successfully applied in a lot of products and where forest feature selection is extremely crucial becausw.",
            "Their features are not available as such, so you take image the huge volume and we asked what features you want to compute.",
            "So in order to make it run real time, you want to have as few features as possible so that the whole process get executed first.",
            "And especially for this data set.",
            "Out of 134 features, our proposed algorithm selected only 21 features and this is actually a very huge data set.",
            "We have like 200,000 negatives and a few 100 positives.",
            "An only my RVM, RVM and boosting.",
            "We were able to run these three algorithms an M IRBM clearly outperforms the other two.",
            "And the scale is a slightly bit different.",
            "Here I'm plotting false positives per volume, so we're talking of the order of like one or two false positive per patient.",
            "That means, so this is an extremely I'm in the left hand corner of the auto SQL."
        ],
        [
            "So as an extension this once we have this scenario, we can easily extend it to something called multi task."
        ],
        [
            "Meaning.",
            "So we want to learn multiple related classifiers.",
            "What probably happens is that we have a shortage of training data for learning classifiers for one task.",
            "But we want to exploit the information from other datasets, so we want to learn a separate classifier for each task, but somehow share information.",
            "So in this scenario.",
            "How we extend?",
            "Essentially we learn a different classifier for each task, but we put a prior so we make sure that the prior is same for all the different tasks and as a result the optimal hyperparameters are estimated from the data set simultaneously, while a separate classifier is learned for each data set."
        ],
        [
            "Just to give you a small example.",
            "This is essentially for lung cancer, so this is a lung CT scan and we want to find out something called modules which are known to be precursors of lung cancer.",
            "There are two kinds of lung nodules, 1 something which are called as a solid nodules and there is something which is appears very hazy recalled as a ground glass opacity.",
            "So we have typically very few data for jeans, and so we want to leverage, though they look very similar.",
            "There is some different characteristics both and audibles, so we want to leverage results from one data set into another and see if we can improve them and everything is done in my set of multiple instance data."
        ],
        [
            "So this result shows that with the multi task learning setup, there is a slightly slight improvement over the single task learning scenario."
        ],
        [
            "So to conclude, we have an algorithm called multiple instance relevance vector machine.",
            "Which does joint feature selection and classify learning in the mile scenario?",
            "It selects much sparser models.",
            "More accurate and faster than most of the competing methods.",
            "And it's easily extendable to multi task learning.",
            "Thank you.",
            "The future is selected device instance, however also included in the single instance.",
            "Ha.",
            "Some of them are, but in general I didn't do a systematic comparison of what features are included by RVM and my RV.",
            "Yeah, exactly, that's my intuition that MI instances a separate problem, so we have to do feature selection in that scenario rather than some other scenario.",
            "Why do you think you can get away with fewer features?",
            "Why do it?",
            "Ha.",
            "Well, it's kind of we're getting more sparser models.",
            "Definitely, that's one of the and we're getting accuracy as better as the as good as using all the features.",
            "In fact, using all features as I showed hurts accuracy rather than using a sparser model.",
            "When you apply just the relevant vectors so it's just a single instance, learning that you label everything, everything in the positive positive.",
            "Creating a harder decision problem?",
            "Yeah, exactly so.",
            "Yeah, so it's very easy to.",
            "Yeah.",
            "In other questions.",
            "So in the in the in the.",
            "Caracole transfer learning multi multi task learning.",
            "You are still using a zero mean, yeah?",
            "Did you consider also putting out a prior that would encourage the weight values to become someone unknown?",
            "Now, this prior is only on the just for the feature selection that's already done.",
            "Red right?",
            "So you could only do boosting your method because most of their my methods the other.",
            "Ha.",
            "Well, maybe yeah, I didn't try that.",
            "I mean, we could do a feature selection of wrapper before we do the.",
            "I'm comparing only with algorithms which both do feature selection and classification jointly.",
            "Anymore questions.",
            "Well, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this work will present multiple instance learning algorithm which does both classifier design and feature selection jointly in a Bayesian paradigm.",
                    "label": 0
                },
                {
                    "sent": "And this is joint work with biology Jimbo rothenberger, all scientists at Siemens Medical Solutions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the brief outline of the talk.",
                    "label": 1
                },
                {
                    "sent": "I'll spend quite a little bit time on what is multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "The second part will introduce the proposed algorithm.",
                    "label": 0
                },
                {
                    "sent": "And in this end will say, how do we do feature selection and also an extension to multi task learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we'll start with simple basics.",
                    "label": 0
                },
                {
                    "sent": "We have binary classification problem.",
                    "label": 1
                },
                {
                    "sent": "We are given a region in a mammogram and we want to predict whether it is cancer one or not.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, in text categorization, we're given a text, and we want to predict whether it belongs to a particular topic, one or zero.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, for a binary classifier, we want to predict whether the label is zero or one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rick.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll be concerned mainly with the linear binary classifier and the classes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Virus of the following form.",
                    "label": 0
                },
                {
                    "sent": "So why is equal to 1 if W transpose X is greater than Theta zero?",
                    "label": 1
                },
                {
                    "sent": "If it's less than Theta, X is my feature vector and W is.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weight vector an the theater determines the threshold of the classifier and you as you sleep data you get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Auto ceco So what does training imply?",
                    "label": 0
                },
                {
                    "sent": "So we are given a set of examples.",
                    "label": 0
                },
                {
                    "sent": "Zion why and we want to choose the weight vector W.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let us see now how we acquire labels for the training data in a typical single instance learning scenario.",
                    "label": 0
                },
                {
                    "sent": "Every example XI has a label why belonging to 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No one.",
                    "label": 0
                },
                {
                    "sent": "Whereas in multiple instance learning a group of examples which we call it as a bag share a common label.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is clear from this diagram where every exam instance on the left has one label, whereas on the right hand side in the mild scenario, a group of examples has one.",
                    "label": 0
                },
                {
                    "sent": "They share a common.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able.",
                    "label": 0
                },
                {
                    "sent": "So this has to be a well framework for certain kind of problems and our motivation mainly comes from computer aided diagnosis.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a typical computer diagnosis, this is a digital mammogram.",
                    "label": 0
                },
                {
                    "sent": "There is something called masses or lesions which are known to be precursors of cancer.",
                    "label": 0
                },
                {
                    "sent": "So a computer aided diagnosis system takes a digital mammogram, and it marks the location of the lesions for example.",
                    "label": 1
                },
                {
                    "sent": "So, in order to train such a system, we show a mammogram to radiologist.",
                    "label": 0
                },
                {
                    "sent": "He looks at it and he marks what is your lesion.",
                    "label": 0
                },
                {
                    "sent": "For example, the red circle you see is what is marked by the radiologist.",
                    "label": 0
                },
                {
                    "sent": "Now we want to train.",
                    "label": 0
                },
                {
                    "sent": "This is all our training data.",
                    "label": 0
                },
                {
                    "sent": "So how does the system work?",
                    "label": 0
                },
                {
                    "sent": "So we have an image processing algorithm which essentially marks a lot of suspicious regions.",
                    "label": 0
                },
                {
                    "sent": "For example, all the blue contours you see are marked as possibly cancerous regions by the image processing algorithm, and at each of these regions we compute a set of features an based on these features we want to predict whether it's cancerous or not.",
                    "label": 0
                },
                {
                    "sent": "So why does this?",
                    "label": 0
                },
                {
                    "sent": "Why is this a mild problem, it's becausw.",
                    "label": 0
                },
                {
                    "sent": "Generally, the candidate generation step produces a lot of regions which are spatially adjacent to each other.",
                    "label": 0
                },
                {
                    "sent": "For example, for this red circle you see that there are three or four blue candidates which are overlapping with it.",
                    "label": 0
                },
                {
                    "sent": "So when we approach show the result to a radiologist, we are fine.",
                    "label": 0
                },
                {
                    "sent": "If you find at least one if at least one of the candidates is detected as positive, then we are OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "A similar kind of happens in even pulmonary embolism protection, where there are a lot of candidates which point to an underlying pulmonary emboli, and we just want to find why at least.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct?",
                    "label": 0
                },
                {
                    "sent": "With this motivation will come to our notion of bags.",
                    "label": 1
                },
                {
                    "sent": "So a bag contains many instances.",
                    "label": 1
                },
                {
                    "sent": "All the instances in the bag share the.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Name label a bag is labeled positive if it contains at least one instance an in our application.",
                    "label": 1
                },
                {
                    "sent": "Elisian is detected if at least one of the candidate which overlaps with it is detected.",
                    "label": 1
                },
                {
                    "sent": "So this definition of a positive bag is exactly mirrors over.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objective.",
                    "label": 0
                },
                {
                    "sent": "And a negative bag means all instances in the bag are negative.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I just keep this.",
                    "label": 0
                },
                {
                    "sent": "This is just a slide to show how the decision surface varies.",
                    "label": 0
                },
                {
                    "sent": "If you take in a single instance and I'm I'll scenario which we've already seen in the previous.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who talks?",
                    "label": 0
                },
                {
                    "sent": "So we come to the proposed.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rhythm.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially I will show you that the algorithm which we proposed can be considered as the multiple instance version of the relevance vector machine, so we call it mirp VM.",
                    "label": 0
                },
                {
                    "sent": "So these are the following key features.",
                    "label": 1
                },
                {
                    "sent": "The single instance model is a simple logistic regression classifier and we modify it to handle them while case.",
                    "label": 0
                },
                {
                    "sent": "The second thing is both feature selection and classifier design is jointly done innovation paradigm.",
                    "label": 1
                },
                {
                    "sent": "The third thing is very easy to extend to multiple task learning.",
                    "label": 0
                },
                {
                    "sent": "And it's extremely fast to implement.",
                    "label": 0
                },
                {
                    "sent": "Extremely fast to implement and also running.",
                    "label": 0
                },
                {
                    "sent": "It's easy to use and there are absolutely no tuning parameters.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is just a notation X.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The feature vector and bold X is your bag, so a bag contains many instead.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instances and the label of the bags wondered why?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have training data which essentially has N bags.",
                    "label": 0
                },
                {
                    "sent": "So now I'm not using the notion of instances.",
                    "label": 0
                },
                {
                    "sent": "X is a BF, so there are N bags.",
                    "label": 0
                },
                {
                    "sent": "Each bag contains K instances and all of them share the same way.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why I?",
                    "label": 0
                },
                {
                    "sent": "So our classifier is of the following form.",
                    "label": 0
                },
                {
                    "sent": "It's a simple linear class.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fire W transpose X.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we need to have a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a simple logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that?",
                    "label": 1
                },
                {
                    "sent": "Y is equal to 1 given X.",
                    "label": 0
                },
                {
                    "sent": "This is for a single instance, is modeled as a sigmoid on W transpose 6.",
                    "label": 1
                },
                {
                    "sent": "So we look at how do we model?",
                    "label": 0
                },
                {
                    "sent": "We want to.",
                    "label": 0
                },
                {
                    "sent": "Adapt this to the multiple instance learning scenario.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we defined a bag as positive if it contains at least one positive instance.",
                    "label": 1
                },
                {
                    "sent": "So what is the probability that, given a bag with all the features that its label is 1, there's nothing but one minus probability that all instances are negative?",
                    "label": 0
                },
                {
                    "sent": "So that is 1 minus product of what is the probability that each instance is negative and that is essentially 1 minus probability that it is positive.",
                    "label": 0
                },
                {
                    "sent": "So we have a simple expression.",
                    "label": 0
                },
                {
                    "sent": "It is 1 minus product of 1 minus sigmoids.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the net negative bag?",
                    "label": 0
                },
                {
                    "sent": "Whatever is not positive negative, so essentially 1 minus.",
                    "label": 0
                },
                {
                    "sent": "This should be your probability that it is Y is equal to 0 given X.",
                    "label": 0
                },
                {
                    "sent": "So what now we have you said nice model which is.",
                    "label": 0
                },
                {
                    "sent": "In W and what is the probability so our task is to estimate.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "W. We can start with standard maximum likelihood estimator which maximizes the log likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model you can simply write the log likelihood as the binomial likelihood, which is summation of these bags where P is essentially the probability that the bag is positive.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DML estimator in general can.",
                    "label": 0
                },
                {
                    "sent": "Exhibit C.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overfitting.",
                    "label": 0
                },
                {
                    "sent": "So we can use a map estimator which uses a prior on W and then tries to find the best way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "RW.",
                    "label": 0
                },
                {
                    "sent": "So this is our prior, so we put a zero mean Gaussian within certain.",
                    "label": 1
                },
                {
                    "sent": "So every weight vector in W as a certain prior which is 0 mean which are priority says that is saying this and then for that there is this particular variance parameter.",
                    "label": 0
                },
                {
                    "sent": "So we call Alpha I as one by variant.",
                    "label": 0
                },
                {
                    "sent": "So it is the precise.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameter.",
                    "label": 0
                },
                {
                    "sent": "And if you put this prior.",
                    "label": 0
                },
                {
                    "sent": "And we get the log likelihood and we have a simple.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimator so you find the W which maximizes L of W. And we can write the gradient for this expression and the Haitian, and you can use a simple Newton's method.",
                    "label": 0
                },
                {
                    "sent": "To compute the best W. So up till now, what I'm saying is, if you knew the hyperparameters, think.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each weight each term in the prior has a certain variance, so this is called hyperparameter of the prior.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "I can compute the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximum likelihood is the map estimated, but the whole question is how do we get this Alpha ice?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this comes to exactly to the feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we impose the prior of the form, which is a normal with zero mean.",
                    "label": 1
                },
                {
                    "sent": "And certain.",
                    "label": 1
                },
                {
                    "sent": "With the hyperparameters, which are just the inverse variances for each of the features.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you know the hyperparameters, we can compute the map estimate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So solution.",
                    "label": 0
                },
                {
                    "sent": "Note that as Alpha K tends to Infinity.",
                    "label": 1
                },
                {
                    "sent": "That means your variance for that hyperparameter is going to 0, so the prior is essentially peaking at 0.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since your posterior is likelihood into prior year, regardless of the training data, the posterior on WK will be also sharply concentrated at 0.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hence that feature will not affect your classification result and it can be essentially removed via feature selection.",
                    "label": 0
                },
                {
                    "sent": "So by putting the.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of prior what we have done is the discrete optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Of corresponding to feature selection, whether a feature should be included or not has been transferred to a.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continuous optimization over hyperparameters, so the feature selection is equal into.",
                    "label": 0
                },
                {
                    "sent": "Now estimate these hyperparameters.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the approach we use is known as the type to marginal likelihood approach for.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially what we're trying to say is what best?",
                    "label": 0
                },
                {
                    "sent": "What are the hyperparameters which best describes the?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Load data.",
                    "label": 0
                },
                {
                    "sent": "So essentially it is probability of data given the hyperparameters you essentially integrated over all the model W. And this is not easy to compute, so we need to use an approximation to it.",
                    "label": 0
                },
                {
                    "sent": "So we can use an approximation by the Taylor series expansion around our given map estimate.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can see in the paper that this would be the exact approx.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission to it.",
                    "label": 0
                },
                {
                    "sent": "An A simple update rule can be obtained by writing the first derivative of it to 0.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we have is the algorithm looks like this.",
                    "label": 1
                },
                {
                    "sent": "First we choose a certain set of hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "Given certain hyperparameters, find the best map estimate.",
                    "label": 1
                },
                {
                    "sent": "But we find that as and we keep repeating this.",
                    "label": 1
                },
                {
                    "sent": "So we doing some iterations, we find that.",
                    "label": 1
                },
                {
                    "sent": "The hyperparameters for several features will tend to Infinity and it will cause some numerical problems, essentially saying that feature is not important.",
                    "label": 1
                },
                {
                    "sent": "So we just remove those features and keep repeating.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ages.",
                    "label": 0
                },
                {
                    "sent": "So let me define now.",
                    "label": 0
                },
                {
                    "sent": "So now we have described an algorithm which can both do classifier design and feature selection jointly.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some of the four datasets I use.",
                    "label": 0
                },
                {
                    "sent": "Mask one must do the elephant and Tiger, but just like to have a look at the number of features Moscone and must have 166 features and elephant and Tiger are just 230 feet.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we compare the following six methods.",
                    "label": 0
                },
                {
                    "sent": "MI RVM is our proposed method.",
                    "label": 1
                },
                {
                    "sent": "MI is just a proposed method without feature selection.",
                    "label": 1
                },
                {
                    "sent": "And our VM is proposed method without a mile.",
                    "label": 1
                },
                {
                    "sent": "So these are two components and MI LR is a variant of logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And I compared with two variants of 1 variant of SVM and boosting.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mild versions of it.",
                    "label": 0
                },
                {
                    "sent": "So the procedure is we do a 10 fold stratified cross validation.",
                    "label": 0
                },
                {
                    "sent": "And we plot also the orosi curves and the area under the Orosi codes and the true positive rate is essentially computed in a bag.",
                    "label": 1
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So this shows the area under the Orosi codes for the different methods.",
                    "label": 1
                },
                {
                    "sent": "So the two things which forget following observe.",
                    "label": 0
                },
                {
                    "sent": "First thing is the proposed method, MI, RVM, and RVM are clearly better.",
                    "label": 1
                },
                {
                    "sent": "Among all the other methods.",
                    "label": 1
                },
                {
                    "sent": "Surprising observation is that for some datasets just a single instance learning RVM is much better than multiple instance RVM.",
                    "label": 1
                },
                {
                    "sent": "And the third observation is feature selection definitely helps.",
                    "label": 0
                },
                {
                    "sent": "If you run Myr, VM is better than MI.",
                    "label": 0
                },
                {
                    "sent": "That is without feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some of the auto see curves here.",
                    "label": 0
                },
                {
                    "sent": "Essentially Moscow and all of them almost doing very similar.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "For the mask too.",
                    "label": 0
                },
                {
                    "sent": "The multiple instance learning and RVM or almost similar.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Must Rea my RPM is the most for the Tiger.",
                    "label": 0
                },
                {
                    "sent": "Nyr game is the most dominant.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm and for the elephant RVM is supposedly the best algorithm of all the other multiple instancing.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the.",
                    "label": 0
                },
                {
                    "sent": "One the key contribution I would say here is you need to see since it does feature selection, we'll compare what are the features selected by our algorithm.",
                    "label": 1
                },
                {
                    "sent": "Right, so I compare the feature selected by our VM and the features selected by multiple instance PBM and the other algorithm which does feature selection is boosting.",
                    "label": 1
                },
                {
                    "sent": "So this is my boost.",
                    "label": 0
                },
                {
                    "sent": "So if you compare it, for example, if you see that.",
                    "label": 0
                },
                {
                    "sent": "Tiger data set.",
                    "label": 0
                },
                {
                    "sent": "Of out of 230 features with my RVM only with just 19 features, we're doing almost as good as the RVM, so you can think of Eminem.",
                    "label": 0
                },
                {
                    "sent": "IRBM are very results are the autosys look pretty much similar, but the features are less than half of the RVM we're seeing if you do feature selection in multiple instance setting, we get much sparser models.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is I mean the reason we, at least in our company.",
                    "label": 0
                },
                {
                    "sent": "We mainly deal with CAD applications and this algorithm has been one.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has been one of the most successfully applied in a lot of products and where forest feature selection is extremely crucial becausw.",
                    "label": 0
                },
                {
                    "sent": "Their features are not available as such, so you take image the huge volume and we asked what features you want to compute.",
                    "label": 0
                },
                {
                    "sent": "So in order to make it run real time, you want to have as few features as possible so that the whole process get executed first.",
                    "label": 0
                },
                {
                    "sent": "And especially for this data set.",
                    "label": 0
                },
                {
                    "sent": "Out of 134 features, our proposed algorithm selected only 21 features and this is actually a very huge data set.",
                    "label": 1
                },
                {
                    "sent": "We have like 200,000 negatives and a few 100 positives.",
                    "label": 0
                },
                {
                    "sent": "An only my RVM, RVM and boosting.",
                    "label": 0
                },
                {
                    "sent": "We were able to run these three algorithms an M IRBM clearly outperforms the other two.",
                    "label": 0
                },
                {
                    "sent": "And the scale is a slightly bit different.",
                    "label": 0
                },
                {
                    "sent": "Here I'm plotting false positives per volume, so we're talking of the order of like one or two false positive per patient.",
                    "label": 0
                },
                {
                    "sent": "That means, so this is an extremely I'm in the left hand corner of the auto SQL.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an extension this once we have this scenario, we can easily extend it to something called multi task.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meaning.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn multiple related classifiers.",
                    "label": 1
                },
                {
                    "sent": "What probably happens is that we have a shortage of training data for learning classifiers for one task.",
                    "label": 1
                },
                {
                    "sent": "But we want to exploit the information from other datasets, so we want to learn a separate classifier for each task, but somehow share information.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario.",
                    "label": 0
                },
                {
                    "sent": "How we extend?",
                    "label": 1
                },
                {
                    "sent": "Essentially we learn a different classifier for each task, but we put a prior so we make sure that the prior is same for all the different tasks and as a result the optimal hyperparameters are estimated from the data set simultaneously, while a separate classifier is learned for each data set.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a small example.",
                    "label": 0
                },
                {
                    "sent": "This is essentially for lung cancer, so this is a lung CT scan and we want to find out something called modules which are known to be precursors of lung cancer.",
                    "label": 0
                },
                {
                    "sent": "There are two kinds of lung nodules, 1 something which are called as a solid nodules and there is something which is appears very hazy recalled as a ground glass opacity.",
                    "label": 0
                },
                {
                    "sent": "So we have typically very few data for jeans, and so we want to leverage, though they look very similar.",
                    "label": 0
                },
                {
                    "sent": "There is some different characteristics both and audibles, so we want to leverage results from one data set into another and see if we can improve them and everything is done in my set of multiple instance data.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this result shows that with the multi task learning setup, there is a slightly slight improvement over the single task learning scenario.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we have an algorithm called multiple instance relevance vector machine.",
                    "label": 1
                },
                {
                    "sent": "Which does joint feature selection and classify learning in the mile scenario?",
                    "label": 1
                },
                {
                    "sent": "It selects much sparser models.",
                    "label": 0
                },
                {
                    "sent": "More accurate and faster than most of the competing methods.",
                    "label": 1
                },
                {
                    "sent": "And it's easily extendable to multi task learning.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "The future is selected device instance, however also included in the single instance.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Some of them are, but in general I didn't do a systematic comparison of what features are included by RVM and my RV.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly, that's my intuition that MI instances a separate problem, so we have to do feature selection in that scenario rather than some other scenario.",
                    "label": 0
                },
                {
                    "sent": "Why do you think you can get away with fewer features?",
                    "label": 0
                },
                {
                    "sent": "Why do it?",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Well, it's kind of we're getting more sparser models.",
                    "label": 0
                },
                {
                    "sent": "Definitely, that's one of the and we're getting accuracy as better as the as good as using all the features.",
                    "label": 0
                },
                {
                    "sent": "In fact, using all features as I showed hurts accuracy rather than using a sparser model.",
                    "label": 0
                },
                {
                    "sent": "When you apply just the relevant vectors so it's just a single instance, learning that you label everything, everything in the positive positive.",
                    "label": 0
                },
                {
                    "sent": "Creating a harder decision problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's very easy to.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "In other questions.",
                    "label": 0
                },
                {
                    "sent": "So in the in the in the.",
                    "label": 0
                },
                {
                    "sent": "Caracole transfer learning multi multi task learning.",
                    "label": 0
                },
                {
                    "sent": "You are still using a zero mean, yeah?",
                    "label": 0
                },
                {
                    "sent": "Did you consider also putting out a prior that would encourage the weight values to become someone unknown?",
                    "label": 0
                },
                {
                    "sent": "Now, this prior is only on the just for the feature selection that's already done.",
                    "label": 0
                },
                {
                    "sent": "Red right?",
                    "label": 0
                },
                {
                    "sent": "So you could only do boosting your method because most of their my methods the other.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe yeah, I didn't try that.",
                    "label": 0
                },
                {
                    "sent": "I mean, we could do a feature selection of wrapper before we do the.",
                    "label": 0
                },
                {
                    "sent": "I'm comparing only with algorithms which both do feature selection and classification jointly.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}