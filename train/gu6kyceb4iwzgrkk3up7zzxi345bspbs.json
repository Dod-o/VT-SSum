{
    "id": "gu6kyceb4iwzgrkk3up7zzxi345bspbs",
    "title": "Improving Data Mining Utility with Projective Sampling",
    "info": {
        "author": [
            "Mark Last, Department of Information Systems Engineering, Ben-Gurion University of the Negev"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_last_idmups/",
    "segmentation": [
        [
            "OK, good afternoon to everybody.",
            "I'm going to talk about.",
            "The projective sampling technique.",
            "That under certain conditions can improve the utility of the data mining process.",
            "And if you wonder about the pictures of the first slide, this is our campus in the middle of the Negev desert in the South of Israel.",
            "So after a brief introduction."
        ],
        [
            "Then I will talk about the previous work and learning curves and progressive sampling.",
            "Then I will describe the proposed projective sampling strategy.",
            "Will show you some empirical results and then conclude my talk."
        ],
        [
            "OK, probably most of us realize that the data is not actually free.",
            "It can be scarce.",
            "It can be costly.",
            "Sometimes we have to pay the real money for the data.",
            "Sometimes the cost is something else.",
            "Here are a few examples of such applications getting patient records from a hospital run in an engineering experiment at a factory collecting seasonal vehicles from agricultural database.",
            "And even when the data is free and is unlimited.",
            "In its amount still we can spend a significant effort on the preprocessing that data.",
            "So the critical question is how much data do we really need?",
            "When we decide to spend our resources in terms of time or money."
        ],
        [
            "And this is the usual situation when we acquire some training set.",
            "To induce a model in my talk, I will be focusing on induction of classification models.",
            "And we assume that in the future we will have some score set.",
            "Where the examples will be classified will be labeled by the induced model.",
            "I'm using here.",
            "The framework developed by our session chair, Gary Vice and his student at Aon.",
            "According to that framework, we can refer to several costs when we talk about the utility of the data mining process.",
            "There is the cost of acquiring one example, the cost of committing one error, committing an error on one testing instance in this core set.",
            "And also there may be the cost of 1 unit of the CPU time.",
            "Also, as a part of the assumptions in that framework, I also assume that we have some fixed amount of the scores."
        ],
        [
            "Examples.",
            "So what is this research about?",
            "We're trying to find the best training set size that should maximize their overall utility.",
            "More specifically, should minimize the total cost of the data mining process.",
            "The idea of projective sampling is very simple.",
            "We take a small subset of the available data data.",
            "We fit long a learning curve, and the runtime curve to that small subset.",
            "And once we know the equation of the learning curve, we can calculate the.",
            "Optimal size of the training set and then we just acquire that amount of examples, not more and not less.",
            "So in this research I will be calculating the optimal training set size for several learning curve equations and I will show you that actually this approach can improve the utility of the data mining process of the classification process.",
            "For any given data set and an algorithm, of course under certain."
        ],
        [
            "Conditions.",
            "Here are just a practical examples from the benchmark datasets that are used.",
            "That's how the learning curves look like for a decision tree algorithm implemented by rapid miner two.",
            "In one case we have rapid rise, then it's slow guys.",
            "Sometimes we have eventually plateau.",
            "And in another data set we can see that the oscillations are quite high.",
            "It goes up and down very unstable, by the way, all the numbers that you see here are averages of 10.",
            "Random random validation runs OK.",
            "It's not just one time.",
            "One time result OK and here we have some more.",
            "Another big data set where there is a very slow rise.",
            "In the classification accuracy, as we accumulate more examples, so from here you can probably conclude that fitting learning curve to given data set is really not it."
        ],
        [
            "Trivial task.",
            "What can be the best fit for a learning curve?",
            "There was one work done by friend Fisher back in 99.",
            "They found that the power law was the best fit for the Civil 5 algorithm on several.",
            "They experimented with several datasets.",
            "In my own work from 2007.",
            "I experimented with another decision tree algorithm which is called information network and also found that actually the power law was the power law was the best feed.",
            "Another I think less known work by saying from 2005 says that actually the power law is only second best to some other alternatives for several."
        ],
        [
            "For several algorithms.",
            "And so there are some papers on progressive sampling strategy.",
            "Probably one of the first by Foster promised in his quarters in 99 with progressive sampling strategy, you start with some initial amount of training data and then incrementally you increase your training set until the cost starts to increase.",
            "And you can apply uniform schedule with a fixed increment.",
            "Or you can apply geometric sampling.",
            "For example, you can double the size of the training set."
        ],
        [
            "At every step.",
            "And progressive sampling has several limitations.",
            "Mainly it can overfit some local perturbations in the error rate and according to the result that results in your paper.",
            "They actually the costs may exceed the optimal ones by 10 up to 200%, which of course is not very good an.",
            "Also, there is some.",
            "This is also an important point.",
            "There is some overhead with each action of acquiring and preprocessing some new sample.",
            "So probably we are interested to minimize the amount of samples, the number of iterations.",
            "And finally, our expectation is that the projective sampling strategy should reduce the data mining cost, since it will find.",
            "The optimal or it will project the optimal."
        ],
        [
            "The size of the trainings.",
            "So what is the projective sampling strategy?",
            "We start with some fixed sampling, increment Delta.",
            "Each acquired sample is handled as one is treated as one data point.",
            "We acquire a new data point and then we compare persons correlation coefficient for each one of candidate fitting functions on the next slide I will show you the list of four.",
            "Different fitting functions that I used in this study.",
            "And also, I assume that the learning curve is not straight line, so I need at least three data points.",
            "The correlation coefficient is calculated between the error and the number of examples.",
            "So we're looking here basically for the lowest correlation coefficient, the correlation coefficient has to be negative, but we have more examples.",
            "The error rate should go down.",
            "That's what we hope at least.",
            "And so we continue to acquire additional data points as long as the correlation coefficient of oh equations is non negative.",
            "Once there is at least one negative correlation coefficient, this procedure, this algorithm picks up the best one, the lowest one.",
            "Of course, if we have not exceeded the maximum amount of available data.",
            "And then we can estimate the coefficients of the equation and from the coefficients of the equation we can calculate the optimal training set size.",
            "And then, oh, we have to do after that is to acquire the calculated number of examples."
        ],
        [
            "Introduce a classification model.",
            "So the candidate fitting functions considered here include the logarithmic learning of the equation mentioned in your paper by Vicenta Jan, Power Law and Exponential equation.",
            "Also, I consider two runtime curves, the linear one and the one behaving by the power law."
        ],
        [
            "Anne.",
            "I can apply very simple transformations to these equations so that the actual coefficients.",
            "Can be calculated.",
            "Using linear regression procedure, the linear regression process."
        ],
        [
            "Azure that's how the coefficient of correlation is calculated.",
            "This is standard equation."
        ],
        [
            "Pearson correlation coefficients and these are the equations for calculating.",
            "The slope and the intercept of the regression line OK, after I did.",
            "The convergence the transformations I can use these equations to find the actual the actual line."
        ],
        [
            "So that's how the total cost functions look like with each one of these learning curve equations, the difference is only in the differences, only in the last part where we calculate.",
            "And the cost.",
            "Of data data."
        ],
        [
            "Mission.",
            "And now to find the optimal training set size.",
            "Of course under the assumption.",
            "And that all functions are convex.",
            "And this assumption is true mathematically.",
            "We just find the derivative the first derivative equal set first derivative equal to zero.",
            "Also the cost of 1 training example is assumed to be equal to 1 and then we just manipulate with erasure of the error cost versus versus the training costs and also.",
            "So for for the sake of simplicity, I assume here the linear relationship between the number of examples and the runtime and that relationship was actually verified by my experiments.",
            "And now I'm just showing you the equations for the optimal training set size based on each one of the optional learning curves logarithmic then.",
            "Formula by center Jan the power law.",
            "The power law and the exponential.",
            "The exponential equation again, we don't know in advance which equation is the best one for a given data set, and this is part of the procedure to find the."
        ],
        [
            "Best equation.",
            "What are the experimental settings?",
            "I took 10 datasets ranging from the size of like 500 examples up to 200,000 training examples.",
            "Each data set was randomly partitioned into the test set and training set, potentially training set.",
            "The sampling increment was set to 1 percentage of the maximum training set size and the error rate was.",
            "Averaged over 10 random partitions of the training set and the projected strategy was compared to the Uniform Progressive Strategy where the equal increments of the training set size.",
            "The geometric strategy with a equal 2, which means doubling the training set size at every step and strawmen strategy which is just taking all available training data.",
            "The largest possible amount of training data.",
            "And also the actual optimum of the utility function of the cost function was found for each data set by enumerating.",
            "The 100 possible options, 1%, two percent etc.",
            "Corporations were ranged from 1 to 50,000.",
            "And there were two sets of experiments without induction costs, which means CPU factor equal to 0 and with induction costs equal to 1, which means 5 minutes and one means that the cost of 1 millisecond of CPU time is the same as acquiring one training example."
        ],
        [
            "OK, just a brief description of the datasets.",
            "You can find it."
        ],
        [
            "In the paper, and these are the main the main results of the experiments.",
            "There was no specific learning curve.",
            "Which was the best fit for all datasets.",
            "You can see that in some cases it is exponential.",
            "In some other cases it's logarithmic or power.",
            "Low power law is definitely not the winner here.",
            "Also, in terms of data points.",
            "In the worst case with the adult with other data set, we had to increment up to 14 data points, but in many other cases three data points was enough to feed the learning curve.",
            "Sometimes it was a bit, it was a bit more.",
            "And you can also see here the equations."
        ],
        [
            "Of of the learning curve.",
            "These are the project projected and actual learning learning curves.",
            "Sometimes the learning curves produced by this method underestimate the actual accuracy.",
            "Sometimes they."
        ],
        [
            "Overestimated and some some some more child that you can also find."
        ],
        [
            "And in the paper.",
            "OK, this is probably the most important slide in terms of results.",
            "Here we can see, especially on the left on the on the left hand side when the CPU factor is zero.",
            "When we ignore the induction costs and the projective sampling approach is second best only to the optimum OK to the actual optimum of the other progressive sampling strategies.",
            "And of course, the Straw man strategy.",
            "All the other strategies have higher costs.",
            "And this is what happens when the cost ratio is relatively low, which means that acquiring training examples is more general is more expensive than committing errors on the score set when the concentration goes up.",
            "Then still the projective sampling strategy is one of the best, but it's very close to the store one strategy, but the errors are very expensive.",
            "The best strategy is actually to pick up the.",
            "All available training data.",
            "And once the CPU factor goes up, which means we have some induction costs, then actually this approach is less benefit."
        ],
        [
            "Here are some details of the performance of sampling schedules for each individual data set.",
            "What you have seen the previous slide, what the average of all of hotel."
        ],
        [
            "Some more results.",
            "OK, since I have don't have much time left, these are these are the conclusions we can estimate the optimal training set size using the projective sampling strategy.",
            "And the method was evaluated on 10 benchmark datasets of variable size using one specific algorithm, which is decision tree.",
            "And according to the results, it is recommended to use the projective sampling strategy when the induction costs are very low or just negligible when the CPU time is very very cheap and when the data acquisition costs are relatively high, which I assume is not a very rare situation."
        ],
        [
            "As for future research, I'm sure this is just just the beginning.",
            "We can try to further optimize the projective sampling schedules.",
            "And we can try to improve the utility of cost sensitive data mining algorithms rather than just looking at the overall accuracy.",
            "And of course this problem is closely related to active learning, active sampling, and probably we can try to model learning curves for some non random sampling strategies.",
            "And of course for some non random labeling techniques."
        ],
        [
            "And.",
            "This is the end of my talk.",
            "Any questions?",
            "Thank you.",
            "Yeah yeah, so I'm staying here.",
            "Correlation coefficient.",
            "Can you repeat your question again?",
            "Yeah.",
            "Yes.",
            "All.",
            "But what was what was the value of the collection confusion?",
            "Or why not check this?",
            "Why is it negative?",
            "Because there is a negative correlation between the error and the number of training examples when the number of training examples goes up, we expect the error the clinical classification error go down.",
            "This means that the correlation is negative and not positive.",
            "So the best correlation coefficient is minus one."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon to everybody.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "The projective sampling technique.",
                    "label": 0
                },
                {
                    "sent": "That under certain conditions can improve the utility of the data mining process.",
                    "label": 1
                },
                {
                    "sent": "And if you wonder about the pictures of the first slide, this is our campus in the middle of the Negev desert in the South of Israel.",
                    "label": 0
                },
                {
                    "sent": "So after a brief introduction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then I will talk about the previous work and learning curves and progressive sampling.",
                    "label": 1
                },
                {
                    "sent": "Then I will describe the proposed projective sampling strategy.",
                    "label": 0
                },
                {
                    "sent": "Will show you some empirical results and then conclude my talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, probably most of us realize that the data is not actually free.",
                    "label": 1
                },
                {
                    "sent": "It can be scarce.",
                    "label": 0
                },
                {
                    "sent": "It can be costly.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we have to pay the real money for the data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the cost is something else.",
                    "label": 0
                },
                {
                    "sent": "Here are a few examples of such applications getting patient records from a hospital run in an engineering experiment at a factory collecting seasonal vehicles from agricultural database.",
                    "label": 1
                },
                {
                    "sent": "And even when the data is free and is unlimited.",
                    "label": 1
                },
                {
                    "sent": "In its amount still we can spend a significant effort on the preprocessing that data.",
                    "label": 0
                },
                {
                    "sent": "So the critical question is how much data do we really need?",
                    "label": 0
                },
                {
                    "sent": "When we decide to spend our resources in terms of time or money.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the usual situation when we acquire some training set.",
                    "label": 1
                },
                {
                    "sent": "To induce a model in my talk, I will be focusing on induction of classification models.",
                    "label": 1
                },
                {
                    "sent": "And we assume that in the future we will have some score set.",
                    "label": 0
                },
                {
                    "sent": "Where the examples will be classified will be labeled by the induced model.",
                    "label": 1
                },
                {
                    "sent": "I'm using here.",
                    "label": 0
                },
                {
                    "sent": "The framework developed by our session chair, Gary Vice and his student at Aon.",
                    "label": 0
                },
                {
                    "sent": "According to that framework, we can refer to several costs when we talk about the utility of the data mining process.",
                    "label": 0
                },
                {
                    "sent": "There is the cost of acquiring one example, the cost of committing one error, committing an error on one testing instance in this core set.",
                    "label": 1
                },
                {
                    "sent": "And also there may be the cost of 1 unit of the CPU time.",
                    "label": 1
                },
                {
                    "sent": "Also, as a part of the assumptions in that framework, I also assume that we have some fixed amount of the scores.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "So what is this research about?",
                    "label": 1
                },
                {
                    "sent": "We're trying to find the best training set size that should maximize their overall utility.",
                    "label": 1
                },
                {
                    "sent": "More specifically, should minimize the total cost of the data mining process.",
                    "label": 0
                },
                {
                    "sent": "The idea of projective sampling is very simple.",
                    "label": 1
                },
                {
                    "sent": "We take a small subset of the available data data.",
                    "label": 0
                },
                {
                    "sent": "We fit long a learning curve, and the runtime curve to that small subset.",
                    "label": 0
                },
                {
                    "sent": "And once we know the equation of the learning curve, we can calculate the.",
                    "label": 0
                },
                {
                    "sent": "Optimal size of the training set and then we just acquire that amount of examples, not more and not less.",
                    "label": 0
                },
                {
                    "sent": "So in this research I will be calculating the optimal training set size for several learning curve equations and I will show you that actually this approach can improve the utility of the data mining process of the classification process.",
                    "label": 1
                },
                {
                    "sent": "For any given data set and an algorithm, of course under certain.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditions.",
                    "label": 0
                },
                {
                    "sent": "Here are just a practical examples from the benchmark datasets that are used.",
                    "label": 0
                },
                {
                    "sent": "That's how the learning curves look like for a decision tree algorithm implemented by rapid miner two.",
                    "label": 1
                },
                {
                    "sent": "In one case we have rapid rise, then it's slow guys.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we have eventually plateau.",
                    "label": 0
                },
                {
                    "sent": "And in another data set we can see that the oscillations are quite high.",
                    "label": 0
                },
                {
                    "sent": "It goes up and down very unstable, by the way, all the numbers that you see here are averages of 10.",
                    "label": 0
                },
                {
                    "sent": "Random random validation runs OK.",
                    "label": 0
                },
                {
                    "sent": "It's not just one time.",
                    "label": 1
                },
                {
                    "sent": "One time result OK and here we have some more.",
                    "label": 0
                },
                {
                    "sent": "Another big data set where there is a very slow rise.",
                    "label": 0
                },
                {
                    "sent": "In the classification accuracy, as we accumulate more examples, so from here you can probably conclude that fitting learning curve to given data set is really not it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trivial task.",
                    "label": 0
                },
                {
                    "sent": "What can be the best fit for a learning curve?",
                    "label": 1
                },
                {
                    "sent": "There was one work done by friend Fisher back in 99.",
                    "label": 1
                },
                {
                    "sent": "They found that the power law was the best fit for the Civil 5 algorithm on several.",
                    "label": 0
                },
                {
                    "sent": "They experimented with several datasets.",
                    "label": 1
                },
                {
                    "sent": "In my own work from 2007.",
                    "label": 0
                },
                {
                    "sent": "I experimented with another decision tree algorithm which is called information network and also found that actually the power law was the power law was the best feed.",
                    "label": 1
                },
                {
                    "sent": "Another I think less known work by saying from 2005 says that actually the power law is only second best to some other alternatives for several.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For several algorithms.",
                    "label": 0
                },
                {
                    "sent": "And so there are some papers on progressive sampling strategy.",
                    "label": 0
                },
                {
                    "sent": "Probably one of the first by Foster promised in his quarters in 99 with progressive sampling strategy, you start with some initial amount of training data and then incrementally you increase your training set until the cost starts to increase.",
                    "label": 1
                },
                {
                    "sent": "And you can apply uniform schedule with a fixed increment.",
                    "label": 1
                },
                {
                    "sent": "Or you can apply geometric sampling.",
                    "label": 0
                },
                {
                    "sent": "For example, you can double the size of the training set.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At every step.",
                    "label": 0
                },
                {
                    "sent": "And progressive sampling has several limitations.",
                    "label": 1
                },
                {
                    "sent": "Mainly it can overfit some local perturbations in the error rate and according to the result that results in your paper.",
                    "label": 1
                },
                {
                    "sent": "They actually the costs may exceed the optimal ones by 10 up to 200%, which of course is not very good an.",
                    "label": 0
                },
                {
                    "sent": "Also, there is some.",
                    "label": 0
                },
                {
                    "sent": "This is also an important point.",
                    "label": 0
                },
                {
                    "sent": "There is some overhead with each action of acquiring and preprocessing some new sample.",
                    "label": 0
                },
                {
                    "sent": "So probably we are interested to minimize the amount of samples, the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "And finally, our expectation is that the projective sampling strategy should reduce the data mining cost, since it will find.",
                    "label": 1
                },
                {
                    "sent": "The optimal or it will project the optimal.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The size of the trainings.",
                    "label": 0
                },
                {
                    "sent": "So what is the projective sampling strategy?",
                    "label": 1
                },
                {
                    "sent": "We start with some fixed sampling, increment Delta.",
                    "label": 0
                },
                {
                    "sent": "Each acquired sample is handled as one is treated as one data point.",
                    "label": 1
                },
                {
                    "sent": "We acquire a new data point and then we compare persons correlation coefficient for each one of candidate fitting functions on the next slide I will show you the list of four.",
                    "label": 1
                },
                {
                    "sent": "Different fitting functions that I used in this study.",
                    "label": 0
                },
                {
                    "sent": "And also, I assume that the learning curve is not straight line, so I need at least three data points.",
                    "label": 0
                },
                {
                    "sent": "The correlation coefficient is calculated between the error and the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So we're looking here basically for the lowest correlation coefficient, the correlation coefficient has to be negative, but we have more examples.",
                    "label": 0
                },
                {
                    "sent": "The error rate should go down.",
                    "label": 0
                },
                {
                    "sent": "That's what we hope at least.",
                    "label": 0
                },
                {
                    "sent": "And so we continue to acquire additional data points as long as the correlation coefficient of oh equations is non negative.",
                    "label": 0
                },
                {
                    "sent": "Once there is at least one negative correlation coefficient, this procedure, this algorithm picks up the best one, the lowest one.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we have not exceeded the maximum amount of available data.",
                    "label": 1
                },
                {
                    "sent": "And then we can estimate the coefficients of the equation and from the coefficients of the equation we can calculate the optimal training set size.",
                    "label": 0
                },
                {
                    "sent": "And then, oh, we have to do after that is to acquire the calculated number of examples.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduce a classification model.",
                    "label": 0
                },
                {
                    "sent": "So the candidate fitting functions considered here include the logarithmic learning of the equation mentioned in your paper by Vicenta Jan, Power Law and Exponential equation.",
                    "label": 1
                },
                {
                    "sent": "Also, I consider two runtime curves, the linear one and the one behaving by the power law.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I can apply very simple transformations to these equations so that the actual coefficients.",
                    "label": 0
                },
                {
                    "sent": "Can be calculated.",
                    "label": 0
                },
                {
                    "sent": "Using linear regression procedure, the linear regression process.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Azure that's how the coefficient of correlation is calculated.",
                    "label": 0
                },
                {
                    "sent": "This is standard equation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pearson correlation coefficients and these are the equations for calculating.",
                    "label": 0
                },
                {
                    "sent": "The slope and the intercept of the regression line OK, after I did.",
                    "label": 1
                },
                {
                    "sent": "The convergence the transformations I can use these equations to find the actual the actual line.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's how the total cost functions look like with each one of these learning curve equations, the difference is only in the differences, only in the last part where we calculate.",
                    "label": 1
                },
                {
                    "sent": "And the cost.",
                    "label": 0
                },
                {
                    "sent": "Of data data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "And now to find the optimal training set size.",
                    "label": 1
                },
                {
                    "sent": "Of course under the assumption.",
                    "label": 0
                },
                {
                    "sent": "And that all functions are convex.",
                    "label": 0
                },
                {
                    "sent": "And this assumption is true mathematically.",
                    "label": 0
                },
                {
                    "sent": "We just find the derivative the first derivative equal set first derivative equal to zero.",
                    "label": 1
                },
                {
                    "sent": "Also the cost of 1 training example is assumed to be equal to 1 and then we just manipulate with erasure of the error cost versus versus the training costs and also.",
                    "label": 0
                },
                {
                    "sent": "So for for the sake of simplicity, I assume here the linear relationship between the number of examples and the runtime and that relationship was actually verified by my experiments.",
                    "label": 0
                },
                {
                    "sent": "And now I'm just showing you the equations for the optimal training set size based on each one of the optional learning curves logarithmic then.",
                    "label": 0
                },
                {
                    "sent": "Formula by center Jan the power law.",
                    "label": 1
                },
                {
                    "sent": "The power law and the exponential.",
                    "label": 0
                },
                {
                    "sent": "The exponential equation again, we don't know in advance which equation is the best one for a given data set, and this is part of the procedure to find the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best equation.",
                    "label": 0
                },
                {
                    "sent": "What are the experimental settings?",
                    "label": 0
                },
                {
                    "sent": "I took 10 datasets ranging from the size of like 500 examples up to 200,000 training examples.",
                    "label": 0
                },
                {
                    "sent": "Each data set was randomly partitioned into the test set and training set, potentially training set.",
                    "label": 0
                },
                {
                    "sent": "The sampling increment was set to 1 percentage of the maximum training set size and the error rate was.",
                    "label": 1
                },
                {
                    "sent": "Averaged over 10 random partitions of the training set and the projected strategy was compared to the Uniform Progressive Strategy where the equal increments of the training set size.",
                    "label": 0
                },
                {
                    "sent": "The geometric strategy with a equal 2, which means doubling the training set size at every step and strawmen strategy which is just taking all available training data.",
                    "label": 0
                },
                {
                    "sent": "The largest possible amount of training data.",
                    "label": 0
                },
                {
                    "sent": "And also the actual optimum of the utility function of the cost function was found for each data set by enumerating.",
                    "label": 0
                },
                {
                    "sent": "The 100 possible options, 1%, two percent etc.",
                    "label": 0
                },
                {
                    "sent": "Corporations were ranged from 1 to 50,000.",
                    "label": 0
                },
                {
                    "sent": "And there were two sets of experiments without induction costs, which means CPU factor equal to 0 and with induction costs equal to 1, which means 5 minutes and one means that the cost of 1 millisecond of CPU time is the same as acquiring one training example.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just a brief description of the datasets.",
                    "label": 0
                },
                {
                    "sent": "You can find it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper, and these are the main the main results of the experiments.",
                    "label": 0
                },
                {
                    "sent": "There was no specific learning curve.",
                    "label": 0
                },
                {
                    "sent": "Which was the best fit for all datasets.",
                    "label": 0
                },
                {
                    "sent": "You can see that in some cases it is exponential.",
                    "label": 0
                },
                {
                    "sent": "In some other cases it's logarithmic or power.",
                    "label": 0
                },
                {
                    "sent": "Low power law is definitely not the winner here.",
                    "label": 0
                },
                {
                    "sent": "Also, in terms of data points.",
                    "label": 0
                },
                {
                    "sent": "In the worst case with the adult with other data set, we had to increment up to 14 data points, but in many other cases three data points was enough to feed the learning curve.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it was a bit, it was a bit more.",
                    "label": 0
                },
                {
                    "sent": "And you can also see here the equations.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of of the learning curve.",
                    "label": 0
                },
                {
                    "sent": "These are the project projected and actual learning learning curves.",
                    "label": 1
                },
                {
                    "sent": "Sometimes the learning curves produced by this method underestimate the actual accuracy.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overestimated and some some some more child that you can also find.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, this is probably the most important slide in terms of results.",
                    "label": 0
                },
                {
                    "sent": "Here we can see, especially on the left on the on the left hand side when the CPU factor is zero.",
                    "label": 0
                },
                {
                    "sent": "When we ignore the induction costs and the projective sampling approach is second best only to the optimum OK to the actual optimum of the other progressive sampling strategies.",
                    "label": 0
                },
                {
                    "sent": "And of course, the Straw man strategy.",
                    "label": 0
                },
                {
                    "sent": "All the other strategies have higher costs.",
                    "label": 0
                },
                {
                    "sent": "And this is what happens when the cost ratio is relatively low, which means that acquiring training examples is more general is more expensive than committing errors on the score set when the concentration goes up.",
                    "label": 0
                },
                {
                    "sent": "Then still the projective sampling strategy is one of the best, but it's very close to the store one strategy, but the errors are very expensive.",
                    "label": 0
                },
                {
                    "sent": "The best strategy is actually to pick up the.",
                    "label": 0
                },
                {
                    "sent": "All available training data.",
                    "label": 0
                },
                {
                    "sent": "And once the CPU factor goes up, which means we have some induction costs, then actually this approach is less benefit.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some details of the performance of sampling schedules for each individual data set.",
                    "label": 0
                },
                {
                    "sent": "What you have seen the previous slide, what the average of all of hotel.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more results.",
                    "label": 0
                },
                {
                    "sent": "OK, since I have don't have much time left, these are these are the conclusions we can estimate the optimal training set size using the projective sampling strategy.",
                    "label": 1
                },
                {
                    "sent": "And the method was evaluated on 10 benchmark datasets of variable size using one specific algorithm, which is decision tree.",
                    "label": 1
                },
                {
                    "sent": "And according to the results, it is recommended to use the projective sampling strategy when the induction costs are very low or just negligible when the CPU time is very very cheap and when the data acquisition costs are relatively high, which I assume is not a very rare situation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As for future research, I'm sure this is just just the beginning.",
                    "label": 0
                },
                {
                    "sent": "We can try to further optimize the projective sampling schedules.",
                    "label": 1
                },
                {
                    "sent": "And we can try to improve the utility of cost sensitive data mining algorithms rather than just looking at the overall accuracy.",
                    "label": 0
                },
                {
                    "sent": "And of course this problem is closely related to active learning, active sampling, and probably we can try to model learning curves for some non random sampling strategies.",
                    "label": 1
                },
                {
                    "sent": "And of course for some non random labeling techniques.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so I'm staying here.",
                    "label": 0
                },
                {
                    "sent": "Correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat your question again?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "But what was what was the value of the collection confusion?",
                    "label": 0
                },
                {
                    "sent": "Or why not check this?",
                    "label": 0
                },
                {
                    "sent": "Why is it negative?",
                    "label": 0
                },
                {
                    "sent": "Because there is a negative correlation between the error and the number of training examples when the number of training examples goes up, we expect the error the clinical classification error go down.",
                    "label": 0
                },
                {
                    "sent": "This means that the correlation is negative and not positive.",
                    "label": 0
                },
                {
                    "sent": "So the best correlation coefficient is minus one.",
                    "label": 0
                }
            ]
        }
    }
}