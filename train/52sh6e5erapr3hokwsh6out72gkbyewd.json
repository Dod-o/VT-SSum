{
    "id": "52sh6e5erapr3hokwsh6out72gkbyewd",
    "title": "Optimal Learners for Multiclass Problems",
    "info": {
        "author": [
            "Amit Daniely, Einstein Institute of Mathematics, The Hebrew University of Jerusalem"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_daniely_learners/",
    "segmentation": [
        [
            "So we talk about multiclass learning.",
            "This is the problem of statistical learning, hypothesis class or function from a domain X2 label space Y, which is discrete and finite.",
            "So this is a very basic problem.",
            "It needs no introduction.",
            "Many methods have been developed for this problem and it has been extensively studied from both theoretical and practical point of view.",
            "But still we will see it is not yet sufficiently understood.",
            "OK, So what are the basic questions in this context?",
            "First, when H is learnable, or more quantitatively, what is the sample complexity of H?",
            "And how to do that?",
            "How to learn H optimally?",
            "So if we visit for a moment the binary case where we have exactly 2 labels, then we are we have very complete and sharp answer to all of this question.",
            "When I just read about when the VC dimension is finite, what is the supper complexity of age?",
            "Simply the VC dimension over epsilon.",
            "And I have to learn age youth.",
            "Erm, anyone you lack.",
            "So it's just not worth to ask what happened in the multiclass classification an it's in formulas like this would be essentially the case in the marketplace classification as well this term sold.",
            "A general hold, but as we will see there are some surprises.",
            "First, there are gaps between different data.",
            "Grimms, who sample complexity is better than earlier and so this is a previous result with sheriff's welchii Bendovi, Denise Ivan Sabato.",
            "And here we show that."
        ],
        [
            "Even a stronger property holds the aircraft that cannot be learnable by a proper algorithm.",
            "This means that an optimal learner must have the ability to return a function that do not belong to their own cost.",
            "An additional things that we will show is that such phenomenons, the first phenomenon of gaps between different theorems, happens even in natural classes that are used in practice.",
            "So these results reopen this basic open questions and we will try to give them some answers.",
            "So we start with the last question after learn H optimally and you will show that the one inclusion algorithm of Winston Bartlett inhibition is actually optimal in the multiclass setting.",
            "For the first two question, we will characterize the sample complexity of H using a sequence that defined using edge, and we also define a new notion of dimension inspired by our analysis that we conjecture to characterize the sample complexity.",
            "OK, so."
        ],
        [
            "Start this out by showing that optimal learners must be important, but first."
        ],
        [
            "Some definitions, so we want to learn a hypothesis HR that belongs to the class H based on sample where the sample the examples or sample from some unknown distribution and you make no assumption on the.",
            "A learning algorithm is any function that takes a sample and return another function.",
            "So two remarks here we completely ignore computation algorithm.",
            "Could be any kind of function, and algorithm can be improper.",
            "It has the ability to function that do not do not belong to H If the algorithm is guaranteed to return a function that belong to which we will say that the algorithm is proper.",
            "Otherwise we will say that it is improper and hear them.",
            "Algorithm is simply an algorithm that is guaranteed to return.",
            "Consistent hypothesis, which means epesses that makes no errors on the sample and belong to each and the two basic definition.",
            "The sample complexity of the algorithm A is defined as follows.",
            "M of Epsilon is the number of example we need to see in order to guarantee that there will be less than epsilon and the sample complexity of age is simply the sample complexity of the best algorithm.",
            "So these are all standard definition."
        ],
        [
            "OK, so as many of you know, improper learning has been often used to overcome computational issues and it seems bit can counter intuitive that computationally unbounded there will benefit in some way by returning hypothesis that do not belong to the lower class.",
            "But as we will show, this is actually the case."
        ],
        [
            "So here's an example.",
            "I think it is quite simple and nice, so I'll do it in almost full detail.",
            "So X will be an arbitrary finite set and the set of labels will be very large.",
            "An maybe it will.",
            "It will consist of all subsets of X together with one more special label that we do not buy stuff for every subset T of X will define a hypothesis H of T that is defined as follow HD of X is the if X.",
            "30, which is also a label if X belongs to T and otherwise it is the special label style and our hypothesis class will consist of all that functions vertie the size of T is exactly half of the size of X. OK, so this is the hypothesis class, and before I'll state the first theorem, let me make few remarks regarding learning this class.",
            "So suppose that a learner get sample labeled by some hypothesis from this class and we want to find a good hypothesis.",
            "Now, if it's with the label T, you know it know for sure that the land hypothesis is HT because the only hypothesis that might return the label T is H of T so.",
            "The algorithm is fully determined by the hypothesis it returns on samples of the Form X1 star, 2X N stop.",
            "OK, So what?"
        ],
        [
            "So if that, even though the sample complexity of this algorithm is upper bounded by one over epsilon, the sample complexity of every proper algorithm is lower bounded by the cardinality of X over epsilon.",
            "So this shows gaps between learnability and proper learnability, and if it is infinite, then we have a class that is learnable but is not learnable by a proper learning algorithm.",
            "And I'll make one remark that slightly weaker phenomenon, namely gap between in arms.",
            "Also happens so this is a very weird class and no one will think to use that in practice.",
            "But we also have a more complicated examples that the phenomenon of gap between the arms happens in classes that are using practice.",
            "But I won't talk about this.",
            "OK, so let me give you the proof.",
            "We start with the easy and maybe less interesting part, that is, sample complexity is upper bounded by one over epsilon.",
            "So here should show an algorithm with sample complexity is upper bounded by one over epsilon.",
            "So we will simply take the first algorithm that you will think about.",
            "So I remind you that we only need to define it on samples of the Form X1 style, 2X and star and on that sample the algorithm will simply return the constant hypothesis, which is the constant function.",
            "Stop OK, so this is the algorithm.",
            "And note that this algorithm is improper, because this function do not belong to H. OK so I created the sample complexity is a bounded by one over epsilon.",
            "So suppose HD is the target hypothesis.",
            "We try to learn.",
            "The point is that a will return either the correct hypothesis H of T or the constants hypothesis style.",
            "Now since there are only two hypothesis, if one of them is bad it will be rejected with high probability.",
            "Upon seeing only one over epsilon samples.",
            "So this is a very simple argument."
        ],
        [
            "So now we will show that the sample complexity of every proper algorithm is lower bounded by X over epsilon, and for simplicity we'll do that for constant epsilon.",
            "So this is just to give you the idea.",
            "OK, so suppose that A is a learning algorithm.",
            "Now we ask fix some set of cardinality X / 2 and the distribution will be the uniform distribution on the set and the land hypothesis will be age of the complement of a.",
            "So after of it we don't care what happens because the probability there is zero and inside the all the labels that we will see our stuff.",
            "OK.",
            "So.",
            "Yeah, suppose we running the algorithm and then it returns some function of the format of the where the size of the equal to Alpha fix.",
            "OK, this is because the algorithm is proper.",
            "So what is the error of their internal part of this?",
            "So we only care about examples inside it, cause outside probability is 0 now inside it.",
            "On the complement of the, the return hypothesis is correct.",
            "It's a style, but on the intersection it it's around us because it's safety an the two labels are style.",
            "So the arrow is proportional to the Intersect intersection of T&E.",
            "Therefore, because the size of both ENTR X / 2 in order to return a good hypothesis, the city must be almost U joints from the setting.",
            "In other words, we have to recover approximately recover the setting and should be quite clear that in order to recover a set of cardinality, it's over two.",
            "We have to see at least order of X examples.",
            "OK, so this is the idea of the proof and.",
            "I won't say anything more about this.",
            "OK, so as I."
        ],
        [
            "33 opens the basic question.",
            "We know that ERM is not optimal here, and we need another generic algorithm.",
            "So."
        ],
        [
            "We show that such an algorithm is the one inclusion algorithm, so this is not a new algorithm version of it for binary classification has been proposed by House Little Stone and once in 94 an it was generalized by orange and Bucket and Robertson.",
            "A to the multiclass setting, and they also analyzed it by their analysis wasn't tight, and they had a factor of log Y.",
            "And by annualized, we show that this algorithm is optimal up to a constant factor and actually up to a quite small constant factor a little bit more than five.",
            "Let me maybe say one of the mark about this log.",
            "Why?",
            "So?",
            "In a sense, it's just a closing Gaelic factor, but in many cases the two complexity of the problem is log of why?",
            "So?",
            "For example, if you want to predict a whole sentence, then.",
            "There.",
            "The cardinality of the space is the number of the number of letters to the length of the center, so here.",
            "The local factor in the number of classes is meaningful.",
            "OK so I won't show you the algorithm due to lack of time.",
            "But I want."
        ],
        [
            "I will say stated you, I'm so we need.",
            "One definition will define the mistake.",
            "Bound affirm.",
            "A form of an algorithm made as the probability that a earth on a new example after seeing them examples.",
            "And what we show is that for any other algorithm A, they mistake one of any algorithm A is lower bounded by a constant time.",
            "The mistake bound of the one inclusion algorithm.",
            "So as I said, this improves rubbish about an analyst and an bystander argument, it follows that the same algorithm is optimal also in the park model up to a factor of up to logarithmic factor of 1 over epsilon.",
            "And one more remark, the one inclusion algorithm is a generic algorithm like, ERM, an in general it is very inefficient, but for linear classes we do derive efficient versions of this algorithm.",
            "It is related to compression schemes, but unfortunately I won't have time to say much about it.",
            "OK, so let's make an instrument."
        ],
        [
            "In summary, we also the last question how to learn H optimally, but what about the first 2 questions when age is learnable and what is the sample complexity of H so?"
        ],
        [
            "We will characterize the sample complexity using a sequence that defined using Edge, so this is the sequ."
        ],
        [
            "We defined the degree of hypothesis edge in Earth in H as number of points X in X such that there exists another function G that is identical to H except at the point X.",
            "And we define the density of the class H as the average degree of hypothesis in edge.",
            "And finally we define the density function of H as follows.",
            "Mu M is the maximum of the density of F restricted to the set F where F runs overall sub or on over all finite subsets of H&S runs over all finite subset of exorcise them.",
            "OK, so this is a certain function that is defined using using edge.",
            "I don't expect you to.",
            "Shifted at this moment.",
            "But anyway, what we show is that the mistake bound of age.",
            "Equal to the sequence over him up to a constant factor, so it follows that the sequence characterizes the density function, characterized the sample complexity, and in particular, H is learnable if or if and only if mu M / M approaches zeros.",
            "M purchase Infinity.",
            "OK so."
        ],
        [
            "Seems like an end of story.",
            "We have answer all these questions.",
            "We characterize the sample complexity.",
            "We gave a generic optimal algorithm, but to be honest, I don't think that this is end of story because we would like a better characterization of the sample complexity.",
            "Would like to characterize the sample complexity using a single number like the VC dimension and not by a whole sequence.",
            "And here I don't have a theorem, but we do have a conjecture.",
            "So the model."
        ],
        [
            "Sequence of our analysis of the one inclusion algorithm is that instead of analyzing growth, we should analyze density.",
            "So let me say a few more words about this.",
            "So as many of you know, the complexity of, ERM algorithms is traditionally analyzed by analyzing the growth function.",
            "So this is true for binary classification to multiclass classification and too many other learning problems.",
            "But as we see here, the summer complexity is governed by the density function, which is a different function.",
            "And therefore, instead of analyzed growth, we should analyze density.",
            "So let's revisit the visit."
        ],
        [
            "And from this point of view.",
            "So when I go back for a moment to binary classification, there are two equivalent ways to define the VC dimension.",
            "One way is the maximal M such that the growth function is maximal.",
            "Another way is the maximal M such that the density function is maximal.",
            "These are these are two equivalent ways for binary classification, but there are no longer equivalent for multiclass classification, even if in the case that y = 3.",
            "And it seems like it is much more natural to adapt the second definition.",
            "And this is what we will do.",
            "We will define the dimension of H as the maximal M such that mu M equal M such that the density function is maximal and we want to relate the sample complexity of H2 this number.",
            "Or equivalently we want to relate the growth of mu M to this number.",
            "So let us review it for a moment.",
            "Again, the binary case, so we."
        ],
        [
            "We have a nice theorem of Hofler little stonean vamos that tell us that MUFE M is equal to the VC dimension up to a constant factor of two, and we simply conjecture that this holds for the generalized VC dimension.",
            "We conjecture that museum is bounded between the dimension of H2 two times the dimension of H. OK, so this is maybe the storing conjecture we can make will be happy with any constant rather than two we know to prove that it holds with.",
            "Log why instead of?",
            "And the consequence of this conjecture, if it's true, is that you have a nice characterization of the sample complexity using this new dimension, and let me make one remark about this conjecture.",
            "So maybe if you if you remember that in called three years ago we had another conjecture that the intelligent dimension characterized the sample complexity.",
            "So this conjecture still might be true, but I think that this conjecture is a better one for free reasons.",
            "First, it is implied by the previous conjecture, so it might be the case that the previous conjecture is false, but this.",
            "New conjecture is still true.",
            "Second, the previous conjecture was more like guess and this is a bit more principle ized.",
            "And failed, and I think most important, this is a very concrete combinatorial conjecture that if to as I said, characterize the sample complexity.",
            "OK."
        ],
        [
            "So let me quickly summarize, we have shown that yeah, arms are not necessarily optimal, and Moreover, in certain cases optimal algorithms must be improper.",
            "We gave answer to this basic question.",
            "We and we arrived, and you conjecture and let me just mention one more open issue is what happens in diagnostics.",
            "So most of the results we don't know to prove in diagnostic setting.",
            "So this is a very interesting there direction for future research.",
            "Can I stop here and take questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we talk about multiclass learning.",
                    "label": 1
                },
                {
                    "sent": "This is the problem of statistical learning, hypothesis class or function from a domain X2 label space Y, which is discrete and finite.",
                    "label": 0
                },
                {
                    "sent": "So this is a very basic problem.",
                    "label": 0
                },
                {
                    "sent": "It needs no introduction.",
                    "label": 0
                },
                {
                    "sent": "Many methods have been developed for this problem and it has been extensively studied from both theoretical and practical point of view.",
                    "label": 0
                },
                {
                    "sent": "But still we will see it is not yet sufficiently understood.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the basic questions in this context?",
                    "label": 0
                },
                {
                    "sent": "First, when H is learnable, or more quantitatively, what is the sample complexity of H?",
                    "label": 0
                },
                {
                    "sent": "And how to do that?",
                    "label": 0
                },
                {
                    "sent": "How to learn H optimally?",
                    "label": 0
                },
                {
                    "sent": "So if we visit for a moment the binary case where we have exactly 2 labels, then we are we have very complete and sharp answer to all of this question.",
                    "label": 0
                },
                {
                    "sent": "When I just read about when the VC dimension is finite, what is the supper complexity of age?",
                    "label": 0
                },
                {
                    "sent": "Simply the VC dimension over epsilon.",
                    "label": 0
                },
                {
                    "sent": "And I have to learn age youth.",
                    "label": 0
                },
                {
                    "sent": "Erm, anyone you lack.",
                    "label": 0
                },
                {
                    "sent": "So it's just not worth to ask what happened in the multiclass classification an it's in formulas like this would be essentially the case in the marketplace classification as well this term sold.",
                    "label": 0
                },
                {
                    "sent": "A general hold, but as we will see there are some surprises.",
                    "label": 0
                },
                {
                    "sent": "First, there are gaps between different data.",
                    "label": 0
                },
                {
                    "sent": "Grimms, who sample complexity is better than earlier and so this is a previous result with sheriff's welchii Bendovi, Denise Ivan Sabato.",
                    "label": 0
                },
                {
                    "sent": "And here we show that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even a stronger property holds the aircraft that cannot be learnable by a proper algorithm.",
                    "label": 0
                },
                {
                    "sent": "This means that an optimal learner must have the ability to return a function that do not belong to their own cost.",
                    "label": 0
                },
                {
                    "sent": "An additional things that we will show is that such phenomenons, the first phenomenon of gaps between different theorems, happens even in natural classes that are used in practice.",
                    "label": 0
                },
                {
                    "sent": "So these results reopen this basic open questions and we will try to give them some answers.",
                    "label": 0
                },
                {
                    "sent": "So we start with the last question after learn H optimally and you will show that the one inclusion algorithm of Winston Bartlett inhibition is actually optimal in the multiclass setting.",
                    "label": 0
                },
                {
                    "sent": "For the first two question, we will characterize the sample complexity of H using a sequence that defined using edge, and we also define a new notion of dimension inspired by our analysis that we conjecture to characterize the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start this out by showing that optimal learners must be important, but first.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some definitions, so we want to learn a hypothesis HR that belongs to the class H based on sample where the sample the examples or sample from some unknown distribution and you make no assumption on the.",
                    "label": 0
                },
                {
                    "sent": "A learning algorithm is any function that takes a sample and return another function.",
                    "label": 0
                },
                {
                    "sent": "So two remarks here we completely ignore computation algorithm.",
                    "label": 0
                },
                {
                    "sent": "Could be any kind of function, and algorithm can be improper.",
                    "label": 0
                },
                {
                    "sent": "It has the ability to function that do not do not belong to H If the algorithm is guaranteed to return a function that belong to which we will say that the algorithm is proper.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we will say that it is improper and hear them.",
                    "label": 0
                },
                {
                    "sent": "Algorithm is simply an algorithm that is guaranteed to return.",
                    "label": 0
                },
                {
                    "sent": "Consistent hypothesis, which means epesses that makes no errors on the sample and belong to each and the two basic definition.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity of the algorithm A is defined as follows.",
                    "label": 1
                },
                {
                    "sent": "M of Epsilon is the number of example we need to see in order to guarantee that there will be less than epsilon and the sample complexity of age is simply the sample complexity of the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "So these are all standard definition.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as many of you know, improper learning has been often used to overcome computational issues and it seems bit can counter intuitive that computationally unbounded there will benefit in some way by returning hypothesis that do not belong to the lower class.",
                    "label": 0
                },
                {
                    "sent": "But as we will show, this is actually the case.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "I think it is quite simple and nice, so I'll do it in almost full detail.",
                    "label": 0
                },
                {
                    "sent": "So X will be an arbitrary finite set and the set of labels will be very large.",
                    "label": 0
                },
                {
                    "sent": "An maybe it will.",
                    "label": 0
                },
                {
                    "sent": "It will consist of all subsets of X together with one more special label that we do not buy stuff for every subset T of X will define a hypothesis H of T that is defined as follow HD of X is the if X.",
                    "label": 0
                },
                {
                    "sent": "30, which is also a label if X belongs to T and otherwise it is the special label style and our hypothesis class will consist of all that functions vertie the size of T is exactly half of the size of X. OK, so this is the hypothesis class, and before I'll state the first theorem, let me make few remarks regarding learning this class.",
                    "label": 0
                },
                {
                    "sent": "So suppose that a learner get sample labeled by some hypothesis from this class and we want to find a good hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Now, if it's with the label T, you know it know for sure that the land hypothesis is HT because the only hypothesis that might return the label T is H of T so.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is fully determined by the hypothesis it returns on samples of the Form X1 star, 2X N stop.",
                    "label": 0
                },
                {
                    "sent": "OK, So what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if that, even though the sample complexity of this algorithm is upper bounded by one over epsilon, the sample complexity of every proper algorithm is lower bounded by the cardinality of X over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this shows gaps between learnability and proper learnability, and if it is infinite, then we have a class that is learnable but is not learnable by a proper learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I'll make one remark that slightly weaker phenomenon, namely gap between in arms.",
                    "label": 0
                },
                {
                    "sent": "Also happens so this is a very weird class and no one will think to use that in practice.",
                    "label": 0
                },
                {
                    "sent": "But we also have a more complicated examples that the phenomenon of gap between the arms happens in classes that are using practice.",
                    "label": 0
                },
                {
                    "sent": "But I won't talk about this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me give you the proof.",
                    "label": 0
                },
                {
                    "sent": "We start with the easy and maybe less interesting part, that is, sample complexity is upper bounded by one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So here should show an algorithm with sample complexity is upper bounded by one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we will simply take the first algorithm that you will think about.",
                    "label": 0
                },
                {
                    "sent": "So I remind you that we only need to define it on samples of the Form X1 style, 2X and star and on that sample the algorithm will simply return the constant hypothesis, which is the constant function.",
                    "label": 1
                },
                {
                    "sent": "Stop OK, so this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And note that this algorithm is improper, because this function do not belong to H. OK so I created the sample complexity is a bounded by one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So suppose HD is the target hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We try to learn.",
                    "label": 0
                },
                {
                    "sent": "The point is that a will return either the correct hypothesis H of T or the constants hypothesis style.",
                    "label": 0
                },
                {
                    "sent": "Now since there are only two hypothesis, if one of them is bad it will be rejected with high probability.",
                    "label": 0
                },
                {
                    "sent": "Upon seeing only one over epsilon samples.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple argument.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we will show that the sample complexity of every proper algorithm is lower bounded by X over epsilon, and for simplicity we'll do that for constant epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this is just to give you the idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose that A is a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we ask fix some set of cardinality X / 2 and the distribution will be the uniform distribution on the set and the land hypothesis will be age of the complement of a.",
                    "label": 0
                },
                {
                    "sent": "So after of it we don't care what happens because the probability there is zero and inside the all the labels that we will see our stuff.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, suppose we running the algorithm and then it returns some function of the format of the where the size of the equal to Alpha fix.",
                    "label": 0
                },
                {
                    "sent": "OK, this is because the algorithm is proper.",
                    "label": 0
                },
                {
                    "sent": "So what is the error of their internal part of this?",
                    "label": 0
                },
                {
                    "sent": "So we only care about examples inside it, cause outside probability is 0 now inside it.",
                    "label": 0
                },
                {
                    "sent": "On the complement of the, the return hypothesis is correct.",
                    "label": 0
                },
                {
                    "sent": "It's a style, but on the intersection it it's around us because it's safety an the two labels are style.",
                    "label": 0
                },
                {
                    "sent": "So the arrow is proportional to the Intersect intersection of T&E.",
                    "label": 0
                },
                {
                    "sent": "Therefore, because the size of both ENTR X / 2 in order to return a good hypothesis, the city must be almost U joints from the setting.",
                    "label": 0
                },
                {
                    "sent": "In other words, we have to recover approximately recover the setting and should be quite clear that in order to recover a set of cardinality, it's over two.",
                    "label": 0
                },
                {
                    "sent": "We have to see at least order of X examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the idea of the proof and.",
                    "label": 0
                },
                {
                    "sent": "I won't say anything more about this.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "33 opens the basic question.",
                    "label": 0
                },
                {
                    "sent": "We know that ERM is not optimal here, and we need another generic algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show that such an algorithm is the one inclusion algorithm, so this is not a new algorithm version of it for binary classification has been proposed by House Little Stone and once in 94 an it was generalized by orange and Bucket and Robertson.",
                    "label": 1
                },
                {
                    "sent": "A to the multiclass setting, and they also analyzed it by their analysis wasn't tight, and they had a factor of log Y.",
                    "label": 0
                },
                {
                    "sent": "And by annualized, we show that this algorithm is optimal up to a constant factor and actually up to a quite small constant factor a little bit more than five.",
                    "label": 0
                },
                {
                    "sent": "Let me maybe say one of the mark about this log.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "In a sense, it's just a closing Gaelic factor, but in many cases the two complexity of the problem is log of why?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to predict a whole sentence, then.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "The cardinality of the space is the number of the number of letters to the length of the center, so here.",
                    "label": 0
                },
                {
                    "sent": "The local factor in the number of classes is meaningful.",
                    "label": 0
                },
                {
                    "sent": "OK so I won't show you the algorithm due to lack of time.",
                    "label": 0
                },
                {
                    "sent": "But I want.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will say stated you, I'm so we need.",
                    "label": 0
                },
                {
                    "sent": "One definition will define the mistake.",
                    "label": 0
                },
                {
                    "sent": "Bound affirm.",
                    "label": 0
                },
                {
                    "sent": "A form of an algorithm made as the probability that a earth on a new example after seeing them examples.",
                    "label": 0
                },
                {
                    "sent": "And what we show is that for any other algorithm A, they mistake one of any algorithm A is lower bounded by a constant time.",
                    "label": 1
                },
                {
                    "sent": "The mistake bound of the one inclusion algorithm.",
                    "label": 1
                },
                {
                    "sent": "So as I said, this improves rubbish about an analyst and an bystander argument, it follows that the same algorithm is optimal also in the park model up to a factor of up to logarithmic factor of 1 over epsilon.",
                    "label": 1
                },
                {
                    "sent": "And one more remark, the one inclusion algorithm is a generic algorithm like, ERM, an in general it is very inefficient, but for linear classes we do derive efficient versions of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It is related to compression schemes, but unfortunately I won't have time to say much about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's make an instrument.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In summary, we also the last question how to learn H optimally, but what about the first 2 questions when age is learnable and what is the sample complexity of H so?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will characterize the sample complexity using a sequence that defined using Edge, so this is the sequ.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We defined the degree of hypothesis edge in Earth in H as number of points X in X such that there exists another function G that is identical to H except at the point X.",
                    "label": 0
                },
                {
                    "sent": "And we define the density of the class H as the average degree of hypothesis in edge.",
                    "label": 0
                },
                {
                    "sent": "And finally we define the density function of H as follows.",
                    "label": 0
                },
                {
                    "sent": "Mu M is the maximum of the density of F restricted to the set F where F runs overall sub or on over all finite subsets of H&S runs over all finite subset of exorcise them.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a certain function that is defined using using edge.",
                    "label": 0
                },
                {
                    "sent": "I don't expect you to.",
                    "label": 0
                },
                {
                    "sent": "Shifted at this moment.",
                    "label": 0
                },
                {
                    "sent": "But anyway, what we show is that the mistake bound of age.",
                    "label": 1
                },
                {
                    "sent": "Equal to the sequence over him up to a constant factor, so it follows that the sequence characterizes the density function, characterized the sample complexity, and in particular, H is learnable if or if and only if mu M / M approaches zeros.",
                    "label": 0
                },
                {
                    "sent": "M purchase Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seems like an end of story.",
                    "label": 0
                },
                {
                    "sent": "We have answer all these questions.",
                    "label": 0
                },
                {
                    "sent": "We characterize the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "We gave a generic optimal algorithm, but to be honest, I don't think that this is end of story because we would like a better characterization of the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Would like to characterize the sample complexity using a single number like the VC dimension and not by a whole sequence.",
                    "label": 0
                },
                {
                    "sent": "And here I don't have a theorem, but we do have a conjecture.",
                    "label": 0
                },
                {
                    "sent": "So the model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sequence of our analysis of the one inclusion algorithm is that instead of analyzing growth, we should analyze density.",
                    "label": 1
                },
                {
                    "sent": "So let me say a few more words about this.",
                    "label": 1
                },
                {
                    "sent": "So as many of you know, the complexity of, ERM algorithms is traditionally analyzed by analyzing the growth function.",
                    "label": 0
                },
                {
                    "sent": "So this is true for binary classification to multiclass classification and too many other learning problems.",
                    "label": 0
                },
                {
                    "sent": "But as we see here, the summer complexity is governed by the density function, which is a different function.",
                    "label": 0
                },
                {
                    "sent": "And therefore, instead of analyzed growth, we should analyze density.",
                    "label": 0
                },
                {
                    "sent": "So let's revisit the visit.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from this point of view.",
                    "label": 0
                },
                {
                    "sent": "So when I go back for a moment to binary classification, there are two equivalent ways to define the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "One way is the maximal M such that the growth function is maximal.",
                    "label": 1
                },
                {
                    "sent": "Another way is the maximal M such that the density function is maximal.",
                    "label": 0
                },
                {
                    "sent": "These are these are two equivalent ways for binary classification, but there are no longer equivalent for multiclass classification, even if in the case that y = 3.",
                    "label": 0
                },
                {
                    "sent": "And it seems like it is much more natural to adapt the second definition.",
                    "label": 0
                },
                {
                    "sent": "And this is what we will do.",
                    "label": 0
                },
                {
                    "sent": "We will define the dimension of H as the maximal M such that mu M equal M such that the density function is maximal and we want to relate the sample complexity of H2 this number.",
                    "label": 1
                },
                {
                    "sent": "Or equivalently we want to relate the growth of mu M to this number.",
                    "label": 0
                },
                {
                    "sent": "So let us review it for a moment.",
                    "label": 0
                },
                {
                    "sent": "Again, the binary case, so we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a nice theorem of Hofler little stonean vamos that tell us that MUFE M is equal to the VC dimension up to a constant factor of two, and we simply conjecture that this holds for the generalized VC dimension.",
                    "label": 1
                },
                {
                    "sent": "We conjecture that museum is bounded between the dimension of H2 two times the dimension of H. OK, so this is maybe the storing conjecture we can make will be happy with any constant rather than two we know to prove that it holds with.",
                    "label": 0
                },
                {
                    "sent": "Log why instead of?",
                    "label": 0
                },
                {
                    "sent": "And the consequence of this conjecture, if it's true, is that you have a nice characterization of the sample complexity using this new dimension, and let me make one remark about this conjecture.",
                    "label": 0
                },
                {
                    "sent": "So maybe if you if you remember that in called three years ago we had another conjecture that the intelligent dimension characterized the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So this conjecture still might be true, but I think that this conjecture is a better one for free reasons.",
                    "label": 0
                },
                {
                    "sent": "First, it is implied by the previous conjecture, so it might be the case that the previous conjecture is false, but this.",
                    "label": 0
                },
                {
                    "sent": "New conjecture is still true.",
                    "label": 0
                },
                {
                    "sent": "Second, the previous conjecture was more like guess and this is a bit more principle ized.",
                    "label": 0
                },
                {
                    "sent": "And failed, and I think most important, this is a very concrete combinatorial conjecture that if to as I said, characterize the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me quickly summarize, we have shown that yeah, arms are not necessarily optimal, and Moreover, in certain cases optimal algorithms must be improper.",
                    "label": 0
                },
                {
                    "sent": "We gave answer to this basic question.",
                    "label": 0
                },
                {
                    "sent": "We and we arrived, and you conjecture and let me just mention one more open issue is what happens in diagnostics.",
                    "label": 0
                },
                {
                    "sent": "So most of the results we don't know to prove in diagnostic setting.",
                    "label": 0
                },
                {
                    "sent": "So this is a very interesting there direction for future research.",
                    "label": 0
                },
                {
                    "sent": "Can I stop here and take questions?",
                    "label": 0
                }
            ]
        }
    }
}