{
    "id": "2lsg5abvl3yoiwh2wmnet7j2aejvvwtv",
    "title": "Who is Afraid of Non-Convex Loss Functions?",
    "info": {
        "author": [
            "Yann LeCun, New York University (NYU)"
        ],
        "published": "Dec. 29, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/eml07_lecun_wia/",
    "segmentation": [
        [
            "OK, so as usual when I give a nips to workshop talk, I'm sure I'm going to make some new friends here.",
            "OK, who is afraid of non convex functions?",
            "Who is afraid of non convex loss function?",
            "OK well please your Highness, I really appreciate that."
        ],
        [
            "OK, so I'm going to try to kind of mystify this.",
            "Try to make you less afraid of convex functions.",
            "Convex mondex.",
            "This community has suffered of a kind of acute convexa rightist epidemic in the last 10 years, would say.",
            "If you look at the new models that have come up from this Community, things like you know CRF.",
            "Is the Bayes many for learning or various other methods, QPS, kernel methods makes margin Markov Nets, exponential family, graphical models?",
            "All of those have a common characteristic, which is that whatever function you manipulates either for inference for learning is essentially linear prioritization.",
            "Linearizing the parameters between projects and then you have some simple quadratic or something like this on top of it, with some constraints sometimes.",
            "And you know the machine learning community seems to have trouble moving beyond simple things like this, such as logistic regression and generalizations thereof, VM differential family, graphical models.",
            "And there are good theoretical reasons for this is because you know, we can.",
            "We can do theory for those models and it's very difficult to do theory for other types of models have non convex loss functions.",
            "You can't prove convergence, you can you know.",
            "So you can get scared right.",
            "On the other hand you get scared.",
            "It's kind of like.",
            "You know you move to certain countries where.",
            "You know you can get.",
            "You can get scared of sort of new things you can do, and maybe you are going to say for in other countries when you have fewer opportunities and.",
            "You know, I've tried to turn into Jeff Hinton now.",
            "Doing sort of bad jokes that sort of.",
            "Anyway, let's do that.",
            "So, you know convexity is sometimes a virtue, but I sort of see it more like a more likely mutation because there's a whole world out there of sort of non convex models that give you a lot more flexibility in what you can do.",
            "And of course you know don't know proofs perhaps, or perhaps that proves the weaker, but on the other hand it's about more fun.",
            "So that's kind of a similar thing that's happening to machine learning that has been happening to control theory, which is control theory really hasn't moved very much beyond legal assistance in terms of the real theory.",
            "I mean, there's a lot of practical things that are not linear, but much of the theories about linear systems.",
            "So if you have nonlinear system, is the first thing you're trying to do is linearize it first.",
            "Right, so one problem though is that we insist on convexity, usually for efficiency reasons, but very often the price we pay for this is actually a non variable increase in the complexity of the model.",
            "We have to use if we wanted to solve the problem.",
            "So a lot of optimization algorithms that people use for kernel methods or or SDP based optimizations.",
            "Are you ordering square or in cube, whether N is the dimension of the input or the number of training samples.",
            "And if you went to the over tutorial on Monday, you will probably realize pretty quickly that that is going to become an acceptable as we get more and more data then it's going to stream and you can't have algorithms that take longer to treat the data at.",
            "The data comes at you.",
            "Otherwise you're going to have to drop data on the floor.",
            "So we need algorithms that are all ran."
        ],
        [
            "Now that means pretty much all the sophisticated ones are out the window.",
            "And you did use very simple ones.",
            "But if you use super learning algorithms, you might want you might need to compensate that by sort of increased complexity of the type of model you want to use.",
            "So other communities aren't as afraid as as we are among convex optimization.",
            "So there are communities that are you could see as sort of application areas of machine learning, but in fact there's sort of pre existing machine learning and you know where machine learning services reinventing a lot of the techniques that have been invented in those communities and simplifying them to make them convex.",
            "So for example, what we call a CRF in this community.",
            "Has been used for about 15 years in handwriting recognition and speech recognition, before even CRF existed.",
            "And essentially the same set of techniques discriminative training for you know you want to make the score of the correct answer better and make the score of the other answers worse so that your Instagram is going to find the best answer.",
            "And you know, most handwriting recognition systems are trained that way, and pretty much every single speech recognition system is trained that way, and every single one of them is nonconvex.",
            "Every single one of them.",
            "Why is that is because convex doesn't work?",
            "You can't do speech recognition with a nonconvex model.",
            "You have to use something like, say, mixture of gas and models, and it's very highly non convex.",
            "In fact, the file is not configured to use them and you have to use K means to initialize it, and there's all kinds of problems associated with this.",
            "But you don't have a choice.",
            "You have to use this on convex models.",
            "They're the ones that work.",
            "No one has really been successful at beating them.",
            "Beating gas mixture model for handwriting for speech recognition with anything else.",
            "Certainly not with convex models.",
            "So this is not by choice, but it's.",
            "You know it's a whole world out there and we should really, you know, concentrate our efforts on.",
            "You know, not trying, perhaps to, you know, get the latest version of kind of a convex simplified convex version of something else we were doing before, but perhaps increase the power complexity of the things."
        ],
        [
            "I want to do to solve those problems.",
            "So one of the type of problems.",
            "So this is kind of an argument that you should do and I have been sort of advocating for the last year or so.",
            "We wrote it kind of a review paper on this this year of scheduling algorithms towards the eye, which was actually the result of another of those workshops that was organized by.",
            "Delete the cost and #2 a couple years ago and.",
            "Going to sort of paraphrase the deep learning satellite session that took place on Thursday afternoon at the main conference.",
            "Ultimately complex running tasks such as a vision, speech or language processing, natural language processing will be implemented with what you could.",
            "You could call deep hierarchical systems, so systems that involve lots of nonlinear operations in them because complex decisions involve lots of nonlinear operations.",
            "If you don't agree with this, just just hold on for a second.",
            "So to learn how hierarchical representations you need to learn low level features, medium level representations of features and high level concepts and between all of those you have some sort of nonlinear transformation that takes place.",
            "So if you want to learn all the stages in the system, so I'm intentionally, you need some sort of necessary.",
            "You're going to come up with going to solve some sort of nonconvex problem.",
            "Um not interesting remark.",
            "You can say there is that.",
            "But animal learning biological learning certainly does not minimize the convex function.",
            "And the reason it doesn't is because, or any sort of evidence, that it doesn't, is the fact that the order in which we learn things actually matters.",
            "If we were to measure convex loss function in the order in which we learn things would not matter.",
            "We will always get to the same minimum, right?",
            "But in fact, the order in which we learn things does matter.",
            "That means you know we can reshaping or loss function in such a way that we get to the right minimum eventually.",
            "Your question.",
            "OK. OK, but wait, but we have theorems that the kernel machines can approximate anything we want or even the two layer neural Nets for that matter can approximate anything we want it did we do, but those theorems say absolutely nothing about the efficiency of the representation with which the efficiency with which we can represent those.",
            "You know any function with those things.",
            "And for example, there are there is empirical and theoretical evidence that shadow architecture is.",
            "So I cannot really implement things like invariant visual recognition efficiently."
        ],
        [
            "So I'm going to just flash a couple examples to you, so these are sort of, you know, some of the latest results on this.",
            "Actually, I forgot a few here.",
            "And you know some of those models are deep, some of them are shadow.",
            "But you know, I'm not expecting you to kind of look at all of this, but basically the one that works best are the deep ones.",
            "Not only do they work best, they actually train faster and they're faster to run at the end."
        ],
        [
            "OK, I'm going to show you a slightly more specific example of that in a minute."
        ],
        [
            "Um?",
            "So here is the slightly more example of that, and I showed this example a couple years ago as well, but let me repeat it here.",
            "So let's say you want to recognize objects.",
            "You have those images that are roughly about two pixels.",
            "You actually get two of them, 'cause if stereo images plug them into a linear classifier, you get 30% error on the test set.",
            "OK, where the test set.",
            "So this is the training set is here.",
            "The test set is there for each of those images.",
            "You have multiple images of each each object, so it's kind of invariant recognition problem, right?",
            "Also 30% for linear classifier 18 between 16 and 18% for Kenny's neighbor, something like 11% for SVM 500% conversion.",
            "It's not.",
            "It's an unfair comparison.",
            "Of course the SVM has no idea where you know it's looking at images.",
            "But still, the difference here is between sort of shadow and deep.",
            "There all the differences as well.",
            "Of course you can always learn the kernel, but then you know if you're on the kernel you again you have a deep deep system."
        ],
        [
            "Um?",
            "It gets worse if you look at the time it takes to train the system.",
            "So they shouldn't go in front of the screen, so maybe I need a pointer.",
            "If anyone is appointed OK, so it gets worse.",
            "So if you look at the time it takes to train the support vector machine to get to 11.6%, it takes 480 minutes on a sort of virtual one giga Hertz machine.",
            "So that's kind of a long time to get the same accuracy with the commercial net you need 64.",
            "Minute giga Hertz.",
            "The same, the same data, so it's basically 8 times faster.",
            "OK, no proofs of convergence snowbound.",
            "Know you know where there are bounds, but there's sort of very general bounds.",
            "Nonconvexity certainly, but it's faster, it works better, and if you sort of keep training for the same length of time at support vector machine, you get half the error rate.",
            "The test time is even worse, it's about 30 times faster to test the commercial net SVM.",
            "It's considerably smaller.",
            "OK, so changing the architecture makes a huge difference, and if you have to throw away convexity for this, fine."
        ],
        [
            "It gets even.",
            "Worser.",
            "If you take those images that are sort of cluttered versions of the previous ones, training within ATM with Gaussian kernel, we get 43% error.",
            "Basically it doesn't work.",
            "You know pretty much older points are super vectors.",
            "I mean it's not clear that the there was any convergence to speak up here, but essentially the best implementation we had at the time of supervision machine, which was the one at a venue.",
            "See actually it wasn't one of those implementation, it was on speaker graphene.",
            "There 'cause that was implementation on parallel machine you couldn't get beyond that level.",
            "Commercial net 5.9% error.",
            "That's actually when the amount of the sort of rock."
        ],
        [
            "That is a little less than 8%.",
            "And then the train time are even worse than test.",
            "I'm even worse.",
            "VM took it actually 10,000 minutes seconds to give us the wrong answer, and in about half the time the conversion negative 7.2.",
            "So enormous differences and with the non convex you know.",
            "Not particularly well justified theoretically."
        ],
        [
            "3rd so you gotta be careful with.",
            "You know, with things like that.",
            "OK. Let's see."
        ],
        [
            "OK, so one of the things that that has become pretty."
        ],
        [
            "Over the last few years, including for things like SVM and Search, is that people have been writing tons of papers on various optimization algorithms.",
            "For SVM, you know, in the in the neural Nets and various other learning machine CRF.",
            "But we came out of.",
            "You know you need to show those work and lose work and some of some work in the in the past is basically simple.",
            "Stochastic gradient descent beats everything by a factor of 10 or 100 or sometimes 1000.",
            "So it's kind of annoying because you come up with all those really sophisticated optimization methods that get you a paper into NIPS.",
            "And if you compare it to the right method, you don't get a paper internets, but you might as well not compare it to the right methods.",
            "So in fact, I kind of wasted an entire year of my life when I was in Toronto as a post Doc actually implementing those optimization algorithms, limited storage BS, and BGS.",
            "And you know all kinds of crazy Newton method conjugate gradients comparing them with stochastic gradient and after six months with this time wasted time, I figured you know just you stochastic gradient publish short paper that you read and then we're talking about life and we can have we discovered this and get some data on you learning algorithms to kind of convince people that that's the case."
        ],
        [
            "Um?",
            "So one problem is."
        ],
        [
            "That"
        ],
        [
            "Um?"
        ],
        [
            "Let me see what you want to hear."
        ],
        [
            "So these are some arguments for why why?",
            "Deep architectures are more efficient, but let me let me skip this.",
            "I don't want to kind of, you know.",
            "Hammer the head really too far down the in the blank so OK, so there are several strategies that people can sort of use machine learning nowadays to kind of try to attack the sort of AI problem.",
            "Or can I solve a sort of general problems?",
            "This is sort of philosophical and I'm sort of recycling an argument that Geoff Hinton's returning orders review papers.",
            "So it's sort of slightly different, but it's kind of paraphrasing what he said.",
            "So what I'm possible attitude is defeatism, since there is no good parametrization for kind of really complex functions.",
            "You know we should stick to kind of building or feature set by hand and stick a convex.",
            "Classifier on top and for each new problem will just find a new set of features or you kernel and you know the be happy with that.",
            "I will keep our jobs.",
            "Um?",
            "The second attitude is denial.",
            "Kernel machines can approximate anything we want, and the VC bounds guaranteed realization where we need.",
            "Why would we need anything else?",
            "But of course, you know there's all those arguments that kernel machines cannot really represent certain classes of functions efficiently, and the examples I showed you with object recognition is kind of evidence to that.",
            "Essentially, that's because supervector machines with gas and kernels is essentially glorified.",
            "Template matching is basically, you know, you sort of.",
            "Do a template comparison between the training samples and the input pattern.",
            "Then you record all the scores of similarities using the Gaussian or whatever color you want to use and then you combine those with a linear combination.",
            "This template matching basically.",
            "So how can you do invariant recognition Gagnon concepts with template matching?",
            "You can.",
            "Now you can do a lot of this complex with template matching.",
            "There's absolutely nothing wrong using this if it's the best for your application.",
            "Actually, you know I'm not saying and I use it for all kinds of applications as well, it's just that issues.",
            "If you want to solve really complex.",
            "Problems I type problems this is not going to work.",
            "This is kind of like you know.",
            "You know, going to make friends again, climbing a ladder to get to reach the moon or something.",
            "Um?",
            "OK, so the last option of course is optimism.",
            "Let's look for some learning models that can be applied to the largest possible subset of the I set, and we're requiring requiring this most amount of specific knowledge for each task.",
            "And."
        ],
        [
            "That's what some of us after.",
            "OK, So what about what about deep learning?",
            "Let's go a little bit into that.",
            "Now you know it used to be that a lot of people in this community were obsessed with training neural Nets, and there were a lot of papers on this and it sort of died out for various reasons.",
            "Didn't quite die out, but you know, there are different fashions, and one of the reasons was because there was this perception that because the optimization and convex you don't understand everything there is to understand about it.",
            "And perhaps sometimes it fails and you don't understand why.",
            "So let's take a very simple example.",
            "Let's say we have the simplest possible modular neural net you can imagine.",
            "It's got one input when heating unit and one output OK. Are you going to train it to compute the identity function right?",
            "How could it be OK?",
            "So it's got only two ways, one from the input unit to the hidden unit and one from the unit to the output unit?",
            "And to learn the identity function, let's assume the hidden is linear.",
            "To learn the identity function, it needs to set one of the weights to some value and the other weights to one over that value, and then you get the identity function right?",
            "So if you look at.",
            "So the loss function, which is the square difference between the output of the network and what you want.",
            "As a function of those two weights, it looks something like this, so this is actually with the sigmoid, but makes make much difference.",
            "So the solution space.",
            "The best solutions are those kind of little is this sort of hyperbola here.",
            "And on the other side right?",
            "You can have either X and 1 / X where both positive or excellent 1 / X when both are negative.",
            "And then everywhere else is kind of high.",
            "High surface and it's got a central point at 00.",
            "So 00 you know if you start from zero, 0 is not going to move anywhere.",
            "Close the gradient to 0 so you have to kind of be a little bit on the side and then it's going to fall into one of the local minima.",
            "OK, so now you have to picture this in high dimension.",
            "If you have a high dimensional neural net you know with lots of lots of units.",
            "Let's say only two layers, you're going to have lots of several points in all kinds of dimensions, and that means in a lot of dimensions the loss function is going to be convex.",
            "Another dimension is going to be concave, and you're going to have several points where they need.",
            "OK, so now if the neural net is has just the right."
        ],
        [
            "Size to solve the problem.",
            "Is just the right size.",
            "There's going to be a very, very small number of those dimensions where you can find a solution, right?",
            "Compared to all the size of all possible dimensions in white space is going to be just a very small number of dimensions that where the error goes down and you have a solution through space.",
            "So it's going to be very hard to find.",
            "But if you make the network really large, then most of the dimensions actually are solutions.",
            "Most of the quadrants.",
            "If you want our solutions.",
            "OK, so it's going to be very easy to find.",
            "So in other words, what that means is that this is also intuitively under intuitive, easy to understand intuitively with other arguments.",
            "If you have a Twitter net with very very very large hidden layer is going to be very easy to train it.",
            "OK, so let's take kind of limit case.",
            "Let's say we have a neural net where the hidden layer is essentially infinite, extremely large OK and you should have done some theoretical work on this.",
            "But let's assume it's so large, in fact, that pretty much all configuration of possible weights that you will ever need for the first layer.",
            "Or we did there.",
            "So the loss function is non convex as a function of those weights.",
            "Of the weights of the Internet, OK?",
            "But the point is, you never need to learn the first layer because the weights are already there.",
            "So the only way that you need to learn other other weights in the top layer you will be able to learn any function, but just adapting the weights in the in the in the second layer.",
            "Because in the first you already have them.",
            "OK, so many of them that you can just pick the Windows correct by setting the outgoing weight of it to some positive or negative value to another value and you're done.",
            "So here is a situation where even though the last function technically is nonconvex as a function of all the parameters, in practice, because of the architecture, the learning is actually very simple and in the limit actually convex, because because you made the system so large that essentially there's kind of a convex subspace in which you will find the solution.",
            "OK, so of course there are sort of two limit cases, But what that means is that the larger the network is going to be, the easier is going to train, so this is going to counter intuitive.",
            "You would think that if you make the network very large, that's going to be also local minima, but in fact local minima are good for you.",
            "They make it easier to find to find a good minimum, because all of those local minima actually more or less equivalent.",
            "So another picture of this is.",
            "Let's imagine you have a loss function in one dimension that has a local minimum, right, and then a mountain, and then another a global minimum.",
            "OK, in one dimension you can go through the mountain to go to this other minimum.",
            "You have to kind of go up so a great in bed will not find it.",
            "But now let's add a dimension.",
            "You're going to be able to go around the mountain, so in that picture of this, in a million dimensions.",
            "I'm sure you will do that.",
            "Then there's going to be lots and lots of dimensions that will go allow you to get around local minima, and in fact, depending on the kind of basic elements that you assemble in the network, it's very difficult to build a box, right?",
            "I mean, if you have a million dimensions to build a box in a million dimensions, you need a hell of a lot of planks.",
            "Each neuron basically is a Planck 'cause it doesn't have space, so it could be you know, hardly complicated to do that.",
            "Right, so you have flat areas of course, because if the if the weights become very large the signals get saturated and then you're so the ways to compensate for this is just not to get there.",
            "You can do this with decay, so there's a few tricks like this, so you have to run the same way.",
            "There are a lot of tricks if you want to make large viens efficient.",
            "This entire workshop is about this.",
            "So which means that means there are tricks, right?",
            "Otherwise why are we here?",
            "So yes, there are tricks, but they're not any more difficult than any other instances.",
            "So one problem, of course, is that if you're sort of starting to play with things like this, the first thing to try is you know a tiny neural net on the exclusive or, and it happens to be very unreliable."
        ],
        [
            "OK, so this idea of so.",
            "Basically in 1957 people sort of figured out with the perception work that you know they only knew about convex optimization back then.",
            "So what they figured is that they only had to make this layer really large, with random basically random functions, random features, and you know there's a good chance there probably will be solved that was 1957.",
            "I had five, you know with back prop.",
            "People figured out women.",
            "Perhaps you could run this.",
            "It sort of works in a lot of cases, but there are situations where it doesn't work.",
            "1982 The kernel machine.",
            "Colonel Machine sort of came out of the.",
            "Of the Bush is an said.",
            "Well if I set if I have 1 hidden unit for each training sample.",
            "Yeah :) next to me.",
            "Yeah, that's right.",
            "Actually, the three offices next to me.",
            "Poser.",
            "Is a big beyond value of ethnic, but anyway.",
            "If you if you said each hidden unit 2 training sample, different training samples, you said the weights of each unit to a different training sample that basically you built a ramp for your training set.",
            "OK, that's like a decoder for random access memory, right?",
            "You plug you plug it in, put and then one of those units is going on because it's going to recognize its corresponding training sample.",
            "And so you just said the way to the output you want, and you're done.",
            "It's a random access memory, so you make the kernel smooth and then you can get kind of a smooth bottom access memory and that support vector machine.",
            "River restaurant basically.",
            "So that's kind of, you know, it's not beyond the perceptron.",
            "In fact, there was several papers at the conference, at least when paper at the conference.",
            "That was sort of arguing for random first layers.",
            "This is sort of drawing from theoretical results from compressed sensing which are."
        ],
        [
            "Really interesting mathematically, but.",
            "So what's the problem with convex learning?",
            "So one thing I mentioned before is that none of what you read in the optimization literature applies.",
            "Encourage you to see over tutorial either the slides or the video, which I hope will be will become available and like the ones last year and the reason is you need to use stochastic methods to take advantage of redundancy in the data.",
            "So sarcastic methods are the methods where basically you did the parameters after each sample instead of kind of computing and updates over the entire training set.",
            "OK, now that exploits the redundancy in the data, so you have very large datasets, particularly their streaming.",
            "That's really what you want to use and produce theoretical results on this few years ago that showed that we talked about this tutorial that shows that.",
            "Even if this does not optimize the loss function on the training set perfectly, this is as good as it's going to get on the test set anyway, so you don't need to optimize more than that.",
            "So the problem is that so of course stochastic methods of horrible asymptotics, essentrics properties, and so if you tell an optimization person I'm going to use this to castigate algorithm says first there is a convergence proof, although they are, but they are kind of weird.",
            "And second, you know the convergence property are you know linear or or even sub linear is really horrible.",
            "OK but they will show that actually works really well.",
            "I mean, many people showed that it was really well, but let me explain why basically.",
            "And the bad news, of course, is that the optimization optimization literature does not talk about stochastic methods at all.",
            "You can't find anything about this anywhere, and so you know configuration information, storage BGS, BGS, quasi Newton.",
            "That's all out the window.",
            "You can't use any of that and many of those methods are ordering squared anyway.",
            "Ordering cube so you know when you send."
        ],
        [
            "So you start with simple methods.",
            "You can publish papers on complex methods because you know the simple methods are the ones that work best.",
            "How disappointing?",
            "So well, this is kind of the same way that I made earlier, right?",
            "So so so this is kind of the point.",
            "That sort of making the system very large, sort of basically sort of, you know, using the trick that SVM taught us.",
            "It's kind of a good way of avoiding the problem of local minima really being a big problem.",
            "So another problem also is sort of breaking the symmetry in the system.",
            "There's a lot of local minima due to the fact that you can, for example, interchange too.",
            "Hidden nodes in a neural net and you get the same solution and so that means there are two local minimums are equivalent due to a permutation, and that means there is a kind of a central point between them and by breaking the symmetry you sort of reducing the."
        ],
        [
            "Over the space.",
            "And that might be one of the reasons why things like convolutional networks so well, because they, even though they're very, very deep.",
            "That's one of the few instances of very deep architectures that work well with back prop, and one reason this perhaps because not only are they wide in terms of so one problem that you have to solve with you, unless you can't make the the hidden layer is very wide, because the number of parameters grows really quickly with the quadratically with the number of nodes.",
            "And then you get killed by over prioritization.",
            "So you have to find architectures where that's not the case where you can increase the size of the number of variables that are manipulated, but not the number of parameters that are being trained.",
            "And convolutional Nets is a way of doing this.",
            "The other advantage of it is this.",
            "You know symmetries are broken in it, so not every node is connected to the same set of variables and so naturally they tend to compute different things."
        ],
        [
            "So I heard this argument several times also, well, you know the problem with neural Nets is that you know you only young can make them work.",
            "Jeff actually made the jokiness.",
            "He doesn't believe a word, of course, but he sort of made a joke.",
            "Anyway, he wants a joke in this tutorial and several people have talked about this and this is entirely not true.",
            "There's actually a guy called Mike O'Neill who can also do it.",
            "You guys have never heard of Michael Deal, right?",
            "OK, let me show you who Mike O'Neill is OK.",
            "So Michael Neal produces really nice website on codeproject.com where he basically took the papers on, you know, convolutional Nets and blah blah blah and my papers and Patricia Marx Papers and and said you know what if I re implemented this so he he implemented it and rebuild this really nice little Windows application where you can click on stuff and recognizing these digits you can download the code and then he explains you know everything how you do this.",
            "How you do this and you know how this code works and blah blah blah and these pages and pages of that stuff and.",
            "You know all the details and experimented with a lot of different things and implemented.",
            "You know stochastic diagonal liver marquard and all that stuff.",
            "All the tricks in the book by just reading the papers you know.",
            "Very nice.",
            "Very nicely done.",
            "So this guy is YPG.",
            "Don't mighty.",
            "What's your guess?",
            "OK, who is he?",
            "This is a guy Michael need is a patent attorney in Southern California where he specializes in computer and software related patents.",
            "He programs as a hobby.",
            "And invented time to keep up at assigned the technology of his clients.",
            "OK, alright, so if a lawyer can do it, you know?",
            "That's his little application here that works really well.",
            "This is Linux, but here I'm running it under wine, which is Windows Emulator.",
            "It works nicely.",
            "Say again.",
            "OK, so this guy did this.",
            "You didn't talk to me right?",
            "He just you know, read the papers did this on his own for fun and then last year at NIPS.",
            "It was actually and it sent me an email saying, you know we're missing.",
            "You know, what do you think you know?",
            "So it's pretty cool.",
            "That's true.",
            "Do something.",
            "There was a patent attorney this."
        ],
        [
            "Yeah, OK, so I mean, you know he's very smart guy, obviously, but.",
            "But but it's not like there is.",
            "You know any mystery in there.",
            "OK, let me skip."
        ],
        [
            "This.",
            "How much time do I have?",
            "How much time are left?",
            "Famous done 6 minutes.",
            "OK good.",
            "Alright so I think what stops people from using non convex optimization is that they just don't have the right tools either right?",
            "Conceptual tools.",
            "Or the right?",
            "Software tools or theoretical tools?",
            "Certainly the theoretical tools you know.",
            "They're kind of missing, but but the other tools aren't, so there is one really cool tool that some of us have been using for about 20 years or 15 at least.",
            "Called automatic differentiation.",
            "Actually a very, very simple version of automatic differentiation, so automatic differentiation is this really there's a whole community on this, mostly in sort of numerical methods where you cannot write a program in FORTRAN or C++ or whatever.",
            "And automatically spits out the code that will compute the derivative of whatever function this program computes.",
            "OK. And I have to say the best reference on this recent reference.",
            "It's not the best reference, but the people who really understood is the best.",
            "It's going to take a few years for the community to really kind of understand what they're doing, but really the people understand the best are Barack parameter Andrzejewski siskin, who is here?",
            "Actually they published a paper, or they actually about to publish paper in programming languages, probably language Journal.",
            "Basically they have a whole type system for for automatic differentiation using.",
            "Calculus and functional programming, and they've completely worked it out and they can produce really efficient code that essentially computer into of anything.",
            "So it's really cool, but one of the systems that Leon I implemented for the large system that we use for research an order but also kind of picked up for his torch system and run miscevic used for his Monty Python system is a very simple idea.",
            "It doesn't use any complicated concepts, you can implement it in almost any language, but you can't implement it in Matlab, which is why you know.",
            "And the idea is very simple.",
            "In fact, you could probably implement symmetric, although probably inefficiently.",
            "If you have a bunch of modules that sort of compute various functions and they pass information to each other in the forms of the form of vectors or whatever for each of those modules, you have to write basically two functions to methods if you want.",
            "So this could be a record class, and then you will have to member functions or two methods, one called F proper whatev does is that it takes the input and compute the output with the input by passing through the function, and one could be proud and what the problem is is that.",
            "Given the gradients of whatever loss function respect to the output multi property Jacobian and compute the gradient of whatever loss function respect to the input.",
            "So it's very simple thing to write, you can write this fallen into modules you have, you know, linear combinations, softmax, you know you know, sigmoid, whatever, and then by assembling things like this.",
            "I'm doing a proper proper proper writing, new classes.",
            "You can essentially build any complex function automatically and you don't have to worry about actually writing the code to compute the gradient, it's automatic.",
            "Basically it's a you know object oriented permutation of chain rule, right?",
            "There's nothing more than that, but it's really really simple, and it allows you to do very very simple build very complex models very simply.",
            "Now the trick you can do which we called the prop, and this actually computes second derivative, so it applies kind of the same algorithm to back propagate second derivatives with those those boxes.",
            "So it turns out there is a very, very simple algorithm which if you have.",
            "What a piece of program that computes the back propagates derivatives through a module with a very simple transformation of the program, which is almost completely mindless.",
            "You can essentially write the program that also back propagate the second derivatives, or at least the diagonal term of the Hessian through the same box.",
            "So basically, if you had the diagonal terms of Hessian of your loss function with respect to the variables coming out of this box, by applying this be prop function you will get the diagonal terms of the Hessian.",
            "So the last function in respect to those wires, or respect to the internal parameters of that system as well.",
            "So the nice thing about diagonal terms of the Hessian is that you can use them to do preconditioning of gradient descent, and that tends to be really efficient, so we worked on this for a few years and look with the details of this, but.",
            "We've been using this to train our neural Nets or nonlinear assistance for four years, so it's basically."
        ],
        [
            "Called.",
            "Stochastic diagonal levenberg Marquardt and the idea is extremely simple.",
            "Basically for each parameter you have a different step size using stochastic gradient and the state size is computed as some constant which would decrease overtime divided by an estimate of the diagonal from the second derivative of the last function with respect to that particular parameter.",
            "That this step size is going to be attached to, press some sort of you know.",
            "So that prevents this thing from blowing up.",
            "If this goes to 0 now, this estimate of the second derivative is not really the real second derivative.",
            "It's more like a like a gas Newton approximation to it, so it's something that's always positive.",
            "And this sort of does very, very simple preconditioning of the.",
            "Of the.",
            "Of the space in such a way that it doesn't cost anything.",
            "Basically you can run this so few time once in awhile, because exactly the same as doing a back prop to compute those secondary estimates.",
            "You don't have to do it as often, because the second derivative don't change very, very quickly most of the time."
        ],
        [
            "And this goes back to my purchases in 87, so this is really old.",
            "There's another problem with the sarcastic gradient, which is you have to be able to compute the best learning rate for it and Leo and I have used heuristics for this for many years.",
            "The heuristic is really simple, you know you the optimal step size when you stochastic gradient does not depend on the size of the training set, so you try it on a small training set.",
            "If it works fine, you sort of, you know, knock it down by a factor of two or three and then apply to large training set and then you decrease the learning rate according to the schedule and they all sort of explained how to do this.",
            "This heuristic works really well, so we figured out a much better way of computing the optimal learning rate for stochastic gradient descent.",
            "We don't have proof or whether it's the appropriate thing, but experimentally works really well and I go into the details of this is based on basically stochastic estimation of the."
        ],
        [
            "Second term of the Hessian using the power method.",
            "We have very simple formula to kind of update.",
            "The estimate of the."
        ],
        [
            "Great.",
            "Mr.",
            "Recipe.",
            "So basically the recipe is Topeka initial vector at random which has the same dimension as your parameter vector are present in pattern to your model and perform a forward problem vector product to compute the gradient Geo W. Then you add.",
            "Alpha times this vector outside to the current parameter vector, so it's kind of a normalized version of this of this vector multiplied by small constant.",
            "Again, you perform forward problem backward to compute the gradient for this new value G prime, and you compute the difference G prime energy.",
            "That gives you an estimate of the product of the Hessian by that that vector PSI, and then you sort of.",
            "Keep doing this basically so you get doing this, repeat it.",
            "And that's kind of equivalent to doing the power method or multiplying the Hessian by a random vector.",
            "And vector is going to converge to the eigenvector of larger second value, which is the one that in which the curvature of the loss functions.",
            "That is the highest and so.",
            "Then you estimate this curvature and said the running rate to one over the curvature and basically this formula does it sort of online if you want.",
            "And so that."
        ],
        [
            "It gives you sort of estimates of the optimal learning time, so optimal learning rate, so this is.",
            "This is what we know is the optimal learning rate.",
            "And.",
            "So this is kind of the error you reach after a certain number of epochs for various number of epochs.",
            "For vice size of the learning rate, and this is the predicted optimal value for the learning rate and exactly is that the minimum.",
            "That means that it predicts the optimal learning rate exactly as."
        ],
        [
            "As you want.",
            "It works for you, Sir.",
            "Various things, so I think I'm going to stop here.",
            "I'm at a time and perhaps."
        ],
        [
            "Some questions, thank you for your attention.",
            "For the power networks, can you also think the same argument if you're trying to learn a dynamical system and and you have recurrent connections, well, OK.",
            "So so recurrent networks have their own set of problems that actually yesterday quite extensively.",
            "So we have Angel had some papers that showed that if you want those recurrent Nets to do something really useful, then learning is going to be intrinsically hard.",
            "And so that's sort of put a really big damper on on people work on recurrent Nets, so people claim to have algorithms that work really well, but it's not clear.",
            "So there are certain cases they will work well.",
            "I mean, I'm not.",
            "I'm not claiming those techniques necessarily apply there.",
            "So you're saying that it's important to choose the learning rate correctly?",
            "And as Liam was saying earlier, tried on small training set.",
            "We're saying that in addition to trying a small training set that on that small training set, set it that you use the stochastic title LM method, or do that.",
            "OK, so get signal limited.",
            "Mark Rd is sort of orthogonal to choosing the right step size where it allows you to do is basically allows you to.",
            "If you have suggested diagonal Livermore, but it does that, preconditions the space so it's going to essentially have the effect of.",
            "Artificially increasing the learning rate in dimensions, whether the loss has low low curvature and sort of decrease it in directions with his high curvature.",
            "So it's like a preconditioning.",
            "But then you still have to choose the constant that will set the the step sides OK, which is kind of a global constant global multiplier for all of those.",
            "All of those updates, and for that you can use the technique that.",
            "How do you mentioned or this technique of stochastic estimation of it?",
            "But it turns out the I don't use that very often because the heuristics heuristic way is so simple and it works well enough that it's really not worth it.",
            "Published.",
            "Well, depends what you're interested in so.",
            "So if you are interested in, you know.",
            "Um industry application with determining, you know, probably autistic regression is going to do good job for you.",
            "So just stick with the question.",
            "But if what you are interested in is building intelligent machines.",
            "Then please don't use social progression.",
            "Find something else right so that's where you really have to think about more more sophisticated models like say deep learning.",
            "Like say, unsupervised learning for kind of pretraining deep network.",
            "There was a whole you know satellites session on this that some of you may have.",
            "I have attended.",
            "That's kind of the best hope we have at the moment.",
            "Sadly, is this pretty much the only one too.",
            "Why?",
            "Large network makes.",
            "Right, so so asking to explain again why?",
            "Like gradient descent is more reliable with large Nets.",
            "Well, so that's kind of the argument.",
            "I tried to make, which is that.",
            "You know there's going to be a lot with a large net.",
            "There's going to be a lot more dimensions that are more directions that are the content solutions.",
            "And in the limit is is the limit case where the first layer, for example of a two layer neural net is very very large, so large that all the weights are already there, so you don't need to learn them at all.",
            "And then the problem becomes effectively convex.",
            "Not technically convex, but effectively convex.",
            "So that's kind of this kind of document.",
            "OK, there's a similar.",
            "There's an analogy that you can make.",
            "Also, if you think about, say, linearly square.",
            "OK, so you're trying to say using this video, something you're trying to find the best least square solution to regression problem.",
            "For example, if you have fewer samples and you have dimensions, it's very simple to solve.",
            "If you have more more samples and dimension is very simple to solve right at the time where you have exactly the same number of samples, you have dimensions.",
            "Then the Hessian become extremely ill condition right at that point is essentially the condition number goes to Infinity.",
            "The tuition algorithms never converge.",
            "It's really bad when the system is just the right size.",
            "It's really bad.",
            "It's much better if its way over parameterized.",
            "Which case you need to regularize, or if its way under parameterized, in which case it's kind of self regularised.",
            "So what is VMS taught us is that you make the system way bigger than it needs to be, and you regularize the hell out of it.",
            "OK, and people in neural Nets didn't didn't use to do this because we were all brainwashed by the fact that the statisticians were telling us, you know, you shouldn't have more parameters than you have training samples.",
            "And it's just not true.",
            "You want a lot more parameters and you have training samples and then just regularize like crazy.",
            "By taking A1 hidden layer neural network with as many hidden Hurons as their input examples, and regularize the hell out of it and see if it performs as well.",
            "So be kind of obvious experiment to make right and I'm sad to say that actually haven't tried but.",
            "It could be we talked about 5 minutes, but.",
            "No, I mean some things we've tried, for example, is with convolutional Nets.",
            "If you make them bigger, so increase number of feature Maps, for example, two very very large numbers.",
            "Two situations, for example, when you have 10 times more parameters in the model, then you have training samples actually work better.",
            "You don't see overtraining.",
            "So what?",
            "It seems to me that optimism what what do we do next?",
            "If you have a technology that's reasonably effective for knowledge, free.",
            "Supervised learning right then maybe we should stop worrying about how much free supervised learning we should start thinking about knowledge, guided supervised learning and cumulative learning, right?",
            "That's the reason to be optimistic.",
            "OK, OK, so the question was, you know, if we have a reasonably good method for knowledge, free, supervised learning, then we should perhaps try to push the envelope towards non knowledge, be supervised running and we could be able to solve more complex problems.",
            "So first of all I don't believe there is any knowledge free supervised learning method that just doesn't exist.",
            "SVM is not knowledge free and it's not unbiased.",
            "So there are certain.",
            "You know it looks like his general purpose, the same way we like to think that our brain is general purpose, but it's not true at all.",
            "It's very, very, very specialized, and so there are certain types of functions.",
            "So again, with the Gaussian kernel for example, will efficiently implement certain types of functions and we would be horribly inefficient for all kinds of other functions.",
            "In fact, would be inefficient for many, many more functions than it will be.",
            "You know, the number of functions would be efficient for, so every model is very restricted, and in fact there is no free lunch.",
            "Theorems that show that you know if you have a generic learning model.",
            "That's really general.",
            "It actually can't run anything because you know there is no.",
            "It's got Infinity.",
            "Mention Barbara so.",
            "Every model is specific and the fact that we think it's general is just an illusion.",
            "It's just not.",
            "But then you say OK, so those are not knowledge people there they have pre.",
            "If you like they are prepared with completely random fictitious knowledge.",
            "Right exactly that right?",
            "So I guess Windows VM's VM for example has the fictitious knowledge that the function you're trying to estimate the smooth.",
            "That's essentially what it is.",
            "What it is.",
            "So if he's got lots of variations in it, it can do it.",
            "So you're Avengers paper on this.",
            "The other thing is that.",
            "So the question is, you know that is hopeless right?",
            "Loading doesn't work well.",
            "Let's just quit.",
            "But but no, it's not.",
            "It's not quite true.",
            "What you want is.",
            "You want the right primary transition for the type of problems you want to solve, so you know different people at different interests with supporting themselves.",
            "But the point I was trying to make is that if the problems you're trying to solve, our sort of AI type problems.",
            "Speech vision, things like that.",
            "Then shadow architectures are not what you want.",
            "You're not going to do it with, you know, show architectures and convex loss functions.",
            "You're going to have to find other families of functions, other parameterisation, and that's where deep learning comes into play.",
            "We know we have a working example of this.",
            "The visual, this is slide.",
            "I skipped.",
            "The visual system is very is very deep and probably learned and so that's at least one example.",
            "We have a working system that's not all approved that they can't do it any other way.",
            "Is there any other way probably?",
            "Perhaps evolution would have found it.",
            "You had a question.",
            "Just following on from Stuart I guess.",
            "So what you're saying is that neural networks with sigmoid units is the right kind of bias to put to solve very difficult problems, which seems a bit.",
            "did I say that?",
            "Well, no, I didn't say that.",
            "But let's build these large general purpose architectures based on sort of amorphous sigmoid units, initialized some random way and we should be able to solve everything with that now.",
            "No, I didn't.",
            "I didn't make that claim.",
            "OK, yeah, that will give you good.",
            "Right?",
            "But the other thing is that maybe that will work when you have vast amounts of data with stochastic gradient descent.",
            "Those are the regimes where you really need the flexibility of these wide and deep architectures.",
            "But what about when you have small amounts of data and you know why?",
            "Is this sort of right?",
            "Inductive bias, right?",
            "So so I guess, so if you repeat the question for the purpose of the camera here, 'cause you probably didn't hear or summarize it.",
            "So first of all, what's so special about?",
            "You know multiple layers of sigmoids and weighted sums.",
            "Why?",
            "Why is there?",
            "Is there any reason to believe that that's a kind of a good model for sort of AI type tasks and and I am not advocating that this is necessarily the best solution to all the world problems.",
            "OK, now there is something slightly special about it though, which is that?",
            "You can't use stochastic gradient descent, for example, or grid and sent in general.",
            "Good is running if the layers you have are all obvious, for example.",
            "So if you have a bunch of carbs and you stack multiple layers of carbs, you can train it.",
            "There's just so many local minima that are really awful, 'cause RBF is a really good local minimum and so it just makes 1 right there.",
            "Every RBF basically makes little minimums if you want, and so that sort of kills the entire the entire thing.",
            "It's not the size, right?",
            "It's just not going to get any whereas.",
            "Sigmoids you know they kind of have planes so.",
            "It's much more difficult for them to build a local minimum, 'cause again you have to build a box out of multiple planks and you only have planks and you can really build a box with that.",
            "So I think there is.",
            "I don't think there is something like magical about them, but I think there is something that sort of non deliberately kind of useful or or favorable.",
            "Then you had a second party question which I don't remember.",
            "Now that OK.",
            "So I don't think it's an issue of large and small, so I mean, I don't think this particular type of architecture is particularly interesting for large data set versus small datasets.",
            "One thing is that VM's tend to work better for for small datasets because they're just better regularize.",
            "This much easier to regularise SVM, then say, allotment right and logical net has just more parameters to talk with, so it's kinda.",
            "It's harder it's not going to work as well on small datasets, so one of the reasons why we organize this deep.",
            "The workshop was that satellite session is that unsupervised training is a way to kind of effectively reduce the amount of free parameters in the system with unlabeled data and therefore then be able to train being able to train the system with fewer labeled data.",
            "And I really thank you to ask that question 'cause it's kind of useful point to make.",
            "Is it all in argument against don't know what you mean.",
            "Sparsity in the parameters or in the representations or in the parameters?",
            "Well, actually, you know pretty much all the neural net we train.",
            "We all use L1 regularization.",
            "It always basically helps eliminate the weights you don't need and you know they just go to zero.",
            "They don't know you.",
            "And if you're smart about how you implement it, you can actually completely take them out of the system to run it fast.",
            "So we do have one regularization all the time.",
            "Absolutely.",
            "I mean there's nothing.",
            "I mean, there are people who show that you know you take the first layer of an SVM with Gaussian guessing kernels, plug on top a logistic regression with one organization on the parameters, and you get pretty much the same performance as the real SVM with the hinge loss.",
            "Problems where the the output has has a clear structure and you need to explain OK, so this is slide.",
            "Actually wanted to put in my talk, which I didn't.",
            "So in fact all those you know there's a slide beginning my talk that talks about handwriting and speech recognition, right?",
            "And those are all.",
            "Structured output learning.",
            "Basically, they're all exactly the same as what people in this community called structured output models.",
            "In terms of, I mean, they're actually considerably more complex because the user is nonlinear.",
            "Prioritization of the energy, but.",
            "So I wrote a tutorial energy based model last year.",
            "I give it to you at least last year on this subject, which sort of tries to present all of those different models and sort of their relation to each other relationship to each other this week, like CRF, like something called Transformer Networks.",
            "You know, Markov models, factor graphs in general.",
            "Or not, and and you know structure model outputs maximum margin Markov Nets and things of that type, and then you can all view view them as sort of different size of a kind of common team.",
            "But again the the stuff that people were doing in speech or handwriting are essentially no.",
            "Actually this is based on the same ideas as a structured output models.",
            "They just use more complex probabilistic log probabilities basically.",
            "Methods.",
            "Really.",
            "Again, you can see there's a guarantee, but the generalization error this guarantee, But the problem with optimization time so people are people are just nervous.",
            "You know that if you go to these numbers method, you know we can't really say.",
            "You tell us that with the with the big hidden layer it's going to converge quickly, but you're saying trust us, but is it hard to say like you know to what extent?",
            "You know we quantified so we can trust them.",
            "So I don't say I'm not saying trust us, I said trust your eyes OK, try it out, see if it works better for you or not, and whether you have proof or not.",
            "So if you have a proof you about on generalization, so the bounds on generalization, I really nothing you can use for practical purpose right?",
            "'cause all those bounds are completely out to lunch in terms of you plug numbers into them, and you say you know it tells you you know.",
            "You know, if you have more than you know 35 zillion examples, then the probability of error is less than one OK.",
            "So I mean, they're really interesting conceptually, but in terms of sort of practical consequences, they absolutely have no no bearing on reality.",
            "And that's because there, you know most of them are derived from sort of worst case analysis, and so you know every reasonable learning algorithm that has with an architecture that has a finite dimension will have bounds there all be bad.",
            "OK, just SVM has terrible bounds, and other models have outrageously terrible bounds.",
            "And and so you know, I don't care, you know.",
            "In reality the the actual performance of the system is going to be whatever is going to be, and you can measure that with you know validation or test error or whatever you want and so just try it out.",
            "If it works better, it works better for you.",
            "Same thing for convergence of course with you can prove that you know QP is you know ordering cube, whatever.",
            "But use for SVM, but in practice the algorithms that work the best, other ones that they own, for example proposed, which are which are much, much faster aseptically their slower.",
            "You can't prove anything about them or not much, but in practice them faster and something for four.",
            "For now."
        ],
        [
            "Let's do this chart that I showed here this one.",
            "OK again this is, you know this is this is the one with the bounds.",
            "This is the one without the bounds.",
            "Um?",
            "I mean, I don't know.",
            "I don't know about you, but you know I see trust like this.",
            "You know I pick the one with other bands.",
            "So you know you have to know what the relevance of the theoretical work you know, whether the the hypothesis are relevant and it's not because we don't have theoretical results that they don't exist.",
            "It's just that or mathematical tools haven't been powerful enough for exploited enough to really produce them.",
            "So don't be afraid.",
            "I mean, that's the the message of this thing, you know.",
            "It's not because you don't have a theory that it's not going to work.",
            "Thank you speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as usual when I give a nips to workshop talk, I'm sure I'm going to make some new friends here.",
                    "label": 0
                },
                {
                    "sent": "OK, who is afraid of non convex functions?",
                    "label": 0
                },
                {
                    "sent": "Who is afraid of non convex loss function?",
                    "label": 0
                },
                {
                    "sent": "OK well please your Highness, I really appreciate that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to try to kind of mystify this.",
                    "label": 0
                },
                {
                    "sent": "Try to make you less afraid of convex functions.",
                    "label": 0
                },
                {
                    "sent": "Convex mondex.",
                    "label": 0
                },
                {
                    "sent": "This community has suffered of a kind of acute convexa rightist epidemic in the last 10 years, would say.",
                    "label": 0
                },
                {
                    "sent": "If you look at the new models that have come up from this Community, things like you know CRF.",
                    "label": 0
                },
                {
                    "sent": "Is the Bayes many for learning or various other methods, QPS, kernel methods makes margin Markov Nets, exponential family, graphical models?",
                    "label": 0
                },
                {
                    "sent": "All of those have a common characteristic, which is that whatever function you manipulates either for inference for learning is essentially linear prioritization.",
                    "label": 0
                },
                {
                    "sent": "Linearizing the parameters between projects and then you have some simple quadratic or something like this on top of it, with some constraints sometimes.",
                    "label": 0
                },
                {
                    "sent": "And you know the machine learning community seems to have trouble moving beyond simple things like this, such as logistic regression and generalizations thereof, VM differential family, graphical models.",
                    "label": 1
                },
                {
                    "sent": "And there are good theoretical reasons for this is because you know, we can.",
                    "label": 0
                },
                {
                    "sent": "We can do theory for those models and it's very difficult to do theory for other types of models have non convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "You can't prove convergence, you can you know.",
                    "label": 0
                },
                {
                    "sent": "So you can get scared right.",
                    "label": 0
                },
                {
                    "sent": "On the other hand you get scared.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like.",
                    "label": 0
                },
                {
                    "sent": "You know you move to certain countries where.",
                    "label": 0
                },
                {
                    "sent": "You know you can get.",
                    "label": 0
                },
                {
                    "sent": "You can get scared of sort of new things you can do, and maybe you are going to say for in other countries when you have fewer opportunities and.",
                    "label": 0
                },
                {
                    "sent": "You know, I've tried to turn into Jeff Hinton now.",
                    "label": 0
                },
                {
                    "sent": "Doing sort of bad jokes that sort of.",
                    "label": 0
                },
                {
                    "sent": "Anyway, let's do that.",
                    "label": 1
                },
                {
                    "sent": "So, you know convexity is sometimes a virtue, but I sort of see it more like a more likely mutation because there's a whole world out there of sort of non convex models that give you a lot more flexibility in what you can do.",
                    "label": 0
                },
                {
                    "sent": "And of course you know don't know proofs perhaps, or perhaps that proves the weaker, but on the other hand it's about more fun.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a similar thing that's happening to machine learning that has been happening to control theory, which is control theory really hasn't moved very much beyond legal assistance in terms of the real theory.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a lot of practical things that are not linear, but much of the theories about linear systems.",
                    "label": 0
                },
                {
                    "sent": "So if you have nonlinear system, is the first thing you're trying to do is linearize it first.",
                    "label": 0
                },
                {
                    "sent": "Right, so one problem though is that we insist on convexity, usually for efficiency reasons, but very often the price we pay for this is actually a non variable increase in the complexity of the model.",
                    "label": 1
                },
                {
                    "sent": "We have to use if we wanted to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So a lot of optimization algorithms that people use for kernel methods or or SDP based optimizations.",
                    "label": 0
                },
                {
                    "sent": "Are you ordering square or in cube, whether N is the dimension of the input or the number of training samples.",
                    "label": 0
                },
                {
                    "sent": "And if you went to the over tutorial on Monday, you will probably realize pretty quickly that that is going to become an acceptable as we get more and more data then it's going to stream and you can't have algorithms that take longer to treat the data at.",
                    "label": 0
                },
                {
                    "sent": "The data comes at you.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you're going to have to drop data on the floor.",
                    "label": 0
                },
                {
                    "sent": "So we need algorithms that are all ran.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that means pretty much all the sophisticated ones are out the window.",
                    "label": 0
                },
                {
                    "sent": "And you did use very simple ones.",
                    "label": 0
                },
                {
                    "sent": "But if you use super learning algorithms, you might want you might need to compensate that by sort of increased complexity of the type of model you want to use.",
                    "label": 0
                },
                {
                    "sent": "So other communities aren't as afraid as as we are among convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So there are communities that are you could see as sort of application areas of machine learning, but in fact there's sort of pre existing machine learning and you know where machine learning services reinventing a lot of the techniques that have been invented in those communities and simplifying them to make them convex.",
                    "label": 0
                },
                {
                    "sent": "So for example, what we call a CRF in this community.",
                    "label": 0
                },
                {
                    "sent": "Has been used for about 15 years in handwriting recognition and speech recognition, before even CRF existed.",
                    "label": 0
                },
                {
                    "sent": "And essentially the same set of techniques discriminative training for you know you want to make the score of the correct answer better and make the score of the other answers worse so that your Instagram is going to find the best answer.",
                    "label": 0
                },
                {
                    "sent": "And you know, most handwriting recognition systems are trained that way, and pretty much every single speech recognition system is trained that way, and every single one of them is nonconvex.",
                    "label": 1
                },
                {
                    "sent": "Every single one of them.",
                    "label": 0
                },
                {
                    "sent": "Why is that is because convex doesn't work?",
                    "label": 0
                },
                {
                    "sent": "You can't do speech recognition with a nonconvex model.",
                    "label": 1
                },
                {
                    "sent": "You have to use something like, say, mixture of gas and models, and it's very highly non convex.",
                    "label": 0
                },
                {
                    "sent": "In fact, the file is not configured to use them and you have to use K means to initialize it, and there's all kinds of problems associated with this.",
                    "label": 0
                },
                {
                    "sent": "But you don't have a choice.",
                    "label": 0
                },
                {
                    "sent": "You have to use this on convex models.",
                    "label": 0
                },
                {
                    "sent": "They're the ones that work.",
                    "label": 0
                },
                {
                    "sent": "No one has really been successful at beating them.",
                    "label": 0
                },
                {
                    "sent": "Beating gas mixture model for handwriting for speech recognition with anything else.",
                    "label": 0
                },
                {
                    "sent": "Certainly not with convex models.",
                    "label": 0
                },
                {
                    "sent": "So this is not by choice, but it's.",
                    "label": 0
                },
                {
                    "sent": "You know it's a whole world out there and we should really, you know, concentrate our efforts on.",
                    "label": 0
                },
                {
                    "sent": "You know, not trying, perhaps to, you know, get the latest version of kind of a convex simplified convex version of something else we were doing before, but perhaps increase the power complexity of the things.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to do to solve those problems.",
                    "label": 0
                },
                {
                    "sent": "So one of the type of problems.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of an argument that you should do and I have been sort of advocating for the last year or so.",
                    "label": 0
                },
                {
                    "sent": "We wrote it kind of a review paper on this this year of scheduling algorithms towards the eye, which was actually the result of another of those workshops that was organized by.",
                    "label": 0
                },
                {
                    "sent": "Delete the cost and #2 a couple years ago and.",
                    "label": 0
                },
                {
                    "sent": "Going to sort of paraphrase the deep learning satellite session that took place on Thursday afternoon at the main conference.",
                    "label": 0
                },
                {
                    "sent": "Ultimately complex running tasks such as a vision, speech or language processing, natural language processing will be implemented with what you could.",
                    "label": 0
                },
                {
                    "sent": "You could call deep hierarchical systems, so systems that involve lots of nonlinear operations in them because complex decisions involve lots of nonlinear operations.",
                    "label": 0
                },
                {
                    "sent": "If you don't agree with this, just just hold on for a second.",
                    "label": 0
                },
                {
                    "sent": "So to learn how hierarchical representations you need to learn low level features, medium level representations of features and high level concepts and between all of those you have some sort of nonlinear transformation that takes place.",
                    "label": 0
                },
                {
                    "sent": "So if you want to learn all the stages in the system, so I'm intentionally, you need some sort of necessary.",
                    "label": 0
                },
                {
                    "sent": "You're going to come up with going to solve some sort of nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "Um not interesting remark.",
                    "label": 0
                },
                {
                    "sent": "You can say there is that.",
                    "label": 0
                },
                {
                    "sent": "But animal learning biological learning certainly does not minimize the convex function.",
                    "label": 0
                },
                {
                    "sent": "And the reason it doesn't is because, or any sort of evidence, that it doesn't, is the fact that the order in which we learn things actually matters.",
                    "label": 0
                },
                {
                    "sent": "If we were to measure convex loss function in the order in which we learn things would not matter.",
                    "label": 0
                },
                {
                    "sent": "We will always get to the same minimum, right?",
                    "label": 0
                },
                {
                    "sent": "But in fact, the order in which we learn things does matter.",
                    "label": 0
                },
                {
                    "sent": "That means you know we can reshaping or loss function in such a way that we get to the right minimum eventually.",
                    "label": 0
                },
                {
                    "sent": "Your question.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, but wait, but we have theorems that the kernel machines can approximate anything we want or even the two layer neural Nets for that matter can approximate anything we want it did we do, but those theorems say absolutely nothing about the efficiency of the representation with which the efficiency with which we can represent those.",
                    "label": 0
                },
                {
                    "sent": "You know any function with those things.",
                    "label": 0
                },
                {
                    "sent": "And for example, there are there is empirical and theoretical evidence that shadow architecture is.",
                    "label": 1
                },
                {
                    "sent": "So I cannot really implement things like invariant visual recognition efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to just flash a couple examples to you, so these are sort of, you know, some of the latest results on this.",
                    "label": 0
                },
                {
                    "sent": "Actually, I forgot a few here.",
                    "label": 0
                },
                {
                    "sent": "And you know some of those models are deep, some of them are shadow.",
                    "label": 0
                },
                {
                    "sent": "But you know, I'm not expecting you to kind of look at all of this, but basically the one that works best are the deep ones.",
                    "label": 0
                },
                {
                    "sent": "Not only do they work best, they actually train faster and they're faster to run at the end.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to show you a slightly more specific example of that in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here is the slightly more example of that, and I showed this example a couple years ago as well, but let me repeat it here.",
                    "label": 0
                },
                {
                    "sent": "So let's say you want to recognize objects.",
                    "label": 0
                },
                {
                    "sent": "You have those images that are roughly about two pixels.",
                    "label": 0
                },
                {
                    "sent": "You actually get two of them, 'cause if stereo images plug them into a linear classifier, you get 30% error on the test set.",
                    "label": 0
                },
                {
                    "sent": "OK, where the test set.",
                    "label": 0
                },
                {
                    "sent": "So this is the training set is here.",
                    "label": 0
                },
                {
                    "sent": "The test set is there for each of those images.",
                    "label": 0
                },
                {
                    "sent": "You have multiple images of each each object, so it's kind of invariant recognition problem, right?",
                    "label": 0
                },
                {
                    "sent": "Also 30% for linear classifier 18 between 16 and 18% for Kenny's neighbor, something like 11% for SVM 500% conversion.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's an unfair comparison.",
                    "label": 0
                },
                {
                    "sent": "Of course the SVM has no idea where you know it's looking at images.",
                    "label": 0
                },
                {
                    "sent": "But still, the difference here is between sort of shadow and deep.",
                    "label": 0
                },
                {
                    "sent": "There all the differences as well.",
                    "label": 0
                },
                {
                    "sent": "Of course you can always learn the kernel, but then you know if you're on the kernel you again you have a deep deep system.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It gets worse if you look at the time it takes to train the system.",
                    "label": 0
                },
                {
                    "sent": "So they shouldn't go in front of the screen, so maybe I need a pointer.",
                    "label": 0
                },
                {
                    "sent": "If anyone is appointed OK, so it gets worse.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the time it takes to train the support vector machine to get to 11.6%, it takes 480 minutes on a sort of virtual one giga Hertz machine.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a long time to get the same accuracy with the commercial net you need 64.",
                    "label": 0
                },
                {
                    "sent": "Minute giga Hertz.",
                    "label": 0
                },
                {
                    "sent": "The same, the same data, so it's basically 8 times faster.",
                    "label": 0
                },
                {
                    "sent": "OK, no proofs of convergence snowbound.",
                    "label": 0
                },
                {
                    "sent": "Know you know where there are bounds, but there's sort of very general bounds.",
                    "label": 0
                },
                {
                    "sent": "Nonconvexity certainly, but it's faster, it works better, and if you sort of keep training for the same length of time at support vector machine, you get half the error rate.",
                    "label": 0
                },
                {
                    "sent": "The test time is even worse, it's about 30 times faster to test the commercial net SVM.",
                    "label": 0
                },
                {
                    "sent": "It's considerably smaller.",
                    "label": 0
                },
                {
                    "sent": "OK, so changing the architecture makes a huge difference, and if you have to throw away convexity for this, fine.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It gets even.",
                    "label": 0
                },
                {
                    "sent": "Worser.",
                    "label": 0
                },
                {
                    "sent": "If you take those images that are sort of cluttered versions of the previous ones, training within ATM with Gaussian kernel, we get 43% error.",
                    "label": 0
                },
                {
                    "sent": "Basically it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "You know pretty much older points are super vectors.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not clear that the there was any convergence to speak up here, but essentially the best implementation we had at the time of supervision machine, which was the one at a venue.",
                    "label": 0
                },
                {
                    "sent": "See actually it wasn't one of those implementation, it was on speaker graphene.",
                    "label": 0
                },
                {
                    "sent": "There 'cause that was implementation on parallel machine you couldn't get beyond that level.",
                    "label": 0
                },
                {
                    "sent": "Commercial net 5.9% error.",
                    "label": 0
                },
                {
                    "sent": "That's actually when the amount of the sort of rock.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is a little less than 8%.",
                    "label": 0
                },
                {
                    "sent": "And then the train time are even worse than test.",
                    "label": 0
                },
                {
                    "sent": "I'm even worse.",
                    "label": 0
                },
                {
                    "sent": "VM took it actually 10,000 minutes seconds to give us the wrong answer, and in about half the time the conversion negative 7.2.",
                    "label": 0
                },
                {
                    "sent": "So enormous differences and with the non convex you know.",
                    "label": 0
                },
                {
                    "sent": "Not particularly well justified theoretically.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd so you gotta be careful with.",
                    "label": 0
                },
                {
                    "sent": "You know, with things like that.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so one of the things that that has become pretty.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the last few years, including for things like SVM and Search, is that people have been writing tons of papers on various optimization algorithms.",
                    "label": 0
                },
                {
                    "sent": "For SVM, you know, in the in the neural Nets and various other learning machine CRF.",
                    "label": 0
                },
                {
                    "sent": "But we came out of.",
                    "label": 0
                },
                {
                    "sent": "You know you need to show those work and lose work and some of some work in the in the past is basically simple.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient descent beats everything by a factor of 10 or 100 or sometimes 1000.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of annoying because you come up with all those really sophisticated optimization methods that get you a paper into NIPS.",
                    "label": 0
                },
                {
                    "sent": "And if you compare it to the right method, you don't get a paper internets, but you might as well not compare it to the right methods.",
                    "label": 0
                },
                {
                    "sent": "So in fact, I kind of wasted an entire year of my life when I was in Toronto as a post Doc actually implementing those optimization algorithms, limited storage BS, and BGS.",
                    "label": 0
                },
                {
                    "sent": "And you know all kinds of crazy Newton method conjugate gradients comparing them with stochastic gradient and after six months with this time wasted time, I figured you know just you stochastic gradient publish short paper that you read and then we're talking about life and we can have we discovered this and get some data on you learning algorithms to kind of convince people that that's the case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So one problem is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me see what you want to hear.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are some arguments for why why?",
                    "label": 0
                },
                {
                    "sent": "Deep architectures are more efficient, but let me let me skip this.",
                    "label": 0
                },
                {
                    "sent": "I don't want to kind of, you know.",
                    "label": 0
                },
                {
                    "sent": "Hammer the head really too far down the in the blank so OK, so there are several strategies that people can sort of use machine learning nowadays to kind of try to attack the sort of AI problem.",
                    "label": 0
                },
                {
                    "sent": "Or can I solve a sort of general problems?",
                    "label": 0
                },
                {
                    "sent": "This is sort of philosophical and I'm sort of recycling an argument that Geoff Hinton's returning orders review papers.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of slightly different, but it's kind of paraphrasing what he said.",
                    "label": 0
                },
                {
                    "sent": "So what I'm possible attitude is defeatism, since there is no good parametrization for kind of really complex functions.",
                    "label": 1
                },
                {
                    "sent": "You know we should stick to kind of building or feature set by hand and stick a convex.",
                    "label": 0
                },
                {
                    "sent": "Classifier on top and for each new problem will just find a new set of features or you kernel and you know the be happy with that.",
                    "label": 0
                },
                {
                    "sent": "I will keep our jobs.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The second attitude is denial.",
                    "label": 0
                },
                {
                    "sent": "Kernel machines can approximate anything we want, and the VC bounds guaranteed realization where we need.",
                    "label": 0
                },
                {
                    "sent": "Why would we need anything else?",
                    "label": 0
                },
                {
                    "sent": "But of course, you know there's all those arguments that kernel machines cannot really represent certain classes of functions efficiently, and the examples I showed you with object recognition is kind of evidence to that.",
                    "label": 1
                },
                {
                    "sent": "Essentially, that's because supervector machines with gas and kernels is essentially glorified.",
                    "label": 0
                },
                {
                    "sent": "Template matching is basically, you know, you sort of.",
                    "label": 0
                },
                {
                    "sent": "Do a template comparison between the training samples and the input pattern.",
                    "label": 0
                },
                {
                    "sent": "Then you record all the scores of similarities using the Gaussian or whatever color you want to use and then you combine those with a linear combination.",
                    "label": 0
                },
                {
                    "sent": "This template matching basically.",
                    "label": 0
                },
                {
                    "sent": "So how can you do invariant recognition Gagnon concepts with template matching?",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Now you can do a lot of this complex with template matching.",
                    "label": 0
                },
                {
                    "sent": "There's absolutely nothing wrong using this if it's the best for your application.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know I'm not saying and I use it for all kinds of applications as well, it's just that issues.",
                    "label": 0
                },
                {
                    "sent": "If you want to solve really complex.",
                    "label": 0
                },
                {
                    "sent": "Problems I type problems this is not going to work.",
                    "label": 0
                },
                {
                    "sent": "This is kind of like you know.",
                    "label": 0
                },
                {
                    "sent": "You know, going to make friends again, climbing a ladder to get to reach the moon or something.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so the last option of course is optimism.",
                    "label": 1
                },
                {
                    "sent": "Let's look for some learning models that can be applied to the largest possible subset of the I set, and we're requiring requiring this most amount of specific knowledge for each task.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's what some of us after.",
                    "label": 0
                },
                {
                    "sent": "OK, So what about what about deep learning?",
                    "label": 0
                },
                {
                    "sent": "Let's go a little bit into that.",
                    "label": 0
                },
                {
                    "sent": "Now you know it used to be that a lot of people in this community were obsessed with training neural Nets, and there were a lot of papers on this and it sort of died out for various reasons.",
                    "label": 0
                },
                {
                    "sent": "Didn't quite die out, but you know, there are different fashions, and one of the reasons was because there was this perception that because the optimization and convex you don't understand everything there is to understand about it.",
                    "label": 0
                },
                {
                    "sent": "And perhaps sometimes it fails and you don't understand why.",
                    "label": 0
                },
                {
                    "sent": "So let's take a very simple example.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have the simplest possible modular neural net you can imagine.",
                    "label": 0
                },
                {
                    "sent": "It's got one input when heating unit and one output OK. Are you going to train it to compute the identity function right?",
                    "label": 0
                },
                {
                    "sent": "How could it be OK?",
                    "label": 0
                },
                {
                    "sent": "So it's got only two ways, one from the input unit to the hidden unit and one from the unit to the output unit?",
                    "label": 0
                },
                {
                    "sent": "And to learn the identity function, let's assume the hidden is linear.",
                    "label": 1
                },
                {
                    "sent": "To learn the identity function, it needs to set one of the weights to some value and the other weights to one over that value, and then you get the identity function right?",
                    "label": 0
                },
                {
                    "sent": "So if you look at.",
                    "label": 0
                },
                {
                    "sent": "So the loss function, which is the square difference between the output of the network and what you want.",
                    "label": 0
                },
                {
                    "sent": "As a function of those two weights, it looks something like this, so this is actually with the sigmoid, but makes make much difference.",
                    "label": 0
                },
                {
                    "sent": "So the solution space.",
                    "label": 0
                },
                {
                    "sent": "The best solutions are those kind of little is this sort of hyperbola here.",
                    "label": 0
                },
                {
                    "sent": "And on the other side right?",
                    "label": 0
                },
                {
                    "sent": "You can have either X and 1 / X where both positive or excellent 1 / X when both are negative.",
                    "label": 0
                },
                {
                    "sent": "And then everywhere else is kind of high.",
                    "label": 0
                },
                {
                    "sent": "High surface and it's got a central point at 00.",
                    "label": 0
                },
                {
                    "sent": "So 00 you know if you start from zero, 0 is not going to move anywhere.",
                    "label": 0
                },
                {
                    "sent": "Close the gradient to 0 so you have to kind of be a little bit on the side and then it's going to fall into one of the local minima.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have to picture this in high dimension.",
                    "label": 0
                },
                {
                    "sent": "If you have a high dimensional neural net you know with lots of lots of units.",
                    "label": 0
                },
                {
                    "sent": "Let's say only two layers, you're going to have lots of several points in all kinds of dimensions, and that means in a lot of dimensions the loss function is going to be convex.",
                    "label": 0
                },
                {
                    "sent": "Another dimension is going to be concave, and you're going to have several points where they need.",
                    "label": 1
                },
                {
                    "sent": "OK, so now if the neural net is has just the right.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Size to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "Is just the right size.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a very, very small number of those dimensions where you can find a solution, right?",
                    "label": 0
                },
                {
                    "sent": "Compared to all the size of all possible dimensions in white space is going to be just a very small number of dimensions that where the error goes down and you have a solution through space.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be very hard to find.",
                    "label": 0
                },
                {
                    "sent": "But if you make the network really large, then most of the dimensions actually are solutions.",
                    "label": 0
                },
                {
                    "sent": "Most of the quadrants.",
                    "label": 0
                },
                {
                    "sent": "If you want our solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's going to be very easy to find.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what that means is that this is also intuitively under intuitive, easy to understand intuitively with other arguments.",
                    "label": 0
                },
                {
                    "sent": "If you have a Twitter net with very very very large hidden layer is going to be very easy to train it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take kind of limit case.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have a neural net where the hidden layer is essentially infinite, extremely large OK and you should have done some theoretical work on this.",
                    "label": 0
                },
                {
                    "sent": "But let's assume it's so large, in fact, that pretty much all configuration of possible weights that you will ever need for the first layer.",
                    "label": 0
                },
                {
                    "sent": "Or we did there.",
                    "label": 0
                },
                {
                    "sent": "So the loss function is non convex as a function of those weights.",
                    "label": 0
                },
                {
                    "sent": "Of the weights of the Internet, OK?",
                    "label": 0
                },
                {
                    "sent": "But the point is, you never need to learn the first layer because the weights are already there.",
                    "label": 0
                },
                {
                    "sent": "So the only way that you need to learn other other weights in the top layer you will be able to learn any function, but just adapting the weights in the in the in the second layer.",
                    "label": 0
                },
                {
                    "sent": "Because in the first you already have them.",
                    "label": 0
                },
                {
                    "sent": "OK, so many of them that you can just pick the Windows correct by setting the outgoing weight of it to some positive or negative value to another value and you're done.",
                    "label": 0
                },
                {
                    "sent": "So here is a situation where even though the last function technically is nonconvex as a function of all the parameters, in practice, because of the architecture, the learning is actually very simple and in the limit actually convex, because because you made the system so large that essentially there's kind of a convex subspace in which you will find the solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course there are sort of two limit cases, But what that means is that the larger the network is going to be, the easier is going to train, so this is going to counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "You would think that if you make the network very large, that's going to be also local minima, but in fact local minima are good for you.",
                    "label": 0
                },
                {
                    "sent": "They make it easier to find to find a good minimum, because all of those local minima actually more or less equivalent.",
                    "label": 0
                },
                {
                    "sent": "So another picture of this is.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine you have a loss function in one dimension that has a local minimum, right, and then a mountain, and then another a global minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, in one dimension you can go through the mountain to go to this other minimum.",
                    "label": 0
                },
                {
                    "sent": "You have to kind of go up so a great in bed will not find it.",
                    "label": 0
                },
                {
                    "sent": "But now let's add a dimension.",
                    "label": 0
                },
                {
                    "sent": "You're going to be able to go around the mountain, so in that picture of this, in a million dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you will do that.",
                    "label": 0
                },
                {
                    "sent": "Then there's going to be lots and lots of dimensions that will go allow you to get around local minima, and in fact, depending on the kind of basic elements that you assemble in the network, it's very difficult to build a box, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have a million dimensions to build a box in a million dimensions, you need a hell of a lot of planks.",
                    "label": 0
                },
                {
                    "sent": "Each neuron basically is a Planck 'cause it doesn't have space, so it could be you know, hardly complicated to do that.",
                    "label": 0
                },
                {
                    "sent": "Right, so you have flat areas of course, because if the if the weights become very large the signals get saturated and then you're so the ways to compensate for this is just not to get there.",
                    "label": 0
                },
                {
                    "sent": "You can do this with decay, so there's a few tricks like this, so you have to run the same way.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of tricks if you want to make large viens efficient.",
                    "label": 0
                },
                {
                    "sent": "This entire workshop is about this.",
                    "label": 0
                },
                {
                    "sent": "So which means that means there are tricks, right?",
                    "label": 0
                },
                {
                    "sent": "Otherwise why are we here?",
                    "label": 0
                },
                {
                    "sent": "So yes, there are tricks, but they're not any more difficult than any other instances.",
                    "label": 0
                },
                {
                    "sent": "So one problem, of course, is that if you're sort of starting to play with things like this, the first thing to try is you know a tiny neural net on the exclusive or, and it happens to be very unreliable.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this idea of so.",
                    "label": 0
                },
                {
                    "sent": "Basically in 1957 people sort of figured out with the perception work that you know they only knew about convex optimization back then.",
                    "label": 0
                },
                {
                    "sent": "So what they figured is that they only had to make this layer really large, with random basically random functions, random features, and you know there's a good chance there probably will be solved that was 1957.",
                    "label": 0
                },
                {
                    "sent": "I had five, you know with back prop.",
                    "label": 0
                },
                {
                    "sent": "People figured out women.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you could run this.",
                    "label": 0
                },
                {
                    "sent": "It sort of works in a lot of cases, but there are situations where it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "1982 The kernel machine.",
                    "label": 0
                },
                {
                    "sent": "Colonel Machine sort of came out of the.",
                    "label": 0
                },
                {
                    "sent": "Of the Bush is an said.",
                    "label": 0
                },
                {
                    "sent": "Well if I set if I have 1 hidden unit for each training sample.",
                    "label": 0
                },
                {
                    "sent": "Yeah :) next to me.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Actually, the three offices next to me.",
                    "label": 0
                },
                {
                    "sent": "Poser.",
                    "label": 0
                },
                {
                    "sent": "Is a big beyond value of ethnic, but anyway.",
                    "label": 0
                },
                {
                    "sent": "If you if you said each hidden unit 2 training sample, different training samples, you said the weights of each unit to a different training sample that basically you built a ramp for your training set.",
                    "label": 0
                },
                {
                    "sent": "OK, that's like a decoder for random access memory, right?",
                    "label": 0
                },
                {
                    "sent": "You plug you plug it in, put and then one of those units is going on because it's going to recognize its corresponding training sample.",
                    "label": 0
                },
                {
                    "sent": "And so you just said the way to the output you want, and you're done.",
                    "label": 0
                },
                {
                    "sent": "It's a random access memory, so you make the kernel smooth and then you can get kind of a smooth bottom access memory and that support vector machine.",
                    "label": 0
                },
                {
                    "sent": "River restaurant basically.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of, you know, it's not beyond the perceptron.",
                    "label": 0
                },
                {
                    "sent": "In fact, there was several papers at the conference, at least when paper at the conference.",
                    "label": 0
                },
                {
                    "sent": "That was sort of arguing for random first layers.",
                    "label": 0
                },
                {
                    "sent": "This is sort of drawing from theoretical results from compressed sensing which are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really interesting mathematically, but.",
                    "label": 0
                },
                {
                    "sent": "So what's the problem with convex learning?",
                    "label": 0
                },
                {
                    "sent": "So one thing I mentioned before is that none of what you read in the optimization literature applies.",
                    "label": 0
                },
                {
                    "sent": "Encourage you to see over tutorial either the slides or the video, which I hope will be will become available and like the ones last year and the reason is you need to use stochastic methods to take advantage of redundancy in the data.",
                    "label": 0
                },
                {
                    "sent": "So sarcastic methods are the methods where basically you did the parameters after each sample instead of kind of computing and updates over the entire training set.",
                    "label": 0
                },
                {
                    "sent": "OK, now that exploits the redundancy in the data, so you have very large datasets, particularly their streaming.",
                    "label": 1
                },
                {
                    "sent": "That's really what you want to use and produce theoretical results on this few years ago that showed that we talked about this tutorial that shows that.",
                    "label": 0
                },
                {
                    "sent": "Even if this does not optimize the loss function on the training set perfectly, this is as good as it's going to get on the test set anyway, so you don't need to optimize more than that.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that so of course stochastic methods of horrible asymptotics, essentrics properties, and so if you tell an optimization person I'm going to use this to castigate algorithm says first there is a convergence proof, although they are, but they are kind of weird.",
                    "label": 0
                },
                {
                    "sent": "And second, you know the convergence property are you know linear or or even sub linear is really horrible.",
                    "label": 0
                },
                {
                    "sent": "OK but they will show that actually works really well.",
                    "label": 0
                },
                {
                    "sent": "I mean, many people showed that it was really well, but let me explain why basically.",
                    "label": 0
                },
                {
                    "sent": "And the bad news, of course, is that the optimization optimization literature does not talk about stochastic methods at all.",
                    "label": 0
                },
                {
                    "sent": "You can't find anything about this anywhere, and so you know configuration information, storage BGS, BGS, quasi Newton.",
                    "label": 0
                },
                {
                    "sent": "That's all out the window.",
                    "label": 0
                },
                {
                    "sent": "You can't use any of that and many of those methods are ordering squared anyway.",
                    "label": 0
                },
                {
                    "sent": "Ordering cube so you know when you send.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you start with simple methods.",
                    "label": 0
                },
                {
                    "sent": "You can publish papers on complex methods because you know the simple methods are the ones that work best.",
                    "label": 0
                },
                {
                    "sent": "How disappointing?",
                    "label": 0
                },
                {
                    "sent": "So well, this is kind of the same way that I made earlier, right?",
                    "label": 0
                },
                {
                    "sent": "So so so this is kind of the point.",
                    "label": 0
                },
                {
                    "sent": "That sort of making the system very large, sort of basically sort of, you know, using the trick that SVM taught us.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a good way of avoiding the problem of local minima really being a big problem.",
                    "label": 0
                },
                {
                    "sent": "So another problem also is sort of breaking the symmetry in the system.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of local minima due to the fact that you can, for example, interchange too.",
                    "label": 0
                },
                {
                    "sent": "Hidden nodes in a neural net and you get the same solution and so that means there are two local minimums are equivalent due to a permutation, and that means there is a kind of a central point between them and by breaking the symmetry you sort of reducing the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the space.",
                    "label": 0
                },
                {
                    "sent": "And that might be one of the reasons why things like convolutional networks so well, because they, even though they're very, very deep.",
                    "label": 0
                },
                {
                    "sent": "That's one of the few instances of very deep architectures that work well with back prop, and one reason this perhaps because not only are they wide in terms of so one problem that you have to solve with you, unless you can't make the the hidden layer is very wide, because the number of parameters grows really quickly with the quadratically with the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "And then you get killed by over prioritization.",
                    "label": 0
                },
                {
                    "sent": "So you have to find architectures where that's not the case where you can increase the size of the number of variables that are manipulated, but not the number of parameters that are being trained.",
                    "label": 0
                },
                {
                    "sent": "And convolutional Nets is a way of doing this.",
                    "label": 0
                },
                {
                    "sent": "The other advantage of it is this.",
                    "label": 0
                },
                {
                    "sent": "You know symmetries are broken in it, so not every node is connected to the same set of variables and so naturally they tend to compute different things.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I heard this argument several times also, well, you know the problem with neural Nets is that you know you only young can make them work.",
                    "label": 0
                },
                {
                    "sent": "Jeff actually made the jokiness.",
                    "label": 0
                },
                {
                    "sent": "He doesn't believe a word, of course, but he sort of made a joke.",
                    "label": 0
                },
                {
                    "sent": "Anyway, he wants a joke in this tutorial and several people have talked about this and this is entirely not true.",
                    "label": 0
                },
                {
                    "sent": "There's actually a guy called Mike O'Neill who can also do it.",
                    "label": 0
                },
                {
                    "sent": "You guys have never heard of Michael Deal, right?",
                    "label": 0
                },
                {
                    "sent": "OK, let me show you who Mike O'Neill is OK.",
                    "label": 0
                },
                {
                    "sent": "So Michael Neal produces really nice website on codeproject.com where he basically took the papers on, you know, convolutional Nets and blah blah blah and my papers and Patricia Marx Papers and and said you know what if I re implemented this so he he implemented it and rebuild this really nice little Windows application where you can click on stuff and recognizing these digits you can download the code and then he explains you know everything how you do this.",
                    "label": 0
                },
                {
                    "sent": "How you do this and you know how this code works and blah blah blah and these pages and pages of that stuff and.",
                    "label": 0
                },
                {
                    "sent": "You know all the details and experimented with a lot of different things and implemented.",
                    "label": 0
                },
                {
                    "sent": "You know stochastic diagonal liver marquard and all that stuff.",
                    "label": 0
                },
                {
                    "sent": "All the tricks in the book by just reading the papers you know.",
                    "label": 0
                },
                {
                    "sent": "Very nice.",
                    "label": 0
                },
                {
                    "sent": "Very nicely done.",
                    "label": 0
                },
                {
                    "sent": "So this guy is YPG.",
                    "label": 0
                },
                {
                    "sent": "Don't mighty.",
                    "label": 0
                },
                {
                    "sent": "What's your guess?",
                    "label": 0
                },
                {
                    "sent": "OK, who is he?",
                    "label": 0
                },
                {
                    "sent": "This is a guy Michael need is a patent attorney in Southern California where he specializes in computer and software related patents.",
                    "label": 0
                },
                {
                    "sent": "He programs as a hobby.",
                    "label": 0
                },
                {
                    "sent": "And invented time to keep up at assigned the technology of his clients.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, so if a lawyer can do it, you know?",
                    "label": 0
                },
                {
                    "sent": "That's his little application here that works really well.",
                    "label": 0
                },
                {
                    "sent": "This is Linux, but here I'm running it under wine, which is Windows Emulator.",
                    "label": 0
                },
                {
                    "sent": "It works nicely.",
                    "label": 0
                },
                {
                    "sent": "Say again.",
                    "label": 0
                },
                {
                    "sent": "OK, so this guy did this.",
                    "label": 0
                },
                {
                    "sent": "You didn't talk to me right?",
                    "label": 0
                },
                {
                    "sent": "He just you know, read the papers did this on his own for fun and then last year at NIPS.",
                    "label": 0
                },
                {
                    "sent": "It was actually and it sent me an email saying, you know we're missing.",
                    "label": 0
                },
                {
                    "sent": "You know, what do you think you know?",
                    "label": 0
                },
                {
                    "sent": "So it's pretty cool.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "Do something.",
                    "label": 0
                },
                {
                    "sent": "There was a patent attorney this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, so I mean, you know he's very smart guy, obviously, but.",
                    "label": 0
                },
                {
                    "sent": "But but it's not like there is.",
                    "label": 0
                },
                {
                    "sent": "You know any mystery in there.",
                    "label": 0
                },
                {
                    "sent": "OK, let me skip.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "How much time are left?",
                    "label": 0
                },
                {
                    "sent": "Famous done 6 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK good.",
                    "label": 0
                },
                {
                    "sent": "Alright so I think what stops people from using non convex optimization is that they just don't have the right tools either right?",
                    "label": 0
                },
                {
                    "sent": "Conceptual tools.",
                    "label": 0
                },
                {
                    "sent": "Or the right?",
                    "label": 0
                },
                {
                    "sent": "Software tools or theoretical tools?",
                    "label": 0
                },
                {
                    "sent": "Certainly the theoretical tools you know.",
                    "label": 0
                },
                {
                    "sent": "They're kind of missing, but but the other tools aren't, so there is one really cool tool that some of us have been using for about 20 years or 15 at least.",
                    "label": 0
                },
                {
                    "sent": "Called automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "Actually a very, very simple version of automatic differentiation, so automatic differentiation is this really there's a whole community on this, mostly in sort of numerical methods where you cannot write a program in FORTRAN or C++ or whatever.",
                    "label": 0
                },
                {
                    "sent": "And automatically spits out the code that will compute the derivative of whatever function this program computes.",
                    "label": 0
                },
                {
                    "sent": "OK. And I have to say the best reference on this recent reference.",
                    "label": 0
                },
                {
                    "sent": "It's not the best reference, but the people who really understood is the best.",
                    "label": 0
                },
                {
                    "sent": "It's going to take a few years for the community to really kind of understand what they're doing, but really the people understand the best are Barack parameter Andrzejewski siskin, who is here?",
                    "label": 0
                },
                {
                    "sent": "Actually they published a paper, or they actually about to publish paper in programming languages, probably language Journal.",
                    "label": 0
                },
                {
                    "sent": "Basically they have a whole type system for for automatic differentiation using.",
                    "label": 0
                },
                {
                    "sent": "Calculus and functional programming, and they've completely worked it out and they can produce really efficient code that essentially computer into of anything.",
                    "label": 0
                },
                {
                    "sent": "So it's really cool, but one of the systems that Leon I implemented for the large system that we use for research an order but also kind of picked up for his torch system and run miscevic used for his Monty Python system is a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "It doesn't use any complicated concepts, you can implement it in almost any language, but you can't implement it in Matlab, which is why you know.",
                    "label": 0
                },
                {
                    "sent": "And the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "In fact, you could probably implement symmetric, although probably inefficiently.",
                    "label": 0
                },
                {
                    "sent": "If you have a bunch of modules that sort of compute various functions and they pass information to each other in the forms of the form of vectors or whatever for each of those modules, you have to write basically two functions to methods if you want.",
                    "label": 0
                },
                {
                    "sent": "So this could be a record class, and then you will have to member functions or two methods, one called F proper whatev does is that it takes the input and compute the output with the input by passing through the function, and one could be proud and what the problem is is that.",
                    "label": 0
                },
                {
                    "sent": "Given the gradients of whatever loss function respect to the output multi property Jacobian and compute the gradient of whatever loss function respect to the input.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple thing to write, you can write this fallen into modules you have, you know, linear combinations, softmax, you know you know, sigmoid, whatever, and then by assembling things like this.",
                    "label": 0
                },
                {
                    "sent": "I'm doing a proper proper proper writing, new classes.",
                    "label": 0
                },
                {
                    "sent": "You can essentially build any complex function automatically and you don't have to worry about actually writing the code to compute the gradient, it's automatic.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a you know object oriented permutation of chain rule, right?",
                    "label": 0
                },
                {
                    "sent": "There's nothing more than that, but it's really really simple, and it allows you to do very very simple build very complex models very simply.",
                    "label": 0
                },
                {
                    "sent": "Now the trick you can do which we called the prop, and this actually computes second derivative, so it applies kind of the same algorithm to back propagate second derivatives with those those boxes.",
                    "label": 0
                },
                {
                    "sent": "So it turns out there is a very, very simple algorithm which if you have.",
                    "label": 0
                },
                {
                    "sent": "What a piece of program that computes the back propagates derivatives through a module with a very simple transformation of the program, which is almost completely mindless.",
                    "label": 0
                },
                {
                    "sent": "You can essentially write the program that also back propagate the second derivatives, or at least the diagonal term of the Hessian through the same box.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you had the diagonal terms of Hessian of your loss function with respect to the variables coming out of this box, by applying this be prop function you will get the diagonal terms of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "So the last function in respect to those wires, or respect to the internal parameters of that system as well.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about diagonal terms of the Hessian is that you can use them to do preconditioning of gradient descent, and that tends to be really efficient, so we worked on this for a few years and look with the details of this, but.",
                    "label": 0
                },
                {
                    "sent": "We've been using this to train our neural Nets or nonlinear assistance for four years, so it's basically.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called.",
                    "label": 0
                },
                {
                    "sent": "Stochastic diagonal levenberg Marquardt and the idea is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "Basically for each parameter you have a different step size using stochastic gradient and the state size is computed as some constant which would decrease overtime divided by an estimate of the diagonal from the second derivative of the last function with respect to that particular parameter.",
                    "label": 0
                },
                {
                    "sent": "That this step size is going to be attached to, press some sort of you know.",
                    "label": 0
                },
                {
                    "sent": "So that prevents this thing from blowing up.",
                    "label": 0
                },
                {
                    "sent": "If this goes to 0 now, this estimate of the second derivative is not really the real second derivative.",
                    "label": 0
                },
                {
                    "sent": "It's more like a like a gas Newton approximation to it, so it's something that's always positive.",
                    "label": 0
                },
                {
                    "sent": "And this sort of does very, very simple preconditioning of the.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "Of the space in such a way that it doesn't cost anything.",
                    "label": 0
                },
                {
                    "sent": "Basically you can run this so few time once in awhile, because exactly the same as doing a back prop to compute those secondary estimates.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do it as often, because the second derivative don't change very, very quickly most of the time.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this goes back to my purchases in 87, so this is really old.",
                    "label": 0
                },
                {
                    "sent": "There's another problem with the sarcastic gradient, which is you have to be able to compute the best learning rate for it and Leo and I have used heuristics for this for many years.",
                    "label": 0
                },
                {
                    "sent": "The heuristic is really simple, you know you the optimal step size when you stochastic gradient does not depend on the size of the training set, so you try it on a small training set.",
                    "label": 0
                },
                {
                    "sent": "If it works fine, you sort of, you know, knock it down by a factor of two or three and then apply to large training set and then you decrease the learning rate according to the schedule and they all sort of explained how to do this.",
                    "label": 0
                },
                {
                    "sent": "This heuristic works really well, so we figured out a much better way of computing the optimal learning rate for stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We don't have proof or whether it's the appropriate thing, but experimentally works really well and I go into the details of this is based on basically stochastic estimation of the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second term of the Hessian using the power method.",
                    "label": 0
                },
                {
                    "sent": "We have very simple formula to kind of update.",
                    "label": 0
                },
                {
                    "sent": "The estimate of the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "Mr.",
                    "label": 0
                },
                {
                    "sent": "Recipe.",
                    "label": 0
                },
                {
                    "sent": "So basically the recipe is Topeka initial vector at random which has the same dimension as your parameter vector are present in pattern to your model and perform a forward problem vector product to compute the gradient Geo W. Then you add.",
                    "label": 0
                },
                {
                    "sent": "Alpha times this vector outside to the current parameter vector, so it's kind of a normalized version of this of this vector multiplied by small constant.",
                    "label": 0
                },
                {
                    "sent": "Again, you perform forward problem backward to compute the gradient for this new value G prime, and you compute the difference G prime energy.",
                    "label": 0
                },
                {
                    "sent": "That gives you an estimate of the product of the Hessian by that that vector PSI, and then you sort of.",
                    "label": 0
                },
                {
                    "sent": "Keep doing this basically so you get doing this, repeat it.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of equivalent to doing the power method or multiplying the Hessian by a random vector.",
                    "label": 0
                },
                {
                    "sent": "And vector is going to converge to the eigenvector of larger second value, which is the one that in which the curvature of the loss functions.",
                    "label": 0
                },
                {
                    "sent": "That is the highest and so.",
                    "label": 0
                },
                {
                    "sent": "Then you estimate this curvature and said the running rate to one over the curvature and basically this formula does it sort of online if you want.",
                    "label": 0
                },
                {
                    "sent": "And so that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It gives you sort of estimates of the optimal learning time, so optimal learning rate, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is what we know is the optimal learning rate.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the error you reach after a certain number of epochs for various number of epochs.",
                    "label": 0
                },
                {
                    "sent": "For vice size of the learning rate, and this is the predicted optimal value for the learning rate and exactly is that the minimum.",
                    "label": 0
                },
                {
                    "sent": "That means that it predicts the optimal learning rate exactly as.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you want.",
                    "label": 0
                },
                {
                    "sent": "It works for you, Sir.",
                    "label": 0
                },
                {
                    "sent": "Various things, so I think I'm going to stop here.",
                    "label": 0
                },
                {
                    "sent": "I'm at a time and perhaps.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some questions, thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "For the power networks, can you also think the same argument if you're trying to learn a dynamical system and and you have recurrent connections, well, OK.",
                    "label": 0
                },
                {
                    "sent": "So so recurrent networks have their own set of problems that actually yesterday quite extensively.",
                    "label": 0
                },
                {
                    "sent": "So we have Angel had some papers that showed that if you want those recurrent Nets to do something really useful, then learning is going to be intrinsically hard.",
                    "label": 0
                },
                {
                    "sent": "And so that's sort of put a really big damper on on people work on recurrent Nets, so people claim to have algorithms that work really well, but it's not clear.",
                    "label": 0
                },
                {
                    "sent": "So there are certain cases they will work well.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming those techniques necessarily apply there.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that it's important to choose the learning rate correctly?",
                    "label": 0
                },
                {
                    "sent": "And as Liam was saying earlier, tried on small training set.",
                    "label": 0
                },
                {
                    "sent": "We're saying that in addition to trying a small training set that on that small training set, set it that you use the stochastic title LM method, or do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so get signal limited.",
                    "label": 0
                },
                {
                    "sent": "Mark Rd is sort of orthogonal to choosing the right step size where it allows you to do is basically allows you to.",
                    "label": 0
                },
                {
                    "sent": "If you have suggested diagonal Livermore, but it does that, preconditions the space so it's going to essentially have the effect of.",
                    "label": 0
                },
                {
                    "sent": "Artificially increasing the learning rate in dimensions, whether the loss has low low curvature and sort of decrease it in directions with his high curvature.",
                    "label": 0
                },
                {
                    "sent": "So it's like a preconditioning.",
                    "label": 0
                },
                {
                    "sent": "But then you still have to choose the constant that will set the the step sides OK, which is kind of a global constant global multiplier for all of those.",
                    "label": 0
                },
                {
                    "sent": "All of those updates, and for that you can use the technique that.",
                    "label": 0
                },
                {
                    "sent": "How do you mentioned or this technique of stochastic estimation of it?",
                    "label": 0
                },
                {
                    "sent": "But it turns out the I don't use that very often because the heuristics heuristic way is so simple and it works well enough that it's really not worth it.",
                    "label": 0
                },
                {
                    "sent": "Published.",
                    "label": 0
                },
                {
                    "sent": "Well, depends what you're interested in so.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested in, you know.",
                    "label": 0
                },
                {
                    "sent": "Um industry application with determining, you know, probably autistic regression is going to do good job for you.",
                    "label": 0
                },
                {
                    "sent": "So just stick with the question.",
                    "label": 0
                },
                {
                    "sent": "But if what you are interested in is building intelligent machines.",
                    "label": 0
                },
                {
                    "sent": "Then please don't use social progression.",
                    "label": 0
                },
                {
                    "sent": "Find something else right so that's where you really have to think about more more sophisticated models like say deep learning.",
                    "label": 0
                },
                {
                    "sent": "Like say, unsupervised learning for kind of pretraining deep network.",
                    "label": 0
                },
                {
                    "sent": "There was a whole you know satellites session on this that some of you may have.",
                    "label": 0
                },
                {
                    "sent": "I have attended.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the best hope we have at the moment.",
                    "label": 0
                },
                {
                    "sent": "Sadly, is this pretty much the only one too.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Large network makes.",
                    "label": 0
                },
                {
                    "sent": "Right, so so asking to explain again why?",
                    "label": 0
                },
                {
                    "sent": "Like gradient descent is more reliable with large Nets.",
                    "label": 0
                },
                {
                    "sent": "Well, so that's kind of the argument.",
                    "label": 0
                },
                {
                    "sent": "I tried to make, which is that.",
                    "label": 0
                },
                {
                    "sent": "You know there's going to be a lot with a large net.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a lot more dimensions that are more directions that are the content solutions.",
                    "label": 0
                },
                {
                    "sent": "And in the limit is is the limit case where the first layer, for example of a two layer neural net is very very large, so large that all the weights are already there, so you don't need to learn them at all.",
                    "label": 0
                },
                {
                    "sent": "And then the problem becomes effectively convex.",
                    "label": 0
                },
                {
                    "sent": "Not technically convex, but effectively convex.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of this kind of document.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a similar.",
                    "label": 0
                },
                {
                    "sent": "There's an analogy that you can make.",
                    "label": 0
                },
                {
                    "sent": "Also, if you think about, say, linearly square.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're trying to say using this video, something you're trying to find the best least square solution to regression problem.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have fewer samples and you have dimensions, it's very simple to solve.",
                    "label": 0
                },
                {
                    "sent": "If you have more more samples and dimension is very simple to solve right at the time where you have exactly the same number of samples, you have dimensions.",
                    "label": 0
                },
                {
                    "sent": "Then the Hessian become extremely ill condition right at that point is essentially the condition number goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The tuition algorithms never converge.",
                    "label": 0
                },
                {
                    "sent": "It's really bad when the system is just the right size.",
                    "label": 0
                },
                {
                    "sent": "It's really bad.",
                    "label": 0
                },
                {
                    "sent": "It's much better if its way over parameterized.",
                    "label": 0
                },
                {
                    "sent": "Which case you need to regularize, or if its way under parameterized, in which case it's kind of self regularised.",
                    "label": 0
                },
                {
                    "sent": "So what is VMS taught us is that you make the system way bigger than it needs to be, and you regularize the hell out of it.",
                    "label": 0
                },
                {
                    "sent": "OK, and people in neural Nets didn't didn't use to do this because we were all brainwashed by the fact that the statisticians were telling us, you know, you shouldn't have more parameters than you have training samples.",
                    "label": 0
                },
                {
                    "sent": "And it's just not true.",
                    "label": 0
                },
                {
                    "sent": "You want a lot more parameters and you have training samples and then just regularize like crazy.",
                    "label": 0
                },
                {
                    "sent": "By taking A1 hidden layer neural network with as many hidden Hurons as their input examples, and regularize the hell out of it and see if it performs as well.",
                    "label": 0
                },
                {
                    "sent": "So be kind of obvious experiment to make right and I'm sad to say that actually haven't tried but.",
                    "label": 0
                },
                {
                    "sent": "It could be we talked about 5 minutes, but.",
                    "label": 0
                },
                {
                    "sent": "No, I mean some things we've tried, for example, is with convolutional Nets.",
                    "label": 0
                },
                {
                    "sent": "If you make them bigger, so increase number of feature Maps, for example, two very very large numbers.",
                    "label": 0
                },
                {
                    "sent": "Two situations, for example, when you have 10 times more parameters in the model, then you have training samples actually work better.",
                    "label": 0
                },
                {
                    "sent": "You don't see overtraining.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "It seems to me that optimism what what do we do next?",
                    "label": 0
                },
                {
                    "sent": "If you have a technology that's reasonably effective for knowledge, free.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning right then maybe we should stop worrying about how much free supervised learning we should start thinking about knowledge, guided supervised learning and cumulative learning, right?",
                    "label": 0
                },
                {
                    "sent": "That's the reason to be optimistic.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so the question was, you know, if we have a reasonably good method for knowledge, free, supervised learning, then we should perhaps try to push the envelope towards non knowledge, be supervised running and we could be able to solve more complex problems.",
                    "label": 0
                },
                {
                    "sent": "So first of all I don't believe there is any knowledge free supervised learning method that just doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "SVM is not knowledge free and it's not unbiased.",
                    "label": 0
                },
                {
                    "sent": "So there are certain.",
                    "label": 0
                },
                {
                    "sent": "You know it looks like his general purpose, the same way we like to think that our brain is general purpose, but it's not true at all.",
                    "label": 0
                },
                {
                    "sent": "It's very, very, very specialized, and so there are certain types of functions.",
                    "label": 0
                },
                {
                    "sent": "So again, with the Gaussian kernel for example, will efficiently implement certain types of functions and we would be horribly inefficient for all kinds of other functions.",
                    "label": 0
                },
                {
                    "sent": "In fact, would be inefficient for many, many more functions than it will be.",
                    "label": 0
                },
                {
                    "sent": "You know, the number of functions would be efficient for, so every model is very restricted, and in fact there is no free lunch.",
                    "label": 0
                },
                {
                    "sent": "Theorems that show that you know if you have a generic learning model.",
                    "label": 0
                },
                {
                    "sent": "That's really general.",
                    "label": 0
                },
                {
                    "sent": "It actually can't run anything because you know there is no.",
                    "label": 0
                },
                {
                    "sent": "It's got Infinity.",
                    "label": 0
                },
                {
                    "sent": "Mention Barbara so.",
                    "label": 0
                },
                {
                    "sent": "Every model is specific and the fact that we think it's general is just an illusion.",
                    "label": 0
                },
                {
                    "sent": "It's just not.",
                    "label": 0
                },
                {
                    "sent": "But then you say OK, so those are not knowledge people there they have pre.",
                    "label": 0
                },
                {
                    "sent": "If you like they are prepared with completely random fictitious knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right exactly that right?",
                    "label": 0
                },
                {
                    "sent": "So I guess Windows VM's VM for example has the fictitious knowledge that the function you're trying to estimate the smooth.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what it is.",
                    "label": 0
                },
                {
                    "sent": "What it is.",
                    "label": 0
                },
                {
                    "sent": "So if he's got lots of variations in it, it can do it.",
                    "label": 0
                },
                {
                    "sent": "So you're Avengers paper on this.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that.",
                    "label": 0
                },
                {
                    "sent": "So the question is, you know that is hopeless right?",
                    "label": 0
                },
                {
                    "sent": "Loading doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "Let's just quit.",
                    "label": 0
                },
                {
                    "sent": "But but no, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not quite true.",
                    "label": 0
                },
                {
                    "sent": "What you want is.",
                    "label": 0
                },
                {
                    "sent": "You want the right primary transition for the type of problems you want to solve, so you know different people at different interests with supporting themselves.",
                    "label": 0
                },
                {
                    "sent": "But the point I was trying to make is that if the problems you're trying to solve, our sort of AI type problems.",
                    "label": 0
                },
                {
                    "sent": "Speech vision, things like that.",
                    "label": 0
                },
                {
                    "sent": "Then shadow architectures are not what you want.",
                    "label": 0
                },
                {
                    "sent": "You're not going to do it with, you know, show architectures and convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "You're going to have to find other families of functions, other parameterisation, and that's where deep learning comes into play.",
                    "label": 0
                },
                {
                    "sent": "We know we have a working example of this.",
                    "label": 0
                },
                {
                    "sent": "The visual, this is slide.",
                    "label": 0
                },
                {
                    "sent": "I skipped.",
                    "label": 0
                },
                {
                    "sent": "The visual system is very is very deep and probably learned and so that's at least one example.",
                    "label": 0
                },
                {
                    "sent": "We have a working system that's not all approved that they can't do it any other way.",
                    "label": 0
                },
                {
                    "sent": "Is there any other way probably?",
                    "label": 0
                },
                {
                    "sent": "Perhaps evolution would have found it.",
                    "label": 0
                },
                {
                    "sent": "You had a question.",
                    "label": 0
                },
                {
                    "sent": "Just following on from Stuart I guess.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying is that neural networks with sigmoid units is the right kind of bias to put to solve very difficult problems, which seems a bit.",
                    "label": 0
                },
                {
                    "sent": "did I say that?",
                    "label": 0
                },
                {
                    "sent": "Well, no, I didn't say that.",
                    "label": 0
                },
                {
                    "sent": "But let's build these large general purpose architectures based on sort of amorphous sigmoid units, initialized some random way and we should be able to solve everything with that now.",
                    "label": 0
                },
                {
                    "sent": "No, I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't make that claim.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, that will give you good.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "But the other thing is that maybe that will work when you have vast amounts of data with stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Those are the regimes where you really need the flexibility of these wide and deep architectures.",
                    "label": 0
                },
                {
                    "sent": "But what about when you have small amounts of data and you know why?",
                    "label": 0
                },
                {
                    "sent": "Is this sort of right?",
                    "label": 0
                },
                {
                    "sent": "Inductive bias, right?",
                    "label": 0
                },
                {
                    "sent": "So so I guess, so if you repeat the question for the purpose of the camera here, 'cause you probably didn't hear or summarize it.",
                    "label": 0
                },
                {
                    "sent": "So first of all, what's so special about?",
                    "label": 0
                },
                {
                    "sent": "You know multiple layers of sigmoids and weighted sums.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why is there?",
                    "label": 0
                },
                {
                    "sent": "Is there any reason to believe that that's a kind of a good model for sort of AI type tasks and and I am not advocating that this is necessarily the best solution to all the world problems.",
                    "label": 0
                },
                {
                    "sent": "OK, now there is something slightly special about it though, which is that?",
                    "label": 0
                },
                {
                    "sent": "You can't use stochastic gradient descent, for example, or grid and sent in general.",
                    "label": 0
                },
                {
                    "sent": "Good is running if the layers you have are all obvious, for example.",
                    "label": 0
                },
                {
                    "sent": "So if you have a bunch of carbs and you stack multiple layers of carbs, you can train it.",
                    "label": 0
                },
                {
                    "sent": "There's just so many local minima that are really awful, 'cause RBF is a really good local minimum and so it just makes 1 right there.",
                    "label": 0
                },
                {
                    "sent": "Every RBF basically makes little minimums if you want, and so that sort of kills the entire the entire thing.",
                    "label": 0
                },
                {
                    "sent": "It's not the size, right?",
                    "label": 0
                },
                {
                    "sent": "It's just not going to get any whereas.",
                    "label": 0
                },
                {
                    "sent": "Sigmoids you know they kind of have planes so.",
                    "label": 0
                },
                {
                    "sent": "It's much more difficult for them to build a local minimum, 'cause again you have to build a box out of multiple planks and you only have planks and you can really build a box with that.",
                    "label": 0
                },
                {
                    "sent": "So I think there is.",
                    "label": 0
                },
                {
                    "sent": "I don't think there is something like magical about them, but I think there is something that sort of non deliberately kind of useful or or favorable.",
                    "label": 0
                },
                {
                    "sent": "Then you had a second party question which I don't remember.",
                    "label": 0
                },
                {
                    "sent": "Now that OK.",
                    "label": 0
                },
                {
                    "sent": "So I don't think it's an issue of large and small, so I mean, I don't think this particular type of architecture is particularly interesting for large data set versus small datasets.",
                    "label": 0
                },
                {
                    "sent": "One thing is that VM's tend to work better for for small datasets because they're just better regularize.",
                    "label": 0
                },
                {
                    "sent": "This much easier to regularise SVM, then say, allotment right and logical net has just more parameters to talk with, so it's kinda.",
                    "label": 0
                },
                {
                    "sent": "It's harder it's not going to work as well on small datasets, so one of the reasons why we organize this deep.",
                    "label": 0
                },
                {
                    "sent": "The workshop was that satellite session is that unsupervised training is a way to kind of effectively reduce the amount of free parameters in the system with unlabeled data and therefore then be able to train being able to train the system with fewer labeled data.",
                    "label": 0
                },
                {
                    "sent": "And I really thank you to ask that question 'cause it's kind of useful point to make.",
                    "label": 0
                },
                {
                    "sent": "Is it all in argument against don't know what you mean.",
                    "label": 0
                },
                {
                    "sent": "Sparsity in the parameters or in the representations or in the parameters?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, you know pretty much all the neural net we train.",
                    "label": 0
                },
                {
                    "sent": "We all use L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "It always basically helps eliminate the weights you don't need and you know they just go to zero.",
                    "label": 0
                },
                {
                    "sent": "They don't know you.",
                    "label": 0
                },
                {
                    "sent": "And if you're smart about how you implement it, you can actually completely take them out of the system to run it fast.",
                    "label": 0
                },
                {
                    "sent": "So we do have one regularization all the time.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "I mean there's nothing.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are people who show that you know you take the first layer of an SVM with Gaussian guessing kernels, plug on top a logistic regression with one organization on the parameters, and you get pretty much the same performance as the real SVM with the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Problems where the the output has has a clear structure and you need to explain OK, so this is slide.",
                    "label": 0
                },
                {
                    "sent": "Actually wanted to put in my talk, which I didn't.",
                    "label": 0
                },
                {
                    "sent": "So in fact all those you know there's a slide beginning my talk that talks about handwriting and speech recognition, right?",
                    "label": 0
                },
                {
                    "sent": "And those are all.",
                    "label": 0
                },
                {
                    "sent": "Structured output learning.",
                    "label": 0
                },
                {
                    "sent": "Basically, they're all exactly the same as what people in this community called structured output models.",
                    "label": 0
                },
                {
                    "sent": "In terms of, I mean, they're actually considerably more complex because the user is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Prioritization of the energy, but.",
                    "label": 0
                },
                {
                    "sent": "So I wrote a tutorial energy based model last year.",
                    "label": 0
                },
                {
                    "sent": "I give it to you at least last year on this subject, which sort of tries to present all of those different models and sort of their relation to each other relationship to each other this week, like CRF, like something called Transformer Networks.",
                    "label": 0
                },
                {
                    "sent": "You know, Markov models, factor graphs in general.",
                    "label": 0
                },
                {
                    "sent": "Or not, and and you know structure model outputs maximum margin Markov Nets and things of that type, and then you can all view view them as sort of different size of a kind of common team.",
                    "label": 0
                },
                {
                    "sent": "But again the the stuff that people were doing in speech or handwriting are essentially no.",
                    "label": 0
                },
                {
                    "sent": "Actually this is based on the same ideas as a structured output models.",
                    "label": 0
                },
                {
                    "sent": "They just use more complex probabilistic log probabilities basically.",
                    "label": 0
                },
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Again, you can see there's a guarantee, but the generalization error this guarantee, But the problem with optimization time so people are people are just nervous.",
                    "label": 0
                },
                {
                    "sent": "You know that if you go to these numbers method, you know we can't really say.",
                    "label": 0
                },
                {
                    "sent": "You tell us that with the with the big hidden layer it's going to converge quickly, but you're saying trust us, but is it hard to say like you know to what extent?",
                    "label": 0
                },
                {
                    "sent": "You know we quantified so we can trust them.",
                    "label": 0
                },
                {
                    "sent": "So I don't say I'm not saying trust us, I said trust your eyes OK, try it out, see if it works better for you or not, and whether you have proof or not.",
                    "label": 0
                },
                {
                    "sent": "So if you have a proof you about on generalization, so the bounds on generalization, I really nothing you can use for practical purpose right?",
                    "label": 0
                },
                {
                    "sent": "'cause all those bounds are completely out to lunch in terms of you plug numbers into them, and you say you know it tells you you know.",
                    "label": 0
                },
                {
                    "sent": "You know, if you have more than you know 35 zillion examples, then the probability of error is less than one OK.",
                    "label": 0
                },
                {
                    "sent": "So I mean, they're really interesting conceptually, but in terms of sort of practical consequences, they absolutely have no no bearing on reality.",
                    "label": 0
                },
                {
                    "sent": "And that's because there, you know most of them are derived from sort of worst case analysis, and so you know every reasonable learning algorithm that has with an architecture that has a finite dimension will have bounds there all be bad.",
                    "label": 0
                },
                {
                    "sent": "OK, just SVM has terrible bounds, and other models have outrageously terrible bounds.",
                    "label": 0
                },
                {
                    "sent": "And and so you know, I don't care, you know.",
                    "label": 0
                },
                {
                    "sent": "In reality the the actual performance of the system is going to be whatever is going to be, and you can measure that with you know validation or test error or whatever you want and so just try it out.",
                    "label": 0
                },
                {
                    "sent": "If it works better, it works better for you.",
                    "label": 0
                },
                {
                    "sent": "Same thing for convergence of course with you can prove that you know QP is you know ordering cube, whatever.",
                    "label": 0
                },
                {
                    "sent": "But use for SVM, but in practice the algorithms that work the best, other ones that they own, for example proposed, which are which are much, much faster aseptically their slower.",
                    "label": 0
                },
                {
                    "sent": "You can't prove anything about them or not much, but in practice them faster and something for four.",
                    "label": 0
                },
                {
                    "sent": "For now.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's do this chart that I showed here this one.",
                    "label": 0
                },
                {
                    "sent": "OK again this is, you know this is this is the one with the bounds.",
                    "label": 0
                },
                {
                    "sent": "This is the one without the bounds.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know about you, but you know I see trust like this.",
                    "label": 0
                },
                {
                    "sent": "You know I pick the one with other bands.",
                    "label": 0
                },
                {
                    "sent": "So you know you have to know what the relevance of the theoretical work you know, whether the the hypothesis are relevant and it's not because we don't have theoretical results that they don't exist.",
                    "label": 0
                },
                {
                    "sent": "It's just that or mathematical tools haven't been powerful enough for exploited enough to really produce them.",
                    "label": 0
                },
                {
                    "sent": "So don't be afraid.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the the message of this thing, you know.",
                    "label": 0
                },
                {
                    "sent": "It's not because you don't have a theory that it's not going to work.",
                    "label": 0
                },
                {
                    "sent": "Thank you speaker again.",
                    "label": 0
                }
            ]
        }
    }
}