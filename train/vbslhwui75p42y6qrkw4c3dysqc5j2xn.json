{
    "id": "vbslhwui75p42y6qrkw4c3dysqc5j2xn",
    "title": "GSP (Geo-Semantic-Parsing)- Geoparsing and Geotagging with Machine Learning on top of Linked Data",
    "info": {
        "author": [
            "Leonardo Nizzoli, Institute of Informatics and Telematics (IIT), ICAR-CNR, Consiglio Nazionale delle Ricerche"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_nizzoli_linked_data/",
    "segmentation": [
        [
            "So good morning to everybody.",
            "My name is Leonard and it's Alia and I am a PhD student at the Department of Information Engineering or the University of Pisa.",
            "And they also collaborate with the Institute for Informatics and Telematics of the SNR of Pizza.",
            "This work was done in collaboration with Professor Marco Venuti of the University of Pisa and with Doctor Steven Kristiane multitask Onie of the IIT CNR of Pizza.",
            "This is an out."
        ],
        [
            "Line of my presentation.",
            "I will introduce the Geo parsing problem and some of the and the schema of the most common state of art solution techniques.",
            "Then I will present our proposal together with an evaluation and the results that we found.",
            "Let's start with the introduction.",
            "Jim."
        ],
        [
            "What singer is on a task consisting in the conversion of free text, location, references into unambiguous geographic identifiers like for example latitude and longitude?",
            "Here we can see an example of departing on two tweets.",
            "That is, we have two tweets with two location mentions that are identified in the text and link to proper coordinates."
        ],
        [
            "Why Japan task is important.",
            "For example, in my main research field that is social media intelligence, we can for example, achieve situational awareness during crisis events via the crowd, sensing paradigma in factor when?",
            "Disaster terrorist attack.",
            "An earthquake and so on, but happens.",
            "People start immediately tweeting about damages and casualties and so they are tests.",
            "Ocean cell sorts, publishing valuable information that can be leveraged in order to achieve situational awareness.",
            "Unfortunately, only one to 4% of tweets as structured geographic information in the metadata.",
            "So it is important to be able to extract this information.",
            "In an automatic way, for example from the text of the tweet itself.",
            "In this way, Geo parsing that is.",
            "This task enables, for example to build a crisis map for rescuers."
        ],
        [
            "The this is a general schema of the most common approaches in the state of art to this problem, and it consists of basically on 2 steps.",
            "The first step is called named entity recognition that is standard natural language Language processing task consisting in located named entities in text and classify them into predefined categories.",
            "So for example, people, organization or location that is our interest.",
            "And this task can be solving in a in various ways.",
            "State of Art is mainly based on two kind of approach, grammar based techniques or statistical models obtaining via machine learning.",
            "In both cases only morphosyntactic features of text are considered and there is no tension on semantics on the semantic of text.",
            "Once we have retrieved locations named entities, we have to search them in a gazeteer that is.",
            "Geographical index in order to extract the coordinates."
        ],
        [
            "The main problem related to this traditional approach is useful language policing.",
            "In fact, are the same token.",
            "In this case, Roma can be referred to a lot of places in the world to a football team to a movie, and with the methods that are based only on morphosyntactic feature.",
            "Disambiguation is very difficult, can be done only with simple heuristics that conveys poor results.",
            "Our main idea."
        ],
        [
            "That is to base our method on semantics.",
            "So we called it Geo semantic parsing in its most simple form of.",
            "In version, it consists of two steps, a semantic annotation step and the parts in step."
        ],
        [
            "The semantic connotation is another general task consisting in identify relevant parts of an input text and link them to the pertinent RDS resources.",
            "In the linked data clouds, for example, to DB pedia in entities.",
            "We used for state of art of the shelf tools like Tag Media, Spotlight, Dexter and Dandylion in order to perform this task.",
            "These tools are available via REST API calls and are able to identify relevant portion of text and link them to the partner resorts in a particular knowledge base.",
            "Let's say for example DB pedia.",
            "Why is doing this this linking task they implicitly perform?",
            "Also, this ambiguation exploiting the information.",
            "That is present in the Knowledge Graph itself in its structure.",
            "Once we have retrieve retrieved proper reference proper entity in the Knowledge Graph, we can exploit the rich information that is contained in order to extract.",
            "For example, in our case the geographical information."
        ],
        [
            "Once we have retrieved the location, it is important as we said the two parts it in order to retrieve coordinates.",
            "Whenever it is possible this parsing step is performed via sparkle queries that fetch geographic metadata from 45 RDF resource predicates, like for example Gelato or G along, and we consider in we built in our tool we fetch.",
            "We built the sparkle queries for 11 knowledge base, different knowledge basis in order to add more knowledge base is it is important to manually add the proper sparkle queries in order to reach the proper endpoints of the new knowledge graph.",
            "So at this stage the our method performance are compatible with the state of our technique, but we can do 2 main improvements that are mutually orthogonal and can be applied both together or separately the 1st."
        ],
        [
            "Improvement is so called the expansion step followed by a voting mechanism.",
            "The important thing is the expansion step and the technique that results from these.",
            "These improvement we called it expanded Geo semantic parsing.",
            "And the."
        ],
        [
            "Expansion step consists in extend our search of information not only to the knowledge graph on which we land by semantic annotation, but also traversing all the other knowledge graph that are searching for semantically equivalent results is.",
            "The goal of this expansion step is to increase the recall of our model, and the caller is a standard machine learning evolution metrics defined as the ratio between the corner in this case between the currency tag location and the location present in the text.",
            "In order to traverse the graph and reach semantically equivalent resources, we have to follow the URL that we can find in the this particular RDF predicate code sense."
        ],
        [
            "We said that we can obtain more than in this case more than one result corresponding to a particular portion of text that was identified as allocation.",
            "So when we parts these multiple resources, we can obtain different couple of coordinates because the Knowledge Graph are different.",
            "And so we need a criterion in order to choose that particular type of coordinates, because our our model.",
            "As too much allocation to one couple of coordinates and in order to do this we perform geographical binning.",
            "Cutting on the 3rd decimal place that correspond more or less on to 100 meters of the Ecuador.",
            "And we select the bin that contains the larger number of of that was voted by the larger number of resource is, let's say.",
            "Yeah, this is also an important.",
            "These also include an important result cause for example, we found that in the Italian DB Pedia the city of Milan's wrong coordinates that point to somewhere in Switzerland.",
            "But of course at least some months ago.",
            "I don't know now if there is also this mistake.",
            "But of course all the other knowledge graph as the proper coordinates so voting mechanism can avoid this kind of error."
        ],
        [
            "The other main improvement, though, that we can introduce is a so-called filtering step.",
            "To follow this, the parsing step and the filtering step goal is to prune possible errors that can be introduced both by the semantic annotator or by the resource content itself in order to solve this task, we trained SVM classifier and the goal of this task is to increase the precision of our model.",
            "The precision is another machine learning standard evaluation metrics.",
            "Consisting in the ratio between the current tags provided by our model with respect to all the tax provided by the model, so also the wrong ones and our goal is to prune the wrong the wrong tags."
        ],
        [
            "So the SVM classifier was trained on feature extracted from text that is part of speech tags.",
            "Morphosyntactic tags named entity recognition tags, all extracted with the.",
            "A state of our tool called T2 Copa.",
            "Yet again, fighting the reference to the article.",
            "And from the link built by this semantic annotators.",
            "So we take into consideration the annotator confidence, the added distance between the token present in the text and the RDF label of the resorts in linked by the semantic annotator and so on.",
            "And the resource.",
            "So for example, we consider the number of occurrence of the of our token into the abstract of the of the RDF resource.",
            "We consider structural properties of the ideas, resources and so on.",
            "The performance of this SVM classifier is very HIG, consisting in an F1 score or zero point 97.",
            "Therefore scorer is another metric of evaluation that can be calculated from precision recall by a simple harmonic mean.",
            "Well, I want to remark that the performance of indeed this performance is referred to this SVM classifier that prune errors not to the whole model.",
            "So here we can see a schema of functioning.",
            "We have three location mentions, 3 front location mentions that are semantically annotated by our annotators in three different knowledge graph.",
            "Please note that here we can.",
            "We are not performing the expansion step.",
            "We jump only on one knowledge graph."
        ],
        [
            "Then we parts Retreiving 3 different couple of coordinates, but one of them is discarded is not present in the output because our SVM classifier judge that these results is not reliable."
        ],
        [
            "Putting all this stuff together, we obtain our complete model that is called expanded.",
            "Do semantic parsing with filtering."
        ],
        [
            "And here we can see a simple schema of functioning.",
            "So we have two different text with two different mention of places.",
            "We the semantic annotation step link portion of text to our knowledge graph to knowledge base and the corresponding resorts we retrieve semantically equivalent resources with this function step we parts the results is searching for coordinates.",
            "We apply the voting mechanism in order to select.",
            "The most voted couple of coordinates, but in one case the results is finally discarded cause the classifier find that it is not reliable."
        ],
        [
            "So finally we evaluated our model and we choose to evaluate it on tweets.",
            "Both cause its main.",
            "Our main interest is in social media intelligence and cause it is very, very difficult task because they are short text.",
            "There is a poor context there and they often contain misspelling and slang language and so on.",
            "So it's very difficult to apply this kind of technique to tweets.",
            "We evaluated the results on a data set of English to it to itself provided by an official competition of named entity recognition and annotated by the organizer.",
            "It consists of 10,000 tweets about various events.",
            "The other data set is a collection of Italian, too.",
            "It's annotated by graduate student and collected by our group and consist of 2000 tweets about the Amelia 2012 earthquake and the Sardinia 2013 flood.",
            "The criterion of truth is that a target is correct if coordinates fall within a certain distance from the ground truth.",
            "That is the human annotation."
        ],
        [
            "So here we can see the results of our model compared with three states of our benchmarks evaluated on the tree evaluation metrics that we introduced before on the English data set, and we can see that the GSP model that is the most simple one as performance compatible with the state of art benchmarks.",
            "One, once we introduced the expansion step as we expected the recall increase with respect to the simple approach.",
            "While when we introduced the filtering step, we increase the precision and put all together, we can increase both precision and recall and so of course the F1 score and we can see that the final F1 score of our best model is more than twice than the best state about benchmark on this data set."
        ],
        [
            "On the Italian datasets we find results that are qualitatively similar but quantitatively different.",
            "In fact, all the performance of all the models are better because this is a simpler task due to the nature of the data set.",
            "That is referred to as a smaller piece of the world and with more and more genius type of text.",
            "Because they are speaking about the same type of events.",
            "And we but we can still find that our our base model performance are compatible with the state of art.",
            "When we introduce the expansion, we increase the recall.",
            "When we introduced the filtering, we increase the precision and when we put all together, we outperformed the state of art.",
            "So inconclusion."
        ],
        [
            "And what we have seen is that our proposal, larger or perform the state of our techniques, especially on the English data Set button, actually on both and is suitable for real world application.",
            "Actually, in the present version our our model is a proof of concept.",
            "In particularly it cannot be served in real time on a real Twitter stream during during crisis event because of the latency introduced by the REST API of the annotators that we employ.",
            "There is a solution that is to employ annotators on a local machine, and we can do it and we will do it in the future.",
            "Possible improvements from our conceptual point of view are, for example, tool one is to look up for information not only in semantical equivalent resources in different knowledge graph, but also in semantically similar resources.",
            "So to the neighbors of the found resorts in each of the knowledge graph that we consider.",
            "And we expect that this can increase the recall of our model.",
            "And the other possible improvement is trying to use multiple annotators altogether with an ensemble approach in order to increase both the precision and recall of the model.",
            "Thank you."
        ],
        [
            "For your attention, I'm here to reply to all your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good morning to everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is Leonard and it's Alia and I am a PhD student at the Department of Information Engineering or the University of Pisa.",
                    "label": 1
                },
                {
                    "sent": "And they also collaborate with the Institute for Informatics and Telematics of the SNR of Pizza.",
                    "label": 0
                },
                {
                    "sent": "This work was done in collaboration with Professor Marco Venuti of the University of Pisa and with Doctor Steven Kristiane multitask Onie of the IIT CNR of Pizza.",
                    "label": 0
                },
                {
                    "sent": "This is an out.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Line of my presentation.",
                    "label": 0
                },
                {
                    "sent": "I will introduce the Geo parsing problem and some of the and the schema of the most common state of art solution techniques.",
                    "label": 1
                },
                {
                    "sent": "Then I will present our proposal together with an evaluation and the results that we found.",
                    "label": 0
                },
                {
                    "sent": "Let's start with the introduction.",
                    "label": 0
                },
                {
                    "sent": "Jim.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What singer is on a task consisting in the conversion of free text, location, references into unambiguous geographic identifiers like for example latitude and longitude?",
                    "label": 1
                },
                {
                    "sent": "Here we can see an example of departing on two tweets.",
                    "label": 0
                },
                {
                    "sent": "That is, we have two tweets with two location mentions that are identified in the text and link to proper coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why Japan task is important.",
                    "label": 0
                },
                {
                    "sent": "For example, in my main research field that is social media intelligence, we can for example, achieve situational awareness during crisis events via the crowd, sensing paradigma in factor when?",
                    "label": 1
                },
                {
                    "sent": "Disaster terrorist attack.",
                    "label": 0
                },
                {
                    "sent": "An earthquake and so on, but happens.",
                    "label": 1
                },
                {
                    "sent": "People start immediately tweeting about damages and casualties and so they are tests.",
                    "label": 1
                },
                {
                    "sent": "Ocean cell sorts, publishing valuable information that can be leveraged in order to achieve situational awareness.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, only one to 4% of tweets as structured geographic information in the metadata.",
                    "label": 0
                },
                {
                    "sent": "So it is important to be able to extract this information.",
                    "label": 0
                },
                {
                    "sent": "In an automatic way, for example from the text of the tweet itself.",
                    "label": 0
                },
                {
                    "sent": "In this way, Geo parsing that is.",
                    "label": 0
                },
                {
                    "sent": "This task enables, for example to build a crisis map for rescuers.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The this is a general schema of the most common approaches in the state of art to this problem, and it consists of basically on 2 steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is called named entity recognition that is standard natural language Language processing task consisting in located named entities in text and classify them into predefined categories.",
                    "label": 1
                },
                {
                    "sent": "So for example, people, organization or location that is our interest.",
                    "label": 0
                },
                {
                    "sent": "And this task can be solving in a in various ways.",
                    "label": 1
                },
                {
                    "sent": "State of Art is mainly based on two kind of approach, grammar based techniques or statistical models obtaining via machine learning.",
                    "label": 0
                },
                {
                    "sent": "In both cases only morphosyntactic features of text are considered and there is no tension on semantics on the semantic of text.",
                    "label": 0
                },
                {
                    "sent": "Once we have retrieved locations named entities, we have to search them in a gazeteer that is.",
                    "label": 0
                },
                {
                    "sent": "Geographical index in order to extract the coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main problem related to this traditional approach is useful language policing.",
                    "label": 1
                },
                {
                    "sent": "In fact, are the same token.",
                    "label": 0
                },
                {
                    "sent": "In this case, Roma can be referred to a lot of places in the world to a football team to a movie, and with the methods that are based only on morphosyntactic feature.",
                    "label": 1
                },
                {
                    "sent": "Disambiguation is very difficult, can be done only with simple heuristics that conveys poor results.",
                    "label": 0
                },
                {
                    "sent": "Our main idea.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is to base our method on semantics.",
                    "label": 0
                },
                {
                    "sent": "So we called it Geo semantic parsing in its most simple form of.",
                    "label": 0
                },
                {
                    "sent": "In version, it consists of two steps, a semantic annotation step and the parts in step.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The semantic connotation is another general task consisting in identify relevant parts of an input text and link them to the pertinent RDS resources.",
                    "label": 1
                },
                {
                    "sent": "In the linked data clouds, for example, to DB pedia in entities.",
                    "label": 0
                },
                {
                    "sent": "We used for state of art of the shelf tools like Tag Media, Spotlight, Dexter and Dandylion in order to perform this task.",
                    "label": 0
                },
                {
                    "sent": "These tools are available via REST API calls and are able to identify relevant portion of text and link them to the partner resorts in a particular knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Let's say for example DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Why is doing this this linking task they implicitly perform?",
                    "label": 0
                },
                {
                    "sent": "Also, this ambiguation exploiting the information.",
                    "label": 0
                },
                {
                    "sent": "That is present in the Knowledge Graph itself in its structure.",
                    "label": 0
                },
                {
                    "sent": "Once we have retrieve retrieved proper reference proper entity in the Knowledge Graph, we can exploit the rich information that is contained in order to extract.",
                    "label": 0
                },
                {
                    "sent": "For example, in our case the geographical information.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we have retrieved the location, it is important as we said the two parts it in order to retrieve coordinates.",
                    "label": 0
                },
                {
                    "sent": "Whenever it is possible this parsing step is performed via sparkle queries that fetch geographic metadata from 45 RDF resource predicates, like for example Gelato or G along, and we consider in we built in our tool we fetch.",
                    "label": 1
                },
                {
                    "sent": "We built the sparkle queries for 11 knowledge base, different knowledge basis in order to add more knowledge base is it is important to manually add the proper sparkle queries in order to reach the proper endpoints of the new knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So at this stage the our method performance are compatible with the state of our technique, but we can do 2 main improvements that are mutually orthogonal and can be applied both together or separately the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Improvement is so called the expansion step followed by a voting mechanism.",
                    "label": 1
                },
                {
                    "sent": "The important thing is the expansion step and the technique that results from these.",
                    "label": 0
                },
                {
                    "sent": "These improvement we called it expanded Geo semantic parsing.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expansion step consists in extend our search of information not only to the knowledge graph on which we land by semantic annotation, but also traversing all the other knowledge graph that are searching for semantically equivalent results is.",
                    "label": 0
                },
                {
                    "sent": "The goal of this expansion step is to increase the recall of our model, and the caller is a standard machine learning evolution metrics defined as the ratio between the corner in this case between the currency tag location and the location present in the text.",
                    "label": 0
                },
                {
                    "sent": "In order to traverse the graph and reach semantically equivalent resources, we have to follow the URL that we can find in the this particular RDF predicate code sense.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We said that we can obtain more than in this case more than one result corresponding to a particular portion of text that was identified as allocation.",
                    "label": 0
                },
                {
                    "sent": "So when we parts these multiple resources, we can obtain different couple of coordinates because the Knowledge Graph are different.",
                    "label": 0
                },
                {
                    "sent": "And so we need a criterion in order to choose that particular type of coordinates, because our our model.",
                    "label": 0
                },
                {
                    "sent": "As too much allocation to one couple of coordinates and in order to do this we perform geographical binning.",
                    "label": 0
                },
                {
                    "sent": "Cutting on the 3rd decimal place that correspond more or less on to 100 meters of the Ecuador.",
                    "label": 1
                },
                {
                    "sent": "And we select the bin that contains the larger number of of that was voted by the larger number of resource is, let's say.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is also an important.",
                    "label": 0
                },
                {
                    "sent": "These also include an important result cause for example, we found that in the Italian DB Pedia the city of Milan's wrong coordinates that point to somewhere in Switzerland.",
                    "label": 0
                },
                {
                    "sent": "But of course at least some months ago.",
                    "label": 0
                },
                {
                    "sent": "I don't know now if there is also this mistake.",
                    "label": 0
                },
                {
                    "sent": "But of course all the other knowledge graph as the proper coordinates so voting mechanism can avoid this kind of error.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other main improvement, though, that we can introduce is a so-called filtering step.",
                    "label": 0
                },
                {
                    "sent": "To follow this, the parsing step and the filtering step goal is to prune possible errors that can be introduced both by the semantic annotator or by the resource content itself in order to solve this task, we trained SVM classifier and the goal of this task is to increase the precision of our model.",
                    "label": 0
                },
                {
                    "sent": "The precision is another machine learning standard evaluation metrics.",
                    "label": 0
                },
                {
                    "sent": "Consisting in the ratio between the current tags provided by our model with respect to all the tax provided by the model, so also the wrong ones and our goal is to prune the wrong the wrong tags.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the SVM classifier was trained on feature extracted from text that is part of speech tags.",
                    "label": 1
                },
                {
                    "sent": "Morphosyntactic tags named entity recognition tags, all extracted with the.",
                    "label": 0
                },
                {
                    "sent": "A state of our tool called T2 Copa.",
                    "label": 0
                },
                {
                    "sent": "Yet again, fighting the reference to the article.",
                    "label": 0
                },
                {
                    "sent": "And from the link built by this semantic annotators.",
                    "label": 0
                },
                {
                    "sent": "So we take into consideration the annotator confidence, the added distance between the token present in the text and the RDF label of the resorts in linked by the semantic annotator and so on.",
                    "label": 1
                },
                {
                    "sent": "And the resource.",
                    "label": 0
                },
                {
                    "sent": "So for example, we consider the number of occurrence of the of our token into the abstract of the of the RDF resource.",
                    "label": 0
                },
                {
                    "sent": "We consider structural properties of the ideas, resources and so on.",
                    "label": 1
                },
                {
                    "sent": "The performance of this SVM classifier is very HIG, consisting in an F1 score or zero point 97.",
                    "label": 0
                },
                {
                    "sent": "Therefore scorer is another metric of evaluation that can be calculated from precision recall by a simple harmonic mean.",
                    "label": 0
                },
                {
                    "sent": "Well, I want to remark that the performance of indeed this performance is referred to this SVM classifier that prune errors not to the whole model.",
                    "label": 0
                },
                {
                    "sent": "So here we can see a schema of functioning.",
                    "label": 0
                },
                {
                    "sent": "We have three location mentions, 3 front location mentions that are semantically annotated by our annotators in three different knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Please note that here we can.",
                    "label": 0
                },
                {
                    "sent": "We are not performing the expansion step.",
                    "label": 0
                },
                {
                    "sent": "We jump only on one knowledge graph.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we parts Retreiving 3 different couple of coordinates, but one of them is discarded is not present in the output because our SVM classifier judge that these results is not reliable.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Putting all this stuff together, we obtain our complete model that is called expanded.",
                    "label": 0
                },
                {
                    "sent": "Do semantic parsing with filtering.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we can see a simple schema of functioning.",
                    "label": 0
                },
                {
                    "sent": "So we have two different text with two different mention of places.",
                    "label": 0
                },
                {
                    "sent": "We the semantic annotation step link portion of text to our knowledge graph to knowledge base and the corresponding resorts we retrieve semantically equivalent resources with this function step we parts the results is searching for coordinates.",
                    "label": 0
                },
                {
                    "sent": "We apply the voting mechanism in order to select.",
                    "label": 0
                },
                {
                    "sent": "The most voted couple of coordinates, but in one case the results is finally discarded cause the classifier find that it is not reliable.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally we evaluated our model and we choose to evaluate it on tweets.",
                    "label": 0
                },
                {
                    "sent": "Both cause its main.",
                    "label": 0
                },
                {
                    "sent": "Our main interest is in social media intelligence and cause it is very, very difficult task because they are short text.",
                    "label": 0
                },
                {
                    "sent": "There is a poor context there and they often contain misspelling and slang language and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's very difficult to apply this kind of technique to tweets.",
                    "label": 0
                },
                {
                    "sent": "We evaluated the results on a data set of English to it to itself provided by an official competition of named entity recognition and annotated by the organizer.",
                    "label": 0
                },
                {
                    "sent": "It consists of 10,000 tweets about various events.",
                    "label": 1
                },
                {
                    "sent": "The other data set is a collection of Italian, too.",
                    "label": 1
                },
                {
                    "sent": "It's annotated by graduate student and collected by our group and consist of 2000 tweets about the Amelia 2012 earthquake and the Sardinia 2013 flood.",
                    "label": 1
                },
                {
                    "sent": "The criterion of truth is that a target is correct if coordinates fall within a certain distance from the ground truth.",
                    "label": 0
                },
                {
                    "sent": "That is the human annotation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we can see the results of our model compared with three states of our benchmarks evaluated on the tree evaluation metrics that we introduced before on the English data set, and we can see that the GSP model that is the most simple one as performance compatible with the state of art benchmarks.",
                    "label": 0
                },
                {
                    "sent": "One, once we introduced the expansion step as we expected the recall increase with respect to the simple approach.",
                    "label": 0
                },
                {
                    "sent": "While when we introduced the filtering step, we increase the precision and put all together, we can increase both precision and recall and so of course the F1 score and we can see that the final F1 score of our best model is more than twice than the best state about benchmark on this data set.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the Italian datasets we find results that are qualitatively similar but quantitatively different.",
                    "label": 0
                },
                {
                    "sent": "In fact, all the performance of all the models are better because this is a simpler task due to the nature of the data set.",
                    "label": 0
                },
                {
                    "sent": "That is referred to as a smaller piece of the world and with more and more genius type of text.",
                    "label": 0
                },
                {
                    "sent": "Because they are speaking about the same type of events.",
                    "label": 0
                },
                {
                    "sent": "And we but we can still find that our our base model performance are compatible with the state of art.",
                    "label": 0
                },
                {
                    "sent": "When we introduce the expansion, we increase the recall.",
                    "label": 0
                },
                {
                    "sent": "When we introduced the filtering, we increase the precision and when we put all together, we outperformed the state of art.",
                    "label": 0
                },
                {
                    "sent": "So inconclusion.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we have seen is that our proposal, larger or perform the state of our techniques, especially on the English data Set button, actually on both and is suitable for real world application.",
                    "label": 1
                },
                {
                    "sent": "Actually, in the present version our our model is a proof of concept.",
                    "label": 1
                },
                {
                    "sent": "In particularly it cannot be served in real time on a real Twitter stream during during crisis event because of the latency introduced by the REST API of the annotators that we employ.",
                    "label": 1
                },
                {
                    "sent": "There is a solution that is to employ annotators on a local machine, and we can do it and we will do it in the future.",
                    "label": 0
                },
                {
                    "sent": "Possible improvements from our conceptual point of view are, for example, tool one is to look up for information not only in semantical equivalent resources in different knowledge graph, but also in semantically similar resources.",
                    "label": 0
                },
                {
                    "sent": "So to the neighbors of the found resorts in each of the knowledge graph that we consider.",
                    "label": 0
                },
                {
                    "sent": "And we expect that this can increase the recall of our model.",
                    "label": 0
                },
                {
                    "sent": "And the other possible improvement is trying to use multiple annotators altogether with an ensemble approach in order to increase both the precision and recall of the model.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For your attention, I'm here to reply to all your questions.",
                    "label": 0
                }
            ]
        }
    }
}