{
    "id": "an3acu36zzdm4bxaqel5dtw6er3lzsvk",
    "title": "Detecting Incorrect Numerical Data in DBpedia",
    "info": {
        "author": [
            "Heiko Paulheim, Institut f\u00fcr Informatik, University of Mannheim"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_paulheim_numerical_data/",
    "segmentation": [
        [
            "So the I guess everybody of note knows DB pedia.",
            "Most of you use DB pedia.",
            "Just a brief wrap up on how it is created.",
            "We have these Wikipedia pages.",
            "We have those infoboxes which provide us with key value pairs and we use these key value pairs to then create a triple which in that case for example states that the height of Michael Jordan is 1.98 meters and this is pretty much everything you need to know about how deep does created to follow the chalk well so this sounds rather simplistic."
        ],
        [
            "Um, but there's a problem here.",
            "The problem is that Wikipedia is made for human consumption.",
            "Wikipedia is not created in a way that makes it easy for us to extract pedia from it.",
            "They are not so nice, unfortunately, and the input format of Wikipedia is by no means constraints, so you can pretty much hack everything into an infobox.",
            "You want to have there, and just to give you an overview of the variety for this value of the height of Michael Jordan, I just told you there are quite a few ways you could take that down.",
            "In the Wikipedia infobox and I guess everyone of you.",
            "Will have no problems at all and understanding all those different possibilities to write that down so you can use Imperial measures there.",
            "And you can use blanks between the foot and the engine, the number or you cannot use the blank.",
            "You can use every arbitrary character to use these apostrophes and Imperial notation and you will understand all these representations perfectly as a human being.",
            "The same holds for metric notations.",
            "You can use all sorts of decimal separators.",
            "You can use mixed notations or you can use just centimeters or meters with decimal separator.",
            "You can also combine both.",
            "This also happens a lot in Wikipedia, so putting 11 value in parentheses after the other and using the Imperial 1st and metrics second or the other way round and again for both of those, you have all degrees of freedom to take them down.",
            "You can also add some additional information there, like a footnote which refers to the place where you found that value, and so on and so forth.",
            "So and as a human being, you perfectly understand all those representations, but it's not so easy to write some.",
            "Piece of code that correctly extracts all that information from all those varieties."
        ],
        [
            "So the challenge here.",
            "Lots of people here talk a lot of that with slowly stepping out of the research labs with linked open data and get some industrial uptake.",
            "Hopefully.",
            "So we see applications, for example in Emergency Management and finance.",
            "You saw maybe the presentation by Benedict campaign on Tuesday, where he talked about providing all that information in an instrument for financial analysts, who then take decisions based on that data.",
            "And actually you want that information to be reliable in those cases, right?",
            "If you are supposed to invest a few thousand euros in some stock indices, you better.",
            "Hope that these numbers that you base your decision on alright even more so in Emergency Management.",
            "Like if you send your rescue squads out somewhere, you better do that on based on correct information.",
            "In this invited talk bifocal, somebody else, do they use DB pedia and medical application?",
            "And so, really, we hope that this information is correct.",
            "So we better have correct information.",
            "And IPD, when we see all those applications, it always scares me a bit to be honest.",
            "We better deal with all those variants of taking down numerical numbers correctly.",
            "The thing is, it's hard to cover each and every those cases even if you have test cases, there might be some rare cases, some some ought far off places where people use yet another way to write down a number, and you just don't know about it, and so it's very hard to to cover each and every case up front.",
            "So the idea I present in this paper is doing it the other way around.",
            "We just extract all these values and then look at the values afterwards and do some posteriori plausibility checking to find those values where we think well.",
            "It might be wrongly extracted that value, it just doesn't sound plausable."
        ],
        [
            "So instead of trying to cover for all of these cases up front, we look at the values after extracting them.",
            "And what you usually use for that sort of problem is outlier detection.",
            "So you have a sample of values of population of values you got from somewhere, and then you have some values that somehow deviate from the overall distribution, and these values are then considered outliers and just the way you could identify those points that are at least suspicious to you.",
            "You have to keep in mind, outliers are not necessarily wrong, they may just be somehow unusual values.",
            "They don't necessarily need to be errors, this is.",
            "Important to keep in mind."
        ],
        [
            "So what we do is we we look at all the properties we extracted, regard them as a data population.",
            "For example, all the values we extracted for the property height, and then we try to find outliers in their population.",
            "And in this paper we compared various methods to do so, like dispersion measures like interquartile range, like kernel density estimation and also an iterative version of KD."
        ],
        [
            "Just a brief run through median absolute deviation is a slight bit older than DB pedia.",
            "It goes back to Gauss and it actually looks at the median deviation from the median of the data and this interval gives you the sort of a trusted interval and if you don't add some factor which says OK to make sure, I just consider everything within three times this distance from the median is correct and the values that are outside that interval of them considered to be outliers."
        ],
        [
            "Interquartile range does something very similar, so for interquartile range you expect your data to be normally distributed and then you take the mean and the variance of the data and computer Gaussian distribution and consider everything which is in the far off ends of the Gaussian distribution as outliers you can use different multipliers there to to adjust the number of outliers to be found."
        ],
        [
            "And for those distributions which are not so nicely normally distributed, you can also use the flag kernel density estimation where you approximate the density of your data as a sum of kernel functions, like a sum of normal distributions.",
            "And this.",
            "This sum then gives you an estimate of the density of the data at each position, and if you have a data point that falls into an area which is not so dense, you can then assume if the density is very low there then the data point is probably an outlier, so."
        ],
        [
            "Just the methods that we looked into.",
            "Anjum we first try to apply them naively, so to say on all the populations we got and the problem here is that many properties are used on a variety of things.",
            "So for example this this height property I showed you in the Michael Jordan example is not only used for persons, but also for vehicles, for example.",
            "So you have two distributions that mix each other and then it gets pretty hard to figure out what is an outlier in that population.",
            "Also, the DB Pedia property population is just used for villages, cities, countries and continents and.",
            "If you take all these together, then you have something like the population of Europe as an outlier in that overall sample, but if you split the samples by types then you might achieve better results.",
            "So that was the idea to to refine the approach by preprocessing the data by splitting the data into subpopulations and then run individual outlier detection runs on the subpopulations."
        ],
        [
            "And to be more precise, we looked into two ways of doing so.",
            "One was splitting the data data populations by the most specific type in the DB Pedia ontology.",
            "So in the example I just showed you, we have one bucket which contains all the villages, one bucket that contains all the cities and so on and so forth.",
            "And then we run outlier detection on each of those buckets in isolation and hopefully get better results.",
            "And the other approach we looked into, we took all the types we got for each single instance and then try to automatically cluster them because.",
            "Some types may be more meaningful than others, and by that we we hope to get more precise results.",
            "So these were the two prepossession processing strategies we looked into and compare them to the baseline.",
            "Just running outlier detection on the overall sample of the property."
        ],
        [
            "To evaluate our approach, we went two steps.",
            "The first thing was we did a pre study on three attributes.",
            "So we just picked high population and elevation because they follow different distributions and they are reasonably well covered in DB pedia and then look for the most promising approaches and then just to validate if our results are not just randomly working well on these three properties.",
            "We also check them back on a random sample.",
            "As an evaluation strategy, it's pretty hard to come up with the gold standard for outlier detection.",
            "So what we did is we ran the outlier detection and then looked at all the outliers found and evaluated them manually afterwards.",
            "So we looked at all the identified outliers and set this one is an outlier.",
            "This one is an actual error and that one is not and this was possible because you could do some shortcuts there.",
            "You sometimes these outliers come in obvious clusters or patterns where you can just mark like 200 of them by just looking at this.",
            "Particular cluster, so it's actually feasible to do so."
        ],
        [
            "For the pre study we have these three properties and the sample sizes are hide there around 50,000 height values.",
            "In DDR 240,000 population values, 200,000 elevation values and they follow different distributions.",
            "Height is approximately normally distributed.",
            "Population follows a power law distribution and elevation is somehow strangely distributed.",
            "I actually have no idea how it's distributed, but it looks quite strange."
        ],
        [
            "Um averaged over these three properties, the results look as follows.",
            "You get quite decent precision.",
            "You also find a reasonable number of outliers, and you see that.",
            "First of all this this grouping and clustering this to pre processing strategies.",
            "They workout quite well.",
            "They improve the result considerably over just running the outlier detection on the simple sample and interquartile range, and kernel density estimation are the best performing approaches.",
            "You also observe that there is not so much difference between the two preprocessing strategy, so just grouping biases.",
            "The most specific type, or running the clustering algorithm is almost the same."
        ],
        [
            "But we saw that the clustering is very slow, so clustering always took more than a day.",
            "So we decided then just to stick to this single type splitting.",
            "We also saw that there is a fast Fourier transformation approximation of kernel density estimation, but that one has a very poor precision on the task at hand, so we didn't consider it any further and so just to validate that this is not due to chance that we just picked out the right three properties that the approach works well on.",
            "We also validated on a random sample which was built as follows.",
            "We first picked 50 random resources from DB pedia.",
            "Got all the data type properties and then retrieved all the triples that use one of those properties in the predicate position and from that sample removed everything where we had less than 100 numbers in the object position or where we had less than half numbers in the object position, and this left us with a sample of roughly 12 million triples, which we then ran the approach."
        ],
        [
            "Again, one predicate after the other.",
            "Um, so first we tried the best performing configuration in the pre study and the results were roughly come parable.",
            "And then we looked at doesn't change when we from there do some more exploration of the parameter space and in the end we saw that pretty much As for the for the three predicates sample we come up with the precision of 81%, so roughly 80% was also what we observed in the pre study and we could mark roughly 1700 values of the sample as outliers, which means that in DB pedia roughly one in 1000 values.",
            "Is wrong, which on the other hand means 99.9% is correct and that is not too bad.",
            "But actually we wanted to do better.",
            "So we looked at these results and thought.",
            "Well, what do we do with it now?",
            "Of course we can remove all these points.",
            "We think we are outliers, but maybe we can even draw some more from that."
        ],
        [
            "And to do so we we looked into the outliers more systematically and try to find out are there any patterns in which these outliers occur?",
            "And are there any irregularities that we can maybe identify?",
            "And here you see an example of this is the height of persons in DB pedia.",
            "The distribution of person Heights and you see roughly follows a nice normal distribution in expect for that glitch on the left hand side.",
            "So there is something that clearly deviates from the normal distribution.",
            "And if you look closer into that we found that there are just roughly 1000 people in the pedia who exactly 1 meter and 52 centimeters high, which is exactly 5 feet.",
            "So what happens here is that the Imperial measure after 5 feet is just truncated during the extraction.",
            "So by looking into those patterns we are able to pinpoint the positions in the code where something goes wrong, and then we can dig into the code and see OK. Where are those?",
            "We have the examples now we now this is an example for something where we truncate after the after the foot marker and then we can look into the code and see OK. Why does that go wrong?",
            "So it helps us debugging the code in the end."
        ],
        [
            "Just just a brief breakdown by types of errors, so the most most severe sources of errors is actually just Imperial converting Imperial numbers because there are so many ways to write down a number in Imperial measures.",
            "This bar is actually truncated.",
            "It goes down to 1000 and something, so just that you have a chance to see the other bars, we truncated it at 100, but this is really the cause of 90% of the outliers we found.",
            "Yeah, then the second largest source is actually factual errors in Wikipedia, so we will never be able to deal with those, probably.",
            "Then there is a random distribution of other stuff, like if you have an additional number which comes with the number at hand or you have problems with interpreting metric values and so on and so forth.",
            "But these are the two most dominant courses.",
            "He's an ex."
        ],
        [
            "Sample for that problem with the additional number there is this village semaphore in southern Australia, which according to DB Pedia has a population of 28 million, whereas all of Australia has roughly 23 million.",
            "So it clearly can't be correct.",
            "And it's also clear outlier among the villages if you look into what's happening.",
            "You see on the right hand side the population is 2832 according to the 2006 census.",
            "Again as a human being you have no problem interpreting that value, but the machine obviously has a problem there.",
            "So here we can see there is a problem and then we can try to go into the code.",
            "And fix hopefully fix it by saying if there is a blank there and then there's another number coming.",
            "Then might be a good idea to ignore the rest of the number.",
            "Might also not.",
            "You never know, but this is this is a typical problem that occurs if you have an additional number there then the extraction code can have problems."
        ],
        [
            "As every approach, this approach has limitations and a typical mutation that occurs if you use outlier detection to find errors in data is that you have natural outliers and natural outliers are not errors.",
            "They are just values that are outlying but they are correct.",
            "So you have a vehicle which is 7.4 meters high.",
            "You of course find that as an outlier and it's pretty hard to tell that teach the machine that this one is not an error, but this one is actually correct and so is this person on the right hand side.",
            "Who was the shortest woman to ever live on Earth?",
            "She was a 58 centimeters high.",
            "Clearly, outlier detection finds that value, tells you it is an error, and again you have to find additional evidence to tell whether this is really whether this is an error in the data or just the natural outlier, but this is a very, very hard task to teach the machine."
        ],
        [
            "So when we start the Q&A session soon, I guess somebody will for sure ask.",
            "Yeah, but this is just the PD.",
            "And what is the relevance to other datasets so can take that one right away.",
            "Actually you can transfer that to any any linked open data set, and as soon as you step out of the comfort zone of transferring a relational database into RDF, you will have these sort of problems.",
            "Here.",
            "You see these approaches coming up where people use information extraction from text with reverb on L. Where they just try to get facts from text which is prone to such sorts of error when they try to heuristically complete facts from different sources, or try to heuristically integrate different data sources into one common data set, and as soon as you use those heuristic methods, you are quite likely to have problems with wrong numbers, and then you can use such an approach so it's not limited to DB pedia.",
            "Although we only evaluated it on DVD."
        ],
        [
            "There are some ongoing work as I showed you there's this problem of natural outliers which are not errors in the data and it is hard to a machine to understand that.",
            "But it's not impossible.",
            "So one thing you can do is you can cross check that value them with other sources and for DB pedia this is actually not so hard because we have the pedia in different language additions.",
            "So you can just look is that value also existing in other language addition in a similar range and if it is you can say OK then it's probably correct because it's quite unlikely.",
            "That people put that value in the exact same format in different language additions and then and then the extraction code does the same error on all the language addition that hardly happens.",
            "But we also look into currently is some preprocessing techniques.",
            "So you showed.",
            "This preprocessing really gives you an advantage and currently looking at maybe we can we can squeeze out some outperformance by using more intelligent preprocessing and finding these meaningful subpopulations of the data.",
            "And one thing which is particularly interesting for us is so far we have looked into these errors manually and then try to pinpoint the things where it goes wrong, but you could actually also do is try to find the text patterns by text patterns induction which tells you this is a typical pattern that the information extraction code cannot handle and this makes it much easier to do actually then pinpoint the errors.",
            "For example, you could find something like there is there is a number followed by.",
            "A year in parenthesis and if you find that pattern on many outliers, you can see OK, we probably are not capable of handling that pattern, and this would clearly make it easier for us to pinpoint the errors in the code.",
            "And actually it could also help identifying natural outliers because natural outliers are rather unlikely to come with such a pattern, which which is prevalent in many data points.",
            "This is rather not going to happen, so it could also help us with that problem."
        ],
        [
            "Yeah, so thanks for listening and I'm now happy to take your questions."
        ],
        [
            "Hi, thanks for the presentation, was awesome.",
            "I think you know what I'm going to ask anyways, but first I wanted to comment that probably your student that all the work because it didn't find any mistakes.",
            "So you said that all the bad stuff was you who did it so.",
            "But the question is how is this going to feed back into DB pedia, right?",
            "So are you guys going to have an extractor that is going to be contributed as open source so that now we can?",
            "Flag the wrong triples in a way so that somebody can review them at least, and the second point is you said that sometimes there's nothing we can do when the data is wrong in Wikipedia, and that's true.",
            "Sometimes we can't do, but I think most times we can.",
            "So Wikipedia has these bots right that so you can have like a wrong number bot or outlier detection bot where you go and flag pages and say, hey, this looks strange and I.",
            "A human could go there and put another flag that says no, it's strange because it is an actual outlier.",
            "There's nothing we can.",
            "We should touch this one again, right?",
            "So I just general question is what are you guys trying to do so that this feeds back into the beta increases equality?",
            "OK, so there are actually quite a few things you could do, so one is you can just collect that data and say OK, here is another data set that has that we're somehow disputed and we have some confidence values in that.",
            "I also talked to Magnus new this morning with this Petra Project which collects.",
            "Different possible errors from different sources and tries to aggregate them, but actually the ultimate goal would be to use that to do as a means to debug the code, because it really can help us, especially once we got this thing with pattern induction, it can really help us pinpointing their positions in the code.",
            "Saying this is something going wrong, and yeah, I mean this would also be the most elegant way of dealing with the problem with eliminating the problem in the code in the end.",
            "But you can also say OK, we can also publish datasets.",
            "Say this is something with the dispute effects and then you can decide whether you want the.",
            "The raw extraction or whether you want the disputed facts ruled out under a certain threshold.",
            "Yeah, and this bot thing.",
            "Clearly it's an interesting thing, but I don't know whether whether we would rather bother the Wikipedia community, but just reading random like millions of requests.",
            "Please double check that value and then it's most of them as natural outliers and people who got these guys again.",
            "Yeah, it is correct.",
            "This city is that large command give me a break.",
            "I don't know what's going to happen, but you can think of that, of course.",
            "Thanks for the presentation.",
            "How can I have three straightforward questions?",
            "I would think so.",
            "You evaluated on three or on single properties.",
            "How about the cases in which the answer to or the Valley answer to a question can be spread over multiple properties?",
            "I was thinking about location headquarters of a company.",
            "I don't have any numerical example in mind, but they are at least five or six.",
            "Properties in DV pedia.",
            "You could use to to find out that information.",
            "So could I guess your approach could be expanded to cover those cases, right?",
            "So currently they are treated in isolation.",
            "Yeah, that's true.",
            "But if they are mapped to the same same property in ontology then then it's fine.",
            "If they are not, then they're treated individually.",
            "Yeah, that's true.",
            "My second question is at the beginning, you gave this examples about the different Heights of the sportsman.",
            "Um?",
            "You actually don't solve that problem, right?",
            "I mean the different way in each week and writes down the height you actually assume you have a value for it.",
            "Actually tried to handle it, so I mean we cannot forbid the Wikipedia community and tell them.",
            "But you can use that format, but you must not use the other format.",
            "That's never going to happen.",
            "So yeah, the goal is to find a way to deal with that and expand the extraction code insofar that it can cover all those cases.",
            "OK, and you would do that with this type of patterns that you meant.",
            "For example, you're just trying to pinpoint.",
            "The positions of the patterns that we cannot handle correctly.",
            "I have two questions, so the first one is related to the code is available for this or not and the next one is about the scalability because if I want to apply this approach to some large data sets, for example giono are linked CJ.",
            "So both of these datasets are over 20 billion triples.",
            "So how you are approached you know we deal with such datasets.",
            "Yes, some of these are actually quite fast like this this medium.",
            "So deviation can be computed quite quickly, it just requires sorting the data once and then you're done and you can do it in a relational database.",
            "Other approach depends, I mean KDE is not so fast and we saw that the fast approximation of KD is not accurate enough.",
            "So it clearly depends on the outlier detection method and some are really scalable, some are not at all.",
            "I'm sorry I forgot about the first question now.",
            "The code is not available at the moment, but I could still push my student even though he's in the US to just publish it yet.",
            "So just a quick question.",
            "If you're crazy enough to and you want to plot on a map the 100 of thousands of populated place of DB pedia, you will end up plotting a lot of things in the sea.",
            "It turns out that Joe coordinates into Pedia are often incorrect - omitted when you saw famous for extra extra extra.",
            "I'm just curious, is it an error that you have encountered in your sample?",
            "And if not, why?",
            "A country in the sample, sorry.",
            "The.",
            "So, so there are a lot of info boxes, for example where you will have two latitude, one we have - an, not the other one.",
            "So there is an incorrect value.",
            "Of course.",
            "Yeah, The thing is that the Geo coordinates are used expected using a different extractor on DVD.",
            "It's not the infobox extractor, it's another mechanism that creates those coordinates.",
            "So actually just not not affected by what you put in the infobox.",
            "Generally it's probably possible to find these with the same methods because you find certain areas.",
            "Of the of coordinates, there should be nothing.",
            "There should be no place there.",
            "The thing is, we've looked here only at a single dimensional approaches, and for that 2 dimensional approach would probably work very much better.",
            "Well, thank you very much for your questions.",
            "Thank you hiker for your talk.",
            "Trying not to make this an outlier.",
            "We will finish here by thanking hiker.",
            "Thank you answer."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the I guess everybody of note knows DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Most of you use DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Just a brief wrap up on how it is created.",
                    "label": 0
                },
                {
                    "sent": "We have these Wikipedia pages.",
                    "label": 0
                },
                {
                    "sent": "We have those infoboxes which provide us with key value pairs and we use these key value pairs to then create a triple which in that case for example states that the height of Michael Jordan is 1.98 meters and this is pretty much everything you need to know about how deep does created to follow the chalk well so this sounds rather simplistic.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, but there's a problem here.",
                    "label": 0
                },
                {
                    "sent": "The problem is that Wikipedia is made for human consumption.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia is not created in a way that makes it easy for us to extract pedia from it.",
                    "label": 0
                },
                {
                    "sent": "They are not so nice, unfortunately, and the input format of Wikipedia is by no means constraints, so you can pretty much hack everything into an infobox.",
                    "label": 0
                },
                {
                    "sent": "You want to have there, and just to give you an overview of the variety for this value of the height of Michael Jordan, I just told you there are quite a few ways you could take that down.",
                    "label": 0
                },
                {
                    "sent": "In the Wikipedia infobox and I guess everyone of you.",
                    "label": 0
                },
                {
                    "sent": "Will have no problems at all and understanding all those different possibilities to write that down so you can use Imperial measures there.",
                    "label": 0
                },
                {
                    "sent": "And you can use blanks between the foot and the engine, the number or you cannot use the blank.",
                    "label": 0
                },
                {
                    "sent": "You can use every arbitrary character to use these apostrophes and Imperial notation and you will understand all these representations perfectly as a human being.",
                    "label": 0
                },
                {
                    "sent": "The same holds for metric notations.",
                    "label": 0
                },
                {
                    "sent": "You can use all sorts of decimal separators.",
                    "label": 0
                },
                {
                    "sent": "You can use mixed notations or you can use just centimeters or meters with decimal separator.",
                    "label": 0
                },
                {
                    "sent": "You can also combine both.",
                    "label": 0
                },
                {
                    "sent": "This also happens a lot in Wikipedia, so putting 11 value in parentheses after the other and using the Imperial 1st and metrics second or the other way round and again for both of those, you have all degrees of freedom to take them down.",
                    "label": 0
                },
                {
                    "sent": "You can also add some additional information there, like a footnote which refers to the place where you found that value, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So and as a human being, you perfectly understand all those representations, but it's not so easy to write some.",
                    "label": 0
                },
                {
                    "sent": "Piece of code that correctly extracts all that information from all those varieties.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the challenge here.",
                    "label": 0
                },
                {
                    "sent": "Lots of people here talk a lot of that with slowly stepping out of the research labs with linked open data and get some industrial uptake.",
                    "label": 0
                },
                {
                    "sent": "Hopefully.",
                    "label": 0
                },
                {
                    "sent": "So we see applications, for example in Emergency Management and finance.",
                    "label": 0
                },
                {
                    "sent": "You saw maybe the presentation by Benedict campaign on Tuesday, where he talked about providing all that information in an instrument for financial analysts, who then take decisions based on that data.",
                    "label": 0
                },
                {
                    "sent": "And actually you want that information to be reliable in those cases, right?",
                    "label": 0
                },
                {
                    "sent": "If you are supposed to invest a few thousand euros in some stock indices, you better.",
                    "label": 0
                },
                {
                    "sent": "Hope that these numbers that you base your decision on alright even more so in Emergency Management.",
                    "label": 0
                },
                {
                    "sent": "Like if you send your rescue squads out somewhere, you better do that on based on correct information.",
                    "label": 0
                },
                {
                    "sent": "In this invited talk bifocal, somebody else, do they use DB pedia and medical application?",
                    "label": 0
                },
                {
                    "sent": "And so, really, we hope that this information is correct.",
                    "label": 0
                },
                {
                    "sent": "So we better have correct information.",
                    "label": 0
                },
                {
                    "sent": "And IPD, when we see all those applications, it always scares me a bit to be honest.",
                    "label": 0
                },
                {
                    "sent": "We better deal with all those variants of taking down numerical numbers correctly.",
                    "label": 0
                },
                {
                    "sent": "The thing is, it's hard to cover each and every those cases even if you have test cases, there might be some rare cases, some some ought far off places where people use yet another way to write down a number, and you just don't know about it, and so it's very hard to to cover each and every case up front.",
                    "label": 0
                },
                {
                    "sent": "So the idea I present in this paper is doing it the other way around.",
                    "label": 0
                },
                {
                    "sent": "We just extract all these values and then look at the values afterwards and do some posteriori plausibility checking to find those values where we think well.",
                    "label": 0
                },
                {
                    "sent": "It might be wrongly extracted that value, it just doesn't sound plausable.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So instead of trying to cover for all of these cases up front, we look at the values after extracting them.",
                    "label": 1
                },
                {
                    "sent": "And what you usually use for that sort of problem is outlier detection.",
                    "label": 1
                },
                {
                    "sent": "So you have a sample of values of population of values you got from somewhere, and then you have some values that somehow deviate from the overall distribution, and these values are then considered outliers and just the way you could identify those points that are at least suspicious to you.",
                    "label": 0
                },
                {
                    "sent": "You have to keep in mind, outliers are not necessarily wrong, they may just be somehow unusual values.",
                    "label": 1
                },
                {
                    "sent": "They don't necessarily need to be errors, this is.",
                    "label": 0
                },
                {
                    "sent": "Important to keep in mind.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we we look at all the properties we extracted, regard them as a data population.",
                    "label": 0
                },
                {
                    "sent": "For example, all the values we extracted for the property height, and then we try to find outliers in their population.",
                    "label": 0
                },
                {
                    "sent": "And in this paper we compared various methods to do so, like dispersion measures like interquartile range, like kernel density estimation and also an iterative version of KD.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a brief run through median absolute deviation is a slight bit older than DB pedia.",
                    "label": 0
                },
                {
                    "sent": "It goes back to Gauss and it actually looks at the median deviation from the median of the data and this interval gives you the sort of a trusted interval and if you don't add some factor which says OK to make sure, I just consider everything within three times this distance from the median is correct and the values that are outside that interval of them considered to be outliers.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interquartile range does something very similar, so for interquartile range you expect your data to be normally distributed and then you take the mean and the variance of the data and computer Gaussian distribution and consider everything which is in the far off ends of the Gaussian distribution as outliers you can use different multipliers there to to adjust the number of outliers to be found.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for those distributions which are not so nicely normally distributed, you can also use the flag kernel density estimation where you approximate the density of your data as a sum of kernel functions, like a sum of normal distributions.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "This sum then gives you an estimate of the density of the data at each position, and if you have a data point that falls into an area which is not so dense, you can then assume if the density is very low there then the data point is probably an outlier, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the methods that we looked into.",
                    "label": 0
                },
                {
                    "sent": "Anjum we first try to apply them naively, so to say on all the populations we got and the problem here is that many properties are used on a variety of things.",
                    "label": 0
                },
                {
                    "sent": "So for example this this height property I showed you in the Michael Jordan example is not only used for persons, but also for vehicles, for example.",
                    "label": 0
                },
                {
                    "sent": "So you have two distributions that mix each other and then it gets pretty hard to figure out what is an outlier in that population.",
                    "label": 0
                },
                {
                    "sent": "Also, the DB Pedia property population is just used for villages, cities, countries and continents and.",
                    "label": 0
                },
                {
                    "sent": "If you take all these together, then you have something like the population of Europe as an outlier in that overall sample, but if you split the samples by types then you might achieve better results.",
                    "label": 0
                },
                {
                    "sent": "So that was the idea to to refine the approach by preprocessing the data by splitting the data into subpopulations and then run individual outlier detection runs on the subpopulations.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to be more precise, we looked into two ways of doing so.",
                    "label": 0
                },
                {
                    "sent": "One was splitting the data data populations by the most specific type in the DB Pedia ontology.",
                    "label": 0
                },
                {
                    "sent": "So in the example I just showed you, we have one bucket which contains all the villages, one bucket that contains all the cities and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And then we run outlier detection on each of those buckets in isolation and hopefully get better results.",
                    "label": 0
                },
                {
                    "sent": "And the other approach we looked into, we took all the types we got for each single instance and then try to automatically cluster them because.",
                    "label": 0
                },
                {
                    "sent": "Some types may be more meaningful than others, and by that we we hope to get more precise results.",
                    "label": 0
                },
                {
                    "sent": "So these were the two prepossession processing strategies we looked into and compare them to the baseline.",
                    "label": 0
                },
                {
                    "sent": "Just running outlier detection on the overall sample of the property.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To evaluate our approach, we went two steps.",
                    "label": 0
                },
                {
                    "sent": "The first thing was we did a pre study on three attributes.",
                    "label": 0
                },
                {
                    "sent": "So we just picked high population and elevation because they follow different distributions and they are reasonably well covered in DB pedia and then look for the most promising approaches and then just to validate if our results are not just randomly working well on these three properties.",
                    "label": 0
                },
                {
                    "sent": "We also check them back on a random sample.",
                    "label": 0
                },
                {
                    "sent": "As an evaluation strategy, it's pretty hard to come up with the gold standard for outlier detection.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we ran the outlier detection and then looked at all the outliers found and evaluated them manually afterwards.",
                    "label": 0
                },
                {
                    "sent": "So we looked at all the identified outliers and set this one is an outlier.",
                    "label": 0
                },
                {
                    "sent": "This one is an actual error and that one is not and this was possible because you could do some shortcuts there.",
                    "label": 0
                },
                {
                    "sent": "You sometimes these outliers come in obvious clusters or patterns where you can just mark like 200 of them by just looking at this.",
                    "label": 0
                },
                {
                    "sent": "Particular cluster, so it's actually feasible to do so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the pre study we have these three properties and the sample sizes are hide there around 50,000 height values.",
                    "label": 0
                },
                {
                    "sent": "In DDR 240,000 population values, 200,000 elevation values and they follow different distributions.",
                    "label": 0
                },
                {
                    "sent": "Height is approximately normally distributed.",
                    "label": 0
                },
                {
                    "sent": "Population follows a power law distribution and elevation is somehow strangely distributed.",
                    "label": 0
                },
                {
                    "sent": "I actually have no idea how it's distributed, but it looks quite strange.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um averaged over these three properties, the results look as follows.",
                    "label": 0
                },
                {
                    "sent": "You get quite decent precision.",
                    "label": 0
                },
                {
                    "sent": "You also find a reasonable number of outliers, and you see that.",
                    "label": 0
                },
                {
                    "sent": "First of all this this grouping and clustering this to pre processing strategies.",
                    "label": 0
                },
                {
                    "sent": "They workout quite well.",
                    "label": 0
                },
                {
                    "sent": "They improve the result considerably over just running the outlier detection on the simple sample and interquartile range, and kernel density estimation are the best performing approaches.",
                    "label": 0
                },
                {
                    "sent": "You also observe that there is not so much difference between the two preprocessing strategy, so just grouping biases.",
                    "label": 0
                },
                {
                    "sent": "The most specific type, or running the clustering algorithm is almost the same.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we saw that the clustering is very slow, so clustering always took more than a day.",
                    "label": 0
                },
                {
                    "sent": "So we decided then just to stick to this single type splitting.",
                    "label": 0
                },
                {
                    "sent": "We also saw that there is a fast Fourier transformation approximation of kernel density estimation, but that one has a very poor precision on the task at hand, so we didn't consider it any further and so just to validate that this is not due to chance that we just picked out the right three properties that the approach works well on.",
                    "label": 0
                },
                {
                    "sent": "We also validated on a random sample which was built as follows.",
                    "label": 0
                },
                {
                    "sent": "We first picked 50 random resources from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Got all the data type properties and then retrieved all the triples that use one of those properties in the predicate position and from that sample removed everything where we had less than 100 numbers in the object position or where we had less than half numbers in the object position, and this left us with a sample of roughly 12 million triples, which we then ran the approach.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, one predicate after the other.",
                    "label": 0
                },
                {
                    "sent": "Um, so first we tried the best performing configuration in the pre study and the results were roughly come parable.",
                    "label": 0
                },
                {
                    "sent": "And then we looked at doesn't change when we from there do some more exploration of the parameter space and in the end we saw that pretty much As for the for the three predicates sample we come up with the precision of 81%, so roughly 80% was also what we observed in the pre study and we could mark roughly 1700 values of the sample as outliers, which means that in DB pedia roughly one in 1000 values.",
                    "label": 0
                },
                {
                    "sent": "Is wrong, which on the other hand means 99.9% is correct and that is not too bad.",
                    "label": 0
                },
                {
                    "sent": "But actually we wanted to do better.",
                    "label": 0
                },
                {
                    "sent": "So we looked at these results and thought.",
                    "label": 0
                },
                {
                    "sent": "Well, what do we do with it now?",
                    "label": 0
                },
                {
                    "sent": "Of course we can remove all these points.",
                    "label": 0
                },
                {
                    "sent": "We think we are outliers, but maybe we can even draw some more from that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to do so we we looked into the outliers more systematically and try to find out are there any patterns in which these outliers occur?",
                    "label": 0
                },
                {
                    "sent": "And are there any irregularities that we can maybe identify?",
                    "label": 0
                },
                {
                    "sent": "And here you see an example of this is the height of persons in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "The distribution of person Heights and you see roughly follows a nice normal distribution in expect for that glitch on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "So there is something that clearly deviates from the normal distribution.",
                    "label": 0
                },
                {
                    "sent": "And if you look closer into that we found that there are just roughly 1000 people in the pedia who exactly 1 meter and 52 centimeters high, which is exactly 5 feet.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that the Imperial measure after 5 feet is just truncated during the extraction.",
                    "label": 0
                },
                {
                    "sent": "So by looking into those patterns we are able to pinpoint the positions in the code where something goes wrong, and then we can dig into the code and see OK. Where are those?",
                    "label": 0
                },
                {
                    "sent": "We have the examples now we now this is an example for something where we truncate after the after the foot marker and then we can look into the code and see OK. Why does that go wrong?",
                    "label": 0
                },
                {
                    "sent": "So it helps us debugging the code in the end.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just just a brief breakdown by types of errors, so the most most severe sources of errors is actually just Imperial converting Imperial numbers because there are so many ways to write down a number in Imperial measures.",
                    "label": 0
                },
                {
                    "sent": "This bar is actually truncated.",
                    "label": 0
                },
                {
                    "sent": "It goes down to 1000 and something, so just that you have a chance to see the other bars, we truncated it at 100, but this is really the cause of 90% of the outliers we found.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then the second largest source is actually factual errors in Wikipedia, so we will never be able to deal with those, probably.",
                    "label": 0
                },
                {
                    "sent": "Then there is a random distribution of other stuff, like if you have an additional number which comes with the number at hand or you have problems with interpreting metric values and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But these are the two most dominant courses.",
                    "label": 0
                },
                {
                    "sent": "He's an ex.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample for that problem with the additional number there is this village semaphore in southern Australia, which according to DB Pedia has a population of 28 million, whereas all of Australia has roughly 23 million.",
                    "label": 0
                },
                {
                    "sent": "So it clearly can't be correct.",
                    "label": 0
                },
                {
                    "sent": "And it's also clear outlier among the villages if you look into what's happening.",
                    "label": 0
                },
                {
                    "sent": "You see on the right hand side the population is 2832 according to the 2006 census.",
                    "label": 0
                },
                {
                    "sent": "Again as a human being you have no problem interpreting that value, but the machine obviously has a problem there.",
                    "label": 0
                },
                {
                    "sent": "So here we can see there is a problem and then we can try to go into the code.",
                    "label": 0
                },
                {
                    "sent": "And fix hopefully fix it by saying if there is a blank there and then there's another number coming.",
                    "label": 0
                },
                {
                    "sent": "Then might be a good idea to ignore the rest of the number.",
                    "label": 0
                },
                {
                    "sent": "Might also not.",
                    "label": 0
                },
                {
                    "sent": "You never know, but this is this is a typical problem that occurs if you have an additional number there then the extraction code can have problems.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As every approach, this approach has limitations and a typical mutation that occurs if you use outlier detection to find errors in data is that you have natural outliers and natural outliers are not errors.",
                    "label": 0
                },
                {
                    "sent": "They are just values that are outlying but they are correct.",
                    "label": 0
                },
                {
                    "sent": "So you have a vehicle which is 7.4 meters high.",
                    "label": 0
                },
                {
                    "sent": "You of course find that as an outlier and it's pretty hard to tell that teach the machine that this one is not an error, but this one is actually correct and so is this person on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Who was the shortest woman to ever live on Earth?",
                    "label": 0
                },
                {
                    "sent": "She was a 58 centimeters high.",
                    "label": 0
                },
                {
                    "sent": "Clearly, outlier detection finds that value, tells you it is an error, and again you have to find additional evidence to tell whether this is really whether this is an error in the data or just the natural outlier, but this is a very, very hard task to teach the machine.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we start the Q&A session soon, I guess somebody will for sure ask.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but this is just the PD.",
                    "label": 0
                },
                {
                    "sent": "And what is the relevance to other datasets so can take that one right away.",
                    "label": 0
                },
                {
                    "sent": "Actually you can transfer that to any any linked open data set, and as soon as you step out of the comfort zone of transferring a relational database into RDF, you will have these sort of problems.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You see these approaches coming up where people use information extraction from text with reverb on L. Where they just try to get facts from text which is prone to such sorts of error when they try to heuristically complete facts from different sources, or try to heuristically integrate different data sources into one common data set, and as soon as you use those heuristic methods, you are quite likely to have problems with wrong numbers, and then you can use such an approach so it's not limited to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Although we only evaluated it on DVD.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are some ongoing work as I showed you there's this problem of natural outliers which are not errors in the data and it is hard to a machine to understand that.",
                    "label": 0
                },
                {
                    "sent": "But it's not impossible.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can do is you can cross check that value them with other sources and for DB pedia this is actually not so hard because we have the pedia in different language additions.",
                    "label": 0
                },
                {
                    "sent": "So you can just look is that value also existing in other language addition in a similar range and if it is you can say OK then it's probably correct because it's quite unlikely.",
                    "label": 0
                },
                {
                    "sent": "That people put that value in the exact same format in different language additions and then and then the extraction code does the same error on all the language addition that hardly happens.",
                    "label": 0
                },
                {
                    "sent": "But we also look into currently is some preprocessing techniques.",
                    "label": 0
                },
                {
                    "sent": "So you showed.",
                    "label": 0
                },
                {
                    "sent": "This preprocessing really gives you an advantage and currently looking at maybe we can we can squeeze out some outperformance by using more intelligent preprocessing and finding these meaningful subpopulations of the data.",
                    "label": 0
                },
                {
                    "sent": "And one thing which is particularly interesting for us is so far we have looked into these errors manually and then try to pinpoint the things where it goes wrong, but you could actually also do is try to find the text patterns by text patterns induction which tells you this is a typical pattern that the information extraction code cannot handle and this makes it much easier to do actually then pinpoint the errors.",
                    "label": 0
                },
                {
                    "sent": "For example, you could find something like there is there is a number followed by.",
                    "label": 0
                },
                {
                    "sent": "A year in parenthesis and if you find that pattern on many outliers, you can see OK, we probably are not capable of handling that pattern, and this would clearly make it easier for us to pinpoint the errors in the code.",
                    "label": 0
                },
                {
                    "sent": "And actually it could also help identifying natural outliers because natural outliers are rather unlikely to come with such a pattern, which which is prevalent in many data points.",
                    "label": 0
                },
                {
                    "sent": "This is rather not going to happen, so it could also help us with that problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so thanks for listening and I'm now happy to take your questions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, thanks for the presentation, was awesome.",
                    "label": 0
                },
                {
                    "sent": "I think you know what I'm going to ask anyways, but first I wanted to comment that probably your student that all the work because it didn't find any mistakes.",
                    "label": 0
                },
                {
                    "sent": "So you said that all the bad stuff was you who did it so.",
                    "label": 0
                },
                {
                    "sent": "But the question is how is this going to feed back into DB pedia, right?",
                    "label": 0
                },
                {
                    "sent": "So are you guys going to have an extractor that is going to be contributed as open source so that now we can?",
                    "label": 0
                },
                {
                    "sent": "Flag the wrong triples in a way so that somebody can review them at least, and the second point is you said that sometimes there's nothing we can do when the data is wrong in Wikipedia, and that's true.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we can't do, but I think most times we can.",
                    "label": 0
                },
                {
                    "sent": "So Wikipedia has these bots right that so you can have like a wrong number bot or outlier detection bot where you go and flag pages and say, hey, this looks strange and I.",
                    "label": 0
                },
                {
                    "sent": "A human could go there and put another flag that says no, it's strange because it is an actual outlier.",
                    "label": 0
                },
                {
                    "sent": "There's nothing we can.",
                    "label": 0
                },
                {
                    "sent": "We should touch this one again, right?",
                    "label": 0
                },
                {
                    "sent": "So I just general question is what are you guys trying to do so that this feeds back into the beta increases equality?",
                    "label": 0
                },
                {
                    "sent": "OK, so there are actually quite a few things you could do, so one is you can just collect that data and say OK, here is another data set that has that we're somehow disputed and we have some confidence values in that.",
                    "label": 0
                },
                {
                    "sent": "I also talked to Magnus new this morning with this Petra Project which collects.",
                    "label": 0
                },
                {
                    "sent": "Different possible errors from different sources and tries to aggregate them, but actually the ultimate goal would be to use that to do as a means to debug the code, because it really can help us, especially once we got this thing with pattern induction, it can really help us pinpointing their positions in the code.",
                    "label": 0
                },
                {
                    "sent": "Saying this is something going wrong, and yeah, I mean this would also be the most elegant way of dealing with the problem with eliminating the problem in the code in the end.",
                    "label": 0
                },
                {
                    "sent": "But you can also say OK, we can also publish datasets.",
                    "label": 0
                },
                {
                    "sent": "Say this is something with the dispute effects and then you can decide whether you want the.",
                    "label": 0
                },
                {
                    "sent": "The raw extraction or whether you want the disputed facts ruled out under a certain threshold.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and this bot thing.",
                    "label": 0
                },
                {
                    "sent": "Clearly it's an interesting thing, but I don't know whether whether we would rather bother the Wikipedia community, but just reading random like millions of requests.",
                    "label": 0
                },
                {
                    "sent": "Please double check that value and then it's most of them as natural outliers and people who got these guys again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is correct.",
                    "label": 0
                },
                {
                    "sent": "This city is that large command give me a break.",
                    "label": 0
                },
                {
                    "sent": "I don't know what's going to happen, but you can think of that, of course.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the presentation.",
                    "label": 0
                },
                {
                    "sent": "How can I have three straightforward questions?",
                    "label": 0
                },
                {
                    "sent": "I would think so.",
                    "label": 0
                },
                {
                    "sent": "You evaluated on three or on single properties.",
                    "label": 0
                },
                {
                    "sent": "How about the cases in which the answer to or the Valley answer to a question can be spread over multiple properties?",
                    "label": 0
                },
                {
                    "sent": "I was thinking about location headquarters of a company.",
                    "label": 0
                },
                {
                    "sent": "I don't have any numerical example in mind, but they are at least five or six.",
                    "label": 0
                },
                {
                    "sent": "Properties in DV pedia.",
                    "label": 0
                },
                {
                    "sent": "You could use to to find out that information.",
                    "label": 0
                },
                {
                    "sent": "So could I guess your approach could be expanded to cover those cases, right?",
                    "label": 0
                },
                {
                    "sent": "So currently they are treated in isolation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "But if they are mapped to the same same property in ontology then then it's fine.",
                    "label": 0
                },
                {
                    "sent": "If they are not, then they're treated individually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "My second question is at the beginning, you gave this examples about the different Heights of the sportsman.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You actually don't solve that problem, right?",
                    "label": 0
                },
                {
                    "sent": "I mean the different way in each week and writes down the height you actually assume you have a value for it.",
                    "label": 0
                },
                {
                    "sent": "Actually tried to handle it, so I mean we cannot forbid the Wikipedia community and tell them.",
                    "label": 0
                },
                {
                    "sent": "But you can use that format, but you must not use the other format.",
                    "label": 0
                },
                {
                    "sent": "That's never going to happen.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the goal is to find a way to deal with that and expand the extraction code insofar that it can cover all those cases.",
                    "label": 0
                },
                {
                    "sent": "OK, and you would do that with this type of patterns that you meant.",
                    "label": 0
                },
                {
                    "sent": "For example, you're just trying to pinpoint.",
                    "label": 0
                },
                {
                    "sent": "The positions of the patterns that we cannot handle correctly.",
                    "label": 0
                },
                {
                    "sent": "I have two questions, so the first one is related to the code is available for this or not and the next one is about the scalability because if I want to apply this approach to some large data sets, for example giono are linked CJ.",
                    "label": 0
                },
                {
                    "sent": "So both of these datasets are over 20 billion triples.",
                    "label": 0
                },
                {
                    "sent": "So how you are approached you know we deal with such datasets.",
                    "label": 0
                },
                {
                    "sent": "Yes, some of these are actually quite fast like this this medium.",
                    "label": 0
                },
                {
                    "sent": "So deviation can be computed quite quickly, it just requires sorting the data once and then you're done and you can do it in a relational database.",
                    "label": 0
                },
                {
                    "sent": "Other approach depends, I mean KDE is not so fast and we saw that the fast approximation of KD is not accurate enough.",
                    "label": 0
                },
                {
                    "sent": "So it clearly depends on the outlier detection method and some are really scalable, some are not at all.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I forgot about the first question now.",
                    "label": 0
                },
                {
                    "sent": "The code is not available at the moment, but I could still push my student even though he's in the US to just publish it yet.",
                    "label": 0
                },
                {
                    "sent": "So just a quick question.",
                    "label": 0
                },
                {
                    "sent": "If you're crazy enough to and you want to plot on a map the 100 of thousands of populated place of DB pedia, you will end up plotting a lot of things in the sea.",
                    "label": 0
                },
                {
                    "sent": "It turns out that Joe coordinates into Pedia are often incorrect - omitted when you saw famous for extra extra extra.",
                    "label": 0
                },
                {
                    "sent": "I'm just curious, is it an error that you have encountered in your sample?",
                    "label": 0
                },
                {
                    "sent": "And if not, why?",
                    "label": 0
                },
                {
                    "sent": "A country in the sample, sorry.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "So, so there are a lot of info boxes, for example where you will have two latitude, one we have - an, not the other one.",
                    "label": 0
                },
                {
                    "sent": "So there is an incorrect value.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, The thing is that the Geo coordinates are used expected using a different extractor on DVD.",
                    "label": 0
                },
                {
                    "sent": "It's not the infobox extractor, it's another mechanism that creates those coordinates.",
                    "label": 0
                },
                {
                    "sent": "So actually just not not affected by what you put in the infobox.",
                    "label": 0
                },
                {
                    "sent": "Generally it's probably possible to find these with the same methods because you find certain areas.",
                    "label": 0
                },
                {
                    "sent": "Of the of coordinates, there should be nothing.",
                    "label": 0
                },
                {
                    "sent": "There should be no place there.",
                    "label": 0
                },
                {
                    "sent": "The thing is, we've looked here only at a single dimensional approaches, and for that 2 dimensional approach would probably work very much better.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you very much for your questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you hiker for your talk.",
                    "label": 0
                },
                {
                    "sent": "Trying not to make this an outlier.",
                    "label": 0
                },
                {
                    "sent": "We will finish here by thanking hiker.",
                    "label": 0
                },
                {
                    "sent": "Thank you answer.",
                    "label": 0
                }
            ]
        }
    }
}