{
    "id": "z6u5jmodggnqw6k7pxlpugr63yra6zui",
    "title": "Introduction to Statistical Machine Learning",
    "info": {
        "author": [
            "Marcus Hutter, IDSIA"
        ],
        "published": "March 11, 2008",
        "recorded": "March 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/mlss08au_hutter_isml/",
    "segmentation": [
        [
            "OK, I'm I will give.",
            "A brief, actually less than lesson, introduction and overview over statistical machine learning.",
            "What the other courses will cover in much more depth and detail so it's.",
            "For those who have no idea about machine learning, you get some big picture, but probably you understand only 10% of the stuff on it.",
            "For those who already took a course on machine learning, it's maybe serves as a refreshment."
        ],
        [
            "OK, so.",
            "First, I will give a mini overview.",
            "Then linear methods for regression.",
            "That sounds pretty boring.",
            "Good old fashioned statistics, but if you don't master that, forget about the rest.",
            "And they are more powerful than you think of.",
            "Nonlinear methods for regression.",
            "Then if you have complex problems like in real life, you need to select models and assess models.",
            "You have to deal with large problems.",
            "And so far it's more less ID, so independent, identically distributed data.",
            "Their various other kinds of supervised learning.",
            "There's also unsupervised learning.",
            "Sequential learning or interactive setting, it's reinforcement learning.",
            "I will explain what it is and then."
        ],
        [
            "The summary.",
            "So first we could ask.",
            "I mean why do we need machine learning?",
            "Why do we need to learn?",
            "Some related fields, applications of machine learning, three shot.",
            "Supervised unsupervised reinforcement learning.",
            "Some other dish automise many introduction to probabilities but.",
            "Yeah, best.",
            "If most of the things are new on this slide, best in or in the evening.",
            "Meet some wiki page about probabilities.",
            "If you don't know this stuff.",
            "OK."
        ],
        [
            "So what is machine learning?",
            "So you could define it as machine learning is concerned with the development of algorithms and techniques that allow computers to learn.",
            "OK, that's pretty obvious.",
            "There's machine, and there's learn, so it should be about machines they learn.",
            "In this context, it's the process of gaining understanding and by constructing models of observed data with intention to use them for normally predictions.",
            "The various related fields.",
            "There's artificial intelligence.",
            "Some people think that machine learnings are part of AI.",
            "Some people think it's different.",
            "Both want to develop smart algorithms.",
            "Their statistics, something that machine learning, such as some obscure branch of statistics.",
            "I think, I believe it's quite different from statistics.",
            "Others think it's the same.",
            "But if you look at the journals and publications, it's hard to get a statistics paper in the Machine Learning Journal and machine learning paper and statistics Journal, so there must be some difference and machine learning is more concerned with algorithms.",
            "Um and statistics more with you know best estimators and so on, but there's a large degree of overlap data mining.",
            "You have a lot of data and you need to process it somehow.",
            "And of course there's a continuum from data mining to machine learning.",
            "OK, and of course computer science because machine learning wants to develop algorithms and in computer science a lot of.",
            "Nice algorithms, efficient algorithms for complex problems have been developed and many of them can be used also."
        ],
        [
            "Machine learning.",
            "OK, so where should we learn?",
            "The first, I mean, there's some things we don't need to learn, for instance, to calculate payroll.",
            "I mean if you can add numbers and memory, multiply and you know the rules, you just do it.",
            "You don't need to learn from past payroll statements.",
            "So learning is used when human expertise does not exist.",
            "Like I mean robot navigating on Mars.",
            "Humans are unable to explain their expertise like in speech recognition.",
            "I mean we all.",
            "Are pretty good at recognizing speech, but it's very hard to write formal models explicit models down.",
            "So what you do is you write a model class down and then you train.",
            "The model class, the parameters in the model class.",
            "Um, if solutions change in time.",
            "I mean, you develop a solution to a problem and to model problem already changed and you have to tweak the model again and it's very time consuming and expensive, so it's better if the computer does it by himself.",
            "Um, OK. Or what also happen happens?",
            "Have a generic problem.",
            "Class.",
            "But every user, sort of.",
            "For every user, the problem are a little bit different, so you so you develop a general system and then it adapts to the specific user needs by itself.",
            "For instance spam classification.",
            "I mean, every user has different feelings about what is spam and what is not.",
            "So one example, it's easy to write a program that learns, for instance, to play checkers.",
            "That was actually one of the first successes of AI or machine learning.",
            "But the word existed there times.",
            "So we're in pigment was spent later but in checkers and big game, and these systems were sort of blank sheets and they just played against itself and learned to play on a Masters level."
        ],
        [
            "Handwritten character recognition is another classical example for machine learning.",
            "I mean you have these characters and like a sip code on your envelope.",
            "And you want to recognize them by a machine.",
            "And I mean you could try to write for every character some algorithm who detected all kinds of variants of ones and fours or so, but it's easier to just give a training example.",
            "So you give these handwritten numbers, then you give the labels so some human finds the label 721 to them.",
            "I mean, human can do that easily, and then a program that learns the pattern and generalizes to future characters."
        ],
        [
            "So here's some applications.",
            "Natural language processing.",
            "Search engine search engines like Google Use for everybody so.",
            "It's the best thing to do here.",
            "If you want to go to the summer school.",
            "If you want to show up at Google later.",
            "Medical diagnosis detecting credit card fraud that actually wants to me.",
            "So I was living in Switzerland and moved to Australia flu.",
            "We're Dubai bought something small in Dubai and then I bought a fridge in Australia and the system through outlook.",
            "I mean this guy living in Switzerland with us with credit card and then he buys a fridge in Australia.",
            "That is a little bit strange and they thought or the computer thought the credit card is stolen and blocked it.",
            "So there's also smart algorithms.",
            "Machine learning algorithms behind.",
            "And these kinds of things.",
            "I mean, in this case he was wrong, but.",
            "OK, stock market analysis.",
            "Bioinformatics with these microarrays dedicating huge amounts of data now and no human can really.",
            "I mean look through this data or interpret them.",
            "So all this is pre processed by data mining algorithms or machine learning algorithms.",
            "Speech in handwritten character recognition is already mentioned.",
            "My prediction is that speech recognition will be the next revolution, so in five, I mean, they're already pretty good systems out there, and some authors really write their books by using this speech recognition systems, But I think it's five years or so every computer will be equipped with such as software and you will use it on a daily basis.",
            "Object recognition in computer vision.",
            "Gameplaying we already had."
        ],
        [
            "Robot locomotion, locomotion, and others.",
            "OK, some fundamental types of learning.",
            "So there's supervised learning, so that's probably the coarsest and most important classification.",
            "Supervised learning is where you know the answer for your training set.",
            "For instance, in the handwritten character recognition you have your feature, which is the input and you have your output, which is then the character zero to 9.",
            "So that's a classification problem, or you have a regression problem where you want to infer a relation between input and real value outputs.",
            "Unsupervised learning is.",
            "You could define it as you don't know what you want.",
            "So.",
            "It's about understanding data, sort of as an intermediate step to doing, then something with this understanding.",
            "So for instance, you have data points playing or in a high dimensional space and their cluster.",
            "And by I see you have these clusters and you want an algorithm who detects maybe where the clusters are, the number of clusters, the shape of the clusters, and so on.",
            "And related density estimation OK. And then there's reinforcement learning, which is in between supervised and unsupervised learning.",
            "It's usually used in the agent framework and agent, like a robot act and observes, and occasionally gets reward for his actions, which means our positive reward.",
            "If it does something good and the negative reward punishment if it does something bad.",
            "Robot walking around, falling down the staircase is negative reward.",
            "Finding the power block, getting recharged positively about here.",
            "Or if you play games, if you win the game.",
            "If the agent with the games it gets rewarded.",
            "If it loses the games that get punished, and from this very scarce.",
            "Feedback the agent should learn, and that's the field of reinforcement learning.",
            "And there are others which are sort of in between semi supervised learning, quite fashionable in recent days, something in between, supervised and unsupervised learning where part of the data is labeled active learning.",
            "Sort of a limited form of reinforcement learning."
        ],
        [
            "OK, OK. What is supervised learning?",
            "You're in for a rule, and the rule is use this rule for.",
            "Predicting future outputs.",
            "From inputs or you can use it for knowledge extraction to just understand what is going on.",
            "Or an for compressing your data.",
            "Or outlier detections another application."
        ],
        [
            "So here, so I said.",
            "Supervised learning is either classification or regression that's roughly true, and here's a classification problem.",
            "Uh, another credit card problem?",
            "You have a new customer and you want to evaluate them whether the customer is worth am having a credit card or which type of credit card and you have two.",
            "Points of information.",
            "You have an income and the average savings of the customer and if the customer has a high income and a lot of savings, there's a low risk customer.",
            "But if either the income is lower, the savings are low.",
            "I mean often you even are often negative side here.",
            "Then it's a high risk customer and you could classify so you have some data points here from past experience, say these customers overdraw through their credit card and these customers were all fine.",
            "And then you could.",
            "Think of OK, maybe there's this boundary here.",
            "Which classifieds them between low risk and high risk?",
            "And in this case they discriminate would be for instance if income is high and savings are high then low risk else high risk.",
            "So there's a classification rule in this case."
        ],
        [
            "And regression is where you have some input.",
            "X for instance, the H of a house, and here is the average price of this House, say in a given suburb, and you want to now you have a house with this age you have no data point and you want to.",
            "Determine what is the reasonable price.",
            "So here your data points.",
            "They look like maybe they're lying on a straight line, so you make a straight line regression which minimizes some error, and then you can determine the price for out of this age.",
            "OK, both are class and both are supervised learning problems."
        ],
        [
            "OK unsupervised, you have no output.",
            "You want to learn what normally happens.",
            "For instance, clustering.",
            "Um, I have an example later.",
            "OK."
        ],
        [
            "OK, so in reinforcement learning.",
            "As already mentioned, you want to learn a policy of an agent which acts with environment.",
            "So you want to determine a sequence of outputs which is good in the sense of maximizing reward.",
            "You have no supervisor which tells the agent.",
            "What to do or what would have been the right action, but only whether the action is.",
            "Good or bad and also the reward can be delayed and that makes a huge difference in the design of algorithms and analysis of the problem because you have the credit assignment problem and.",
            "I should be here, but comes later and you run into the exploration versus exploitation problem.",
            "Where the agent always has to consider.",
            "Should I exploit what I know and maximize greedily my reward in the next time instance?",
            "Or should I learn a little bit more?",
            "Walk around.",
            "Explore some possibilities, even if the agent knows that this does not lead to good results immediately, but learning means that it may lead to higher reward in the long run.",
            "I mean, you're all sitting here and learning something.",
            "Some of you probably, in the hope of getting better, are good later.",
            "Good job at earning some more and then.",
            "Catching up with these last years in quotation mark.",
            "I mean, some of you learn because they have fun in learning, but.",
            "Yeah.",
            "OK. No, it's quite interesting, but I mean why do children like to play?",
            "They like to play because biology built it in and it's a good thing because it maximizes long-term reward and you.",
            "Most of you would like to learn because of genetical biological reasons here, because it's good for survival or in former times, is what goes for survival in the long run.",
            "And nowadays it's maybe good for a good salary."
        ],
        [
            "OK, so so some more ways you can classify machine learning.",
            "OK, rip this slide out of other slides so the blue and black has no meaning.",
            "OK, the blue means.",
            "That's the scope.",
            "The typical scope of my lectures, yeah?",
            "Research and not of this one here.",
            "OK, so there's the.",
            "The classification between US statistical and machine learning approach to intelligent data analysis and sort of the good old fashioned AI.",
            "So go find means good old fashioned AI based approach which is more knowledge based, logic orientated, inferring rules and so on.",
            "OK. Then I mean this is very important and.",
            "You should remember this line goes from left to right.",
            "I'm, I mean often you're concerned with an induction problem.",
            "You have some data and you want to determine a good model for your data, but why should you care?",
            "Or why should somebody care about this good model?",
            "In which sense it is good because you use this model typically later for doing predictions.",
            "So you try all your life.",
            "Geology to build weather.",
            "Evolution models.",
            "And you do that all your life, but you do that because somebody else uses these models to make predictions.",
            "And then you hear the weather forecast.",
            "I don't know, hopefully 90% sun today.",
            "But why should we care about doing predictions?",
            "Or why should somebody care?",
            "Because later these predictions are used for doing some decisions.",
            "I mean, you decide whether you go to the beach or take your sunglasses or take an umbrella or so.",
            "And ultimately, if this decissions influence your environment, I mean taking the umbrella or some taking your sunglasses does not influence the weather.",
            "Except for the butterfly effect, maybe.",
            "But playing chess, you move influences what your opponent has significantly, so if.",
            "You actually influence the environment.",
            "Then you again in the reinforcement learning.",
            "Or the active learning setup or reinforcement learning setup.",
            "And then these actions become really important.",
            "To to to consider them seriously.",
            "OK, so this is natural path from induction to prediction to decision.",
            "And then to action.",
            "OK regression classification.",
            "I already mentioned most of machine learning.",
            "Is sort of IID independent, identically distributed data?",
            "Um, first because many data are like that or assumed like that an second 'cause it's much easier to analyze than sequential data.",
            "Generally non IID data.",
            "Online versus offline.",
            "Oh, I'm sure somebody else will explain it.",
            "Um?",
            "Let's see parametric versus nonparametric.",
            "It's also an important difference.",
            "Often you have a model class you have a fixed number of parameters and you just want to learn them.",
            "Then you have a parametric problem or there's no terminology problem.",
            "Nonparametric typically means that you still have parameters, but the number of parameters can be flexible.",
            "So for instance, you have data and you want to.",
            "Find a polynomial with.",
            "Go through the data, but you don't fix the degree of the polynomial.",
            "So then, depending on the degree you have more or less parameters, so that is called nonparametric.",
            "Or if you have no parameters at all like K nearest neighbor, then it's normally called model free.",
            "But for me it's also nonparametric because you don't have parameters but.",
            "OK, let's try the medical terminology.",
            "Then you could be more concerned about the conceptual mathematical issues, which sort of the statistical side or the computational issues.",
            "Which is more of the machine learning side.",
            "But this is really, I mean just very soft.",
            "This distinction.",
            "And also more exact or principled or holistic things.",
            "And this supervisors unsupervised versus reinforcement learning already mentioned.",
            "So it's good if you have a new problem.",
            "That first you try to characterize it with respect to.",
            "These things and then maybe you can look up in some table and maybe there already exists a good algorithm, or at least you have narrowed down.",
            "I'm better search."
        ],
        [
            "OK, I'm so now a brief introduction to probabilities.",
            "It's just some terminology which is used throughout all these lectures, I mean.",
            "Sample space I hope you know most of these things I mean.",
            ".06 sided die and it could be 123456 then.",
            "This is sample space.",
            "Then events are subsets of the sample space could be 246.",
            "It's even the set or the event that ought comes up.",
            "Then it's 135.",
            "Then the probability is just the long run.",
            "OK, the very nice interpretation is the long run relative frequency.",
            "Outcomes so the probability of six is 1 / 6.",
            "And probably even allow this 1/2.",
            "So six or any other number here is an outcome in it must be Omega.",
            "Conditional probability of a given B is the probability of A&B divided by the probability of B either by definition or.",
            "I mean if you look at the frequency picture, I mean it's obvious.",
            "So for instance in this case the probability.",
            "There are six comes up.",
            "If you know that it was even, it's just one 6 / 1/2 is 1/3 OK and here the general Kolmogorov axioms of probability theory probabilities are between zero and one.",
            "This is the definition of conditional probabilities and this rule you know if A is.",
            "Disjunct from B, then this is absent, and then probability of a Union BS, A+B, and in the more general case you have this rule.",
            "So all other statements about probabilities can be or are derived from this general axioms.",
            "So it's actually that's it."
        ],
        [
            "OK, next slide more probability jargon so the past slide everybody probably knows.",
            "Here's a little bit of jargon so.",
            "Think of a coin now.",
            "Has had entails.",
            "You usually map them to number, so 01 for instance.",
            "And now think of a biased coin, or better, maybe not think of a coin but just a process which can be.",
            "Two binary outcomes.",
            "And is independent identically distributed, but you don't know the biased at around that error can be somewhere between zero and one for the fair coin it would be 1/2, so the likelihood to see a sequence 1101 of outcomes given the bias data is then of course simply theater times theater times Theta times 1 minus Theta, so the probability of a one is Theta and three times and the probability of zero is 1 minus Theta.",
            "And that's called likelihood.",
            "The maximum likelihood estimator now for this data is just maximizing this probability.",
            "So the idea is you take a model and you see the outcome and if the model.",
            "Is a very bad model of your reality.",
            "Then this outcome has very small probability.",
            "So why is versa?",
            "And if the outcome is very likely, then this model contains probably or hopefully some truth, so the maximum likelihood estimate would be here three 4th, so this is a beta distribution which has a maximum of three force OK.",
            "So I'm this.",
            "Works often bail for small model classes, but if you have large model classes, you get into overfitting problem and one way to get to solve this problem is to take a Bayesian approach and where you define a prior over tater so you have some prior belief in these parameters.",
            "For instance, if you see a coin, you would say with 98% probability it is fair and then maybe distribute the other 2%.",
            "Among the other values of Theta, or if you don't know anything about your underlying process, then maybe you take a uniform prior.",
            "So if you're in different than for instance, a uniform prior might be good.",
            "OK, so then you can compute this.",
            "Strange quantity called evidence which is hard to interpret.",
            "So it's sort of some say it's letting the data speak for yourself, so you have the probability given the model.",
            "But you will trust multiply little prior of the model and and some or in this case integrate over it and then you get the probability of the data itself.",
            "Which is in, in this case 120.",
            "These numbers are typically very small or very large and have no good direct interpretation, but nevertheless it's sort of the key quantity.",
            "If you can compute this, then the rest often easily follows and.",
            "It's needed for other things.",
            "So for instance, the posterior probability.",
            "This is what you really want.",
            "You have a prior belief, then you see your data and then you want to update your belief.",
            "Now you have seen this sequence 1101 after having no idea about the coin.",
            "Let's assume and then you ask what should I believe afterwards?",
            "So you can compute it with base rule.",
            "The probability of a given B is probably be given a * P of a / P of B.",
            "And like everything you need to get just cater to three 1 minus Theta with some coefficient which are present here.",
            "Which has a maximum somewhere and the maximum is called the maximum posteriori estimator, and in this case it gives the same answer as the frequent.",
            "The estimated 3/4, but it would change if you use a different prior here.",
            "And indeed, the constant price is not really there.",
            "Best one.",
            "OK.",
            "So you could think of more, say, abstractly, philosophically.",
            "I mean this theater is.",
            "What is that?",
            "I mean, you probably regard it as a property of the coin, but can you really sort of see this property?",
            "I mean, the only way this property is interesting is that if you flip the coin in the future, this property influences the future outcomes.",
            "So what you do with the theater?",
            "So with this model, this is what I explained before.",
            "So this was the induction step.",
            "What you do with this model later you want to use it for prediction.",
            "So why don't use it directly for prediction?",
            "So what is the probability that the next outcome is 1 given our past outcomes?",
            "I mean this is what we really want to do.",
            "We want to predict the future.",
            "All future events, so this conditional probability you can either compute from the posterior.",
            "Or directly from a ratio of evidences.",
            "Becausw?",
            "I mean you just use the definition.",
            "And then you see it's just this ratio of two evidences, so you see.",
            "How they can be used in their answers to 3rd?",
            "Which is actually more reasonable than the three 4th, but I will not go into this now.",
            "Expectations, I hope you all know I'm in the expectation of a function is just a function times it's probability and then summed over all over the sample space.",
            "So here the expected value of Theta because one problem with the posterior is that it's a highly complicated object.",
            "I mean this is a function here because we have many parameters.",
            "Then I mean it's nice to have this answer, but what do you do with it?",
            "You typically need summaries of this answer, and one summary is to compute either the maximum.",
            "So that's a map estimator.",
            "Or you compute the mean.",
            "And this would be to 3rd in this case.",
            "If you want an estimate of the accuracy.",
            "Of your estimation, then you often use the variance which is defined in this way.",
            "And then you know there's a distinction between probabilities and probability densities.",
            "So it's just.",
            "I mean, you take the probability of a small small interval divided by the length and let the limit goes to 0.",
            "I mean, that's not not most naive way to their kind of density.",
            "OKO I'm running late, it's.",
            "Time for coffee break now.",
            "So we will always have the first coffee break in the morning.",
            "Will be some real coffee break.",
            "Down there, get your coffee and tea.",
            "Please be back in time so we even if you run late.",
            "Chris, you should have stopped me.",
            "We started even if you run late, we start in time.",
            "And the second break is a smaller I mean, the time is the same, but you know there will be only instant coffee and so on in the same in the afternoon.",
            "OK, see you back in 10 minutes."
        ],
        [
            "Yeah, a bunch of.",
            "Methods.",
            "And most of them will be explained in detail by the other lectures.",
            "So if you can't follow one, no problem.",
            "We get mostly a full lecture about them anyway.",
            "Later OK, linear methods for regression I already mentioned.",
            "It doesn't sound very exciting to fit a line through data, but it's really, really important and it's more powerful than you think of.",
            "Some methods for regularization, linear methods for classification, linear basis, functional regression, and this is the reason why they are so important.",
            "For instance, splines, facelets.",
            "They're all linear methods for regression.",
            "Kernel methods local smoothing.",
            "I."
        ],
        [
            "Yeah.",
            "OK.",
            "So let's start.",
            "So that's the classical.",
            "A classic problem or regression problem.",
            "You have some input feature vector X in some D dimensional space, so you have the components X one up to XD.",
            "Typically you add another component is 0 and you said it identically to one that is just to make the math a little bit easier, then you have a response valued.",
            "Which is often noisy.",
            "And you want to infer a relation between the input variables, the features and the output variable.",
            "Um?",
            "OK, so and if the relation is linear or you assume this relation is linear then the most general linear function is this here, so it's just these coefficients W 0 up to WD.",
            "So remember, X0 is just one.",
            "OK, so now we have data so there are pairs of features or feature vectors and labels, say N. And.",
            "Another thing you need is a loss function.",
            "If you don't specify, then it's often implicit and often quadratic, so you look for the.",
            "The real outcome here why I?",
            "And you compare it with your prediction.",
            "Take a square difference, for instance, and some overall data points.",
            "That is your loss.",
            "And of course you want to minimize your loss, so you look for the W. Which minimizes this.",
            "And here's an example, so you appear X, one X2.",
            "You have very noisy data points and this is the best plane.",
            "So the least square playing through the data points.",
            "And you can use this plane then for making predictions for other data points.",
            "OK. Yeah, here was for example the person's weight as a function of its age and height.",
            "Probably a local piece only for adults, because I mean it should normally be highly nonlinear."
        ],
        [
            "OK um.",
            "Problem is.",
            "If you have a lot of features.",
            "For instance, in spam classification, I mean there's a classification problem now, but the features are typically diverts and there are 10 thousands of words.",
            "Use a linear model or something related.",
            "You will totally overfit.",
            "And your problem, which would lead to bad prediction.",
            "So if these very large compared to the number of data points then you get the overfitting problem.",
            "So there are many ways to solve this problem and one is to identify a small subset of your features which you lose use.",
            "So there is subset selection, so I assume you want to choose K out of the features and you want to choose them in such a way that they minimize the least square error.",
            "Um?",
            "OK, that's all for now.",
            "If you know the K. Then you're sort of safe if you also want to infer that K, then you need to do some proper model selection account."
        ],
        [
            "Later.",
            "OK, another way would be coefficient shrinkage.",
            "I mean filtering out useless features is the same as setting the coefficients to 0.",
            "So you could think of being less drastically and just shrink these coefficients and you can do that by adding to the loss penalty, which increases with increasing rate.",
            "If you use the square norm then you get rich regression and the small coefficients shrink.",
            "If you use the one norm looks like a minor change.",
            "But then you get socalled method called Lasso.",
            "Then also this method will set some coefficients to 0.",
            "Like in the previous method, but Additionally shrink the other coefficients.",
            "And then there's Bayesian linear regression.",
            "Bear is always the same story you choose some prior over the W. And for instance, I mean if you have a Gaussian modeling a Gaussian prior then introduces exactly too rich regression.",
            "But you can do choose other priors and.",
            "So you choose your prior and.",
            "You have your sampling model.",
            "Then you compute your posterior and you take for instance the map estimator and then also regularize your problem."
        ],
        [
            "OK.",
            "So that was linear regression.",
            "So.",
            "One way to solve classification is to reduce it to a regression problem.",
            "And for instance, OK, take this standard example again, spam versus not spam.",
            "So you have your bag of words, model or whatever, and your labels, the labels you transform them to say minus one or one.",
            "401 but here, minus one to one is better.",
            "And then you regard the label which is now minus one, and one is a real number becausw minus one and one are real numbers.",
            "And then you have a regression problem.",
            "Apart from the problem that the output now is a real number, which makes no sense, it should be minus one or one.",
            "But you simply say if the output so the value of the function is positive, then you label it as one.",
            "So non spam.",
            "If it's negative you label as spam.",
            "OK. You can do that, but often what you want in a classification problem or can afford is rather than out putting, you know this new example, spam or not, you wanted more graded and the algorithm should tell you how confident it is.",
            "So give a probability estimate.",
            "And we have now this real valued function, so it's very, very positive.",
            "Looks like that the algorithm is very sure that it's non spam in analog for negative.",
            "So if you can transform the real numbers because this function is linear goes from minus Infinity to Infinity, we have to transform that somehow to the interval zero and one.",
            "And you can do that with this log odds transformation.",
            "So assume P of Y equal 1 is 1, then PA y = 0 is zero.",
            "They have 1 / 0 is Infinity and the logarithm of Infinity is Infinity OK, and otherwise you have 0 divided by Infinity and the logarithm then is minus Infinity.",
            "So this function transforms the interval 01 to the whole range of real numbers.",
            "So if you use it inversely, you get from this real number of probability in this interval.",
            "So it's sort of a. Sigmoid function, which looks like.",
            "This year OK?",
            "With both methods, there are various problems.",
            "Very roughly, the problem is with data points which are far away from the separating point here.",
            "They have the highest weights.",
            "Then in determining the function, but they should have the lowest rate because they are least critical and.",
            "The various ways to solve this problem.",
            "So and the support vector machine is the most successful one today and you will hear a whole here whole lecture, or at least one lecture.",
            "Probably many about support vector machines.",
            "So it's sort of this is probably the natural historical regression, so those first perception maximum margin hyperplane algorithm and then support vector machines, and there are others like linear discriminant analysis and.",
            "I'm OK this weekend.",
            "OK, so that was binary classification.",
            "And of course you can generalize things, but it's not always obvious or there's not always a unique way to do that to multi class classifications.",
            "So for instance, if here 3 classes like here, you could do a bunch of binary classifiers or directly a classifier which can handle multi classes."
        ],
        [
            "OK, so now so that was the linear stuff and I'll come to this slide.",
            "Why is linear so important?",
            "Are powerful and this is the cause.",
            "If a problem is nonlinear.",
            "You can often make it linear by transforming your feature space.",
            "So OK, I said that often the response is not linear in X.",
            "So what you simply do is you transform your extra 5X.",
            "And the dimension of the target space can even be different, so you have a D dimensional feature vector and you transform it and they go into P dimensional space, which is often or could be much larger than the original space.",
            "And then you assume or hope.",
            "That your problem, your label your.",
            "Your library is now linear in five.",
            "OK, so the most simple.",
            "Transformation is to do nothing, so.",
            "You have a D dimensional feature vector and you just select the ice feature, so it's the identical mapping and you're back to linear regression.",
            "OK, so that's sort of the standard mathematicians choke or first example.",
            "OK, so here's the first interesting one, so consider this problem.",
            "Say if they datapoints.",
            "Like like this.",
            "Well, looks like a parabola.",
            "Definitely not like a line.",
            "OK, that's a new problem.",
            "That's not linear regression anymore, but you can very easily transform it to linear regression if you just say I met my ex to a vector X1 and X2.",
            "So that is my function 5.",
            "And this is X&X squared now.",
            "Actually it should have here.",
            "I mean the one is always hanging around somewhere.",
            "OK.",
            "So then if you draw the picture.",
            "I have another access here.",
            "To the back.",
            "And these data points have also, you know a dimension in this direction and now you can fit very nicely a plane through it.",
            "And then you project everything back and you get.",
            "The best particular fitting for the data.",
            "But just using this embedding trick and using linear regression.",
            "Another problem is or.",
            "Good operation thing is binningen your data points.",
            "You been your problem and you make a piecewise constant regression.",
            "So there's also linear regression where the basis functions are.",
            "Step functions like this, so this is.",
            "5 one and say.",
            "5K.",
            "Or piecewise polynomials, or splines?",
            "They're all linear regression.",
            "So yeah, here's some exam."
        ],
        [
            "Piecewise constant, piecewise linear, continuous piecewise linear, and there's some based basis function for piecewise linear regressor."
        ],
        [
            "Um?",
            "2 dimensional splines here.",
            "So what you essentially do, you have this basis functions here and you have some smooth function you.",
            "Which you can.",
            "Model by averaging or by summing with appropriate weights.",
            "You know these bumps here and there are bumps which are sort of at every places and with various risks.",
            "OK, and on the right is a wavelet basis so.",
            "Wavelets are also linear regression.",
            "Or bracelets also can be reduced to linear regression."
        ],
        [
            "OK, now the linear method is.",
            "Local smoothing and currently regression.",
            "So what you do here is you have your data points.",
            "And you just take a local neighborhood and averaged divide values and then you get the value which gives you the green curve.",
            "This still gives you a very big curve because when you move this window data points pop in and out, so it's even discontinuous.",
            "So what you can do is instead of taking a uniform average, you can average the points in such a way that you take a higher rate for the ones.",
            "Which are, you know, close to your target point or lower right outside.",
            "So for instance, and it is the quadratic kernel here.",
            "And you can always write it in this form.",
            "Your estimating function is.",
            "I'm an average over the Y values and the weight of this average depends on the target point X.",
            "And is characterized by this by this kernel function K of XXI, for instance.",
            "Here it's it's one if X.",
            "If XY if XI is close to X and zero outside, and here it's this quadratic function here."
        ],
        [
            "Yeah, thanks.",
            "OK, I'm yes, OK, another linear method is the regular regularised regression with quadratic loss function and panelization.",
            "So what you do is so this is here.",
            "Your quadratic loss function.",
            "But now you ask so you don't give some model class 4F, so some restricted one likes polynomials, but you ask for any function F, say any mathematical reasonable one.",
            "But if you would just do that then you will get a function which perfectly fits your data.",
            "I mean you will get something like.",
            "This year, whatever.",
            "And it's intuitive, obvious that there is not a good thing to do, and you can show that it also has very poor predictive performance.",
            "So what you do is you penalize wiggly functions.",
            "So you say, for instance, that the curvature, so the second derivative squared should be small over the whole domain.",
            "So you're at this penalty with this parameter Lambda.",
            "So if Lambda is zero, I already draw function, you get any functions through the data.",
            "If Lambda is Infinity.",
            "I mean, this term counts all so the second derivative must be zero, which means that you get a straight line fitting, so you get the original linear regression back.",
            "And if it's somewhere in between.",
            "You get piecewise cubic regression with continuous derivative.",
            "So it's from one data point to the other.",
            "It's a cubic.",
            "But the derivative is continuous.",
            "So it's splined.",
            "Regression.",
            "OK, that are the linear ones.",
            "And.",
            "I think it's a little bit early for the break, so I can't go to the nonlinear ones now."
        ],
        [
            "So.",
            "This basic function trick is very, very powerful, but also has its limitations and then and I would say only then you should go to nonlinear methods.",
            "For instance, like artificial neural networks.",
            "And support vector machines."
        ],
        [
            "And so on.",
            "OK, here's one slide about neural networks, because I think it will not be covered.",
            "In detail by other lectures.",
            "So what you have is you have.",
            "So this is your feature vector X one up to X. D. You have now.",
            "M. Linear functions here.",
            "So you take your access, take a weighted average.",
            "So we have M linear functions.",
            "But then and this is the new element you pipe it through a nonlinear function.",
            "And then you do the same thing in the second layer.",
            "So I mean that would be 1 hidden layer neural network.",
            "Of course there are generalizations.",
            "OK, so and this.",
            "So-called activation functions often have or typically have a sigmoid.",
            "Form and the reason is if you take this linear average, then for some inputs you get huge outputs and they're quite bad.",
            "If you feed them in the next layer.",
            "So what you do is you just found them in a reasonable interval, for instance 01 back.",
            "And then you go to the next layer.",
            "And.",
            "Um?",
            "And I mean, the learning task is then as before.",
            "I mean you have some training examples.",
            "So input and output and you want to learn and the weights.",
            "So you have here set of weights and here set of weights which best fit your data.",
            "So, so you have your outputs Y and your regression function F which depends on this.",
            "Vector W which is W1 and W2 and you for instance you want to minimize the square loss, and this is typically so there is no closed form solution now to this anymore.",
            "Is one of the reasons to avoid nonlinear regression.",
            "Um?",
            "The backpropagation algorithm.",
            "The Classical one, is just the gradient descent method."
        ],
        [
            "OK. OK, here's an example.",
            "The classical example, for instance, of an image processing problem you have here an image.",
            "And you want to detect some features.",
            "So for instance small line segments, or whether there's a corner.",
            "So you take a local neighborhood over Pixel.",
            "And, um.",
            "With these weights I mean you have a linear weighted average.",
            "And then a secret function, and you get a value here.",
            "And you do that for every pixel.",
            "And then you subsample and you know.",
            "Make IT delivery course of your data points.",
            "OK. And I think now is a good time for.",
            "Ending.",
            "Are there any questions so far?",
            "Yes.",
            "In accent.",
            "Um?",
            "No, not linear means linear in the WS, that's the important thing.",
            "I mean, ultimately, I mean you have I mean this linear function.",
            "I mean it's a scalar product between WS and say this feature vector five.",
            "Yeah, so it's also linear in file necessarily.",
            "OK, any other question?",
            "OK, one thing I forgot.",
            "If you're in a cottage with.",
            "Isn't it?",
            "Outlets, but no wireless.",
            "We have a lot of cables, just come here and I will give you some cable."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm I will give.",
                    "label": 0
                },
                {
                    "sent": "A brief, actually less than lesson, introduction and overview over statistical machine learning.",
                    "label": 1
                },
                {
                    "sent": "What the other courses will cover in much more depth and detail so it's.",
                    "label": 0
                },
                {
                    "sent": "For those who have no idea about machine learning, you get some big picture, but probably you understand only 10% of the stuff on it.",
                    "label": 1
                },
                {
                    "sent": "For those who already took a course on machine learning, it's maybe serves as a refreshment.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "First, I will give a mini overview.",
                    "label": 0
                },
                {
                    "sent": "Then linear methods for regression.",
                    "label": 1
                },
                {
                    "sent": "That sounds pretty boring.",
                    "label": 0
                },
                {
                    "sent": "Good old fashioned statistics, but if you don't master that, forget about the rest.",
                    "label": 0
                },
                {
                    "sent": "And they are more powerful than you think of.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear methods for regression.",
                    "label": 0
                },
                {
                    "sent": "Then if you have complex problems like in real life, you need to select models and assess models.",
                    "label": 1
                },
                {
                    "sent": "You have to deal with large problems.",
                    "label": 0
                },
                {
                    "sent": "And so far it's more less ID, so independent, identically distributed data.",
                    "label": 0
                },
                {
                    "sent": "Their various other kinds of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "There's also unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Sequential learning or interactive setting, it's reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I will explain what it is and then.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The summary.",
                    "label": 0
                },
                {
                    "sent": "So first we could ask.",
                    "label": 0
                },
                {
                    "sent": "I mean why do we need machine learning?",
                    "label": 0
                },
                {
                    "sent": "Why do we need to learn?",
                    "label": 0
                },
                {
                    "sent": "Some related fields, applications of machine learning, three shot.",
                    "label": 1
                },
                {
                    "sent": "Supervised unsupervised reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Some other dish automise many introduction to probabilities but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, best.",
                    "label": 0
                },
                {
                    "sent": "If most of the things are new on this slide, best in or in the evening.",
                    "label": 0
                },
                {
                    "sent": "Meet some wiki page about probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you don't know this stuff.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is machine learning?",
                    "label": 1
                },
                {
                    "sent": "So you could define it as machine learning is concerned with the development of algorithms and techniques that allow computers to learn.",
                    "label": 1
                },
                {
                    "sent": "OK, that's pretty obvious.",
                    "label": 0
                },
                {
                    "sent": "There's machine, and there's learn, so it should be about machines they learn.",
                    "label": 0
                },
                {
                    "sent": "In this context, it's the process of gaining understanding and by constructing models of observed data with intention to use them for normally predictions.",
                    "label": 1
                },
                {
                    "sent": "The various related fields.",
                    "label": 0
                },
                {
                    "sent": "There's artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "Some people think that machine learnings are part of AI.",
                    "label": 0
                },
                {
                    "sent": "Some people think it's different.",
                    "label": 0
                },
                {
                    "sent": "Both want to develop smart algorithms.",
                    "label": 0
                },
                {
                    "sent": "Their statistics, something that machine learning, such as some obscure branch of statistics.",
                    "label": 0
                },
                {
                    "sent": "I think, I believe it's quite different from statistics.",
                    "label": 0
                },
                {
                    "sent": "Others think it's the same.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the journals and publications, it's hard to get a statistics paper in the Machine Learning Journal and machine learning paper and statistics Journal, so there must be some difference and machine learning is more concerned with algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um and statistics more with you know best estimators and so on, but there's a large degree of overlap data mining.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of data and you need to process it somehow.",
                    "label": 0
                },
                {
                    "sent": "And of course there's a continuum from data mining to machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, and of course computer science because machine learning wants to develop algorithms and in computer science a lot of.",
                    "label": 0
                },
                {
                    "sent": "Nice algorithms, efficient algorithms for complex problems have been developed and many of them can be used also.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so where should we learn?",
                    "label": 0
                },
                {
                    "sent": "The first, I mean, there's some things we don't need to learn, for instance, to calculate payroll.",
                    "label": 1
                },
                {
                    "sent": "I mean if you can add numbers and memory, multiply and you know the rules, you just do it.",
                    "label": 0
                },
                {
                    "sent": "You don't need to learn from past payroll statements.",
                    "label": 1
                },
                {
                    "sent": "So learning is used when human expertise does not exist.",
                    "label": 1
                },
                {
                    "sent": "Like I mean robot navigating on Mars.",
                    "label": 1
                },
                {
                    "sent": "Humans are unable to explain their expertise like in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "I mean we all.",
                    "label": 0
                },
                {
                    "sent": "Are pretty good at recognizing speech, but it's very hard to write formal models explicit models down.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you write a model class down and then you train.",
                    "label": 0
                },
                {
                    "sent": "The model class, the parameters in the model class.",
                    "label": 0
                },
                {
                    "sent": "Um, if solutions change in time.",
                    "label": 0
                },
                {
                    "sent": "I mean, you develop a solution to a problem and to model problem already changed and you have to tweak the model again and it's very time consuming and expensive, so it's better if the computer does it by himself.",
                    "label": 0
                },
                {
                    "sent": "Um, OK. Or what also happen happens?",
                    "label": 0
                },
                {
                    "sent": "Have a generic problem.",
                    "label": 0
                },
                {
                    "sent": "Class.",
                    "label": 0
                },
                {
                    "sent": "But every user, sort of.",
                    "label": 0
                },
                {
                    "sent": "For every user, the problem are a little bit different, so you so you develop a general system and then it adapts to the specific user needs by itself.",
                    "label": 0
                },
                {
                    "sent": "For instance spam classification.",
                    "label": 1
                },
                {
                    "sent": "I mean, every user has different feelings about what is spam and what is not.",
                    "label": 0
                },
                {
                    "sent": "So one example, it's easy to write a program that learns, for instance, to play checkers.",
                    "label": 0
                },
                {
                    "sent": "That was actually one of the first successes of AI or machine learning.",
                    "label": 0
                },
                {
                    "sent": "But the word existed there times.",
                    "label": 0
                },
                {
                    "sent": "So we're in pigment was spent later but in checkers and big game, and these systems were sort of blank sheets and they just played against itself and learned to play on a Masters level.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Handwritten character recognition is another classical example for machine learning.",
                    "label": 1
                },
                {
                    "sent": "I mean you have these characters and like a sip code on your envelope.",
                    "label": 0
                },
                {
                    "sent": "And you want to recognize them by a machine.",
                    "label": 0
                },
                {
                    "sent": "And I mean you could try to write for every character some algorithm who detected all kinds of variants of ones and fours or so, but it's easier to just give a training example.",
                    "label": 0
                },
                {
                    "sent": "So you give these handwritten numbers, then you give the labels so some human finds the label 721 to them.",
                    "label": 0
                },
                {
                    "sent": "I mean, human can do that easily, and then a program that learns the pattern and generalizes to future characters.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's some applications.",
                    "label": 0
                },
                {
                    "sent": "Natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Search engine search engines like Google Use for everybody so.",
                    "label": 0
                },
                {
                    "sent": "It's the best thing to do here.",
                    "label": 0
                },
                {
                    "sent": "If you want to go to the summer school.",
                    "label": 0
                },
                {
                    "sent": "If you want to show up at Google later.",
                    "label": 0
                },
                {
                    "sent": "Medical diagnosis detecting credit card fraud that actually wants to me.",
                    "label": 1
                },
                {
                    "sent": "So I was living in Switzerland and moved to Australia flu.",
                    "label": 0
                },
                {
                    "sent": "We're Dubai bought something small in Dubai and then I bought a fridge in Australia and the system through outlook.",
                    "label": 0
                },
                {
                    "sent": "I mean this guy living in Switzerland with us with credit card and then he buys a fridge in Australia.",
                    "label": 0
                },
                {
                    "sent": "That is a little bit strange and they thought or the computer thought the credit card is stolen and blocked it.",
                    "label": 0
                },
                {
                    "sent": "So there's also smart algorithms.",
                    "label": 0
                },
                {
                    "sent": "Machine learning algorithms behind.",
                    "label": 0
                },
                {
                    "sent": "And these kinds of things.",
                    "label": 0
                },
                {
                    "sent": "I mean, in this case he was wrong, but.",
                    "label": 0
                },
                {
                    "sent": "OK, stock market analysis.",
                    "label": 0
                },
                {
                    "sent": "Bioinformatics with these microarrays dedicating huge amounts of data now and no human can really.",
                    "label": 0
                },
                {
                    "sent": "I mean look through this data or interpret them.",
                    "label": 0
                },
                {
                    "sent": "So all this is pre processed by data mining algorithms or machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Speech in handwritten character recognition is already mentioned.",
                    "label": 0
                },
                {
                    "sent": "My prediction is that speech recognition will be the next revolution, so in five, I mean, they're already pretty good systems out there, and some authors really write their books by using this speech recognition systems, But I think it's five years or so every computer will be equipped with such as software and you will use it on a daily basis.",
                    "label": 1
                },
                {
                    "sent": "Object recognition in computer vision.",
                    "label": 0
                },
                {
                    "sent": "Gameplaying we already had.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Robot locomotion, locomotion, and others.",
                    "label": 0
                },
                {
                    "sent": "OK, some fundamental types of learning.",
                    "label": 1
                },
                {
                    "sent": "So there's supervised learning, so that's probably the coarsest and most important classification.",
                    "label": 1
                },
                {
                    "sent": "Supervised learning is where you know the answer for your training set.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the handwritten character recognition you have your feature, which is the input and you have your output, which is then the character zero to 9.",
                    "label": 0
                },
                {
                    "sent": "So that's a classification problem, or you have a regression problem where you want to infer a relation between input and real value outputs.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning is.",
                    "label": 0
                },
                {
                    "sent": "You could define it as you don't know what you want.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's about understanding data, sort of as an intermediate step to doing, then something with this understanding.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you have data points playing or in a high dimensional space and their cluster.",
                    "label": 0
                },
                {
                    "sent": "And by I see you have these clusters and you want an algorithm who detects maybe where the clusters are, the number of clusters, the shape of the clusters, and so on.",
                    "label": 1
                },
                {
                    "sent": "And related density estimation OK. And then there's reinforcement learning, which is in between supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "It's usually used in the agent framework and agent, like a robot act and observes, and occasionally gets reward for his actions, which means our positive reward.",
                    "label": 0
                },
                {
                    "sent": "If it does something good and the negative reward punishment if it does something bad.",
                    "label": 0
                },
                {
                    "sent": "Robot walking around, falling down the staircase is negative reward.",
                    "label": 0
                },
                {
                    "sent": "Finding the power block, getting recharged positively about here.",
                    "label": 0
                },
                {
                    "sent": "Or if you play games, if you win the game.",
                    "label": 0
                },
                {
                    "sent": "If the agent with the games it gets rewarded.",
                    "label": 0
                },
                {
                    "sent": "If it loses the games that get punished, and from this very scarce.",
                    "label": 0
                },
                {
                    "sent": "Feedback the agent should learn, and that's the field of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And there are others which are sort of in between semi supervised learning, quite fashionable in recent days, something in between, supervised and unsupervised learning where part of the data is labeled active learning.",
                    "label": 0
                },
                {
                    "sent": "Sort of a limited form of reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, OK. What is supervised learning?",
                    "label": 1
                },
                {
                    "sent": "You're in for a rule, and the rule is use this rule for.",
                    "label": 1
                },
                {
                    "sent": "Predicting future outputs.",
                    "label": 0
                },
                {
                    "sent": "From inputs or you can use it for knowledge extraction to just understand what is going on.",
                    "label": 0
                },
                {
                    "sent": "Or an for compressing your data.",
                    "label": 0
                },
                {
                    "sent": "Or outlier detections another application.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, so I said.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning is either classification or regression that's roughly true, and here's a classification problem.",
                    "label": 0
                },
                {
                    "sent": "Uh, another credit card problem?",
                    "label": 0
                },
                {
                    "sent": "You have a new customer and you want to evaluate them whether the customer is worth am having a credit card or which type of credit card and you have two.",
                    "label": 0
                },
                {
                    "sent": "Points of information.",
                    "label": 0
                },
                {
                    "sent": "You have an income and the average savings of the customer and if the customer has a high income and a lot of savings, there's a low risk customer.",
                    "label": 0
                },
                {
                    "sent": "But if either the income is lower, the savings are low.",
                    "label": 0
                },
                {
                    "sent": "I mean often you even are often negative side here.",
                    "label": 0
                },
                {
                    "sent": "Then it's a high risk customer and you could classify so you have some data points here from past experience, say these customers overdraw through their credit card and these customers were all fine.",
                    "label": 0
                },
                {
                    "sent": "And then you could.",
                    "label": 0
                },
                {
                    "sent": "Think of OK, maybe there's this boundary here.",
                    "label": 0
                },
                {
                    "sent": "Which classifieds them between low risk and high risk?",
                    "label": 0
                },
                {
                    "sent": "And in this case they discriminate would be for instance if income is high and savings are high then low risk else high risk.",
                    "label": 1
                },
                {
                    "sent": "So there's a classification rule in this case.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And regression is where you have some input.",
                    "label": 0
                },
                {
                    "sent": "X for instance, the H of a house, and here is the average price of this House, say in a given suburb, and you want to now you have a house with this age you have no data point and you want to.",
                    "label": 0
                },
                {
                    "sent": "Determine what is the reasonable price.",
                    "label": 0
                },
                {
                    "sent": "So here your data points.",
                    "label": 0
                },
                {
                    "sent": "They look like maybe they're lying on a straight line, so you make a straight line regression which minimizes some error, and then you can determine the price for out of this age.",
                    "label": 0
                },
                {
                    "sent": "OK, both are class and both are supervised learning problems.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK unsupervised, you have no output.",
                    "label": 1
                },
                {
                    "sent": "You want to learn what normally happens.",
                    "label": 1
                },
                {
                    "sent": "For instance, clustering.",
                    "label": 0
                },
                {
                    "sent": "Um, I have an example later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "As already mentioned, you want to learn a policy of an agent which acts with environment.",
                    "label": 0
                },
                {
                    "sent": "So you want to determine a sequence of outputs which is good in the sense of maximizing reward.",
                    "label": 1
                },
                {
                    "sent": "You have no supervisor which tells the agent.",
                    "label": 0
                },
                {
                    "sent": "What to do or what would have been the right action, but only whether the action is.",
                    "label": 0
                },
                {
                    "sent": "Good or bad and also the reward can be delayed and that makes a huge difference in the design of algorithms and analysis of the problem because you have the credit assignment problem and.",
                    "label": 0
                },
                {
                    "sent": "I should be here, but comes later and you run into the exploration versus exploitation problem.",
                    "label": 0
                },
                {
                    "sent": "Where the agent always has to consider.",
                    "label": 0
                },
                {
                    "sent": "Should I exploit what I know and maximize greedily my reward in the next time instance?",
                    "label": 0
                },
                {
                    "sent": "Or should I learn a little bit more?",
                    "label": 0
                },
                {
                    "sent": "Walk around.",
                    "label": 0
                },
                {
                    "sent": "Explore some possibilities, even if the agent knows that this does not lead to good results immediately, but learning means that it may lead to higher reward in the long run.",
                    "label": 0
                },
                {
                    "sent": "I mean, you're all sitting here and learning something.",
                    "label": 0
                },
                {
                    "sent": "Some of you probably, in the hope of getting better, are good later.",
                    "label": 0
                },
                {
                    "sent": "Good job at earning some more and then.",
                    "label": 0
                },
                {
                    "sent": "Catching up with these last years in quotation mark.",
                    "label": 0
                },
                {
                    "sent": "I mean, some of you learn because they have fun in learning, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. No, it's quite interesting, but I mean why do children like to play?",
                    "label": 0
                },
                {
                    "sent": "They like to play because biology built it in and it's a good thing because it maximizes long-term reward and you.",
                    "label": 0
                },
                {
                    "sent": "Most of you would like to learn because of genetical biological reasons here, because it's good for survival or in former times, is what goes for survival in the long run.",
                    "label": 0
                },
                {
                    "sent": "And nowadays it's maybe good for a good salary.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so some more ways you can classify machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, rip this slide out of other slides so the blue and black has no meaning.",
                    "label": 0
                },
                {
                    "sent": "OK, the blue means.",
                    "label": 0
                },
                {
                    "sent": "That's the scope.",
                    "label": 0
                },
                {
                    "sent": "The typical scope of my lectures, yeah?",
                    "label": 1
                },
                {
                    "sent": "Research and not of this one here.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's the.",
                    "label": 0
                },
                {
                    "sent": "The classification between US statistical and machine learning approach to intelligent data analysis and sort of the good old fashioned AI.",
                    "label": 0
                },
                {
                    "sent": "So go find means good old fashioned AI based approach which is more knowledge based, logic orientated, inferring rules and so on.",
                    "label": 0
                },
                {
                    "sent": "OK. Then I mean this is very important and.",
                    "label": 0
                },
                {
                    "sent": "You should remember this line goes from left to right.",
                    "label": 0
                },
                {
                    "sent": "I'm, I mean often you're concerned with an induction problem.",
                    "label": 0
                },
                {
                    "sent": "You have some data and you want to determine a good model for your data, but why should you care?",
                    "label": 0
                },
                {
                    "sent": "Or why should somebody care about this good model?",
                    "label": 0
                },
                {
                    "sent": "In which sense it is good because you use this model typically later for doing predictions.",
                    "label": 0
                },
                {
                    "sent": "So you try all your life.",
                    "label": 0
                },
                {
                    "sent": "Geology to build weather.",
                    "label": 0
                },
                {
                    "sent": "Evolution models.",
                    "label": 0
                },
                {
                    "sent": "And you do that all your life, but you do that because somebody else uses these models to make predictions.",
                    "label": 0
                },
                {
                    "sent": "And then you hear the weather forecast.",
                    "label": 0
                },
                {
                    "sent": "I don't know, hopefully 90% sun today.",
                    "label": 0
                },
                {
                    "sent": "But why should we care about doing predictions?",
                    "label": 0
                },
                {
                    "sent": "Or why should somebody care?",
                    "label": 0
                },
                {
                    "sent": "Because later these predictions are used for doing some decisions.",
                    "label": 0
                },
                {
                    "sent": "I mean, you decide whether you go to the beach or take your sunglasses or take an umbrella or so.",
                    "label": 0
                },
                {
                    "sent": "And ultimately, if this decissions influence your environment, I mean taking the umbrella or some taking your sunglasses does not influence the weather.",
                    "label": 0
                },
                {
                    "sent": "Except for the butterfly effect, maybe.",
                    "label": 0
                },
                {
                    "sent": "But playing chess, you move influences what your opponent has significantly, so if.",
                    "label": 0
                },
                {
                    "sent": "You actually influence the environment.",
                    "label": 0
                },
                {
                    "sent": "Then you again in the reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Or the active learning setup or reinforcement learning setup.",
                    "label": 0
                },
                {
                    "sent": "And then these actions become really important.",
                    "label": 0
                },
                {
                    "sent": "To to to consider them seriously.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is natural path from induction to prediction to decision.",
                    "label": 0
                },
                {
                    "sent": "And then to action.",
                    "label": 0
                },
                {
                    "sent": "OK regression classification.",
                    "label": 1
                },
                {
                    "sent": "I already mentioned most of machine learning.",
                    "label": 1
                },
                {
                    "sent": "Is sort of IID independent, identically distributed data?",
                    "label": 0
                },
                {
                    "sent": "Um, first because many data are like that or assumed like that an second 'cause it's much easier to analyze than sequential data.",
                    "label": 0
                },
                {
                    "sent": "Generally non IID data.",
                    "label": 0
                },
                {
                    "sent": "Online versus offline.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sure somebody else will explain it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's see parametric versus nonparametric.",
                    "label": 0
                },
                {
                    "sent": "It's also an important difference.",
                    "label": 0
                },
                {
                    "sent": "Often you have a model class you have a fixed number of parameters and you just want to learn them.",
                    "label": 0
                },
                {
                    "sent": "Then you have a parametric problem or there's no terminology problem.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric typically means that you still have parameters, but the number of parameters can be flexible.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you have data and you want to.",
                    "label": 0
                },
                {
                    "sent": "Find a polynomial with.",
                    "label": 0
                },
                {
                    "sent": "Go through the data, but you don't fix the degree of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "So then, depending on the degree you have more or less parameters, so that is called nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Or if you have no parameters at all like K nearest neighbor, then it's normally called model free.",
                    "label": 0
                },
                {
                    "sent": "But for me it's also nonparametric because you don't have parameters but.",
                    "label": 1
                },
                {
                    "sent": "OK, let's try the medical terminology.",
                    "label": 0
                },
                {
                    "sent": "Then you could be more concerned about the conceptual mathematical issues, which sort of the statistical side or the computational issues.",
                    "label": 0
                },
                {
                    "sent": "Which is more of the machine learning side.",
                    "label": 0
                },
                {
                    "sent": "But this is really, I mean just very soft.",
                    "label": 0
                },
                {
                    "sent": "This distinction.",
                    "label": 0
                },
                {
                    "sent": "And also more exact or principled or holistic things.",
                    "label": 0
                },
                {
                    "sent": "And this supervisors unsupervised versus reinforcement learning already mentioned.",
                    "label": 0
                },
                {
                    "sent": "So it's good if you have a new problem.",
                    "label": 0
                },
                {
                    "sent": "That first you try to characterize it with respect to.",
                    "label": 0
                },
                {
                    "sent": "These things and then maybe you can look up in some table and maybe there already exists a good algorithm, or at least you have narrowed down.",
                    "label": 0
                },
                {
                    "sent": "I'm better search.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm so now a brief introduction to probabilities.",
                    "label": 0
                },
                {
                    "sent": "It's just some terminology which is used throughout all these lectures, I mean.",
                    "label": 1
                },
                {
                    "sent": "Sample space I hope you know most of these things I mean.",
                    "label": 0
                },
                {
                    "sent": ".06 sided die and it could be 123456 then.",
                    "label": 0
                },
                {
                    "sent": "This is sample space.",
                    "label": 0
                },
                {
                    "sent": "Then events are subsets of the sample space could be 246.",
                    "label": 1
                },
                {
                    "sent": "It's even the set or the event that ought comes up.",
                    "label": 0
                },
                {
                    "sent": "Then it's 135.",
                    "label": 0
                },
                {
                    "sent": "Then the probability is just the long run.",
                    "label": 1
                },
                {
                    "sent": "OK, the very nice interpretation is the long run relative frequency.",
                    "label": 1
                },
                {
                    "sent": "Outcomes so the probability of six is 1 / 6.",
                    "label": 0
                },
                {
                    "sent": "And probably even allow this 1/2.",
                    "label": 0
                },
                {
                    "sent": "So six or any other number here is an outcome in it must be Omega.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability of a given B is the probability of A&B divided by the probability of B either by definition or.",
                    "label": 0
                },
                {
                    "sent": "I mean if you look at the frequency picture, I mean it's obvious.",
                    "label": 0
                },
                {
                    "sent": "So for instance in this case the probability.",
                    "label": 0
                },
                {
                    "sent": "There are six comes up.",
                    "label": 1
                },
                {
                    "sent": "If you know that it was even, it's just one 6 / 1/2 is 1/3 OK and here the general Kolmogorov axioms of probability theory probabilities are between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This is the definition of conditional probabilities and this rule you know if A is.",
                    "label": 0
                },
                {
                    "sent": "Disjunct from B, then this is absent, and then probability of a Union BS, A+B, and in the more general case you have this rule.",
                    "label": 0
                },
                {
                    "sent": "So all other statements about probabilities can be or are derived from this general axioms.",
                    "label": 0
                },
                {
                    "sent": "So it's actually that's it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next slide more probability jargon so the past slide everybody probably knows.",
                    "label": 0
                },
                {
                    "sent": "Here's a little bit of jargon so.",
                    "label": 0
                },
                {
                    "sent": "Think of a coin now.",
                    "label": 0
                },
                {
                    "sent": "Has had entails.",
                    "label": 0
                },
                {
                    "sent": "You usually map them to number, so 01 for instance.",
                    "label": 0
                },
                {
                    "sent": "And now think of a biased coin, or better, maybe not think of a coin but just a process which can be.",
                    "label": 0
                },
                {
                    "sent": "Two binary outcomes.",
                    "label": 0
                },
                {
                    "sent": "And is independent identically distributed, but you don't know the biased at around that error can be somewhere between zero and one for the fair coin it would be 1/2, so the likelihood to see a sequence 1101 of outcomes given the bias data is then of course simply theater times theater times Theta times 1 minus Theta, so the probability of a one is Theta and three times and the probability of zero is 1 minus Theta.",
                    "label": 0
                },
                {
                    "sent": "And that's called likelihood.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood estimator now for this data is just maximizing this probability.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you take a model and you see the outcome and if the model.",
                    "label": 0
                },
                {
                    "sent": "Is a very bad model of your reality.",
                    "label": 0
                },
                {
                    "sent": "Then this outcome has very small probability.",
                    "label": 0
                },
                {
                    "sent": "So why is versa?",
                    "label": 0
                },
                {
                    "sent": "And if the outcome is very likely, then this model contains probably or hopefully some truth, so the maximum likelihood estimate would be here three 4th, so this is a beta distribution which has a maximum of three force OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm this.",
                    "label": 0
                },
                {
                    "sent": "Works often bail for small model classes, but if you have large model classes, you get into overfitting problem and one way to get to solve this problem is to take a Bayesian approach and where you define a prior over tater so you have some prior belief in these parameters.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you see a coin, you would say with 98% probability it is fair and then maybe distribute the other 2%.",
                    "label": 0
                },
                {
                    "sent": "Among the other values of Theta, or if you don't know anything about your underlying process, then maybe you take a uniform prior.",
                    "label": 0
                },
                {
                    "sent": "So if you're in different than for instance, a uniform prior might be good.",
                    "label": 0
                },
                {
                    "sent": "OK, so then you can compute this.",
                    "label": 0
                },
                {
                    "sent": "Strange quantity called evidence which is hard to interpret.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of some say it's letting the data speak for yourself, so you have the probability given the model.",
                    "label": 0
                },
                {
                    "sent": "But you will trust multiply little prior of the model and and some or in this case integrate over it and then you get the probability of the data itself.",
                    "label": 0
                },
                {
                    "sent": "Which is in, in this case 120.",
                    "label": 0
                },
                {
                    "sent": "These numbers are typically very small or very large and have no good direct interpretation, but nevertheless it's sort of the key quantity.",
                    "label": 0
                },
                {
                    "sent": "If you can compute this, then the rest often easily follows and.",
                    "label": 0
                },
                {
                    "sent": "It's needed for other things.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the posterior probability.",
                    "label": 0
                },
                {
                    "sent": "This is what you really want.",
                    "label": 0
                },
                {
                    "sent": "You have a prior belief, then you see your data and then you want to update your belief.",
                    "label": 0
                },
                {
                    "sent": "Now you have seen this sequence 1101 after having no idea about the coin.",
                    "label": 0
                },
                {
                    "sent": "Let's assume and then you ask what should I believe afterwards?",
                    "label": 0
                },
                {
                    "sent": "So you can compute it with base rule.",
                    "label": 0
                },
                {
                    "sent": "The probability of a given B is probably be given a * P of a / P of B.",
                    "label": 0
                },
                {
                    "sent": "And like everything you need to get just cater to three 1 minus Theta with some coefficient which are present here.",
                    "label": 0
                },
                {
                    "sent": "Which has a maximum somewhere and the maximum is called the maximum posteriori estimator, and in this case it gives the same answer as the frequent.",
                    "label": 0
                },
                {
                    "sent": "The estimated 3/4, but it would change if you use a different prior here.",
                    "label": 0
                },
                {
                    "sent": "And indeed, the constant price is not really there.",
                    "label": 0
                },
                {
                    "sent": "Best one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you could think of more, say, abstractly, philosophically.",
                    "label": 0
                },
                {
                    "sent": "I mean this theater is.",
                    "label": 0
                },
                {
                    "sent": "What is that?",
                    "label": 0
                },
                {
                    "sent": "I mean, you probably regard it as a property of the coin, but can you really sort of see this property?",
                    "label": 0
                },
                {
                    "sent": "I mean, the only way this property is interesting is that if you flip the coin in the future, this property influences the future outcomes.",
                    "label": 0
                },
                {
                    "sent": "So what you do with the theater?",
                    "label": 0
                },
                {
                    "sent": "So with this model, this is what I explained before.",
                    "label": 0
                },
                {
                    "sent": "So this was the induction step.",
                    "label": 0
                },
                {
                    "sent": "What you do with this model later you want to use it for prediction.",
                    "label": 0
                },
                {
                    "sent": "So why don't use it directly for prediction?",
                    "label": 0
                },
                {
                    "sent": "So what is the probability that the next outcome is 1 given our past outcomes?",
                    "label": 0
                },
                {
                    "sent": "I mean this is what we really want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to predict the future.",
                    "label": 0
                },
                {
                    "sent": "All future events, so this conditional probability you can either compute from the posterior.",
                    "label": 0
                },
                {
                    "sent": "Or directly from a ratio of evidences.",
                    "label": 0
                },
                {
                    "sent": "Becausw?",
                    "label": 0
                },
                {
                    "sent": "I mean you just use the definition.",
                    "label": 0
                },
                {
                    "sent": "And then you see it's just this ratio of two evidences, so you see.",
                    "label": 0
                },
                {
                    "sent": "How they can be used in their answers to 3rd?",
                    "label": 0
                },
                {
                    "sent": "Which is actually more reasonable than the three 4th, but I will not go into this now.",
                    "label": 0
                },
                {
                    "sent": "Expectations, I hope you all know I'm in the expectation of a function is just a function times it's probability and then summed over all over the sample space.",
                    "label": 0
                },
                {
                    "sent": "So here the expected value of Theta because one problem with the posterior is that it's a highly complicated object.",
                    "label": 0
                },
                {
                    "sent": "I mean this is a function here because we have many parameters.",
                    "label": 0
                },
                {
                    "sent": "Then I mean it's nice to have this answer, but what do you do with it?",
                    "label": 0
                },
                {
                    "sent": "You typically need summaries of this answer, and one summary is to compute either the maximum.",
                    "label": 0
                },
                {
                    "sent": "So that's a map estimator.",
                    "label": 0
                },
                {
                    "sent": "Or you compute the mean.",
                    "label": 0
                },
                {
                    "sent": "And this would be to 3rd in this case.",
                    "label": 0
                },
                {
                    "sent": "If you want an estimate of the accuracy.",
                    "label": 0
                },
                {
                    "sent": "Of your estimation, then you often use the variance which is defined in this way.",
                    "label": 0
                },
                {
                    "sent": "And then you know there's a distinction between probabilities and probability densities.",
                    "label": 0
                },
                {
                    "sent": "So it's just.",
                    "label": 0
                },
                {
                    "sent": "I mean, you take the probability of a small small interval divided by the length and let the limit goes to 0.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's not not most naive way to their kind of density.",
                    "label": 0
                },
                {
                    "sent": "OKO I'm running late, it's.",
                    "label": 0
                },
                {
                    "sent": "Time for coffee break now.",
                    "label": 0
                },
                {
                    "sent": "So we will always have the first coffee break in the morning.",
                    "label": 0
                },
                {
                    "sent": "Will be some real coffee break.",
                    "label": 0
                },
                {
                    "sent": "Down there, get your coffee and tea.",
                    "label": 0
                },
                {
                    "sent": "Please be back in time so we even if you run late.",
                    "label": 0
                },
                {
                    "sent": "Chris, you should have stopped me.",
                    "label": 0
                },
                {
                    "sent": "We started even if you run late, we start in time.",
                    "label": 0
                },
                {
                    "sent": "And the second break is a smaller I mean, the time is the same, but you know there will be only instant coffee and so on in the same in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "OK, see you back in 10 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, a bunch of.",
                    "label": 0
                },
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "And most of them will be explained in detail by the other lectures.",
                    "label": 0
                },
                {
                    "sent": "So if you can't follow one, no problem.",
                    "label": 0
                },
                {
                    "sent": "We get mostly a full lecture about them anyway.",
                    "label": 0
                },
                {
                    "sent": "Later OK, linear methods for regression I already mentioned.",
                    "label": 1
                },
                {
                    "sent": "It doesn't sound very exciting to fit a line through data, but it's really, really important and it's more powerful than you think of.",
                    "label": 1
                },
                {
                    "sent": "Some methods for regularization, linear methods for classification, linear basis, functional regression, and this is the reason why they are so important.",
                    "label": 0
                },
                {
                    "sent": "For instance, splines, facelets.",
                    "label": 0
                },
                {
                    "sent": "They're all linear methods for regression.",
                    "label": 1
                },
                {
                    "sent": "Kernel methods local smoothing.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's start.",
                    "label": 0
                },
                {
                    "sent": "So that's the classical.",
                    "label": 0
                },
                {
                    "sent": "A classic problem or regression problem.",
                    "label": 0
                },
                {
                    "sent": "You have some input feature vector X in some D dimensional space, so you have the components X one up to XD.",
                    "label": 1
                },
                {
                    "sent": "Typically you add another component is 0 and you said it identically to one that is just to make the math a little bit easier, then you have a response valued.",
                    "label": 0
                },
                {
                    "sent": "Which is often noisy.",
                    "label": 0
                },
                {
                    "sent": "And you want to infer a relation between the input variables, the features and the output variable.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so and if the relation is linear or you assume this relation is linear then the most general linear function is this here, so it's just these coefficients W 0 up to WD.",
                    "label": 0
                },
                {
                    "sent": "So remember, X0 is just one.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have data so there are pairs of features or feature vectors and labels, say N. And.",
                    "label": 0
                },
                {
                    "sent": "Another thing you need is a loss function.",
                    "label": 0
                },
                {
                    "sent": "If you don't specify, then it's often implicit and often quadratic, so you look for the.",
                    "label": 0
                },
                {
                    "sent": "The real outcome here why I?",
                    "label": 0
                },
                {
                    "sent": "And you compare it with your prediction.",
                    "label": 0
                },
                {
                    "sent": "Take a square difference, for instance, and some overall data points.",
                    "label": 0
                },
                {
                    "sent": "That is your loss.",
                    "label": 0
                },
                {
                    "sent": "And of course you want to minimize your loss, so you look for the W. Which minimizes this.",
                    "label": 0
                },
                {
                    "sent": "And here's an example, so you appear X, one X2.",
                    "label": 0
                },
                {
                    "sent": "You have very noisy data points and this is the best plane.",
                    "label": 0
                },
                {
                    "sent": "So the least square playing through the data points.",
                    "label": 0
                },
                {
                    "sent": "And you can use this plane then for making predictions for other data points.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, here was for example the person's weight as a function of its age and height.",
                    "label": 1
                },
                {
                    "sent": "Probably a local piece only for adults, because I mean it should normally be highly nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "Problem is.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of features.",
                    "label": 0
                },
                {
                    "sent": "For instance, in spam classification, I mean there's a classification problem now, but the features are typically diverts and there are 10 thousands of words.",
                    "label": 0
                },
                {
                    "sent": "Use a linear model or something related.",
                    "label": 0
                },
                {
                    "sent": "You will totally overfit.",
                    "label": 0
                },
                {
                    "sent": "And your problem, which would lead to bad prediction.",
                    "label": 0
                },
                {
                    "sent": "So if these very large compared to the number of data points then you get the overfitting problem.",
                    "label": 0
                },
                {
                    "sent": "So there are many ways to solve this problem and one is to identify a small subset of your features which you lose use.",
                    "label": 1
                },
                {
                    "sent": "So there is subset selection, so I assume you want to choose K out of the features and you want to choose them in such a way that they minimize the least square error.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, that's all for now.",
                    "label": 0
                },
                {
                    "sent": "If you know the K. Then you're sort of safe if you also want to infer that K, then you need to do some proper model selection account.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "OK, another way would be coefficient shrinkage.",
                    "label": 0
                },
                {
                    "sent": "I mean filtering out useless features is the same as setting the coefficients to 0.",
                    "label": 0
                },
                {
                    "sent": "So you could think of being less drastically and just shrink these coefficients and you can do that by adding to the loss penalty, which increases with increasing rate.",
                    "label": 0
                },
                {
                    "sent": "If you use the square norm then you get rich regression and the small coefficients shrink.",
                    "label": 0
                },
                {
                    "sent": "If you use the one norm looks like a minor change.",
                    "label": 0
                },
                {
                    "sent": "But then you get socalled method called Lasso.",
                    "label": 0
                },
                {
                    "sent": "Then also this method will set some coefficients to 0.",
                    "label": 0
                },
                {
                    "sent": "Like in the previous method, but Additionally shrink the other coefficients.",
                    "label": 0
                },
                {
                    "sent": "And then there's Bayesian linear regression.",
                    "label": 1
                },
                {
                    "sent": "Bear is always the same story you choose some prior over the W. And for instance, I mean if you have a Gaussian modeling a Gaussian prior then introduces exactly too rich regression.",
                    "label": 0
                },
                {
                    "sent": "But you can do choose other priors and.",
                    "label": 0
                },
                {
                    "sent": "So you choose your prior and.",
                    "label": 0
                },
                {
                    "sent": "You have your sampling model.",
                    "label": 0
                },
                {
                    "sent": "Then you compute your posterior and you take for instance the map estimator and then also regularize your problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that was linear regression.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One way to solve classification is to reduce it to a regression problem.",
                    "label": 0
                },
                {
                    "sent": "And for instance, OK, take this standard example again, spam versus not spam.",
                    "label": 0
                },
                {
                    "sent": "So you have your bag of words, model or whatever, and your labels, the labels you transform them to say minus one or one.",
                    "label": 0
                },
                {
                    "sent": "401 but here, minus one to one is better.",
                    "label": 0
                },
                {
                    "sent": "And then you regard the label which is now minus one, and one is a real number becausw minus one and one are real numbers.",
                    "label": 0
                },
                {
                    "sent": "And then you have a regression problem.",
                    "label": 0
                },
                {
                    "sent": "Apart from the problem that the output now is a real number, which makes no sense, it should be minus one or one.",
                    "label": 0
                },
                {
                    "sent": "But you simply say if the output so the value of the function is positive, then you label it as one.",
                    "label": 0
                },
                {
                    "sent": "So non spam.",
                    "label": 1
                },
                {
                    "sent": "If it's negative you label as spam.",
                    "label": 0
                },
                {
                    "sent": "OK. You can do that, but often what you want in a classification problem or can afford is rather than out putting, you know this new example, spam or not, you wanted more graded and the algorithm should tell you how confident it is.",
                    "label": 0
                },
                {
                    "sent": "So give a probability estimate.",
                    "label": 0
                },
                {
                    "sent": "And we have now this real valued function, so it's very, very positive.",
                    "label": 0
                },
                {
                    "sent": "Looks like that the algorithm is very sure that it's non spam in analog for negative.",
                    "label": 0
                },
                {
                    "sent": "So if you can transform the real numbers because this function is linear goes from minus Infinity to Infinity, we have to transform that somehow to the interval zero and one.",
                    "label": 0
                },
                {
                    "sent": "And you can do that with this log odds transformation.",
                    "label": 0
                },
                {
                    "sent": "So assume P of Y equal 1 is 1, then PA y = 0 is zero.",
                    "label": 0
                },
                {
                    "sent": "They have 1 / 0 is Infinity and the logarithm of Infinity is Infinity OK, and otherwise you have 0 divided by Infinity and the logarithm then is minus Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this function transforms the interval 01 to the whole range of real numbers.",
                    "label": 0
                },
                {
                    "sent": "So if you use it inversely, you get from this real number of probability in this interval.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a. Sigmoid function, which looks like.",
                    "label": 0
                },
                {
                    "sent": "This year OK?",
                    "label": 0
                },
                {
                    "sent": "With both methods, there are various problems.",
                    "label": 0
                },
                {
                    "sent": "Very roughly, the problem is with data points which are far away from the separating point here.",
                    "label": 0
                },
                {
                    "sent": "They have the highest weights.",
                    "label": 0
                },
                {
                    "sent": "Then in determining the function, but they should have the lowest rate because they are least critical and.",
                    "label": 0
                },
                {
                    "sent": "The various ways to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So and the support vector machine is the most successful one today and you will hear a whole here whole lecture, or at least one lecture.",
                    "label": 0
                },
                {
                    "sent": "Probably many about support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of this is probably the natural historical regression, so those first perception maximum margin hyperplane algorithm and then support vector machines, and there are others like linear discriminant analysis and.",
                    "label": 1
                },
                {
                    "sent": "I'm OK this weekend.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was binary classification.",
                    "label": 0
                },
                {
                    "sent": "And of course you can generalize things, but it's not always obvious or there's not always a unique way to do that to multi class classifications.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if here 3 classes like here, you could do a bunch of binary classifiers or directly a classifier which can handle multi classes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now so that was the linear stuff and I'll come to this slide.",
                    "label": 0
                },
                {
                    "sent": "Why is linear so important?",
                    "label": 0
                },
                {
                    "sent": "Are powerful and this is the cause.",
                    "label": 0
                },
                {
                    "sent": "If a problem is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "You can often make it linear by transforming your feature space.",
                    "label": 0
                },
                {
                    "sent": "So OK, I said that often the response is not linear in X.",
                    "label": 1
                },
                {
                    "sent": "So what you simply do is you transform your extra 5X.",
                    "label": 0
                },
                {
                    "sent": "And the dimension of the target space can even be different, so you have a D dimensional feature vector and you transform it and they go into P dimensional space, which is often or could be much larger than the original space.",
                    "label": 0
                },
                {
                    "sent": "And then you assume or hope.",
                    "label": 0
                },
                {
                    "sent": "That your problem, your label your.",
                    "label": 0
                },
                {
                    "sent": "Your library is now linear in five.",
                    "label": 1
                },
                {
                    "sent": "OK, so the most simple.",
                    "label": 0
                },
                {
                    "sent": "Transformation is to do nothing, so.",
                    "label": 0
                },
                {
                    "sent": "You have a D dimensional feature vector and you just select the ice feature, so it's the identical mapping and you're back to linear regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort of the standard mathematicians choke or first example.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the first interesting one, so consider this problem.",
                    "label": 0
                },
                {
                    "sent": "Say if they datapoints.",
                    "label": 0
                },
                {
                    "sent": "Like like this.",
                    "label": 0
                },
                {
                    "sent": "Well, looks like a parabola.",
                    "label": 0
                },
                {
                    "sent": "Definitely not like a line.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a new problem.",
                    "label": 0
                },
                {
                    "sent": "That's not linear regression anymore, but you can very easily transform it to linear regression if you just say I met my ex to a vector X1 and X2.",
                    "label": 1
                },
                {
                    "sent": "So that is my function 5.",
                    "label": 0
                },
                {
                    "sent": "And this is X&X squared now.",
                    "label": 0
                },
                {
                    "sent": "Actually it should have here.",
                    "label": 0
                },
                {
                    "sent": "I mean the one is always hanging around somewhere.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then if you draw the picture.",
                    "label": 0
                },
                {
                    "sent": "I have another access here.",
                    "label": 0
                },
                {
                    "sent": "To the back.",
                    "label": 0
                },
                {
                    "sent": "And these data points have also, you know a dimension in this direction and now you can fit very nicely a plane through it.",
                    "label": 0
                },
                {
                    "sent": "And then you project everything back and you get.",
                    "label": 0
                },
                {
                    "sent": "The best particular fitting for the data.",
                    "label": 0
                },
                {
                    "sent": "But just using this embedding trick and using linear regression.",
                    "label": 0
                },
                {
                    "sent": "Another problem is or.",
                    "label": 1
                },
                {
                    "sent": "Good operation thing is binningen your data points.",
                    "label": 0
                },
                {
                    "sent": "You been your problem and you make a piecewise constant regression.",
                    "label": 0
                },
                {
                    "sent": "So there's also linear regression where the basis functions are.",
                    "label": 0
                },
                {
                    "sent": "Step functions like this, so this is.",
                    "label": 0
                },
                {
                    "sent": "5 one and say.",
                    "label": 0
                },
                {
                    "sent": "5K.",
                    "label": 0
                },
                {
                    "sent": "Or piecewise polynomials, or splines?",
                    "label": 0
                },
                {
                    "sent": "They're all linear regression.",
                    "label": 0
                },
                {
                    "sent": "So yeah, here's some exam.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Piecewise constant, piecewise linear, continuous piecewise linear, and there's some based basis function for piecewise linear regressor.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "2 dimensional splines here.",
                    "label": 0
                },
                {
                    "sent": "So what you essentially do, you have this basis functions here and you have some smooth function you.",
                    "label": 0
                },
                {
                    "sent": "Which you can.",
                    "label": 0
                },
                {
                    "sent": "Model by averaging or by summing with appropriate weights.",
                    "label": 0
                },
                {
                    "sent": "You know these bumps here and there are bumps which are sort of at every places and with various risks.",
                    "label": 0
                },
                {
                    "sent": "OK, and on the right is a wavelet basis so.",
                    "label": 0
                },
                {
                    "sent": "Wavelets are also linear regression.",
                    "label": 0
                },
                {
                    "sent": "Or bracelets also can be reduced to linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now the linear method is.",
                    "label": 0
                },
                {
                    "sent": "Local smoothing and currently regression.",
                    "label": 0
                },
                {
                    "sent": "So what you do here is you have your data points.",
                    "label": 0
                },
                {
                    "sent": "And you just take a local neighborhood and averaged divide values and then you get the value which gives you the green curve.",
                    "label": 0
                },
                {
                    "sent": "This still gives you a very big curve because when you move this window data points pop in and out, so it's even discontinuous.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is instead of taking a uniform average, you can average the points in such a way that you take a higher rate for the ones.",
                    "label": 0
                },
                {
                    "sent": "Which are, you know, close to your target point or lower right outside.",
                    "label": 0
                },
                {
                    "sent": "So for instance, and it is the quadratic kernel here.",
                    "label": 0
                },
                {
                    "sent": "And you can always write it in this form.",
                    "label": 0
                },
                {
                    "sent": "Your estimating function is.",
                    "label": 0
                },
                {
                    "sent": "I'm an average over the Y values and the weight of this average depends on the target point X.",
                    "label": 0
                },
                {
                    "sent": "And is characterized by this by this kernel function K of XXI, for instance.",
                    "label": 0
                },
                {
                    "sent": "Here it's it's one if X.",
                    "label": 0
                },
                {
                    "sent": "If XY if XI is close to X and zero outside, and here it's this quadratic function here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm yes, OK, another linear method is the regular regularised regression with quadratic loss function and panelization.",
                    "label": 0
                },
                {
                    "sent": "So what you do is so this is here.",
                    "label": 0
                },
                {
                    "sent": "Your quadratic loss function.",
                    "label": 0
                },
                {
                    "sent": "But now you ask so you don't give some model class 4F, so some restricted one likes polynomials, but you ask for any function F, say any mathematical reasonable one.",
                    "label": 0
                },
                {
                    "sent": "But if you would just do that then you will get a function which perfectly fits your data.",
                    "label": 0
                },
                {
                    "sent": "I mean you will get something like.",
                    "label": 0
                },
                {
                    "sent": "This year, whatever.",
                    "label": 0
                },
                {
                    "sent": "And it's intuitive, obvious that there is not a good thing to do, and you can show that it also has very poor predictive performance.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you penalize wiggly functions.",
                    "label": 0
                },
                {
                    "sent": "So you say, for instance, that the curvature, so the second derivative squared should be small over the whole domain.",
                    "label": 0
                },
                {
                    "sent": "So you're at this penalty with this parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "So if Lambda is zero, I already draw function, you get any functions through the data.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is Infinity.",
                    "label": 0
                },
                {
                    "sent": "I mean, this term counts all so the second derivative must be zero, which means that you get a straight line fitting, so you get the original linear regression back.",
                    "label": 0
                },
                {
                    "sent": "And if it's somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "You get piecewise cubic regression with continuous derivative.",
                    "label": 1
                },
                {
                    "sent": "So it's from one data point to the other.",
                    "label": 0
                },
                {
                    "sent": "It's a cubic.",
                    "label": 0
                },
                {
                    "sent": "But the derivative is continuous.",
                    "label": 0
                },
                {
                    "sent": "So it's splined.",
                    "label": 0
                },
                {
                    "sent": "Regression.",
                    "label": 0
                },
                {
                    "sent": "OK, that are the linear ones.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think it's a little bit early for the break, so I can't go to the nonlinear ones now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This basic function trick is very, very powerful, but also has its limitations and then and I would say only then you should go to nonlinear methods.",
                    "label": 0
                },
                {
                    "sent": "For instance, like artificial neural networks.",
                    "label": 1
                },
                {
                    "sent": "And support vector machines.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK, here's one slide about neural networks, because I think it will not be covered.",
                    "label": 1
                },
                {
                    "sent": "In detail by other lectures.",
                    "label": 0
                },
                {
                    "sent": "So what you have is you have.",
                    "label": 0
                },
                {
                    "sent": "So this is your feature vector X one up to X. D. You have now.",
                    "label": 0
                },
                {
                    "sent": "M. Linear functions here.",
                    "label": 0
                },
                {
                    "sent": "So you take your access, take a weighted average.",
                    "label": 0
                },
                {
                    "sent": "So we have M linear functions.",
                    "label": 1
                },
                {
                    "sent": "But then and this is the new element you pipe it through a nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "And then you do the same thing in the second layer.",
                    "label": 0
                },
                {
                    "sent": "So I mean that would be 1 hidden layer neural network.",
                    "label": 1
                },
                {
                    "sent": "Of course there are generalizations.",
                    "label": 0
                },
                {
                    "sent": "OK, so and this.",
                    "label": 1
                },
                {
                    "sent": "So-called activation functions often have or typically have a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Form and the reason is if you take this linear average, then for some inputs you get huge outputs and they're quite bad.",
                    "label": 0
                },
                {
                    "sent": "If you feed them in the next layer.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you just found them in a reasonable interval, for instance 01 back.",
                    "label": 0
                },
                {
                    "sent": "And then you go to the next layer.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I mean, the learning task is then as before.",
                    "label": 0
                },
                {
                    "sent": "I mean you have some training examples.",
                    "label": 0
                },
                {
                    "sent": "So input and output and you want to learn and the weights.",
                    "label": 0
                },
                {
                    "sent": "So you have here set of weights and here set of weights which best fit your data.",
                    "label": 0
                },
                {
                    "sent": "So, so you have your outputs Y and your regression function F which depends on this.",
                    "label": 0
                },
                {
                    "sent": "Vector W which is W1 and W2 and you for instance you want to minimize the square loss, and this is typically so there is no closed form solution now to this anymore.",
                    "label": 1
                },
                {
                    "sent": "Is one of the reasons to avoid nonlinear regression.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "The Classical one, is just the gradient descent method.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, here's an example.",
                    "label": 0
                },
                {
                    "sent": "The classical example, for instance, of an image processing problem you have here an image.",
                    "label": 0
                },
                {
                    "sent": "And you want to detect some features.",
                    "label": 0
                },
                {
                    "sent": "So for instance small line segments, or whether there's a corner.",
                    "label": 0
                },
                {
                    "sent": "So you take a local neighborhood over Pixel.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "With these weights I mean you have a linear weighted average.",
                    "label": 0
                },
                {
                    "sent": "And then a secret function, and you get a value here.",
                    "label": 0
                },
                {
                    "sent": "And you do that for every pixel.",
                    "label": 0
                },
                {
                    "sent": "And then you subsample and you know.",
                    "label": 0
                },
                {
                    "sent": "Make IT delivery course of your data points.",
                    "label": 0
                },
                {
                    "sent": "OK. And I think now is a good time for.",
                    "label": 0
                },
                {
                    "sent": "Ending.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In accent.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, not linear means linear in the WS, that's the important thing.",
                    "label": 0
                },
                {
                    "sent": "I mean, ultimately, I mean you have I mean this linear function.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a scalar product between WS and say this feature vector five.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's also linear in file necessarily.",
                    "label": 0
                },
                {
                    "sent": "OK, any other question?",
                    "label": 0
                },
                {
                    "sent": "OK, one thing I forgot.",
                    "label": 0
                },
                {
                    "sent": "If you're in a cottage with.",
                    "label": 0
                },
                {
                    "sent": "Isn't it?",
                    "label": 0
                },
                {
                    "sent": "Outlets, but no wireless.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of cables, just come here and I will give you some cable.",
                    "label": 0
                }
            ]
        }
    }
}