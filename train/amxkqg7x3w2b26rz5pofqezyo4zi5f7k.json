{
    "id": "amxkqg7x3w2b26rz5pofqezyo4zi5f7k",
    "title": "More data means less inference: A pseudo-max approach to structured learning",
    "info": {
        "author": [
            "David Sontag, Computer Science Department, New York University (NYU)"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_sontag_mdm/",
    "segmentation": [
        [
            "So this is joint work with permission to macula and near Globerson, so many of the prediction problems that are arising in machine learning these days are have very large output spaces, which which are quite well structured.",
            "So for example in multi label prediction were given as input a document and we'd like to predict what have many labels might be assigned to this document.",
            "And in this talk and our poster, I'll be telling you about a efficient learning algorithm for structured prediction."
        ],
        [
            "So each prediction task is specified by a feature function F and a vector valued feature function and by a weight vector W. The feature function takes arguments the input X and a assignment Y.",
            "On a new example X, the prediction is given by taking the DOT product.",
            "The friction is given by finding the assignment Y hat which maximizes the dot product of W and the feature function evaluated at White Hat.",
            "This maximization corresponds to a discrete optimization problem.",
            "So at prediction time, typically what's done is solving a discrete optimization by integer linear programming or similar.",
            "For a large number of structured prediction problems, the corresponding prediction task.",
            "For example, if the labels if.",
            "Our pairwise interaction between the labels is NP hard."
        ],
        [
            "So we're concerned with the learning problem so.",
            "In a separable setting, you might assume that there exists some weight vector W such that for each of our data points on the input XM, the label YM is is the label with the highest score with respect to W. Score, I mean the dot product of WF on my hat.",
            "There's been a large number of algorithms that have been proposed for learning in this structured prediction setting, but all of these algorithms use some sort of prediction in the inner loop of learning.",
            "So, for example, the structured perception algorithm alternates between doing prediction for a given example and updating the weight vector to push the predicted assignment towards the correct assignment.",
            "However, when prediction is slow.",
            "It makes it very difficult to apply these structured learning algorithms to train large training sets.",
            "So in this work we ask, is there some way to avoid the complexity of prediction during learning?",
            "Now work is inspired by the use of pseudo likelihood as an efficient as a computationally efficient estimator in maximum likelihood learning and in our work we look at the structured prediction setting and give an algorithm which similarly to pseudo likelihood when the training data is nice enough.",
            "Can be shown to recover the true parameters of your model."
        ],
        [
            "So an exact formulation of the structured learning problem in the separable setting is given by the following set of linear inequalities.",
            "For each data point M and for every possible assignment to that data point other than the true assignment YM, we want that the score of YM should be larger than the score of Y.",
            "So any weight vector W that satisfies all these constraints can be shown to correctly classify all of the training points.",
            "However, notice that there are exponentially many constraints that make up the set, namely one for every possible assignment to the variables in a given example.",
            "So our approach, which we call the pseudo Max approach will be consider a carefully selected subset of these constraints, namely, rather than, rather than having one constraint for every possible assignment.",
            "To the variables of a given data point, what would do is have will only consider the assignments that are close to YN.",
            "In particular, we have for every data point M and every variable I in in your structured setting and every possible alternative label Yi to that variable.",
            "A constraint saying that the score of YM should be larger than the score of YM.",
            "With this alternative label Y in place of the original 1.",
            "So I quit surprisingly, what we do is we show that for a large number of.",
            "Even with very few assumptions on the distribution of training examples P of X, this method can be shown.",
            "This method, which only uses a few constraints, can be shown to correctly find the true parameters W. We voted our method on two different domains, one of which was structured.",
            "The multi level prediction problem and another which is to learn the energy function for protein side chain placement and we found that our method both both was extremely fast at training time and succeeded in finding very effective parameters.",
            "That is to say we gave good performance at this time.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with permission to macula and near Globerson, so many of the prediction problems that are arising in machine learning these days are have very large output spaces, which which are quite well structured.",
                    "label": 0
                },
                {
                    "sent": "So for example in multi label prediction were given as input a document and we'd like to predict what have many labels might be assigned to this document.",
                    "label": 0
                },
                {
                    "sent": "And in this talk and our poster, I'll be telling you about a efficient learning algorithm for structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So each prediction task is specified by a feature function F and a vector valued feature function and by a weight vector W. The feature function takes arguments the input X and a assignment Y.",
                    "label": 1
                },
                {
                    "sent": "On a new example X, the prediction is given by taking the DOT product.",
                    "label": 0
                },
                {
                    "sent": "The friction is given by finding the assignment Y hat which maximizes the dot product of W and the feature function evaluated at White Hat.",
                    "label": 0
                },
                {
                    "sent": "This maximization corresponds to a discrete optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So at prediction time, typically what's done is solving a discrete optimization by integer linear programming or similar.",
                    "label": 0
                },
                {
                    "sent": "For a large number of structured prediction problems, the corresponding prediction task.",
                    "label": 0
                },
                {
                    "sent": "For example, if the labels if.",
                    "label": 0
                },
                {
                    "sent": "Our pairwise interaction between the labels is NP hard.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're concerned with the learning problem so.",
                    "label": 1
                },
                {
                    "sent": "In a separable setting, you might assume that there exists some weight vector W such that for each of our data points on the input XM, the label YM is is the label with the highest score with respect to W. Score, I mean the dot product of WF on my hat.",
                    "label": 1
                },
                {
                    "sent": "There's been a large number of algorithms that have been proposed for learning in this structured prediction setting, but all of these algorithms use some sort of prediction in the inner loop of learning.",
                    "label": 0
                },
                {
                    "sent": "So, for example, the structured perception algorithm alternates between doing prediction for a given example and updating the weight vector to push the predicted assignment towards the correct assignment.",
                    "label": 0
                },
                {
                    "sent": "However, when prediction is slow.",
                    "label": 1
                },
                {
                    "sent": "It makes it very difficult to apply these structured learning algorithms to train large training sets.",
                    "label": 0
                },
                {
                    "sent": "So in this work we ask, is there some way to avoid the complexity of prediction during learning?",
                    "label": 1
                },
                {
                    "sent": "Now work is inspired by the use of pseudo likelihood as an efficient as a computationally efficient estimator in maximum likelihood learning and in our work we look at the structured prediction setting and give an algorithm which similarly to pseudo likelihood when the training data is nice enough.",
                    "label": 0
                },
                {
                    "sent": "Can be shown to recover the true parameters of your model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an exact formulation of the structured learning problem in the separable setting is given by the following set of linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "For each data point M and for every possible assignment to that data point other than the true assignment YM, we want that the score of YM should be larger than the score of Y.",
                    "label": 0
                },
                {
                    "sent": "So any weight vector W that satisfies all these constraints can be shown to correctly classify all of the training points.",
                    "label": 0
                },
                {
                    "sent": "However, notice that there are exponentially many constraints that make up the set, namely one for every possible assignment to the variables in a given example.",
                    "label": 0
                },
                {
                    "sent": "So our approach, which we call the pseudo Max approach will be consider a carefully selected subset of these constraints, namely, rather than, rather than having one constraint for every possible assignment.",
                    "label": 0
                },
                {
                    "sent": "To the variables of a given data point, what would do is have will only consider the assignments that are close to YN.",
                    "label": 0
                },
                {
                    "sent": "In particular, we have for every data point M and every variable I in in your structured setting and every possible alternative label Yi to that variable.",
                    "label": 0
                },
                {
                    "sent": "A constraint saying that the score of YM should be larger than the score of YM.",
                    "label": 0
                },
                {
                    "sent": "With this alternative label Y in place of the original 1.",
                    "label": 0
                },
                {
                    "sent": "So I quit surprisingly, what we do is we show that for a large number of.",
                    "label": 0
                },
                {
                    "sent": "Even with very few assumptions on the distribution of training examples P of X, this method can be shown.",
                    "label": 0
                },
                {
                    "sent": "This method, which only uses a few constraints, can be shown to correctly find the true parameters W. We voted our method on two different domains, one of which was structured.",
                    "label": 0
                },
                {
                    "sent": "The multi level prediction problem and another which is to learn the energy function for protein side chain placement and we found that our method both both was extremely fast at training time and succeeded in finding very effective parameters.",
                    "label": 0
                },
                {
                    "sent": "That is to say we gave good performance at this time.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}