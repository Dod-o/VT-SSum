{
    "id": "iy2ujdxrlpl2skyr5hjnbtdmxd2cs6s5",
    "title": "Algorithms and hardness results for parallel large margin learning",
    "info": {
        "author": [
            "Phil Long, Sentient Technologies USA LLC"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods",
            "Top->Computer Science->Machine Learning->Classification",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_long_learning/",
    "segmentation": [
        [
            "So in this work on Roku and I studied the pack model, so we consider the case where the algorithm is trying to learn a gamma separated half space.",
            "So we assume that no examples ever will ever fall within distance gamova separating hyperplane, and we assume that the data is generated from the unit ball.",
            "So the computational model that we used is the parallel RAM model and the question that drove this work is whether it's possible to output an epsilon accurate hypothesis, an amount of parallel time that grows Poly logarithmically in the relevant parameters using a polynomial number of processors."
        ],
        [
            "So the first observation is that the dependence on the inverse of the accuracy was already handled by previous work by you offline.",
            "Angular is boost by majority algorithm so.",
            "So the revised goal was just to obtain polylogarithmic time with respect to the other parameters, and if we could do that, then we would then we could use that as a weak learner together with boost by majority to solve the original problem.",
            "So the next observation is that it's possible to get a log rhythmic time in either of the parameters if the other parameter is fixed using existing techniques.",
            "So using the perceptron algorithm or using a large margin learning algorithm based on smooth boosts, it was easy to.",
            "Paralyze it to to arrive in an algorithm that takes roughly one over gamma squared times log N time parallel time, and just using off the shelf fast linear programming sequential algorithm can run in log rhythmic time in the inverse of the margin, and in our paper we showed that it's possible to learn in time roughly one over gamma and log in."
        ],
        [
            "So algorithm like I mentioned before on the outside has boost by majority of the parallel version by your front and then the weak learner works as follows.",
            "First we do a random projection to reduce the number of variables and then we apply an interior point method.",
            "So the interior point method is a loop over.",
            "Iterates over on Newtons method iterations and the bottleneck as far as parallel, timeless concerned is inverting Hessian.",
            "So to do that we apply fast parallel matrix inversion algorithm due to John Rife, but his algorithm requires you to give it an input that has coarse grain inputs, so we needed to round intermediate solutions to the nearest.",
            "Nearest rational numbers with the.",
            "Denominators that weren't very big, and doing that in the way that preserved the margin was the technical."
        ],
        [
            "Contribution.",
            "And we also established a negative result for algorithms based on boosting, so this was motivated by the fact that a number of boosting algorithms have been described that that build decision trees or branching programs or hypothesis like that, and these seem like natural candidates for parallelism because you could run all the weak learners in a given layer of the model.",
            "I'm in parallel, and so we asked whether it was possible to save time by doing this parallel time and what we show is that if you call the weak learner as a black box, then you can't save any iterations and so you can apply a lower bound on the number of iterations studio.",
            "You're trying to show that you need at least one over gamma squared iterations.",
            "If you try that way."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work on Roku and I studied the pack model, so we consider the case where the algorithm is trying to learn a gamma separated half space.",
                    "label": 0
                },
                {
                    "sent": "So we assume that no examples ever will ever fall within distance gamova separating hyperplane, and we assume that the data is generated from the unit ball.",
                    "label": 0
                },
                {
                    "sent": "So the computational model that we used is the parallel RAM model and the question that drove this work is whether it's possible to output an epsilon accurate hypothesis, an amount of parallel time that grows Poly logarithmically in the relevant parameters using a polynomial number of processors.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first observation is that the dependence on the inverse of the accuracy was already handled by previous work by you offline.",
                    "label": 1
                },
                {
                    "sent": "Angular is boost by majority algorithm so.",
                    "label": 0
                },
                {
                    "sent": "So the revised goal was just to obtain polylogarithmic time with respect to the other parameters, and if we could do that, then we would then we could use that as a weak learner together with boost by majority to solve the original problem.",
                    "label": 0
                },
                {
                    "sent": "So the next observation is that it's possible to get a log rhythmic time in either of the parameters if the other parameter is fixed using existing techniques.",
                    "label": 0
                },
                {
                    "sent": "So using the perceptron algorithm or using a large margin learning algorithm based on smooth boosts, it was easy to.",
                    "label": 0
                },
                {
                    "sent": "Paralyze it to to arrive in an algorithm that takes roughly one over gamma squared times log N time parallel time, and just using off the shelf fast linear programming sequential algorithm can run in log rhythmic time in the inverse of the margin, and in our paper we showed that it's possible to learn in time roughly one over gamma and log in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So algorithm like I mentioned before on the outside has boost by majority of the parallel version by your front and then the weak learner works as follows.",
                    "label": 0
                },
                {
                    "sent": "First we do a random projection to reduce the number of variables and then we apply an interior point method.",
                    "label": 1
                },
                {
                    "sent": "So the interior point method is a loop over.",
                    "label": 0
                },
                {
                    "sent": "Iterates over on Newtons method iterations and the bottleneck as far as parallel, timeless concerned is inverting Hessian.",
                    "label": 1
                },
                {
                    "sent": "So to do that we apply fast parallel matrix inversion algorithm due to John Rife, but his algorithm requires you to give it an input that has coarse grain inputs, so we needed to round intermediate solutions to the nearest.",
                    "label": 0
                },
                {
                    "sent": "Nearest rational numbers with the.",
                    "label": 0
                },
                {
                    "sent": "Denominators that weren't very big, and doing that in the way that preserved the margin was the technical.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contribution.",
                    "label": 0
                },
                {
                    "sent": "And we also established a negative result for algorithms based on boosting, so this was motivated by the fact that a number of boosting algorithms have been described that that build decision trees or branching programs or hypothesis like that, and these seem like natural candidates for parallelism because you could run all the weak learners in a given layer of the model.",
                    "label": 1
                },
                {
                    "sent": "I'm in parallel, and so we asked whether it was possible to save time by doing this parallel time and what we show is that if you call the weak learner as a black box, then you can't save any iterations and so you can apply a lower bound on the number of iterations studio.",
                    "label": 0
                },
                {
                    "sent": "You're trying to show that you need at least one over gamma squared iterations.",
                    "label": 0
                },
                {
                    "sent": "If you try that way.",
                    "label": 0
                }
            ]
        }
    }
}