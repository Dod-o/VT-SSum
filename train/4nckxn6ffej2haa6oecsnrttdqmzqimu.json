{
    "id": "4nckxn6ffej2haa6oecsnrttdqmzqimu",
    "title": "Splash Belief Propagation: Efficient Parallelization Through Asynchronous Scheduling",
    "info": {
        "author": [
            "Joseph Gonzalez, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_gonzalez_sbpe/",
    "segmentation": [
        [
            "Repeatedly until they stop changing.",
            "And so it's a natural parallel algorithm.",
            "And like we showed earlier, we can take all of the."
        ],
        [
            "Messages split it over the processors and compute the new versions in parallel."
        ],
        [
            "And we get this earlier picture of an iterative message update.",
            "This is synchronous algorithm and we might ask, does this have this sequential dependencies within efficiency?"
        ],
        [
            "OK, so we can illustrate this by looking at this large loopy graph.",
            "Embedded in this loopy graph could be a strong sequential sequence of independent variables forming affectively a chain.",
            "And we can look at the performance of this synchronous algorithm on this chain.",
            "OK, so we can do this by introducing evidence at opposite ends and then running the synchron."
        ],
        [
            "Algorithm at each iteration using P processors."
        ],
        [
            "And we can see that we can move the messages in one iteration using P processors in time 2 and over P friend variables.",
            "OK, now as you see, the evidence play moves one iteration each time and I need to move the evidence across the graph and so this will take an iteration through the evidence across the graph for running time of two X ^2 / P. OK, so how good is this?"
        ],
        [
            "OK, so we can look back at the optimal sequential algorithm using one processor, and that's the past messages sequentially forward along the graph and then backward.",
            "The running time of this single processor algorithm is too high.",
            "So that means I would need P processors just to get the same running time as the optimal sequential algorithm with one processor and using two end processors.",
            "On this problem, I would actually only get a factor of two speedup.",
            "That's a lot of processors to only get a factor of 2 speed up.",
            "OK, now we could."
        ],
        [
            "What's the optimal algorithm for this problem and the optimal algorithm using only two processors is send messages sequentially in One Direction using one processor and sequentially another direction using a second processor, two processors, and the running time is N for a factor of 2 speed up.",
            "And what this really identified is the sort of inherent sequential structure to this problem.",
            "This requires efficient method scheduling.",
            "But sort of unfortunate is unless we do introduce an additional approximation, it's not possible to get better than a factor to speed up here.",
            "So let's look at additional approximation so you can can."
        ],
        [
            "For the sequence of messages that we've passed sequentially all the way across the chain, it takes awhile.",
            "And we could say then will supposedly replace the message from three to four with the uniform message.",
            "This is effectively starting my chain at 4.",
            "I can then pass the message sequentially.",
            "And I might find in practice the message from 9 to 10 differs only slightly from the original true message to 9 to 10.",
            "And then in general, there's this distance tell epsilon in which I must pass messages sequentially.",
            "But after which I can replace the message with the uniform distribution.",
            "We call this to tell us an approximation.",
            "And so given this idea, we might ask how can we define a parallel algorithm which achieves a tellepsen approximation in the chain graphical model for every vertex, and it's actually very simple algorithm, so I'll show you now."
        ],
        [
            "So we cut the chain over the processors defining within each block of central vertex, and then we actually run the very simple sequential forward backward algorithm, passing messages inward in the forward pass.",
            "And backward in the outward pass.",
            "We then exchanged messages across processors.",
            "And we repeat this process.",
            "And what's neat is this algorithm.",
            "Actually Richie achieves a running time of an over P plus plus line for a tellepsen level approximation for every vertex, and we can actually show that this is the optimal running time for this problem.",
            "For achieving a telephone approximation.",
            "So this is an optimal algorithm on the chain.",
            "And we did."
        ],
        [
            "To generalize this idea to loopy graphs because after all chains are pretty boring.",
            "And we do that by as before constructing or selecting a center vertex, then growing a breadth first search spanning tree.",
            "And then passing messages sequentially from the boundary of the tree into the root.",
            "And then from the route back to the boundary in the forward pass backward pass form.",
            "And so this is sort of a natural generalization of this.",
            "This chain algorithm described earlier, and I would like to make this into a full large scale parallel algorithm for large loopy graphs.",
            "And so we'll do it just as we did before.",
            "We'll take our large graphical model.",
            "Split it over our processors."
        ],
        [
            "And then on each processor run this splash.",
            "This generalization of this forward backward procedure.",
            "Exchange messages across processors and repeat.",
            "And so when we do this, we have a few key challenges we have to address which are here.",
            "We have to explain how we're going to schedule splashes on each of these processors.",
            "And how do we partition the graph?",
            "So let's first address how we're going to schedule flashes."
        ],
        [
            "So we actually adopted very simple heuristic, which we call belief residual scheduling, and we essentially assign priorities based on the cumulative change in the belief.",
            "So every time we produce to receive a new message to a vertex, we update the belief that vertex.",
            "And look how much it changes.",
            "And we record the accumulated change for all the messages that protect receipts.",
            "And so effectively a vertexes belief has changed substantially since he was last updated will likely produce informative messages, and so that's a vertex we'd like to schedule sooner.",
            "So then, using this notion of priority, we can actually introduce a priority queue on each processor.",
            "And what we do is we take the vertex with the highest priority.",
            "And constructive slash centered at that vertex.",
            "When we do that, we're going to send messages, change the priority of other vertices on that processor.",
            "Then demote the top vertex."
        ],
        [
            "And then schedule the next flash.",
            "Repeat this procedure until the properties of these vertices fall below our convergence threshold epsilon.",
            "And so this these priorities allow us to actually do something very neat to these flashes we can."
        ],
        [
            "Dynamically prune them.",
            "So if you look at this graph here, there are a lot of races, low belief residual which are effectively almost converged and we have this region with high belief residual.",
            "And we grow our spanning tree.",
            "We can actually have it adapt to the region of the graph that hasn't converged, and focusing our computation where it is most needed.",
            "OK, and this actually allows us to automatically determ."
        ],
        [
            "The appropriate size for splashes without having to fix the primary events.",
            "So if we don't include pruning, you can see there is sort of an optimal size for these flashes.",
            "But if we introduce pruning, this flashes automatically scale as the algorithm needs.",
            "And so actually illustrate this is a neat video.",
            "I had this synthetic noising problem.",
            "It's a sunset, the synthetic sunset and I'm going to represent it as a factor graph on this rate plot.",
            "I'm going to show the number of updates."
        ],
        [
            "As a brighter region in this picture this video.",
            "Let's see here.",
            "So you see initially, as these splashes are uniformly spread over the space.",
            "For the algorithm proceeds, it quickly identifies a region of the graph that need the most effort.",
            "And then it focuses on those regions tored the end, solving the important updates rather than the entire picture.",
            "Far to use a synchronous algorithm might have updated all of this black region just as often is that that bright region.",
            "So there's a sequential component right here as well.",
            "OK, and so in summary, this."
        ],
        [
            "There are parallel slash algorithm which we've basically placed.",
            "These are factor graph partitioning over the processors and then we run these local slashes scheduling using these belief priorities.",
            "Of course, I didn't tell you how we're going to, sorry.",
            "So first I want to say that if we do this with a with a uniform partitioning on the chain, we can actually recover the optimal running time that we provided for that very simple argument beginning.",
            "But more importantly, I didn't explain how I'm going to partition this."
        ],
        [
            "After graph over the processors.",
            "OK, so that's the last question we need to answer, and so the partition of the factor graph has to address.",
            "Things relate to storage, computation and communication, and so our goal here is to balance the computation while trying to minimize minimize overall communication.",
            "Now it's actually a lot harder than it would seem, and the reason why is if you look at this picture here.",
            "This noisy image problem before we run the algorithm, we didn't know which regions were going."
        ],
        [
            "Acquire the nurse work, but after we run the algorithm, we see that the regions that requires the most work were actually the boundaries of the noisy major.",
            "Trying to solve in the first place.",
            "So it's unlikely we'll be able to predict this region without actually having run our algorithm first.",
            "And if we were to just uniformly partition a graph or naively cut into two regions, we could place all the work on one CPU and very little work on the other, and this would not be a balanced computation.",
            "So we actually is a very simple technique."
        ],
        [
            "In which, rather than cutting the graph into two pieces for two processors, we could cut the graph into 12 pieces for two processors and then randomly assign those pieces to the individual processors.",
            "This has the advantage of increasing the balance, but the disadvantage of increasing the communication.",
            "Sorry, but in practice I don't have the party, but in practice when we do this, we actually find that the cost communications that is much less than you might expect an.",
            "I'll show the speedup curves in a moment."
        ],
        [
            "OK, so in summary, here's our parallel slash algorithm in which we over partition the factor graph randomly assigning the pieces of processors, and then we schedule the slashes locally using the belief residuals, and then we transmit the message along the boundary."
        ],
        [
            "So we implemented this in P threads in the shared memory setting and in MPI distributed memory setting we use a shared memory AMD system with four Quad core processors for up to 16 cores for shared memory, and we used A-15 node Quad core Dual Quad Core system with 120 cores total for this stupid memory setting and we test this on Markov logic networks."
        ],
        [
            "Alright, so first I'll show you the shared memory results.",
            "So this is a synchronous algorithm we described earlier.",
            "The very simple, parallel synchronously propagation.",
            "And then this is it, speed up.",
            "That's a bad until we show it.",
            "Flash looks like this.",
            "This flash running time and you can see on one processor we already beat synchronous BP on 16 processors.",
            "And of course we got pretty similar speedup.",
            "But of course this is not a fair speed up."
        ],
        [
            "Not an accurate speed of curve because all the speedup should be relative to the best single processor algorithm.",
            "If you saw our splash algorithms much faster, so if we correct for that, synchronous PvP doesn't really stop at all.",
            "OK. And then we can actually go to a much bigger model.",
            "Unfortunately synchronous."
        ],
        [
            "You know where converges on this model.",
            "But our algorithm doesn't adduser very quickly.",
            "And we get an amazing speedup.",
            "Because of course this problem is hard enough to actually challenge 16 processors.",
            "Of course, now we'd like to go even bigger model, so we go to 100."
        ],
        [
            "Many processors and distributed setting.",
            "And here we have amount of 8000 variables in four and 6000 factors.",
            "A single processor running time for this promise one hour.",
            "And we get a speedup curve, which goes.",
            "Actually, it goes better than linear up to around 90 processors.",
            "And then we get close to linear speedup at 120 process.",
            "So actually I give the answer here, but the the reason we go above linear is not as magical as in my team.",
            "We actually as you increase in process you also get more cash and so we get an added advantage of improved cash as we scale the system."
        ],
        [
            "Alright, so in summary we introduced our method of exploiting parallelism by running these independent parallel slashes.",
            "We also addressed the issue about rhythmic efficiency by using the splash structure and the belief residual scheduling, and then we address the issues of implementation efficiency by using our partitioning as well as several other methods that discuss.",
            "And then this allowed us to get experimental results with linear to super linear speedup 120 processors.",
            "Battle."
        ],
        [
            "OK question.",
            "A little while here.",
            "Yeah.",
            "Speed up your memory so the shared memory on the smaller model.",
            "I think we get up to.",
            "10 on 16 process.",
            "To speed up here.",
            "Or 8 sorry.",
            "So they challenged over to share machines.",
            "Is the memory issues that were discussed earlier, we don't.",
            "I mean our splash operation because it it hits each message twice within this flash.",
            "It does help with the cache efficiency, but we don't organize our messages in a cache efficient manner and so we can actually flood.",
            "And we've done some experiments to assess our memory usage.",
            "We flood the memory bus so.",
            "Yeah.",
            "So we built this in a distributed setting here.",
            "It does really well.",
            "This is 120.",
            "It's a a parallel system.",
            "It's a cluster of.",
            "A10 Story 15 processors each eight cores.",
            "15 sorry, 15 machines connected by Gigabit Ethernet.",
            "Yep.",
            "Spirit, just standard blades.",
            "Or actually I think their center.com.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repeatedly until they stop changing.",
                    "label": 0
                },
                {
                    "sent": "And so it's a natural parallel algorithm.",
                    "label": 0
                },
                {
                    "sent": "And like we showed earlier, we can take all of the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Messages split it over the processors and compute the new versions in parallel.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we get this earlier picture of an iterative message update.",
                    "label": 0
                },
                {
                    "sent": "This is synchronous algorithm and we might ask, does this have this sequential dependencies within efficiency?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can illustrate this by looking at this large loopy graph.",
                    "label": 0
                },
                {
                    "sent": "Embedded in this loopy graph could be a strong sequential sequence of independent variables forming affectively a chain.",
                    "label": 0
                },
                {
                    "sent": "And we can look at the performance of this synchronous algorithm on this chain.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can do this by introducing evidence at opposite ends and then running the synchron.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm at each iteration using P processors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see that we can move the messages in one iteration using P processors in time 2 and over P friend variables.",
                    "label": 0
                },
                {
                    "sent": "OK, now as you see, the evidence play moves one iteration each time and I need to move the evidence across the graph and so this will take an iteration through the evidence across the graph for running time of two X ^2 / P. OK, so how good is this?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can look back at the optimal sequential algorithm using one processor, and that's the past messages sequentially forward along the graph and then backward.",
                    "label": 0
                },
                {
                    "sent": "The running time of this single processor algorithm is too high.",
                    "label": 0
                },
                {
                    "sent": "So that means I would need P processors just to get the same running time as the optimal sequential algorithm with one processor and using two end processors.",
                    "label": 1
                },
                {
                    "sent": "On this problem, I would actually only get a factor of two speedup.",
                    "label": 0
                },
                {
                    "sent": "That's a lot of processors to only get a factor of 2 speed up.",
                    "label": 0
                },
                {
                    "sent": "OK, now we could.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the optimal algorithm for this problem and the optimal algorithm using only two processors is send messages sequentially in One Direction using one processor and sequentially another direction using a second processor, two processors, and the running time is N for a factor of 2 speed up.",
                    "label": 0
                },
                {
                    "sent": "And what this really identified is the sort of inherent sequential structure to this problem.",
                    "label": 1
                },
                {
                    "sent": "This requires efficient method scheduling.",
                    "label": 0
                },
                {
                    "sent": "But sort of unfortunate is unless we do introduce an additional approximation, it's not possible to get better than a factor to speed up here.",
                    "label": 0
                },
                {
                    "sent": "So let's look at additional approximation so you can can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the sequence of messages that we've passed sequentially all the way across the chain, it takes awhile.",
                    "label": 0
                },
                {
                    "sent": "And we could say then will supposedly replace the message from three to four with the uniform message.",
                    "label": 0
                },
                {
                    "sent": "This is effectively starting my chain at 4.",
                    "label": 0
                },
                {
                    "sent": "I can then pass the message sequentially.",
                    "label": 0
                },
                {
                    "sent": "And I might find in practice the message from 9 to 10 differs only slightly from the original true message to 9 to 10.",
                    "label": 0
                },
                {
                    "sent": "And then in general, there's this distance tell epsilon in which I must pass messages sequentially.",
                    "label": 0
                },
                {
                    "sent": "But after which I can replace the message with the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "We call this to tell us an approximation.",
                    "label": 0
                },
                {
                    "sent": "And so given this idea, we might ask how can we define a parallel algorithm which achieves a tellepsen approximation in the chain graphical model for every vertex, and it's actually very simple algorithm, so I'll show you now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we cut the chain over the processors defining within each block of central vertex, and then we actually run the very simple sequential forward backward algorithm, passing messages inward in the forward pass.",
                    "label": 0
                },
                {
                    "sent": "And backward in the outward pass.",
                    "label": 0
                },
                {
                    "sent": "We then exchanged messages across processors.",
                    "label": 0
                },
                {
                    "sent": "And we repeat this process.",
                    "label": 0
                },
                {
                    "sent": "And what's neat is this algorithm.",
                    "label": 1
                },
                {
                    "sent": "Actually Richie achieves a running time of an over P plus plus line for a tellepsen level approximation for every vertex, and we can actually show that this is the optimal running time for this problem.",
                    "label": 0
                },
                {
                    "sent": "For achieving a telephone approximation.",
                    "label": 0
                },
                {
                    "sent": "So this is an optimal algorithm on the chain.",
                    "label": 0
                },
                {
                    "sent": "And we did.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To generalize this idea to loopy graphs because after all chains are pretty boring.",
                    "label": 0
                },
                {
                    "sent": "And we do that by as before constructing or selecting a center vertex, then growing a breadth first search spanning tree.",
                    "label": 1
                },
                {
                    "sent": "And then passing messages sequentially from the boundary of the tree into the root.",
                    "label": 0
                },
                {
                    "sent": "And then from the route back to the boundary in the forward pass backward pass form.",
                    "label": 1
                },
                {
                    "sent": "And so this is sort of a natural generalization of this.",
                    "label": 0
                },
                {
                    "sent": "This chain algorithm described earlier, and I would like to make this into a full large scale parallel algorithm for large loopy graphs.",
                    "label": 0
                },
                {
                    "sent": "And so we'll do it just as we did before.",
                    "label": 0
                },
                {
                    "sent": "We'll take our large graphical model.",
                    "label": 0
                },
                {
                    "sent": "Split it over our processors.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then on each processor run this splash.",
                    "label": 0
                },
                {
                    "sent": "This generalization of this forward backward procedure.",
                    "label": 0
                },
                {
                    "sent": "Exchange messages across processors and repeat.",
                    "label": 0
                },
                {
                    "sent": "And so when we do this, we have a few key challenges we have to address which are here.",
                    "label": 0
                },
                {
                    "sent": "We have to explain how we're going to schedule splashes on each of these processors.",
                    "label": 0
                },
                {
                    "sent": "And how do we partition the graph?",
                    "label": 1
                },
                {
                    "sent": "So let's first address how we're going to schedule flashes.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we actually adopted very simple heuristic, which we call belief residual scheduling, and we essentially assign priorities based on the cumulative change in the belief.",
                    "label": 1
                },
                {
                    "sent": "So every time we produce to receive a new message to a vertex, we update the belief that vertex.",
                    "label": 0
                },
                {
                    "sent": "And look how much it changes.",
                    "label": 0
                },
                {
                    "sent": "And we record the accumulated change for all the messages that protect receipts.",
                    "label": 1
                },
                {
                    "sent": "And so effectively a vertexes belief has changed substantially since he was last updated will likely produce informative messages, and so that's a vertex we'd like to schedule sooner.",
                    "label": 0
                },
                {
                    "sent": "So then, using this notion of priority, we can actually introduce a priority queue on each processor.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we take the vertex with the highest priority.",
                    "label": 0
                },
                {
                    "sent": "And constructive slash centered at that vertex.",
                    "label": 0
                },
                {
                    "sent": "When we do that, we're going to send messages, change the priority of other vertices on that processor.",
                    "label": 0
                },
                {
                    "sent": "Then demote the top vertex.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then schedule the next flash.",
                    "label": 0
                },
                {
                    "sent": "Repeat this procedure until the properties of these vertices fall below our convergence threshold epsilon.",
                    "label": 0
                },
                {
                    "sent": "And so this these priorities allow us to actually do something very neat to these flashes we can.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dynamically prune them.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this graph here, there are a lot of races, low belief residual which are effectively almost converged and we have this region with high belief residual.",
                    "label": 1
                },
                {
                    "sent": "And we grow our spanning tree.",
                    "label": 0
                },
                {
                    "sent": "We can actually have it adapt to the region of the graph that hasn't converged, and focusing our computation where it is most needed.",
                    "label": 0
                },
                {
                    "sent": "OK, and this actually allows us to automatically determ.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The appropriate size for splashes without having to fix the primary events.",
                    "label": 0
                },
                {
                    "sent": "So if we don't include pruning, you can see there is sort of an optimal size for these flashes.",
                    "label": 0
                },
                {
                    "sent": "But if we introduce pruning, this flashes automatically scale as the algorithm needs.",
                    "label": 0
                },
                {
                    "sent": "And so actually illustrate this is a neat video.",
                    "label": 0
                },
                {
                    "sent": "I had this synthetic noising problem.",
                    "label": 0
                },
                {
                    "sent": "It's a sunset, the synthetic sunset and I'm going to represent it as a factor graph on this rate plot.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show the number of updates.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a brighter region in this picture this video.",
                    "label": 0
                },
                {
                    "sent": "Let's see here.",
                    "label": 0
                },
                {
                    "sent": "So you see initially, as these splashes are uniformly spread over the space.",
                    "label": 0
                },
                {
                    "sent": "For the algorithm proceeds, it quickly identifies a region of the graph that need the most effort.",
                    "label": 0
                },
                {
                    "sent": "And then it focuses on those regions tored the end, solving the important updates rather than the entire picture.",
                    "label": 1
                },
                {
                    "sent": "Far to use a synchronous algorithm might have updated all of this black region just as often is that that bright region.",
                    "label": 0
                },
                {
                    "sent": "So there's a sequential component right here as well.",
                    "label": 0
                },
                {
                    "sent": "OK, and so in summary, this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are parallel slash algorithm which we've basically placed.",
                    "label": 0
                },
                {
                    "sent": "These are factor graph partitioning over the processors and then we run these local slashes scheduling using these belief priorities.",
                    "label": 1
                },
                {
                    "sent": "Of course, I didn't tell you how we're going to, sorry.",
                    "label": 0
                },
                {
                    "sent": "So first I want to say that if we do this with a with a uniform partitioning on the chain, we can actually recover the optimal running time that we provided for that very simple argument beginning.",
                    "label": 1
                },
                {
                    "sent": "But more importantly, I didn't explain how I'm going to partition this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After graph over the processors.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the last question we need to answer, and so the partition of the factor graph has to address.",
                    "label": 1
                },
                {
                    "sent": "Things relate to storage, computation and communication, and so our goal here is to balance the computation while trying to minimize minimize overall communication.",
                    "label": 0
                },
                {
                    "sent": "Now it's actually a lot harder than it would seem, and the reason why is if you look at this picture here.",
                    "label": 0
                },
                {
                    "sent": "This noisy image problem before we run the algorithm, we didn't know which regions were going.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acquire the nurse work, but after we run the algorithm, we see that the regions that requires the most work were actually the boundaries of the noisy major.",
                    "label": 0
                },
                {
                    "sent": "Trying to solve in the first place.",
                    "label": 0
                },
                {
                    "sent": "So it's unlikely we'll be able to predict this region without actually having run our algorithm first.",
                    "label": 0
                },
                {
                    "sent": "And if we were to just uniformly partition a graph or naively cut into two regions, we could place all the work on one CPU and very little work on the other, and this would not be a balanced computation.",
                    "label": 0
                },
                {
                    "sent": "So we actually is a very simple technique.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which, rather than cutting the graph into two pieces for two processors, we could cut the graph into 12 pieces for two processors and then randomly assign those pieces to the individual processors.",
                    "label": 0
                },
                {
                    "sent": "This has the advantage of increasing the balance, but the disadvantage of increasing the communication.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but in practice I don't have the party, but in practice when we do this, we actually find that the cost communications that is much less than you might expect an.",
                    "label": 0
                },
                {
                    "sent": "I'll show the speedup curves in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in summary, here's our parallel slash algorithm in which we over partition the factor graph randomly assigning the pieces of processors, and then we schedule the slashes locally using the belief residuals, and then we transmit the message along the boundary.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we implemented this in P threads in the shared memory setting and in MPI distributed memory setting we use a shared memory AMD system with four Quad core processors for up to 16 cores for shared memory, and we used A-15 node Quad core Dual Quad Core system with 120 cores total for this stupid memory setting and we test this on Markov logic networks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so first I'll show you the shared memory results.",
                    "label": 1
                },
                {
                    "sent": "So this is a synchronous algorithm we described earlier.",
                    "label": 0
                },
                {
                    "sent": "The very simple, parallel synchronously propagation.",
                    "label": 0
                },
                {
                    "sent": "And then this is it, speed up.",
                    "label": 0
                },
                {
                    "sent": "That's a bad until we show it.",
                    "label": 0
                },
                {
                    "sent": "Flash looks like this.",
                    "label": 1
                },
                {
                    "sent": "This flash running time and you can see on one processor we already beat synchronous BP on 16 processors.",
                    "label": 0
                },
                {
                    "sent": "And of course we got pretty similar speedup.",
                    "label": 0
                },
                {
                    "sent": "But of course this is not a fair speed up.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not an accurate speed of curve because all the speedup should be relative to the best single processor algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you saw our splash algorithms much faster, so if we correct for that, synchronous PvP doesn't really stop at all.",
                    "label": 0
                },
                {
                    "sent": "OK. And then we can actually go to a much bigger model.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately synchronous.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know where converges on this model.",
                    "label": 0
                },
                {
                    "sent": "But our algorithm doesn't adduser very quickly.",
                    "label": 0
                },
                {
                    "sent": "And we get an amazing speedup.",
                    "label": 0
                },
                {
                    "sent": "Because of course this problem is hard enough to actually challenge 16 processors.",
                    "label": 0
                },
                {
                    "sent": "Of course, now we'd like to go even bigger model, so we go to 100.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many processors and distributed setting.",
                    "label": 0
                },
                {
                    "sent": "And here we have amount of 8000 variables in four and 6000 factors.",
                    "label": 0
                },
                {
                    "sent": "A single processor running time for this promise one hour.",
                    "label": 1
                },
                {
                    "sent": "And we get a speedup curve, which goes.",
                    "label": 1
                },
                {
                    "sent": "Actually, it goes better than linear up to around 90 processors.",
                    "label": 0
                },
                {
                    "sent": "And then we get close to linear speedup at 120 process.",
                    "label": 0
                },
                {
                    "sent": "So actually I give the answer here, but the the reason we go above linear is not as magical as in my team.",
                    "label": 0
                },
                {
                    "sent": "We actually as you increase in process you also get more cash and so we get an added advantage of improved cash as we scale the system.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so in summary we introduced our method of exploiting parallelism by running these independent parallel slashes.",
                    "label": 0
                },
                {
                    "sent": "We also addressed the issue about rhythmic efficiency by using the splash structure and the belief residual scheduling, and then we address the issues of implementation efficiency by using our partitioning as well as several other methods that discuss.",
                    "label": 1
                },
                {
                    "sent": "And then this allowed us to get experimental results with linear to super linear speedup 120 processors.",
                    "label": 1
                },
                {
                    "sent": "Battle.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK question.",
                    "label": 0
                },
                {
                    "sent": "A little while here.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Speed up your memory so the shared memory on the smaller model.",
                    "label": 0
                },
                {
                    "sent": "I think we get up to.",
                    "label": 0
                },
                {
                    "sent": "10 on 16 process.",
                    "label": 0
                },
                {
                    "sent": "To speed up here.",
                    "label": 0
                },
                {
                    "sent": "Or 8 sorry.",
                    "label": 0
                },
                {
                    "sent": "So they challenged over to share machines.",
                    "label": 0
                },
                {
                    "sent": "Is the memory issues that were discussed earlier, we don't.",
                    "label": 0
                },
                {
                    "sent": "I mean our splash operation because it it hits each message twice within this flash.",
                    "label": 0
                },
                {
                    "sent": "It does help with the cache efficiency, but we don't organize our messages in a cache efficient manner and so we can actually flood.",
                    "label": 0
                },
                {
                    "sent": "And we've done some experiments to assess our memory usage.",
                    "label": 0
                },
                {
                    "sent": "We flood the memory bus so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we built this in a distributed setting here.",
                    "label": 0
                },
                {
                    "sent": "It does really well.",
                    "label": 0
                },
                {
                    "sent": "This is 120.",
                    "label": 0
                },
                {
                    "sent": "It's a a parallel system.",
                    "label": 0
                },
                {
                    "sent": "It's a cluster of.",
                    "label": 0
                },
                {
                    "sent": "A10 Story 15 processors each eight cores.",
                    "label": 0
                },
                {
                    "sent": "15 sorry, 15 machines connected by Gigabit Ethernet.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Spirit, just standard blades.",
                    "label": 0
                },
                {
                    "sent": "Or actually I think their center.com.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}