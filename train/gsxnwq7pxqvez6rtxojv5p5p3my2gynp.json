{
    "id": "gsxnwq7pxqvez6rtxojv5p5p3my2gynp",
    "title": "Learning invariant features using the Transformed Indian Buffet Process",
    "info": {
        "author": [
            "Joseph L. Austerweil, Computational Cognitive Science Lab, Department of Psychology, UC Berkeley"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_austerweil_lif/",
    "segmentation": [
        [
            "Thank you, the human brain is constantly bombarded with information from its senses."
        ],
        [
            "To cope with this, it learns feature representations that encode reoccuring objects.",
            "This is also an important problem in machine learning.",
            "One approach that's been successful in the nonparametric Bayesian statistics literature is the Indian buffet process.",
            "It learns two matrices, one of which encodes whether or not each object has each feature, and another matrix which encodes how each features instantiated into the images.",
            "It does this without presupposing the number of features is known ahead of time, although this has been really successful.",
            "Unfortunately, it presupposes that the features occur identically across instantiation, so occurs the same way every single time.",
            "This isn't realistic of the world that we live in.",
            "When I secured my eye, many of the features are still in my retinal image, but they've been transformed to occur in a different place in the retinal image.",
            "So what we did was following on.",
            "Soteras and others work for the transform dealership process.",
            "We extended the Indian buffet process to include transformations.",
            "So what this does is it's a simple extension to the generative process where when a customer or an object comes into the restaurant and it takes a dish or a feature, she puts a spice on it from the spice rack.",
            "And you can think of the spice as its transformation.",
            "So each objects features get its own transformation.",
            "And then afterwards you compare the transformed features to the data distribution to see how well that encodes the objects that you've observed.",
            "So once you allow objects and features to be transformed, you start getting some ambiguity about what the appropriate feature representation should be.",
            "So on the object."
        ],
        [
            "The left there's at least two good future representations, the one on the top, which is 1 feature that has two vertical bars and the one on the bottom that has two features, each of which is a vertical bar with its own horizontal translation, and which feature representation is appropriate is going to depend on the context that object occurs in.",
            "So if it occurs in the context that I've called the unitized context, then the top feature representation only having one is going to be appropriate.",
            "According to our intuitions and also the model, and also then with these separate objects at the two second feature representation is more appropriate.",
            "And this also has important consequences for generalization and behavior.",
            "As you can see that the unitized object set excuse me feature set.",
            "If you're using that, then you generalize to new unitized objects that you haven't observed before, but you don't generalize.",
            "Two new independent locations of the vertical bars, but if you have the separate feature representation then you generalize both to the new unit eyes and two.",
            "A new separate object, and this is something that arm."
        ],
        [
            "Well predicts.",
            "And now you might think to extend this that we should just put in all the transformations we could possibly think of, and that way we can get sort of the most invariant features.",
            "But it turns out that that's inappropriate for human feature learning, and this has been pointed out from a long time ago in the 1800s by Ernst Mach.",
            "So if you take a square and rotate, it becomes a diamond, and so the identity of the shape is changed when you transform its rotation, and so our hypothesis is that for learning.",
            "Which transformations are allowed?",
            "Which type of transformations are going to look at the objects you've observed to be a member of that shape class, and the model is easy to extend.",
            "To do that, you just include a binary variable for each type of transformation, and we found that both the model and people, if they're given squares that change in size, then they'll generalize to a new larger square, but not to the diamond.",
            "But if you have a square and you have different orientations of it, then that square will generalize too.",
            "Diamond thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, the human brain is constantly bombarded with information from its senses.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To cope with this, it learns feature representations that encode reoccuring objects.",
                    "label": 0
                },
                {
                    "sent": "This is also an important problem in machine learning.",
                    "label": 0
                },
                {
                    "sent": "One approach that's been successful in the nonparametric Bayesian statistics literature is the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "It learns two matrices, one of which encodes whether or not each object has each feature, and another matrix which encodes how each features instantiated into the images.",
                    "label": 0
                },
                {
                    "sent": "It does this without presupposing the number of features is known ahead of time, although this has been really successful.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it presupposes that the features occur identically across instantiation, so occurs the same way every single time.",
                    "label": 1
                },
                {
                    "sent": "This isn't realistic of the world that we live in.",
                    "label": 0
                },
                {
                    "sent": "When I secured my eye, many of the features are still in my retinal image, but they've been transformed to occur in a different place in the retinal image.",
                    "label": 0
                },
                {
                    "sent": "So what we did was following on.",
                    "label": 0
                },
                {
                    "sent": "Soteras and others work for the transform dealership process.",
                    "label": 0
                },
                {
                    "sent": "We extended the Indian buffet process to include transformations.",
                    "label": 1
                },
                {
                    "sent": "So what this does is it's a simple extension to the generative process where when a customer or an object comes into the restaurant and it takes a dish or a feature, she puts a spice on it from the spice rack.",
                    "label": 0
                },
                {
                    "sent": "And you can think of the spice as its transformation.",
                    "label": 0
                },
                {
                    "sent": "So each objects features get its own transformation.",
                    "label": 0
                },
                {
                    "sent": "And then afterwards you compare the transformed features to the data distribution to see how well that encodes the objects that you've observed.",
                    "label": 1
                },
                {
                    "sent": "So once you allow objects and features to be transformed, you start getting some ambiguity about what the appropriate feature representation should be.",
                    "label": 0
                },
                {
                    "sent": "So on the object.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The left there's at least two good future representations, the one on the top, which is 1 feature that has two vertical bars and the one on the bottom that has two features, each of which is a vertical bar with its own horizontal translation, and which feature representation is appropriate is going to depend on the context that object occurs in.",
                    "label": 0
                },
                {
                    "sent": "So if it occurs in the context that I've called the unitized context, then the top feature representation only having one is going to be appropriate.",
                    "label": 0
                },
                {
                    "sent": "According to our intuitions and also the model, and also then with these separate objects at the two second feature representation is more appropriate.",
                    "label": 0
                },
                {
                    "sent": "And this also has important consequences for generalization and behavior.",
                    "label": 0
                },
                {
                    "sent": "As you can see that the unitized object set excuse me feature set.",
                    "label": 0
                },
                {
                    "sent": "If you're using that, then you generalize to new unitized objects that you haven't observed before, but you don't generalize.",
                    "label": 0
                },
                {
                    "sent": "Two new independent locations of the vertical bars, but if you have the separate feature representation then you generalize both to the new unit eyes and two.",
                    "label": 0
                },
                {
                    "sent": "A new separate object, and this is something that arm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well predicts.",
                    "label": 0
                },
                {
                    "sent": "And now you might think to extend this that we should just put in all the transformations we could possibly think of, and that way we can get sort of the most invariant features.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that that's inappropriate for human feature learning, and this has been pointed out from a long time ago in the 1800s by Ernst Mach.",
                    "label": 0
                },
                {
                    "sent": "So if you take a square and rotate, it becomes a diamond, and so the identity of the shape is changed when you transform its rotation, and so our hypothesis is that for learning.",
                    "label": 0
                },
                {
                    "sent": "Which transformations are allowed?",
                    "label": 0
                },
                {
                    "sent": "Which type of transformations are going to look at the objects you've observed to be a member of that shape class, and the model is easy to extend.",
                    "label": 0
                },
                {
                    "sent": "To do that, you just include a binary variable for each type of transformation, and we found that both the model and people, if they're given squares that change in size, then they'll generalize to a new larger square, but not to the diamond.",
                    "label": 0
                },
                {
                    "sent": "But if you have a square and you have different orientations of it, then that square will generalize too.",
                    "label": 0
                },
                {
                    "sent": "Diamond thank you.",
                    "label": 0
                }
            ]
        }
    }
}