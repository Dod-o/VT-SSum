{
    "id": "55ilsfizzhhrvlgfteekxeyh4xe4pxpc",
    "title": "Dirichlet Process: Practical Course",
    "info": {
        "author": [
            "Dilan G\u00f6r\u00fcr, Yahoo! Research"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/mlss2012_gorur_dirichlet_practical/",
    "segmentation": [
        [
            "Alright, so.",
            "Peter did a very nice introduction to Bayesian nonparametric Santa driven processes and this is going to be.",
            "Semi practical course.",
            "About the additional processes, so I'm going to be using."
        ],
        [
            "The code from you I tell, which hopefully all of you by now has.",
            "And what you need to do initially is basically change to the code directory directory and run init path to initialize the path.",
            "And then."
        ],
        [
            "Throughout the lecture I'm going to show you different demo so you know what part of that toolbox is used for what.",
            "So as Peter was talking about, there's a process is a tool for defining Bayesian nonparametric models.",
            "And there are other well known processes in machine learning for defining, again, based in nonparametric models like Gaussian processes, which will be talking about tomorrow ending before process which Peter talked about and Kim's coalescent which I talked about yesterday.",
            "So but the focus now is under the process is so there's a process is basically.",
            "Distribution over distributions which you can use to represent your uncertainty about the parametric form of the distribution that you want to use in your model.",
            "So the typical thing to do would be in a Bayesian parametric setting would be to have a bunch of parameters for your model and then represent the uncertainty about the value of those parameters by in terms of some parametric distribution.",
            "Let's say a Gaussian distribution.",
            "But you don't know the value of the parameter, so you would put a Gaussian distribution on top of it with some mean and some covariance and you will try to get the posterior of that Gaussian when you do inference, whereas with additional processes your instead of putting that Gaussian, you're putting the process with a certain base distribution on the pyramid on the distribution itself, so that you're not only learn the perimeter, but also learn the.",
            "Kind of the parametric form, or like the roughly the form of the distribution itself as well.",
            "So this is something a little harder to understand.",
            "You need to get used to the idea of using Bayesian non parametrics to understand it.",
            "This is this is a.",
            "This is a very simplistic way of thinking about the judicial process.",
            "Basically as a glorified dishnet distribution, glorified because it's over a countably many infinitely many dimensions, so we."
        ],
        [
            "'cause the dish, the distribution and addition process is very closely related.",
            "I wanted to go over the basic properties of the additional distribution.",
            "I don't know why this keeps changing.",
            "No, it's a spoiler, so the K dimensional do, so there's the distribution we write like this is the digital distribution with parameters A1 to Alpha K. Over this vector that is Kaden dimensional Pi 1 to \u03c0 K. So we denote the distribution like this and the distribution is defined like this.",
            "So the indices of this vector or the entries of this vector are non negative and they sum to one.",
            "So this I used to denote the probability simplex, 2K dimensional probability simplex.",
            "So the scale of the dish, the parameters these alphas determines the concentration of the probability mass.",
            "On the probability simplex."
        ],
        [
            "For example.",
            "So about that.",
            "So if you have additional distribution with parameters 111.",
            "Then you have this kind of a distribution over the simplex, so it's a uniform distribution.",
            "So these are these are the parameters, and if so we have K parameters for the K dimensional.",
            "There's a distribution, and if all these parameters are equal, we call this symmetric dishware distribution.",
            "You have probably heard of this when talking about the reset process.",
            "So of course it doesn't always have to be equal.",
            "Depending on your modeling needs, you may set these parameters differently, but just for now, let's look at what happens when you play around with the scale of these parameters.",
            "So these are."
        ],
        [
            "These are values where maybe I can just close it.",
            "OK, so these are.",
            "The density distribution densities with parameters 222555, TEN, 1010.",
            "So you see that as you increase the scale of the parameters, the distribution gets more and more peaked.",
            "Not that these are all greater than one, so it's interesting to see what happens when things are.",
            "I will pause it."
        ],
        [
            "These are greater than smaller than one, so when the parameter is smaller than one, then rather than focus on getting concentrated on the center of this triangle, the probability mass starts going towards the edges and the more you decrease the values away from one, the more concentrated around the edges they become.",
            "So this is an interesting property for the.",
            "Through the distribution."
        ],
        [
            "And."
        ],
        [
            "This is basically."
        ],
        [
            "These are."
        ],
        [
            "Basically what it looks like when you have different values for when you don't."
        ],
        [
            "Those magic 3 shows, but the slideshow doesn't really want you to look at it very long.",
            "You can play around with the distributions by using the."
        ],
        [
            "There density that M you don't need to do that now, but this is basically going to show you what happens when you input different parameter values and so on."
        ],
        [
            "So the additional distribution again has an interesting property where if you combine entries of additional distributed probable two vector, you will still end up with distributed vectors but with less number of dimensions this time.",
            "So if we have a K dimensional vector pipe that is rich distributed with parameters A1 to Alpha K, and if we simply sum up.",
            "The first 2 entries of this vector and similarly sum up the first 2 parameters of this distribution.",
            "This is what we get.",
            "So the vector with one less dimensions is still distributed with this shit.",
            "This time all the parameters the same except for this first parameter being the sum of the first 2 parameters there.",
            "And this is not specific to.",
            "Of course, the first dimension at first dimensions only.",
            "You can do it for any dimensions and the converse of this is also true.",
            "So you can have parameters vector of K dimension.",
            "Difference being there said distributed with A1 to Alpha K If you sample.",
            "These T1T2 values from a 2 dimensional drishtee which is also called the better function.",
            "A bit better distribution and then you multiply the corresponding entries with the corresponding values like this by 1 * 10 to one and \u03c0 one time startup similar to A1 times better one and A1 times better 2.",
            "I think there's a typo there.",
            "Making sure that this better one, plus with better two is equal to 1, then you get additional distribution so.",
            "You can run DP generate.",
            "To see that we can actually draw from additional process by using this property.",
            "So we start with a uniform distribution over zero 10.",
            "Therefore this is at 0.1 and we keep refining by drawing from this better distribution.",
            "We keep refining this space.",
            "Like this for example.",
            "So when we first draw from the beta distribution, it divides it into 2.",
            "Two parts and the height of that is proportional to the values that we draw, and then we'll keep recursing on each of these helps like this like this.",
            "And if we do it long enough.",
            "We're going to have something like this, so you actually you're going to have infinitesimals.",
            "Infinitesimal intervals, some of which will have huge spikes and some of most of which will have tiny spikes.",
            "So this is a very easy and fun way of obtaining draw from the process, actually.",
            "It shows you that you can get it as the infinite limit of the distribution."
        ],
        [
            "There are different ways where you can define the process.",
            "You can construct samples from the process or you can sample from the process and one way to do it is by using the stick breaking construction.",
            "So if you go to the code the SB generates dot M code is going to generate sticks for you, so these are going to be sticks that are that are representative of the.",
            "Of these weights for the dish lid process.",
            "Parameter values, so the sticks are going to look like this because they are defined using this recursive formula where new K is drawn from the beta 1A distribution in expectation they're going to be exponentially decreasing, so this red parts are the stick length of the process, so these are going to be exponentially decreasing in expectation.",
            "Therefore you're going to get to infinitesimally small values.",
            "Not very long from starting sampling, so.",
            "Here I."
        ],
        [
            "Generated some sticks, so with different Alpha parameters this is a good way of seeing the effect of the Alpha parameter, so the Alpha parameter of the digital process controls.",
            "The the expected number of clusters in the process mixture models, for example.",
            "You can say you can say it also.",
            "The reason why it does that is it controls the distribution of these \u03c0 case.",
            "So if Alpha is small then the Pi case will decrease much faster and only a small number of them are going to have significant values.",
            "Whereas if Alpha is larger than they're going to more and more sticks will have significant mass compared to others.",
            "But there will be still infinitely many of them with infinitesimal mass.",
            "So these are examples from Alpha equals 1A equals 5A equals 25, and so on."
        ],
        [
            "And so in the previous slide, I only showed you the length of the sticks in the order that I sampled them.",
            "So we started with this."
        ],
        [
            "Remember, so we started with a stick of feeling unit length and then we broke the stick.",
            "Here we kept this as the as the stick.",
            "The first stick length and then we recurse on this remaining part, broke it off here.",
            "This was the second statement and so on."
        ],
        [
            "And.",
            "And basically I show here the sticks in the order that I draw them.",
            "These being the stick identity.",
            "So the first sticks I can state Congress and so on here."
        ],
        [
            "I'm showing it in a different way.",
            "I also sampled theaters from the base distribution G Note and now this is the value of Theta and these are basically the length of the sticks the same length as before.",
            "I just wanted to show you that.",
            "The Alpha parameter makes it makes the distribution concentrate more and more on the base distribution.",
            "When it is larger, so that's why it's called the concentration parameter it to me it's first sounded counter intuitive that a parameter that has a value is small, and if it has a value that small, it leads to less number of components.",
            "So it sounds like it is more concentrated.",
            "The model is more concentrated, and when you increase the value of Alpha it gets more and more components.",
            "That sounds like is less concentrated model.",
            "But the concentration here is about concentrating on the base distribution rather than concentrating on like a few number of components.",
            "OK, so as I said you can.",
            "You can run this code using the SP generate code.",
            "By the way, if there are any questions about.",
            "About the way to run the code or what is in the code.",
            "This is supposed to be a practical section, so just let me know."
        ],
        [
            "Another way to generate from the three step process is basically using the polius aren't scheme, which is different from the stick breaking construction.",
            "So here we don't have any Pi values, so the Pi values don't going to play here.",
            "We're directly sampling the data values and the way we do that is we start with data one sampled from the note and every consecutive sample is sampled by.",
            "Conditioning on that title one.",
            "Andy Andy, not the not.",
            "The collection of basically did not.",
            "The posterior of the digital process conditioned on the 1st and samples.",
            "So this is the form GN is going to take and GI is going to be sampled from GI minus one basically, so we iteratively sample.",
            "So the interesting thing about this sampling scheme is that GN converges almost surely as N goes to Infinity to random discrete distribution G. And she has additional process Alpha genomic distribution.",
            "Which means the samples the collection of samples that you get, the sequence of Theta I I from one to N is the sample from G. OK."
        ],
        [
            "So here is an example of doing that.",
            "So we start with a base distribution like this, a Gaussian based distribution that's G, not.",
            "We sample from this and we sample this location.",
            "So it's going to have some weights.",
            "Basically."
        ],
        [
            "Its weight is going to be one over Alpha plus N and the weight of the Geno distribution is going to be Alpha."
        ],
        [
            "Like this and then G2 is going to be including this conditioning on this.",
            "We sample another point and that gives us G2 and we keep doing this to get."
        ],
        [
            "More and more samples.",
            "So as we increase the number of samples, this is what we get for G. 10,000.",
            "So this is this is a sample from the dish.",
            "This is obtained using the polio urn sampling scheme.",
            "OK, so again about."
        ],
        [
            "The concentration parameter.",
            "So if we have Alpha equals one, this is what sample from the DP looks like.",
            "If we have Alpha equals 100, this is what our sample from the DP looks like.",
            "So you can see again that for the case where Alpha is small, the concentration parameter is small.",
            "The way the discrete distribution looks like is nothing like the base distribution, whereas the discrete distribution looks more like kind of a discrete discretized.",
            "Histogram of the of the base distribution just re emphasize that.",
            "That's why Alpha is called the concentration parameter."
        ],
        [
            "OK, so there's a fun interactive.",
            "Demo that you can also run.",
            "This is basically.",
            "Right now it's only the samples from the base distribution and an average of all the samples we have so far, with weighted more heavily towards the recent samples.",
            "So the blue ones are single samples and the black one is an average of all the samples.",
            "And this is the additional process with conjugates Gaussian based distribution.",
            "So when we click on the plot, that means we add one data point OK at the Red Cross.",
            "Here at somewhere close to 10 I added one data point and you can see that the process is now focusing more on this point, so it's kind of peaking towards that, although there is still some probability of having other peaks away from that point.",
            "But if I just.",
            "Keep adding data points.",
            "There you see that more and more evidence is gathered, so the model is more and more sure about where the peak should be right.",
            "If I now introduce other data, let's say some outlier data which is away from this peak, it still has some information, right?",
            "It gives the information that there is not only one peak there, but there may be something going on here as well, so the samples are also getting peeked around that, but not as heavily as that, because basically is overwhelming.",
            "But if I add more and more data points now we see that.",
            "Is focusing on a bimodal distribution right?",
            "So you can have fun with this code.",
            "Just clicking around to introduce more and more peaks if you want or to join the peaks that you created like this to again go back to one peak or two peaks and so on so.",
            "It basically shows you that the reset process is.",
            "Flexible tool that can flexibly adjust to new data so you don't have to pre specify the number of components.",
            "It's going to find the number of components that is necessary to fit the data and as you add more and more data is going to be able to flexibly change to fit the new data."
        ],
        [
            "So.",
            "So let's compare the finite mixture models with this.",
            "So I'm showing here again A1 dimensional density fit for the finite mixture model where.",
            "By by the red curve I showed the mean by the dashed blue tick curve I showed the median and then this is the 95% quantile.",
            "So you and all the rest is basically samples from the prior.",
            "So this is a prior distribution for a two component finite mixture model.",
            "So if we are."
        ],
        [
            "Other data points that are shown by these black dots here.",
            "And we fit this two component finite mixture model.",
            "We see that we get these nice two peaks around where the data lies and uncertainty of where these the.",
            "Probability mass should lie decreases, so compare the Gray area here to the Gray area here.",
            "So since we have more data now we have less uncertain about what the model should look like, right?",
            "So it's doing a quite good job so and we can say we can see clearly that the data.",
            "Is clustered into clusters, so this is a reasonable estimate.",
            "I actually generated the data from 2 Gaussians."
        ],
        [
            "But here I generated the data from 3 Gaussians.",
            "So the true model for this data would be 3 components mixture model, whereas I used again two component mixture model to fit the data.",
            "Now you can see that there is this great Gray area, which is a lot."
        ],
        [
            "Larger than here, so it's a good."
        ],
        [
            "Sing about the Bayesian framework that when there is uncertainty about the about the model this learned, then it still expresses that uncertainty, so you don't need to discard Bayesian models altogether.",
            "An migrate altogether too based in Nonparametrics but basically all it's telling you is it finds bimodal distribution because that's all it can do, but there is high, so high uncertainty.",
            "OK, so it's self guides those self guards you by saying well I'm very uncertain about what's going on here, but it still gives you the model with two modes only."
        ],
        [
            "OK, this is now the samples from the prior for the additional process case.",
            "So Adresa process with conjugates based distribution which is Gaussian distributed."
        ],
        [
            "And this is the fit to data that's generated from 2 Gaussians.",
            "As expected, you have these two peaks.",
            "And."
        ],
        [
            "This is data generated from 3 Gaussians and now you have three peaks, so this shouldn't be surprising because we know that the addition process is going to fit to the data and it's going to use the necessary amount of components to be able to have a good representation of data.",
            "OK, so.",
            "You can see things like this.",
            "For example, there are peaks if you sample from the posterior, there are peaks at places where there is not any data at all.",
            "So that comes because the process still gives some probability to other places.",
            "Then it's so data.",
            "So you may see this as an advantage or disadvantage if you believe that your data is only what you saw and there will be no other.",
            "Data lying outside of the region that you observed data before then maybe this is something to worry about.",
            "An maybe actually this is a model that you shouldn't use then because you don't want all that flexibility you want to constrain your model.",
            "But if you want to be flexible and give some probability to data coming from other regions, then this may not be such a bad idea to use this."
        ],
        [
            "OK, so.",
            "The infinite mixture model using the additional processes is defined by this, so we have a.",
            "We have a data points or we have N data points indexed by.",
            "I hear conditioned on some parameter and that parameter defines the distribution of the data.",
            "And that parameter is drawn from some distribution G which is drawn from the Trisha process.",
            "With these parameters Alpha the concentration parameter, Angie, not the base distribution, and this is the graphical model showing this.",
            "These equations basically so this is, this is how we define the infinite mixture model."
        ],
        [
            "Equivalently, we can show it like this, where we have pies and status rather than gezond.",
            "Yeah, rather than GS we have pies and status where we basically represent G using this infinite sum that comes from the stick breaking construction.",
            "OK."
        ],
        [
            "So.",
            "Given this model, how do we do inference on it?",
            "So typically the easiest case to use these models in practice is to use a conjugate base distribution so that the parameters can be integrated out and the predictive probabilities can be computed using the sufficient statistics of the data.",
            "So Peter was talking about two kinds of conjugacy.",
            "One is the quantity of the dresser process itself, so the posterior process is still there is a process that's one type of conjugacy.",
            "And that's always present in the process models, but at least the.",
            "Vanilla there's a process models, let's say.",
            "And the other type of conjugacy is about the conjugacy of the parameters of the model to the base distribution.",
            "So if you want to model an infinite mixture model with Gaussian components, you need to specify a prior on top of those components, and that prior needs to be conjugate to the Gaussian distribution.",
            "Because you assume Gaussian components for for the data generating process.",
            "So what is the?",
            "What is the conjugate distribution for the Gaussian distribution?",
            "So if you're using both, if you.",
            "If you assume that both mean and covariance is uncertain, that they're not known, then normal inverse Wishart distribution is the conjugate to the Gaussian distribution.",
            "Or if you're fixing the fixing the mean and assuming only you want to learn the covariance, then you need the inverse Wishart distribution on the covariance.",
            "Or if you know what the what the covariances or you're pretty sure you can set it finally and you only want to put a distribution over the means, then the normal distribution is the conjugate distribution for the normal means OK, and Similarly if you have discrete data, if you're using multinomial distribution for example to model your data, than the distribution is the conjugate distribution for the multinomial, so that you would use a.",
            "A recent multinomial models for that.",
            "OK, so.",
            "Of course we are not tight to using conjugate base distributions, but things become a little more tricky when you when you want to use non conjugate models.",
            "And that's why here I'm going to be talking only about how to do inference for the conjugate case.",
            "So in Gibbs sampling what you do at each iteration is for each data point you remove the data point from its current component.",
            "So you start off with some random initially.",
            "Initializations of some initialization of a number of components with assignments to of data points to different components.",
            "And when you when you start something, you remove the single data point from its current component.",
            "And if that component has become empty, so if that convert that data point that you removed from the component was the only one that was assigned to it, you delete that component from the representation because nobody owns it anymore.",
            "And then you look at the conditional probabilities of the data points going to any one of the existing components or another new component, and you assign the data to one of the components.",
            "Get this conditional probabilities.",
            "Basically you sample from a multinomial distribution with those probabilities."
        ],
        [
            "So what are those probabilities?",
            "Basically, they have components coming from the prior and likelihood terms, so the probability of assigning to components for which there are more than zero number of things assigned to.",
            "So going to one of the non empty components basically is proportional to the number of things that belong to that component except you except the data point that you're sampling for now.",
            "So this is what this notation means, so you take out the ice data point.",
            "From the component that it belongs to, so it may have belonged to K or not.",
            "We don't know that, but if it is belonging to K, then we decrement and buy one.",
            "If not, we don't touch N, so this term is coming from the prior and this is basically the probability of the data under that component.",
            "So how likely is it to observe that data given the component parameter and the probability of assigning to a new component is basically proportional to Alpha times this?",
            "So this is basically the probability of this, This data point under this base distribution.",
            "So you integrate out all data, all parameters using the base distribution.",
            "OK, so this is kind of coming from the likelihood term and this is coming from the prior term.",
            "So we can.",
            "We can do this for the components that have data associated with them, because they already have some parameters that are assigned to them and we need to do this, we need to integrate out these parameters because there are infinitely many of them and we cannot.",
            "Basically, physically we cannot consider being assigned to every one of those infinitely many cases, right?",
            "So this integral basically says I'm taking into account.",
            "Being assigned to any one of the possible infinitely many cases.",
            "Of course, this proportional to how much mass the base distribution gives to those those parameter values."
        ],
        [
            "So we can also do it also differently, so right?"
        ],
        [
            "Then knowing the parameter values and conditioning on the parameter values to get predictive value for that particular point under that that parameter value."
        ],
        [
            "Can integrate this out given the.",
            "Posterior distribution basically of that component.",
            "So by this one here I mean.",
            "This is the probability of the data given the parameter and basically this parameter is going to have a distribution that is defined by the things that are the data points that are assigned to.",
            "That's component, so G minus IK Theta.",
            "Here means it is somewhat like the base distribution, but it is like the posterior base distribution.",
            "Which is conditioned on the data observed for that particular component.",
            "OK, is that clear.",
            "Any questions about this?",
            "OK."
        ],
        [
            "Right, so details about this code over everything I'm talking about is implemented in DPM GIFs dot M. So we are given.",
            "This role cell vector of data items and there are many of them.",
            "Many data items.",
            "We set the Alpha parameter.",
            "We decide on which hyperparameter to use, which value of hyperparameter.",
            "And we keep track of this.",
            "We keep track of the number of active clusters.",
            "The mixture components basically that are summarized by the sufficient statistics of the assigned data.",
            "Because this code."
        ],
        [
            "It makes use of this form of sampling rather than this form of sampling, so it doesn't need to keep around the parameter values."
        ],
        [
            "And then.",
            "The role vector of cluster indicator variables and the role vector of number of data items per cluster.",
            "OK, so."
        ],
        [
            "In detail, this is basically what the program looks like.",
            "We're initializing the parameters or the data.",
            "Like"
        ],
        [
            "This and then for at each iteration we remove each data item from the model and edit back edit back in according to the conditional probabilities.",
            "So this is where we remove the data item from the component that is is belonging to.",
            "Basically we retrieve.",
            "It's assignment so it is assigned to the I. OK so K is is current assignment.",
            "We subtract 1 from an K because we no longer have that component there, we remove it from there and we also remove it from the sufficient statistics of this component, which is given by QK OK.",
            "So in doing this, we're kind of pretending like the component that we observe is the is the.",
            "In a secret, if you think of this sequential sampling scheme, you have data points coming in, and you keep sampling from the distal posterior conditioned on all the previous samples, right?",
            "So in a way, when we were doing this Gibbs sampling, we're pretending like the data point that we observe is observed for the first time, and it is the last data point that is observed.",
            "So we observed and minus one data points before, and now we are observing one additional data point.",
            "For the first time, that's why we pretend like it wasn't there, so we remove it from from both the number of components, things that are assigned to that component and we remove it from the sufficient statistics of that component.",
            "OK, so once we do that, the model doesn't know about that data anymore, so it can pretend like it just arrived as the Earth data point."
        ],
        [
            "And as I said, we after removing the data point, we compute the conditional probabilities of the data item.",
            "I belong to the probability of belonging to each component K. And this is some implementation detail.",
            "We compute the probabilities in the log domain and then take the exponential just to avoid numerical issues.",
            "OK, so because some of these values can become very small and we don't want to always have zero probabilities.",
            "OK, so there is a small piece of code look predictive that actually calculates the predictive likelihood of this data under this component.",
            "So also in the package.",
            "So you can.",
            "You can have a look.",
            "It basically evaluates what's the probability of this data given whoever else is assigned to that component.",
            "OK, as I said this is done for numerical stability and this is basically normalizing the piece so that you can later on sample from it from using these as the multinomial distribution."
        ],
        [
            "OK, so we sample the component by sampling from the conditional probabilities and if a new component is added, which means if the data points chose to go to a component that nobody else was sitting at, then we need to instantiate the new active component by setting setting the number of things that it has and setting the sufficient statistics."
        ],
        [
            "And so on.",
            "And then after instantiating, we add the data item back into the model by adding one, basically incrementing the number of data items in the component K, the component that he chose to go to and add the item to the sufficient statistics of that component that it decided to go to.",
            "OK, so."
        ],
        [
            "There is this part of the demo which you won't be able to do now because you don't have the data, probably because you cannot download, but I encourage you to.",
            "Get this later after you leave the island, just not too.",
            "How is it?",
            "Yeah, but not everybody has it.",
            "OK, so there was a USB stick going around which has that data.",
            "If you get ahold of that USB stick then you can copy that data and already start.",
            "Playing around with it.",
            "But I just want to give you an overview of what data is about and what the model assumptions there are.",
            "There is already code implemented that does the modeling of this data as well as the modeling of all the other, like the demos and so on that I showed you, but it will be useful if you try to do your own sampler so that you get a better feeling of what this.",
            "Sampling is doing actually, so this.",
            "This data represents each.",
            "NIPS paper is a bag of words representation, which means the feature vectors are of the size of the vocabulary V. OK, so there we words in our vocabulary and paper.",
            "I is represented by this vector that's of dimension V and its entries are basically set to the number of words that occur.",
            "Number of times a particular word occurs in that document.",
            "OK.",
            "This data basically has a small subset of the papers and a small subset of the informative words to keep the data size and dimension small."
        ],
        [
            "So we can construct adresa process mixture model for this data.",
            "Since now we have a discrete distribution, we have words right and we have the number of words, so this is a discrete data, so we used multinomial mixture for to model the data.",
            "And of course the infinite mixture model.",
            "So as I said earlier, the distribution is going to get to the multinomial, so we're going to use symmetric distribution as the base measure.",
            "Which looks like this.",
            "So this is the model multinomial and this is the.",
            "There's the distribution physically with parameter B, which is off the size of the vocabulary."
        ],
        [
            "So this is what the model looks like.",
            "We have data generated from the multinomial.",
            "Sorry.",
            "And the parameters of the multinomial are sampled from G, which is a sample from the judicial process itself.",
            "With this space distribution, which is traditional symmetric to each day, which will be over V, this is also a capital V. So this basically is symmetric to each day over the over the words in the vocabulary so.",
            "Alpha controls the expected number of clusters.",
            "Here Alpha is the concentration parameter and it controls the expected number of clusters and be basically this year the perimeter of jeannerod controls the distribution of words among clusters.",
            "So remember the probability simplex I was showing you depending on what the scale of the parameters were.",
            "It changed the distribution slightly, right?",
            "So if you had a larger scale than things tended to concentrate on the on the center of the triangle, if you had less, then things were more diffused, right?",
            "So this is basically what controls the distribution of words among the clusters."
        ],
        [
            "And yeah, try it out when you can.",
            "Just a few more slides.",
            "And then I guess we have some time, right?"
        ],
        [
            "OK, so.",
            "What I didn't talk about and what the code doesn't handle.",
            "The code that I've showed you doesn't handle is basically learning hyperparameters.",
            "So hyperparameters in this case is Alpha, for example, or the B value for multinomial that we assumed for the NIPS papers so.",
            "The value of Alpha is very important for the type of model that we have, right?",
            "It tells you the expected the prior expected number of components.",
            "So your result may change really a lot by assuming a small or a large value of Alpha.",
            "So although in Bayesian nonparametrics we say we don't have the model selection problem because we do everything automatically, the number of things are set automatically by the model itself.",
            "There is still wanting to decide on right.",
            "There is still a parameter that is Alpha, which we call the hyperparameter.",
            "That's kind of tells you that this is also a choice of how many, how many components you want to appear in the model, so it's not as rigid as the finite case like finite mixture case where you have to set K and you're done with it, but you still need to think well about what you want to do with Alpha and one way to deal with this is again being basing about things and putting a prior on Alpha.",
            "So let Alpha also vary with the with the data right?",
            "So here I'm showing the number of components versus the number of data points.",
            "So this is the size of these squares, tells you the probability of that combination.",
            "So if you have these many data points, if you have 20 data points, then having one component has this much probability.",
            "Mass has two components having this much probability mass and so on.",
            "So if you assume gamma.",
            "A prior on Alpha, then this is kind of what you get.",
            "OK, so there is a heavy bias on towards the smaller number of components, but there is still non zero probability of having larger number of components.",
            "So if your data supported you can end up with a large number of components.",
            "But there is something that is funny about this, so the probability diminishes really fast, right?",
            "So if you want to be."
        ],
        [
            "A bit more secure about this, so you want a more more uniform ish.",
            "Distribution, then you can use an inverse gamma with again point 5.5 values, parameter values and basically this is the same plot the number of components K versus the number of data points and the size of these rectangles show you what the probability of each of those congregations is under this prior over Alpha.",
            "OK, so compare this to."
        ],
        [
            "So this so you see that."
        ],
        [
            "There is a.",
            "There is a more uniform distribution among these possible K values here, so again, there is no best way of choosing priors.",
            "But these are two possible things that I tried, and depending on your model needs depending on your intuition about what you want to use for your data, you can choose to use this or this."
        ],
        [
            "Or maybe something else.",
            "OK, so if you if you believe that your data probably comes from a really small number of components, but you still want to give some probability to larger number of components, maybe you're better off using just a gamma whereas."
        ],
        [
            "If you're not sure and you want to be more flexible about the number of components growing larger, then you may want to use inverse gamma.",
            "And."
        ],
        [
            "One less thing, which again I didn't talk about the implementation of, but which is about conjugate unconditionally conjugal or conjugate, an non conjugate models basically.",
            "So here, by conditionally conjugate I mean rather than having a parametric form that we can integrate out all the parameters, we have a parametric form where we can integrate out only some of the parameters.",
            "But we have to keep around the rest of the parameters.",
            "OK, so if we.",
            "Try to model this data using.",
            "This process is using a conjugate model that I showed you how to use in the previous slides.",
            "We get density distribution that looks like this.",
            "There are three peaks that are kind of smooth.",
            "Where is menu user conditionally conjugate model A model where you cannot integrate out all of the parameters.",
            "You can only integrate out some of the parameters and your condition on the rest of the parameters you get a distribution that looks like this, so it has, so it looks less Gaussian and you can see that it has these.",
            "It has more probability mass and these points, and that's probably because of.",
            "These components being like outlying and pulling this source here and as a comparison comparison, this is a kernel density estimation or the parzen window fit to the density.",
            "You can see that.",
            "This is more grainy simply because what this model does is it assumes a Gaussian bump on each of these data points, so it basically it is a nonparametric model, but not in those nonparametric Bayesian sense.",
            "So it's a non parametric nonparametric model in the sense that it's.",
            "It's complexity also increases with the number of data points because it assumes the density is.",
            "This density is basically some of these Gaussian bumps that are in many of them.",
            "If you have any data points so you can see that this is a little too of course, But then this is maybe a little too smooth, so maybe this is a better representation of your data, but as I said, it may be a lot harder to do inference on conditionally conjugate or non conjugate models as opposed to conjugate models both in terms of runtime implementation and getting a sampler too actually.",
            "The correct sampling from."
        ],
        [
            "These are the statistics for."
        ],
        [
            "This data the this data set."
        ],
        [
            "That which is here.",
            "Basically comparing the conditional.",
            "Sorry the conjugate model and the non conjugate model.",
            "You can see clear and these are different datasets, so you can see why the difference comes about right.",
            "So the conjugate model tends to use a lot less components, is a lot more conservative in the number of components it uses as opposed to the non conjugate model.",
            "Everything about these models were the same all the parameter settings hyperparameter settings.",
            "ETC except for the form of the prior distributions form of the.",
            "Base distribution basically so you can see that there's a big difference between assuming critical versus non conjugate."
        ],
        [
            "And this is basically showing the number of data points assigned to each of these components, so there's a clear difference between that too.",
            "OK, so that's it.",
            "I hope you get a better understanding of the rich.",
            "The process now so.",
            "I think the takehome messages.",
            "I think some of you don't have any way to take home messages.",
            "They're not hard to use their they're very easy to implement for at least some cases, and they do give really good fits to data.",
            "But then you have to be careful about what your prior assumptions are.",
            "You cannot just go blindly and take a reset process out of the drawer and apply it to our data and think that you're going to get the universal solution, so you still need to think hard about the prior assumptions of your model.",
            "Although this is not.",
            "Metric is not going to solve all your problems.",
            "I get."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Peter did a very nice introduction to Bayesian nonparametric Santa driven processes and this is going to be.",
                    "label": 0
                },
                {
                    "sent": "Semi practical course.",
                    "label": 0
                },
                {
                    "sent": "About the additional processes, so I'm going to be using.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The code from you I tell, which hopefully all of you by now has.",
                    "label": 1
                },
                {
                    "sent": "And what you need to do initially is basically change to the code directory directory and run init path to initialize the path.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Throughout the lecture I'm going to show you different demo so you know what part of that toolbox is used for what.",
                    "label": 0
                },
                {
                    "sent": "So as Peter was talking about, there's a process is a tool for defining Bayesian nonparametric models.",
                    "label": 1
                },
                {
                    "sent": "And there are other well known processes in machine learning for defining, again, based in nonparametric models like Gaussian processes, which will be talking about tomorrow ending before process which Peter talked about and Kim's coalescent which I talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "So but the focus now is under the process is so there's a process is basically.",
                    "label": 0
                },
                {
                    "sent": "Distribution over distributions which you can use to represent your uncertainty about the parametric form of the distribution that you want to use in your model.",
                    "label": 0
                },
                {
                    "sent": "So the typical thing to do would be in a Bayesian parametric setting would be to have a bunch of parameters for your model and then represent the uncertainty about the value of those parameters by in terms of some parametric distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "But you don't know the value of the parameter, so you would put a Gaussian distribution on top of it with some mean and some covariance and you will try to get the posterior of that Gaussian when you do inference, whereas with additional processes your instead of putting that Gaussian, you're putting the process with a certain base distribution on the pyramid on the distribution itself, so that you're not only learn the perimeter, but also learn the.",
                    "label": 0
                },
                {
                    "sent": "Kind of the parametric form, or like the roughly the form of the distribution itself as well.",
                    "label": 0
                },
                {
                    "sent": "So this is something a little harder to understand.",
                    "label": 0
                },
                {
                    "sent": "You need to get used to the idea of using Bayesian non parametrics to understand it.",
                    "label": 0
                },
                {
                    "sent": "This is this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a very simplistic way of thinking about the judicial process.",
                    "label": 0
                },
                {
                    "sent": "Basically as a glorified dishnet distribution, glorified because it's over a countably many infinitely many dimensions, so we.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause the dish, the distribution and addition process is very closely related.",
                    "label": 0
                },
                {
                    "sent": "I wanted to go over the basic properties of the additional distribution.",
                    "label": 0
                },
                {
                    "sent": "I don't know why this keeps changing.",
                    "label": 0
                },
                {
                    "sent": "No, it's a spoiler, so the K dimensional do, so there's the distribution we write like this is the digital distribution with parameters A1 to Alpha K. Over this vector that is Kaden dimensional Pi 1 to \u03c0 K. So we denote the distribution like this and the distribution is defined like this.",
                    "label": 0
                },
                {
                    "sent": "So the indices of this vector or the entries of this vector are non negative and they sum to one.",
                    "label": 0
                },
                {
                    "sent": "So this I used to denote the probability simplex, 2K dimensional probability simplex.",
                    "label": 0
                },
                {
                    "sent": "So the scale of the dish, the parameters these alphas determines the concentration of the probability mass.",
                    "label": 1
                },
                {
                    "sent": "On the probability simplex.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "So about that.",
                    "label": 0
                },
                {
                    "sent": "So if you have additional distribution with parameters 111.",
                    "label": 0
                },
                {
                    "sent": "Then you have this kind of a distribution over the simplex, so it's a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So these are these are the parameters, and if so we have K parameters for the K dimensional.",
                    "label": 0
                },
                {
                    "sent": "There's a distribution, and if all these parameters are equal, we call this symmetric dishware distribution.",
                    "label": 0
                },
                {
                    "sent": "You have probably heard of this when talking about the reset process.",
                    "label": 0
                },
                {
                    "sent": "So of course it doesn't always have to be equal.",
                    "label": 0
                },
                {
                    "sent": "Depending on your modeling needs, you may set these parameters differently, but just for now, let's look at what happens when you play around with the scale of these parameters.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are values where maybe I can just close it.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are.",
                    "label": 0
                },
                {
                    "sent": "The density distribution densities with parameters 222555, TEN, 1010.",
                    "label": 0
                },
                {
                    "sent": "So you see that as you increase the scale of the parameters, the distribution gets more and more peaked.",
                    "label": 0
                },
                {
                    "sent": "Not that these are all greater than one, so it's interesting to see what happens when things are.",
                    "label": 0
                },
                {
                    "sent": "I will pause it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are greater than smaller than one, so when the parameter is smaller than one, then rather than focus on getting concentrated on the center of this triangle, the probability mass starts going towards the edges and the more you decrease the values away from one, the more concentrated around the edges they become.",
                    "label": 0
                },
                {
                    "sent": "So this is an interesting property for the.",
                    "label": 0
                },
                {
                    "sent": "Through the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is basically.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically what it looks like when you have different values for when you don't.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those magic 3 shows, but the slideshow doesn't really want you to look at it very long.",
                    "label": 0
                },
                {
                    "sent": "You can play around with the distributions by using the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There density that M you don't need to do that now, but this is basically going to show you what happens when you input different parameter values and so on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the additional distribution again has an interesting property where if you combine entries of additional distributed probable two vector, you will still end up with distributed vectors but with less number of dimensions this time.",
                    "label": 1
                },
                {
                    "sent": "So if we have a K dimensional vector pipe that is rich distributed with parameters A1 to Alpha K, and if we simply sum up.",
                    "label": 1
                },
                {
                    "sent": "The first 2 entries of this vector and similarly sum up the first 2 parameters of this distribution.",
                    "label": 0
                },
                {
                    "sent": "This is what we get.",
                    "label": 0
                },
                {
                    "sent": "So the vector with one less dimensions is still distributed with this shit.",
                    "label": 0
                },
                {
                    "sent": "This time all the parameters the same except for this first parameter being the sum of the first 2 parameters there.",
                    "label": 0
                },
                {
                    "sent": "And this is not specific to.",
                    "label": 0
                },
                {
                    "sent": "Of course, the first dimension at first dimensions only.",
                    "label": 1
                },
                {
                    "sent": "You can do it for any dimensions and the converse of this is also true.",
                    "label": 0
                },
                {
                    "sent": "So you can have parameters vector of K dimension.",
                    "label": 0
                },
                {
                    "sent": "Difference being there said distributed with A1 to Alpha K If you sample.",
                    "label": 0
                },
                {
                    "sent": "These T1T2 values from a 2 dimensional drishtee which is also called the better function.",
                    "label": 0
                },
                {
                    "sent": "A bit better distribution and then you multiply the corresponding entries with the corresponding values like this by 1 * 10 to one and \u03c0 one time startup similar to A1 times better one and A1 times better 2.",
                    "label": 0
                },
                {
                    "sent": "I think there's a typo there.",
                    "label": 0
                },
                {
                    "sent": "Making sure that this better one, plus with better two is equal to 1, then you get additional distribution so.",
                    "label": 0
                },
                {
                    "sent": "You can run DP generate.",
                    "label": 0
                },
                {
                    "sent": "To see that we can actually draw from additional process by using this property.",
                    "label": 0
                },
                {
                    "sent": "So we start with a uniform distribution over zero 10.",
                    "label": 0
                },
                {
                    "sent": "Therefore this is at 0.1 and we keep refining by drawing from this better distribution.",
                    "label": 0
                },
                {
                    "sent": "We keep refining this space.",
                    "label": 0
                },
                {
                    "sent": "Like this for example.",
                    "label": 0
                },
                {
                    "sent": "So when we first draw from the beta distribution, it divides it into 2.",
                    "label": 1
                },
                {
                    "sent": "Two parts and the height of that is proportional to the values that we draw, and then we'll keep recursing on each of these helps like this like this.",
                    "label": 0
                },
                {
                    "sent": "And if we do it long enough.",
                    "label": 1
                },
                {
                    "sent": "We're going to have something like this, so you actually you're going to have infinitesimals.",
                    "label": 0
                },
                {
                    "sent": "Infinitesimal intervals, some of which will have huge spikes and some of most of which will have tiny spikes.",
                    "label": 0
                },
                {
                    "sent": "So this is a very easy and fun way of obtaining draw from the process, actually.",
                    "label": 0
                },
                {
                    "sent": "It shows you that you can get it as the infinite limit of the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are different ways where you can define the process.",
                    "label": 0
                },
                {
                    "sent": "You can construct samples from the process or you can sample from the process and one way to do it is by using the stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "So if you go to the code the SB generates dot M code is going to generate sticks for you, so these are going to be sticks that are that are representative of the.",
                    "label": 0
                },
                {
                    "sent": "Of these weights for the dish lid process.",
                    "label": 0
                },
                {
                    "sent": "Parameter values, so the sticks are going to look like this because they are defined using this recursive formula where new K is drawn from the beta 1A distribution in expectation they're going to be exponentially decreasing, so this red parts are the stick length of the process, so these are going to be exponentially decreasing in expectation.",
                    "label": 0
                },
                {
                    "sent": "Therefore you're going to get to infinitesimally small values.",
                    "label": 0
                },
                {
                    "sent": "Not very long from starting sampling, so.",
                    "label": 0
                },
                {
                    "sent": "Here I.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generated some sticks, so with different Alpha parameters this is a good way of seeing the effect of the Alpha parameter, so the Alpha parameter of the digital process controls.",
                    "label": 0
                },
                {
                    "sent": "The the expected number of clusters in the process mixture models, for example.",
                    "label": 0
                },
                {
                    "sent": "You can say you can say it also.",
                    "label": 0
                },
                {
                    "sent": "The reason why it does that is it controls the distribution of these \u03c0 case.",
                    "label": 0
                },
                {
                    "sent": "So if Alpha is small then the Pi case will decrease much faster and only a small number of them are going to have significant values.",
                    "label": 0
                },
                {
                    "sent": "Whereas if Alpha is larger than they're going to more and more sticks will have significant mass compared to others.",
                    "label": 0
                },
                {
                    "sent": "But there will be still infinitely many of them with infinitesimal mass.",
                    "label": 0
                },
                {
                    "sent": "So these are examples from Alpha equals 1A equals 5A equals 25, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in the previous slide, I only showed you the length of the sticks in the order that I sampled them.",
                    "label": 0
                },
                {
                    "sent": "So we started with this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember, so we started with a stick of feeling unit length and then we broke the stick.",
                    "label": 0
                },
                {
                    "sent": "Here we kept this as the as the stick.",
                    "label": 0
                },
                {
                    "sent": "The first stick length and then we recurse on this remaining part, broke it off here.",
                    "label": 0
                },
                {
                    "sent": "This was the second statement and so on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And basically I show here the sticks in the order that I draw them.",
                    "label": 0
                },
                {
                    "sent": "These being the stick identity.",
                    "label": 0
                },
                {
                    "sent": "So the first sticks I can state Congress and so on here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm showing it in a different way.",
                    "label": 0
                },
                {
                    "sent": "I also sampled theaters from the base distribution G Note and now this is the value of Theta and these are basically the length of the sticks the same length as before.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to show you that.",
                    "label": 0
                },
                {
                    "sent": "The Alpha parameter makes it makes the distribution concentrate more and more on the base distribution.",
                    "label": 0
                },
                {
                    "sent": "When it is larger, so that's why it's called the concentration parameter it to me it's first sounded counter intuitive that a parameter that has a value is small, and if it has a value that small, it leads to less number of components.",
                    "label": 0
                },
                {
                    "sent": "So it sounds like it is more concentrated.",
                    "label": 0
                },
                {
                    "sent": "The model is more concentrated, and when you increase the value of Alpha it gets more and more components.",
                    "label": 0
                },
                {
                    "sent": "That sounds like is less concentrated model.",
                    "label": 0
                },
                {
                    "sent": "But the concentration here is about concentrating on the base distribution rather than concentrating on like a few number of components.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said you can.",
                    "label": 0
                },
                {
                    "sent": "You can run this code using the SP generate code.",
                    "label": 0
                },
                {
                    "sent": "By the way, if there are any questions about.",
                    "label": 0
                },
                {
                    "sent": "About the way to run the code or what is in the code.",
                    "label": 0
                },
                {
                    "sent": "This is supposed to be a practical section, so just let me know.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another way to generate from the three step process is basically using the polius aren't scheme, which is different from the stick breaking construction.",
                    "label": 1
                },
                {
                    "sent": "So here we don't have any Pi values, so the Pi values don't going to play here.",
                    "label": 1
                },
                {
                    "sent": "We're directly sampling the data values and the way we do that is we start with data one sampled from the note and every consecutive sample is sampled by.",
                    "label": 0
                },
                {
                    "sent": "Conditioning on that title one.",
                    "label": 1
                },
                {
                    "sent": "Andy Andy, not the not.",
                    "label": 0
                },
                {
                    "sent": "The collection of basically did not.",
                    "label": 1
                },
                {
                    "sent": "The posterior of the digital process conditioned on the 1st and samples.",
                    "label": 0
                },
                {
                    "sent": "So this is the form GN is going to take and GI is going to be sampled from GI minus one basically, so we iteratively sample.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing about this sampling scheme is that GN converges almost surely as N goes to Infinity to random discrete distribution G. And she has additional process Alpha genomic distribution.",
                    "label": 1
                },
                {
                    "sent": "Which means the samples the collection of samples that you get, the sequence of Theta I I from one to N is the sample from G. OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example of doing that.",
                    "label": 0
                },
                {
                    "sent": "So we start with a base distribution like this, a Gaussian based distribution that's G, not.",
                    "label": 0
                },
                {
                    "sent": "We sample from this and we sample this location.",
                    "label": 0
                },
                {
                    "sent": "So it's going to have some weights.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Its weight is going to be one over Alpha plus N and the weight of the Geno distribution is going to be Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this and then G2 is going to be including this conditioning on this.",
                    "label": 0
                },
                {
                    "sent": "We sample another point and that gives us G2 and we keep doing this to get.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More and more samples.",
                    "label": 0
                },
                {
                    "sent": "So as we increase the number of samples, this is what we get for G. 10,000.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a sample from the dish.",
                    "label": 0
                },
                {
                    "sent": "This is obtained using the polio urn sampling scheme.",
                    "label": 0
                },
                {
                    "sent": "OK, so again about.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The concentration parameter.",
                    "label": 0
                },
                {
                    "sent": "So if we have Alpha equals one, this is what sample from the DP looks like.",
                    "label": 1
                },
                {
                    "sent": "If we have Alpha equals 100, this is what our sample from the DP looks like.",
                    "label": 0
                },
                {
                    "sent": "So you can see again that for the case where Alpha is small, the concentration parameter is small.",
                    "label": 0
                },
                {
                    "sent": "The way the discrete distribution looks like is nothing like the base distribution, whereas the discrete distribution looks more like kind of a discrete discretized.",
                    "label": 0
                },
                {
                    "sent": "Histogram of the of the base distribution just re emphasize that.",
                    "label": 1
                },
                {
                    "sent": "That's why Alpha is called the concentration parameter.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's a fun interactive.",
                    "label": 0
                },
                {
                    "sent": "Demo that you can also run.",
                    "label": 0
                },
                {
                    "sent": "This is basically.",
                    "label": 0
                },
                {
                    "sent": "Right now it's only the samples from the base distribution and an average of all the samples we have so far, with weighted more heavily towards the recent samples.",
                    "label": 0
                },
                {
                    "sent": "So the blue ones are single samples and the black one is an average of all the samples.",
                    "label": 1
                },
                {
                    "sent": "And this is the additional process with conjugates Gaussian based distribution.",
                    "label": 1
                },
                {
                    "sent": "So when we click on the plot, that means we add one data point OK at the Red Cross.",
                    "label": 0
                },
                {
                    "sent": "Here at somewhere close to 10 I added one data point and you can see that the process is now focusing more on this point, so it's kind of peaking towards that, although there is still some probability of having other peaks away from that point.",
                    "label": 0
                },
                {
                    "sent": "But if I just.",
                    "label": 1
                },
                {
                    "sent": "Keep adding data points.",
                    "label": 0
                },
                {
                    "sent": "There you see that more and more evidence is gathered, so the model is more and more sure about where the peak should be right.",
                    "label": 0
                },
                {
                    "sent": "If I now introduce other data, let's say some outlier data which is away from this peak, it still has some information, right?",
                    "label": 0
                },
                {
                    "sent": "It gives the information that there is not only one peak there, but there may be something going on here as well, so the samples are also getting peeked around that, but not as heavily as that, because basically is overwhelming.",
                    "label": 1
                },
                {
                    "sent": "But if I add more and more data points now we see that.",
                    "label": 0
                },
                {
                    "sent": "Is focusing on a bimodal distribution right?",
                    "label": 0
                },
                {
                    "sent": "So you can have fun with this code.",
                    "label": 0
                },
                {
                    "sent": "Just clicking around to introduce more and more peaks if you want or to join the peaks that you created like this to again go back to one peak or two peaks and so on so.",
                    "label": 0
                },
                {
                    "sent": "It basically shows you that the reset process is.",
                    "label": 0
                },
                {
                    "sent": "Flexible tool that can flexibly adjust to new data so you don't have to pre specify the number of components.",
                    "label": 0
                },
                {
                    "sent": "It's going to find the number of components that is necessary to fit the data and as you add more and more data is going to be able to flexibly change to fit the new data.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So let's compare the finite mixture models with this.",
                    "label": 0
                },
                {
                    "sent": "So I'm showing here again A1 dimensional density fit for the finite mixture model where.",
                    "label": 0
                },
                {
                    "sent": "By by the red curve I showed the mean by the dashed blue tick curve I showed the median and then this is the 95% quantile.",
                    "label": 0
                },
                {
                    "sent": "So you and all the rest is basically samples from the prior.",
                    "label": 0
                },
                {
                    "sent": "So this is a prior distribution for a two component finite mixture model.",
                    "label": 1
                },
                {
                    "sent": "So if we are.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other data points that are shown by these black dots here.",
                    "label": 0
                },
                {
                    "sent": "And we fit this two component finite mixture model.",
                    "label": 1
                },
                {
                    "sent": "We see that we get these nice two peaks around where the data lies and uncertainty of where these the.",
                    "label": 0
                },
                {
                    "sent": "Probability mass should lie decreases, so compare the Gray area here to the Gray area here.",
                    "label": 0
                },
                {
                    "sent": "So since we have more data now we have less uncertain about what the model should look like, right?",
                    "label": 0
                },
                {
                    "sent": "So it's doing a quite good job so and we can say we can see clearly that the data.",
                    "label": 0
                },
                {
                    "sent": "Is clustered into clusters, so this is a reasonable estimate.",
                    "label": 0
                },
                {
                    "sent": "I actually generated the data from 2 Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But here I generated the data from 3 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So the true model for this data would be 3 components mixture model, whereas I used again two component mixture model to fit the data.",
                    "label": 1
                },
                {
                    "sent": "Now you can see that there is this great Gray area, which is a lot.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Larger than here, so it's a good.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing about the Bayesian framework that when there is uncertainty about the about the model this learned, then it still expresses that uncertainty, so you don't need to discard Bayesian models altogether.",
                    "label": 0
                },
                {
                    "sent": "An migrate altogether too based in Nonparametrics but basically all it's telling you is it finds bimodal distribution because that's all it can do, but there is high, so high uncertainty.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's self guides those self guards you by saying well I'm very uncertain about what's going on here, but it still gives you the model with two modes only.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is now the samples from the prior for the additional process case.",
                    "label": 0
                },
                {
                    "sent": "So Adresa process with conjugates based distribution which is Gaussian distributed.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the fit to data that's generated from 2 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "As expected, you have these two peaks.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is data generated from 3 Gaussians and now you have three peaks, so this shouldn't be surprising because we know that the addition process is going to fit to the data and it's going to use the necessary amount of components to be able to have a good representation of data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You can see things like this.",
                    "label": 0
                },
                {
                    "sent": "For example, there are peaks if you sample from the posterior, there are peaks at places where there is not any data at all.",
                    "label": 0
                },
                {
                    "sent": "So that comes because the process still gives some probability to other places.",
                    "label": 0
                },
                {
                    "sent": "Then it's so data.",
                    "label": 0
                },
                {
                    "sent": "So you may see this as an advantage or disadvantage if you believe that your data is only what you saw and there will be no other.",
                    "label": 0
                },
                {
                    "sent": "Data lying outside of the region that you observed data before then maybe this is something to worry about.",
                    "label": 0
                },
                {
                    "sent": "An maybe actually this is a model that you shouldn't use then because you don't want all that flexibility you want to constrain your model.",
                    "label": 0
                },
                {
                    "sent": "But if you want to be flexible and give some probability to data coming from other regions, then this may not be such a bad idea to use this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The infinite mixture model using the additional processes is defined by this, so we have a.",
                    "label": 0
                },
                {
                    "sent": "We have a data points or we have N data points indexed by.",
                    "label": 0
                },
                {
                    "sent": "I hear conditioned on some parameter and that parameter defines the distribution of the data.",
                    "label": 0
                },
                {
                    "sent": "And that parameter is drawn from some distribution G which is drawn from the Trisha process.",
                    "label": 0
                },
                {
                    "sent": "With these parameters Alpha the concentration parameter, Angie, not the base distribution, and this is the graphical model showing this.",
                    "label": 0
                },
                {
                    "sent": "These equations basically so this is, this is how we define the infinite mixture model.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equivalently, we can show it like this, where we have pies and status rather than gezond.",
                    "label": 0
                },
                {
                    "sent": "Yeah, rather than GS we have pies and status where we basically represent G using this infinite sum that comes from the stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Given this model, how do we do inference on it?",
                    "label": 0
                },
                {
                    "sent": "So typically the easiest case to use these models in practice is to use a conjugate base distribution so that the parameters can be integrated out and the predictive probabilities can be computed using the sufficient statistics of the data.",
                    "label": 1
                },
                {
                    "sent": "So Peter was talking about two kinds of conjugacy.",
                    "label": 0
                },
                {
                    "sent": "One is the quantity of the dresser process itself, so the posterior process is still there is a process that's one type of conjugacy.",
                    "label": 0
                },
                {
                    "sent": "And that's always present in the process models, but at least the.",
                    "label": 0
                },
                {
                    "sent": "Vanilla there's a process models, let's say.",
                    "label": 0
                },
                {
                    "sent": "And the other type of conjugacy is about the conjugacy of the parameters of the model to the base distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you want to model an infinite mixture model with Gaussian components, you need to specify a prior on top of those components, and that prior needs to be conjugate to the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Because you assume Gaussian components for for the data generating process.",
                    "label": 0
                },
                {
                    "sent": "So what is the?",
                    "label": 0
                },
                {
                    "sent": "What is the conjugate distribution for the Gaussian distribution?",
                    "label": 0
                },
                {
                    "sent": "So if you're using both, if you.",
                    "label": 0
                },
                {
                    "sent": "If you assume that both mean and covariance is uncertain, that they're not known, then normal inverse Wishart distribution is the conjugate to the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Or if you're fixing the fixing the mean and assuming only you want to learn the covariance, then you need the inverse Wishart distribution on the covariance.",
                    "label": 0
                },
                {
                    "sent": "Or if you know what the what the covariances or you're pretty sure you can set it finally and you only want to put a distribution over the means, then the normal distribution is the conjugate distribution for the normal means OK, and Similarly if you have discrete data, if you're using multinomial distribution for example to model your data, than the distribution is the conjugate distribution for the multinomial, so that you would use a.",
                    "label": 0
                },
                {
                    "sent": "A recent multinomial models for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Of course we are not tight to using conjugate base distributions, but things become a little more tricky when you when you want to use non conjugate models.",
                    "label": 0
                },
                {
                    "sent": "And that's why here I'm going to be talking only about how to do inference for the conjugate case.",
                    "label": 1
                },
                {
                    "sent": "So in Gibbs sampling what you do at each iteration is for each data point you remove the data point from its current component.",
                    "label": 0
                },
                {
                    "sent": "So you start off with some random initially.",
                    "label": 0
                },
                {
                    "sent": "Initializations of some initialization of a number of components with assignments to of data points to different components.",
                    "label": 0
                },
                {
                    "sent": "And when you when you start something, you remove the single data point from its current component.",
                    "label": 0
                },
                {
                    "sent": "And if that component has become empty, so if that convert that data point that you removed from the component was the only one that was assigned to it, you delete that component from the representation because nobody owns it anymore.",
                    "label": 0
                },
                {
                    "sent": "And then you look at the conditional probabilities of the data points going to any one of the existing components or another new component, and you assign the data to one of the components.",
                    "label": 0
                },
                {
                    "sent": "Get this conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "Basically you sample from a multinomial distribution with those probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are those probabilities?",
                    "label": 0
                },
                {
                    "sent": "Basically, they have components coming from the prior and likelihood terms, so the probability of assigning to components for which there are more than zero number of things assigned to.",
                    "label": 1
                },
                {
                    "sent": "So going to one of the non empty components basically is proportional to the number of things that belong to that component except you except the data point that you're sampling for now.",
                    "label": 0
                },
                {
                    "sent": "So this is what this notation means, so you take out the ice data point.",
                    "label": 0
                },
                {
                    "sent": "From the component that it belongs to, so it may have belonged to K or not.",
                    "label": 0
                },
                {
                    "sent": "We don't know that, but if it is belonging to K, then we decrement and buy one.",
                    "label": 0
                },
                {
                    "sent": "If not, we don't touch N, so this term is coming from the prior and this is basically the probability of the data under that component.",
                    "label": 0
                },
                {
                    "sent": "So how likely is it to observe that data given the component parameter and the probability of assigning to a new component is basically proportional to Alpha times this?",
                    "label": 1
                },
                {
                    "sent": "So this is basically the probability of this, This data point under this base distribution.",
                    "label": 0
                },
                {
                    "sent": "So you integrate out all data, all parameters using the base distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of coming from the likelihood term and this is coming from the prior term.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can do this for the components that have data associated with them, because they already have some parameters that are assigned to them and we need to do this, we need to integrate out these parameters because there are infinitely many of them and we cannot.",
                    "label": 0
                },
                {
                    "sent": "Basically, physically we cannot consider being assigned to every one of those infinitely many cases, right?",
                    "label": 0
                },
                {
                    "sent": "So this integral basically says I'm taking into account.",
                    "label": 0
                },
                {
                    "sent": "Being assigned to any one of the possible infinitely many cases.",
                    "label": 0
                },
                {
                    "sent": "Of course, this proportional to how much mass the base distribution gives to those those parameter values.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can also do it also differently, so right?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then knowing the parameter values and conditioning on the parameter values to get predictive value for that particular point under that that parameter value.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can integrate this out given the.",
                    "label": 0
                },
                {
                    "sent": "Posterior distribution basically of that component.",
                    "label": 0
                },
                {
                    "sent": "So by this one here I mean.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of the data given the parameter and basically this parameter is going to have a distribution that is defined by the things that are the data points that are assigned to.",
                    "label": 0
                },
                {
                    "sent": "That's component, so G minus IK Theta.",
                    "label": 0
                },
                {
                    "sent": "Here means it is somewhat like the base distribution, but it is like the posterior base distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is conditioned on the data observed for that particular component.",
                    "label": 0
                },
                {
                    "sent": "OK, is that clear.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so details about this code over everything I'm talking about is implemented in DPM GIFs dot M. So we are given.",
                    "label": 0
                },
                {
                    "sent": "This role cell vector of data items and there are many of them.",
                    "label": 1
                },
                {
                    "sent": "Many data items.",
                    "label": 1
                },
                {
                    "sent": "We set the Alpha parameter.",
                    "label": 0
                },
                {
                    "sent": "We decide on which hyperparameter to use, which value of hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "And we keep track of this.",
                    "label": 1
                },
                {
                    "sent": "We keep track of the number of active clusters.",
                    "label": 1
                },
                {
                    "sent": "The mixture components basically that are summarized by the sufficient statistics of the assigned data.",
                    "label": 0
                },
                {
                    "sent": "Because this code.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It makes use of this form of sampling rather than this form of sampling, so it doesn't need to keep around the parameter values.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The role vector of cluster indicator variables and the role vector of number of data items per cluster.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In detail, this is basically what the program looks like.",
                    "label": 0
                },
                {
                    "sent": "We're initializing the parameters or the data.",
                    "label": 0
                },
                {
                    "sent": "Like",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This and then for at each iteration we remove each data item from the model and edit back edit back in according to the conditional probabilities.",
                    "label": 1
                },
                {
                    "sent": "So this is where we remove the data item from the component that is is belonging to.",
                    "label": 0
                },
                {
                    "sent": "Basically we retrieve.",
                    "label": 0
                },
                {
                    "sent": "It's assignment so it is assigned to the I. OK so K is is current assignment.",
                    "label": 0
                },
                {
                    "sent": "We subtract 1 from an K because we no longer have that component there, we remove it from there and we also remove it from the sufficient statistics of this component, which is given by QK OK.",
                    "label": 0
                },
                {
                    "sent": "So in doing this, we're kind of pretending like the component that we observe is the is the.",
                    "label": 0
                },
                {
                    "sent": "In a secret, if you think of this sequential sampling scheme, you have data points coming in, and you keep sampling from the distal posterior conditioned on all the previous samples, right?",
                    "label": 0
                },
                {
                    "sent": "So in a way, when we were doing this Gibbs sampling, we're pretending like the data point that we observe is observed for the first time, and it is the last data point that is observed.",
                    "label": 0
                },
                {
                    "sent": "So we observed and minus one data points before, and now we are observing one additional data point.",
                    "label": 0
                },
                {
                    "sent": "For the first time, that's why we pretend like it wasn't there, so we remove it from from both the number of components, things that are assigned to that component and we remove it from the sufficient statistics of that component.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we do that, the model doesn't know about that data anymore, so it can pretend like it just arrived as the Earth data point.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I said, we after removing the data point, we compute the conditional probabilities of the data item.",
                    "label": 1
                },
                {
                    "sent": "I belong to the probability of belonging to each component K. And this is some implementation detail.",
                    "label": 1
                },
                {
                    "sent": "We compute the probabilities in the log domain and then take the exponential just to avoid numerical issues.",
                    "label": 0
                },
                {
                    "sent": "OK, so because some of these values can become very small and we don't want to always have zero probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a small piece of code look predictive that actually calculates the predictive likelihood of this data under this component.",
                    "label": 0
                },
                {
                    "sent": "So also in the package.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can have a look.",
                    "label": 0
                },
                {
                    "sent": "It basically evaluates what's the probability of this data given whoever else is assigned to that component.",
                    "label": 0
                },
                {
                    "sent": "OK, as I said this is done for numerical stability and this is basically normalizing the piece so that you can later on sample from it from using these as the multinomial distribution.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we sample the component by sampling from the conditional probabilities and if a new component is added, which means if the data points chose to go to a component that nobody else was sitting at, then we need to instantiate the new active component by setting setting the number of things that it has and setting the sufficient statistics.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And then after instantiating, we add the data item back into the model by adding one, basically incrementing the number of data items in the component K, the component that he chose to go to and add the item to the sufficient statistics of that component that it decided to go to.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is this part of the demo which you won't be able to do now because you don't have the data, probably because you cannot download, but I encourage you to.",
                    "label": 0
                },
                {
                    "sent": "Get this later after you leave the island, just not too.",
                    "label": 0
                },
                {
                    "sent": "How is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but not everybody has it.",
                    "label": 0
                },
                {
                    "sent": "OK, so there was a USB stick going around which has that data.",
                    "label": 0
                },
                {
                    "sent": "If you get ahold of that USB stick then you can copy that data and already start.",
                    "label": 0
                },
                {
                    "sent": "Playing around with it.",
                    "label": 0
                },
                {
                    "sent": "But I just want to give you an overview of what data is about and what the model assumptions there are.",
                    "label": 0
                },
                {
                    "sent": "There is already code implemented that does the modeling of this data as well as the modeling of all the other, like the demos and so on that I showed you, but it will be useful if you try to do your own sampler so that you get a better feeling of what this.",
                    "label": 0
                },
                {
                    "sent": "Sampling is doing actually, so this.",
                    "label": 0
                },
                {
                    "sent": "This data represents each.",
                    "label": 0
                },
                {
                    "sent": "NIPS paper is a bag of words representation, which means the feature vectors are of the size of the vocabulary V. OK, so there we words in our vocabulary and paper.",
                    "label": 1
                },
                {
                    "sent": "I is represented by this vector that's of dimension V and its entries are basically set to the number of words that occur.",
                    "label": 0
                },
                {
                    "sent": "Number of times a particular word occurs in that document.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This data basically has a small subset of the papers and a small subset of the informative words to keep the data size and dimension small.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can construct adresa process mixture model for this data.",
                    "label": 1
                },
                {
                    "sent": "Since now we have a discrete distribution, we have words right and we have the number of words, so this is a discrete data, so we used multinomial mixture for to model the data.",
                    "label": 0
                },
                {
                    "sent": "And of course the infinite mixture model.",
                    "label": 0
                },
                {
                    "sent": "So as I said earlier, the distribution is going to get to the multinomial, so we're going to use symmetric distribution as the base measure.",
                    "label": 1
                },
                {
                    "sent": "Which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So this is the model multinomial and this is the.",
                    "label": 0
                },
                {
                    "sent": "There's the distribution physically with parameter B, which is off the size of the vocabulary.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what the model looks like.",
                    "label": 0
                },
                {
                    "sent": "We have data generated from the multinomial.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And the parameters of the multinomial are sampled from G, which is a sample from the judicial process itself.",
                    "label": 0
                },
                {
                    "sent": "With this space distribution, which is traditional symmetric to each day, which will be over V, this is also a capital V. So this basically is symmetric to each day over the over the words in the vocabulary so.",
                    "label": 0
                },
                {
                    "sent": "Alpha controls the expected number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Here Alpha is the concentration parameter and it controls the expected number of clusters and be basically this year the perimeter of jeannerod controls the distribution of words among clusters.",
                    "label": 1
                },
                {
                    "sent": "So remember the probability simplex I was showing you depending on what the scale of the parameters were.",
                    "label": 0
                },
                {
                    "sent": "It changed the distribution slightly, right?",
                    "label": 0
                },
                {
                    "sent": "So if you had a larger scale than things tended to concentrate on the on the center of the triangle, if you had less, then things were more diffused, right?",
                    "label": 0
                },
                {
                    "sent": "So this is basically what controls the distribution of words among the clusters.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yeah, try it out when you can.",
                    "label": 1
                },
                {
                    "sent": "Just a few more slides.",
                    "label": 0
                },
                {
                    "sent": "And then I guess we have some time, right?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What I didn't talk about and what the code doesn't handle.",
                    "label": 0
                },
                {
                    "sent": "The code that I've showed you doesn't handle is basically learning hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So hyperparameters in this case is Alpha, for example, or the B value for multinomial that we assumed for the NIPS papers so.",
                    "label": 0
                },
                {
                    "sent": "The value of Alpha is very important for the type of model that we have, right?",
                    "label": 0
                },
                {
                    "sent": "It tells you the expected the prior expected number of components.",
                    "label": 0
                },
                {
                    "sent": "So your result may change really a lot by assuming a small or a large value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So although in Bayesian nonparametrics we say we don't have the model selection problem because we do everything automatically, the number of things are set automatically by the model itself.",
                    "label": 0
                },
                {
                    "sent": "There is still wanting to decide on right.",
                    "label": 0
                },
                {
                    "sent": "There is still a parameter that is Alpha, which we call the hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "That's kind of tells you that this is also a choice of how many, how many components you want to appear in the model, so it's not as rigid as the finite case like finite mixture case where you have to set K and you're done with it, but you still need to think well about what you want to do with Alpha and one way to deal with this is again being basing about things and putting a prior on Alpha.",
                    "label": 0
                },
                {
                    "sent": "So let Alpha also vary with the with the data right?",
                    "label": 0
                },
                {
                    "sent": "So here I'm showing the number of components versus the number of data points.",
                    "label": 1
                },
                {
                    "sent": "So this is the size of these squares, tells you the probability of that combination.",
                    "label": 0
                },
                {
                    "sent": "So if you have these many data points, if you have 20 data points, then having one component has this much probability.",
                    "label": 0
                },
                {
                    "sent": "Mass has two components having this much probability mass and so on.",
                    "label": 0
                },
                {
                    "sent": "So if you assume gamma.",
                    "label": 0
                },
                {
                    "sent": "A prior on Alpha, then this is kind of what you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a heavy bias on towards the smaller number of components, but there is still non zero probability of having larger number of components.",
                    "label": 0
                },
                {
                    "sent": "So if your data supported you can end up with a large number of components.",
                    "label": 0
                },
                {
                    "sent": "But there is something that is funny about this, so the probability diminishes really fast, right?",
                    "label": 0
                },
                {
                    "sent": "So if you want to be.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit more secure about this, so you want a more more uniform ish.",
                    "label": 0
                },
                {
                    "sent": "Distribution, then you can use an inverse gamma with again point 5.5 values, parameter values and basically this is the same plot the number of components K versus the number of data points and the size of these rectangles show you what the probability of each of those congregations is under this prior over Alpha.",
                    "label": 1
                },
                {
                    "sent": "OK, so compare this to.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this so you see that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a.",
                    "label": 0
                },
                {
                    "sent": "There is a more uniform distribution among these possible K values here, so again, there is no best way of choosing priors.",
                    "label": 0
                },
                {
                    "sent": "But these are two possible things that I tried, and depending on your model needs depending on your intuition about what you want to use for your data, you can choose to use this or this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or maybe something else.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you if you believe that your data probably comes from a really small number of components, but you still want to give some probability to larger number of components, maybe you're better off using just a gamma whereas.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're not sure and you want to be more flexible about the number of components growing larger, then you may want to use inverse gamma.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One less thing, which again I didn't talk about the implementation of, but which is about conjugate unconditionally conjugal or conjugate, an non conjugate models basically.",
                    "label": 0
                },
                {
                    "sent": "So here, by conditionally conjugate I mean rather than having a parametric form that we can integrate out all the parameters, we have a parametric form where we can integrate out only some of the parameters.",
                    "label": 0
                },
                {
                    "sent": "But we have to keep around the rest of the parameters.",
                    "label": 1
                },
                {
                    "sent": "OK, so if we.",
                    "label": 0
                },
                {
                    "sent": "Try to model this data using.",
                    "label": 0
                },
                {
                    "sent": "This process is using a conjugate model that I showed you how to use in the previous slides.",
                    "label": 0
                },
                {
                    "sent": "We get density distribution that looks like this.",
                    "label": 0
                },
                {
                    "sent": "There are three peaks that are kind of smooth.",
                    "label": 0
                },
                {
                    "sent": "Where is menu user conditionally conjugate model A model where you cannot integrate out all of the parameters.",
                    "label": 1
                },
                {
                    "sent": "You can only integrate out some of the parameters and your condition on the rest of the parameters you get a distribution that looks like this, so it has, so it looks less Gaussian and you can see that it has these.",
                    "label": 1
                },
                {
                    "sent": "It has more probability mass and these points, and that's probably because of.",
                    "label": 0
                },
                {
                    "sent": "These components being like outlying and pulling this source here and as a comparison comparison, this is a kernel density estimation or the parzen window fit to the density.",
                    "label": 0
                },
                {
                    "sent": "You can see that.",
                    "label": 0
                },
                {
                    "sent": "This is more grainy simply because what this model does is it assumes a Gaussian bump on each of these data points, so it basically it is a nonparametric model, but not in those nonparametric Bayesian sense.",
                    "label": 0
                },
                {
                    "sent": "So it's a non parametric nonparametric model in the sense that it's.",
                    "label": 0
                },
                {
                    "sent": "It's complexity also increases with the number of data points because it assumes the density is.",
                    "label": 0
                },
                {
                    "sent": "This density is basically some of these Gaussian bumps that are in many of them.",
                    "label": 0
                },
                {
                    "sent": "If you have any data points so you can see that this is a little too of course, But then this is maybe a little too smooth, so maybe this is a better representation of your data, but as I said, it may be a lot harder to do inference on conditionally conjugate or non conjugate models as opposed to conjugate models both in terms of runtime implementation and getting a sampler too actually.",
                    "label": 0
                },
                {
                    "sent": "The correct sampling from.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the statistics for.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data the this data set.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That which is here.",
                    "label": 0
                },
                {
                    "sent": "Basically comparing the conditional.",
                    "label": 0
                },
                {
                    "sent": "Sorry the conjugate model and the non conjugate model.",
                    "label": 0
                },
                {
                    "sent": "You can see clear and these are different datasets, so you can see why the difference comes about right.",
                    "label": 0
                },
                {
                    "sent": "So the conjugate model tends to use a lot less components, is a lot more conservative in the number of components it uses as opposed to the non conjugate model.",
                    "label": 0
                },
                {
                    "sent": "Everything about these models were the same all the parameter settings hyperparameter settings.",
                    "label": 0
                },
                {
                    "sent": "ETC except for the form of the prior distributions form of the.",
                    "label": 1
                },
                {
                    "sent": "Base distribution basically so you can see that there's a big difference between assuming critical versus non conjugate.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is basically showing the number of data points assigned to each of these components, so there's a clear difference between that too.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's it.",
                    "label": 1
                },
                {
                    "sent": "I hope you get a better understanding of the rich.",
                    "label": 0
                },
                {
                    "sent": "The process now so.",
                    "label": 0
                },
                {
                    "sent": "I think the takehome messages.",
                    "label": 0
                },
                {
                    "sent": "I think some of you don't have any way to take home messages.",
                    "label": 0
                },
                {
                    "sent": "They're not hard to use their they're very easy to implement for at least some cases, and they do give really good fits to data.",
                    "label": 0
                },
                {
                    "sent": "But then you have to be careful about what your prior assumptions are.",
                    "label": 0
                },
                {
                    "sent": "You cannot just go blindly and take a reset process out of the drawer and apply it to our data and think that you're going to get the universal solution, so you still need to think hard about the prior assumptions of your model.",
                    "label": 0
                },
                {
                    "sent": "Although this is not.",
                    "label": 0
                },
                {
                    "sent": "Metric is not going to solve all your problems.",
                    "label": 0
                },
                {
                    "sent": "I get.",
                    "label": 0
                }
            ]
        }
    }
}