{
    "id": "t4sjgp2wv63grrgki6vl2opzcwxeb7qw",
    "title": "Learning equivalence classes of directed acyclic latent variable models from multiple datasets with overlapping variables, incl. discussion by Ricardo Silva",
    "info": {
        "author": [
            "Ricardo Silva, University College London",
            "Robert E. Tillman, Carnegie Mellon University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/aistats2011_tillman_learning/",
    "segmentation": [
        [
            "OK, so this is joint work with Peter's parties, and so given that our title is a little long, I'm just going to go ahead and discuss the problem, an motivation."
        ],
        [
            "OK, so when we are typically interested in doing structure learning either for.",
            "To later do probabilistic inference, or if we want to make assumptions about the data generating process and try and predict causal relationships in either of these contexts, we typically assume that we're working with a single data sets of IID observations, and we're going to be able to use this data set to do structure learning from.",
            "Maybe there's missing values, but we typically then assume that they're missing at random, and so we can just compute the missing values an.",
            "Go forward as normal.",
            "In"
        ],
        [
            "In some circumstances, though, we don't actually have this kind of data, or we can obtain a single data set that has all the variables that we're interested in.",
            "So in particular, maybe we get data from several different sources, so maybe we get a data set from one lab in a data set from another lab and.",
            "Difference in that they may be used different recording instruments or one lab gives us continuous data, the other has some kind of discretization of the same variables.",
            "So we can't just put the datasets together.",
            "And Furthermore each of these labs may measure some of the variables that we're interested in.",
            "But maybe not all of them, and they may be interested in slightly different variable sets.",
            "So we end up with like 2 datasets here where they have some variables in common an.",
            "Variables that are not in common."
        ],
        [
            "So one area in particular that this comes up is an application that we're calling learning neural cascades during cognitive tasks.",
            "So in particular, what these researchers seem to be interested in is when an individual is doing some type of cognitive task, like trying to decide whether 2 words rhyme.",
            "They want to know what the cascade of interactions are between different brain regions.",
            "So what brain region causes another brain region and then causes another brain region?",
            "During this particular cognitive task, so the way they obtained data for these tasks is that one particular individual is put under an fMRI machine.",
            "They do the task, they get the brain data than they do it again, and they repeat this several times.",
            "So you get one data set per individual, and then you scan another individual and repeat this process.",
            "So you end up with a bunch of different datasets for each individual and.",
            "Even though that we assume some kind of underlying structure for the brain, we are assuming that there's going to be slight variations among individuals, so we can't really put these datasets together, and also because of randomness and people moving their heads and stuff like that.",
            "Sometimes we're not able to.",
            "Actually, in any given data set, they're not able to actually measure all of the variables that they try and measure, or they don't get useful recordings for all those variables.",
            "So you end up with a bunch of data sets with slightly different subsets.",
            "Of this bigger set of variables that you're interested in.",
            "OK, so now that we have that in form."
        ],
        [
            "Motivation formally will define this problem as we have some underlying structure for a set of variables of interest V. And we obtain observation ULL data for several subsets of proper subsets of these variables, V1V2 etc."
        ],
        [
            "And now we're going to make another assumption, which is that maybe this entire set of variables that we're interested in is not representable via a DAG, or we can't represent this data generating process with the tag.",
            "But if we add enough latent variables that were not observing, then we should be able to represent it as a DAG over all the all of the variables plus hidden variables."
        ],
        [
            "So the goal then is to try and learn this underlying structure from the datasets."
        ],
        [
            "And in the context we're looking at here is we want to try and learn as much information as we can using conditional independence information.",
            "So there's several different ways of."
        ],
        [
            "Going about structure learning in this way, but in this case we want to just see how good can we do just using conditional independence information, and this is equivalent to basically scoring Dags because high scoring if you are using a score like BICS, then it's going to assign the highest score to the tag that has the right independence is so since we're only caring about conditional independence, what this reduces to is we want to try and learn.",
            "A complete Markov equivalence class with these latent variables that we're adding using all of this data that we have OK.",
            "So now so since we're having to add latent variables, there's the reason we're doing this is."
        ],
        [
            "There's essentially two types of problems that happen which make it not possible to represent certain types of things with tags.",
            "When there's variables that are UN observed.",
            "So the first, which is the most common thing, is, which I think everyone familiar with is confounding, and so that is you just have someone observed variable Z that is causing two variables X&Y."
        ],
        [
            "And so if you don't observe C, then you're going to predict some kind of Arc between X&Y, and there's no really no way to represent this using a DAG, so you need something additional."
        ],
        [
            "The other thing that can happen is something that's a little bit more extreme, which is if you have a structure like this, But Z is an observed and Z also happens to be conditioned on.",
            "So this is related to selection bias.",
            "Then what you're actually going."
        ],
        [
            "Infer is that there should be some kind of dependence between X&Y but X&Y are actually independent, so this is a somewhat worse example where when you don't observe a particular variable it can cause you to produce a structure that's clearly going to have errors in it are not going to represent what you're actually trying to represent."
        ],
        [
            "So to represent these types of things, we need a more rich class of graphs than Dags to work with.",
            "And so we're going to work with something called maximal ancestral graphs, which is a class of graphs that were introduced in this paper by Richardson and Sporty's.",
            "So what these graphs represents are essentially sets of Dags that share the same conditional independence information amongst the observed variables.",
            "They might have different relationships amongst the unobserved variables, but they have common conditional independence information amongst whatever you're observing."
        ],
        [
            "So, um, these graphs have directed edges as you would have in a DAG, but you also have BI directed edges like this, and what these by directed edges represents are where you have confounding.",
            "You have some unobserved variable that is a cause of.",
            "In this case, would be a cause of X&W and."
        ],
        [
            "They also have undirected edges, and this represents essentially when you have.",
            "So in this case this undirected edge between X&Z indicates that there's some variable that is an observed, which Z&X are both causing, and this variable is being conditioned on.",
            "So effectively it's causing you to seek some kind of independence that is not actually there.",
            "So."
        ],
        [
            "These graphs have two important properties.",
            "Since we're going to be considering only looking at subsets of the variables that we're interested in.",
            "Particularly, there closed under conditioning and marginalization and the second criti."
        ],
        [
            "Area that we're interested that turns out to be good is they have a separation criteria called M separation, which relates conditional independence is that are in the graph or well conditional in dependencies that are true in a joint probability distribution to the graph topology.",
            "So this is like the separation in the case of Dags and the case if you have a mag that actually turns out to be a deck, it just reduces to D separation.",
            "Hum.",
            "OK, so wait.",
            "So since we're here, I'll just define a little bit more of the mag semantics.",
            "So whenever you have any type of edge between X anytime between two variables.",
            "So like between X and you hear then will say that.",
            "I mean why and you will say that why and you are adjacent, so an adjacency just refers to any any edge between variables.",
            "The second thing is so in the case of Dags, typically whenever you have a head to heads and point like that, that's referred to as V structure.",
            "So we're going to continue using that terminology.",
            "But since in mag we could also have a case like this where you have this head to head formulation and also ahead here and head here, we're also going to say that that.",
            "Is a V structure, so anytime that you have this head formation it's AV structure and if it turns out like so, for instance, why Wu?",
            "This is a V structure here.",
            "Whenever you have the structure, if this edge were not present between the edge between the nodes on either side, then we will say that it's an immorality an just as we would say in the case with tags.",
            "But again we don't care about what the endpoints is here."
        ],
        [
            "Right, OK, so we can say in the case of DAG is we know that two Dags are Markov equivalent, just if they share the same adjacency's and the same.",
            "Immoralities, and since Mags are a little bit richer, we need to go a little bit further, so and consider different types of path so it turns out that mags or Markov equivalence if they share the same adjacency's immorality's and another type of structure which is called a discriminating path, be structure, so I won't go through the formal definition of what this type of structure is, but basically it occurs when you have these.",
            "Chains of BI directed edges like this.",
            "But this defines when two magsar Markov equivalent and just as."
        ],
        [
            "Just as we have in the case of Dags away a certain type of graphical structure referred to as P dag, that represents the entire set of Markov equivalent Dags, we have something like this in the case of mags.",
            "So a partial ancestral graph this so this right here, is a partial ancestral graph, and what this represents is a Markov equivalence class of mags.",
            "So this is 1 particular mag that is contained in this Markov equivalence class that's represented by here.",
            "So these structures have a third type of endpoints, which is this circle and what that basically means is that there is some mag in this equivalence class that has.",
            "Either a head endpoint there or tail end point there wherever you see definite head or a definite tail, it means that every mag in that particular Markov equivalence class that's being represented has.",
            "Every such mag has ahead or retail there, but the circle just means it's invariant, OK?"
        ],
        [
            "So now that we have that we can restate the formal goal so when we have data like this where we have some variable multiple datasets with some variables in common, some not in common, what we want to then learn is set of pags, which is as much.",
            "We want to learn as much as we can about the data generating process, but since we're only using conditional independence information, we can only go as far as Apag Now, since we are in, since we're.",
            "When we have multiple data sets like this and there are some sets of variables that are never going to be measured jointly in a particular data set, there's going to be some conditional independence information that we don't have access to, so we actually not only can we not always distinguish it to a particular tag, but we may have to go to a set of tags.",
            "So we looked at doing."
        ],
        [
            "Something very similar to this in this NIPS paper.",
            "So in this paper our goal was rather than having these types of these datasets with some variables in common, some variables not in common.",
            "What we looked at was if we actually have the structures themselves that correspond to this data set, how do we combine these structures and find all of the possible structures over the entire set of variables that's measured in any structure?",
            "And so then, with this we effectively get the same thing is the output is going to be a set of tags here.",
            "So this is this.",
            "This algorithm turns out to be useful because sometimes you don't actually have the data sets in.",
            "If you're in some particular domain.",
            "Sometimes people have constructed graphs using domain knowledge, so you may want to actually work with the graphs themselves, but trying to go about it this way leads to some problems, which I'll talk about in a second.",
            "So we're going to try and do something a little bit different.",
            "We"
        ],
        [
            "Could however, just try and use this approach from this previous paper to tackle our current problem, and so the way that we would do this is just take each of our individual datasets an we already had this algorithm called FCI which what this algorithm?",
            "This is a correct and complete algorithm which will give us a PAG for any particular data set and then we."
        ],
        [
            "Just inputs each of these tags too.",
            "Previous approach, called ion that we have and that will return us the set of tags over all of the variables now."
        ],
        [
            "There's several problems as I mentioned before.",
            "With this, the first is that.",
            "If we if we try and learn each datasets individually then we may have one statistical error in one data set but not in another data set.",
            "And what that's going to cause us to do is learn one structure that entails one conditional independence that is assumed to be a dependence in the other.",
            "Structure, So what?",
            "This causes it to have is 2 graphs that entail conflicting sets of conditional independence in dependencies.",
            "So this turns out to be a pretty pathological problem for trying to use this algorithm with real data, even though it's correct in."
        ],
        [
            "Awesome tonically.",
            "So the other problem with trying to use this approach is that we even if.",
            "Well, each each data set that we have, it doesn't take advantage of the same variables that are contained in another data set.",
            "So we're ignoring a lot of useful information if we try and learn from each each individual data set separately."
        ],
        [
            "And finally, this approach just takes a lot of a lot of time and uses a lot of memory, so we'd like to do a little bit better than this approach."
        ],
        [
            "So what our approach is, is we're going to rather than learn structures for each of these individual datasets and integrate them, we're just going to try and work directly from the datasets, and we're always whenever we need to find conditional independence information, we're going to use as many of the datasets as possible to do our conditional independence testing.",
            "OK, so."
        ],
        [
            "So we need to think about then how are we going to you do conditional independence testing when we have multiple datasets so.",
            "One way that we could do this is just try and concatenate the data set."
        ],
        [
            "But if these are coming from different places as we already mentioned earlier, there's going to be slight differences that may exist even after we standardize our data.",
            "So this is not going to work so well.",
            "Another approach."
        ],
        [
            "That we looked at trying to do in, not in the context of learning from multiple datasets with different variable sets, but in the context of structure learning was something called pooling P values, and So what we do in this case is."
        ],
        [
            "Is that so?",
            "We take each data set an we pick pick a particular conditional independence test that would be useful for each of those datasets, and then for the test statistic that's used in that particular conditional independence test.",
            "We just find the."
        ],
        [
            "Value that we get for that particular test statistic and then."
        ],
        [
            "Using all of these P values that we get from the different test statistics which correspond to different datasets, we can get a new test statistic that corresponds to the distribution of these P values under the null."
        ],
        [
            "And so we tried several ways of doing this in this paper, and it turns out that one test statistic which recalling the Fisher test statistic, which essentially just turns into a weighted sum of log P values, leads to the best results for doing this."
        ],
        [
            "So what we're going to do then, is every time we need to do a conditional independence test, we're going to use in our approach.",
            "This Fisher test statistic, with each data set that contains the relevant variables.",
            "So just find every data set that contains the variables that we need in the conditional independence tests.",
            "Get the P values form the new test statistic, and then we can make an accept or reject precision decision based on."
        ],
        [
            "This this new value.",
            "Right, OK, so now we can get to the basic structure of how the algorithm itself is going to work.",
            "So what we're going to do is begin with a complete graph over all of these variables that we're interested in.",
            "So a fully connected graph over all of this complete variable set V. And then, um."
        ],
        [
            "From that, we're going to do.",
            "All of the conditional independence tests that this FCI algorithm tells us we need to do but with respect to each variable set that we're looking at in a particular data set."
        ],
        [
            "And what we can show is that if we follow these steps then this is going to get us a structure that contains a super set of the edges that are contained in the correct bag.",
            "The bag, which corresponds to the true data generating process and a subset of the immorality Zan discriminating path be structures that are in this.",
            "In this pack.",
            "Right so, um, from that I mean what?"
        ],
        [
            "Basically tells us is then we all we need to do is take this structure that we get and find out all the ways that we could remove edges or Adam moralities.",
            "Ads discriminating path, be structures and still be consistent with the conditional independence information.",
            "That's true in all of the datasets that we observe."
        ],
        [
            "So we just need to find all the ways that we can generate graphs by edge removal and adding in moralities.",
            "And then once we do that, we just need to apply a final set of orientation rules that gets us apag from that resulting structure.",
            "So.",
            "Right, I mean one."
        ],
        [
            "So we need some way of deciding what edges we can remove and how we can know whether this is going to still be consistent with all this conditional independence information that we have.",
            "So in the paper we give one theorem that gives us something we can use to do this."
        ],
        [
            "So what this theorem says is that so let's G star V1V2 dot dot be marginalization's of the correct mag structure after we marginalized.",
            "Are all the variables that are not contained in V1 and V2 so."
        ],
        [
            "Then what we can say is G is consistent with all of these graphs."
        ],
        [
            "If there exists a sets Z which is a subset of any one of these particular graphs.",
            "Such is there such that two variables X&Y are in separated given Z?",
            "Then then this is satisfied in our graph G. So basically, in other words, if there's any Z which separates two variables X&Y in any any of these marginal graphs, then we have to have that M separation true in R. In the pan.",
            "The correct rat grab graph G that we're considering the correct mag.",
            "And then there's finally one more condition."
        ],
        [
            "In which is that if we have two variables that are adjacent in any one of these marginal graphs, then any graph G That's Markov equivalent to the correct structure has to contain another type of path, which I'll skip defining but something very simple that we can chat called and inducing path, but basically introducing path just represents sets of.",
            "So sheated with when sets of M connecting paths exist.",
            "So whenever we have these two conditions satisfied, then we know that a given mag is going to be consistent with all of our marginal datasets that we observe.",
            "OK, and."
        ],
        [
            "Furthermore, we know that.",
            "The conditional independencies that FCI tells us to check gives us a sufficient number of the Z sets to check.",
            "So basically FCI does all of the independence tests that we need to do to check this theorem here.",
            "And Furthermore we get from FCI.",
            "We can use based on the the conditional independence test it chooses to do this set."
        ],
        [
            "It's of inducing paths that we need to check."
        ],
        [
            "And using this information we can put it together and decide what edges.",
            "We can remove and what immorality, Zan discriminating pathway structures we can put into the structure to get a mag that is going to be Markov equivalent to the true data generating process?",
            "So just to give you an."
        ],
        [
            "Example of how this works.",
            "So if this is the true graph right here.",
            "And we have two datasets V1 and V2.",
            "The first contains all the variables except V. The second all accepts W."
        ],
        [
            "Then in the first stage, we're going to create this complete graph over all these variables, then from."
        ],
        [
            "That we're going to do the first stages of the FCI rules with each of these datasets and we're going to structure like this.",
            "An from that we're going."
        ],
        [
            "To be able to tell that we can actually generate 2 bags here that are Markov equivalent to the marginalization's with respect to each of these particular datasets.",
            "So right, so in this case, given that there was some conditional independence information that we did not have access to, we're only able to.",
            "We weren't able to get a particular marvelous class.",
            "We were only able to distinguish it to two particular Markov equivalence classes."
        ],
        [
            "But there are some examples where even though we can observe all the variables, we can actually distinguish it to one particular tag.",
            "This is one such example."
        ],
        [
            "OK, so.",
            "Right, so then we can formally say that using this procedure, if Maggie describes the true data generating mechanism, then in large sample limits."
        ],
        [
            "The algorithm is correct.",
            "In other words, all of the graphs in the output imply the correct are consistent with the data sets we have."
        ],
        [
            "And it's also complete, and that if there's any any mag or any any pag representing it, that implies all of these conditional in dependencies, then it's going to be included in the output of our algorithm."
        ],
        [
            "So finally we evaluated this on a few datasets, so I'll just go through this quickly because running out of time.",
            "So we try to complete data, complete set of variables at 14 an at 8:00 and then we try to two data set example in a three data set example.",
            "Here we removed two variables from each data set.",
            "Here we moved here, removed one here with two."
        ],
        [
            "In each case, the blue line is our algorithm and the green line is.",
            "The result of running FCI on each data set and then putting it to the ion algorithm and so we do a lot better in terms of precision and recall and also in terms of runtime and memory usage."
        ],
        [
            "And the results look about the same for the three data set example."
        ],
        [
            "So and finally we applied this to some neuroimaging data that was the riming task that I described earlier, so.",
            "Basically, each of these variables corresponds to a particular brain region.",
            "In this case we had 13 datasets, 160 samples.",
            "Each data set was missing, one to four variables, so this variable represents the input stimulus, and So what we see is a cascade starting on the left side.",
            "So these are all the left brain regions and then eventually moving to the right side, which is consistent with some domain knowledge we have."
        ],
        [
            "OK, so finally to conclude we.",
            "So basically we give a correct incomplete algorithm for learning from multiple datasets with overlapping variables."
        ],
        [
            "With this we get some results which relate mags to sets of Markov equivalent mags over different variable sets."
        ],
        [
            "And we showed how we can incorporate this other conditional independence methods, which takes care of the pathological cases of ion.",
            "And this leads to much greater act."
        ],
        [
            "Or see intractability.",
            "Right and so."
        ],
        [
            "So in future work we want to look at."
        ],
        [
            "So if we can combine this."
        ],
        [
            "Swift datasets where some variables are subject to an intervention also possibly combine this with structured learning methods based on non gaussianity and nonlinear."
        ],
        [
            "Pretty, and if we can relax the simplicity assumption, that would be good as well, so that concludes."
        ],
        [
            "Now let's talk about Lady authors on their very interesting contribution and also organizes other conference of starting this new tradition here in machine learning.",
            "So Jason couldn't show up.",
            "You have to copy him yet again for another talk.",
            "But then I'm going to be brief, so I'd like to divide.",
            "The powers are static."
        ],
        [
            "Having three main classes, balance of estimation, computation, identification and this paper is very nice because it tackles off three of these issues at the same time.",
            "It actually actually reminds me of some old paper by Peter spurts on AI stats 10 years ago when this is to call a workshop on artificial intelligence and statistics.",
            "So his problem that time was outing for this causes structures when we have limited time and limited data so you don't trust higher water independence test, so don't be able to.",
            "Test them cause ran out of time, so in one sense we had this problem of infrastructure.",
            "When only a subset of the condition independence could be observed, and this contribution by team in sports can be seen some sort of generalization of that, even though it came from completely different perspective."
        ],
        [
            "Also, there are some side effects.",
            "Other formulations are very interesting.",
            "This is wonderful call to buy power in his textbook and causality on actually how can get extra robustness on friended structures if we have data with slightly different setups.",
            "Becausw accidental qualitative information such as condition depends go straight constraints might disappear when C based from slightly different perturbed models and this might be already something is.",
            "Comes for for free.",
            "From the contribution in sports will be interesting to see on exactly some sort of empirical evaluation.",
            "If you have some extra robustness out of this formulation.",
            "So."
        ],
        [
            "Robert talked about this possibility of dealing with selection bias.",
            "When you have this hidden latent variables that are conditioning on.",
            "But is a beast and difficult.",
            "Imagine how this would happen in practice.",
            "We have independent studies 'cause usually this selection buses don't come by design.",
            "They happen by chance and you are assuming here the selection biases are shared among different studies, so this might not be very plausable.",
            "At least is that with difficulty.",
            "Imagine situations where this would happen.",
            "So Electro mention also this."
        ],
        [
            "Some work there in the second metrics literature on large structural learning models.",
            "Overlapping variables when sometimes we have tests that give the patient over different, let's say medical centers, and this test overlap only a few subset of variables and people like to fit factor analysis models to this data.",
            "And sometimes you can infer situations like that covariance between various than ever be served.",
            "If you're willing to make some extra assumptions about the common structure, they have very interesting to see which kind of relationship.",
            "These methods have their general peg formulation."
        ],
        [
            "So they mention the possibility of exploiting other constraints with saying dependence constraints such as functional constraints that you've seen some approaches for causal inference, for example, exploiting the fact that might have additive errors in your deck structure.",
            "But questions like that and not really closed under marginalization, so they might pose a problem if some overlap exists where the assumptions in different datasets actually don't match each other.",
            "Interesting to see how can we actually generalize this overlapping cases when you have this conflicting assumptions."
        ],
        [
            "Finally, I just follow up in dates.",
            "Comments is be be interesting to see.",
            "How can you do a baseline version of that?",
            "Actually the problem is even easier when you have different parameter sets 'cause you don't have to worry about time parameters from models that are actually over different variables.",
            "In this case don't even missing data models Eater because can represent the marginals overdubs variables directly without having explicit latent variables from the point of view of regularization."
        ],
        [
            "Can you have the link to compose a likelihood, which is also nice in the sense that you don't need to specify a full likelihood for all of your variables?",
            "Frankfurt standing calls relations only for subset of your variables might want to specify likelihood functions just for some subsets, and letting particle distribution deal of the rest and compose likelihood also have this nice properties that can design A penalty function along with it to be able to search for different structures there.",
            "And you have this source problems you might."
        ],
        [
            "Also want to make the link.",
            "2.",
            "Constrained optimization if we have ice core function like this, which is a composed likelihood function, you might want to enforce effect that over these different subsets have common independent sub model and this common depends.",
            "Models might be written as constraints as logical constraints in optimization function and the question is how to deal with the actual very large number of these constraints with some cutting plane approach might help here.",
            "Write constraints as they are needed.",
            "Alright, so I would like to think again they speakers as you saying their voice statical SoC.",
            "I'd like to propose a vote of thanks or this paper in a movie I stats."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with Peter's parties, and so given that our title is a little long, I'm just going to go ahead and discuss the problem, an motivation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when we are typically interested in doing structure learning either for.",
                    "label": 0
                },
                {
                    "sent": "To later do probabilistic inference, or if we want to make assumptions about the data generating process and try and predict causal relationships in either of these contexts, we typically assume that we're working with a single data sets of IID observations, and we're going to be able to use this data set to do structure learning from.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's missing values, but we typically then assume that they're missing at random, and so we can just compute the missing values an.",
                    "label": 0
                },
                {
                    "sent": "Go forward as normal.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In some circumstances, though, we don't actually have this kind of data, or we can obtain a single data set that has all the variables that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So in particular, maybe we get data from several different sources, so maybe we get a data set from one lab in a data set from another lab and.",
                    "label": 0
                },
                {
                    "sent": "Difference in that they may be used different recording instruments or one lab gives us continuous data, the other has some kind of discretization of the same variables.",
                    "label": 0
                },
                {
                    "sent": "So we can't just put the datasets together.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore each of these labs may measure some of the variables that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "But maybe not all of them, and they may be interested in slightly different variable sets.",
                    "label": 0
                },
                {
                    "sent": "So we end up with like 2 datasets here where they have some variables in common an.",
                    "label": 0
                },
                {
                    "sent": "Variables that are not in common.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one area in particular that this comes up is an application that we're calling learning neural cascades during cognitive tasks.",
                    "label": 1
                },
                {
                    "sent": "So in particular, what these researchers seem to be interested in is when an individual is doing some type of cognitive task, like trying to decide whether 2 words rhyme.",
                    "label": 0
                },
                {
                    "sent": "They want to know what the cascade of interactions are between different brain regions.",
                    "label": 0
                },
                {
                    "sent": "So what brain region causes another brain region and then causes another brain region?",
                    "label": 0
                },
                {
                    "sent": "During this particular cognitive task, so the way they obtained data for these tasks is that one particular individual is put under an fMRI machine.",
                    "label": 0
                },
                {
                    "sent": "They do the task, they get the brain data than they do it again, and they repeat this several times.",
                    "label": 0
                },
                {
                    "sent": "So you get one data set per individual, and then you scan another individual and repeat this process.",
                    "label": 0
                },
                {
                    "sent": "So you end up with a bunch of different datasets for each individual and.",
                    "label": 0
                },
                {
                    "sent": "Even though that we assume some kind of underlying structure for the brain, we are assuming that there's going to be slight variations among individuals, so we can't really put these datasets together, and also because of randomness and people moving their heads and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we're not able to.",
                    "label": 0
                },
                {
                    "sent": "Actually, in any given data set, they're not able to actually measure all of the variables that they try and measure, or they don't get useful recordings for all those variables.",
                    "label": 0
                },
                {
                    "sent": "So you end up with a bunch of data sets with slightly different subsets.",
                    "label": 0
                },
                {
                    "sent": "Of this bigger set of variables that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we have that in form.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivation formally will define this problem as we have some underlying structure for a set of variables of interest V. And we obtain observation ULL data for several subsets of proper subsets of these variables, V1V2 etc.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we're going to make another assumption, which is that maybe this entire set of variables that we're interested in is not representable via a DAG, or we can't represent this data generating process with the tag.",
                    "label": 0
                },
                {
                    "sent": "But if we add enough latent variables that were not observing, then we should be able to represent it as a DAG over all the all of the variables plus hidden variables.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the goal then is to try and learn this underlying structure from the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the context we're looking at here is we want to try and learn as much information as we can using conditional independence information.",
                    "label": 0
                },
                {
                    "sent": "So there's several different ways of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going about structure learning in this way, but in this case we want to just see how good can we do just using conditional independence information, and this is equivalent to basically scoring Dags because high scoring if you are using a score like BICS, then it's going to assign the highest score to the tag that has the right independence is so since we're only caring about conditional independence, what this reduces to is we want to try and learn.",
                    "label": 0
                },
                {
                    "sent": "A complete Markov equivalence class with these latent variables that we're adding using all of this data that we have OK.",
                    "label": 1
                },
                {
                    "sent": "So now so since we're having to add latent variables, there's the reason we're doing this is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's essentially two types of problems that happen which make it not possible to represent certain types of things with tags.",
                    "label": 0
                },
                {
                    "sent": "When there's variables that are UN observed.",
                    "label": 0
                },
                {
                    "sent": "So the first, which is the most common thing, is, which I think everyone familiar with is confounding, and so that is you just have someone observed variable Z that is causing two variables X&Y.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you don't observe C, then you're going to predict some kind of Arc between X&Y, and there's no really no way to represent this using a DAG, so you need something additional.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing that can happen is something that's a little bit more extreme, which is if you have a structure like this, But Z is an observed and Z also happens to be conditioned on.",
                    "label": 0
                },
                {
                    "sent": "So this is related to selection bias.",
                    "label": 0
                },
                {
                    "sent": "Then what you're actually going.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Infer is that there should be some kind of dependence between X&Y but X&Y are actually independent, so this is a somewhat worse example where when you don't observe a particular variable it can cause you to produce a structure that's clearly going to have errors in it are not going to represent what you're actually trying to represent.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to represent these types of things, we need a more rich class of graphs than Dags to work with.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to work with something called maximal ancestral graphs, which is a class of graphs that were introduced in this paper by Richardson and Sporty's.",
                    "label": 1
                },
                {
                    "sent": "So what these graphs represents are essentially sets of Dags that share the same conditional independence information amongst the observed variables.",
                    "label": 1
                },
                {
                    "sent": "They might have different relationships amongst the unobserved variables, but they have common conditional independence information amongst whatever you're observing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um, these graphs have directed edges as you would have in a DAG, but you also have BI directed edges like this, and what these by directed edges represents are where you have confounding.",
                    "label": 0
                },
                {
                    "sent": "You have some unobserved variable that is a cause of.",
                    "label": 0
                },
                {
                    "sent": "In this case, would be a cause of X&W and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They also have undirected edges, and this represents essentially when you have.",
                    "label": 0
                },
                {
                    "sent": "So in this case this undirected edge between X&Z indicates that there's some variable that is an observed, which Z&X are both causing, and this variable is being conditioned on.",
                    "label": 0
                },
                {
                    "sent": "So effectively it's causing you to seek some kind of independence that is not actually there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These graphs have two important properties.",
                    "label": 0
                },
                {
                    "sent": "Since we're going to be considering only looking at subsets of the variables that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Particularly, there closed under conditioning and marginalization and the second criti.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area that we're interested that turns out to be good is they have a separation criteria called M separation, which relates conditional independence is that are in the graph or well conditional in dependencies that are true in a joint probability distribution to the graph topology.",
                    "label": 1
                },
                {
                    "sent": "So this is like the separation in the case of Dags and the case if you have a mag that actually turns out to be a deck, it just reduces to D separation.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "OK, so wait.",
                    "label": 0
                },
                {
                    "sent": "So since we're here, I'll just define a little bit more of the mag semantics.",
                    "label": 0
                },
                {
                    "sent": "So whenever you have any type of edge between X anytime between two variables.",
                    "label": 0
                },
                {
                    "sent": "So like between X and you hear then will say that.",
                    "label": 0
                },
                {
                    "sent": "I mean why and you will say that why and you are adjacent, so an adjacency just refers to any any edge between variables.",
                    "label": 1
                },
                {
                    "sent": "The second thing is so in the case of Dags, typically whenever you have a head to heads and point like that, that's referred to as V structure.",
                    "label": 0
                },
                {
                    "sent": "So we're going to continue using that terminology.",
                    "label": 0
                },
                {
                    "sent": "But since in mag we could also have a case like this where you have this head to head formulation and also ahead here and head here, we're also going to say that that.",
                    "label": 0
                },
                {
                    "sent": "Is a V structure, so anytime that you have this head formation it's AV structure and if it turns out like so, for instance, why Wu?",
                    "label": 0
                },
                {
                    "sent": "This is a V structure here.",
                    "label": 0
                },
                {
                    "sent": "Whenever you have the structure, if this edge were not present between the edge between the nodes on either side, then we will say that it's an immorality an just as we would say in the case with tags.",
                    "label": 0
                },
                {
                    "sent": "But again we don't care about what the endpoints is here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, OK, so we can say in the case of DAG is we know that two Dags are Markov equivalent, just if they share the same adjacency's and the same.",
                    "label": 1
                },
                {
                    "sent": "Immoralities, and since Mags are a little bit richer, we need to go a little bit further, so and consider different types of path so it turns out that mags or Markov equivalence if they share the same adjacency's immorality's and another type of structure which is called a discriminating path, be structure, so I won't go through the formal definition of what this type of structure is, but basically it occurs when you have these.",
                    "label": 0
                },
                {
                    "sent": "Chains of BI directed edges like this.",
                    "label": 0
                },
                {
                    "sent": "But this defines when two magsar Markov equivalent and just as.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just as we have in the case of Dags away a certain type of graphical structure referred to as P dag, that represents the entire set of Markov equivalent Dags, we have something like this in the case of mags.",
                    "label": 0
                },
                {
                    "sent": "So a partial ancestral graph this so this right here, is a partial ancestral graph, and what this represents is a Markov equivalence class of mags.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 particular mag that is contained in this Markov equivalence class that's represented by here.",
                    "label": 0
                },
                {
                    "sent": "So these structures have a third type of endpoints, which is this circle and what that basically means is that there is some mag in this equivalence class that has.",
                    "label": 0
                },
                {
                    "sent": "Either a head endpoint there or tail end point there wherever you see definite head or a definite tail, it means that every mag in that particular Markov equivalence class that's being represented has.",
                    "label": 0
                },
                {
                    "sent": "Every such mag has ahead or retail there, but the circle just means it's invariant, OK?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that we have that we can restate the formal goal so when we have data like this where we have some variable multiple datasets with some variables in common, some not in common, what we want to then learn is set of pags, which is as much.",
                    "label": 0
                },
                {
                    "sent": "We want to learn as much as we can about the data generating process, but since we're only using conditional independence information, we can only go as far as Apag Now, since we are in, since we're.",
                    "label": 0
                },
                {
                    "sent": "When we have multiple data sets like this and there are some sets of variables that are never going to be measured jointly in a particular data set, there's going to be some conditional independence information that we don't have access to, so we actually not only can we not always distinguish it to a particular tag, but we may have to go to a set of tags.",
                    "label": 0
                },
                {
                    "sent": "So we looked at doing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something very similar to this in this NIPS paper.",
                    "label": 0
                },
                {
                    "sent": "So in this paper our goal was rather than having these types of these datasets with some variables in common, some variables not in common.",
                    "label": 0
                },
                {
                    "sent": "What we looked at was if we actually have the structures themselves that correspond to this data set, how do we combine these structures and find all of the possible structures over the entire set of variables that's measured in any structure?",
                    "label": 0
                },
                {
                    "sent": "And so then, with this we effectively get the same thing is the output is going to be a set of tags here.",
                    "label": 0
                },
                {
                    "sent": "So this is this.",
                    "label": 0
                },
                {
                    "sent": "This algorithm turns out to be useful because sometimes you don't actually have the data sets in.",
                    "label": 0
                },
                {
                    "sent": "If you're in some particular domain.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people have constructed graphs using domain knowledge, so you may want to actually work with the graphs themselves, but trying to go about it this way leads to some problems, which I'll talk about in a second.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try and do something a little bit different.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Could however, just try and use this approach from this previous paper to tackle our current problem, and so the way that we would do this is just take each of our individual datasets an we already had this algorithm called FCI which what this algorithm?",
                    "label": 0
                },
                {
                    "sent": "This is a correct and complete algorithm which will give us a PAG for any particular data set and then we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just inputs each of these tags too.",
                    "label": 0
                },
                {
                    "sent": "Previous approach, called ion that we have and that will return us the set of tags over all of the variables now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's several problems as I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "With this, the first is that.",
                    "label": 0
                },
                {
                    "sent": "If we if we try and learn each datasets individually then we may have one statistical error in one data set but not in another data set.",
                    "label": 0
                },
                {
                    "sent": "And what that's going to cause us to do is learn one structure that entails one conditional independence that is assumed to be a dependence in the other.",
                    "label": 0
                },
                {
                    "sent": "Structure, So what?",
                    "label": 0
                },
                {
                    "sent": "This causes it to have is 2 graphs that entail conflicting sets of conditional independence in dependencies.",
                    "label": 0
                },
                {
                    "sent": "So this turns out to be a pretty pathological problem for trying to use this algorithm with real data, even though it's correct in.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Awesome tonically.",
                    "label": 0
                },
                {
                    "sent": "So the other problem with trying to use this approach is that we even if.",
                    "label": 0
                },
                {
                    "sent": "Well, each each data set that we have, it doesn't take advantage of the same variables that are contained in another data set.",
                    "label": 0
                },
                {
                    "sent": "So we're ignoring a lot of useful information if we try and learn from each each individual data set separately.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, this approach just takes a lot of a lot of time and uses a lot of memory, so we'd like to do a little bit better than this approach.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what our approach is, is we're going to rather than learn structures for each of these individual datasets and integrate them, we're just going to try and work directly from the datasets, and we're always whenever we need to find conditional independence information, we're going to use as many of the datasets as possible to do our conditional independence testing.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to think about then how are we going to you do conditional independence testing when we have multiple datasets so.",
                    "label": 0
                },
                {
                    "sent": "One way that we could do this is just try and concatenate the data set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if these are coming from different places as we already mentioned earlier, there's going to be slight differences that may exist even after we standardize our data.",
                    "label": 0
                },
                {
                    "sent": "So this is not going to work so well.",
                    "label": 0
                },
                {
                    "sent": "Another approach.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we looked at trying to do in, not in the context of learning from multiple datasets with different variable sets, but in the context of structure learning was something called pooling P values, and So what we do in this case is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that so?",
                    "label": 0
                },
                {
                    "sent": "We take each data set an we pick pick a particular conditional independence test that would be useful for each of those datasets, and then for the test statistic that's used in that particular conditional independence test.",
                    "label": 1
                },
                {
                    "sent": "We just find the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value that we get for that particular test statistic and then.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using all of these P values that we get from the different test statistics which correspond to different datasets, we can get a new test statistic that corresponds to the distribution of these P values under the null.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we tried several ways of doing this in this paper, and it turns out that one test statistic which recalling the Fisher test statistic, which essentially just turns into a weighted sum of log P values, leads to the best results for doing this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're going to do then, is every time we need to do a conditional independence test, we're going to use in our approach.",
                    "label": 1
                },
                {
                    "sent": "This Fisher test statistic, with each data set that contains the relevant variables.",
                    "label": 1
                },
                {
                    "sent": "So just find every data set that contains the variables that we need in the conditional independence tests.",
                    "label": 0
                },
                {
                    "sent": "Get the P values form the new test statistic, and then we can make an accept or reject precision decision based on.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This this new value.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so now we can get to the basic structure of how the algorithm itself is going to work.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is begin with a complete graph over all of these variables that we're interested in.",
                    "label": 1
                },
                {
                    "sent": "So a fully connected graph over all of this complete variable set V. And then, um.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From that, we're going to do.",
                    "label": 0
                },
                {
                    "sent": "All of the conditional independence tests that this FCI algorithm tells us we need to do but with respect to each variable set that we're looking at in a particular data set.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we can show is that if we follow these steps then this is going to get us a structure that contains a super set of the edges that are contained in the correct bag.",
                    "label": 0
                },
                {
                    "sent": "The bag, which corresponds to the true data generating process and a subset of the immorality Zan discriminating path be structures that are in this.",
                    "label": 0
                },
                {
                    "sent": "In this pack.",
                    "label": 0
                },
                {
                    "sent": "Right so, um, from that I mean what?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically tells us is then we all we need to do is take this structure that we get and find out all the ways that we could remove edges or Adam moralities.",
                    "label": 0
                },
                {
                    "sent": "Ads discriminating path, be structures and still be consistent with the conditional independence information.",
                    "label": 1
                },
                {
                    "sent": "That's true in all of the datasets that we observe.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we just need to find all the ways that we can generate graphs by edge removal and adding in moralities.",
                    "label": 0
                },
                {
                    "sent": "And then once we do that, we just need to apply a final set of orientation rules that gets us apag from that resulting structure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean one.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need some way of deciding what edges we can remove and how we can know whether this is going to still be consistent with all this conditional independence information that we have.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we give one theorem that gives us something we can use to do this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this theorem says is that so let's G star V1V2 dot dot be marginalization's of the correct mag structure after we marginalized.",
                    "label": 0
                },
                {
                    "sent": "Are all the variables that are not contained in V1 and V2 so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then what we can say is G is consistent with all of these graphs.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If there exists a sets Z which is a subset of any one of these particular graphs.",
                    "label": 0
                },
                {
                    "sent": "Such is there such that two variables X&Y are in separated given Z?",
                    "label": 0
                },
                {
                    "sent": "Then then this is satisfied in our graph G. So basically, in other words, if there's any Z which separates two variables X&Y in any any of these marginal graphs, then we have to have that M separation true in R. In the pan.",
                    "label": 0
                },
                {
                    "sent": "The correct rat grab graph G that we're considering the correct mag.",
                    "label": 0
                },
                {
                    "sent": "And then there's finally one more condition.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which is that if we have two variables that are adjacent in any one of these marginal graphs, then any graph G That's Markov equivalent to the correct structure has to contain another type of path, which I'll skip defining but something very simple that we can chat called and inducing path, but basically introducing path just represents sets of.",
                    "label": 0
                },
                {
                    "sent": "So sheated with when sets of M connecting paths exist.",
                    "label": 0
                },
                {
                    "sent": "So whenever we have these two conditions satisfied, then we know that a given mag is going to be consistent with all of our marginal datasets that we observe.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, we know that.",
                    "label": 0
                },
                {
                    "sent": "The conditional independencies that FCI tells us to check gives us a sufficient number of the Z sets to check.",
                    "label": 0
                },
                {
                    "sent": "So basically FCI does all of the independence tests that we need to do to check this theorem here.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore we get from FCI.",
                    "label": 0
                },
                {
                    "sent": "We can use based on the the conditional independence test it chooses to do this set.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's of inducing paths that we need to check.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And using this information we can put it together and decide what edges.",
                    "label": 0
                },
                {
                    "sent": "We can remove and what immorality, Zan discriminating pathway structures we can put into the structure to get a mag that is going to be Markov equivalent to the true data generating process?",
                    "label": 0
                },
                {
                    "sent": "So just to give you an.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of how this works.",
                    "label": 0
                },
                {
                    "sent": "So if this is the true graph right here.",
                    "label": 0
                },
                {
                    "sent": "And we have two datasets V1 and V2.",
                    "label": 0
                },
                {
                    "sent": "The first contains all the variables except V. The second all accepts W.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then in the first stage, we're going to create this complete graph over all these variables, then from.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're going to do the first stages of the FCI rules with each of these datasets and we're going to structure like this.",
                    "label": 0
                },
                {
                    "sent": "An from that we're going.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be able to tell that we can actually generate 2 bags here that are Markov equivalent to the marginalization's with respect to each of these particular datasets.",
                    "label": 0
                },
                {
                    "sent": "So right, so in this case, given that there was some conditional independence information that we did not have access to, we're only able to.",
                    "label": 0
                },
                {
                    "sent": "We weren't able to get a particular marvelous class.",
                    "label": 0
                },
                {
                    "sent": "We were only able to distinguish it to two particular Markov equivalence classes.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are some examples where even though we can observe all the variables, we can actually distinguish it to one particular tag.",
                    "label": 0
                },
                {
                    "sent": "This is one such example.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right, so then we can formally say that using this procedure, if Maggie describes the true data generating mechanism, then in large sample limits.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm is correct.",
                    "label": 0
                },
                {
                    "sent": "In other words, all of the graphs in the output imply the correct are consistent with the data sets we have.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's also complete, and that if there's any any mag or any any pag representing it, that implies all of these conditional in dependencies, then it's going to be included in the output of our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally we evaluated this on a few datasets, so I'll just go through this quickly because running out of time.",
                    "label": 0
                },
                {
                    "sent": "So we try to complete data, complete set of variables at 14 an at 8:00 and then we try to two data set example in a three data set example.",
                    "label": 0
                },
                {
                    "sent": "Here we removed two variables from each data set.",
                    "label": 0
                },
                {
                    "sent": "Here we moved here, removed one here with two.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In each case, the blue line is our algorithm and the green line is.",
                    "label": 0
                },
                {
                    "sent": "The result of running FCI on each data set and then putting it to the ion algorithm and so we do a lot better in terms of precision and recall and also in terms of runtime and memory usage.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the results look about the same for the three data set example.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and finally we applied this to some neuroimaging data that was the riming task that I described earlier, so.",
                    "label": 0
                },
                {
                    "sent": "Basically, each of these variables corresponds to a particular brain region.",
                    "label": 0
                },
                {
                    "sent": "In this case we had 13 datasets, 160 samples.",
                    "label": 1
                },
                {
                    "sent": "Each data set was missing, one to four variables, so this variable represents the input stimulus, and So what we see is a cascade starting on the left side.",
                    "label": 0
                },
                {
                    "sent": "So these are all the left brain regions and then eventually moving to the right side, which is consistent with some domain knowledge we have.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so finally to conclude we.",
                    "label": 0
                },
                {
                    "sent": "So basically we give a correct incomplete algorithm for learning from multiple datasets with overlapping variables.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this we get some results which relate mags to sets of Markov equivalent mags over different variable sets.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we showed how we can incorporate this other conditional independence methods, which takes care of the pathological cases of ion.",
                    "label": 0
                },
                {
                    "sent": "And this leads to much greater act.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or see intractability.",
                    "label": 0
                },
                {
                    "sent": "Right and so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in future work we want to look at.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we can combine this.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Swift datasets where some variables are subject to an intervention also possibly combine this with structured learning methods based on non gaussianity and nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty, and if we can relax the simplicity assumption, that would be good as well, so that concludes.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's talk about Lady authors on their very interesting contribution and also organizes other conference of starting this new tradition here in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So Jason couldn't show up.",
                    "label": 0
                },
                {
                    "sent": "You have to copy him yet again for another talk.",
                    "label": 0
                },
                {
                    "sent": "But then I'm going to be brief, so I'd like to divide.",
                    "label": 0
                },
                {
                    "sent": "The powers are static.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having three main classes, balance of estimation, computation, identification and this paper is very nice because it tackles off three of these issues at the same time.",
                    "label": 1
                },
                {
                    "sent": "It actually actually reminds me of some old paper by Peter spurts on AI stats 10 years ago when this is to call a workshop on artificial intelligence and statistics.",
                    "label": 0
                },
                {
                    "sent": "So his problem that time was outing for this causes structures when we have limited time and limited data so you don't trust higher water independence test, so don't be able to.",
                    "label": 0
                },
                {
                    "sent": "Test them cause ran out of time, so in one sense we had this problem of infrastructure.",
                    "label": 1
                },
                {
                    "sent": "When only a subset of the condition independence could be observed, and this contribution by team in sports can be seen some sort of generalization of that, even though it came from completely different perspective.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, there are some side effects.",
                    "label": 0
                },
                {
                    "sent": "Other formulations are very interesting.",
                    "label": 0
                },
                {
                    "sent": "This is wonderful call to buy power in his textbook and causality on actually how can get extra robustness on friended structures if we have data with slightly different setups.",
                    "label": 0
                },
                {
                    "sent": "Becausw accidental qualitative information such as condition depends go straight constraints might disappear when C based from slightly different perturbed models and this might be already something is.",
                    "label": 0
                },
                {
                    "sent": "Comes for for free.",
                    "label": 0
                },
                {
                    "sent": "From the contribution in sports will be interesting to see on exactly some sort of empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "If you have some extra robustness out of this formulation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Robert talked about this possibility of dealing with selection bias.",
                    "label": 1
                },
                {
                    "sent": "When you have this hidden latent variables that are conditioning on.",
                    "label": 0
                },
                {
                    "sent": "But is a beast and difficult.",
                    "label": 0
                },
                {
                    "sent": "Imagine how this would happen in practice.",
                    "label": 0
                },
                {
                    "sent": "We have independent studies 'cause usually this selection buses don't come by design.",
                    "label": 0
                },
                {
                    "sent": "They happen by chance and you are assuming here the selection biases are shared among different studies, so this might not be very plausable.",
                    "label": 1
                },
                {
                    "sent": "At least is that with difficulty.",
                    "label": 0
                },
                {
                    "sent": "Imagine situations where this would happen.",
                    "label": 0
                },
                {
                    "sent": "So Electro mention also this.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some work there in the second metrics literature on large structural learning models.",
                    "label": 0
                },
                {
                    "sent": "Overlapping variables when sometimes we have tests that give the patient over different, let's say medical centers, and this test overlap only a few subset of variables and people like to fit factor analysis models to this data.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you can infer situations like that covariance between various than ever be served.",
                    "label": 0
                },
                {
                    "sent": "If you're willing to make some extra assumptions about the common structure, they have very interesting to see which kind of relationship.",
                    "label": 0
                },
                {
                    "sent": "These methods have their general peg formulation.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they mention the possibility of exploiting other constraints with saying dependence constraints such as functional constraints that you've seen some approaches for causal inference, for example, exploiting the fact that might have additive errors in your deck structure.",
                    "label": 0
                },
                {
                    "sent": "But questions like that and not really closed under marginalization, so they might pose a problem if some overlap exists where the assumptions in different datasets actually don't match each other.",
                    "label": 0
                },
                {
                    "sent": "Interesting to see how can we actually generalize this overlapping cases when you have this conflicting assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, I just follow up in dates.",
                    "label": 0
                },
                {
                    "sent": "Comments is be be interesting to see.",
                    "label": 0
                },
                {
                    "sent": "How can you do a baseline version of that?",
                    "label": 0
                },
                {
                    "sent": "Actually the problem is even easier when you have different parameter sets 'cause you don't have to worry about time parameters from models that are actually over different variables.",
                    "label": 0
                },
                {
                    "sent": "In this case don't even missing data models Eater because can represent the marginals overdubs variables directly without having explicit latent variables from the point of view of regularization.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you have the link to compose a likelihood, which is also nice in the sense that you don't need to specify a full likelihood for all of your variables?",
                    "label": 0
                },
                {
                    "sent": "Frankfurt standing calls relations only for subset of your variables might want to specify likelihood functions just for some subsets, and letting particle distribution deal of the rest and compose likelihood also have this nice properties that can design A penalty function along with it to be able to search for different structures there.",
                    "label": 0
                },
                {
                    "sent": "And you have this source problems you might.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also want to make the link.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Constrained optimization if we have ice core function like this, which is a composed likelihood function, you might want to enforce effect that over these different subsets have common independent sub model and this common depends.",
                    "label": 0
                },
                {
                    "sent": "Models might be written as constraints as logical constraints in optimization function and the question is how to deal with the actual very large number of these constraints with some cutting plane approach might help here.",
                    "label": 0
                },
                {
                    "sent": "Write constraints as they are needed.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I would like to think again they speakers as you saying their voice statical SoC.",
                    "label": 0
                },
                {
                    "sent": "I'd like to propose a vote of thanks or this paper in a movie I stats.",
                    "label": 0
                }
            ]
        }
    }
}