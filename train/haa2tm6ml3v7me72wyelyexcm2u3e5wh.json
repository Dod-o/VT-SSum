{
    "id": "haa2tm6ml3v7me72wyelyexcm2u3e5wh",
    "title": "Living on the edge: Phase transitions in convex programs with random data",
    "info": {
        "author": [
            "Joel Tropp, Department of Computing and Mathematical Sciences, California Institute of Technology (Caltech)"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Information Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_tropp_phase_transitions/",
    "segmentation": [
        [
            "It's a great pleasure to be here, and I'll be speaking about some joint work that I.",
            "Did with my former student, Michael McCoy, who is now running Kofas it incorporated.",
            "So then assemble Langson, who's just started at the City University of Hong Kong and Martin Law tattoos on the faculty of the University of Manchester.",
            "I'd like to emphasize that these people did almost all of the work that I'm going to be telling you about.",
            "I'm just the Messenger.",
            "I think the.",
            "They really did some great stuff here, but I don't deserve the credit for it so."
        ],
        [
            "Let me start out with something I hope is familiar.",
            "The compressed sensing problem, by which I mean something very specific because I've been doing this since before it was cool.",
            "So turns out sparsity did not start in 2004.",
            "So let's start with an unknown vector X natural.",
            "The natural indicates that this is the ground truth.",
            "It's a D dimensional vector, and we're going to assume that it's sparse.",
            "It is S non zero entries, and we're going to acquire some information about this unknown vector by taking linear measurements.",
            "So in other words, we're going to see an image of this vector by a matrix A.",
            "We will obtain a vector Z which has M entries.",
            "Each of the entries is an independent random linear measurement of X natural, and we're going to assume that these measurements are taken by a Gaussian measurement process, and so something very ideal and our goal is given this underdetermined problem.",
            "The number of measurements is a lot smaller than the ambient dimension.",
            "Our goal is to recover the ground truth X natural, and we're going to do so by solving a convex optimization problem.",
            "So we're going to minimize the.",
            "L1 norm of our decision variable subject to the decision variable producing the same measurements, so the decision variable needs to be consistent with what we have observed, and we're going to minimize the L1 norm to promote sparsity.",
            "So all one is a structure inducing regularizer and we hope that the estimate that we obtained by solving this problem is equal to the ground truth.",
            "OK, so this is an abstract optimization problem.",
            "There are no numerics here and so it is reasonable to talk about the exact equality of these two points.",
            "We hope that this is the unique that X natural is unique minimizer of this problem.",
            "OK, so.",
            "You've probably seen this before."
        ],
        [
            "Or computer experiment to see what happens if we have a sparse unknown and we try and reconstruct it from random measurements.",
            "So on the horizontal axis we have the number of nonzeros in the ground truth, which seems to be denoted by a slightly different variable.",
            "So this is the sparsity level, and on this axis is the number of measurements and the heat map.",
            "Here is the probability of success for a given sparsity lonely given number of measurements.",
            "So black is.",
            "100% failure White's 100% success.",
            "And So what you can see here is that at a given sparsity level, as the number of measurements increases, you fail.",
            "You fail, you fail, you fail, you fail all of a sudden, you start to succeed.",
            "Sometime in the very soon you succeed all of the time.",
            "As soon as you have enough measurements.",
            "And the number of measurements you need depends in a sort of tractive way on the sparsity level.",
            "And so the question is.",
            "How many measurements do you need for a given sparsity?",
            "No, there's something interesting that's happening here, which is that this is a problem in dimension 100, so there's a vector of length 100, so the sparsity clearly ranges from zero to 100, and number of measurements you need certainly is never more than 100.",
            "You need more measurements in the sparsity level, which would be the diagonal line here.",
            "But how many more?",
            "And when you look at the same problem in 600 dimensions with the same scale for the axis, you see the same picture, but now the transition is a little less blurry.",
            "So what happens is that the region over which you transit from.",
            "Failure to success is much narrower than it is here, and you can imagine that as you continue to increase the ambient dimension and repeat this experiment that eventually this will tend to a step function.",
            "OK, and so we call this a phase transition.",
            "It's a sharp change in the behavior of a computational problem as the parameters vary and so the question is what's going on here?",
            "Can you prove there's a phase transition?",
            "How?",
            "Why does the transition region does this up here and other problems?",
            "Can you prove that appears in other problems and so that's the kind of question that I'll be discussing in this lecture?",
            "Are there any questions about this diagram?",
            "OK, so."
        ],
        [
            "So this is a specific example of what you might call a convex optimization problem with random data, so this is a sensing problem.",
            "You collect random measurements of some unknown and you reconstructed by optimization.",
            "There are lots and lots of other examples of convex programs where the data for the program is random.",
            "This happens all the time in statistics and machine learning, where you believe that you've collected.",
            "Some data and you often model it as coming from a probability distribution.",
            "And as Francis just discussed, we often fit a model to that data via optimization.",
            "So the question is what happens when the data is random?",
            "How do these optimization problems behave?",
            "This happens in coding.",
            "You have random models for channel and you use an optimization procedure to decode what has been sent through the channel.",
            "And the motivations that we might pose for doing this are.",
            "Pretty extensive, I'm going to focus on 2, one of which is an average case analysis, so the randomness describes somehow the typical behavior of these optimization problems, and it's really quite appropriate in settings like statistics, machine learning, and sensing where you really do perceive that there's an underlying distribution behind the data that goes into the optimization.",
            "Another reason that this is interesting is that you can think about these as providing fundamental bounds, so these are opportunities for convex optimization methods.",
            "In these areas and limits on the performance of these optimization methods.",
            "This is a sort of a heuristic statement, but the general perception is that with tractable methods in some sense these phase transitions are.",
            "The best you can do.",
            "But this is something that has not been established rigorously."
        ],
        [
            "So there is a research challenge underlying all of this, which is to understand and predict the precise behavior of these random convex programs, and this is.",
            "Set out pretty clearly as a program of research and some of David Donoho's papers."
        ],
        [
            "So there's a theory that has emerged over the last decade that addresses this kind of question of indicated and read the papers that are most relevant to the discussion today.",
            "Emphasize that a lot of people have contributed to this line of work and.",
            "It continued after the research that I'm going to be describing in this talk from my paper living on the edge.",
            "Some of these ideas go all the way back to the literature in discrete geometry and optimization, so version sports have developed an asymptotically estimate for the number of steps in the simplex method with random data, and those were the ideas that ultimately inspired Donoho and Tanner, and in some sense have fed into the work that we're doing here using a more modern formulation of the theory.",
            "OK, so I'm going to provide references to these and more papers at the bottom of my slides, but I'm not going to talk in very much detail about who did what because we don't have a lot of time.",
            "I don't want to emphasize the huge number of people have contributed to this research."
        ],
        [
            "OK, so what's going on in the compressed sensing problem?",
            "I think you've probably seen this picture before too, so when you look at this optimization problem, we're enforcing this consistency condition on the decision variable X that it produced the same measurements as X natural, and what that means is that the decision variables constrained a lie on this line X natural plus the kernel of a, so it's an affine space, and in that I find space we're going to pick out the point that has the smallest L1 norm and.",
            "Geometrically, this means that we're going to inflate the L1 norm until it hits that affine space, and the question is whether it's that I find space at X natural or not.",
            "Now, one way of looking at this is to.",
            "Imagine the set of dissent directions of the L1 norm.",
            "At this point, X natural, and if there is no direction in which you can decrease the L1 norm in X natural and remain in this constraint space, then X natural must be the unique solution of that optimization problem.",
            "Conversely, if there is a direction in which you can decrease the L1 norm while remaining in this constraint space in the next, natural is not the solution to the optimization problem.",
            "So in this case I want imitation succeeds in recovering the ground truth.",
            "In this case I'll one minimization fails to recover the ground truth.",
            "Now we're in a setting where we're assuming that the Matrix A is random, so this Colonel is not just some affine space that's handed to.",
            "You have a statistical model for it, and so when a is Gaussian, the kernel is uniformly distributed on the grass moniem, which means that the space is spinning around at random, and so sometimes it's going to hit this cone of descent directions.",
            "Other times it's not, and so the geometric question you have to answer is what is the probability that a randomly.",
            "Oriented subspace intersects a cone.",
            "And it's not surprising that the core."
        ],
        [
            "Question is.",
            "How big is a cone?",
            "Now when we ask how big are cone as we mean this in a very specific sense, namely, how big is a comb from this geometric respective."
        ],
        [
            "The probability that an affine, their random subspace shares of array with that cone.",
            "And that's the question we need to answer.",
            "Any questions on this?",
            "OK so I'm."
        ],
        [
            "And introduce a quantity called the statistical dimension that helps you answer this question."
        ],
        [
            "And the idea behind the statistical dimension is.",
            "Is geometric is very simple.",
            "So imagine that you've got a cone K. You take a Gaussian vector from the Sky and you project it onto the cone K. So Pi K of G is the closest point in the cone to G with respect to the Euclidean distance.",
            "And because this is Gaussian, you can think about this is spinning around in space, and sometimes the projection is going to be small if G is over.",
            "Here the projection is going to be bigger if G is up here it will project onto the vertex of the cone and you can sort of see intuitively that when the cone is small, it's often the case that the projection of the Gaussian to the cone is a small vector.",
            "So the norm of this projection is small.",
            "On the other hand, if K is a big cone, then the projection.",
            "Of this, Gaussian to the cone is often going to be large.",
            "And.",
            "So you can think about the average size of this projection as a measure of the size of a cone, and it turns out that this is exactly the quantity that you need in this setting."
        ],
        [
            "So we'll define the statistical dimension of a closed convex cone to be the expected squared norm of the projection of a standard normal vector onto the cone as I just illustrated in that last picture.",
            "And.",
            "Again, the intuition here is that when the cone is small, this quantity is small.",
            "When the cone is large, this quantity is big and this really is a measure of dimension, and that if the cone is a subspace, this quantity reproduces the dimension of the subspace just by the marginal property of a Gaussian variable.",
            "The projection of a Gaussian onto a subspace is a Gaussian.",
            "OK, so."
        ],
        [
            "You can make some basic calculations of this quantity, so as I mentioned, for a J dimensional subspaces, statistical dimension is J.",
            "And there are a bunch of other basic sets where you can calculate this exactly the nonnegative orthant has statistics in D dimensions, has statistical dimension 1/2 the ambient dimension, so this is a lot different from the volume of the orthon, which is exponentially small.",
            "It's a very different way to measure size as compared with the volume, and these other calculations are all quite straightforward, and their adapted from this paper by Venkat Chandrasekharan Ben Recht.",
            "But one thing I would like to emphasize that the statistical dimension is always between zero and the ambient dimension, and it is a Canonical extension of the dimension of a subspace to the class of convex cones.",
            "So this really has some ontology as a geometric quantity.",
            "Really quite fundamental now."
        ],
        [
            "You can make these calculations for other things, so a circular cone is the cone generated by a circular cap on the sphere, and as the angle of the cap increases, the statistical dimension increases.",
            "In here are normalized by the ambient dimension."
        ],
        [
            "You can calculate the statistical dimension of a descent cone of a function so the descent cone is a set of directions in which the function decreases if the function is convex and.",
            "It's easy to see that these things are convex cones.",
            "And then the question is, how big is the descent code of a function?",
            "We already saw that this arose in the case of the L1 norm and you can calculate the statistical."
        ],
        [
            "Action of the dissent.",
            "Cones of the L1 norm at sparse vectors.",
            "So on this axis we have the number of nonzeros.",
            "As compared with the ambient dimension.",
            "On this axis we have the statistical dimension divided by the ambient dimension.",
            "You can see that the statistical dimension of the L1 norm increases in this way as the sparsity increases.",
            "You may have seen this curve before.",
            "One of my other slides.",
            "This lower curve is what happens if you add a non negativity constraint to the L1 norm and these calculations are asymptotically sharp, which is a new contribution of the research that we did.",
            "OK, you can do the same thing for the chat and one norm also known as the trace norm of the nuclear norm.",
            "On this axis we have the rank over the small dimension of the matrix.",
            "On this axis we have the statistical dimension divided by the number of entries in the matrix and these different curves are for matrices of different aspect ratios.",
            "So blue is square, red is or an aspect ratio of two to one and the green curve is the aspect ratio of the matrix tending to 0, so very.",
            "Very fat matrices, and so you can calculate exactly the asymptotic statistical dimension of these class of descent cones of the shot and one norm at low rank matrices.",
            "And of course this plays a role and matrix in the analysis of matrix recovery problems.",
            "OK, so the point here is just you can make these calculate."
        ],
        [
            "Now, what does this have to do with the problem we were talking about?",
            "So there's a result that we call the approximate kinematic formula, which says that if you'd like to know the probability that a fixed cone and a randomly rotated cone, think of randomly rotated subspace, have a Ray in common, then you can answer this question completely up to a small error using this statistical dimension.",
            "So if the total dimension of the two cones are smaller than the ambient dimension.",
            "Then it's extremely unlikely that they share array.",
            "If the total dimension of the two cones exceeds the ambient dimension, then it's very likely that they do share array, and this is exactly what happens in the case of two randomly oriented subspaces which you can verify just by making a dimension counting argument.",
            "Since this is a short talk, I don't want to belabor this point, but this is really what drives the theoretical results that I'll show you in a moment."
        ],
        [
            "One quick aside, some you may have heard of the Gaussian width.",
            "It's very closely related to the statistical dimension.",
            "In fact, the square of the width is within one of the statistical dimension.",
            "In some settings, one parameter is more convenient than the other.",
            "The main reason that I like the statistical dimension better is that it has this geometric canonic ality that the width really does not.",
            "And there are all sorts of beautiful identities that the statistical dimension satisfies that only hold approximately true when you move to the width so."
        ],
        [
            "But any questions about this material before I move on?",
            "Alright, so let's come back to the problem.",
            "We started with the compressed sensing problem, but on stairs."
        ],
        [
            "Voids, so now we're going to let X natural be an unknown vector that we're going to assume to be structured.",
            "So structure is whatever you like it to be, and we're going to let F be a convex function that reflects structure, so the function is very small for structured vectors and very large for unstructured vectors.",
            "So think of F is the L1 norm.",
            "In the case of sparsity, if X natural or low rank matrix, you should think about F as the trace norm or the Max norm or something like that.",
            "So this is completely general, though.",
            "We're going to let a be a matrix that Maps vectors in Rd to a set of M measurements where I'm a smaller than D and we're going to try and produce an estimate of the unknown X natural by solving a convex optimization problem subject to the decision variable producing the same measurements that we observed, we're going to minimize the complexity of the decision variable, and our hope is that the estimate we obtained in this fashion.",
            "Equals the ground truth.",
            "OK, so this is a very very general set up there.",
            "No assumptions here essentially, except that this is a convex function.",
            "And so then the question is when does this work?",
            "So to analyze this we're going to."
        ],
        [
            "Look at the geometry again.",
            "Oh, it's the same picture we saw before, except that now instead of the L1 ball we've got sublevel sets of the regularizer and instead of the descent cone of the L1 norm we have descent cones of this function at this ground truth, and the question once again is whether or not the kernel of the.",
            "Measurement operator shares array with the descent cone or not.",
            "We succeed when there's no intersection.",
            "We fail when there is an intersection, and so once again we need to understand which of these settings were in, so we'll introduce randomness so that we can try and understand this problem.",
            "So if A is a Gaussian matrix, then the kernel is a.",
            "Random subspace, and so we'd like the probability that a random subspace intersects.",
            "A convex cone, but this is exactly the kind of problem that the approximate kinematic formula allows you to address, and it gives you a very detailed answer which is in caps."
        ],
        [
            "Later in this theorem.",
            "So X natural is an unknown vector we observe.",
            "Gaussian measurements of X natural and M is the number of measurements we solve.",
            "This convex optimization problem and then the question is.",
            "Did we recover the ground truth or not?",
            "And it turns out that there is a watershed exactly at the statistical dimension of the descent cone of the regularizer at the ground truth.",
            "If the number of measurements exceeds the size of the descent cone, we recover the ground truth.",
            "If the number of measurements is smaller than the statistical dimension of the descent cone, we fail to recover the ground truth, and these are correct up to lower order terms.",
            "So when you look at problems and large mention, you really will see a phase transition at this point.",
            "OK, so the terms of suppressed here have strictly lower order.",
            "Any questions on this theorem?",
            "So it states for.",
            "Any?",
            "Convex reconstruction problem with Gaussian measurements.",
            "There will always exist a phase transition precisely at the statistical dimension.",
            "And that's true for every problem.",
            "There are no assumptions here, OK?"
        ],
        [
            "So let me show you 2 examples.",
            "Here's the picture we saw in the first slide, so sparsity on this axis.",
            "Number of measurements on this axis we're using L1 minimization to recover the unknown Black's failure.",
            "Why to success?",
            "This red curve here is the 50% isocline.",
            "For success computed from the data, the yellow curve underneath that you can barely see is the theoretical calculation.",
            "There's a statistical dimension of the L1.",
            "Norm at its descent cones.",
            "OK, and So what you can see is that these are in perfect agreement of also indicated the 5% and 95% success I so client so you can see the width of the transition region.",
            "You can see that it really is lower order and these will squeeze together as the dimension increases.",
            "So this is 100 dimensional problem and that's why the phase transition still visible.",
            "So these are very short vectors.",
            "Any questions on this?",
            "Experiment.",
            "So as I said, this works for everything, so let's look at something else."
        ],
        [
            "Recovery of low rank matrices from Gaussian measurements via trace norm minimization.",
            "This is the model that Marium Fazel Ben Recht and Pablo Parrilo looked at another Siam review paper.",
            "So on this axis we have the rank of a square matrix is 30 by 30.",
            "On this axis we have the number of measurements where it ranges from zero to 900.",
            "Black is failure, White's success.",
            "Red is the 50% success isocline computed from the data.",
            "Yellow is the statistical dimension curve.",
            "So you can see that we've.",
            "Identify the phase transition exactly using these calculations.",
            "And these are not very big problems.",
            "One point is that the phase transition is narrower here because this is a 900 dimensional problem as opposed to 100 dimensional problem and so you can see that this becomes sharp quite quickly.",
            "Number of measurements from 20 from round 20 onwards so you.",
            "Don't I think it may be a numerical error, or like some imprecision in the plotting.",
            "It may also be that you need a much bigger example before you can start to see the difference up here, but I don't think that this curve hits the axis until the very end, so there may.",
            "I think there may be some plotting issues here.",
            "OK, now let me address a few."
        ],
        [
            "Questions that sometimes come up here, so this applies to every nonsmooth convex regularizer.",
            "This includes L1 with the non negativity constraint sparsity, and group sparsity, total variation, L, Infinity.",
            "For saturated signals, the Max norm tensor norms, it doesn't matter.",
            "This applies to everything that we use as a non smooth regularizer to promote structure.",
            "The only catch is you have to calculate the statistical mention if you want to know where that transition is, and this is not always so easy.",
            "Although we do have some upper estimates in the literature that go back to the work of Mihaylo, Steinreich, Salmonoid Mac and later Venkat Chandrasekharan been wrecked.",
            "So this really is a very versatile tool now."
        ],
        [
            "Sometimes people complain.",
            "Wait a minute.",
            "My measurements aren't Gaussian I I'm using Fourier measurements.",
            "I'm doing something completely different well.",
            "I think that this actually kind of a misleading line of talk, because there's actually a lot of evidence that these phase transitions are universal for a lot of structures.",
            "Assuming that the measurement service centered in isotropic random vectors.",
            "So here's an image from one of Jared Tanners papers with Dave Donoho, where they do compressed sensing with random foray measurements.",
            "Black is the phase transition curve that they.",
            "Computed using somewhat different techniques.",
            "And this is for Gaussian measurements.",
            "The heatmap here is the probability of success with Fourier measurements.",
            "So these aren't Gaussian and yet they agree.",
            "Completely OK, so my argument here is that you shouldn't sweat the exact distribution of the measurement.",
            "You're going to get the same answer in a lot of settings.",
            "We have some theory that's currently in progress that I think will help explain this in some generality.",
            "So."
        ],
        [
            "One of the things that we heard yesterday is that it's actually very important to sample nonuniformly, and this is a significant issue and I actually don't think this model completely addresses this concern, but you can change variables if you have measurements that are not isotropic.",
            "So now if your observations are Gaussian matrix with a.",
            "A linear distortion Sigma.",
            "Then you can work through the same theory, but now what you're going to see is that you need to look at the statistical dimension of the descent cone of the regularizer composed with the inverse of the linear distortion at a distorted version of the signal.",
            "OK, so you can try and get it.",
            "What happens for nonuniform sampling?",
            "You still have to calculate the statistical dimension, and when Sigma is very badly conditioned, it's likely that you're going to start seeing different behavior.",
            "When you start changing out the measurement distributions so I don't think that this is a panacea, but nevertheless I think this theory really does have a very broad scope and describes what you see in a lot of ideal problems, and I think that there's even evidence empirical and some of.",
            "Jaron Daves papers that suggests that this really is descriptive of things that happen and more practical settings, and so these tools really are.",
            "You think of valuable."
        ],
        [
            "Attribution to our toolkit.",
            "Now I'd like to close with a slide that I displayed last summer.",
            "It's odd we have a new graduate program at Caltech and Computing and Mathematical Sciences.",
            "It's basically what this meeting is about.",
            "We have a convergence of disciplines.",
            "Now optimization, algorithms, statistics, signal processing networks and markets, mathematics applications, and there aren't really programs that train students in the background for this.",
            "A lot of the faculty who do these things at Caltech are Co, located on computer science and electrical engineering, and so we started a program to try to teach students this new core so that they can make an impact in a variety of different areas and so.",
            "We would love to have your best undergraduate supply.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a great pleasure to be here, and I'll be speaking about some joint work that I.",
                    "label": 0
                },
                {
                    "sent": "Did with my former student, Michael McCoy, who is now running Kofas it incorporated.",
                    "label": 0
                },
                {
                    "sent": "So then assemble Langson, who's just started at the City University of Hong Kong and Martin Law tattoos on the faculty of the University of Manchester.",
                    "label": 1
                },
                {
                    "sent": "I'd like to emphasize that these people did almost all of the work that I'm going to be telling you about.",
                    "label": 0
                },
                {
                    "sent": "I'm just the Messenger.",
                    "label": 0
                },
                {
                    "sent": "I think the.",
                    "label": 0
                },
                {
                    "sent": "They really did some great stuff here, but I don't deserve the credit for it so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start out with something I hope is familiar.",
                    "label": 0
                },
                {
                    "sent": "The compressed sensing problem, by which I mean something very specific because I've been doing this since before it was cool.",
                    "label": 1
                },
                {
                    "sent": "So turns out sparsity did not start in 2004.",
                    "label": 1
                },
                {
                    "sent": "So let's start with an unknown vector X natural.",
                    "label": 0
                },
                {
                    "sent": "The natural indicates that this is the ground truth.",
                    "label": 0
                },
                {
                    "sent": "It's a D dimensional vector, and we're going to assume that it's sparse.",
                    "label": 0
                },
                {
                    "sent": "It is S non zero entries, and we're going to acquire some information about this unknown vector by taking linear measurements.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we're going to see an image of this vector by a matrix A.",
                    "label": 1
                },
                {
                    "sent": "We will obtain a vector Z which has M entries.",
                    "label": 0
                },
                {
                    "sent": "Each of the entries is an independent random linear measurement of X natural, and we're going to assume that these measurements are taken by a Gaussian measurement process, and so something very ideal and our goal is given this underdetermined problem.",
                    "label": 0
                },
                {
                    "sent": "The number of measurements is a lot smaller than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to recover the ground truth X natural, and we're going to do so by solving a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we're going to minimize the.",
                    "label": 0
                },
                {
                    "sent": "L1 norm of our decision variable subject to the decision variable producing the same measurements, so the decision variable needs to be consistent with what we have observed, and we're going to minimize the L1 norm to promote sparsity.",
                    "label": 0
                },
                {
                    "sent": "So all one is a structure inducing regularizer and we hope that the estimate that we obtained by solving this problem is equal to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an abstract optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There are no numerics here and so it is reasonable to talk about the exact equality of these two points.",
                    "label": 0
                },
                {
                    "sent": "We hope that this is the unique that X natural is unique minimizer of this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You've probably seen this before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or computer experiment to see what happens if we have a sparse unknown and we try and reconstruct it from random measurements.",
                    "label": 0
                },
                {
                    "sent": "So on the horizontal axis we have the number of nonzeros in the ground truth, which seems to be denoted by a slightly different variable.",
                    "label": 0
                },
                {
                    "sent": "So this is the sparsity level, and on this axis is the number of measurements and the heat map.",
                    "label": 0
                },
                {
                    "sent": "Here is the probability of success for a given sparsity lonely given number of measurements.",
                    "label": 0
                },
                {
                    "sent": "So black is.",
                    "label": 0
                },
                {
                    "sent": "100% failure White's 100% success.",
                    "label": 0
                },
                {
                    "sent": "And So what you can see here is that at a given sparsity level, as the number of measurements increases, you fail.",
                    "label": 0
                },
                {
                    "sent": "You fail, you fail, you fail, you fail all of a sudden, you start to succeed.",
                    "label": 0
                },
                {
                    "sent": "Sometime in the very soon you succeed all of the time.",
                    "label": 0
                },
                {
                    "sent": "As soon as you have enough measurements.",
                    "label": 0
                },
                {
                    "sent": "And the number of measurements you need depends in a sort of tractive way on the sparsity level.",
                    "label": 0
                },
                {
                    "sent": "And so the question is.",
                    "label": 0
                },
                {
                    "sent": "How many measurements do you need for a given sparsity?",
                    "label": 0
                },
                {
                    "sent": "No, there's something interesting that's happening here, which is that this is a problem in dimension 100, so there's a vector of length 100, so the sparsity clearly ranges from zero to 100, and number of measurements you need certainly is never more than 100.",
                    "label": 0
                },
                {
                    "sent": "You need more measurements in the sparsity level, which would be the diagonal line here.",
                    "label": 0
                },
                {
                    "sent": "But how many more?",
                    "label": 0
                },
                {
                    "sent": "And when you look at the same problem in 600 dimensions with the same scale for the axis, you see the same picture, but now the transition is a little less blurry.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that the region over which you transit from.",
                    "label": 0
                },
                {
                    "sent": "Failure to success is much narrower than it is here, and you can imagine that as you continue to increase the ambient dimension and repeat this experiment that eventually this will tend to a step function.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we call this a phase transition.",
                    "label": 0
                },
                {
                    "sent": "It's a sharp change in the behavior of a computational problem as the parameters vary and so the question is what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Can you prove there's a phase transition?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Why does the transition region does this up here and other problems?",
                    "label": 0
                },
                {
                    "sent": "Can you prove that appears in other problems and so that's the kind of question that I'll be discussing in this lecture?",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about this diagram?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a specific example of what you might call a convex optimization problem with random data, so this is a sensing problem.",
                    "label": 0
                },
                {
                    "sent": "You collect random measurements of some unknown and you reconstructed by optimization.",
                    "label": 0
                },
                {
                    "sent": "There are lots and lots of other examples of convex programs where the data for the program is random.",
                    "label": 0
                },
                {
                    "sent": "This happens all the time in statistics and machine learning, where you believe that you've collected.",
                    "label": 0
                },
                {
                    "sent": "Some data and you often model it as coming from a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And as Francis just discussed, we often fit a model to that data via optimization.",
                    "label": 0
                },
                {
                    "sent": "So the question is what happens when the data is random?",
                    "label": 0
                },
                {
                    "sent": "How do these optimization problems behave?",
                    "label": 0
                },
                {
                    "sent": "This happens in coding.",
                    "label": 0
                },
                {
                    "sent": "You have random models for channel and you use an optimization procedure to decode what has been sent through the channel.",
                    "label": 0
                },
                {
                    "sent": "And the motivations that we might pose for doing this are.",
                    "label": 0
                },
                {
                    "sent": "Pretty extensive, I'm going to focus on 2, one of which is an average case analysis, so the randomness describes somehow the typical behavior of these optimization problems, and it's really quite appropriate in settings like statistics, machine learning, and sensing where you really do perceive that there's an underlying distribution behind the data that goes into the optimization.",
                    "label": 0
                },
                {
                    "sent": "Another reason that this is interesting is that you can think about these as providing fundamental bounds, so these are opportunities for convex optimization methods.",
                    "label": 0
                },
                {
                    "sent": "In these areas and limits on the performance of these optimization methods.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of a heuristic statement, but the general perception is that with tractable methods in some sense these phase transitions are.",
                    "label": 0
                },
                {
                    "sent": "The best you can do.",
                    "label": 0
                },
                {
                    "sent": "But this is something that has not been established rigorously.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is a research challenge underlying all of this, which is to understand and predict the precise behavior of these random convex programs, and this is.",
                    "label": 0
                },
                {
                    "sent": "Set out pretty clearly as a program of research and some of David Donoho's papers.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a theory that has emerged over the last decade that addresses this kind of question of indicated and read the papers that are most relevant to the discussion today.",
                    "label": 0
                },
                {
                    "sent": "Emphasize that a lot of people have contributed to this line of work and.",
                    "label": 0
                },
                {
                    "sent": "It continued after the research that I'm going to be describing in this talk from my paper living on the edge.",
                    "label": 1
                },
                {
                    "sent": "Some of these ideas go all the way back to the literature in discrete geometry and optimization, so version sports have developed an asymptotically estimate for the number of steps in the simplex method with random data, and those were the ideas that ultimately inspired Donoho and Tanner, and in some sense have fed into the work that we're doing here using a more modern formulation of the theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to provide references to these and more papers at the bottom of my slides, but I'm not going to talk in very much detail about who did what because we don't have a lot of time.",
                    "label": 0
                },
                {
                    "sent": "I don't want to emphasize the huge number of people have contributed to this research.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's going on in the compressed sensing problem?",
                    "label": 0
                },
                {
                    "sent": "I think you've probably seen this picture before too, so when you look at this optimization problem, we're enforcing this consistency condition on the decision variable X that it produced the same measurements as X natural, and what that means is that the decision variables constrained a lie on this line X natural plus the kernel of a, so it's an affine space, and in that I find space we're going to pick out the point that has the smallest L1 norm and.",
                    "label": 0
                },
                {
                    "sent": "Geometrically, this means that we're going to inflate the L1 norm until it hits that affine space, and the question is whether it's that I find space at X natural or not.",
                    "label": 0
                },
                {
                    "sent": "Now, one way of looking at this is to.",
                    "label": 0
                },
                {
                    "sent": "Imagine the set of dissent directions of the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "At this point, X natural, and if there is no direction in which you can decrease the L1 norm in X natural and remain in this constraint space, then X natural must be the unique solution of that optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Conversely, if there is a direction in which you can decrease the L1 norm while remaining in this constraint space in the next, natural is not the solution to the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case I want imitation succeeds in recovering the ground truth.",
                    "label": 0
                },
                {
                    "sent": "In this case I'll one minimization fails to recover the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Now we're in a setting where we're assuming that the Matrix A is random, so this Colonel is not just some affine space that's handed to.",
                    "label": 0
                },
                {
                    "sent": "You have a statistical model for it, and so when a is Gaussian, the kernel is uniformly distributed on the grass moniem, which means that the space is spinning around at random, and so sometimes it's going to hit this cone of descent directions.",
                    "label": 0
                },
                {
                    "sent": "Other times it's not, and so the geometric question you have to answer is what is the probability that a randomly.",
                    "label": 0
                },
                {
                    "sent": "Oriented subspace intersects a cone.",
                    "label": 0
                },
                {
                    "sent": "And it's not surprising that the core.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question is.",
                    "label": 0
                },
                {
                    "sent": "How big is a cone?",
                    "label": 0
                },
                {
                    "sent": "Now when we ask how big are cone as we mean this in a very specific sense, namely, how big is a comb from this geometric respective.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The probability that an affine, their random subspace shares of array with that cone.",
                    "label": 0
                },
                {
                    "sent": "And that's the question we need to answer.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this?",
                    "label": 0
                },
                {
                    "sent": "OK so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And introduce a quantity called the statistical dimension that helps you answer this question.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the idea behind the statistical dimension is.",
                    "label": 1
                },
                {
                    "sent": "Is geometric is very simple.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you've got a cone K. You take a Gaussian vector from the Sky and you project it onto the cone K. So Pi K of G is the closest point in the cone to G with respect to the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "And because this is Gaussian, you can think about this is spinning around in space, and sometimes the projection is going to be small if G is over.",
                    "label": 0
                },
                {
                    "sent": "Here the projection is going to be bigger if G is up here it will project onto the vertex of the cone and you can sort of see intuitively that when the cone is small, it's often the case that the projection of the Gaussian to the cone is a small vector.",
                    "label": 0
                },
                {
                    "sent": "So the norm of this projection is small.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, if K is a big cone, then the projection.",
                    "label": 0
                },
                {
                    "sent": "Of this, Gaussian to the cone is often going to be large.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So you can think about the average size of this projection as a measure of the size of a cone, and it turns out that this is exactly the quantity that you need in this setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'll define the statistical dimension of a closed convex cone to be the expected squared norm of the projection of a standard normal vector onto the cone as I just illustrated in that last picture.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Again, the intuition here is that when the cone is small, this quantity is small.",
                    "label": 0
                },
                {
                    "sent": "When the cone is large, this quantity is big and this really is a measure of dimension, and that if the cone is a subspace, this quantity reproduces the dimension of the subspace just by the marginal property of a Gaussian variable.",
                    "label": 0
                },
                {
                    "sent": "The projection of a Gaussian onto a subspace is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can make some basic calculations of this quantity, so as I mentioned, for a J dimensional subspaces, statistical dimension is J.",
                    "label": 0
                },
                {
                    "sent": "And there are a bunch of other basic sets where you can calculate this exactly the nonnegative orthant has statistics in D dimensions, has statistical dimension 1/2 the ambient dimension, so this is a lot different from the volume of the orthon, which is exponentially small.",
                    "label": 0
                },
                {
                    "sent": "It's a very different way to measure size as compared with the volume, and these other calculations are all quite straightforward, and their adapted from this paper by Venkat Chandrasekharan Ben Recht.",
                    "label": 0
                },
                {
                    "sent": "But one thing I would like to emphasize that the statistical dimension is always between zero and the ambient dimension, and it is a Canonical extension of the dimension of a subspace to the class of convex cones.",
                    "label": 1
                },
                {
                    "sent": "So this really has some ontology as a geometric quantity.",
                    "label": 0
                },
                {
                    "sent": "Really quite fundamental now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can make these calculations for other things, so a circular cone is the cone generated by a circular cap on the sphere, and as the angle of the cap increases, the statistical dimension increases.",
                    "label": 0
                },
                {
                    "sent": "In here are normalized by the ambient dimension.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can calculate the statistical dimension of a descent cone of a function so the descent cone is a set of directions in which the function decreases if the function is convex and.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see that these things are convex cones.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, how big is the descent code of a function?",
                    "label": 0
                },
                {
                    "sent": "We already saw that this arose in the case of the L1 norm and you can calculate the statistical.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action of the dissent.",
                    "label": 0
                },
                {
                    "sent": "Cones of the L1 norm at sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have the number of nonzeros.",
                    "label": 0
                },
                {
                    "sent": "As compared with the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the statistical dimension divided by the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "You can see that the statistical dimension of the L1 norm increases in this way as the sparsity increases.",
                    "label": 0
                },
                {
                    "sent": "You may have seen this curve before.",
                    "label": 0
                },
                {
                    "sent": "One of my other slides.",
                    "label": 0
                },
                {
                    "sent": "This lower curve is what happens if you add a non negativity constraint to the L1 norm and these calculations are asymptotically sharp, which is a new contribution of the research that we did.",
                    "label": 0
                },
                {
                    "sent": "OK, you can do the same thing for the chat and one norm also known as the trace norm of the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the rank over the small dimension of the matrix.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the statistical dimension divided by the number of entries in the matrix and these different curves are for matrices of different aspect ratios.",
                    "label": 0
                },
                {
                    "sent": "So blue is square, red is or an aspect ratio of two to one and the green curve is the aspect ratio of the matrix tending to 0, so very.",
                    "label": 0
                },
                {
                    "sent": "Very fat matrices, and so you can calculate exactly the asymptotic statistical dimension of these class of descent cones of the shot and one norm at low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "And of course this plays a role and matrix in the analysis of matrix recovery problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so the point here is just you can make these calculate.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, what does this have to do with the problem we were talking about?",
                    "label": 0
                },
                {
                    "sent": "So there's a result that we call the approximate kinematic formula, which says that if you'd like to know the probability that a fixed cone and a randomly rotated cone, think of randomly rotated subspace, have a Ray in common, then you can answer this question completely up to a small error using this statistical dimension.",
                    "label": 0
                },
                {
                    "sent": "So if the total dimension of the two cones are smaller than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "Then it's extremely unlikely that they share array.",
                    "label": 0
                },
                {
                    "sent": "If the total dimension of the two cones exceeds the ambient dimension, then it's very likely that they do share array, and this is exactly what happens in the case of two randomly oriented subspaces which you can verify just by making a dimension counting argument.",
                    "label": 0
                },
                {
                    "sent": "Since this is a short talk, I don't want to belabor this point, but this is really what drives the theoretical results that I'll show you in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One quick aside, some you may have heard of the Gaussian width.",
                    "label": 0
                },
                {
                    "sent": "It's very closely related to the statistical dimension.",
                    "label": 0
                },
                {
                    "sent": "In fact, the square of the width is within one of the statistical dimension.",
                    "label": 0
                },
                {
                    "sent": "In some settings, one parameter is more convenient than the other.",
                    "label": 0
                },
                {
                    "sent": "The main reason that I like the statistical dimension better is that it has this geometric canonic ality that the width really does not.",
                    "label": 0
                },
                {
                    "sent": "And there are all sorts of beautiful identities that the statistical dimension satisfies that only hold approximately true when you move to the width so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But any questions about this material before I move on?",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's come back to the problem.",
                    "label": 0
                },
                {
                    "sent": "We started with the compressed sensing problem, but on stairs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Voids, so now we're going to let X natural be an unknown vector that we're going to assume to be structured.",
                    "label": 0
                },
                {
                    "sent": "So structure is whatever you like it to be, and we're going to let F be a convex function that reflects structure, so the function is very small for structured vectors and very large for unstructured vectors.",
                    "label": 0
                },
                {
                    "sent": "So think of F is the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "In the case of sparsity, if X natural or low rank matrix, you should think about F as the trace norm or the Max norm or something like that.",
                    "label": 0
                },
                {
                    "sent": "So this is completely general, though.",
                    "label": 0
                },
                {
                    "sent": "We're going to let a be a matrix that Maps vectors in Rd to a set of M measurements where I'm a smaller than D and we're going to try and produce an estimate of the unknown X natural by solving a convex optimization problem subject to the decision variable producing the same measurements that we observed, we're going to minimize the complexity of the decision variable, and our hope is that the estimate we obtained in this fashion.",
                    "label": 0
                },
                {
                    "sent": "Equals the ground truth.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very very general set up there.",
                    "label": 0
                },
                {
                    "sent": "No assumptions here essentially, except that this is a convex function.",
                    "label": 0
                },
                {
                    "sent": "And so then the question is when does this work?",
                    "label": 0
                },
                {
                    "sent": "So to analyze this we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the geometry again.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's the same picture we saw before, except that now instead of the L1 ball we've got sublevel sets of the regularizer and instead of the descent cone of the L1 norm we have descent cones of this function at this ground truth, and the question once again is whether or not the kernel of the.",
                    "label": 0
                },
                {
                    "sent": "Measurement operator shares array with the descent cone or not.",
                    "label": 0
                },
                {
                    "sent": "We succeed when there's no intersection.",
                    "label": 0
                },
                {
                    "sent": "We fail when there is an intersection, and so once again we need to understand which of these settings were in, so we'll introduce randomness so that we can try and understand this problem.",
                    "label": 0
                },
                {
                    "sent": "So if A is a Gaussian matrix, then the kernel is a.",
                    "label": 0
                },
                {
                    "sent": "Random subspace, and so we'd like the probability that a random subspace intersects.",
                    "label": 0
                },
                {
                    "sent": "A convex cone, but this is exactly the kind of problem that the approximate kinematic formula allows you to address, and it gives you a very detailed answer which is in caps.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later in this theorem.",
                    "label": 0
                },
                {
                    "sent": "So X natural is an unknown vector we observe.",
                    "label": 0
                },
                {
                    "sent": "Gaussian measurements of X natural and M is the number of measurements we solve.",
                    "label": 0
                },
                {
                    "sent": "This convex optimization problem and then the question is.",
                    "label": 0
                },
                {
                    "sent": "Did we recover the ground truth or not?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that there is a watershed exactly at the statistical dimension of the descent cone of the regularizer at the ground truth.",
                    "label": 0
                },
                {
                    "sent": "If the number of measurements exceeds the size of the descent cone, we recover the ground truth.",
                    "label": 0
                },
                {
                    "sent": "If the number of measurements is smaller than the statistical dimension of the descent cone, we fail to recover the ground truth, and these are correct up to lower order terms.",
                    "label": 0
                },
                {
                    "sent": "So when you look at problems and large mention, you really will see a phase transition at this point.",
                    "label": 0
                },
                {
                    "sent": "OK, so the terms of suppressed here have strictly lower order.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this theorem?",
                    "label": 0
                },
                {
                    "sent": "So it states for.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "Convex reconstruction problem with Gaussian measurements.",
                    "label": 0
                },
                {
                    "sent": "There will always exist a phase transition precisely at the statistical dimension.",
                    "label": 0
                },
                {
                    "sent": "And that's true for every problem.",
                    "label": 0
                },
                {
                    "sent": "There are no assumptions here, OK?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you 2 examples.",
                    "label": 0
                },
                {
                    "sent": "Here's the picture we saw in the first slide, so sparsity on this axis.",
                    "label": 0
                },
                {
                    "sent": "Number of measurements on this axis we're using L1 minimization to recover the unknown Black's failure.",
                    "label": 0
                },
                {
                    "sent": "Why to success?",
                    "label": 0
                },
                {
                    "sent": "This red curve here is the 50% isocline.",
                    "label": 0
                },
                {
                    "sent": "For success computed from the data, the yellow curve underneath that you can barely see is the theoretical calculation.",
                    "label": 0
                },
                {
                    "sent": "There's a statistical dimension of the L1.",
                    "label": 0
                },
                {
                    "sent": "Norm at its descent cones.",
                    "label": 0
                },
                {
                    "sent": "OK, and So what you can see is that these are in perfect agreement of also indicated the 5% and 95% success I so client so you can see the width of the transition region.",
                    "label": 0
                },
                {
                    "sent": "You can see that it really is lower order and these will squeeze together as the dimension increases.",
                    "label": 0
                },
                {
                    "sent": "So this is 100 dimensional problem and that's why the phase transition still visible.",
                    "label": 0
                },
                {
                    "sent": "So these are very short vectors.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this?",
                    "label": 0
                },
                {
                    "sent": "Experiment.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this works for everything, so let's look at something else.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recovery of low rank matrices from Gaussian measurements via trace norm minimization.",
                    "label": 0
                },
                {
                    "sent": "This is the model that Marium Fazel Ben Recht and Pablo Parrilo looked at another Siam review paper.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have the rank of a square matrix is 30 by 30.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the number of measurements where it ranges from zero to 900.",
                    "label": 0
                },
                {
                    "sent": "Black is failure, White's success.",
                    "label": 0
                },
                {
                    "sent": "Red is the 50% success isocline computed from the data.",
                    "label": 0
                },
                {
                    "sent": "Yellow is the statistical dimension curve.",
                    "label": 0
                },
                {
                    "sent": "So you can see that we've.",
                    "label": 0
                },
                {
                    "sent": "Identify the phase transition exactly using these calculations.",
                    "label": 0
                },
                {
                    "sent": "And these are not very big problems.",
                    "label": 0
                },
                {
                    "sent": "One point is that the phase transition is narrower here because this is a 900 dimensional problem as opposed to 100 dimensional problem and so you can see that this becomes sharp quite quickly.",
                    "label": 0
                },
                {
                    "sent": "Number of measurements from 20 from round 20 onwards so you.",
                    "label": 0
                },
                {
                    "sent": "Don't I think it may be a numerical error, or like some imprecision in the plotting.",
                    "label": 0
                },
                {
                    "sent": "It may also be that you need a much bigger example before you can start to see the difference up here, but I don't think that this curve hits the axis until the very end, so there may.",
                    "label": 0
                },
                {
                    "sent": "I think there may be some plotting issues here.",
                    "label": 0
                },
                {
                    "sent": "OK, now let me address a few.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions that sometimes come up here, so this applies to every nonsmooth convex regularizer.",
                    "label": 0
                },
                {
                    "sent": "This includes L1 with the non negativity constraint sparsity, and group sparsity, total variation, L, Infinity.",
                    "label": 0
                },
                {
                    "sent": "For saturated signals, the Max norm tensor norms, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "This applies to everything that we use as a non smooth regularizer to promote structure.",
                    "label": 0
                },
                {
                    "sent": "The only catch is you have to calculate the statistical mention if you want to know where that transition is, and this is not always so easy.",
                    "label": 0
                },
                {
                    "sent": "Although we do have some upper estimates in the literature that go back to the work of Mihaylo, Steinreich, Salmonoid Mac and later Venkat Chandrasekharan been wrecked.",
                    "label": 0
                },
                {
                    "sent": "So this really is a very versatile tool now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes people complain.",
                    "label": 0
                },
                {
                    "sent": "Wait a minute.",
                    "label": 0
                },
                {
                    "sent": "My measurements aren't Gaussian I I'm using Fourier measurements.",
                    "label": 0
                },
                {
                    "sent": "I'm doing something completely different well.",
                    "label": 0
                },
                {
                    "sent": "I think that this actually kind of a misleading line of talk, because there's actually a lot of evidence that these phase transitions are universal for a lot of structures.",
                    "label": 0
                },
                {
                    "sent": "Assuming that the measurement service centered in isotropic random vectors.",
                    "label": 0
                },
                {
                    "sent": "So here's an image from one of Jared Tanners papers with Dave Donoho, where they do compressed sensing with random foray measurements.",
                    "label": 0
                },
                {
                    "sent": "Black is the phase transition curve that they.",
                    "label": 0
                },
                {
                    "sent": "Computed using somewhat different techniques.",
                    "label": 0
                },
                {
                    "sent": "And this is for Gaussian measurements.",
                    "label": 0
                },
                {
                    "sent": "The heatmap here is the probability of success with Fourier measurements.",
                    "label": 0
                },
                {
                    "sent": "So these aren't Gaussian and yet they agree.",
                    "label": 0
                },
                {
                    "sent": "Completely OK, so my argument here is that you shouldn't sweat the exact distribution of the measurement.",
                    "label": 0
                },
                {
                    "sent": "You're going to get the same answer in a lot of settings.",
                    "label": 0
                },
                {
                    "sent": "We have some theory that's currently in progress that I think will help explain this in some generality.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the things that we heard yesterday is that it's actually very important to sample nonuniformly, and this is a significant issue and I actually don't think this model completely addresses this concern, but you can change variables if you have measurements that are not isotropic.",
                    "label": 0
                },
                {
                    "sent": "So now if your observations are Gaussian matrix with a.",
                    "label": 0
                },
                {
                    "sent": "A linear distortion Sigma.",
                    "label": 0
                },
                {
                    "sent": "Then you can work through the same theory, but now what you're going to see is that you need to look at the statistical dimension of the descent cone of the regularizer composed with the inverse of the linear distortion at a distorted version of the signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can try and get it.",
                    "label": 0
                },
                {
                    "sent": "What happens for nonuniform sampling?",
                    "label": 0
                },
                {
                    "sent": "You still have to calculate the statistical dimension, and when Sigma is very badly conditioned, it's likely that you're going to start seeing different behavior.",
                    "label": 0
                },
                {
                    "sent": "When you start changing out the measurement distributions so I don't think that this is a panacea, but nevertheless I think this theory really does have a very broad scope and describes what you see in a lot of ideal problems, and I think that there's even evidence empirical and some of.",
                    "label": 0
                },
                {
                    "sent": "Jaron Daves papers that suggests that this really is descriptive of things that happen and more practical settings, and so these tools really are.",
                    "label": 0
                },
                {
                    "sent": "You think of valuable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attribution to our toolkit.",
                    "label": 0
                },
                {
                    "sent": "Now I'd like to close with a slide that I displayed last summer.",
                    "label": 0
                },
                {
                    "sent": "It's odd we have a new graduate program at Caltech and Computing and Mathematical Sciences.",
                    "label": 0
                },
                {
                    "sent": "It's basically what this meeting is about.",
                    "label": 0
                },
                {
                    "sent": "We have a convergence of disciplines.",
                    "label": 0
                },
                {
                    "sent": "Now optimization, algorithms, statistics, signal processing networks and markets, mathematics applications, and there aren't really programs that train students in the background for this.",
                    "label": 0
                },
                {
                    "sent": "A lot of the faculty who do these things at Caltech are Co, located on computer science and electrical engineering, and so we started a program to try to teach students this new core so that they can make an impact in a variety of different areas and so.",
                    "label": 0
                },
                {
                    "sent": "We would love to have your best undergraduate supply.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}