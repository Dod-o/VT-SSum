{
    "id": "534exlkvufadwrfucyo6dkqcbpw7zr4h",
    "title": "Multi-sensor System for Driver\u2019s Hand-Gesture Recognition",
    "info": {
        "presenter": [
            "Pavlo Molchanov, Department of Signal Processing, Tampere University of Technology"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis",
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_molchanov_hand_gesture/",
    "segmentation": [
        [
            "My name is Pablo most channel and today I'm going to show you our preliminary results in drivers can gesture recognition using Multisensor system.",
            "As this work we did together with Shalini, Kevin and Carrie, we are all from NVIDIA and.",
            "Yeah, let's talk."
        ],
        [
            "So our main motivation for this work comes from the fact that when we want to use some multimedia devices in the car, we need to put our gaze away from the road and in order to focus and find them, which might be dangerous, right, and?"
        ],
        [
            "That's why touchless interfaces there are becoming much more popular.",
            "And because when we use them, we don't need to look out of the road, we can still focus there and then perform gestures in inner space, and one of the forms of such interactions we consider as a dynamic gestures."
        ],
        [
            "And when we talk about.",
            "Touchless interfaces and another gesture recognition.",
            "I want to mention these two works as the first work was done by Natalia never over and it was charged on challenge from which we heard during the previous talk where we had the RGB data plus upper body skeleton and the best method which won the competition was based on deep neural networks and this gives us motivation that deep neural network and can perform good for different gesture recognition tasks.",
            "Also another broker about the car gesture recognition.",
            "We should mention the work published last year in Intelligent Transportation Systems where the authors of invariant Red is a check.",
            "Different feature extraction methods from the special temporal volume for data captured with RGB did data and they found that Hawk plus Hawk squared features together with SVM classifiers.",
            "That gives the best.",
            "Performance for drivers.",
            "Congestion recognition and we want to use this work as a baseline to compare with.",
            "His only drawback was that the authors checks the data which they can capture during the day and evening, whereas in the car we can also perform gesture during the night or during other extreme variations of the lighting."
        ],
        [
            "So the general overview of our scheme for gesture recognition in the car as shown on this slide, the ideas that we have.",
            "This multi sensor system, shown here as a prototype.",
            "Then the driver performs a gesture as a gesture is sensed by three sensors.",
            "In our case it is optical sensor plus time of flight sensor which are both we get from softkinetic disease with 125 sensor plus we use the reader which is our own.",
            "Invention and then we when we capture data from all the stress three sensors, we feed it to the deep neural network, which gives us a prediction for the particular gesture which driver performs.",
            "Now the main challenge for the drivers can gesture recognition is I said before extreme variation of the in the lighting and which actually cause that some sensors may fail under different lighting conditions."
        ],
        [
            "So that's why we want to consider this multi sensor system like for example color camera can provide very good image during the day.",
            "For the gesture recognition.",
            "But during the night it fails because of the lack of illumination, right?",
            "R. The same goes for.",
            "Commodity depth sensors.",
            "They work very.",
            "They work perfectly indoor.",
            "We get a very nice depth image of the hand, but outside when we have the straight sunlight comes which is coming to the object.",
            "This object will be saturated or we will have a big gap in the data.",
            "So we need to use multiple sensors in order to overcome the lock or features a sensor.",
            "Anne."
        ],
        [
            "We also use the radar sensor which comes from the following motivations.",
            "The first motivations that rather works with all lighting conditions.",
            "The second motivation is that it has no interference with the Sun, which actually comes from the fact that rather uses.",
            "A lower frequencies and.",
            "I'm a flight sensors and plus because of the Doppler effect, we get information about local velocities which happen in the.",
            "Seen, and this is actually very important because we want to classify the dynamic gestures.",
            "Where are some velocities for sure as there so we want to send them with the reader, and in this work which I present today, we consider mainly only the sort advantage.",
            "So we want to only check that how velocities can improve our result."
        ],
        [
            "On this slide I show our rather system which we built because there is no commercially available sensors for which we needed, so we had to build 1 and it uses frequency modulation continuous wave architecture, which is highly power efficient with the central frequency of 24 here hurts.",
            "And if you want to know more information about it, I can provide it after the talk, because we have a separate paper which in details explains as a Raider prototype.",
            "But what does it measure?",
            "We can.",
            "Emphasize the sense that we measure the range with a fixed resolution of four centimeters.",
            "We also measure the velocity with within the uniform grid and very high resolution, about four centimeters per second.",
            "Plus because of the structure, because we have three receivers.",
            "We can use the pairs of receivers to get the angle, so we kind of get the azimuth and elevation for for the object and then we can of course move it to some occlusion space."
        ],
        [
            "And the way we use the weather, it will be shown on this slide.",
            "But I want to mention in the beginning that the main way how's the radar sensors the world.",
            "It will be the range Doppler image, so it's kind of the distribution joint distribution in the range and velocity of all of the objects which appear in front of the reader and.",
            "Because this range Doppler images for all of these receivers, but their amplitude will look extremely the very similar, but the face will be different because of the different position of the receivers and we can use this phase difference to get the special location of the objects which are moving.",
            "And now if we will blend it to the data we get from the depth sensor, the reader will show us a number of points where for issues a point we have a four dimensional vector.",
            "It will be 3D.",
            "Position Plaza radial velocity.",
            "So once once we have these points we projected to the dimensional plane and we want to extrapolate this velocities for the whole frame to everywhere where it happens.",
            "We do this through the warranty diagram.",
            "Then we use the mask which we get from the depth sensor of the hand and just simply by multiplying them we get velocity layer which will show us information about instantaneous velocities and out from only this picture we can see that there is a hand which we most likely will do a close gesture because of the velocities we have over here."
        ],
        [
            "And now I want to show a small demo of how the reader works in order to give you a better idea.",
            "So on the bottom you will see the range Doppler image with the detected objects, which we highlight by the rectangle and then you will have velocity image on the top right when the color image.",
            "So when I start to move the hand we we can see that it position changes because we have non zero velocity plus the range.",
            "We also have velocity which will be uniform because I move the hand altogether.",
            "But when I start to do some gesturing it appears in the range Doppler image as many points, so that gives us a way to estimate velocities and then if I have multiple hands, as long as I can separate the objects in the range of dollar plan, I will.",
            "I will have separate values for them."
        ],
        [
            "We also use the reader to perform the gestural segmentation because it is quite straightforward.",
            "Rather gives us information about velocities and we can just drag the maximum velocity we have on the scene.",
            "And then put a threshold over there and say OK if the maximum velocity exceeds this threshold then we say it is a gesture."
        ],
        [
            "We can also use the concept of the power efficiency because the weather is quite power efficient and we can switch on other sensors only when the gesture is actually segmented and then to turn this heavy power devices.",
            "But this is just a concept and we found that it can give us up to 16 times power efficiency."
        ],
        [
            "Now how we do the gesture classification?",
            "We have the data which comes from from 6 sensors.",
            "We first downsample data from each of the sensor to have a fixed size of 32 by 32 frames and then we can coordinate them together as a channels to perform."
        ],
        [
            "To form a frame and then becausw gestures can be of different temporal lengths.",
            "We also need to interpolate in in time in temporal domain too."
        ],
        [
            "Fix it size of 60 frames and this is how we represent our gesture.",
            "Now."
        ],
        [
            "I want to classify this gesture with the deep neural network approach and mainly with the convolutional neural networks.",
            "We found that this structure works best for us.",
            "It's quite small network with two 3D convolutional layers.",
            "Each of them is followed by Max pooling layer.",
            "Then we have two fully connected layers and.",
            "And then because softmax, which gives us probabilities for for, for, for, different class labels, and if you want to have more information about how we perform the training training initialization, I would refer you to the paper where it is discussed."
        ],
        [
            "Now let's talk about the data we collected to test our algorithm.",
            "We collected data in two setups.",
            "The first was indoor car simulator and the second one was outdoor in the parked car and we captured the data from 3 persons.",
            "We asked each person to perform this 10 through gestures plus a lot of random gestures and ask to repeat it from 10 to 20 times and in total we record the data.",
            "410 experiments or which we call sessions in our case and during the data collection we tried to variacs tremely different lighting conditions and in total we end up with around 1700 gestures."
        ],
        [
            "Now let's see how the data was Brian from from depending on different lighting conditions, like when we capture data during the evening, we found that like all those sensors work quite correctly and then all of the information is useful.",
            "But then when we come to the night situation, color sensor doesn't give us too much information and the same with sunlight as you saw before.",
            "So we tried to capture this different situation when the sensor may fail.",
            "And we can see."
        ],
        [
            "Good.",
            "Then, uh.",
            "Gestures for the classification the 1st four of them was adjust translational gestures to the left, right, up, down, then to swipe gestures to the left and to the right, then shaking gesture rotations clockwise and until clockwise Plaza calling gesture, and then as I told before, we also captured a lot of false gestures such as coffee drinking or talking on the mobile phone."
        ],
        [
            "And in the beginning we performed analysis using only one session out cross validation, where removal and hold experiment from the data.",
            "In this case the subject was known for the training data, but it was just under different example lighting conditions or different set up and in the beginning we want."
        ],
        [
            "To check how the single sensor will perform, and we found that the best of course works the depth sensor, it gives more than 90% of correct gesture classification then we.",
            "Train tower networks with a pair of the pair of the sensors and we found that each pair improved result.",
            "And then invent when we."
        ],
        [
            "Is also the sensors to train the network.",
            "We found that result is even better is more than 95% of the correct classification in this case."
        ],
        [
            "And then we asked ourself where this improvement comes from and we grouped our data depending on the different lighting conditions like what we talked before night, evening, day and then they plus sunlight.",
            "And when we checked, this results for the for the network which trained on depth plus radar sensor.",
            "We found that it works.",
            "Reasonably good for all of the lighting conditions, unless it is day sunlight.",
            "Because of this occlusion regions.",
            "But"
        ],
        [
            "When we add the color sensor, the optical sensor to as a modality, we found that all of the results improve and significantly they prove.",
            "For case of the sunlight."
        ],
        [
            "We also compared our results to the hog based method feature handcrafted features which I talked in the beginning about four plus Hawks squared features trained with SVM classifier and as you can see from from this comparison hours, all our method does better and it's based especially does better in cases when a lot of the sensors can fail like at night and during the day sunlight."
        ],
        [
            "We also tested our result of using only one subject out cross validation to see how it performs for unseen subjects, and we actually found that result is not that high.",
            "As before we had a classification error about 25%, even though we still do better than the handcrafted features, it still unreasonable result for such systems and.",
            "In our minds, this happens because of the, because we need more subjects to train because for gestures especially, there is a problem that deeper people perform gestures in a different way, right?",
            "So we need more capture more data for training or the other solution could be performing biometric registration ones and you driver seats to the car."
        ],
        [
            "Now let's see demo.",
            "Of how how it works in kind of real time.",
            "So when I sorry.",
            "Right?",
            "Where is this?",
            "Yeah I will.",
            "I will run it again.",
            "His explanation.",
            "So what you will see here is the color sensor data on the left.",
            "Then depth sensor on the right on the top of this corner you will see if the decision is made.",
            "This hand is detected or not just because of this maximum velocity threshold and on the left corner you will see the decision is made about which class it was.",
            "Oh no, sorry.",
            "Yeah.",
            "Again, last time try.",
            "Yes, so once I start to perform suggestions as they are classified, then because we use the deep neural network approach is also quite fast for the forward pass.",
            "So if even don't see this delay which happens after the gesture has been classified.",
            "Then you also saw this is this a lot of data is missing because of the sun.",
            "Alright, and the last gesture calling."
        ],
        [
            "So this concludes my talk.",
            "We came up with the following conclusions that when when we use multi sensor system of course it can improve our result plus make it more robust to different lighting conditions.",
            "Then we also find that conversion and network is does quite good job on.",
            "Removing the shortcomings of features, separate sensor and we also see this used in this way we can outperform the state of the art handcrafted features and the last conclusion we point is that if we add the reader to such systems, we can reduce power consumption.",
            "We can get better results.",
            "Plus we can get quite easy segmentation of the hand.",
            "Thank you."
        ],
        [
            "Much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Pablo most channel and today I'm going to show you our preliminary results in drivers can gesture recognition using Multisensor system.",
                    "label": 1
                },
                {
                    "sent": "As this work we did together with Shalini, Kevin and Carrie, we are all from NVIDIA and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our main motivation for this work comes from the fact that when we want to use some multimedia devices in the car, we need to put our gaze away from the road and in order to focus and find them, which might be dangerous, right, and?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's why touchless interfaces there are becoming much more popular.",
                    "label": 0
                },
                {
                    "sent": "And because when we use them, we don't need to look out of the road, we can still focus there and then perform gestures in inner space, and one of the forms of such interactions we consider as a dynamic gestures.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when we talk about.",
                    "label": 0
                },
                {
                    "sent": "Touchless interfaces and another gesture recognition.",
                    "label": 0
                },
                {
                    "sent": "I want to mention these two works as the first work was done by Natalia never over and it was charged on challenge from which we heard during the previous talk where we had the RGB data plus upper body skeleton and the best method which won the competition was based on deep neural networks and this gives us motivation that deep neural network and can perform good for different gesture recognition tasks.",
                    "label": 1
                },
                {
                    "sent": "Also another broker about the car gesture recognition.",
                    "label": 0
                },
                {
                    "sent": "We should mention the work published last year in Intelligent Transportation Systems where the authors of invariant Red is a check.",
                    "label": 0
                },
                {
                    "sent": "Different feature extraction methods from the special temporal volume for data captured with RGB did data and they found that Hawk plus Hawk squared features together with SVM classifiers.",
                    "label": 0
                },
                {
                    "sent": "That gives the best.",
                    "label": 0
                },
                {
                    "sent": "Performance for drivers.",
                    "label": 0
                },
                {
                    "sent": "Congestion recognition and we want to use this work as a baseline to compare with.",
                    "label": 1
                },
                {
                    "sent": "His only drawback was that the authors checks the data which they can capture during the day and evening, whereas in the car we can also perform gesture during the night or during other extreme variations of the lighting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general overview of our scheme for gesture recognition in the car as shown on this slide, the ideas that we have.",
                    "label": 0
                },
                {
                    "sent": "This multi sensor system, shown here as a prototype.",
                    "label": 0
                },
                {
                    "sent": "Then the driver performs a gesture as a gesture is sensed by three sensors.",
                    "label": 0
                },
                {
                    "sent": "In our case it is optical sensor plus time of flight sensor which are both we get from softkinetic disease with 125 sensor plus we use the reader which is our own.",
                    "label": 0
                },
                {
                    "sent": "Invention and then we when we capture data from all the stress three sensors, we feed it to the deep neural network, which gives us a prediction for the particular gesture which driver performs.",
                    "label": 1
                },
                {
                    "sent": "Now the main challenge for the drivers can gesture recognition is I said before extreme variation of the in the lighting and which actually cause that some sensors may fail under different lighting conditions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why we want to consider this multi sensor system like for example color camera can provide very good image during the day.",
                    "label": 0
                },
                {
                    "sent": "For the gesture recognition.",
                    "label": 0
                },
                {
                    "sent": "But during the night it fails because of the lack of illumination, right?",
                    "label": 0
                },
                {
                    "sent": "R. The same goes for.",
                    "label": 0
                },
                {
                    "sent": "Commodity depth sensors.",
                    "label": 0
                },
                {
                    "sent": "They work very.",
                    "label": 0
                },
                {
                    "sent": "They work perfectly indoor.",
                    "label": 0
                },
                {
                    "sent": "We get a very nice depth image of the hand, but outside when we have the straight sunlight comes which is coming to the object.",
                    "label": 0
                },
                {
                    "sent": "This object will be saturated or we will have a big gap in the data.",
                    "label": 0
                },
                {
                    "sent": "So we need to use multiple sensors in order to overcome the lock or features a sensor.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also use the radar sensor which comes from the following motivations.",
                    "label": 1
                },
                {
                    "sent": "The first motivations that rather works with all lighting conditions.",
                    "label": 1
                },
                {
                    "sent": "The second motivation is that it has no interference with the Sun, which actually comes from the fact that rather uses.",
                    "label": 1
                },
                {
                    "sent": "A lower frequencies and.",
                    "label": 0
                },
                {
                    "sent": "I'm a flight sensors and plus because of the Doppler effect, we get information about local velocities which happen in the.",
                    "label": 0
                },
                {
                    "sent": "Seen, and this is actually very important because we want to classify the dynamic gestures.",
                    "label": 0
                },
                {
                    "sent": "Where are some velocities for sure as there so we want to send them with the reader, and in this work which I present today, we consider mainly only the sort advantage.",
                    "label": 0
                },
                {
                    "sent": "So we want to only check that how velocities can improve our result.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this slide I show our rather system which we built because there is no commercially available sensors for which we needed, so we had to build 1 and it uses frequency modulation continuous wave architecture, which is highly power efficient with the central frequency of 24 here hurts.",
                    "label": 0
                },
                {
                    "sent": "And if you want to know more information about it, I can provide it after the talk, because we have a separate paper which in details explains as a Raider prototype.",
                    "label": 0
                },
                {
                    "sent": "But what does it measure?",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Emphasize the sense that we measure the range with a fixed resolution of four centimeters.",
                    "label": 1
                },
                {
                    "sent": "We also measure the velocity with within the uniform grid and very high resolution, about four centimeters per second.",
                    "label": 0
                },
                {
                    "sent": "Plus because of the structure, because we have three receivers.",
                    "label": 1
                },
                {
                    "sent": "We can use the pairs of receivers to get the angle, so we kind of get the azimuth and elevation for for the object and then we can of course move it to some occlusion space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the way we use the weather, it will be shown on this slide.",
                    "label": 0
                },
                {
                    "sent": "But I want to mention in the beginning that the main way how's the radar sensors the world.",
                    "label": 0
                },
                {
                    "sent": "It will be the range Doppler image, so it's kind of the distribution joint distribution in the range and velocity of all of the objects which appear in front of the reader and.",
                    "label": 1
                },
                {
                    "sent": "Because this range Doppler images for all of these receivers, but their amplitude will look extremely the very similar, but the face will be different because of the different position of the receivers and we can use this phase difference to get the special location of the objects which are moving.",
                    "label": 0
                },
                {
                    "sent": "And now if we will blend it to the data we get from the depth sensor, the reader will show us a number of points where for issues a point we have a four dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "It will be 3D.",
                    "label": 0
                },
                {
                    "sent": "Position Plaza radial velocity.",
                    "label": 0
                },
                {
                    "sent": "So once once we have these points we projected to the dimensional plane and we want to extrapolate this velocities for the whole frame to everywhere where it happens.",
                    "label": 0
                },
                {
                    "sent": "We do this through the warranty diagram.",
                    "label": 0
                },
                {
                    "sent": "Then we use the mask which we get from the depth sensor of the hand and just simply by multiplying them we get velocity layer which will show us information about instantaneous velocities and out from only this picture we can see that there is a hand which we most likely will do a close gesture because of the velocities we have over here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now I want to show a small demo of how the reader works in order to give you a better idea.",
                    "label": 0
                },
                {
                    "sent": "So on the bottom you will see the range Doppler image with the detected objects, which we highlight by the rectangle and then you will have velocity image on the top right when the color image.",
                    "label": 1
                },
                {
                    "sent": "So when I start to move the hand we we can see that it position changes because we have non zero velocity plus the range.",
                    "label": 0
                },
                {
                    "sent": "We also have velocity which will be uniform because I move the hand altogether.",
                    "label": 0
                },
                {
                    "sent": "But when I start to do some gesturing it appears in the range Doppler image as many points, so that gives us a way to estimate velocities and then if I have multiple hands, as long as I can separate the objects in the range of dollar plan, I will.",
                    "label": 0
                },
                {
                    "sent": "I will have separate values for them.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also use the reader to perform the gestural segmentation because it is quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "Rather gives us information about velocities and we can just drag the maximum velocity we have on the scene.",
                    "label": 1
                },
                {
                    "sent": "And then put a threshold over there and say OK if the maximum velocity exceeds this threshold then we say it is a gesture.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also use the concept of the power efficiency because the weather is quite power efficient and we can switch on other sensors only when the gesture is actually segmented and then to turn this heavy power devices.",
                    "label": 0
                },
                {
                    "sent": "But this is just a concept and we found that it can give us up to 16 times power efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now how we do the gesture classification?",
                    "label": 0
                },
                {
                    "sent": "We have the data which comes from from 6 sensors.",
                    "label": 0
                },
                {
                    "sent": "We first downsample data from each of the sensor to have a fixed size of 32 by 32 frames and then we can coordinate them together as a channels to perform.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To form a frame and then becausw gestures can be of different temporal lengths.",
                    "label": 0
                },
                {
                    "sent": "We also need to interpolate in in time in temporal domain too.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fix it size of 60 frames and this is how we represent our gesture.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to classify this gesture with the deep neural network approach and mainly with the convolutional neural networks.",
                    "label": 1
                },
                {
                    "sent": "We found that this structure works best for us.",
                    "label": 1
                },
                {
                    "sent": "It's quite small network with two 3D convolutional layers.",
                    "label": 0
                },
                {
                    "sent": "Each of them is followed by Max pooling layer.",
                    "label": 1
                },
                {
                    "sent": "Then we have two fully connected layers and.",
                    "label": 0
                },
                {
                    "sent": "And then because softmax, which gives us probabilities for for, for, for, different class labels, and if you want to have more information about how we perform the training training initialization, I would refer you to the paper where it is discussed.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's talk about the data we collected to test our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We collected data in two setups.",
                    "label": 0
                },
                {
                    "sent": "The first was indoor car simulator and the second one was outdoor in the parked car and we captured the data from 3 persons.",
                    "label": 1
                },
                {
                    "sent": "We asked each person to perform this 10 through gestures plus a lot of random gestures and ask to repeat it from 10 to 20 times and in total we record the data.",
                    "label": 0
                },
                {
                    "sent": "410 experiments or which we call sessions in our case and during the data collection we tried to variacs tremely different lighting conditions and in total we end up with around 1700 gestures.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see how the data was Brian from from depending on different lighting conditions, like when we capture data during the evening, we found that like all those sensors work quite correctly and then all of the information is useful.",
                    "label": 0
                },
                {
                    "sent": "But then when we come to the night situation, color sensor doesn't give us too much information and the same with sunlight as you saw before.",
                    "label": 0
                },
                {
                    "sent": "So we tried to capture this different situation when the sensor may fail.",
                    "label": 0
                },
                {
                    "sent": "And we can see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Then, uh.",
                    "label": 0
                },
                {
                    "sent": "Gestures for the classification the 1st four of them was adjust translational gestures to the left, right, up, down, then to swipe gestures to the left and to the right, then shaking gesture rotations clockwise and until clockwise Plaza calling gesture, and then as I told before, we also captured a lot of false gestures such as coffee drinking or talking on the mobile phone.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the beginning we performed analysis using only one session out cross validation, where removal and hold experiment from the data.",
                    "label": 0
                },
                {
                    "sent": "In this case the subject was known for the training data, but it was just under different example lighting conditions or different set up and in the beginning we want.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To check how the single sensor will perform, and we found that the best of course works the depth sensor, it gives more than 90% of correct gesture classification then we.",
                    "label": 0
                },
                {
                    "sent": "Train tower networks with a pair of the pair of the sensors and we found that each pair improved result.",
                    "label": 0
                },
                {
                    "sent": "And then invent when we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is also the sensors to train the network.",
                    "label": 0
                },
                {
                    "sent": "We found that result is even better is more than 95% of the correct classification in this case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we asked ourself where this improvement comes from and we grouped our data depending on the different lighting conditions like what we talked before night, evening, day and then they plus sunlight.",
                    "label": 1
                },
                {
                    "sent": "And when we checked, this results for the for the network which trained on depth plus radar sensor.",
                    "label": 0
                },
                {
                    "sent": "We found that it works.",
                    "label": 1
                },
                {
                    "sent": "Reasonably good for all of the lighting conditions, unless it is day sunlight.",
                    "label": 0
                },
                {
                    "sent": "Because of this occlusion regions.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we add the color sensor, the optical sensor to as a modality, we found that all of the results improve and significantly they prove.",
                    "label": 0
                },
                {
                    "sent": "For case of the sunlight.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared our results to the hog based method feature handcrafted features which I talked in the beginning about four plus Hawks squared features trained with SVM classifier and as you can see from from this comparison hours, all our method does better and it's based especially does better in cases when a lot of the sensors can fail like at night and during the day sunlight.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also tested our result of using only one subject out cross validation to see how it performs for unseen subjects, and we actually found that result is not that high.",
                    "label": 1
                },
                {
                    "sent": "As before we had a classification error about 25%, even though we still do better than the handcrafted features, it still unreasonable result for such systems and.",
                    "label": 0
                },
                {
                    "sent": "In our minds, this happens because of the, because we need more subjects to train because for gestures especially, there is a problem that deeper people perform gestures in a different way, right?",
                    "label": 1
                },
                {
                    "sent": "So we need more capture more data for training or the other solution could be performing biometric registration ones and you driver seats to the car.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see demo.",
                    "label": 0
                },
                {
                    "sent": "Of how how it works in kind of real time.",
                    "label": 0
                },
                {
                    "sent": "So when I sorry.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Where is this?",
                    "label": 0
                },
                {
                    "sent": "Yeah I will.",
                    "label": 0
                },
                {
                    "sent": "I will run it again.",
                    "label": 0
                },
                {
                    "sent": "His explanation.",
                    "label": 0
                },
                {
                    "sent": "So what you will see here is the color sensor data on the left.",
                    "label": 0
                },
                {
                    "sent": "Then depth sensor on the right on the top of this corner you will see if the decision is made.",
                    "label": 0
                },
                {
                    "sent": "This hand is detected or not just because of this maximum velocity threshold and on the left corner you will see the decision is made about which class it was.",
                    "label": 0
                },
                {
                    "sent": "Oh no, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Again, last time try.",
                    "label": 0
                },
                {
                    "sent": "Yes, so once I start to perform suggestions as they are classified, then because we use the deep neural network approach is also quite fast for the forward pass.",
                    "label": 0
                },
                {
                    "sent": "So if even don't see this delay which happens after the gesture has been classified.",
                    "label": 0
                },
                {
                    "sent": "Then you also saw this is this a lot of data is missing because of the sun.",
                    "label": 0
                },
                {
                    "sent": "Alright, and the last gesture calling.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "We came up with the following conclusions that when when we use multi sensor system of course it can improve our result plus make it more robust to different lighting conditions.",
                    "label": 0
                },
                {
                    "sent": "Then we also find that conversion and network is does quite good job on.",
                    "label": 0
                },
                {
                    "sent": "Removing the shortcomings of features, separate sensor and we also see this used in this way we can outperform the state of the art handcrafted features and the last conclusion we point is that if we add the reader to such systems, we can reduce power consumption.",
                    "label": 0
                },
                {
                    "sent": "We can get better results.",
                    "label": 0
                },
                {
                    "sent": "Plus we can get quite easy segmentation of the hand.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}