{
    "id": "heyek7lvml5q2tfhoqbp22z3kbdyvztz",
    "title": "Modeling and Summarizing News Events using Semantic Triples",
    "info": {
        "author": [
            "Radityo Eko Prasojo, KRDB Research Centre for Knowledge and Data, Free University of Bozen-Bolzano"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_eko_prasojo_semantic_triples/",
    "segmentation": [
        [
            "And thank you everyone for coming.",
            "I'm going to present our work modeling answer summarizing news events using semantic triples.",
            "My name is radicchio, and this is a joint work with my supervisors.",
            "May not catch man.",
            "We're going to.",
            "We are researchers of the Caribbean Center for Knowledge and data in the University of Tulsa, no.",
            "So automotive fish."
        ],
        [
            "When is represented by if the news reader here.",
            "So she loves to write news articles, but whenever something important happens so it is all news outlets here always start to give news articles with different level of details an for if this."
        ],
        [
            "Becomes so time consuming to follow all of them.",
            "So for."
        ],
        [
            "At recent then, well, he says that if I will fill the summary for you, so then begin the journey of Wally to create an approach that can automatically summarize multiple news articles into a concise summary.",
            "So the first thing to consider is that what are the kinds of automated summarization technique that exists out there?",
            "There are in general techniques.",
            "The first one is the extractive ones and the second one is the abstractive ones.",
            "So for the extractive summarization we consider.",
            "Multiple news articles and then we consider a several sentences from these articles to be important.",
            "And then we simply copy all of them into one summary, and then for the abstract summarization we instead look at the sentences of chunk of information and then we annotate this chunk of information that we consider to be important.",
            "And if we follow the active approach, we then see that we combine them into one summary and we have some redundancy there.",
            "So if you see the colors and we have multiple clues there.",
            "But in the abstractive approach with semantic understanding, we can see that the politics are actually the same.",
            "Then we probably can generate one summary sentence that contains one of all of the information in a concise way.",
            "So therefore this is what we want abstractive summarization."
        ],
        [
            "So how does an abstractive summarization approach work?",
            "Actually?",
            "So here, consider an example of a collection of document an.",
            "We select three sentences from them.",
            "And then from 2 sentences we basically create word tokens from them and I'm using some open information extraction technique.",
            "We can segment this sentence into a subject predicate objects.",
            "And then to understand the connection between this chunks of information, we can for example do some entity disambiguation or do entity linking to say that this Hurricane Nate, Nate and Hurricane are actually the same entities, and at the Louisiana and the State of Louisiana are also the same entities.",
            "And then with some 30 some equation, or for plinking, we can see that the SLAM and struck in the two sentences are actually also the same.",
            "And then by using some Fusion or emerging techniques, we can come up with summary sentences.",
            "So in abstractive summary setting we can see that.",
            "This is just copy it.",
            "We have Hurricane Nate Slam Louisiana and the Parrot can kill two people.",
            "But in abstractive summary with conjoined facts we can have the sentence hurricane.",
            "It's Louisiana killing two people which is more concise?"
        ],
        [
            "So what are actually the existing approaches for abstractive summarization?",
            "So we identified two.",
            "We found two different techniques.",
            "In general.",
            "The first one is what is called fresh selection based techniques which basically identify different phrases from sentences.",
            "Usually it is a subject and verb phrases and then later check for the compatibility to generate new sentence is from this raises and the second one is called pattern graph Fusion which basically looks for similar tokens in the sentences and then try to use the.",
            "Tokens forming a graph which from the graph generate summary sentences.",
            "So first."
        ],
        [
            "You'll see the PSV example, so consider these three sentences.",
            "And then in PSP what they do is to annotate the first phrase and the subject risk from each of the sentences.",
            "And there is some compatibility check whether the subject and verb phrases are compatible and there will be some similarity measures to see whether some subject phrases and some purposes are similar to each other.",
            "And then from the similarity and compatibility, we can have some cross compatibility.",
            "So from the.",
            "Killed two people here that is compatible with it and that the fact that it is similar to Hurricane 8 and we can say that Eric and it is also compatible to this purpose below and then we can have the summary sentence hurricane.",
            "It's Louisiana killed two people.",
            "So."
        ],
        [
            "But for the PSP approach, there is some limitation.",
            "That is, we find we will find dilemma.",
            "That is we will lose some information or we have some redundancy in our summaries.",
            "So let's get in this exam."
        ],
        [
            "Well, we have again subject for Sanford phrase from these two sentences and then in the prices we see that the fur prices are complicated, so a bit complex, not just a single verb and object.",
            "And here we are faced with this decision whether to say that these two phrases are similar or not.",
            "So depending on the technique of the similarity we will come up with this possible 2 cases.",
            "The first one is when we say that they are similar.",
            "Then we say that there.",
            "Replaceable to each other, so we just just one of them in our summary, then we lose some information, and in the second case then we say that they are not similar.",
            "Then probably they are important to be included in the summary part of them.",
            "Then we put both sentences in the summer it and we have some redundancy.",
            "Here you see that the slim Louisiana is mentioned twice in this sentences, which which is not concise enough."
        ],
        [
            "OK, So what about the second technique?",
            "That is the pattern graph Fusion.",
            "So in the pattern graph you see and we have a collection of documents with sentences again and then the first step is to extract subject predicate object from the sentences using an open information extraction technique.",
            "Here in our baseline by Lee ET al.",
            "So well, here is a baseline by late Al.",
            "They use what is called Hollywood.",
            "Just one of the open information extraction technique that is available there.",
            "And then we have this subject objectors, and then in the second step, we annotate its head work of the arguments that is the subject or the object with the typing.",
            "So the typing will take the typing that is output by Stanford NLP or decimal for semantic role labeling.",
            "So here people will be back as person and then hurricane as the protagonist and etc.",
            "And then from the typing we have the graph using itself.",
            "So we try to combine all this pipings result into one interconnected graph.",
            "Here, for example, we have that person is killed by the protagonist and the person also type in location.",
            "So in this case we because we merge them into based on the fine grade level, that is the word token.",
            "We don't lose any information like the previous approach.",
            "So as I mentioned, this approach was based on the work of little, which is the first approach that tried to apply.",
            "Fusion paste for abstractive summarization in news articles."
        ],
        [
            "So we will try to see some examples on how distribution works.",
            "So here for example, consider these three sentences.",
            "So in the extraction part, then we simply use only to get this triple like structures.",
            "So in only you can also have the arguments that can be up for appeals to the sentence.",
            "So here is the result."
        ],
        [
            "And then in the second step, that is the typing, we apply Stanford NLP and some effort to get the types of its head words of the arguments of the output of Oly.",
            "So here for example, Nate is person, an Louisiana is location and then Saturday estate and etc.",
            "And once we have this result."
        ],
        [
            "Then we built a diffusion, so from each of the annotated sentence that we have before we try to combine them.",
            "So here is first first, first sentence that we have, and then we add the second one like this and the third one.",
            "And then we generate the summary sentence based on the sum ranking procedure for every possible path that we have in the graphs or sample.",
            "This is our example sentence for our summary sentence."
        ],
        [
            "But so the better graphics and also have some limitations.",
            "So here consider the same example as before.",
            "But we add another sentence like this.",
            "So the sentence that Pope Francis and his prayer for Louisiana.",
            "And then we have this summary sentence which is actually doesn't make sense.",
            "So because this is a result of merging based on the typing in the in the in the graph."
        ],
        [
            "OK, the other limitation is that this pattern graph using the baseline doesn't consider semantically similar.",
            "First, they are not merged together, which may result in incomplete information being generated in the summary, which is a conference issue.",
            "And then for the grammatical EP, this merging Mail is to ungrammatical result.",
            "So for example, Tus pulled up from the particles disappointment among different values.",
            "And then."
        ],
        [
            "So based on our our observations to this limitations we have device auto summarization technique that tried to address each of these limitation.",
            "So the first one is we introduce entity linking to.",
            "To address the correctness and coverage and then for plinking to address the cover is an finally practical fixing to address climate equality."
        ],
        [
            "OK, so in our summarization the pipeline goes like this.",
            "So this is the pipeline of the baseline.",
            "So we have the data set and we extract the triples and then we do some clustering, offered the triples and from each cluster we do some typing and then refuse all these type type peoples together as an from each of the cluster that we have we generate one summary sentence.",
            "So in our improvements here, the blue box will mean that something we have introduced and a green box mean something we have changed.",
            "So we introduce entity linking, which is space from TPD and for plinking which is space from worth net and then we do some changes in the Fusion ranking in generation."
        ],
        [
            "So for the entity linking considered examples, we have three sentences, and then first we do some named entity recognition using the PDF spotlight.",
            "So and then the blue words, there are annotated entities by DPD Spotlight and then we do what we call the name normalization so for.",
            "Enter the occurrences that are not appearing in the Canonicalized name.",
            "We left wrist.",
            "We keep pace with direct of relation of it and we see that it is actually.",
            "Analias of the entity Louisiana.",
            "And then we do something different solution.",
            "Also leveraging DB pedia.",
            "So indeed there are types.",
            "So here Hurricane it is type as hurricane in the PDF.",
            "So we check for existence of the hurricane in the text and we then annotated as a conference to Hurricane Nate.",
            "And then we simply use Stanford preferences solution to find someone for an affordable solution."
        ],
        [
            "And then for the first linking it is simple, so we just work that similarity with when power model similarity approach an if it is above 0.9 that we consider them to be similar."
        ],
        [
            "So once we have this similarity metrics, then the next question is how to fuse them together so they are possible.",
            "There are several possibilities to do the merging, so they are whether we want to merge based on the entities the verbs are typing.",
            "Still, if it exists and probably some non stop words there.",
            "So for example, here when we try to fuse the graph from the previous example using only the entities and verbally have something like this.",
            "So this is the first sentence and when we get to the second sentence, we say that.",
            "It is here in eight and then Louisiana is the Creole state and then killed two people here and then.",
            "Similarly, with the third example.",
            "So there is one caveat here that because the reconnect and slam were merged together.",
            "So here we say that this actually contains 2 ages here when we merge them together.",
            "So we keep this X value."
        ],
        [
            "And then once we have this graph connected to each other, then we want to generate.",
            "The sentence from this graph, so to do that like like I said before, when in the baseline, then we want to collect all possible paths from the starting path to the ending, pass into graph, and then we Kingdom.",
            "So the ranking criteria that we have is that first is the path coverage, that is, how many sentences actually that the path offering.",
            "So very simple this there is a color code here.",
            "Then if the path has two color codes in there but then it has two path ranking and if it is true that is 3.",
            "So for example here.",
            "Sorry.",
            "So in the second.",
            "2nd is the node decree, so it's simply based on the total number of incoming and outgoing edges of the nodes.",
            "And so, for example, here, when we consider this path, we say here that the PATH conferences 2 because it covers two sentences from the original text.",
            "And then that affects no degree is 2.14.",
            "And in this past year than the back of the student effort, no degree is 2.5.",
            "So the idea here is if the path consists of multiple sentences from the original text, then we believe that this is our combination or abstraction of multiple facts that were in the original sentences and in the notebook realistically means that we want to include all the important notes in the in the graph."
        ],
        [
            "So and then, once we have this then how we generate the sentence so because if we go back to the previous example we say we see that the entities actually were appearing in different forms, so there were in the conference are the non Canonical's name.",
            "So how to generate this into sentence so it simply for the entity we used to Canonicalize name form of pedia.",
            "And if we choose the most frequent ones that is appearing in the cluster.",
            "Are indicative."
        ],
        [
            "So and then.",
            "We do some grammatical fix for dangling verb.",
            "So for when we combine multiple sentences, then sometimes we find something like this.",
            "So in this example we first identify which is the 1st, that is in the incorrect form.",
            "So here is the kill.",
            "So we use Stanford dependency parser to identify this verb.",
            "That is, we see that it doesn't have any subject.",
            "So in the correct well formed sentence, an effort should have a subject.",
            "So here we see that it doesn't have one.",
            "Then we say that.",
            "It is in aquatic form and then we check whether it is actually a passive or active.",
            "For one example, we check the D object arc here and then we fix the verb.",
            "So in this example we transform into a present participle."
        ],
        [
            "So for the evaluation we use.",
            "Did you see 2014, 2007 data set, so which contains 95 news topics an for each topic and newsarticles an for human summaries as the gold standard and 42 C 2007, we have semantic gold standard, so in the settings is we want to try out different merging start strategies.",
            "So whether it's based on entities, verbs, typings are non stop words and then for the metrics we use Rouge which is actually just an engram overlaps with.",
            "Gold standard and then the pyramid, which is a semantic comparison between our output and semantical standard and in the human assessment will be used to measure the coherence, which includes grammaticality and the correctness of our result.",
            "So."
        ],
        [
            "These are rules result.",
            "So what we do here is basically we take first baseline which uses the typing and the non stop verse for merging and then we try to add our improvements that are the entity linking and defer blinking and then later we want to try to remove what baselines right to do.",
            "So here we first at the entity linking and then for plinking and later on we try to remove the non standard non stop words and typing and we see that the best result is when we apply to merging paste the entity linking and for plinking and sometimes when we.",
            "Repiping we find something new.",
            "OK."
        ],
        [
            "And for the pyramid we use semantic similarity to check with the gold standard to check whether the.",
            "The semantic content in the in the in the pyramid that is actually present in our summary result.",
            "So an we see that we have improvements compared to the baseline and then for the manual summary we see that we have just a little improvement for the coherence and then larger improvement sort of correctness.",
            "OK."
        ],
        [
            "OK, I'm a little bit ugly, I think so for the conclusions we see that the Pattern Graph Fusion based technique is best fit for athletic summarization compared to the extractive approaches and the other abstractive approach.",
            "And then we try to enrich the pattern graphics and approach by semantic annotations from the knowledge base we add the entity and for plinking and apart ranking leveraging the node degrees and we also tried to add the grammatical fixes.",
            "And in our experiments, basically show that our enrichments outperform the baseline.",
            "And so forth."
        ],
        [
            "Future work we want to have more fine, great representation of the fact.",
            "So we want to have to identify for its event that is happening.",
            "So there should be a man for Dan.",
            "Some additional details and then we want to separate the two of them.",
            "So which one is the main factor?",
            "Which one is the additional details?",
            "And then by doing this we can do some semantic ranking so not probably not all of these details are important.",
            "So by doing semantic ranking we can prune out these details that are not important and just include.",
            "Is that important in the summary?",
            "And then for fluency, we want to also consider the possibility of having relationship between sentences.",
            "So which sentence would go before and after the sentence?",
            "OK. And that's basically it."
        ],
        [
            "This summary by Wally so and that ends my presentation.",
            "If you have any questions, thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And thank you everyone for coming.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present our work modeling answer summarizing news events using semantic triples.",
                    "label": 1
                },
                {
                    "sent": "My name is radicchio, and this is a joint work with my supervisors.",
                    "label": 0
                },
                {
                    "sent": "May not catch man.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "We are researchers of the Caribbean Center for Knowledge and data in the University of Tulsa, no.",
                    "label": 0
                },
                {
                    "sent": "So automotive fish.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When is represented by if the news reader here.",
                    "label": 0
                },
                {
                    "sent": "So she loves to write news articles, but whenever something important happens so it is all news outlets here always start to give news articles with different level of details an for if this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Becomes so time consuming to follow all of them.",
                    "label": 0
                },
                {
                    "sent": "So for.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At recent then, well, he says that if I will fill the summary for you, so then begin the journey of Wally to create an approach that can automatically summarize multiple news articles into a concise summary.",
                    "label": 0
                },
                {
                    "sent": "So the first thing to consider is that what are the kinds of automated summarization technique that exists out there?",
                    "label": 0
                },
                {
                    "sent": "There are in general techniques.",
                    "label": 0
                },
                {
                    "sent": "The first one is the extractive ones and the second one is the abstractive ones.",
                    "label": 0
                },
                {
                    "sent": "So for the extractive summarization we consider.",
                    "label": 0
                },
                {
                    "sent": "Multiple news articles and then we consider a several sentences from these articles to be important.",
                    "label": 0
                },
                {
                    "sent": "And then we simply copy all of them into one summary, and then for the abstract summarization we instead look at the sentences of chunk of information and then we annotate this chunk of information that we consider to be important.",
                    "label": 0
                },
                {
                    "sent": "And if we follow the active approach, we then see that we combine them into one summary and we have some redundancy there.",
                    "label": 0
                },
                {
                    "sent": "So if you see the colors and we have multiple clues there.",
                    "label": 0
                },
                {
                    "sent": "But in the abstractive approach with semantic understanding, we can see that the politics are actually the same.",
                    "label": 0
                },
                {
                    "sent": "Then we probably can generate one summary sentence that contains one of all of the information in a concise way.",
                    "label": 0
                },
                {
                    "sent": "So therefore this is what we want abstractive summarization.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does an abstractive summarization approach work?",
                    "label": 0
                },
                {
                    "sent": "Actually?",
                    "label": 0
                },
                {
                    "sent": "So here, consider an example of a collection of document an.",
                    "label": 0
                },
                {
                    "sent": "We select three sentences from them.",
                    "label": 0
                },
                {
                    "sent": "And then from 2 sentences we basically create word tokens from them and I'm using some open information extraction technique.",
                    "label": 0
                },
                {
                    "sent": "We can segment this sentence into a subject predicate objects.",
                    "label": 0
                },
                {
                    "sent": "And then to understand the connection between this chunks of information, we can for example do some entity disambiguation or do entity linking to say that this Hurricane Nate, Nate and Hurricane are actually the same entities, and at the Louisiana and the State of Louisiana are also the same entities.",
                    "label": 1
                },
                {
                    "sent": "And then with some 30 some equation, or for plinking, we can see that the SLAM and struck in the two sentences are actually also the same.",
                    "label": 0
                },
                {
                    "sent": "And then by using some Fusion or emerging techniques, we can come up with summary sentences.",
                    "label": 0
                },
                {
                    "sent": "So in abstractive summary setting we can see that.",
                    "label": 0
                },
                {
                    "sent": "This is just copy it.",
                    "label": 0
                },
                {
                    "sent": "We have Hurricane Nate Slam Louisiana and the Parrot can kill two people.",
                    "label": 0
                },
                {
                    "sent": "But in abstractive summary with conjoined facts we can have the sentence hurricane.",
                    "label": 1
                },
                {
                    "sent": "It's Louisiana killing two people which is more concise?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are actually the existing approaches for abstractive summarization?",
                    "label": 0
                },
                {
                    "sent": "So we identified two.",
                    "label": 0
                },
                {
                    "sent": "We found two different techniques.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "The first one is what is called fresh selection based techniques which basically identify different phrases from sentences.",
                    "label": 0
                },
                {
                    "sent": "Usually it is a subject and verb phrases and then later check for the compatibility to generate new sentence is from this raises and the second one is called pattern graph Fusion which basically looks for similar tokens in the sentences and then try to use the.",
                    "label": 1
                },
                {
                    "sent": "Tokens forming a graph which from the graph generate summary sentences.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You'll see the PSV example, so consider these three sentences.",
                    "label": 0
                },
                {
                    "sent": "And then in PSP what they do is to annotate the first phrase and the subject risk from each of the sentences.",
                    "label": 0
                },
                {
                    "sent": "And there is some compatibility check whether the subject and verb phrases are compatible and there will be some similarity measures to see whether some subject phrases and some purposes are similar to each other.",
                    "label": 0
                },
                {
                    "sent": "And then from the similarity and compatibility, we can have some cross compatibility.",
                    "label": 0
                },
                {
                    "sent": "So from the.",
                    "label": 0
                },
                {
                    "sent": "Killed two people here that is compatible with it and that the fact that it is similar to Hurricane 8 and we can say that Eric and it is also compatible to this purpose below and then we can have the summary sentence hurricane.",
                    "label": 0
                },
                {
                    "sent": "It's Louisiana killed two people.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for the PSP approach, there is some limitation.",
                    "label": 0
                },
                {
                    "sent": "That is, we find we will find dilemma.",
                    "label": 0
                },
                {
                    "sent": "That is we will lose some information or we have some redundancy in our summaries.",
                    "label": 1
                },
                {
                    "sent": "So let's get in this exam.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we have again subject for Sanford phrase from these two sentences and then in the prices we see that the fur prices are complicated, so a bit complex, not just a single verb and object.",
                    "label": 0
                },
                {
                    "sent": "And here we are faced with this decision whether to say that these two phrases are similar or not.",
                    "label": 0
                },
                {
                    "sent": "So depending on the technique of the similarity we will come up with this possible 2 cases.",
                    "label": 0
                },
                {
                    "sent": "The first one is when we say that they are similar.",
                    "label": 0
                },
                {
                    "sent": "Then we say that there.",
                    "label": 0
                },
                {
                    "sent": "Replaceable to each other, so we just just one of them in our summary, then we lose some information, and in the second case then we say that they are not similar.",
                    "label": 0
                },
                {
                    "sent": "Then probably they are important to be included in the summary part of them.",
                    "label": 0
                },
                {
                    "sent": "Then we put both sentences in the summer it and we have some redundancy.",
                    "label": 0
                },
                {
                    "sent": "Here you see that the slim Louisiana is mentioned twice in this sentences, which which is not concise enough.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what about the second technique?",
                    "label": 0
                },
                {
                    "sent": "That is the pattern graph Fusion.",
                    "label": 1
                },
                {
                    "sent": "So in the pattern graph you see and we have a collection of documents with sentences again and then the first step is to extract subject predicate object from the sentences using an open information extraction technique.",
                    "label": 1
                },
                {
                    "sent": "Here in our baseline by Lee ET al.",
                    "label": 0
                },
                {
                    "sent": "So well, here is a baseline by late Al.",
                    "label": 0
                },
                {
                    "sent": "They use what is called Hollywood.",
                    "label": 0
                },
                {
                    "sent": "Just one of the open information extraction technique that is available there.",
                    "label": 0
                },
                {
                    "sent": "And then we have this subject objectors, and then in the second step, we annotate its head work of the arguments that is the subject or the object with the typing.",
                    "label": 0
                },
                {
                    "sent": "So the typing will take the typing that is output by Stanford NLP or decimal for semantic role labeling.",
                    "label": 0
                },
                {
                    "sent": "So here people will be back as person and then hurricane as the protagonist and etc.",
                    "label": 0
                },
                {
                    "sent": "And then from the typing we have the graph using itself.",
                    "label": 0
                },
                {
                    "sent": "So we try to combine all this pipings result into one interconnected graph.",
                    "label": 1
                },
                {
                    "sent": "Here, for example, we have that person is killed by the protagonist and the person also type in location.",
                    "label": 0
                },
                {
                    "sent": "So in this case we because we merge them into based on the fine grade level, that is the word token.",
                    "label": 0
                },
                {
                    "sent": "We don't lose any information like the previous approach.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, this approach was based on the work of little, which is the first approach that tried to apply.",
                    "label": 0
                },
                {
                    "sent": "Fusion paste for abstractive summarization in news articles.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we will try to see some examples on how distribution works.",
                    "label": 0
                },
                {
                    "sent": "So here for example, consider these three sentences.",
                    "label": 0
                },
                {
                    "sent": "So in the extraction part, then we simply use only to get this triple like structures.",
                    "label": 0
                },
                {
                    "sent": "So in only you can also have the arguments that can be up for appeals to the sentence.",
                    "label": 0
                },
                {
                    "sent": "So here is the result.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the second step, that is the typing, we apply Stanford NLP and some effort to get the types of its head words of the arguments of the output of Oly.",
                    "label": 0
                },
                {
                    "sent": "So here for example, Nate is person, an Louisiana is location and then Saturday estate and etc.",
                    "label": 0
                },
                {
                    "sent": "And once we have this result.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we built a diffusion, so from each of the annotated sentence that we have before we try to combine them.",
                    "label": 0
                },
                {
                    "sent": "So here is first first, first sentence that we have, and then we add the second one like this and the third one.",
                    "label": 0
                },
                {
                    "sent": "And then we generate the summary sentence based on the sum ranking procedure for every possible path that we have in the graphs or sample.",
                    "label": 0
                },
                {
                    "sent": "This is our example sentence for our summary sentence.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But so the better graphics and also have some limitations.",
                    "label": 0
                },
                {
                    "sent": "So here consider the same example as before.",
                    "label": 0
                },
                {
                    "sent": "But we add another sentence like this.",
                    "label": 0
                },
                {
                    "sent": "So the sentence that Pope Francis and his prayer for Louisiana.",
                    "label": 1
                },
                {
                    "sent": "And then we have this summary sentence which is actually doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "So because this is a result of merging based on the typing in the in the in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the other limitation is that this pattern graph using the baseline doesn't consider semantically similar.",
                    "label": 0
                },
                {
                    "sent": "First, they are not merged together, which may result in incomplete information being generated in the summary, which is a conference issue.",
                    "label": 0
                },
                {
                    "sent": "And then for the grammatical EP, this merging Mail is to ungrammatical result.",
                    "label": 0
                },
                {
                    "sent": "So for example, Tus pulled up from the particles disappointment among different values.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So based on our our observations to this limitations we have device auto summarization technique that tried to address each of these limitation.",
                    "label": 0
                },
                {
                    "sent": "So the first one is we introduce entity linking to.",
                    "label": 0
                },
                {
                    "sent": "To address the correctness and coverage and then for plinking to address the cover is an finally practical fixing to address climate equality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in our summarization the pipeline goes like this.",
                    "label": 1
                },
                {
                    "sent": "So this is the pipeline of the baseline.",
                    "label": 0
                },
                {
                    "sent": "So we have the data set and we extract the triples and then we do some clustering, offered the triples and from each cluster we do some typing and then refuse all these type type peoples together as an from each of the cluster that we have we generate one summary sentence.",
                    "label": 0
                },
                {
                    "sent": "So in our improvements here, the blue box will mean that something we have introduced and a green box mean something we have changed.",
                    "label": 0
                },
                {
                    "sent": "So we introduce entity linking, which is space from TPD and for plinking which is space from worth net and then we do some changes in the Fusion ranking in generation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the entity linking considered examples, we have three sentences, and then first we do some named entity recognition using the PDF spotlight.",
                    "label": 0
                },
                {
                    "sent": "So and then the blue words, there are annotated entities by DPD Spotlight and then we do what we call the name normalization so for.",
                    "label": 0
                },
                {
                    "sent": "Enter the occurrences that are not appearing in the Canonicalized name.",
                    "label": 0
                },
                {
                    "sent": "We left wrist.",
                    "label": 0
                },
                {
                    "sent": "We keep pace with direct of relation of it and we see that it is actually.",
                    "label": 0
                },
                {
                    "sent": "Analias of the entity Louisiana.",
                    "label": 0
                },
                {
                    "sent": "And then we do something different solution.",
                    "label": 0
                },
                {
                    "sent": "Also leveraging DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So indeed there are types.",
                    "label": 0
                },
                {
                    "sent": "So here Hurricane it is type as hurricane in the PDF.",
                    "label": 0
                },
                {
                    "sent": "So we check for existence of the hurricane in the text and we then annotated as a conference to Hurricane Nate.",
                    "label": 0
                },
                {
                    "sent": "And then we simply use Stanford preferences solution to find someone for an affordable solution.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for the first linking it is simple, so we just work that similarity with when power model similarity approach an if it is above 0.9 that we consider them to be similar.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we have this similarity metrics, then the next question is how to fuse them together so they are possible.",
                    "label": 0
                },
                {
                    "sent": "There are several possibilities to do the merging, so they are whether we want to merge based on the entities the verbs are typing.",
                    "label": 0
                },
                {
                    "sent": "Still, if it exists and probably some non stop words there.",
                    "label": 0
                },
                {
                    "sent": "So for example, here when we try to fuse the graph from the previous example using only the entities and verbally have something like this.",
                    "label": 0
                },
                {
                    "sent": "So this is the first sentence and when we get to the second sentence, we say that.",
                    "label": 0
                },
                {
                    "sent": "It is here in eight and then Louisiana is the Creole state and then killed two people here and then.",
                    "label": 0
                },
                {
                    "sent": "Similarly, with the third example.",
                    "label": 0
                },
                {
                    "sent": "So there is one caveat here that because the reconnect and slam were merged together.",
                    "label": 0
                },
                {
                    "sent": "So here we say that this actually contains 2 ages here when we merge them together.",
                    "label": 0
                },
                {
                    "sent": "So we keep this X value.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then once we have this graph connected to each other, then we want to generate.",
                    "label": 0
                },
                {
                    "sent": "The sentence from this graph, so to do that like like I said before, when in the baseline, then we want to collect all possible paths from the starting path to the ending, pass into graph, and then we Kingdom.",
                    "label": 0
                },
                {
                    "sent": "So the ranking criteria that we have is that first is the path coverage, that is, how many sentences actually that the path offering.",
                    "label": 0
                },
                {
                    "sent": "So very simple this there is a color code here.",
                    "label": 0
                },
                {
                    "sent": "Then if the path has two color codes in there but then it has two path ranking and if it is true that is 3.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So in the second.",
                    "label": 0
                },
                {
                    "sent": "2nd is the node decree, so it's simply based on the total number of incoming and outgoing edges of the nodes.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, here, when we consider this path, we say here that the PATH conferences 2 because it covers two sentences from the original text.",
                    "label": 0
                },
                {
                    "sent": "And then that affects no degree is 2.14.",
                    "label": 0
                },
                {
                    "sent": "And in this past year than the back of the student effort, no degree is 2.5.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is if the path consists of multiple sentences from the original text, then we believe that this is our combination or abstraction of multiple facts that were in the original sentences and in the notebook realistically means that we want to include all the important notes in the in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and then, once we have this then how we generate the sentence so because if we go back to the previous example we say we see that the entities actually were appearing in different forms, so there were in the conference are the non Canonical's name.",
                    "label": 0
                },
                {
                    "sent": "So how to generate this into sentence so it simply for the entity we used to Canonicalize name form of pedia.",
                    "label": 0
                },
                {
                    "sent": "And if we choose the most frequent ones that is appearing in the cluster.",
                    "label": 0
                },
                {
                    "sent": "Are indicative.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and then.",
                    "label": 0
                },
                {
                    "sent": "We do some grammatical fix for dangling verb.",
                    "label": 0
                },
                {
                    "sent": "So for when we combine multiple sentences, then sometimes we find something like this.",
                    "label": 0
                },
                {
                    "sent": "So in this example we first identify which is the 1st, that is in the incorrect form.",
                    "label": 0
                },
                {
                    "sent": "So here is the kill.",
                    "label": 0
                },
                {
                    "sent": "So we use Stanford dependency parser to identify this verb.",
                    "label": 0
                },
                {
                    "sent": "That is, we see that it doesn't have any subject.",
                    "label": 0
                },
                {
                    "sent": "So in the correct well formed sentence, an effort should have a subject.",
                    "label": 0
                },
                {
                    "sent": "So here we see that it doesn't have one.",
                    "label": 0
                },
                {
                    "sent": "Then we say that.",
                    "label": 0
                },
                {
                    "sent": "It is in aquatic form and then we check whether it is actually a passive or active.",
                    "label": 0
                },
                {
                    "sent": "For one example, we check the D object arc here and then we fix the verb.",
                    "label": 0
                },
                {
                    "sent": "So in this example we transform into a present participle.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the evaluation we use.",
                    "label": 0
                },
                {
                    "sent": "Did you see 2014, 2007 data set, so which contains 95 news topics an for each topic and newsarticles an for human summaries as the gold standard and 42 C 2007, we have semantic gold standard, so in the settings is we want to try out different merging start strategies.",
                    "label": 0
                },
                {
                    "sent": "So whether it's based on entities, verbs, typings are non stop words and then for the metrics we use Rouge which is actually just an engram overlaps with.",
                    "label": 0
                },
                {
                    "sent": "Gold standard and then the pyramid, which is a semantic comparison between our output and semantical standard and in the human assessment will be used to measure the coherence, which includes grammaticality and the correctness of our result.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are rules result.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is basically we take first baseline which uses the typing and the non stop verse for merging and then we try to add our improvements that are the entity linking and defer blinking and then later we want to try to remove what baselines right to do.",
                    "label": 0
                },
                {
                    "sent": "So here we first at the entity linking and then for plinking and later on we try to remove the non standard non stop words and typing and we see that the best result is when we apply to merging paste the entity linking and for plinking and sometimes when we.",
                    "label": 0
                },
                {
                    "sent": "Repiping we find something new.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the pyramid we use semantic similarity to check with the gold standard to check whether the.",
                    "label": 0
                },
                {
                    "sent": "The semantic content in the in the in the pyramid that is actually present in our summary result.",
                    "label": 0
                },
                {
                    "sent": "So an we see that we have improvements compared to the baseline and then for the manual summary we see that we have just a little improvement for the coherence and then larger improvement sort of correctness.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm a little bit ugly, I think so for the conclusions we see that the Pattern Graph Fusion based technique is best fit for athletic summarization compared to the extractive approaches and the other abstractive approach.",
                    "label": 0
                },
                {
                    "sent": "And then we try to enrich the pattern graphics and approach by semantic annotations from the knowledge base we add the entity and for plinking and apart ranking leveraging the node degrees and we also tried to add the grammatical fixes.",
                    "label": 0
                },
                {
                    "sent": "And in our experiments, basically show that our enrichments outperform the baseline.",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future work we want to have more fine, great representation of the fact.",
                    "label": 0
                },
                {
                    "sent": "So we want to have to identify for its event that is happening.",
                    "label": 0
                },
                {
                    "sent": "So there should be a man for Dan.",
                    "label": 0
                },
                {
                    "sent": "Some additional details and then we want to separate the two of them.",
                    "label": 0
                },
                {
                    "sent": "So which one is the main factor?",
                    "label": 0
                },
                {
                    "sent": "Which one is the additional details?",
                    "label": 0
                },
                {
                    "sent": "And then by doing this we can do some semantic ranking so not probably not all of these details are important.",
                    "label": 0
                },
                {
                    "sent": "So by doing semantic ranking we can prune out these details that are not important and just include.",
                    "label": 0
                },
                {
                    "sent": "Is that important in the summary?",
                    "label": 0
                },
                {
                    "sent": "And then for fluency, we want to also consider the possibility of having relationship between sentences.",
                    "label": 0
                },
                {
                    "sent": "So which sentence would go before and after the sentence?",
                    "label": 0
                },
                {
                    "sent": "OK. And that's basically it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This summary by Wally so and that ends my presentation.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions, thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}