{
    "id": "cnw2is2mz2n2jcktarn2tkmktm26vcw4",
    "title": "De-Identifying Facial Images Using Projections on Hyperspheres",
    "info": {
        "author": [
            "Anastasios Tefas, Department of Informatics, Aristotle University of Thessaloniki"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_tefas_facial_images/",
    "segmentation": [
        [
            "So the outline of my presentation."
        ],
        [
            "I will introducing the problem, then I then talk about projections from hyperspheres and the support vector data Description method we use.",
            "I will give some experiments and some future directions guarding our work."
        ],
        [
            "So the need for the identification has already been explained in detail.",
            "We have setting with patient limits data, social media, medical data, where able sensors and so on.",
            "So there is an emerging need for the identification or for privacy protection in all kinds of media, but it's already has been discussed.",
            "Different applications have different needs.",
            "So there is a debate on about the utility of the data, so we want to identify them.",
            "But maybe we want to keep specific attributes, gender expressions, race and so on.",
            "In other cases, we want to identify them from automatic algorithms, but we want persons or human beings to be able to identify otherwise they lose the quality.",
            "So as I said before, different applications have different needs.",
            "And we cannot have all methods dealing with all these problems.",
            "So in out."
        ],
        [
            "Chase the idea is.",
            "To have a method that is able to prevent automatic recognition algorithms from identifying official images.",
            "And in the same time we're trying to maintain enough visual information for a human being.",
            "So in that case not to lose the quality as much as possible.",
            "The quality of the data.",
            "For someone to be able to let us say upload it in his social network.",
            "And we are using projections on hyperspheres.",
            "For the identification and I will talk a little bit about some extensions too.",
            "How we can moderate this basic idea in order to be able to also deal with concepts like Kane, Enimity or how we can preserve specific utility of data if we want to?"
        ],
        [
            "So as I said before the proposed faced defecation method utilizes projections, hyperspheres.",
            "So the goal is to build hyper sphere and then to project official limits on this hyper sphere in order to.",
            "Prevent automatic facial recognition algorithms for identifying the.",
            "The ideal for specific events.",
            "In this paper we have proposed two different approaches, easily extended to other cases, so the one is projection using the origin of the feature space and the other is projection using the average image as a center of the hypersphere.",
            "The idea."
        ],
        [
            "Is rather simple so I have hyper sphere.",
            "Enter.",
            "Centered begin in the origin of future space and where X is the data point, so it's images vectorized.",
            "It's it's a multi dimensional feature point in the N dimensional feature space and we can project this specific feature point up to this fear.",
            "Basically by normalizing its.",
            "One selected please select it, Eleanor.",
            "For example, the L2 norm.",
            "So all the all the images go on the hypersphere.",
            "So this is basically doesn't do much beyond that changes there.",
            "The luminous oh the lightning of the facial images so."
        ],
        [
            "Most of the approaches who have already seen into her there in the literature, they use the average GMAT.",
            "So for example, to obtain K anonymity we project all the images on the middle point, the average amount, and this in some sense this catch all the identity formation and keeps only the average image multiple times for several different identities.",
            "In our case, what we are using is that we're trying to combine the average image with this normalized, let us say on the hyper sphere image to produce their deidentified phase.",
            "So we're dating advice they identified face in our case, as to component one coming from the average miles and the second coming from the initial normalization on the hyper sphere.",
            "So."
        ],
        [
            "The second approach is not to use the origin, cause the origin is vulnerable to parrots attacks, so someone can do reverse engineering and maybe if of course there is always the parameter of R, which is the radius of the method.",
            "But someone can try to also to reverse engineer the method finding bug identification.",
            "So instead of using the origin, we can use another.",
            "Correspondence letter same as this can be any images of the data set or for example can be the average amounts.",
            "So this is basically that we project the match on the hyper sphere that is centered on the average image.",
            "In this case we have two points of privacy or difficulty for an attacker to find.",
            "So one is that he has to estimate somehow the radius air and the 2nd is that he has also estimate.",
            "The immense they are pretty much that we have used for this transform."
        ],
        [
            "Zoom.",
            "These normalizations are able.",
            "Two more or less gather all the images that we want to identify near the center that we have pre selected either the origin or they mean image and then is also compute linear combination of the correspondence and the deidentified image.",
            "What we can do to regularize this method is to choose appropriately the radius R of the proposed approach.",
            "So if we, for example, we choose a large R, then all the images more or less stay in the initial position, so we don't gain much the identification if we use a very small are, then only Matt is going to collapse on the center of the hypersphere, and then we have of course the identification, but.",
            "Also, less visible and.",
            "Quality, so we're trying to automatically estimate this R by considering that we can model all the images that are available.",
            "In our case, in this in this paper, but also you can think that we can try to find the hyper sphere that encloses all the images that have the same concept that we have.",
            "We want to handle for example if we want to encapsulate it all the male subjects we build a hyper sphere around the males and then we put all these images towards the center of the males and also the same for the females keeping the specific attribute if we want."
        ],
        [
            "So the idea is to use.",
            "A method, in our case, we use the support vector data description which is able to estimate the radius of the hypersphere that encloses all the data in the training set.",
            "And this method is.",
            "In our case gives us also the ability to have small errors, so we have a robust estimation of the R."
        ],
        [
            "OK, the theory is known.",
            "Some of us I will briefly describe that we form quadratic optimization problems similar to SVM's when we're trying to enclose all samples XI in the hyper sphere that is centered on the sender U and we're trying to find the parameter R that performs this task with minimum error and we allow some sample.",
            "Points to go.",
            "Off this hypersphere.",
            "If we."
        ],
        [
            "Use LaGrange multipliers to solve this problem.",
            "Then we form the dual objective function and by taking the KT KT conditions we can find the optimal.",
            "Optimality conditions it is straightforward to see that the.",
            "We have a linear combination of data samples.",
            "In order to estimate the parameters that we want to.",
            "So."
        ],
        [
            "All the concepts results in the quadratic optimization problem with constraints.",
            "And lagazo multiplier and the output of these algorithms that it gives us the estimate of the radius R that we should use in our case to calculate all the data inside the hypersphere.",
            "We take this parameter R and we're trying to evaluate if this gives us enough the identification for the specific task.",
            "At this point, let me say that this is very important, 'cause we don't.",
            "We don't know beforehand what is the radius, the value of this radius, as I said before, if we want to model different concepts, then for example the hyper sphere that encapsulates all the emails is different from the.",
            "Hypersphere, that calculates all the happy of Kacian so so, so we we we need to.",
            "Evaluate this.",
            "Parameter automatically and in a robust way.",
            "So come."
        ],
        [
            "To the experiments we have evaluated the proposed approach.",
            "This initial proposed approach in two data sets.",
            "We accept videos database with 388 training samples at 256 tests and we extend Gale database with 1200 for training and another 1200 for tests.",
            "And we have used of course pixel vectorization.",
            "Feature vector and also LDA based representation of the feature.",
            "The facial images and as classifiers.",
            "We have tried to see how the proposed approach.",
            "Is able to cope with different classifiers.",
            "We have used KNN K nearest neighbor, nearest centroid classifier which performs well with linear discriminant analysis and also naive Bayes classifier."
        ],
        [
            "Regarding the valuation on one hand we have how much that we are updating the identification of the data set.",
            "On the other hand, as I said before, we are trying to keep as much as possible the quality of the images, so we're trying also to find out how the proposed approach reduces the main square error between the original images and the identified ones.",
            "So we also calculate the mean mean squared error of the entire data set.",
            "And we."
        ],
        [
            "Tables you can see for several values of the radius R. So for several values of radius R, as I said before, small values of the radius give high datafication.",
            "Former chaudes this is the error on the different classifiers but also have a large distortion of the original images.",
            "So in red it's the value we obtained using the support vector description, so you can see here that we have value that more or less attains a good data fication performance, but also a good mean squared error in terms of quality reduction.",
            "These are also the.",
            "For the, he'll be that I said the same result we have here.",
            "You can see that we have different values for the radius because it's a different data set, so This is why it is very important.",
            "Revolution automatic methods for defining the radius R."
        ],
        [
            "Siri, you can see some deidentified images.",
            "Of course, as I said before, for very small radius we have more or less very blurred images, and we have also caused effects and as the radius goes up we have improved quality.",
            "So the the right users will find automatically by the support vector description.",
            "It was around 7th in this case.",
            "And."
        ],
        [
            "Here is the results for the method that uses the average matches correspondent Simmons.",
            "So once again we have in read the value that this has been obtained by the proposed approach for support vector data description.",
            "And once again."
        ],
        [
            "Some visual results.",
            "Support vector data screen gives around this image.",
            "Notice that of course, the ghost effect can be reduced as in the other methods if we use prior to this method, facial limits, alignment, point detection, and so on, this can be significantly reduced, but we have."
        ],
        [
            "Not performed in this case.",
            "So the developed projection method aims to limit the effectiveness of phase education automatic phase identification.",
            "For privacy we have obtained good results in two specific data sets.",
            "The end product we can consider that it's acceptable for everyday life would use in terms that a human being is able more or less to recognize the correct person, whereas the specific classifiers that we have used fail and for."
        ],
        [
            "Future research directions.",
            "We can define multiple hyper spheres.",
            "As I said before, send up the different samples to retain Kane anonymity and also these fears can be selected by a privacy key so will not be easy to reverse engineer this values and also can define hyperspheres for specific utilities on facial specific expression, specific genders, age intervals and so on.",
            "And the features are going.",
            "We can use privacy key.",
            "We consider this also important for the specific method."
        ],
        [
            "So this work has been performed.",
            "And partial support from a cost action and also by the project.",
            "Free TV S and this brings my talking.",
            "Then open for questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the outline of my presentation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will introducing the problem, then I then talk about projections from hyperspheres and the support vector data Description method we use.",
                    "label": 0
                },
                {
                    "sent": "I will give some experiments and some future directions guarding our work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the need for the identification has already been explained in detail.",
                    "label": 0
                },
                {
                    "sent": "We have setting with patient limits data, social media, medical data, where able sensors and so on.",
                    "label": 1
                },
                {
                    "sent": "So there is an emerging need for the identification or for privacy protection in all kinds of media, but it's already has been discussed.",
                    "label": 1
                },
                {
                    "sent": "Different applications have different needs.",
                    "label": 0
                },
                {
                    "sent": "So there is a debate on about the utility of the data, so we want to identify them.",
                    "label": 0
                },
                {
                    "sent": "But maybe we want to keep specific attributes, gender expressions, race and so on.",
                    "label": 0
                },
                {
                    "sent": "In other cases, we want to identify them from automatic algorithms, but we want persons or human beings to be able to identify otherwise they lose the quality.",
                    "label": 0
                },
                {
                    "sent": "So as I said before, different applications have different needs.",
                    "label": 0
                },
                {
                    "sent": "And we cannot have all methods dealing with all these problems.",
                    "label": 0
                },
                {
                    "sent": "So in out.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chase the idea is.",
                    "label": 0
                },
                {
                    "sent": "To have a method that is able to prevent automatic recognition algorithms from identifying official images.",
                    "label": 0
                },
                {
                    "sent": "And in the same time we're trying to maintain enough visual information for a human being.",
                    "label": 1
                },
                {
                    "sent": "So in that case not to lose the quality as much as possible.",
                    "label": 0
                },
                {
                    "sent": "The quality of the data.",
                    "label": 0
                },
                {
                    "sent": "For someone to be able to let us say upload it in his social network.",
                    "label": 0
                },
                {
                    "sent": "And we are using projections on hyperspheres.",
                    "label": 1
                },
                {
                    "sent": "For the identification and I will talk a little bit about some extensions too.",
                    "label": 0
                },
                {
                    "sent": "How we can moderate this basic idea in order to be able to also deal with concepts like Kane, Enimity or how we can preserve specific utility of data if we want to?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said before the proposed faced defecation method utilizes projections, hyperspheres.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to build hyper sphere and then to project official limits on this hyper sphere in order to.",
                    "label": 1
                },
                {
                    "sent": "Prevent automatic facial recognition algorithms for identifying the.",
                    "label": 0
                },
                {
                    "sent": "The ideal for specific events.",
                    "label": 0
                },
                {
                    "sent": "In this paper we have proposed two different approaches, easily extended to other cases, so the one is projection using the origin of the feature space and the other is projection using the average image as a center of the hypersphere.",
                    "label": 0
                },
                {
                    "sent": "The idea.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is rather simple so I have hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "Centered begin in the origin of future space and where X is the data point, so it's images vectorized.",
                    "label": 1
                },
                {
                    "sent": "It's it's a multi dimensional feature point in the N dimensional feature space and we can project this specific feature point up to this fear.",
                    "label": 0
                },
                {
                    "sent": "Basically by normalizing its.",
                    "label": 0
                },
                {
                    "sent": "One selected please select it, Eleanor.",
                    "label": 0
                },
                {
                    "sent": "For example, the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So all the all the images go on the hypersphere.",
                    "label": 0
                },
                {
                    "sent": "So this is basically doesn't do much beyond that changes there.",
                    "label": 1
                },
                {
                    "sent": "The luminous oh the lightning of the facial images so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Most of the approaches who have already seen into her there in the literature, they use the average GMAT.",
                    "label": 1
                },
                {
                    "sent": "So for example, to obtain K anonymity we project all the images on the middle point, the average amount, and this in some sense this catch all the identity formation and keeps only the average image multiple times for several different identities.",
                    "label": 0
                },
                {
                    "sent": "In our case, what we are using is that we're trying to combine the average image with this normalized, let us say on the hyper sphere image to produce their deidentified phase.",
                    "label": 0
                },
                {
                    "sent": "So we're dating advice they identified face in our case, as to component one coming from the average miles and the second coming from the initial normalization on the hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second approach is not to use the origin, cause the origin is vulnerable to parrots attacks, so someone can do reverse engineering and maybe if of course there is always the parameter of R, which is the radius of the method.",
                    "label": 0
                },
                {
                    "sent": "But someone can try to also to reverse engineer the method finding bug identification.",
                    "label": 0
                },
                {
                    "sent": "So instead of using the origin, we can use another.",
                    "label": 0
                },
                {
                    "sent": "Correspondence letter same as this can be any images of the data set or for example can be the average amounts.",
                    "label": 0
                },
                {
                    "sent": "So this is basically that we project the match on the hyper sphere that is centered on the average image.",
                    "label": 0
                },
                {
                    "sent": "In this case we have two points of privacy or difficulty for an attacker to find.",
                    "label": 0
                },
                {
                    "sent": "So one is that he has to estimate somehow the radius air and the 2nd is that he has also estimate.",
                    "label": 0
                },
                {
                    "sent": "The immense they are pretty much that we have used for this transform.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zoom.",
                    "label": 0
                },
                {
                    "sent": "These normalizations are able.",
                    "label": 0
                },
                {
                    "sent": "Two more or less gather all the images that we want to identify near the center that we have pre selected either the origin or they mean image and then is also compute linear combination of the correspondence and the deidentified image.",
                    "label": 0
                },
                {
                    "sent": "What we can do to regularize this method is to choose appropriately the radius R of the proposed approach.",
                    "label": 1
                },
                {
                    "sent": "So if we, for example, we choose a large R, then all the images more or less stay in the initial position, so we don't gain much the identification if we use a very small are, then only Matt is going to collapse on the center of the hypersphere, and then we have of course the identification, but.",
                    "label": 1
                },
                {
                    "sent": "Also, less visible and.",
                    "label": 0
                },
                {
                    "sent": "Quality, so we're trying to automatically estimate this R by considering that we can model all the images that are available.",
                    "label": 0
                },
                {
                    "sent": "In our case, in this in this paper, but also you can think that we can try to find the hyper sphere that encloses all the images that have the same concept that we have.",
                    "label": 0
                },
                {
                    "sent": "We want to handle for example if we want to encapsulate it all the male subjects we build a hyper sphere around the males and then we put all these images towards the center of the males and also the same for the females keeping the specific attribute if we want.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is to use.",
                    "label": 0
                },
                {
                    "sent": "A method, in our case, we use the support vector data description which is able to estimate the radius of the hypersphere that encloses all the data in the training set.",
                    "label": 1
                },
                {
                    "sent": "And this method is.",
                    "label": 0
                },
                {
                    "sent": "In our case gives us also the ability to have small errors, so we have a robust estimation of the R.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the theory is known.",
                    "label": 0
                },
                {
                    "sent": "Some of us I will briefly describe that we form quadratic optimization problems similar to SVM's when we're trying to enclose all samples XI in the hyper sphere that is centered on the sender U and we're trying to find the parameter R that performs this task with minimum error and we allow some sample.",
                    "label": 0
                },
                {
                    "sent": "Points to go.",
                    "label": 0
                },
                {
                    "sent": "Off this hypersphere.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use LaGrange multipliers to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Then we form the dual objective function and by taking the KT KT conditions we can find the optimal.",
                    "label": 0
                },
                {
                    "sent": "Optimality conditions it is straightforward to see that the.",
                    "label": 1
                },
                {
                    "sent": "We have a linear combination of data samples.",
                    "label": 0
                },
                {
                    "sent": "In order to estimate the parameters that we want to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the concepts results in the quadratic optimization problem with constraints.",
                    "label": 0
                },
                {
                    "sent": "And lagazo multiplier and the output of these algorithms that it gives us the estimate of the radius R that we should use in our case to calculate all the data inside the hypersphere.",
                    "label": 0
                },
                {
                    "sent": "We take this parameter R and we're trying to evaluate if this gives us enough the identification for the specific task.",
                    "label": 0
                },
                {
                    "sent": "At this point, let me say that this is very important, 'cause we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't know beforehand what is the radius, the value of this radius, as I said before, if we want to model different concepts, then for example the hyper sphere that encapsulates all the emails is different from the.",
                    "label": 0
                },
                {
                    "sent": "Hypersphere, that calculates all the happy of Kacian so so, so we we we need to.",
                    "label": 0
                },
                {
                    "sent": "Evaluate this.",
                    "label": 0
                },
                {
                    "sent": "Parameter automatically and in a robust way.",
                    "label": 0
                },
                {
                    "sent": "So come.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the experiments we have evaluated the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "This initial proposed approach in two data sets.",
                    "label": 0
                },
                {
                    "sent": "We accept videos database with 388 training samples at 256 tests and we extend Gale database with 1200 for training and another 1200 for tests.",
                    "label": 0
                },
                {
                    "sent": "And we have used of course pixel vectorization.",
                    "label": 0
                },
                {
                    "sent": "Feature vector and also LDA based representation of the feature.",
                    "label": 0
                },
                {
                    "sent": "The facial images and as classifiers.",
                    "label": 0
                },
                {
                    "sent": "We have tried to see how the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "Is able to cope with different classifiers.",
                    "label": 0
                },
                {
                    "sent": "We have used KNN K nearest neighbor, nearest centroid classifier which performs well with linear discriminant analysis and also naive Bayes classifier.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the valuation on one hand we have how much that we are updating the identification of the data set.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, as I said before, we are trying to keep as much as possible the quality of the images, so we're trying also to find out how the proposed approach reduces the main square error between the original images and the identified ones.",
                    "label": 0
                },
                {
                    "sent": "So we also calculate the mean mean squared error of the entire data set.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tables you can see for several values of the radius R. So for several values of radius R, as I said before, small values of the radius give high datafication.",
                    "label": 0
                },
                {
                    "sent": "Former chaudes this is the error on the different classifiers but also have a large distortion of the original images.",
                    "label": 0
                },
                {
                    "sent": "So in red it's the value we obtained using the support vector description, so you can see here that we have value that more or less attains a good data fication performance, but also a good mean squared error in terms of quality reduction.",
                    "label": 0
                },
                {
                    "sent": "These are also the.",
                    "label": 0
                },
                {
                    "sent": "For the, he'll be that I said the same result we have here.",
                    "label": 0
                },
                {
                    "sent": "You can see that we have different values for the radius because it's a different data set, so This is why it is very important.",
                    "label": 0
                },
                {
                    "sent": "Revolution automatic methods for defining the radius R.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Siri, you can see some deidentified images.",
                    "label": 0
                },
                {
                    "sent": "Of course, as I said before, for very small radius we have more or less very blurred images, and we have also caused effects and as the radius goes up we have improved quality.",
                    "label": 0
                },
                {
                    "sent": "So the the right users will find automatically by the support vector description.",
                    "label": 0
                },
                {
                    "sent": "It was around 7th in this case.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the results for the method that uses the average matches correspondent Simmons.",
                    "label": 0
                },
                {
                    "sent": "So once again we have in read the value that this has been obtained by the proposed approach for support vector data description.",
                    "label": 0
                },
                {
                    "sent": "And once again.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some visual results.",
                    "label": 0
                },
                {
                    "sent": "Support vector data screen gives around this image.",
                    "label": 0
                },
                {
                    "sent": "Notice that of course, the ghost effect can be reduced as in the other methods if we use prior to this method, facial limits, alignment, point detection, and so on, this can be significantly reduced, but we have.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not performed in this case.",
                    "label": 0
                },
                {
                    "sent": "So the developed projection method aims to limit the effectiveness of phase education automatic phase identification.",
                    "label": 1
                },
                {
                    "sent": "For privacy we have obtained good results in two specific data sets.",
                    "label": 1
                },
                {
                    "sent": "The end product we can consider that it's acceptable for everyday life would use in terms that a human being is able more or less to recognize the correct person, whereas the specific classifiers that we have used fail and for.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Future research directions.",
                    "label": 0
                },
                {
                    "sent": "We can define multiple hyper spheres.",
                    "label": 0
                },
                {
                    "sent": "As I said before, send up the different samples to retain Kane anonymity and also these fears can be selected by a privacy key so will not be easy to reverse engineer this values and also can define hyperspheres for specific utilities on facial specific expression, specific genders, age intervals and so on.",
                    "label": 1
                },
                {
                    "sent": "And the features are going.",
                    "label": 0
                },
                {
                    "sent": "We can use privacy key.",
                    "label": 0
                },
                {
                    "sent": "We consider this also important for the specific method.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this work has been performed.",
                    "label": 1
                },
                {
                    "sent": "And partial support from a cost action and also by the project.",
                    "label": 0
                },
                {
                    "sent": "Free TV S and this brings my talking.",
                    "label": 0
                },
                {
                    "sent": "Then open for questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}