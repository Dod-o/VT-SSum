{
    "id": "db22f74v6tkfsdum26jvwxkiu6fjw2qt",
    "title": "Multiclass-multilabel classification with more labels than examples",
    "info": {
        "author": [
            "Ofer Dekel, Microsoft Research"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2010_dekel_mmcw/",
    "segmentation": [
        [
            "Yeah, so this is joint work with Ohatchee Amir from the Hebrew University and what I'm going to talk about today is about multiclass multilabel classification problems, where we have a very very big label set, so millions of labels and Moreover the property of the problem is as we collect more and more labeled examples.",
            "Basically the label set keeps growing and growing, so this is going to be the topic of the talk and just to motivate this."
        ],
        [
            "I'll start with a few examples.",
            "So this is Flickr.",
            "It's a website for sharing photos.",
            "I hope most of you know it and you know you can just post any photo that you have, so here's a photo called Ocean Mist.",
            "And if you notice here on the bottom there's a set of tags, so I large this so the person who uploaded this photograph basically said that his photograph is about landscape and oceans and seas, and gave basically a very long list of 10."
        ],
        [
            "Another example is Wikipedia.",
            "So basically if you Scroll down to the bottom of every single Wikipedia page, then you'll see this thing, so it'll say it says categories and then it has a list of maybe 20 or 30 categories, sometimes maybe only three or four.",
            "So every Wikipedia page has this, and again, the labels can be anything that the author decides to put there.",
            "This is just free text, and it really has the flexibility of put."
        ],
        [
            "Anything that he that he wants so you can imagine how chaotic this can become.",
            "So again, for the Wikipedia page for Leonardo Davinci, for instance, this is a subset of the categories on Wikipedia, so let's just look at these to get a feel for them.",
            "So one category is people born in 1452.",
            "In other categories, people that died in 1519 and then this long category, people from the province of Florence.",
            "So there may be another category, another tag in Wikipedia.",
            "People from Florence and that would be considered a different tag just because it's free text.",
            "And if the strings are not exactly identical, then the label is not the same.",
            "And then there's some other funny ones.",
            "I mean, there's overall something like 20 labels for Leonardo Davinci, and you know some of them are not the ones that you may expect, so you know Italian.",
            "Where is this Italian?",
            "Vegetarians is one of them, or people prosecuted under anti homosexuality law so very very long.",
            "Kind of free text things.",
            "And then again if someone will decide to put exactly this label but not without this hyphenation here, then it would be considered different labels.",
            "So you have the power of free text and then this is prone to spelling mistakes or any type of thing that would occur in this setting.",
            "So it's very very chaotic type of labels label set.",
            "Nevertheless, this is the label set that we want to work with."
        ],
        [
            "So let's define our problem.",
            "We have a multiclass multilabel classification setting.",
            "The label said is what we call a folksonomy.",
            "So these types of tag sets that are developed through a collaborative effort of many, many people usually working on the Internet are called folk ennemies or collaborative tagging or social tagging scheme.",
            "So basically anyone can add A tag as you add a new example, you can basically add new tags that never appeared before and This is why the number of labels can keep growing and growing as the number of labeled examples, pros and we're assuming that we have a very large labeled training set.",
            "So for instance Wikipedia would be a training set.",
            "We have 3 million Wikipedia pages, each one with you know 20 or 30 categories associated with it, and the goal is to categorise new unseen instances.",
            "So for instance, the concrete goal we were thinking of here is to categorise the entire Internet, so we want to take these labels that we have on Wikipedia.",
            "There are 100, there are 1 1/2 million different individual categories on Wikipedia, so we want to label the entire Internet with these 1 1/2 million labels, so that's."
        ],
        [
            "Task.",
            "And some properties of the problem.",
            "So we have M labeled examples, Anki categories, and the important assumption.",
            "Here is we're going to assume basically the both of these things grow together to Infinity.",
            "So the typical assumption in machine learning with multi class classification problems is usually that K is some fixed number.",
            "So I have 1000 classes and now we ask what happens when the sample keeps growing.",
            "I mean what happens asymptotically as I have more and more examples?",
            "And what about the rates?",
            "How fast does this thing converge?",
            "But usually K the number of labels is fixed, but here these two things are going to go together and I still have to tell you exactly how they go together, because this is not a well defined.",
            "Mathematical statement here, but just, you know, realize that these things grow.",
            "And again, the motivation is that you know, in 2009 at least, when we did these experiments, Wikipedia basically had around 3 million articles, an 1 1/2 million categories.",
            "So roughly the same order of of examples an classes, and we're even more interested in this weird setting where we have K bigger than M. So when we have more labels than examples and we're basically facing two types of problems here, one is a statistical problem, and the problem is that even as we sample more and more examples.",
            "We're not guaranteed to get an infinite sample from each and every class, so there could be a class for which we get 3 examples, and that's it.",
            "So it could be a label that has three examples and no matter how much we keep sampling, we're not going to see more instances of that class, so that's statistical problem, and the computational problem is just that the problem is huge.",
            "I mean, as the label set grows and grows, this becomes computationally more and more difficult when we're dealing with 1 1/2 million examples than most of the standard multiclass multilabel techniques will just choke.",
            "That just won't work.",
            "So for instance, in the standard way of doing multiclass, or one standard way of doing multiclass multilabel.",
            "Problems is to train a binary classifier for each one of the individual classes.",
            "So maintaining you know 1 1/2 million of these classifiers really isn't a feasible thing in most applications.",
            "So the first thing I want to address is that."
        ],
        [
            "Potential problem and I want to show you one way to get started to get some initial classification and this is going to be going to be a little bit of a touristic way and it's not going to be the focus of my talk, but I want to make sure that we understand this.",
            "So again, an efficient way to get an initial labeling of all the web pages is the following thing, so we can use what we call the click graph the search engine click click graph so we have logs from a search engine and basically what people do when they use a search engine is they enter a query and they click on one of the results.",
            "So the click graph is basically a bipartite graph that's derived from these search engine logs on the left hand side we have all the queries on the right hand side.",
            "We have all of the URLs, all the web pages, and then every time a user issues a query and then clicks on URL, we draw an edge.",
            "So here if there's multiple edges, you multiply the number of clicks kind of corresponds to the width of this.",
            "Of this age we have fat edges and thin edges, so this is called the click graph, and if we look at the right hand side, it's all the webpages.",
            "Some subset of this set is Wikipedia pages.",
            "For those we have labels.",
            "And what we can do is we can just propagate the labels along the edges of this click graph to the unlabeled pages.",
            "So basically, Wikipedia pages will have a label and then will copy it over to the query which is adjacent to this to this guy.",
            "And then he'll copied over to other web pages.",
            "So basically this is capturing the idea that you know if two web pages are both the result of the same query, then they say they have some type of topical similarity and therefore the labels that apply to go to one of the two URLs will apply to the other one.",
            "So there are various ways to do this.",
            "You could maybe only consider very very thick edges.",
            "Or maybe you could give more weight in the situation where label comes from 2 directions.",
            "So for instance, if this is an unlabeled page and this has a label and it copies it over to this guy, and then this guy maybe has the same label and copies it over again.",
            "So this guy got the label from two different directions.",
            "Maybe you give it, you know higher confidence or so on, but I'm not going to focus on this anymore, so assume that we have some efficient way to get some initial labeling of our Internet of our space.",
            "And efficiencies because we exploit here the locality of this graph."
        ],
        [
            "So that's why it becomes very efficient.",
            "But here's an example that is kind of illuminating to what I want to get to.",
            "So let's take one of these web pages.",
            "So again, this is the Wikipedia page for Luna Davinci.",
            "And it has, you know, as we saw something like 20 labels and it passes these labels.",
            "Too many other websites that it's a neighbor of in this click graph.",
            "So one of them is, you know, www.gradeitalians.com, which is a website about greater times and among these labels it passes Renaissance artists, which is probably a good label to have for this guy.",
            "So that's a good label that's passed across.",
            "But it will also pass across this weird label, you know, 1452 births, which is relevant to dinardo's page, but probably is not relevant to the more general page on.",
            "On greater talents, so the point is that we will have good labels and bad labels.",
            "There will be labels that kind of don't really match our representation of the data like this click graph representation or any other representation that you choose maybe will not be compatible with some of the labels.",
            "So this is the important observation.",
            "1452 births this label will induce many many false positives.",
            "Will basically label a lot of these webpages by, you know, by a false label.",
            "So you make lots of mistakes.",
            "Maybe it's just best to remove it altogether, so maybe we should just take this initial classifier that we got using our heuristic.",
            "Technique and just prevent it from ever out putting these bad labels.",
            "So I wanted to identify these guys, improve them away and this I'm going to this post.",
            "Learning step is what I'm going to do in a nice theoretical principled way so the initial labeling is obtained using some hand WAVY Ristic thing.",
            "All I need for that is I want it to be computationally feasible.",
            "I wanted to be fast, but now I'm going to fix it up in a principled way.",
            "And what do I mean by removing the label?",
            "So basically when you remove a label from a classifier, you're taking all of the false positives and turning them into true negatives because you're preventing the.",
            "Classifier for making that mistake, but on the other hand, you're paying a price.",
            "You're taking all the true positives and turning them into false negatives, so there's a tradeoff here, and the question is, you know, is it worth my while to remove this label or not?"
        ],
        [
            "I do this for each one of the labels, so I want to find things a little bit more formally so we can move forward with this.",
            "So here's my notation.",
            "I have an instant SpaceX Anna label set.",
            "Why so?",
            "My set of labels is just these these K dimensional vectors of indicators.",
            "So they basically indicate whether I be longer.",
            "I don't belong to each one of the classes.",
            "I have some joint distribution over X&YI have S which is a sample sampled IID from this distribution.",
            "I'm assuming I have two M points and that's just because I'm going to be in a minute tell you that I'm going to split the sample into two halves and only use the second half.",
            "So I need endpoints and that's why I assume that I have two MID points to begin with.",
            "Now a classifier classifier is just a function that takes instances and Maps them to these indicator vectors.",
            "It suffers this gamma weighted loss.",
            "So Gamma is the way that I associate with false positives and 1 minus gamma is the way I associate with false negatives.",
            "So basically the loss of function H on an example XY is just going to be the number of false positives times gamma plus the number of false negatives times 1 minus gamma.",
            "So Gamma is a user defined parameter.",
            "Risk is just using different definition of risk.",
            "It's just this loss average across the entire distribution.",
            "This is something that I can't measure, but I can measure an empirical proxy of it on any sample.",
            "So this is just the empirical risk, the average loss."
        ],
        [
            "On this sample.",
            "So here's the algorithm.",
            "I mean, for those of you who have lost me and have stop listening, this is the time to start listening again.",
            "This is the kind of the crux of what we're doing, so it's very, very simple, very straightforward algorithm.",
            "We take our data set and we split into these two halves, so we have S1 and S2.",
            "We use S1 to train this initial classifier using any technique you like, for example, using this quick graphic that I described before.",
            "But you could use anything you'd like and we get this hypothesis H pre so premiums is pre pruning is before we put anything out of it.",
            "Then here's where it starts becoming interesting.",
            "We take this hypothesis and apply it to this second independent half of the data, and we count false positives and true positives for each one of the classes.",
            "For each one of the labels.",
            "And now for each label we say, should I put it or not?",
            "I basically decided to remove it if the ratio for that label of false positives to true positives exceeds some threshold which is defined by this game.",
            "So again, the user defined parameter of importance and all this means is that gamma times false positives.",
            "That's what I'm paying without pruning it.",
            "If I would put it away, then all these true positives would turn into false negatives and I paid 1 minus gamma for those.",
            "So what's bigger?",
            "And this basically boils down to just minimize just putting in the way that explicitly in a greedy way minimizes.",
            "The empirical risk on this S 2, so I obtain an you prune classifier will call this H post and this is just, you know, a closed form way of saying that I'm just trying to explicitly minimize the empirical risk on this second sample.",
            "By pruning away labels."
        ],
        [
            "So it's very simple, right?",
            "I'm just doing using those kind of a brute force technique to do to the sample.",
            "What I want actually want it to happen to the distribution.",
            "So does that happen?",
            "So for the analysis I'd like to think of this initial hypothesis, that thing that I got using the first sample just as a fixed thing.",
            "So I got that from someone.",
            "I get some fixed hypothesis, but think of the second sample S2 as a random variable.",
            "So this is going to be important for the Alesis and what I want to prove is that with high probability.",
            "Over the sampling of this second sample, then the risk of the post pruning hypothesis is smaller than the risk of the original thing, with some gap.",
            "Even so, I actually did a good thing.",
            "I made the risk of my of my classifier smaller, so this is attempt number one.",
            "This is kind of clumsy attempt.",
            "It's going to be wrong, but you know, you could maybe think maybe I can get uniform convergence for each one of the labels.",
            "So maybe you would assume.",
            "But think of bright positive is just my, you know not.",
            "I didn't want to write what I what I what's there?",
            "I just want to say there's some gap between the two so I want to say it's better.",
            "I want to show that I improved that's all I'm saying here.",
            "Once I prove once I'm prove something you'll see what I prove.",
            "OK, but the point is that you know this is kind of a rough estimate of when approving here you can think, well maybe.",
            "You know, for each individual label I could see if it's a good label or band label.",
            "If the label that is harmful to me, or if it's labeled that I should leave in there.",
            "Basically what I want is for each label to get kind of convergence of this empirical thing that I measure the ratio of empirical false positive false positives to empirical true positives will converge to the expect to the ratio of the expected things, right?",
            "So if I knew this and I could do a perfect job, but I only know these, and of course this does not hold exactly for the reason that I said before that I have very very small samples from a lot of the classes, so I may have 345 examples.",
            "Or maybe one or two examples from some of the classes, and there's no way that I can get this type of convergence and be confident about each label individually, so we can actually prove this.",
            "We know we don't know, we know that we don't know which labels are good and which labels are bad on an individual labeled basis, so we will definitely with this technique throw out some good labels and leave in some bad labels.",
            "But our analysis has to be global.",
            "We're going to say something like overall we're doing good.",
            "Overall, we're removing a lot of the bad guys.",
            "Maybe some of the good guys, but certainly more bad than good.",
            "And overall we're improving the.",
            "Call"
        ],
        [
            "Over classifier.",
            "So the first attempt didn't work.",
            "Here's a second attempt, and this is kind of the thing that you know someone who does.",
            "Learning theory a lot would would would think at the end of the first thing that comes to mind using the standard tricks of statistical learning theory.",
            "And here it is.",
            "So basically what I'm doing with my algorithm is doing is explicitly making this empirical risk the empirical risk of the proof guy smaller than the empirical risk of the pre proof guy, right?",
            "But what I want is I want the expected version of this to be smaller than expected version of that.",
            "So maybe all I have to prove now is that this is close to its expectation.",
            "This is close to the expectation, in other words that the empirical risk of the pre prune guy will converge to its actual risk as MNK go to Infinity and the empirical risk of the post and I will do the same.",
            "So this is easy just because you know the preprint guy, just a fixed hypothesis independent of the sample.",
            "So just the standard tail bounds you know the haunting type bounds will easily show that this convergence happens.",
            "This is where it gets interesting, and this actually doesn't hold, but you might, you know, be hopeful initially and hope that this will little work.",
            "I mean this is kind of the thing that most of our a lot of statistical learning theories about efca would stay fixed in only M would go to Infinity.",
            "You could probably prove this with a VCR command or some argument about the complexity of the class of possible proof hypothesis you have.",
            "So if you can somehow cover that with a small covering number or reason about its Rademacher complexity, then you can show that this thing indeed converges to its expectation.",
            "But since M&K go to Infinity together, this does not work in this break, so we can actually construct very very simple cases where K&M kind of grow linearly with each other, where the gap between these two guys stays constant, no matter how big M is.",
            "So this is, you know, was one of the insightful things we saw right away that the standard tricks that we use in learning theory won't work in this setting.",
            "So we have to go to something kind of different and more less obvious and a little bit more complex.",
            "So this is the third."
        ],
        [
            "And to prove it, and this one works.",
            "This is what we have in the paper, and this is kind of a little bit of an atypical analysis.",
            "Let me go over it and and one of the things I have to do here is make a little bit of a few assumptions, so I prove these two lemmas here.",
            "The first time it has to assume some sparsity.",
            "So basically I'm assuming that the output of this initial hypothesis with probability one will only have S labels in those vectors, so these are binary indicator vectors at most of them will be one, so the initial classifier.",
            "Is is sparse, has this S sparsity to it with probability one and under this assumption I can prove that this guy.",
            "The risk of this random hypothesis, so again, this is the hypothesis after the pruning.",
            "This depends on my random data set.",
            "Therefore it's a random hypothesis.",
            "This is what I care about.",
            "I care about the risk of this random hypothesis.",
            "I can show that this thing converges to the expectation over all possible.",
            "Samples over all possible asses.",
            "So basically the risk of my hypothesis after pruning will behave like a typical like, like the risk of hypothesis given a typical training for some kind of averaged across all possible sets.",
            "So I can show this conversions at the first thing.",
            "The second thing that we want to prove in here I'll just give a simplified version, assuming gammas half again gamma is the importance of false positive versus false negatives.",
            "Also, they are equally important, and then the assumption here is that if I sort my labels in decreasing frequency, so I put the most frequent label 1st and then the second one and so on based on the output of the initial classifier, then they'd be this power law.",
            "So if they're bare parallel like this.",
            "With the coefficient R between zero and two.",
            "So basically if they decay if the frequency of the labels decays fast enough, then I know that this typical risk of a pruned classifier under my algorithm is going to be smaller than the original risk that I started with the initial classifier by some again positive number minus this term.",
            "So so this is zero, just means that the gap is positive.",
            "So for this to be 0 it means that M has to grow just a little bit faster than K to the power of 2 Sr.",
            "So for instance, imagine R = 1.",
            "It just means that K has the grow just slightly less than linear.",
            "If R is bigger than one, then K can grow linearly with M. So I just want to point out you know, are these assumptions reasonable at all?",
            "So the sparsity assumption certainly is true.",
            "For instance, in Wikipedia we have 1 1/2 million labels, but each individual pages may be labeled by 20 or 30 things.",
            "So this first assumption holds very trivially.",
            "The second one about."
        ],
        [
            "This power law so I just took Wikipedia here and I just plotted the frequency of each category.",
            "So just this is the most frequent one in the second most frequent, and so on.",
            "And just we show both axes here in log scale.",
            "And when you do that, you'd expect the power law to just look like a linear line and the slope of the line will be the exponent of the power law, and in this case you know doesn't look like a linear line, but it certainly is upper bounded by linear line, and this slope of this line is something like 1.6.",
            "So Wikipedia itself satisfies a parallel with R = 1.6.",
            "So for Wikipedia we can certainly have.",
            "The number of labels grow linearly or even faster with the number of exam."
        ],
        [
            "I'll I'm at a time so I will only briefly mention this.",
            "I mean, we did some pretty serious experiments here.",
            "It looks like a silly little plot, but we actually did label the entire Internet.",
            "I mean, this is a non trivial engineering task.",
            "What we show here is that for different values of gamma, how our algorithm does its the blue line versus the purple line, which is just the pre printing.",
            "The original thing that we get just by propagating labels across this click graph.",
            "So the reason why it looks like this is we basically had a test set also taken as a random subset of Wikipedia and we picked that set and we saw which labels we should prove, which we should make sure we looked at the test set and got this gold standard of what's the best we can do by just putting out these labels.",
            "And here we just show the ratio of our loss with that optimal loss.",
            "Well being at 1 means that were.",
            "Good as the Oracle that knows what the test set is in our algorithm approaches that from a wide range of gamma, whereas the previous original algorithm sucks pretty badly, and then this black line.",
            "Forget about that."
        ],
        [
            "So I'll conclude.",
            "It was a very very simple algorithm, so it was pretty obvious what you want to do.",
            "You want to do something to the distribution, so instead just do it to your sample.",
            "So the obvious thing works, but certainly not for the obvious reasons, so the complexity and analyzing this kind of exposes that under the hood there's a lot happening here.",
            "It's not as simple as it look.",
            "There's actually statistically there's a lot of complexity happening here under the hood.",
            "The important assumption again is K goes to Infinity.",
            "So the fact that the labels it grows and basically violates a lot of the analysis that we see out there.",
            "I mean most of the standard multi class analysis will not work with this assumption and that's why.",
            "Our analysis is kind of unique and the reason why it does work.",
            "I mean, the main thing that drove that drove us here is that our analysis is not an extension of an analysis for binary classification, so a lot of these analysis that we see for multiclass classifications are direct extensions of an analysis of learning theoretic analysis for binary classification and that kind of implies that you want this per label convergence.",
            "You want uniform convergence across each and everyone of the labels, and that's not going to work when K goes to Infinity and the last thing I want to say is about you know where we can go with this.",
            "So I just talked about the very simplest way of kind of fixing up.",
            "Broken classifier using a principled way of removing or putting away labels.",
            "But we can talk about more complex operators.",
            "For instance, we can maybe think of instead of removing the label, we can say maybe we can merge two labels that seem to be synonyms to us based on some evidence.",
            "Or maybe we can substitute.",
            "We can think of rules that say every time that the classifier the initial classifier, wants to output a.",
            "Maybe put B in its place and that will improve empirically.",
            "And hopefully we'll be able to extend this analysis analysis and show that that will also improve our performance expectation over the distribution.",
            "That's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so this is joint work with Ohatchee Amir from the Hebrew University and what I'm going to talk about today is about multiclass multilabel classification problems, where we have a very very big label set, so millions of labels and Moreover the property of the problem is as we collect more and more labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Basically the label set keeps growing and growing, so this is going to be the topic of the talk and just to motivate this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll start with a few examples.",
                    "label": 0
                },
                {
                    "sent": "So this is Flickr.",
                    "label": 0
                },
                {
                    "sent": "It's a website for sharing photos.",
                    "label": 0
                },
                {
                    "sent": "I hope most of you know it and you know you can just post any photo that you have, so here's a photo called Ocean Mist.",
                    "label": 0
                },
                {
                    "sent": "And if you notice here on the bottom there's a set of tags, so I large this so the person who uploaded this photograph basically said that his photograph is about landscape and oceans and seas, and gave basically a very long list of 10.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example is Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So basically if you Scroll down to the bottom of every single Wikipedia page, then you'll see this thing, so it'll say it says categories and then it has a list of maybe 20 or 30 categories, sometimes maybe only three or four.",
                    "label": 0
                },
                {
                    "sent": "So every Wikipedia page has this, and again, the labels can be anything that the author decides to put there.",
                    "label": 0
                },
                {
                    "sent": "This is just free text, and it really has the flexibility of put.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anything that he that he wants so you can imagine how chaotic this can become.",
                    "label": 0
                },
                {
                    "sent": "So again, for the Wikipedia page for Leonardo Davinci, for instance, this is a subset of the categories on Wikipedia, so let's just look at these to get a feel for them.",
                    "label": 0
                },
                {
                    "sent": "So one category is people born in 1452.",
                    "label": 0
                },
                {
                    "sent": "In other categories, people that died in 1519 and then this long category, people from the province of Florence.",
                    "label": 1
                },
                {
                    "sent": "So there may be another category, another tag in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "People from Florence and that would be considered a different tag just because it's free text.",
                    "label": 0
                },
                {
                    "sent": "And if the strings are not exactly identical, then the label is not the same.",
                    "label": 0
                },
                {
                    "sent": "And then there's some other funny ones.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's overall something like 20 labels for Leonardo Davinci, and you know some of them are not the ones that you may expect, so you know Italian.",
                    "label": 0
                },
                {
                    "sent": "Where is this Italian?",
                    "label": 0
                },
                {
                    "sent": "Vegetarians is one of them, or people prosecuted under anti homosexuality law so very very long.",
                    "label": 0
                },
                {
                    "sent": "Kind of free text things.",
                    "label": 0
                },
                {
                    "sent": "And then again if someone will decide to put exactly this label but not without this hyphenation here, then it would be considered different labels.",
                    "label": 0
                },
                {
                    "sent": "So you have the power of free text and then this is prone to spelling mistakes or any type of thing that would occur in this setting.",
                    "label": 0
                },
                {
                    "sent": "So it's very very chaotic type of labels label set.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, this is the label set that we want to work with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's define our problem.",
                    "label": 0
                },
                {
                    "sent": "We have a multiclass multilabel classification setting.",
                    "label": 1
                },
                {
                    "sent": "The label said is what we call a folksonomy.",
                    "label": 1
                },
                {
                    "sent": "So these types of tag sets that are developed through a collaborative effort of many, many people usually working on the Internet are called folk ennemies or collaborative tagging or social tagging scheme.",
                    "label": 0
                },
                {
                    "sent": "So basically anyone can add A tag as you add a new example, you can basically add new tags that never appeared before and This is why the number of labels can keep growing and growing as the number of labeled examples, pros and we're assuming that we have a very large labeled training set.",
                    "label": 0
                },
                {
                    "sent": "So for instance Wikipedia would be a training set.",
                    "label": 0
                },
                {
                    "sent": "We have 3 million Wikipedia pages, each one with you know 20 or 30 categories associated with it, and the goal is to categorise new unseen instances.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the concrete goal we were thinking of here is to categorise the entire Internet, so we want to take these labels that we have on Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "There are 100, there are 1 1/2 million different individual categories on Wikipedia, so we want to label the entire Internet with these 1 1/2 million labels, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Task.",
                    "label": 0
                },
                {
                    "sent": "And some properties of the problem.",
                    "label": 0
                },
                {
                    "sent": "So we have M labeled examples, Anki categories, and the important assumption.",
                    "label": 1
                },
                {
                    "sent": "Here is we're going to assume basically the both of these things grow together to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So the typical assumption in machine learning with multi class classification problems is usually that K is some fixed number.",
                    "label": 0
                },
                {
                    "sent": "So I have 1000 classes and now we ask what happens when the sample keeps growing.",
                    "label": 0
                },
                {
                    "sent": "I mean what happens asymptotically as I have more and more examples?",
                    "label": 0
                },
                {
                    "sent": "And what about the rates?",
                    "label": 0
                },
                {
                    "sent": "How fast does this thing converge?",
                    "label": 0
                },
                {
                    "sent": "But usually K the number of labels is fixed, but here these two things are going to go together and I still have to tell you exactly how they go together, because this is not a well defined.",
                    "label": 0
                },
                {
                    "sent": "Mathematical statement here, but just, you know, realize that these things grow.",
                    "label": 0
                },
                {
                    "sent": "And again, the motivation is that you know, in 2009 at least, when we did these experiments, Wikipedia basically had around 3 million articles, an 1 1/2 million categories.",
                    "label": 0
                },
                {
                    "sent": "So roughly the same order of of examples an classes, and we're even more interested in this weird setting where we have K bigger than M. So when we have more labels than examples and we're basically facing two types of problems here, one is a statistical problem, and the problem is that even as we sample more and more examples.",
                    "label": 0
                },
                {
                    "sent": "We're not guaranteed to get an infinite sample from each and every class, so there could be a class for which we get 3 examples, and that's it.",
                    "label": 1
                },
                {
                    "sent": "So it could be a label that has three examples and no matter how much we keep sampling, we're not going to see more instances of that class, so that's statistical problem, and the computational problem is just that the problem is huge.",
                    "label": 0
                },
                {
                    "sent": "I mean, as the label set grows and grows, this becomes computationally more and more difficult when we're dealing with 1 1/2 million examples than most of the standard multiclass multilabel techniques will just choke.",
                    "label": 0
                },
                {
                    "sent": "That just won't work.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the standard way of doing multiclass, or one standard way of doing multiclass multilabel.",
                    "label": 0
                },
                {
                    "sent": "Problems is to train a binary classifier for each one of the individual classes.",
                    "label": 0
                },
                {
                    "sent": "So maintaining you know 1 1/2 million of these classifiers really isn't a feasible thing in most applications.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I want to address is that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Potential problem and I want to show you one way to get started to get some initial classification and this is going to be going to be a little bit of a touristic way and it's not going to be the focus of my talk, but I want to make sure that we understand this.",
                    "label": 0
                },
                {
                    "sent": "So again, an efficient way to get an initial labeling of all the web pages is the following thing, so we can use what we call the click graph the search engine click click graph so we have logs from a search engine and basically what people do when they use a search engine is they enter a query and they click on one of the results.",
                    "label": 0
                },
                {
                    "sent": "So the click graph is basically a bipartite graph that's derived from these search engine logs on the left hand side we have all the queries on the right hand side.",
                    "label": 1
                },
                {
                    "sent": "We have all of the URLs, all the web pages, and then every time a user issues a query and then clicks on URL, we draw an edge.",
                    "label": 0
                },
                {
                    "sent": "So here if there's multiple edges, you multiply the number of clicks kind of corresponds to the width of this.",
                    "label": 0
                },
                {
                    "sent": "Of this age we have fat edges and thin edges, so this is called the click graph, and if we look at the right hand side, it's all the webpages.",
                    "label": 1
                },
                {
                    "sent": "Some subset of this set is Wikipedia pages.",
                    "label": 0
                },
                {
                    "sent": "For those we have labels.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is we can just propagate the labels along the edges of this click graph to the unlabeled pages.",
                    "label": 0
                },
                {
                    "sent": "So basically, Wikipedia pages will have a label and then will copy it over to the query which is adjacent to this to this guy.",
                    "label": 1
                },
                {
                    "sent": "And then he'll copied over to other web pages.",
                    "label": 0
                },
                {
                    "sent": "So basically this is capturing the idea that you know if two web pages are both the result of the same query, then they say they have some type of topical similarity and therefore the labels that apply to go to one of the two URLs will apply to the other one.",
                    "label": 0
                },
                {
                    "sent": "So there are various ways to do this.",
                    "label": 0
                },
                {
                    "sent": "You could maybe only consider very very thick edges.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you could give more weight in the situation where label comes from 2 directions.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if this is an unlabeled page and this has a label and it copies it over to this guy, and then this guy maybe has the same label and copies it over again.",
                    "label": 0
                },
                {
                    "sent": "So this guy got the label from two different directions.",
                    "label": 0
                },
                {
                    "sent": "Maybe you give it, you know higher confidence or so on, but I'm not going to focus on this anymore, so assume that we have some efficient way to get some initial labeling of our Internet of our space.",
                    "label": 0
                },
                {
                    "sent": "And efficiencies because we exploit here the locality of this graph.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's why it becomes very efficient.",
                    "label": 0
                },
                {
                    "sent": "But here's an example that is kind of illuminating to what I want to get to.",
                    "label": 0
                },
                {
                    "sent": "So let's take one of these web pages.",
                    "label": 0
                },
                {
                    "sent": "So again, this is the Wikipedia page for Luna Davinci.",
                    "label": 0
                },
                {
                    "sent": "And it has, you know, as we saw something like 20 labels and it passes these labels.",
                    "label": 0
                },
                {
                    "sent": "Too many other websites that it's a neighbor of in this click graph.",
                    "label": 0
                },
                {
                    "sent": "So one of them is, you know, www.gradeitalians.com, which is a website about greater times and among these labels it passes Renaissance artists, which is probably a good label to have for this guy.",
                    "label": 0
                },
                {
                    "sent": "So that's a good label that's passed across.",
                    "label": 0
                },
                {
                    "sent": "But it will also pass across this weird label, you know, 1452 births, which is relevant to dinardo's page, but probably is not relevant to the more general page on.",
                    "label": 0
                },
                {
                    "sent": "On greater talents, so the point is that we will have good labels and bad labels.",
                    "label": 0
                },
                {
                    "sent": "There will be labels that kind of don't really match our representation of the data like this click graph representation or any other representation that you choose maybe will not be compatible with some of the labels.",
                    "label": 0
                },
                {
                    "sent": "So this is the important observation.",
                    "label": 0
                },
                {
                    "sent": "1452 births this label will induce many many false positives.",
                    "label": 1
                },
                {
                    "sent": "Will basically label a lot of these webpages by, you know, by a false label.",
                    "label": 0
                },
                {
                    "sent": "So you make lots of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's just best to remove it altogether, so maybe we should just take this initial classifier that we got using our heuristic.",
                    "label": 1
                },
                {
                    "sent": "Technique and just prevent it from ever out putting these bad labels.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to identify these guys, improve them away and this I'm going to this post.",
                    "label": 0
                },
                {
                    "sent": "Learning step is what I'm going to do in a nice theoretical principled way so the initial labeling is obtained using some hand WAVY Ristic thing.",
                    "label": 0
                },
                {
                    "sent": "All I need for that is I want it to be computationally feasible.",
                    "label": 0
                },
                {
                    "sent": "I wanted to be fast, but now I'm going to fix it up in a principled way.",
                    "label": 0
                },
                {
                    "sent": "And what do I mean by removing the label?",
                    "label": 0
                },
                {
                    "sent": "So basically when you remove a label from a classifier, you're taking all of the false positives and turning them into true negatives because you're preventing the.",
                    "label": 0
                },
                {
                    "sent": "Classifier for making that mistake, but on the other hand, you're paying a price.",
                    "label": 0
                },
                {
                    "sent": "You're taking all the true positives and turning them into false negatives, so there's a tradeoff here, and the question is, you know, is it worth my while to remove this label or not?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I do this for each one of the labels, so I want to find things a little bit more formally so we can move forward with this.",
                    "label": 0
                },
                {
                    "sent": "So here's my notation.",
                    "label": 0
                },
                {
                    "sent": "I have an instant SpaceX Anna label set.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "My set of labels is just these these K dimensional vectors of indicators.",
                    "label": 0
                },
                {
                    "sent": "So they basically indicate whether I be longer.",
                    "label": 0
                },
                {
                    "sent": "I don't belong to each one of the classes.",
                    "label": 0
                },
                {
                    "sent": "I have some joint distribution over X&YI have S which is a sample sampled IID from this distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming I have two M points and that's just because I'm going to be in a minute tell you that I'm going to split the sample into two halves and only use the second half.",
                    "label": 0
                },
                {
                    "sent": "So I need endpoints and that's why I assume that I have two MID points to begin with.",
                    "label": 0
                },
                {
                    "sent": "Now a classifier classifier is just a function that takes instances and Maps them to these indicator vectors.",
                    "label": 1
                },
                {
                    "sent": "It suffers this gamma weighted loss.",
                    "label": 0
                },
                {
                    "sent": "So Gamma is the way that I associate with false positives and 1 minus gamma is the way I associate with false negatives.",
                    "label": 0
                },
                {
                    "sent": "So basically the loss of function H on an example XY is just going to be the number of false positives times gamma plus the number of false negatives times 1 minus gamma.",
                    "label": 0
                },
                {
                    "sent": "So Gamma is a user defined parameter.",
                    "label": 1
                },
                {
                    "sent": "Risk is just using different definition of risk.",
                    "label": 0
                },
                {
                    "sent": "It's just this loss average across the entire distribution.",
                    "label": 0
                },
                {
                    "sent": "This is something that I can't measure, but I can measure an empirical proxy of it on any sample.",
                    "label": 1
                },
                {
                    "sent": "So this is just the empirical risk, the average loss.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this sample.",
                    "label": 0
                },
                {
                    "sent": "So here's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean, for those of you who have lost me and have stop listening, this is the time to start listening again.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of the crux of what we're doing, so it's very, very simple, very straightforward algorithm.",
                    "label": 0
                },
                {
                    "sent": "We take our data set and we split into these two halves, so we have S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "We use S1 to train this initial classifier using any technique you like, for example, using this quick graphic that I described before.",
                    "label": 1
                },
                {
                    "sent": "But you could use anything you'd like and we get this hypothesis H pre so premiums is pre pruning is before we put anything out of it.",
                    "label": 0
                },
                {
                    "sent": "Then here's where it starts becoming interesting.",
                    "label": 0
                },
                {
                    "sent": "We take this hypothesis and apply it to this second independent half of the data, and we count false positives and true positives for each one of the classes.",
                    "label": 0
                },
                {
                    "sent": "For each one of the labels.",
                    "label": 0
                },
                {
                    "sent": "And now for each label we say, should I put it or not?",
                    "label": 0
                },
                {
                    "sent": "I basically decided to remove it if the ratio for that label of false positives to true positives exceeds some threshold which is defined by this game.",
                    "label": 0
                },
                {
                    "sent": "So again, the user defined parameter of importance and all this means is that gamma times false positives.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm paying without pruning it.",
                    "label": 0
                },
                {
                    "sent": "If I would put it away, then all these true positives would turn into false negatives and I paid 1 minus gamma for those.",
                    "label": 0
                },
                {
                    "sent": "So what's bigger?",
                    "label": 0
                },
                {
                    "sent": "And this basically boils down to just minimize just putting in the way that explicitly in a greedy way minimizes.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk on this S 2, so I obtain an you prune classifier will call this H post and this is just, you know, a closed form way of saying that I'm just trying to explicitly minimize the empirical risk on this second sample.",
                    "label": 0
                },
                {
                    "sent": "By pruning away labels.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's very simple, right?",
                    "label": 0
                },
                {
                    "sent": "I'm just doing using those kind of a brute force technique to do to the sample.",
                    "label": 0
                },
                {
                    "sent": "What I want actually want it to happen to the distribution.",
                    "label": 0
                },
                {
                    "sent": "So does that happen?",
                    "label": 0
                },
                {
                    "sent": "So for the analysis I'd like to think of this initial hypothesis, that thing that I got using the first sample just as a fixed thing.",
                    "label": 0
                },
                {
                    "sent": "So I got that from someone.",
                    "label": 0
                },
                {
                    "sent": "I get some fixed hypothesis, but think of the second sample S2 as a random variable.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be important for the Alesis and what I want to prove is that with high probability.",
                    "label": 0
                },
                {
                    "sent": "Over the sampling of this second sample, then the risk of the post pruning hypothesis is smaller than the risk of the original thing, with some gap.",
                    "label": 0
                },
                {
                    "sent": "Even so, I actually did a good thing.",
                    "label": 0
                },
                {
                    "sent": "I made the risk of my of my classifier smaller, so this is attempt number one.",
                    "label": 0
                },
                {
                    "sent": "This is kind of clumsy attempt.",
                    "label": 0
                },
                {
                    "sent": "It's going to be wrong, but you know, you could maybe think maybe I can get uniform convergence for each one of the labels.",
                    "label": 0
                },
                {
                    "sent": "So maybe you would assume.",
                    "label": 0
                },
                {
                    "sent": "But think of bright positive is just my, you know not.",
                    "label": 0
                },
                {
                    "sent": "I didn't want to write what I what I what's there?",
                    "label": 0
                },
                {
                    "sent": "I just want to say there's some gap between the two so I want to say it's better.",
                    "label": 0
                },
                {
                    "sent": "I want to show that I improved that's all I'm saying here.",
                    "label": 0
                },
                {
                    "sent": "Once I prove once I'm prove something you'll see what I prove.",
                    "label": 0
                },
                {
                    "sent": "OK, but the point is that you know this is kind of a rough estimate of when approving here you can think, well maybe.",
                    "label": 0
                },
                {
                    "sent": "You know, for each individual label I could see if it's a good label or band label.",
                    "label": 0
                },
                {
                    "sent": "If the label that is harmful to me, or if it's labeled that I should leave in there.",
                    "label": 0
                },
                {
                    "sent": "Basically what I want is for each label to get kind of convergence of this empirical thing that I measure the ratio of empirical false positive false positives to empirical true positives will converge to the expect to the ratio of the expected things, right?",
                    "label": 0
                },
                {
                    "sent": "So if I knew this and I could do a perfect job, but I only know these, and of course this does not hold exactly for the reason that I said before that I have very very small samples from a lot of the classes, so I may have 345 examples.",
                    "label": 0
                },
                {
                    "sent": "Or maybe one or two examples from some of the classes, and there's no way that I can get this type of convergence and be confident about each label individually, so we can actually prove this.",
                    "label": 0
                },
                {
                    "sent": "We know we don't know, we know that we don't know which labels are good and which labels are bad on an individual labeled basis, so we will definitely with this technique throw out some good labels and leave in some bad labels.",
                    "label": 0
                },
                {
                    "sent": "But our analysis has to be global.",
                    "label": 0
                },
                {
                    "sent": "We're going to say something like overall we're doing good.",
                    "label": 0
                },
                {
                    "sent": "Overall, we're removing a lot of the bad guys.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of the good guys, but certainly more bad than good.",
                    "label": 0
                },
                {
                    "sent": "And overall we're improving the.",
                    "label": 0
                },
                {
                    "sent": "Call",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over classifier.",
                    "label": 0
                },
                {
                    "sent": "So the first attempt didn't work.",
                    "label": 0
                },
                {
                    "sent": "Here's a second attempt, and this is kind of the thing that you know someone who does.",
                    "label": 0
                },
                {
                    "sent": "Learning theory a lot would would would think at the end of the first thing that comes to mind using the standard tricks of statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm doing with my algorithm is doing is explicitly making this empirical risk the empirical risk of the proof guy smaller than the empirical risk of the pre proof guy, right?",
                    "label": 0
                },
                {
                    "sent": "But what I want is I want the expected version of this to be smaller than expected version of that.",
                    "label": 0
                },
                {
                    "sent": "So maybe all I have to prove now is that this is close to its expectation.",
                    "label": 0
                },
                {
                    "sent": "This is close to the expectation, in other words that the empirical risk of the pre prune guy will converge to its actual risk as MNK go to Infinity and the empirical risk of the post and I will do the same.",
                    "label": 0
                },
                {
                    "sent": "So this is easy just because you know the preprint guy, just a fixed hypothesis independent of the sample.",
                    "label": 0
                },
                {
                    "sent": "So just the standard tail bounds you know the haunting type bounds will easily show that this convergence happens.",
                    "label": 0
                },
                {
                    "sent": "This is where it gets interesting, and this actually doesn't hold, but you might, you know, be hopeful initially and hope that this will little work.",
                    "label": 0
                },
                {
                    "sent": "I mean this is kind of the thing that most of our a lot of statistical learning theories about efca would stay fixed in only M would go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You could probably prove this with a VCR command or some argument about the complexity of the class of possible proof hypothesis you have.",
                    "label": 0
                },
                {
                    "sent": "So if you can somehow cover that with a small covering number or reason about its Rademacher complexity, then you can show that this thing indeed converges to its expectation.",
                    "label": 0
                },
                {
                    "sent": "But since M&K go to Infinity together, this does not work in this break, so we can actually construct very very simple cases where K&M kind of grow linearly with each other, where the gap between these two guys stays constant, no matter how big M is.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, was one of the insightful things we saw right away that the standard tricks that we use in learning theory won't work in this setting.",
                    "label": 0
                },
                {
                    "sent": "So we have to go to something kind of different and more less obvious and a little bit more complex.",
                    "label": 0
                },
                {
                    "sent": "So this is the third.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to prove it, and this one works.",
                    "label": 0
                },
                {
                    "sent": "This is what we have in the paper, and this is kind of a little bit of an atypical analysis.",
                    "label": 0
                },
                {
                    "sent": "Let me go over it and and one of the things I have to do here is make a little bit of a few assumptions, so I prove these two lemmas here.",
                    "label": 0
                },
                {
                    "sent": "The first time it has to assume some sparsity.",
                    "label": 0
                },
                {
                    "sent": "So basically I'm assuming that the output of this initial hypothesis with probability one will only have S labels in those vectors, so these are binary indicator vectors at most of them will be one, so the initial classifier.",
                    "label": 0
                },
                {
                    "sent": "Is is sparse, has this S sparsity to it with probability one and under this assumption I can prove that this guy.",
                    "label": 0
                },
                {
                    "sent": "The risk of this random hypothesis, so again, this is the hypothesis after the pruning.",
                    "label": 0
                },
                {
                    "sent": "This depends on my random data set.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's a random hypothesis.",
                    "label": 0
                },
                {
                    "sent": "This is what I care about.",
                    "label": 0
                },
                {
                    "sent": "I care about the risk of this random hypothesis.",
                    "label": 0
                },
                {
                    "sent": "I can show that this thing converges to the expectation over all possible.",
                    "label": 0
                },
                {
                    "sent": "Samples over all possible asses.",
                    "label": 0
                },
                {
                    "sent": "So basically the risk of my hypothesis after pruning will behave like a typical like, like the risk of hypothesis given a typical training for some kind of averaged across all possible sets.",
                    "label": 0
                },
                {
                    "sent": "So I can show this conversions at the first thing.",
                    "label": 0
                },
                {
                    "sent": "The second thing that we want to prove in here I'll just give a simplified version, assuming gammas half again gamma is the importance of false positive versus false negatives.",
                    "label": 0
                },
                {
                    "sent": "Also, they are equally important, and then the assumption here is that if I sort my labels in decreasing frequency, so I put the most frequent label 1st and then the second one and so on based on the output of the initial classifier, then they'd be this power law.",
                    "label": 0
                },
                {
                    "sent": "So if they're bare parallel like this.",
                    "label": 0
                },
                {
                    "sent": "With the coefficient R between zero and two.",
                    "label": 0
                },
                {
                    "sent": "So basically if they decay if the frequency of the labels decays fast enough, then I know that this typical risk of a pruned classifier under my algorithm is going to be smaller than the original risk that I started with the initial classifier by some again positive number minus this term.",
                    "label": 0
                },
                {
                    "sent": "So so this is zero, just means that the gap is positive.",
                    "label": 0
                },
                {
                    "sent": "So for this to be 0 it means that M has to grow just a little bit faster than K to the power of 2 Sr.",
                    "label": 0
                },
                {
                    "sent": "So for instance, imagine R = 1.",
                    "label": 0
                },
                {
                    "sent": "It just means that K has the grow just slightly less than linear.",
                    "label": 0
                },
                {
                    "sent": "If R is bigger than one, then K can grow linearly with M. So I just want to point out you know, are these assumptions reasonable at all?",
                    "label": 0
                },
                {
                    "sent": "So the sparsity assumption certainly is true.",
                    "label": 0
                },
                {
                    "sent": "For instance, in Wikipedia we have 1 1/2 million labels, but each individual pages may be labeled by 20 or 30 things.",
                    "label": 0
                },
                {
                    "sent": "So this first assumption holds very trivially.",
                    "label": 0
                },
                {
                    "sent": "The second one about.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This power law so I just took Wikipedia here and I just plotted the frequency of each category.",
                    "label": 0
                },
                {
                    "sent": "So just this is the most frequent one in the second most frequent, and so on.",
                    "label": 0
                },
                {
                    "sent": "And just we show both axes here in log scale.",
                    "label": 0
                },
                {
                    "sent": "And when you do that, you'd expect the power law to just look like a linear line and the slope of the line will be the exponent of the power law, and in this case you know doesn't look like a linear line, but it certainly is upper bounded by linear line, and this slope of this line is something like 1.6.",
                    "label": 0
                },
                {
                    "sent": "So Wikipedia itself satisfies a parallel with R = 1.6.",
                    "label": 1
                },
                {
                    "sent": "So for Wikipedia we can certainly have.",
                    "label": 0
                },
                {
                    "sent": "The number of labels grow linearly or even faster with the number of exam.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll I'm at a time so I will only briefly mention this.",
                    "label": 0
                },
                {
                    "sent": "I mean, we did some pretty serious experiments here.",
                    "label": 0
                },
                {
                    "sent": "It looks like a silly little plot, but we actually did label the entire Internet.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a non trivial engineering task.",
                    "label": 0
                },
                {
                    "sent": "What we show here is that for different values of gamma, how our algorithm does its the blue line versus the purple line, which is just the pre printing.",
                    "label": 0
                },
                {
                    "sent": "The original thing that we get just by propagating labels across this click graph.",
                    "label": 0
                },
                {
                    "sent": "So the reason why it looks like this is we basically had a test set also taken as a random subset of Wikipedia and we picked that set and we saw which labels we should prove, which we should make sure we looked at the test set and got this gold standard of what's the best we can do by just putting out these labels.",
                    "label": 0
                },
                {
                    "sent": "And here we just show the ratio of our loss with that optimal loss.",
                    "label": 0
                },
                {
                    "sent": "Well being at 1 means that were.",
                    "label": 0
                },
                {
                    "sent": "Good as the Oracle that knows what the test set is in our algorithm approaches that from a wide range of gamma, whereas the previous original algorithm sucks pretty badly, and then this black line.",
                    "label": 0
                },
                {
                    "sent": "Forget about that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll conclude.",
                    "label": 0
                },
                {
                    "sent": "It was a very very simple algorithm, so it was pretty obvious what you want to do.",
                    "label": 0
                },
                {
                    "sent": "You want to do something to the distribution, so instead just do it to your sample.",
                    "label": 0
                },
                {
                    "sent": "So the obvious thing works, but certainly not for the obvious reasons, so the complexity and analyzing this kind of exposes that under the hood there's a lot happening here.",
                    "label": 1
                },
                {
                    "sent": "It's not as simple as it look.",
                    "label": 0
                },
                {
                    "sent": "There's actually statistically there's a lot of complexity happening here under the hood.",
                    "label": 0
                },
                {
                    "sent": "The important assumption again is K goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So the fact that the labels it grows and basically violates a lot of the analysis that we see out there.",
                    "label": 0
                },
                {
                    "sent": "I mean most of the standard multi class analysis will not work with this assumption and that's why.",
                    "label": 0
                },
                {
                    "sent": "Our analysis is kind of unique and the reason why it does work.",
                    "label": 1
                },
                {
                    "sent": "I mean, the main thing that drove that drove us here is that our analysis is not an extension of an analysis for binary classification, so a lot of these analysis that we see for multiclass classifications are direct extensions of an analysis of learning theoretic analysis for binary classification and that kind of implies that you want this per label convergence.",
                    "label": 0
                },
                {
                    "sent": "You want uniform convergence across each and everyone of the labels, and that's not going to work when K goes to Infinity and the last thing I want to say is about you know where we can go with this.",
                    "label": 0
                },
                {
                    "sent": "So I just talked about the very simplest way of kind of fixing up.",
                    "label": 1
                },
                {
                    "sent": "Broken classifier using a principled way of removing or putting away labels.",
                    "label": 0
                },
                {
                    "sent": "But we can talk about more complex operators.",
                    "label": 0
                },
                {
                    "sent": "For instance, we can maybe think of instead of removing the label, we can say maybe we can merge two labels that seem to be synonyms to us based on some evidence.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we can substitute.",
                    "label": 0
                },
                {
                    "sent": "We can think of rules that say every time that the classifier the initial classifier, wants to output a.",
                    "label": 0
                },
                {
                    "sent": "Maybe put B in its place and that will improve empirically.",
                    "label": 0
                },
                {
                    "sent": "And hopefully we'll be able to extend this analysis analysis and show that that will also improve our performance expectation over the distribution.",
                    "label": 0
                },
                {
                    "sent": "That's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}