{
    "id": "q57dbadons3hmmc5toezyxei6lpdy27l",
    "title": "Parallel Online Learning",
    "info": {
        "author": [
            "Nikos Karampatziakis, Department of Computer Science, Cornell University"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_karampatziakis_pol/",
    "segmentation": [
        [
            "Hi, I'm Nicholson.",
            "This is."
        ],
        [
            "Joint work with John and Daniel so.",
            "Where I'm talking about parallel online learning?",
            "What's online.",
            "Learning?",
            "It's a protocol where a learner gets an example.",
            "It makes a prediction, then it gives the label and suffer loss, then updates itself.",
            "So as you have all heard in this workshop today, it's simple, it's fast.",
            "And the updates are usually you have a linear predictor so.",
            "You just do something like gradient descent, in which case if your losses convex, say, is optimal.",
            "And as a lot of people have said, in this workshop, online learning is.",
            "Linear algorithm in the number of examples, but it's also sequential.",
            "So when you have a huge data set you want to distribute somehow the prediction and the update and what we're looking at is ways of.",
            "Doing this while minimum."
        ],
        [
            "Using the communication.",
            "So.",
            "And as a lot of other speakers have said, when you parallelize an algorithm, you have this problem with deley you have some kind of you have to do some communication to.",
            "To reconcile the different updates that you get from it's.",
            "It's a CPU.",
            "This is exacerbated so mostly people have talked about casting the stochastic setting, but many times you have temporally correlated or even adversarial examples.",
            "So we're looking at this setting a little bit so we will talk about schemes that try to minimize delay."
        ],
        [
            "So the way we do this is.",
            "Instead of splitting our data set in.",
            "Or horizontally like it's a it's if it gets a different set of examples, we just look at it, see if you're getting a different set of subsets of the features.",
            "And then these features are combined in some hierarchical way with a linear predictor or whatever model you want to put a plug in here.",
            "So it's if you compute some some prediction sends it to higher level an.",
            "Then this also makes a prediction based on its inputs.",
            "Now when when you get the final prediction at the top, there are different ways of updating your model."
        ],
        [
            "One way that has nodelay is you just.",
            "Send a prediction and then every node makes makes an update as if it was trying to predict the label, it won't.",
            "OK, Ann.",
            "Um?"
        ],
        [
            "This is like a model that has.",
            "Representational power between naive Bayes and centralized linear model.",
            "This can help or hurt.",
            "In some experiments, it helps in some it hurts.",
            "We also experimented with the global updates which do something like back proof on this network and for details and experiments, please come to the poster.",
            "It's over here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Nicholson.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint work with John and Daniel so.",
                    "label": 0
                },
                {
                    "sent": "Where I'm talking about parallel online learning?",
                    "label": 0
                },
                {
                    "sent": "What's online.",
                    "label": 0
                },
                {
                    "sent": "Learning?",
                    "label": 0
                },
                {
                    "sent": "It's a protocol where a learner gets an example.",
                    "label": 1
                },
                {
                    "sent": "It makes a prediction, then it gives the label and suffer loss, then updates itself.",
                    "label": 1
                },
                {
                    "sent": "So as you have all heard in this workshop today, it's simple, it's fast.",
                    "label": 0
                },
                {
                    "sent": "And the updates are usually you have a linear predictor so.",
                    "label": 0
                },
                {
                    "sent": "You just do something like gradient descent, in which case if your losses convex, say, is optimal.",
                    "label": 0
                },
                {
                    "sent": "And as a lot of people have said, in this workshop, online learning is.",
                    "label": 0
                },
                {
                    "sent": "Linear algorithm in the number of examples, but it's also sequential.",
                    "label": 0
                },
                {
                    "sent": "So when you have a huge data set you want to distribute somehow the prediction and the update and what we're looking at is ways of.",
                    "label": 0
                },
                {
                    "sent": "Doing this while minimum.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using the communication.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And as a lot of other speakers have said, when you parallelize an algorithm, you have this problem with deley you have some kind of you have to do some communication to.",
                    "label": 0
                },
                {
                    "sent": "To reconcile the different updates that you get from it's.",
                    "label": 0
                },
                {
                    "sent": "It's a CPU.",
                    "label": 0
                },
                {
                    "sent": "This is exacerbated so mostly people have talked about casting the stochastic setting, but many times you have temporally correlated or even adversarial examples.",
                    "label": 1
                },
                {
                    "sent": "So we're looking at this setting a little bit so we will talk about schemes that try to minimize delay.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we do this is.",
                    "label": 0
                },
                {
                    "sent": "Instead of splitting our data set in.",
                    "label": 0
                },
                {
                    "sent": "Or horizontally like it's a it's if it gets a different set of examples, we just look at it, see if you're getting a different set of subsets of the features.",
                    "label": 0
                },
                {
                    "sent": "And then these features are combined in some hierarchical way with a linear predictor or whatever model you want to put a plug in here.",
                    "label": 0
                },
                {
                    "sent": "So it's if you compute some some prediction sends it to higher level an.",
                    "label": 0
                },
                {
                    "sent": "Then this also makes a prediction based on its inputs.",
                    "label": 0
                },
                {
                    "sent": "Now when when you get the final prediction at the top, there are different ways of updating your model.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way that has nodelay is you just.",
                    "label": 0
                },
                {
                    "sent": "Send a prediction and then every node makes makes an update as if it was trying to predict the label, it won't.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is like a model that has.",
                    "label": 0
                },
                {
                    "sent": "Representational power between naive Bayes and centralized linear model.",
                    "label": 0
                },
                {
                    "sent": "This can help or hurt.",
                    "label": 1
                },
                {
                    "sent": "In some experiments, it helps in some it hurts.",
                    "label": 1
                },
                {
                    "sent": "We also experimented with the global updates which do something like back proof on this network and for details and experiments, please come to the poster.",
                    "label": 0
                },
                {
                    "sent": "It's over here.",
                    "label": 0
                }
            ]
        }
    }
}