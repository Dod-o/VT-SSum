{
    "id": "qcovmvbo3wl5p3d34mehpjp4hae4nyrh",
    "title": "Stacks of Restricted Boltzmann Machines",
    "info": {
        "author": [
            "Honglak Lee, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_lee_boltzmann_machines/",
    "segmentation": [
        [
            "Give a tutorial about IBM's and also stacks of RBM's, by the way, the the talk is designed to be maybe a little bit shorter than one and half hour, so you are welcome to interrupt me whenever you have any questions, so maybe will go a little bit slowly and also since I uncovered IBM, I may be best forwarding that part.",
            "So the main part I'm going to cover is the concept of deep belief network so.",
            "Although maybe nowadays just supervised deep neural net has been more popularized with the big data, I think it's good to just review some concept of deep belief Nets, especially some probabilistic perspective an as well as some some theoretical justification.",
            "So maybe that will be one.",
            "Take home message out of this lecture.",
            "And I'll talk about also talk about some applications of deep belief Nets as well.",
            "More like a supervised.",
            "OK, so I'll try to speak a little bit louder.",
            "Actually, can you hear me back in the back?",
            "Not really.",
            "Is microphone working actually?",
            "Oh OK, I see alright.",
            "OK, so I'll try my best to speak up OK. Alright, so.",
            "Let"
        ],
        [
            "So start."
        ],
        [
            "So in IBM, basically we want to represent the data using some kind of dictionary, so this is maybe one example where you look at the natural images.",
            "And learn some basis vector.",
            "So let's say we take them 14 by 14 Patch.",
            "And learn maybe 100 or 200 basis vector and you are trying to represent it as a sum existence of non existence of these basis vector as a combination.",
            "So for example this Patch which is 14 by 14 pixels can be represented as one or zero vector representing whether some pattern exists or not exists in this Patch.",
            "So arguably this is a good representation because you are abstracting from the pixel level to this qualifications.",
            "Well so called agitator OK.",
            "So arguably these are more compact and also more easily interpretable.",
            "So this is 1 intuition about what can be learned by RBM's."
        ],
        [
            "And we can also think about some stacking these PBM's into higher layers.",
            "So when you learn this first layer and represent this Patch as a coefficients.",
            "Just pointing to this basis vector, you can also learn the second layer.",
            "VM then learn some dependencies between this first layer coefficients and this can be modeling some higher order statistics of these edges."
        ],
        [
            "So maybe I'll just Fast forward this part because Aaron should have covered this in fair fair amount of details.",
            "So in GBM we have visible units and hidden units.",
            "So here I'm going to just talk about binary cases and we represent using.",
            "Basically, the energy function an the proper joint probability is just a I undirected graphical model represented by this energy.",
            "So let me just give you some intuition.",
            "Maybe this may be useful as a quick recap so we can.",
            "Oh yeah, so I'll just.",
            "Stand this way.",
            "OK, alright so we can think about IBM energy function in this decomposition.",
            "So I color coded so different colors for different basis vector.",
            "An, for example, W one correspond to basis vector for this first hidden unit and blue correspond to the second unit and so on.",
            "And essentially what it will."
        ],
        [
            "It means is that if you think about this first time, you are basically taking the inner product between this W one vector and the input data an.",
            "Intuitively, this is trying to find some match, or it can be viewed as a sum pattern matching.",
            "So when you have high correlation between W one and V, which means that the filter W one matches the pattern in this V, then this inner product will be higher an in order to minimize the energy which correspond to high probability state.",
            "Basically, GBM will encode some high probability for H1.",
            "So just as a intuition, I mean basically W. One can be viewed as a athlete, actor or some kind of filter.",
            "If the filter matches the input data, then you assign high probability for activating H1 and you do the same thing for W2.",
            "So you just take the inner product between WMV an if the value is high, it means that it's highly correlated, so you give high probability for activating H2.",
            "Otherwise you set it to 0.",
            "With high probability as well.",
            "So basically you can think about some kind of independent.",
            "These are not these terms, and essentially it turns out that procedure can be also."
        ],
        [
            "So represented as a factorized distribution, which means that you just think about this posterior probability of HJ being being equal to 1 as a some kind of independent.",
            "Event so and it turns out that this probability can be simply sigmoid function of inner product up W vector and some bias added to that.",
            "And this is basically some more compact way of writing this.",
            "An it corresponds to basically a sigmoid neural net.",
            "That's another reason why RBM has been also used as a initializing.",
            "The deep neural network and also talk about that in the case of deep belief network as well.",
            "OK, so this is how we do influence and influence is extremely simple.",
            "You just take the inner product between the vector W vector WJ, vectors and RV and add bias and take the sigmoid nonlinearity.",
            "And this P of H given V can be viewed as a feature representation.",
            "An since the energy function is symmetric, you can also do the same type of influence for estimating V invisible unit conditioned on the H, and it takes the same form.",
            "And that's how you do info."
        ],
        [
            "And for real value data is so."
        ],
        [
            "Small sample variability, some variation here, but essentially the idea is very similar, but the difference is that you take you add some quadratic term to encode the fact that these D take the real values.",
            "OK, so I'm going to just move on for this real value case, I'm just I just want to say that it's actually fairly straightforward to also modeled IBM when the we've we take the visible real values, OK?"
        ],
        [
            "Alright, so conditional distribution is very easy to estimate.",
            "It is simply just some sigmoid sigmoid function of something or some simple form of inner product between WMV or H and you can see the previous slide and you can do alternate Gibbs sampling.",
            "Just repeating this P of Y given H&POH given V. So joint distribution can be estimated using this alternate Gibbs sampling, and it turns out that if you run this gift chain infinite steps then it will recover the equilibrium distribution.",
            "Of course, that's maybe not that practical, but this is how we do influence in GBM as a principle, and this is a pseudocode of how we do.",
            "For example, how we sample from this joint distribution of the model.",
            "So graphical is just alternating between sampling H while fixing D, An sampling V while fixing H and so on.",
            "Alright."
        ],
        [
            "So yeah, I'm going to maybe skip this part so you can do maximum likelihood training.",
            "And since the gradient is difficult to calculate, you can do some sampling based approximation.",
            "And essentially after you calculate the gradient it is just a gradient stochastic gradient descent for optimizing the parameters."
        ],
        [
            "So how to calculate gradient is the key.",
            "But yeah, this is the contrastive divergent San I'm going to skip this part.",
            "Alright."
        ],
        [
            "OK, so."
        ],
        [
            "Yeah, just to recap, when we do stochastic gradient using contrastive divergent, basically the learning rule looks like this.",
            "So gradient of the this log likelihood with respect to the weight.",
            "It is some product between input and the procedure P of HJ given V. That's some sigmoid function of W and the inner product of WMV plus some bias.",
            "And you do the same thing, but here this time is estimated from the contrastive divergent by running the gifts gift sampling K steps.",
            "Alright, so.",
            "OK, some details.",
            "So typically we use mini batch for doing this gradient estimation an."
        ],
        [
            "Lamentacion is very easy and yeah, I'm going to skip this part because Aaron should have covered this so there are some improved way of doing this.",
            "Gradient estimation such as contrastive persistent CD and some variants and also there are some other algorithms such as score matching that also try to make this calculation more tractable.",
            "Alright, so."
        ],
        [
            "I'm going to maybe."
        ],
        [
            "Talk about this.",
            "Some variation of this RBM's so.",
            "One intuition is that IBM is unsupervised learning algorithm and sometimes we need a good regularization.",
            "So for example, if we want to learn a very large number of basis vector then we would have overcomplete basis and it is actually difficult to learn a good model and so in that type of situation using some regularization is useful.",
            "So one useful.",
            "Regularization is sparsity, so the basic idea is that we enforce the model to have sparse representation, so that activation on average is sparse.",
            "So you can think about this as a penalized version of the log likelihood, so you have the log likelihood plus some penalty based on the sparsity.",
            "And.",
            "Spicy can be evaluated in an empirical way.",
            "So for example, you calculate the sample activation of JT unit across all the training data and then you have maybe a target sparsity such as maybe 5% or 10% of these activation.",
            "Our end here.",
            "For example, this sample can be a sample.",
            "Activation can be just defined as a average over all the hidden unit activations in the training data.",
            "And so this is so basic idea."
        ],
        [
            "And this type of regularization can be useful in learning so called sparse spaces.",
            "And.",
            "And here so I'm showing the basis vector and you see some kind of pencil patterns and what it means is that.",
            "Basically you are trying to represent the input data using some spice combination of these pen strokes.",
            "So you can think about each of these Patch in the training sample as a sum combination of these basis where the coefficients can be viewed as zero or one.",
            "An yeah, in often cases by City actually helps for training the PBM, so it has been used quite a lot in many scenarios."
        ],
        [
            "And here's another variation of IBM called vectorized RBM.",
            "And the basic idea is that in the case of Gaussian RBM, the conditional distribution of V given H is a simply just independent Gaussians.",
            "So product of independent Gaussians, which doesn't model the dependency between these, like some correlation or pixel values, maybe corresponding to nearby nearby location, and so the basic idea is that you model this correlation.",
            "You know better way an it is some modification of original IBM, but the basic idea is that you include some kind of covariance matrix which is non Gaussian an you can model this covariance spaces over this matrix as or inverse covariance matrix as some combination of basis vector with some.",
            "Some diagonal diagonal component and.",
            "So here is a graphical representation an for this talk.",
            "I'm not going to go into details, but the basic idea is that you can think about some kind of covariance modeling where this H is a coefficient anti some basis, so the inverse covariance can be modeled as this some combination of basis vector."
        ],
        [
            "So using this type of model you can actually do better modeling of natural images.",
            "So for example, if you just so Gaussian RBM, then these are the.",
            "Samples from the model and you see that.",
            "The local correlation of the pixel is not very well preserved.",
            "Here are some variation of the Gaussian RBM where you connect the between the visible units and it seems to improve this local correlations better and it has been shown that using this covariance RBM you can actually arguably generate more realistic samples.",
            "So you see that these patches in the top show some.",
            "Kind of more fine grained local correlation and also longer edges and so on.",
            "So here's just some variation of this RBM."
        ],
        [
            "Um?",
            "And I'm going to talk about this stacking.",
            "So how to stack this as a deep lift network?"
        ],
        [
            "So deep belief net is a probabilistic generating model with this multiple layers so.",
            "The main idea is that you can actually use stacking of RBM's as a good initialization.",
            "So this model has been quite asked, has been quite popular over the years.",
            "Maybe although nowadays is slightly less popular because of just purely supervised training from large amount of data.",
            "But still this DBN training can be useful when you don't have large amount of supervision.",
            "So and also it's a useful thing to know as some maybe theoretical perspective and a little bit details.",
            "So there's an interesting fact about this DBN.",
            "Where stacking the IBM can be viewed as maximizing the lower bound of the likelihood, and you have some very nice theory and I'm going to talk a little bit about that so that you can get some idea behind what it means to do this, what it means with this theoretical justification.",
            "In practical sense, there are two ways of training this.",
            "Deep belief Nets.",
            "One is called a generative training.",
            "There are some algorithm called up down algorithm that's used for fine tuning as a probabilistic model, but maybe I'll also in more practical sense.",
            "I should say that people often just convert the deep belief net as a just a neural net.",
            "So since the activation fee for the activation of the belief net almost looks the same as just a sigmoid.",
            "Neural net you can just straightforwardly convert that into a deep neural net and do some back propagation.",
            "Ann, I should say that probably this discriminated backpropagation is.",
            "Much more popular, so majority of the papers that actually talks about DBN is actually mostly talking about this discriminated version of.",
            "Discriminative training of this DBN as our neural net."
        ],
        [
            "So here's a structure of the belief networks.",
            "Anne.",
            "It's actually good to be maybe noting the existence of arrows or direction of this arrows here.",
            "So in the top layer.",
            "We see that this connection is undirected, so there is no error between this H2 and H3.",
            "So this top 2 hidden layers are modeled as a restricted Boltzmann machine RBM.",
            "And the bottom layers are modeled as a sigmoid sigmoid belief net or just some kind of top down conditional probability using sigmoid function as a nonlinearity.",
            "So this is a full specification of these deep belief net, so you can think about this DBN's POV, an D, H1, H2.",
            "So just modeling the full joint distribution.",
            "And basically you first multiply this top layer joint distribution PR.",
            "For example here H2 and H3.",
            "So this is the first joint distribution modeled by the PBM, and you have H1.",
            "Given H2 and PV given H1.",
            "So where this P of H L -- 2 given H -- 1 is some kind of sigmoid function."
        ],
        [
            "So.",
            "So basically you can.",
            "You can think about this as a a generative model.",
            "First you sample this top 2 hidden layers, H2 and H3, and since this is an undirected graphical model, you can basically sample from this model by doing alternate Gibbs sampling.",
            "Actually did uncover about how to sample from undirected model.",
            "OK, good so yeah, so you can just think about the alternative sampling for many iterations an assume that it has reached to some approximate equilibrium distribution between H2 and H3, and that's how you first start with sampling from the DBN.",
            "An after you sample from this top two layers.",
            "Then you can just fix the H2 and then generate H1 given H2 and then also generate V given H1.",
            "So that's the top down generated process of this model.",
            "So here this top down generating model part can be viewed as just the sigmoid function of some W transpose H plus some bias.",
            "So it is pretty much the symmetric form of doing this influence in the reverse way for the case of GBM.",
            "And it turns out that this specific architecture, or this combination of undirected part and directed part, is actually very subtle but important fact in theoretical justification.",
            "So any question about this so far?",
            "Alright, so so this is how we do generative in generative sampling from this DBN.",
            "An I'd like to talk about how we do some influence an in fact doing the exact inference.",
            "Percier influences intractable because it is basically a graphical model from top to down.",
            "So when you do posterior inference, it means that you are given input data such as image V, and you want to estimate some posteriors of this H variables and you are trying to reverse this direction.",
            "In this graphical model, Ann is actually known to be intractable in general.",
            "And also this type of problem is called explaining away so because because this higher load factors are explaining the input data and when you are trying to do procedure then there is a complex dependency between this H1 and H2 and H3.",
            "So you cannot simply just do some independent posterior estimation for individual hidden unit or do feed forward in influence either.",
            "So still basically the same kind of heuristic.",
            "Or maybe you can think of also as a variational approximation perspective is simply just still approximating this posterior of H as a sum feedforward function.",
            "So not surprisingly, since we have seen that we do pursue estimation of IBM using the sigmoid function.",
            "We can just assume that there is a posterior or approximate distribution of this H variables that's written as simply just product top sigmoid function of W times some lower layer activation.",
            "So essentially you multiply some W, one with D and add some bias and take the sigmoid function and you just treat this perceived as a factorized.",
            "There is 1 where you simply multiply the sigmoid probabilities for the first layer and then you do the same thing for the second layer and just repeatedly do this type of inference.",
            "And of course this is yes.",
            "So this is an assumption or is some.",
            "You can also think about this as a variational approximation.",
            "So the main idea is that in graphical model, whenever you have intractable influence, the often the useful trick is to just approximate the posterior using some simple distribution.",
            "And then you try to optimize something or you try to learn the parameter so that it can.",
            "This approximate distribution actually can match the actual posterior.",
            "So of course this is natural as true posterior distribution, but this is approximate distribution and you can also think about this as a variational approximation.",
            "So, um.",
            "This is how we do sampling an inference in deep belief Nets.",
            "So it's actually somewhat somewhat complicated because you have this generated part and generated part needs top down sampling at the top, which requires multiple generations of Gibbs sampling, and then you top to bottom generation of the input data.",
            "Which is actually fine.",
            "I mean, once you have learned the parameter W1W 2W3, doing this sampling from the model is actually not a problem.",
            "However, basically when you do post your influence, this part is actually a big issue.",
            "Because there's really no guarantee that this approximate distribution is close to the true posterior."
        ],
        [
            "However, I'm going to show that how this type of specific network architecture and greedy training can actually lead to some interesting theoretical justification of training this deep architecture.",
            "So here's the algorithm so called greedy training.",
            "Or you can think about this as a stacking RBM's.",
            "So in the first layer you start from the input visible unit and then just train the first layer IBM.",
            "An you can use whatever algorithm, maybe contrastive diversions or persistent, consistent, persistent CD, and so on.",
            "So you learn this first layer.",
            "DBM"
        ],
        [
            "And then you can think about calculating some posterior distribution of H1 for the IBM an photo case of GBM.",
            "This posterior distribution is simply just factorized product sigmoid.",
            "W 1 * V OK.",
            "So you sample or just feed some input H1 as a as a secondary training.",
            "So the idea here is that I mean this is actually just a theoretical construction to just show that it is possible to show some relationship between the first layer IBM and the second layer deep belief Nets, so the theoretical construction here is that you.",
            "Basically copy the number of visible units.",
            "For about 2, the number of hidden units in the second layer.",
            "So you just mirror the structure in the second layer and also users mirror this one and just initialize as the wait for the second layer as well.",
            "And.",
            "And if you think about this way, then you can basically just train the second layer using as another RBM and then repeat this procedure.",
            "So maybe just going before you go back to some theoretical justification.",
            "I mean, maybe just let's just recap the algorithm, so we simply just train the RBM in the first layer.",
            "And calculate this procedure as a sigmoid function of W times W 1 * B plus some bias.",
            "And that's the H1.",
            "And then you take this H1 as an input and train another layer of RDN."
        ],
        [
            "And then you repeat this procedure.",
            "And we just defined this.",
            "Whatever sigmoid function of W * V as a posterior inference for this feedforward procedure.",
            "And that's how we do greedy training of deep lift Nets.",
            "So it looks actually very simple heuristic.",
            "We just basically repeat this recursive training RBM.",
            "But it turns out."
        ],
        [
            "There's some interesting theory here.",
            "So.",
            "So the basic idea here is that.",
            "We just got up.",
            "We can basically copy this visible Unit V into the second layer."
        ],
        [
            "And.",
            "So here is a some theoretical justification.",
            "So basically we have the log likelihood of input data, so I use X.",
            "But you can also think about just the visible unit B.",
            "So the log likelihood of any probabilistic model can be written as this form.",
            "So yeah, I can also do some more derivation here, but.",
            "But basically you have low P of X that's larger than some.",
            "Some entropy of this posterior plus some additional terms here.",
            "Um?",
            "And it turns out that when we train this second layer with very specific initialization of W1 and W2, and also this network architecture described previously can be."
        ],
        [
            "And as some kind of.",
            "Some kind of unfolding this IBM into the second layer DBN and then do some continued training so.",
            "So this is a very special case where if you have W 2 equal to W one transpose, which means just mirroring the weight in the second layer.",
            "Which requires that the number of hidden you need in the second layer should be the same as number of visible units.",
            "An this lower bound actually is tight, which means that the first layer IBM and the secondary DBN is actually the same.",
            "That's how we can actually justify training this second level DBM further and show that this can actually improve the local likelihood.",
            "So this may sound a little bit."
        ],
        [
            "Two are dense, so maybe I'll just show some slightly more detailed derivation here.",
            "So this first I equality is basically a equality that holds for any probabilistic model, so T of V probably low probability of V is.",
            "Written as this term where this Q of H can be any distribution.",
            "OK, so this is so called variational approximation or variational also also called variational bound.",
            "So QH can be any distribution.",
            "But the basic idea is that here you look at the third term and the third time is the KL divergent between this Q distribution and posterior distribution of H given B.",
            "So which means that if you have a posterior distribution 80 of H given B an if that matches to your variational distribution Q of H, then this KL time will be small.",
            "Since the Cal divergences always positive if you just remove this scale, divergent, and then the remaining time should be less than the original summation, which means that this second line actually act as a lower bound of these low probability.",
            "Actually, can you raise hands if you are familiar with this variational bound?",
            "OK, so about half of half of you are familiar, so if you have if you have any questions I'm you can stop here.",
            "Stop me here and then.",
            "I mean, I can also go through some derivation as well.",
            "Yes.",
            "Huh?",
            "Is there a reason?",
            "So for the case of DBN.",
            "I think the main reason for mixing this obvious there is some very special reason of this kind of unfolding from the first layer.",
            "IBM to the second layer DBN and connecting this together.",
            "So I mean, that's why we talk about this variational bound.",
            "And also I like in this context of DBN.",
            "Generally speaking, for the case of undirected model, typically influence is done by Gibbs sampling.",
            "So although it can be expensive and you can do give, I mean skip sampling Alternatively, so it's not too bad influences, I should say relatively easier than influence interactive graphical model.",
            "So typically this variational bound is more useful when you deal with directed graphical model or some hybrid graphical model.",
            "Any other questions?",
            "Yes.",
            "Yeah, cave divergences always passive, so that's why you get the second line from the first line.",
            "Alright, I mean so the first line actually is.",
            "It is actually very simple equality so.",
            "So if you just plug in the definition of the KL divergent and then just some, all these together, then you actually get back to the 1st first equality.",
            "Let me see.",
            "Right, I can also just do some.",
            "Some derivation on the tablet, so let's see if I can just quickly try that, although some for some of you it may be very familiar.",
            "I think it's good to just go over this derivation very quickly, so I'm going to just do first line derivation.",
            "So in the first line, basically the right hand side.",
            "Can be written as key of QH log of T of each given B minus some of HQH.",
            "Log Q of it's an.",
            "If I just expand the definition of the KL divergent then it is QH log of QH, divided or?",
            "Divided by P of H given V OK.",
            "So I mean it's just a matter of some arithmetic.",
            "So essentially this queue of H Lo que of H and this time actually just cancels out.",
            "So you just have some Asian of HQ of H. Log P of age given B minus some of HQH log up PO.",
            "He's given me so, so it is a simple simple cancellation of terms, and if you look at this.",
            "Fine.",
            "Basically P of H, V is a joint distribution of HV anti of H. Given these, the posterior of H given be so.",
            "It can be just return as summation of HQH log of tier of age given D /, P of TAH, V / P of H given V. But this this term essentially is just PAVI mean just the definition of the this conditional probability right so?",
            "So you get a local POV.",
            "So since the V is a fixed vector, so you can think about this as a sum fixed constant, whereas the Q is any it can be some variable because we don't know what the Q distribution is and it's actually it's a variational distribution.",
            "So But anyway this is a constant, so some summation over this.",
            "So at the end you can show that this right hand side.",
            "Right hand side is the basically P log of TV.",
            "So.",
            "To just recap what I have shown is that if you just do some arithmetic on the right hand side, you just do simple calculation and it turns out to be the same as the low probability of the input V. So I mean, this is a very useful trick in variational approximation or variational inference, because the conclusion here is that the left hand side is the same as the right hand side regardless of what the Q distribution is.",
            "OK, so this first line is true for any Q distribution.",
            "But basically, since the KL divergences always positive, you basically take this out, then you get a lower bound and that's the second line.",
            "So in practice we always want to find the Q distribution.",
            "That's a close approximation to this procedure, so that can be also viewed as a finding a tight lower bound of this likelihood, OK?",
            "Right?",
            "So that's the second line.",
            "So maybe you can go back any questions here before we move on.",
            "Yes uh-huh yeah.",
            "Uh huh.",
            "Yeah.",
            "Always.",
            "So that's so great, yeah?",
            "So for the case of DBA, I think there is some mathematical reason behind it, but intuition wise I mean I mean essentially how people use DBN is typically just as a feedforward influence module, so people actually don't really care about.",
            "You know how these things are constructed as a mixup, undirected and directed, but I'm just talking about some theory behind this so.",
            "So.",
            "So yeah, DBN is just constructed with this mixup.",
            "Directed and undirected just for some theoretical reason.",
            "But also regarding your question, there is also algorithm Cody Boltzmann machine which is all undirected Ann.",
            "I believe Russell Russell could enough.",
            "We'll talk about that in great detail.",
            "So I'm not going to talk about it today.",
            "Both are not.",
            "Basically that's just another class of model and there are like another class of algorithms for doing inference and learning so.",
            "So there are some pros and cons of undirected versus directed.",
            "So, as I briefly mentioned, for directed model, inference is difficult because of doing the posterior inference where you have to actually deal with explaining away effect.",
            "So that's typically why we resort to some variational approximations, such as approximating the queue using some feed for distribution, even though it's not actually true in reality.",
            "For the case of.",
            "Undirected model you can do some mean field or Gibbs sampling, so I think it's slightly more approx.",
            "I mean more approximate than the true posterior.",
            "So typically.",
            "Yeah, typically you can do better approximate inference using undirected model, but at the same time when you do learning, then learning an undirected model is actually more difficult, whereas learning in directed model is actually much easier.",
            "So, but anyway, I think it it requires some more in depth discussion of this graphical model.",
            "So probably I will just defer that for more offline discussion.",
            "So let's just move on to this.",
            "Bound calculation so.",
            "So I just talked about this variational bound, which is true for any distribution.",
            "So for the context of deep deep belief Nets, we just want to model this Q distribution as a simple like product of sigmoid function.",
            "So Q of H is simply just the sigmoid of HJ equal to 1 given B.",
            "In fact, it is simply some kind of sigmoid of WJ transpose.",
            "B plus BJ?",
            "OK, so that's what we have seen before.",
            "So I'm trying to just connect between this IBM and EPN.",
            "So when we have this special condition satisfied, where W2 is equal to W 1.",
            "Then these two models are actually identical, so let me explain what.",
            "Let me explain this in more detail, so I IBM here means you just have a visible unit here and then you have first layer weights W one and users have this part as PBM, so.",
            "So this is the IBM model I'm talking about.",
            "And DBN is a a different model which is a product of this undirected part plus multiplied by this directed part.",
            "That's what I mean by DBN.",
            "But it turns out that when W2 is the equal to W one transpose, these are identical, and the reason here is that essentially the if you think about this.",
            "Pop two layers.",
            "You can just marginalized.",
            "So this is the undirected part so.",
            "Let me just write it down this part so this joint distribution can be viewed as PA V1V1 given H 1 * P of H1, H2, so that's that's the DBN distribution.",
            "So you first define the path to layer as undirected model and define the top to bottom layer using this conditional distribution.",
            "And since OK so here is H1.",
            "So if you are just marginalized, suppose that you are trying to marginalized the H2.",
            "Then you are just marginalizing this.",
            "H2 variable here.",
            "Then you can.",
            "You can only worry about this time so you are just marginalizing H2 for this joint distribution an it's just correspond to doing a marginal influence for the second layer RBM.",
            "OK, so summing over H2 for this joint distribution is essentially summing over the H2 in this upper part, which is the IBM an.",
            "If you have W 1 equal to W2 as a transpose, then this marginal distribution of H1 is the same for the IBM and also for the DBN.",
            "OK, so it's a very like special, contrived way of constructing this model.",
            "So the bottom line here is that now we can connect the log likelihood of the PBM and the local likelihood of the DBN an under this special special condition.",
            "These two are the same, so that's why we can somehow connect between this greedy training.",
            "Yes.",
            "OK, so maybe I can explain that slight more details so.",
            "Maybe I'll just put.",
            "So basically here we are talking about this type of model.",
            "Where the distribution of this P of H130 of V. 8182 it is the P of the given H 1 * P of H 1 * 8 Two an this is the IBM and this is just the sigmoid.",
            "Sigmoid belief Nets.",
            "So if you just think about this second layer RBM.",
            "So if you just think about this part.",
            "Then I mean, intuitively speaking, it is.",
            "Thus, if W2 is the same as W one transpose, it is basically inverting the PBM in Uptown Way.",
            "So just inverting the PBM by swapping the visible unit an hidden unit, so.",
            "So the actual marginal distribution of H1 by summing over all these H2, it is the same as marginal distribution of the original IBM by summing over Adobe.",
            "OK, so that's why basically summing over H2 basically just gives you the original IBM's prior, so P of H defined by the origonal RBM with www.1s08.",
            "So I mean so if you sum over the H2 here.",
            "Then use get piov.",
            "Be given eight 1 * P of eight one, but this one is the same as the original.",
            "Same as.",
            "P of eight, one from the RPM.",
            "OK, so if you have GBM for the first layer.",
            "Basically this this is the same as this when you have a. Huh?",
            "When W2 is equal to W, one transpose, maybe I can also talk more about this offline is is this clear or do you do OK?",
            "OK, any other questions here?",
            "Alright, so.",
            "Let's go to this slide again, so.",
            "So basically I have just talked about how these two models are connected, so PBM and the second layer DBN have the same log likelihood when it's initialized in this specific way."
        ],
        [
            "So.",
            "So now we can just think about what it means to train this in a greedy way, so I'm just expanding this times line by line.",
            "So the first time first line is the log likelihood of the DBN when we have marginalized all the H1 and H2 for the case of this special case where W2 is initialized with W one transpose, then it is the same as the RBM.",
            "So.",
            "So these two are the same and same log likelihood.",
            "And also if you apply this variational bound up the original form where in the original form we have this KL divergent term.",
            "But further case of IBM, you actually can do exact inference.",
            "So which means that when you define this QH as a sigmoid function of W, one transpose V plus bias, then this Q of H1 is exactly the same as this posterior of H1 given D. Because of this very special construction.",
            "So which means that these two times are the same as the third line.",
            "Well, this Cal time is simply zero OK?",
            "So which means that this lower bound is actually tight lower bound, because KL divergent is 0, which is actually a good thing.",
            "So then we can expand this third line to the 4th by just applying simple definition of conditional probability.",
            "So here what I did is simply just.",
            "Expanding Missy, expanding the first time, so joint distribution of H1, V as a product of prior distribution of H1 times the conditional distribution of V1 given be given H1.",
            "So this is some simple like expansion.",
            "So so basically this is what we get out of this variational bound and also the special condition that W2 is equal to W one transpose.",
            "Alright, so if you think about what it means to train the second layer.",
            "We can think about what is fixed in the second layer and what is not fixed.",
            "So Q H1 is simply defined as.",
            "Just as a recap, I mean QQ of H1 was defined as the sigmoid function of something where it only has a parameter dependence on W 1.",
            "Whereas if you are trying to train the second layer, we are going to just train the W2.",
            "So what it means is that we have initialized W2 as a W1 transpose, But now we are going to fix the W one, but now we unfreeze W 2 so we are going to fully change W2 starting from this initialization so W2 is a free parameter, free, valuable that you can change where is W one is fixed.",
            "So which means that Q distribution is fixed here.",
            "So you can treat this as a constant.",
            "Right, and similarly, this top down distribution appear V. Given H1 is also just dependent on W one, so it only depends on W2, but not sorry, it only depends on W one but not W2, so you can also treat this as a fixed and similar.",
            "This is fixed and also this is fixed.",
            "So if you think about what is not fixed, it is basically this distribution which can be viewed as a. Summation of H2 or P of this second layer, IBM.",
            "Of eight 1, is 2.",
            "So although it is written as a marginalized form, you can also just write it as a second layer.",
            "IBM with H1H2, there's some Dover.",
            "Some some the world.",
            "Age 2 and then the parameter for this secondary GBM is W2, so we are going to just up to some gradient descent to optimize W2.",
            "That will increase this quantity.",
            "OK, so essentially the second layer, training or DBN.",
            "Is its corresponding to just greedily optimizing the second layer, IBM, while the first layer parameters are all fixed?",
            "So basically, this construction shows that if you initialize the second IBM in this particular way, and then if you can find W2 that can find a better model than the initial model starting from W one transpose, then basically this will increase.",
            "So as a result you can show that this log likelihood of the secondary DBN will be higher than the original.",
            "Local I cleared at IBM.",
            "OK so it's actually a bit subtle and some what.",
            "Yeah, somewhat detailed derivation, but yeah, I wanted to cover this because main topic today is stacks of IBM, but it's often the case that people just treat this as a heuristic, but I wanted to show that there's some deeper some interesting mathematics behind it.",
            "So let me see, so let's just summarize."
        ],
        [
            "Nice setup, local message here is that you start from the IBM and then we initialize the secondary DBN by initializing W2.",
            "As everyone transpose and these are the same.",
            "As a result these two have the same log likelihood and also the same joint distribution if you just marginalized H2.",
            "And then what you do is to fix W one and only change W2 as a variable in the training.",
            "And that's exactly the same as training the second layer as PBM an.",
            "Because of this construction, we can actually show that if you do this greedy training, the log likelihood will be better than the original IBM.",
            "So.",
            "Yes.",
            "So great question.",
            "So in the second first layer to second layer, what?"
        ],
        [
            "What was actually very nice is that here the KL term is exactly 0 becausw.",
            "Because of this very special construction, it turns out, but when we do the third layer training, we don't have any guarantee that this Q distribution is actually correct.",
            "As a result, this this bound becomes actually is a lower bound and but there's no guarantees are tight, which means that essentially we can do similar type of derivation for the third layer to show that.",
            "The lower bound defined by this type of variational.",
            "There is an approximation will improve so we can show that were bound will improve, but there is no guarantee that actual look likelihood will improve.",
            "So that's actually one subtlety in this type of."
        ],
        [
            "You re so.",
            "To summarize, basically from the first second we actually have actual exact proof that the local I could improve for the 2nd third layer or higher layers.",
            "The lower bound will become better, but not necessarily the actual local local likelihood.",
            "So that's."
        ],
        [
            "Now actually we justify this greedy training of the this DBN as a stacking about IBM.",
            "Alright, so.",
            "Just to go back to some more big picture, what I talked about was basically some generated version of fine tuning.",
            "Or generated training at this DBN an in fact there are some other version where you actually have a supervision.",
            "So if you have class label Y and then you can also do some joint optimization of this joint.",
            "Likelihood of Y&X together.",
            "And there's some algorithm called up down algorithm is described by Hinton at all 2006 neural computation paper.",
            "But at the same time, I mean, when we do all these type of stacking about BM.",
            "Maybe in the most practical and straightforward usage of this type of network is to simply convert this as on your mat and do back propagation.",
            "An in fact there are lots of some mix up terminology, so when you read papers sometimes it says deep belief Nets, but it actually sometimes means is a DPS net converted into a neural net and train using back problem.",
            "So most of the time, actually that's what it means when people say is deep belief Nets.",
            "So there's some subtlety in terms of terminology.",
            "And I should say that this first version is way more practical than the second one for some some reason.",
            "I mean, maybe the first version is much simpler to just.",
            "To the surprise, so probably that's the reason and also the performance is actually quite good, so."
        ],
        [
            "Let me just talk a little bit more about the generative perspective and also talk about just discriminated back propagation.",
            "So here is the initial model that Hinton Arrow used for modeling the digits using deep belief Nets.",
            "So essentially you see that there is a class label, so ten class labels here and the top layer toddler can be viewed as a social code associated memory, but essentially it's just the IBM where you have class labels and some hidden units that are connected in undirected way.",
            "So without this label you can think about this as just regular IBM, but when you have class label you can just think about some concatenation to the visible unit where you always observe this class labels during the training time.",
            "So this top layer is undirected and you have a top down directive connection, so the green arrows correspond to the actual prime parameter of the model.",
            "So you have undirected connection on the top and top down connections in the lower layers and the Red Arrows correspond to variational parameters.",
            "So actually in the paper they say this green arrows as a generative model or generated parameter, whereas the red ones as a recognition recognition model or recognition parameters.",
            "Um?",
            "So the basic idea is that you essentially initially approximate this red, red directional influence of, which is a recognition unit as some sigmoid function of W, one transpose V, and also sigmoid of W2 transpose HH one and so on.",
            "So you just do some people influence and hoping that it will actually match the actual posterior.",
            "So in the paper they."
        ],
        [
            "Talk about so called up an algorithm that's designed to do this type of fine tuning.",
            "So in the undirected graphical model perspective may."
        ],
        [
            "We just going back to here.",
            "Essentially, this model is so called a chain graph, which is a very special instance of some graphical model where you have mixed up, undirected and directed models.",
            "So essentially, in order to do some learning for this chain graph, what you need to do is first do some posterior inference.",
            "Ann, you do procedure in France and then you do some gradient descent.",
            "Fickle tease that you cannot do actual exactly similar, so you actually resort to this approximation, parameterized by some.",
            "Fit for sigmoid, neural, net and.",
            "By the end you your goal is to do some posterior inference and then do some gradient calculation."
        ],
        [
            "And that's actually in essence, that's what this Uptown algorithm is trying to do.",
            "So this first part can be viewed as a some approximate version of this posterior inference.",
            "So it is basically assuming that with this bottom up pass, you actually get the posterior.",
            "Of course this is not exact procedure, but that's assumption and then you do some contrastive divergent training for the top layer and also for the bottom layers.",
            "Actually, training is very straightforward because it's a training for the.",
            "Directed graphical model.",
            "But anyway, yeah, I'm not going to go into details, but the high level idea is that you do you do posterior inference, but since the posterior inference is intractable, users approximate using this feedforward sigmoidal neural net and then do some contrastive divergent and some additional gradient calculation.",
            "So yeah, so let's say that's how we do this."
        ],
        [
            "Fine tuning.",
            "And uh, for.",
            "Sampling from the model I just talked about how to do this, which means that you do some Gibbs sampling for this top two layers and then generate this top to bottom directional."
        ],
        [
            "Sample generation and here are some examples of generating samples."
        ],
        [
            "And here are some results on this database, so at that time actually they show that generated training actually leads to a very competitive performance to the state of the art at the time, so.",
            "So that's."
        ],
        [
            "Some results an and.",
            "Here's some nice demo of this algorithm, so actually I don't have an Internet connection now, so I cannot show this demo, but it has some nice some animation where."
        ],
        [
            "Example, if you if you fix the one up fix this label neuron to one at the 10 digit class, then actually you can do this posterior inference by generating the hidden units and then also sampling from this pixels pixel images an either or basically hallucinate the actual digit image that that's kind of changing overtime cause given the fixed classloaders knows.",
            "Like 1 to one mapping from the class to the actual pixel image.",
            "So it will basically do some sampling over this class class, conditional distribution.",
            "And similarly, if you actually condition over the pixels pixel image, then you can do this inference and then you can infer about the this class labels as well.",
            "So one nice thing about this type of model is that you can do also some probabilistic inference, which means that if you have some power only the part of the input image observed, then you can do some bottom up and then top down inference in iteration and then you can basically fill out the rest of the missing image.",
            "So that's one actually interesting."
        ],
        [
            "Thing you can do using this type of."
        ],
        [
            "Probabilistic model."
        ],
        [
            "So."
        ],
        [
            "Best."
        ],
        [
            "How actually we can justify an understand this more like a generative version of the deep belief net?"
        ],
        [
            "I'm going to prove."
        ],
        [
            "You talk about stacking as a stacking these carbs as just a neural net.",
            "Maybe this is something that most of you are much more familiar with and this is also how things are done in practice.",
            "Most of the time.",
            "So the basic idea is that users.",
            "Think about this approximate feed for influence as just a neural net.",
            "Which is actually quite reasonable, cause the actual field for function is exactly the same as the sigmoid deep neural net.",
            "An basically you can just initialize the DBN and then do some back propagation and fine tune."
        ],
        [
            "Bing and so on.",
            "So for example.",
            "You can train this.",
            "Train the stacks of RBM from the handwritten digits and you train the first level BM Secondly RBM.",
            "And Thirdly IBM and then stack them together and then put the class label on top and then just treat this as a neural network.",
            "And then you just do some back propagation.",
            "Ann actually works quite well.",
            "Anne."
        ],
        [
            "Also, this type of network can be also used as a autoencoder.",
            "Which is useful for dimensionality reduction.",
            "So for example you do the same style of stacking greedy training and then put them together.",
            "But in this case you can unroll the this.",
            "Visible unit had hidden unit in this reverse direction, so essentially then it becomes the auto encoder motor encoder where you take the image and it converts into some hidden unit and then they sit in.",
            "It is used for reconstructing the original image.",
            "And if you put some bottleneck here, such as like small number of hidden unit in the middle, then this can be used as a dimensionality reduction and also can be useful for visualization as well."
        ],
        [
            "So it has been shown that through this type of auto encoding you can map the different class labels in much closer.",
            "Closer vicinities compared to just a simple like linear mapping.",
            "So this is basically the visualization or projection of some.",
            "This made small number of hidden units, so you see that same class examples are much more closer and also more separated from other class labels compared to just using the linear mapping."
        ],
        [
            "And also you can do some tuning up this with some additional objectives.",
            "So basically here you can think about open coding as a one objective and also you can think about some kind of similarity between examples from the same class or different class.",
            "So intuition is that you want the example from the same class to be close to each other and also you want the example from different class to be far away, so you can actually define some loss function that.",
            "Encode that type of similarity in this middle layer and then actually it can also provide even better separation between different classes and also can be useful for classification as well."
        ],
        [
            "So I'm going to talk about Convolutional network tomorrow, so I'm not going to go into details here, but you can basically also run model for some kind of.",
            "Feature hierarchy for images and audio and different sensory modalities.",
            "So here's one example.",
            "So given the input image, you can learn some first level features.",
            "That is encoding the image as a combination of edges, and then you can learn the second layer correlation, which includes some, maybe conjunction or combination of edges, and that can be used as a maybe higher level feature detector such as maybe corners and contours.",
            "And so on.",
            "So I'm going to talk more detail about this tomorrow, yes?",
            "Yeah.",
            "So I'm talking about some convolutional model, so it's not exactly just starting from Patch to this type of mapping, but maybe I'll talk more about this tomorrow, but in principle you can do.",
            "You can just take the Patch and learn the first layers are same Patch size basis and the secondary it will be just some combination of these first layer patches, so it's a little bit different, but in principle, so very.",
            "Similar idea, OK?"
        ],
        [
            "And you can also do greedy stacking up the IBM's for the speech data.",
            "So in this case you have some speech features such as MFC or spectrogram, and then users learn the first layer, second layer, and then so on and then stack the class table on top and then do some supervised backpropagation."
        ],
        [
            "And this is another example.",
            "So in this case you do stacking and instead of doing just a classification on top, you actually put the hidden Markov model on this top layer.",
            "So you can still do inference and learning.",
            "The only difference is that these features are learnable, so you can do some backpropagation to train this feature and so on, but essentially.",
            "It is actually trained fairly easily so.",
            "This type of model actually was used for making breakthroughs in speech recognition in 2012.",
            "An around the time.",
            "And."
        ],
        [
            "And there are some other applications such as human pose or some applying to multimodal data or facial recognition and so on."
        ],
        [
            "But it's actually fairly straightforward to apply this to many different scenarios.",
            "So just as a remark, I just say that.",
            "GBM is actually quite good generative model, although it's not probably state of the art, it can be used as a good model for learning distributed representation for handling some high dimensional data.",
            "And I talked fairly extensively about how this initialization VPN through the stacking of PBM's can be justifies in theory, but also maybe practical sense.",
            "If you didn't get all the details, the take home message is that it can be also used as just initialization for the deep neural Nets.",
            "And.",
            "It's actually known that for classification problem this DBN pretraining or GBM stacking can be still very useful.",
            "Even these days when you don't have lots of huge amount of labeled data, OK so for the case of when you have like millions of labels such as image, net or some other equivalent large database then probably this pre training may not be useful.",
            "But for for the case when you don't have lots of labels which may be actually the case when you maybe work on some new domains, such as maybe maybe medical or some health care problem, or maybe computational biology and so on, where you is not easy to get large amount of labels, then this type of stacking can be still very useful.",
            "And.",
            "And also we talked about how this DBN can be viewed as some performing some probabilistic inference.",
            "Let me see."
        ],
        [
            "See any questions here so far.",
            "So I think I'm pretty much done, but I'm yes.",
            "So I think.",
            "Yeah.",
            "Set up the various.",
            "Right, uh, yeah.",
            "Yeah, right?",
            "So that's a very good question, so I think in practice people don't actually constrain the number of second layer units to be the same as the visible unit.",
            "So people just use whatever arbitrary number of units for the second layer.",
            "So in that sense, theory doesn't hold in that type of situation.",
            "But it still works in practice, so yeah, yeah.",
            "Any other questions?",
            "OK, so here's another very interesting example of VPN that can be used for some probabilistic inference, so here.",
            "For example, the problem is that you are given.",
            "Some images but that are occluded, so you just provide the image where you only see the half of the image and then you are asked to info about the rest of the half.",
            "So if you just use just a simple feedforward neural network, is is actually difficult to do some reasoning about this missing values but using the VPN.",
            "If you just treat this as a probabilistic model, you can actually do some influence on this missing values, so this was done by Marco and runs at oh.",
            "The CPI paper where basically you take the input image and then you just do feedforward infants and then you calculate this top layer hidden variables and then you.",
            "You do generations so you just generate from this procedure so you will generate some reconstructed version of the space image an at that point.",
            "It will kind of fill in some missing values in the first iteration.",
            "And what they do is do this clamp for this observable, because you know, we know that these are just like observed.",
            "So we just fix this, but you use the hallucinated version of this missing values as an input to the second iteration of bottom up influence.",
            "So then that's the this third column, and then you repeat this bottom up and top down where you always clamp this bottom half an.",
            "At the end.",
            "You can basically hallucinate.",
            "The rest of the half of the image and it actually gives quite decent results, so this is actually one example of how you can use the DBN as some probabilistic model.",
            "So I should say that maybe further case where.",
            "Well, you don't have any ambiguity or missing value or some some confusing things in your data.",
            "Then probably feedforward influences good enough.",
            "So for example when you are just doing classification with noisy noisy listen, very unambiguous image, then people is probably fine.",
            "And then just doing some pure backpropagation as a neural net is good enough when you have some kind of missing values or some uncertainties or.",
            "Ambiguities in your observation?",
            "Then this type of problems influence is actually useful thing.",
            "Alright.",
            "Yes.",
            "I think that's actually a very good research question.",
            "I don't think people really solve that question.",
            "There are some work where, for example, using this type of model to generate some natural scene.",
            "Yeah, I think there are some some success where we can generate scenes or maybe small Patch like see 510.",
            "Level image I guess Alan should have shown some examples of image generation, but for the case of really large large sized image where you have my hundreds of hundred 100 * 100 pixels, and if you want to generate some some fairly detailed high resolution scene of objects and some natural like.",
            "Natural things, then I think that's actually an unsolved problem.",
            "It's actually very hard problem, so generally speaking, generating image or generated model of image is actually not that easy so.",
            "But maybe this type of model shows some promise that probably is also not the case that this is ultimately the right model to do.",
            "I mean to you so people are still trying to develop better ways of doing this type of generative modeling these days.",
            "OK, so let me see I."
        ],
        [
            "Also yeah, since we have a few minutes left, I want to just talk a little bit about some slightly different way of.",
            "Using deep are BMS for modeling I some distribution at the output space so."
        ],
        [
            "There's a kind of idea where.",
            "Nowadays, I mean, we know that this type of RBM's or distributed representation is very useful for modeling a complex distribution.",
            "And it has been very successful in dealing with complex input distributions, but when you want to work on some complex problems that deal with some out some output distribution that has large amounts of possibilities such as segmentation or detection for the case of computer vision problems, then maybe it's actually a good idea to actually also run our representation of these outputs.",
            "OK, so."
        ],
        [
            "So this type of problem, I mean, is known as a structural prediction problem.",
            "So for example, there are so many examples, but for example, if you just consider A1 specific problem."
        ],
        [
            "Such as image segmentation.",
            "This is how people solve the problem and maybe this also connects back to the undirected graphical model that are unexplained today.",
            "So given the input image, you can basically construct some graph, some graphical model where the in this case you can consider the hidden nodes as as nodes as a sum label for this segment or some super pixel and you connect between this node.",
            "Whenever these super pixels are adjacent an these label mode actually is connected to the feature, so some observation from the input image.",
            "So you have some Singleton potential from the superpixel feature to this label.",
            "And also there's some relationship between these adjacent labels.",
            "So the intuition here is that you want to enforce some local consistency.",
            "Where?",
            "If the two super pixels are adjacent, you want to assign same label for this superpixel.",
            "However, this type of model, although it's actually very standard and it's actually one very popular model used in computer vision, called the paralyzed condition random field.",
            "It doesn't actually infer some global consistency, and it's actually not that easy to do so.",
            "So what I mean by global consistency is some kind of global shape of this object, or in this case, is a face.",
            "So."
        ],
        [
            "No.",
            "Is actually a very sensible thing to try to model this output space using RBM or some variant variant of PBM, so.",
            "In this particular example, we can just think about GBM that's modeling this output space, so you can just treat this as a just pixel level image where you have some three type of outputs.",
            "So you can think about one hot encoding, encoding, skin or hair or background, and so on.",
            "And you can think about IBM that's modeling this stable space."
        ],
        [
            "So if we actually just few things together, you can think about this.",
            "Emma, another version of some hybrid graphical model where you are just stacking the conditional random field and putting the IBM on top."
        ],
        [
            "So in this case, this turns out to be just the undirected model.",
            "So since because the conditional random field is undirected and GBM is undirected, so we can just add the energy functions together.",
            "So here the why is the label that we want to estimate given the image.",
            "So we have energy terms coming from the conditional random field and also energy time coming from the PBM part, so that's.",
            "Interesting here is that the PBM will basically infer some global consistency of this label space that's difficult to do.",
            "So for the case of CRF.",
            "So these are actually complementary."
        ],
        [
            "And it turns out it turns out that this IBM part.",
            "So if you visualize the hidden unit on top, it can actually encode some kind of pattern of this outputs such as maybe having appeared or long hair, or some direction of the Phase I mean, which way the faces?"
        ],
        [
            "Looking and so on and also if you just sample from this top layer, you can actually do some generation of these samples an they actually look quite decent so the top row shows the samples generated from the model.",
            "The bottom row shows the closest match to the two.",
            "Some examples in your training data.",
            "So these are similar to each other but not identical, which means that the model is able to extrapolate from the training training."
        ],
        [
            "Data so that influence can be done actually quite straightforwardly.",
            "So basically, since the hidden in it and this label units are latent in the inference time, we can do alternating Gibbs sampling or being filled in France.",
            "So for example, if you fix the H, then you can just calculate the top down influence from this hidden units, and that infers some global shape.",
            "Constraint and then you can still do influence in the conditional random field.",
            "So when you fix the H, then influencing the why is exactly the same as the original conditional random field.",
            "An another iteration is to fix the wide variables and then do influence on the H and it is just the same inference as GBM that I just talked about.",
            "It is just a sigmoid function of some W times this Y, so you can just repeat this alternating sampling or mean field an basically just adding this top layer doesn't really add much complexity in terms of the influence it is using the exactly the same inference algorithm.",
            "For the original."
        ],
        [
            "RF but nice thing is they can actually infer some global consistency.",
            "So for example.",
            "If we compare this type of model where we have conditional random field with RBM.",
            "And if you see some baselines, such as just a logistic regression that only use a Singleton potential or the conditional random field that has paralyzed potential, we see that it makes some mistakes in the beginning and conditional random field tries to clean things up, but it actually doesn't successfully clean things up, because it only you don't need.",
            "Consider some local consistency.",
            "Whereas using this IBM as a popular as a shape prior and you can basically clean things up much better way 'cause it will try to penalize a lot when the segmentation looks actually unrealistic or it does if it doesn't look like a face shape and then it will.",
            "It will incur lots of penalties.",
            "So in the energy function.",
            "So as a result it tries to find a more consistent.",
            "Pattern that looks like an actual face."
        ],
        [
            "So there are some quantitative evaluations showing that this type of putting this top layer prior is a useful thing so.",
            "Oh yeah, this is just one kind of way of using this RBM or distributed representation for modeling the output space, But this was maybe some anecdotal thing I wanted to talk about because we had a little bit of time, but maybe just let's go back to yeah, I'm basically done.",
            "So if you have any questions I will be happy to answer and then also I'm happy to talk about this offline.",
            "OK, thank you very much, yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give a tutorial about IBM's and also stacks of RBM's, by the way, the the talk is designed to be maybe a little bit shorter than one and half hour, so you are welcome to interrupt me whenever you have any questions, so maybe will go a little bit slowly and also since I uncovered IBM, I may be best forwarding that part.",
                    "label": 0
                },
                {
                    "sent": "So the main part I'm going to cover is the concept of deep belief network so.",
                    "label": 0
                },
                {
                    "sent": "Although maybe nowadays just supervised deep neural net has been more popularized with the big data, I think it's good to just review some concept of deep belief Nets, especially some probabilistic perspective an as well as some some theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "So maybe that will be one.",
                    "label": 0
                },
                {
                    "sent": "Take home message out of this lecture.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about also talk about some applications of deep belief Nets as well.",
                    "label": 0
                },
                {
                    "sent": "More like a supervised.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll try to speak a little bit louder.",
                    "label": 0
                },
                {
                    "sent": "Actually, can you hear me back in the back?",
                    "label": 0
                },
                {
                    "sent": "Not really.",
                    "label": 0
                },
                {
                    "sent": "Is microphone working actually?",
                    "label": 0
                },
                {
                    "sent": "Oh OK, I see alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll try my best to speak up OK. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Let",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So start.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in IBM, basically we want to represent the data using some kind of dictionary, so this is maybe one example where you look at the natural images.",
                    "label": 0
                },
                {
                    "sent": "And learn some basis vector.",
                    "label": 0
                },
                {
                    "sent": "So let's say we take them 14 by 14 Patch.",
                    "label": 0
                },
                {
                    "sent": "And learn maybe 100 or 200 basis vector and you are trying to represent it as a sum existence of non existence of these basis vector as a combination.",
                    "label": 0
                },
                {
                    "sent": "So for example this Patch which is 14 by 14 pixels can be represented as one or zero vector representing whether some pattern exists or not exists in this Patch.",
                    "label": 0
                },
                {
                    "sent": "So arguably this is a good representation because you are abstracting from the pixel level to this qualifications.",
                    "label": 0
                },
                {
                    "sent": "Well so called agitator OK.",
                    "label": 0
                },
                {
                    "sent": "So arguably these are more compact and also more easily interpretable.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 intuition about what can be learned by RBM's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also think about some stacking these PBM's into higher layers.",
                    "label": 0
                },
                {
                    "sent": "So when you learn this first layer and represent this Patch as a coefficients.",
                    "label": 0
                },
                {
                    "sent": "Just pointing to this basis vector, you can also learn the second layer.",
                    "label": 0
                },
                {
                    "sent": "VM then learn some dependencies between this first layer coefficients and this can be modeling some higher order statistics of these edges.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe I'll just Fast forward this part because Aaron should have covered this in fair fair amount of details.",
                    "label": 0
                },
                {
                    "sent": "So in GBM we have visible units and hidden units.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to just talk about binary cases and we represent using.",
                    "label": 0
                },
                {
                    "sent": "Basically, the energy function an the proper joint probability is just a I undirected graphical model represented by this energy.",
                    "label": 0
                },
                {
                    "sent": "So let me just give you some intuition.",
                    "label": 0
                },
                {
                    "sent": "Maybe this may be useful as a quick recap so we can.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so I'll just.",
                    "label": 0
                },
                {
                    "sent": "Stand this way.",
                    "label": 0
                },
                {
                    "sent": "OK, alright so we can think about IBM energy function in this decomposition.",
                    "label": 0
                },
                {
                    "sent": "So I color coded so different colors for different basis vector.",
                    "label": 0
                },
                {
                    "sent": "An, for example, W one correspond to basis vector for this first hidden unit and blue correspond to the second unit and so on.",
                    "label": 0
                },
                {
                    "sent": "And essentially what it will.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It means is that if you think about this first time, you are basically taking the inner product between this W one vector and the input data an.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this is trying to find some match, or it can be viewed as a sum pattern matching.",
                    "label": 0
                },
                {
                    "sent": "So when you have high correlation between W one and V, which means that the filter W one matches the pattern in this V, then this inner product will be higher an in order to minimize the energy which correspond to high probability state.",
                    "label": 0
                },
                {
                    "sent": "Basically, GBM will encode some high probability for H1.",
                    "label": 0
                },
                {
                    "sent": "So just as a intuition, I mean basically W. One can be viewed as a athlete, actor or some kind of filter.",
                    "label": 0
                },
                {
                    "sent": "If the filter matches the input data, then you assign high probability for activating H1 and you do the same thing for W2.",
                    "label": 0
                },
                {
                    "sent": "So you just take the inner product between WMV an if the value is high, it means that it's highly correlated, so you give high probability for activating H2.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you set it to 0.",
                    "label": 0
                },
                {
                    "sent": "With high probability as well.",
                    "label": 0
                },
                {
                    "sent": "So basically you can think about some kind of independent.",
                    "label": 0
                },
                {
                    "sent": "These are not these terms, and essentially it turns out that procedure can be also.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So represented as a factorized distribution, which means that you just think about this posterior probability of HJ being being equal to 1 as a some kind of independent.",
                    "label": 0
                },
                {
                    "sent": "Event so and it turns out that this probability can be simply sigmoid function of inner product up W vector and some bias added to that.",
                    "label": 0
                },
                {
                    "sent": "And this is basically some more compact way of writing this.",
                    "label": 0
                },
                {
                    "sent": "An it corresponds to basically a sigmoid neural net.",
                    "label": 0
                },
                {
                    "sent": "That's another reason why RBM has been also used as a initializing.",
                    "label": 1
                },
                {
                    "sent": "The deep neural network and also talk about that in the case of deep belief network as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how we do influence and influence is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "You just take the inner product between the vector W vector WJ, vectors and RV and add bias and take the sigmoid nonlinearity.",
                    "label": 1
                },
                {
                    "sent": "And this P of H given V can be viewed as a feature representation.",
                    "label": 0
                },
                {
                    "sent": "An since the energy function is symmetric, you can also do the same type of influence for estimating V invisible unit conditioned on the H, and it takes the same form.",
                    "label": 0
                },
                {
                    "sent": "And that's how you do info.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for real value data is so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small sample variability, some variation here, but essentially the idea is very similar, but the difference is that you take you add some quadratic term to encode the fact that these D take the real values.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to just move on for this real value case, I'm just I just want to say that it's actually fairly straightforward to also modeled IBM when the we've we take the visible real values, OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so conditional distribution is very easy to estimate.",
                    "label": 1
                },
                {
                    "sent": "It is simply just some sigmoid sigmoid function of something or some simple form of inner product between WMV or H and you can see the previous slide and you can do alternate Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "Just repeating this P of Y given H&POH given V. So joint distribution can be estimated using this alternate Gibbs sampling, and it turns out that if you run this gift chain infinite steps then it will recover the equilibrium distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's maybe not that practical, but this is how we do influence in GBM as a principle, and this is a pseudocode of how we do.",
                    "label": 1
                },
                {
                    "sent": "For example, how we sample from this joint distribution of the model.",
                    "label": 0
                },
                {
                    "sent": "So graphical is just alternating between sampling H while fixing D, An sampling V while fixing H and so on.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, I'm going to maybe skip this part so you can do maximum likelihood training.",
                    "label": 1
                },
                {
                    "sent": "And since the gradient is difficult to calculate, you can do some sampling based approximation.",
                    "label": 1
                },
                {
                    "sent": "And essentially after you calculate the gradient it is just a gradient stochastic gradient descent for optimizing the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to calculate gradient is the key.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this is the contrastive divergent San I'm going to skip this part.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, just to recap, when we do stochastic gradient using contrastive divergent, basically the learning rule looks like this.",
                    "label": 0
                },
                {
                    "sent": "So gradient of the this log likelihood with respect to the weight.",
                    "label": 0
                },
                {
                    "sent": "It is some product between input and the procedure P of HJ given V. That's some sigmoid function of W and the inner product of WMV plus some bias.",
                    "label": 0
                },
                {
                    "sent": "And you do the same thing, but here this time is estimated from the contrastive divergent by running the gifts gift sampling K steps.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "OK, some details.",
                    "label": 0
                },
                {
                    "sent": "So typically we use mini batch for doing this gradient estimation an.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lamentacion is very easy and yeah, I'm going to skip this part because Aaron should have covered this so there are some improved way of doing this.",
                    "label": 0
                },
                {
                    "sent": "Gradient estimation such as contrastive persistent CD and some variants and also there are some other algorithms such as score matching that also try to make this calculation more tractable.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to maybe.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about this.",
                    "label": 0
                },
                {
                    "sent": "Some variation of this RBM's so.",
                    "label": 0
                },
                {
                    "sent": "One intuition is that IBM is unsupervised learning algorithm and sometimes we need a good regularization.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we want to learn a very large number of basis vector then we would have overcomplete basis and it is actually difficult to learn a good model and so in that type of situation using some regularization is useful.",
                    "label": 0
                },
                {
                    "sent": "So one useful.",
                    "label": 0
                },
                {
                    "sent": "Regularization is sparsity, so the basic idea is that we enforce the model to have sparse representation, so that activation on average is sparse.",
                    "label": 1
                },
                {
                    "sent": "So you can think about this as a penalized version of the log likelihood, so you have the log likelihood plus some penalty based on the sparsity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Spicy can be evaluated in an empirical way.",
                    "label": 0
                },
                {
                    "sent": "So for example, you calculate the sample activation of JT unit across all the training data and then you have maybe a target sparsity such as maybe 5% or 10% of these activation.",
                    "label": 0
                },
                {
                    "sent": "Our end here.",
                    "label": 0
                },
                {
                    "sent": "For example, this sample can be a sample.",
                    "label": 0
                },
                {
                    "sent": "Activation can be just defined as a average over all the hidden unit activations in the training data.",
                    "label": 1
                },
                {
                    "sent": "And so this is so basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this type of regularization can be useful in learning so called sparse spaces.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And here so I'm showing the basis vector and you see some kind of pencil patterns and what it means is that.",
                    "label": 0
                },
                {
                    "sent": "Basically you are trying to represent the input data using some spice combination of these pen strokes.",
                    "label": 0
                },
                {
                    "sent": "So you can think about each of these Patch in the training sample as a sum combination of these basis where the coefficients can be viewed as zero or one.",
                    "label": 0
                },
                {
                    "sent": "An yeah, in often cases by City actually helps for training the PBM, so it has been used quite a lot in many scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's another variation of IBM called vectorized RBM.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that in the case of Gaussian RBM, the conditional distribution of V given H is a simply just independent Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So product of independent Gaussians, which doesn't model the dependency between these, like some correlation or pixel values, maybe corresponding to nearby nearby location, and so the basic idea is that you model this correlation.",
                    "label": 0
                },
                {
                    "sent": "You know better way an it is some modification of original IBM, but the basic idea is that you include some kind of covariance matrix which is non Gaussian an you can model this covariance spaces over this matrix as or inverse covariance matrix as some combination of basis vector with some.",
                    "label": 0
                },
                {
                    "sent": "Some diagonal diagonal component and.",
                    "label": 0
                },
                {
                    "sent": "So here is a graphical representation an for this talk.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into details, but the basic idea is that you can think about some kind of covariance modeling where this H is a coefficient anti some basis, so the inverse covariance can be modeled as this some combination of basis vector.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using this type of model you can actually do better modeling of natural images.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you just so Gaussian RBM, then these are the.",
                    "label": 0
                },
                {
                    "sent": "Samples from the model and you see that.",
                    "label": 0
                },
                {
                    "sent": "The local correlation of the pixel is not very well preserved.",
                    "label": 0
                },
                {
                    "sent": "Here are some variation of the Gaussian RBM where you connect the between the visible units and it seems to improve this local correlations better and it has been shown that using this covariance RBM you can actually arguably generate more realistic samples.",
                    "label": 0
                },
                {
                    "sent": "So you see that these patches in the top show some.",
                    "label": 0
                },
                {
                    "sent": "Kind of more fine grained local correlation and also longer edges and so on.",
                    "label": 0
                },
                {
                    "sent": "So here's just some variation of this RBM.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about this stacking.",
                    "label": 0
                },
                {
                    "sent": "So how to stack this as a deep lift network?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So deep belief net is a probabilistic generating model with this multiple layers so.",
                    "label": 1
                },
                {
                    "sent": "The main idea is that you can actually use stacking of RBM's as a good initialization.",
                    "label": 0
                },
                {
                    "sent": "So this model has been quite asked, has been quite popular over the years.",
                    "label": 0
                },
                {
                    "sent": "Maybe although nowadays is slightly less popular because of just purely supervised training from large amount of data.",
                    "label": 0
                },
                {
                    "sent": "But still this DBN training can be useful when you don't have large amount of supervision.",
                    "label": 0
                },
                {
                    "sent": "So and also it's a useful thing to know as some maybe theoretical perspective and a little bit details.",
                    "label": 0
                },
                {
                    "sent": "So there's an interesting fact about this DBN.",
                    "label": 0
                },
                {
                    "sent": "Where stacking the IBM can be viewed as maximizing the lower bound of the likelihood, and you have some very nice theory and I'm going to talk a little bit about that so that you can get some idea behind what it means to do this, what it means with this theoretical justification.",
                    "label": 1
                },
                {
                    "sent": "In practical sense, there are two ways of training this.",
                    "label": 0
                },
                {
                    "sent": "Deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "One is called a generative training.",
                    "label": 0
                },
                {
                    "sent": "There are some algorithm called up down algorithm that's used for fine tuning as a probabilistic model, but maybe I'll also in more practical sense.",
                    "label": 0
                },
                {
                    "sent": "I should say that people often just convert the deep belief net as a just a neural net.",
                    "label": 0
                },
                {
                    "sent": "So since the activation fee for the activation of the belief net almost looks the same as just a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Neural net you can just straightforwardly convert that into a deep neural net and do some back propagation.",
                    "label": 0
                },
                {
                    "sent": "Ann, I should say that probably this discriminated backpropagation is.",
                    "label": 0
                },
                {
                    "sent": "Much more popular, so majority of the papers that actually talks about DBN is actually mostly talking about this discriminated version of.",
                    "label": 0
                },
                {
                    "sent": "Discriminative training of this DBN as our neural net.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a structure of the belief networks.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It's actually good to be maybe noting the existence of arrows or direction of this arrows here.",
                    "label": 0
                },
                {
                    "sent": "So in the top layer.",
                    "label": 0
                },
                {
                    "sent": "We see that this connection is undirected, so there is no error between this H2 and H3.",
                    "label": 1
                },
                {
                    "sent": "So this top 2 hidden layers are modeled as a restricted Boltzmann machine RBM.",
                    "label": 1
                },
                {
                    "sent": "And the bottom layers are modeled as a sigmoid sigmoid belief net or just some kind of top down conditional probability using sigmoid function as a nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "So this is a full specification of these deep belief net, so you can think about this DBN's POV, an D, H1, H2.",
                    "label": 0
                },
                {
                    "sent": "So just modeling the full joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And basically you first multiply this top layer joint distribution PR.",
                    "label": 0
                },
                {
                    "sent": "For example here H2 and H3.",
                    "label": 0
                },
                {
                    "sent": "So this is the first joint distribution modeled by the PBM, and you have H1.",
                    "label": 0
                },
                {
                    "sent": "Given H2 and PV given H1.",
                    "label": 0
                },
                {
                    "sent": "So where this P of H L -- 2 given H -- 1 is some kind of sigmoid function.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically you can.",
                    "label": 0
                },
                {
                    "sent": "You can think about this as a a generative model.",
                    "label": 0
                },
                {
                    "sent": "First you sample this top 2 hidden layers, H2 and H3, and since this is an undirected graphical model, you can basically sample from this model by doing alternate Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "Actually did uncover about how to sample from undirected model.",
                    "label": 0
                },
                {
                    "sent": "OK, good so yeah, so you can just think about the alternative sampling for many iterations an assume that it has reached to some approximate equilibrium distribution between H2 and H3, and that's how you first start with sampling from the DBN.",
                    "label": 0
                },
                {
                    "sent": "An after you sample from this top two layers.",
                    "label": 0
                },
                {
                    "sent": "Then you can just fix the H2 and then generate H1 given H2 and then also generate V given H1.",
                    "label": 0
                },
                {
                    "sent": "So that's the top down generated process of this model.",
                    "label": 0
                },
                {
                    "sent": "So here this top down generating model part can be viewed as just the sigmoid function of some W transpose H plus some bias.",
                    "label": 0
                },
                {
                    "sent": "So it is pretty much the symmetric form of doing this influence in the reverse way for the case of GBM.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this specific architecture, or this combination of undirected part and directed part, is actually very subtle but important fact in theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "So any question about this so far?",
                    "label": 0
                },
                {
                    "sent": "Alright, so so this is how we do generative in generative sampling from this DBN.",
                    "label": 0
                },
                {
                    "sent": "An I'd like to talk about how we do some influence an in fact doing the exact inference.",
                    "label": 0
                },
                {
                    "sent": "Percier influences intractable because it is basically a graphical model from top to down.",
                    "label": 0
                },
                {
                    "sent": "So when you do posterior inference, it means that you are given input data such as image V, and you want to estimate some posteriors of this H variables and you are trying to reverse this direction.",
                    "label": 0
                },
                {
                    "sent": "In this graphical model, Ann is actually known to be intractable in general.",
                    "label": 0
                },
                {
                    "sent": "And also this type of problem is called explaining away so because because this higher load factors are explaining the input data and when you are trying to do procedure then there is a complex dependency between this H1 and H2 and H3.",
                    "label": 0
                },
                {
                    "sent": "So you cannot simply just do some independent posterior estimation for individual hidden unit or do feed forward in influence either.",
                    "label": 0
                },
                {
                    "sent": "So still basically the same kind of heuristic.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you can think of also as a variational approximation perspective is simply just still approximating this posterior of H as a sum feedforward function.",
                    "label": 0
                },
                {
                    "sent": "So not surprisingly, since we have seen that we do pursue estimation of IBM using the sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "We can just assume that there is a posterior or approximate distribution of this H variables that's written as simply just product top sigmoid function of W times some lower layer activation.",
                    "label": 0
                },
                {
                    "sent": "So essentially you multiply some W, one with D and add some bias and take the sigmoid function and you just treat this perceived as a factorized.",
                    "label": 0
                },
                {
                    "sent": "There is 1 where you simply multiply the sigmoid probabilities for the first layer and then you do the same thing for the second layer and just repeatedly do this type of inference.",
                    "label": 0
                },
                {
                    "sent": "And of course this is yes.",
                    "label": 0
                },
                {
                    "sent": "So this is an assumption or is some.",
                    "label": 0
                },
                {
                    "sent": "You can also think about this as a variational approximation.",
                    "label": 0
                },
                {
                    "sent": "So the main idea is that in graphical model, whenever you have intractable influence, the often the useful trick is to just approximate the posterior using some simple distribution.",
                    "label": 0
                },
                {
                    "sent": "And then you try to optimize something or you try to learn the parameter so that it can.",
                    "label": 0
                },
                {
                    "sent": "This approximate distribution actually can match the actual posterior.",
                    "label": 0
                },
                {
                    "sent": "So of course this is natural as true posterior distribution, but this is approximate distribution and you can also think about this as a variational approximation.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "This is how we do sampling an inference in deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "So it's actually somewhat somewhat complicated because you have this generated part and generated part needs top down sampling at the top, which requires multiple generations of Gibbs sampling, and then you top to bottom generation of the input data.",
                    "label": 0
                },
                {
                    "sent": "Which is actually fine.",
                    "label": 0
                },
                {
                    "sent": "I mean, once you have learned the parameter W1W 2W3, doing this sampling from the model is actually not a problem.",
                    "label": 0
                },
                {
                    "sent": "However, basically when you do post your influence, this part is actually a big issue.",
                    "label": 0
                },
                {
                    "sent": "Because there's really no guarantee that this approximate distribution is close to the true posterior.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, I'm going to show that how this type of specific network architecture and greedy training can actually lead to some interesting theoretical justification of training this deep architecture.",
                    "label": 0
                },
                {
                    "sent": "So here's the algorithm so called greedy training.",
                    "label": 1
                },
                {
                    "sent": "Or you can think about this as a stacking RBM's.",
                    "label": 1
                },
                {
                    "sent": "So in the first layer you start from the input visible unit and then just train the first layer IBM.",
                    "label": 0
                },
                {
                    "sent": "An you can use whatever algorithm, maybe contrastive diversions or persistent, consistent, persistent CD, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you learn this first layer.",
                    "label": 0
                },
                {
                    "sent": "DBM",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can think about calculating some posterior distribution of H1 for the IBM an photo case of GBM.",
                    "label": 0
                },
                {
                    "sent": "This posterior distribution is simply just factorized product sigmoid.",
                    "label": 0
                },
                {
                    "sent": "W 1 * V OK.",
                    "label": 0
                },
                {
                    "sent": "So you sample or just feed some input H1 as a as a secondary training.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that I mean this is actually just a theoretical construction to just show that it is possible to show some relationship between the first layer IBM and the second layer deep belief Nets, so the theoretical construction here is that you.",
                    "label": 0
                },
                {
                    "sent": "Basically copy the number of visible units.",
                    "label": 0
                },
                {
                    "sent": "For about 2, the number of hidden units in the second layer.",
                    "label": 0
                },
                {
                    "sent": "So you just mirror the structure in the second layer and also users mirror this one and just initialize as the wait for the second layer as well.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this way, then you can basically just train the second layer using as another RBM and then repeat this procedure.",
                    "label": 0
                },
                {
                    "sent": "So maybe just going before you go back to some theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe just let's just recap the algorithm, so we simply just train the RBM in the first layer.",
                    "label": 0
                },
                {
                    "sent": "And calculate this procedure as a sigmoid function of W times W 1 * B plus some bias.",
                    "label": 0
                },
                {
                    "sent": "And that's the H1.",
                    "label": 0
                },
                {
                    "sent": "And then you take this H1 as an input and train another layer of RDN.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you repeat this procedure.",
                    "label": 0
                },
                {
                    "sent": "And we just defined this.",
                    "label": 0
                },
                {
                    "sent": "Whatever sigmoid function of W * V as a posterior inference for this feedforward procedure.",
                    "label": 0
                },
                {
                    "sent": "And that's how we do greedy training of deep lift Nets.",
                    "label": 0
                },
                {
                    "sent": "So it looks actually very simple heuristic.",
                    "label": 0
                },
                {
                    "sent": "We just basically repeat this recursive training RBM.",
                    "label": 0
                },
                {
                    "sent": "But it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some interesting theory here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea here is that.",
                    "label": 0
                },
                {
                    "sent": "We just got up.",
                    "label": 0
                },
                {
                    "sent": "We can basically copy this visible Unit V into the second layer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So here is a some theoretical justification.",
                    "label": 1
                },
                {
                    "sent": "So basically we have the log likelihood of input data, so I use X.",
                    "label": 0
                },
                {
                    "sent": "But you can also think about just the visible unit B.",
                    "label": 0
                },
                {
                    "sent": "So the log likelihood of any probabilistic model can be written as this form.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I can also do some more derivation here, but.",
                    "label": 0
                },
                {
                    "sent": "But basically you have low P of X that's larger than some.",
                    "label": 0
                },
                {
                    "sent": "Some entropy of this posterior plus some additional terms here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And it turns out that when we train this second layer with very specific initialization of W1 and W2, and also this network architecture described previously can be.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as some kind of.",
                    "label": 0
                },
                {
                    "sent": "Some kind of unfolding this IBM into the second layer DBN and then do some continued training so.",
                    "label": 1
                },
                {
                    "sent": "So this is a very special case where if you have W 2 equal to W one transpose, which means just mirroring the weight in the second layer.",
                    "label": 0
                },
                {
                    "sent": "Which requires that the number of hidden you need in the second layer should be the same as number of visible units.",
                    "label": 0
                },
                {
                    "sent": "An this lower bound actually is tight, which means that the first layer IBM and the secondary DBN is actually the same.",
                    "label": 1
                },
                {
                    "sent": "That's how we can actually justify training this second level DBM further and show that this can actually improve the local likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this may sound a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two are dense, so maybe I'll just show some slightly more detailed derivation here.",
                    "label": 0
                },
                {
                    "sent": "So this first I equality is basically a equality that holds for any probabilistic model, so T of V probably low probability of V is.",
                    "label": 1
                },
                {
                    "sent": "Written as this term where this Q of H can be any distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so called variational approximation or variational also also called variational bound.",
                    "label": 0
                },
                {
                    "sent": "So QH can be any distribution.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is that here you look at the third term and the third time is the KL divergent between this Q distribution and posterior distribution of H given B.",
                    "label": 0
                },
                {
                    "sent": "So which means that if you have a posterior distribution 80 of H given B an if that matches to your variational distribution Q of H, then this KL time will be small.",
                    "label": 0
                },
                {
                    "sent": "Since the Cal divergences always positive if you just remove this scale, divergent, and then the remaining time should be less than the original summation, which means that this second line actually act as a lower bound of these low probability.",
                    "label": 0
                },
                {
                    "sent": "Actually, can you raise hands if you are familiar with this variational bound?",
                    "label": 0
                },
                {
                    "sent": "OK, so about half of half of you are familiar, so if you have if you have any questions I'm you can stop here.",
                    "label": 0
                },
                {
                    "sent": "Stop me here and then.",
                    "label": 0
                },
                {
                    "sent": "I mean, I can also go through some derivation as well.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "Is there a reason?",
                    "label": 0
                },
                {
                    "sent": "So for the case of DBN.",
                    "label": 0
                },
                {
                    "sent": "I think the main reason for mixing this obvious there is some very special reason of this kind of unfolding from the first layer.",
                    "label": 0
                },
                {
                    "sent": "IBM to the second layer DBN and connecting this together.",
                    "label": 0
                },
                {
                    "sent": "So I mean, that's why we talk about this variational bound.",
                    "label": 0
                },
                {
                    "sent": "And also I like in this context of DBN.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking, for the case of undirected model, typically influence is done by Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "So although it can be expensive and you can do give, I mean skip sampling Alternatively, so it's not too bad influences, I should say relatively easier than influence interactive graphical model.",
                    "label": 0
                },
                {
                    "sent": "So typically this variational bound is more useful when you deal with directed graphical model or some hybrid graphical model.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, cave divergences always passive, so that's why you get the second line from the first line.",
                    "label": 0
                },
                {
                    "sent": "Alright, I mean so the first line actually is.",
                    "label": 0
                },
                {
                    "sent": "It is actually very simple equality so.",
                    "label": 0
                },
                {
                    "sent": "So if you just plug in the definition of the KL divergent and then just some, all these together, then you actually get back to the 1st first equality.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "Right, I can also just do some.",
                    "label": 0
                },
                {
                    "sent": "Some derivation on the tablet, so let's see if I can just quickly try that, although some for some of you it may be very familiar.",
                    "label": 0
                },
                {
                    "sent": "I think it's good to just go over this derivation very quickly, so I'm going to just do first line derivation.",
                    "label": 0
                },
                {
                    "sent": "So in the first line, basically the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Can be written as key of QH log of T of each given B minus some of HQH.",
                    "label": 0
                },
                {
                    "sent": "Log Q of it's an.",
                    "label": 0
                },
                {
                    "sent": "If I just expand the definition of the KL divergent then it is QH log of QH, divided or?",
                    "label": 0
                },
                {
                    "sent": "Divided by P of H given V OK.",
                    "label": 0
                },
                {
                    "sent": "So I mean it's just a matter of some arithmetic.",
                    "label": 0
                },
                {
                    "sent": "So essentially this queue of H Lo que of H and this time actually just cancels out.",
                    "label": 0
                },
                {
                    "sent": "So you just have some Asian of HQ of H. Log P of age given B minus some of HQH log up PO.",
                    "label": 0
                },
                {
                    "sent": "He's given me so, so it is a simple simple cancellation of terms, and if you look at this.",
                    "label": 0
                },
                {
                    "sent": "Fine.",
                    "label": 0
                },
                {
                    "sent": "Basically P of H, V is a joint distribution of HV anti of H. Given these, the posterior of H given be so.",
                    "label": 0
                },
                {
                    "sent": "It can be just return as summation of HQH log of tier of age given D /, P of TAH, V / P of H given V. But this this term essentially is just PAVI mean just the definition of the this conditional probability right so?",
                    "label": 0
                },
                {
                    "sent": "So you get a local POV.",
                    "label": 0
                },
                {
                    "sent": "So since the V is a fixed vector, so you can think about this as a sum fixed constant, whereas the Q is any it can be some variable because we don't know what the Q distribution is and it's actually it's a variational distribution.",
                    "label": 0
                },
                {
                    "sent": "So But anyway this is a constant, so some summation over this.",
                    "label": 0
                },
                {
                    "sent": "So at the end you can show that this right hand side.",
                    "label": 0
                },
                {
                    "sent": "Right hand side is the basically P log of TV.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To just recap what I have shown is that if you just do some arithmetic on the right hand side, you just do simple calculation and it turns out to be the same as the low probability of the input V. So I mean, this is a very useful trick in variational approximation or variational inference, because the conclusion here is that the left hand side is the same as the right hand side regardless of what the Q distribution is.",
                    "label": 0
                },
                {
                    "sent": "OK, so this first line is true for any Q distribution.",
                    "label": 0
                },
                {
                    "sent": "But basically, since the KL divergences always positive, you basically take this out, then you get a lower bound and that's the second line.",
                    "label": 0
                },
                {
                    "sent": "So in practice we always want to find the Q distribution.",
                    "label": 0
                },
                {
                    "sent": "That's a close approximation to this procedure, so that can be also viewed as a finding a tight lower bound of this likelihood, OK?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's the second line.",
                    "label": 0
                },
                {
                    "sent": "So maybe you can go back any questions here before we move on.",
                    "label": 0
                },
                {
                    "sent": "Yes uh-huh yeah.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Always.",
                    "label": 0
                },
                {
                    "sent": "So that's so great, yeah?",
                    "label": 0
                },
                {
                    "sent": "So for the case of DBA, I think there is some mathematical reason behind it, but intuition wise I mean I mean essentially how people use DBN is typically just as a feedforward influence module, so people actually don't really care about.",
                    "label": 0
                },
                {
                    "sent": "You know how these things are constructed as a mixup, undirected and directed, but I'm just talking about some theory behind this so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So yeah, DBN is just constructed with this mixup.",
                    "label": 0
                },
                {
                    "sent": "Directed and undirected just for some theoretical reason.",
                    "label": 0
                },
                {
                    "sent": "But also regarding your question, there is also algorithm Cody Boltzmann machine which is all undirected Ann.",
                    "label": 0
                },
                {
                    "sent": "I believe Russell Russell could enough.",
                    "label": 0
                },
                {
                    "sent": "We'll talk about that in great detail.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to talk about it today.",
                    "label": 0
                },
                {
                    "sent": "Both are not.",
                    "label": 0
                },
                {
                    "sent": "Basically that's just another class of model and there are like another class of algorithms for doing inference and learning so.",
                    "label": 0
                },
                {
                    "sent": "So there are some pros and cons of undirected versus directed.",
                    "label": 0
                },
                {
                    "sent": "So, as I briefly mentioned, for directed model, inference is difficult because of doing the posterior inference where you have to actually deal with explaining away effect.",
                    "label": 0
                },
                {
                    "sent": "So that's typically why we resort to some variational approximations, such as approximating the queue using some feed for distribution, even though it's not actually true in reality.",
                    "label": 0
                },
                {
                    "sent": "For the case of.",
                    "label": 0
                },
                {
                    "sent": "Undirected model you can do some mean field or Gibbs sampling, so I think it's slightly more approx.",
                    "label": 0
                },
                {
                    "sent": "I mean more approximate than the true posterior.",
                    "label": 0
                },
                {
                    "sent": "So typically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, typically you can do better approximate inference using undirected model, but at the same time when you do learning, then learning an undirected model is actually more difficult, whereas learning in directed model is actually much easier.",
                    "label": 0
                },
                {
                    "sent": "So, but anyway, I think it it requires some more in depth discussion of this graphical model.",
                    "label": 0
                },
                {
                    "sent": "So probably I will just defer that for more offline discussion.",
                    "label": 0
                },
                {
                    "sent": "So let's just move on to this.",
                    "label": 0
                },
                {
                    "sent": "Bound calculation so.",
                    "label": 0
                },
                {
                    "sent": "So I just talked about this variational bound, which is true for any distribution.",
                    "label": 1
                },
                {
                    "sent": "So for the context of deep deep belief Nets, we just want to model this Q distribution as a simple like product of sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "So Q of H is simply just the sigmoid of HJ equal to 1 given B.",
                    "label": 0
                },
                {
                    "sent": "In fact, it is simply some kind of sigmoid of WJ transpose.",
                    "label": 0
                },
                {
                    "sent": "B plus BJ?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what we have seen before.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to just connect between this IBM and EPN.",
                    "label": 0
                },
                {
                    "sent": "So when we have this special condition satisfied, where W2 is equal to W 1.",
                    "label": 0
                },
                {
                    "sent": "Then these two models are actually identical, so let me explain what.",
                    "label": 0
                },
                {
                    "sent": "Let me explain this in more detail, so I IBM here means you just have a visible unit here and then you have first layer weights W one and users have this part as PBM, so.",
                    "label": 0
                },
                {
                    "sent": "So this is the IBM model I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "And DBN is a a different model which is a product of this undirected part plus multiplied by this directed part.",
                    "label": 0
                },
                {
                    "sent": "That's what I mean by DBN.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that when W2 is the equal to W one transpose, these are identical, and the reason here is that essentially the if you think about this.",
                    "label": 0
                },
                {
                    "sent": "Pop two layers.",
                    "label": 0
                },
                {
                    "sent": "You can just marginalized.",
                    "label": 0
                },
                {
                    "sent": "So this is the undirected part so.",
                    "label": 0
                },
                {
                    "sent": "Let me just write it down this part so this joint distribution can be viewed as PA V1V1 given H 1 * P of H1, H2, so that's that's the DBN distribution.",
                    "label": 0
                },
                {
                    "sent": "So you first define the path to layer as undirected model and define the top to bottom layer using this conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And since OK so here is H1.",
                    "label": 0
                },
                {
                    "sent": "So if you are just marginalized, suppose that you are trying to marginalized the H2.",
                    "label": 0
                },
                {
                    "sent": "Then you are just marginalizing this.",
                    "label": 0
                },
                {
                    "sent": "H2 variable here.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                },
                {
                    "sent": "You can only worry about this time so you are just marginalizing H2 for this joint distribution an it's just correspond to doing a marginal influence for the second layer RBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so summing over H2 for this joint distribution is essentially summing over the H2 in this upper part, which is the IBM an.",
                    "label": 0
                },
                {
                    "sent": "If you have W 1 equal to W2 as a transpose, then this marginal distribution of H1 is the same for the IBM and also for the DBN.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very like special, contrived way of constructing this model.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line here is that now we can connect the log likelihood of the PBM and the local likelihood of the DBN an under this special special condition.",
                    "label": 0
                },
                {
                    "sent": "These two are the same, so that's why we can somehow connect between this greedy training.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I can explain that slight more details so.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll just put.",
                    "label": 0
                },
                {
                    "sent": "So basically here we are talking about this type of model.",
                    "label": 0
                },
                {
                    "sent": "Where the distribution of this P of H130 of V. 8182 it is the P of the given H 1 * P of H 1 * 8 Two an this is the IBM and this is just the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Sigmoid belief Nets.",
                    "label": 0
                },
                {
                    "sent": "So if you just think about this second layer RBM.",
                    "label": 0
                },
                {
                    "sent": "So if you just think about this part.",
                    "label": 0
                },
                {
                    "sent": "Then I mean, intuitively speaking, it is.",
                    "label": 0
                },
                {
                    "sent": "Thus, if W2 is the same as W one transpose, it is basically inverting the PBM in Uptown Way.",
                    "label": 0
                },
                {
                    "sent": "So just inverting the PBM by swapping the visible unit an hidden unit, so.",
                    "label": 0
                },
                {
                    "sent": "So the actual marginal distribution of H1 by summing over all these H2, it is the same as marginal distribution of the original IBM by summing over Adobe.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why basically summing over H2 basically just gives you the original IBM's prior, so P of H defined by the origonal RBM with www.1s08.",
                    "label": 0
                },
                {
                    "sent": "So I mean so if you sum over the H2 here.",
                    "label": 0
                },
                {
                    "sent": "Then use get piov.",
                    "label": 0
                },
                {
                    "sent": "Be given eight 1 * P of eight one, but this one is the same as the original.",
                    "label": 0
                },
                {
                    "sent": "Same as.",
                    "label": 0
                },
                {
                    "sent": "P of eight, one from the RPM.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have GBM for the first layer.",
                    "label": 0
                },
                {
                    "sent": "Basically this this is the same as this when you have a. Huh?",
                    "label": 0
                },
                {
                    "sent": "When W2 is equal to W, one transpose, maybe I can also talk more about this offline is is this clear or do you do OK?",
                    "label": 0
                },
                {
                    "sent": "OK, any other questions here?",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Let's go to this slide again, so.",
                    "label": 0
                },
                {
                    "sent": "So basically I have just talked about how these two models are connected, so PBM and the second layer DBN have the same log likelihood when it's initialized in this specific way.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So now we can just think about what it means to train this in a greedy way, so I'm just expanding this times line by line.",
                    "label": 0
                },
                {
                    "sent": "So the first time first line is the log likelihood of the DBN when we have marginalized all the H1 and H2 for the case of this special case where W2 is initialized with W one transpose, then it is the same as the RBM.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So these two are the same and same log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And also if you apply this variational bound up the original form where in the original form we have this KL divergent term.",
                    "label": 0
                },
                {
                    "sent": "But further case of IBM, you actually can do exact inference.",
                    "label": 0
                },
                {
                    "sent": "So which means that when you define this QH as a sigmoid function of W, one transpose V plus bias, then this Q of H1 is exactly the same as this posterior of H1 given D. Because of this very special construction.",
                    "label": 0
                },
                {
                    "sent": "So which means that these two times are the same as the third line.",
                    "label": 0
                },
                {
                    "sent": "Well, this Cal time is simply zero OK?",
                    "label": 0
                },
                {
                    "sent": "So which means that this lower bound is actually tight lower bound, because KL divergent is 0, which is actually a good thing.",
                    "label": 1
                },
                {
                    "sent": "So then we can expand this third line to the 4th by just applying simple definition of conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So here what I did is simply just.",
                    "label": 0
                },
                {
                    "sent": "Expanding Missy, expanding the first time, so joint distribution of H1, V as a product of prior distribution of H1 times the conditional distribution of V1 given be given H1.",
                    "label": 0
                },
                {
                    "sent": "So this is some simple like expansion.",
                    "label": 0
                },
                {
                    "sent": "So so basically this is what we get out of this variational bound and also the special condition that W2 is equal to W one transpose.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if you think about what it means to train the second layer.",
                    "label": 0
                },
                {
                    "sent": "We can think about what is fixed in the second layer and what is not fixed.",
                    "label": 0
                },
                {
                    "sent": "So Q H1 is simply defined as.",
                    "label": 0
                },
                {
                    "sent": "Just as a recap, I mean QQ of H1 was defined as the sigmoid function of something where it only has a parameter dependence on W 1.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you are trying to train the second layer, we are going to just train the W2.",
                    "label": 0
                },
                {
                    "sent": "So what it means is that we have initialized W2 as a W1 transpose, But now we are going to fix the W one, but now we unfreeze W 2 so we are going to fully change W2 starting from this initialization so W2 is a free parameter, free, valuable that you can change where is W one is fixed.",
                    "label": 0
                },
                {
                    "sent": "So which means that Q distribution is fixed here.",
                    "label": 0
                },
                {
                    "sent": "So you can treat this as a constant.",
                    "label": 0
                },
                {
                    "sent": "Right, and similarly, this top down distribution appear V. Given H1 is also just dependent on W one, so it only depends on W2, but not sorry, it only depends on W one but not W2, so you can also treat this as a fixed and similar.",
                    "label": 0
                },
                {
                    "sent": "This is fixed and also this is fixed.",
                    "label": 0
                },
                {
                    "sent": "So if you think about what is not fixed, it is basically this distribution which can be viewed as a. Summation of H2 or P of this second layer, IBM.",
                    "label": 0
                },
                {
                    "sent": "Of eight 1, is 2.",
                    "label": 0
                },
                {
                    "sent": "So although it is written as a marginalized form, you can also just write it as a second layer.",
                    "label": 0
                },
                {
                    "sent": "IBM with H1H2, there's some Dover.",
                    "label": 0
                },
                {
                    "sent": "Some some the world.",
                    "label": 0
                },
                {
                    "sent": "Age 2 and then the parameter for this secondary GBM is W2, so we are going to just up to some gradient descent to optimize W2.",
                    "label": 0
                },
                {
                    "sent": "That will increase this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially the second layer, training or DBN.",
                    "label": 1
                },
                {
                    "sent": "Is its corresponding to just greedily optimizing the second layer, IBM, while the first layer parameters are all fixed?",
                    "label": 1
                },
                {
                    "sent": "So basically, this construction shows that if you initialize the second IBM in this particular way, and then if you can find W2 that can find a better model than the initial model starting from W one transpose, then basically this will increase.",
                    "label": 0
                },
                {
                    "sent": "So as a result you can show that this log likelihood of the secondary DBN will be higher than the original.",
                    "label": 0
                },
                {
                    "sent": "Local I cleared at IBM.",
                    "label": 0
                },
                {
                    "sent": "OK so it's actually a bit subtle and some what.",
                    "label": 0
                },
                {
                    "sent": "Yeah, somewhat detailed derivation, but yeah, I wanted to cover this because main topic today is stacks of IBM, but it's often the case that people just treat this as a heuristic, but I wanted to show that there's some deeper some interesting mathematics behind it.",
                    "label": 0
                },
                {
                    "sent": "So let me see, so let's just summarize.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice setup, local message here is that you start from the IBM and then we initialize the secondary DBN by initializing W2.",
                    "label": 0
                },
                {
                    "sent": "As everyone transpose and these are the same.",
                    "label": 0
                },
                {
                    "sent": "As a result these two have the same log likelihood and also the same joint distribution if you just marginalized H2.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is to fix W one and only change W2 as a variable in the training.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the same as training the second layer as PBM an.",
                    "label": 1
                },
                {
                    "sent": "Because of this construction, we can actually show that if you do this greedy training, the log likelihood will be better than the original IBM.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So great question.",
                    "label": 0
                },
                {
                    "sent": "So in the second first layer to second layer, what?",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What was actually very nice is that here the KL term is exactly 0 becausw.",
                    "label": 0
                },
                {
                    "sent": "Because of this very special construction, it turns out, but when we do the third layer training, we don't have any guarantee that this Q distribution is actually correct.",
                    "label": 0
                },
                {
                    "sent": "As a result, this this bound becomes actually is a lower bound and but there's no guarantees are tight, which means that essentially we can do similar type of derivation for the third layer to show that.",
                    "label": 0
                },
                {
                    "sent": "The lower bound defined by this type of variational.",
                    "label": 0
                },
                {
                    "sent": "There is an approximation will improve so we can show that were bound will improve, but there is no guarantee that actual look likelihood will improve.",
                    "label": 0
                },
                {
                    "sent": "So that's actually one subtlety in this type of.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You re so.",
                    "label": 0
                },
                {
                    "sent": "To summarize, basically from the first second we actually have actual exact proof that the local I could improve for the 2nd third layer or higher layers.",
                    "label": 0
                },
                {
                    "sent": "The lower bound will become better, but not necessarily the actual local local likelihood.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now actually we justify this greedy training of the this DBN as a stacking about IBM.",
                    "label": 1
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Just to go back to some more big picture, what I talked about was basically some generated version of fine tuning.",
                    "label": 0
                },
                {
                    "sent": "Or generated training at this DBN an in fact there are some other version where you actually have a supervision.",
                    "label": 0
                },
                {
                    "sent": "So if you have class label Y and then you can also do some joint optimization of this joint.",
                    "label": 0
                },
                {
                    "sent": "Likelihood of Y&X together.",
                    "label": 0
                },
                {
                    "sent": "And there's some algorithm called up down algorithm is described by Hinton at all 2006 neural computation paper.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, I mean, when we do all these type of stacking about BM.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the most practical and straightforward usage of this type of network is to simply convert this as on your mat and do back propagation.",
                    "label": 0
                },
                {
                    "sent": "An in fact there are lots of some mix up terminology, so when you read papers sometimes it says deep belief Nets, but it actually sometimes means is a DPS net converted into a neural net and train using back problem.",
                    "label": 0
                },
                {
                    "sent": "So most of the time, actually that's what it means when people say is deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "So there's some subtlety in terms of terminology.",
                    "label": 1
                },
                {
                    "sent": "And I should say that this first version is way more practical than the second one for some some reason.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe the first version is much simpler to just.",
                    "label": 0
                },
                {
                    "sent": "To the surprise, so probably that's the reason and also the performance is actually quite good, so.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just talk a little bit more about the generative perspective and also talk about just discriminated back propagation.",
                    "label": 0
                },
                {
                    "sent": "So here is the initial model that Hinton Arrow used for modeling the digits using deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "So essentially you see that there is a class label, so ten class labels here and the top layer toddler can be viewed as a social code associated memory, but essentially it's just the IBM where you have class labels and some hidden units that are connected in undirected way.",
                    "label": 0
                },
                {
                    "sent": "So without this label you can think about this as just regular IBM, but when you have class label you can just think about some concatenation to the visible unit where you always observe this class labels during the training time.",
                    "label": 0
                },
                {
                    "sent": "So this top layer is undirected and you have a top down directive connection, so the green arrows correspond to the actual prime parameter of the model.",
                    "label": 0
                },
                {
                    "sent": "So you have undirected connection on the top and top down connections in the lower layers and the Red Arrows correspond to variational parameters.",
                    "label": 0
                },
                {
                    "sent": "So actually in the paper they say this green arrows as a generative model or generated parameter, whereas the red ones as a recognition recognition model or recognition parameters.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that you essentially initially approximate this red, red directional influence of, which is a recognition unit as some sigmoid function of W, one transpose V, and also sigmoid of W2 transpose HH one and so on.",
                    "label": 0
                },
                {
                    "sent": "So you just do some people influence and hoping that it will actually match the actual posterior.",
                    "label": 0
                },
                {
                    "sent": "So in the paper they.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about so called up an algorithm that's designed to do this type of fine tuning.",
                    "label": 0
                },
                {
                    "sent": "So in the undirected graphical model perspective may.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just going back to here.",
                    "label": 0
                },
                {
                    "sent": "Essentially, this model is so called a chain graph, which is a very special instance of some graphical model where you have mixed up, undirected and directed models.",
                    "label": 0
                },
                {
                    "sent": "So essentially, in order to do some learning for this chain graph, what you need to do is first do some posterior inference.",
                    "label": 0
                },
                {
                    "sent": "Ann, you do procedure in France and then you do some gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Fickle tease that you cannot do actual exactly similar, so you actually resort to this approximation, parameterized by some.",
                    "label": 0
                },
                {
                    "sent": "Fit for sigmoid, neural, net and.",
                    "label": 0
                },
                {
                    "sent": "By the end you your goal is to do some posterior inference and then do some gradient calculation.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's actually in essence, that's what this Uptown algorithm is trying to do.",
                    "label": 0
                },
                {
                    "sent": "So this first part can be viewed as a some approximate version of this posterior inference.",
                    "label": 0
                },
                {
                    "sent": "So it is basically assuming that with this bottom up pass, you actually get the posterior.",
                    "label": 0
                },
                {
                    "sent": "Of course this is not exact procedure, but that's assumption and then you do some contrastive divergent training for the top layer and also for the bottom layers.",
                    "label": 0
                },
                {
                    "sent": "Actually, training is very straightforward because it's a training for the.",
                    "label": 0
                },
                {
                    "sent": "Directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "But anyway, yeah, I'm not going to go into details, but the high level idea is that you do you do posterior inference, but since the posterior inference is intractable, users approximate using this feedforward sigmoidal neural net and then do some contrastive divergent and some additional gradient calculation.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so let's say that's how we do this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fine tuning.",
                    "label": 0
                },
                {
                    "sent": "And uh, for.",
                    "label": 0
                },
                {
                    "sent": "Sampling from the model I just talked about how to do this, which means that you do some Gibbs sampling for this top two layers and then generate this top to bottom directional.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample generation and here are some examples of generating samples.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some results on this database, so at that time actually they show that generated training actually leads to a very competitive performance to the state of the art at the time, so.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some results an and.",
                    "label": 0
                },
                {
                    "sent": "Here's some nice demo of this algorithm, so actually I don't have an Internet connection now, so I cannot show this demo, but it has some nice some animation where.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, if you if you fix the one up fix this label neuron to one at the 10 digit class, then actually you can do this posterior inference by generating the hidden units and then also sampling from this pixels pixel images an either or basically hallucinate the actual digit image that that's kind of changing overtime cause given the fixed classloaders knows.",
                    "label": 0
                },
                {
                    "sent": "Like 1 to one mapping from the class to the actual pixel image.",
                    "label": 0
                },
                {
                    "sent": "So it will basically do some sampling over this class class, conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And similarly, if you actually condition over the pixels pixel image, then you can do this inference and then you can infer about the this class labels as well.",
                    "label": 0
                },
                {
                    "sent": "So one nice thing about this type of model is that you can do also some probabilistic inference, which means that if you have some power only the part of the input image observed, then you can do some bottom up and then top down inference in iteration and then you can basically fill out the rest of the missing image.",
                    "label": 0
                },
                {
                    "sent": "So that's one actually interesting.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing you can do using this type of.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilistic model.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How actually we can justify an understand this more like a generative version of the deep belief net?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to prove.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You talk about stacking as a stacking these carbs as just a neural net.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is something that most of you are much more familiar with and this is also how things are done in practice.",
                    "label": 0
                },
                {
                    "sent": "Most of the time.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that users.",
                    "label": 0
                },
                {
                    "sent": "Think about this approximate feed for influence as just a neural net.",
                    "label": 0
                },
                {
                    "sent": "Which is actually quite reasonable, cause the actual field for function is exactly the same as the sigmoid deep neural net.",
                    "label": 1
                },
                {
                    "sent": "An basically you can just initialize the DBN and then do some back propagation and fine tune.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bing and so on.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "You can train this.",
                    "label": 0
                },
                {
                    "sent": "Train the stacks of RBM from the handwritten digits and you train the first level BM Secondly RBM.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly IBM and then stack them together and then put the class label on top and then just treat this as a neural network.",
                    "label": 1
                },
                {
                    "sent": "And then you just do some back propagation.",
                    "label": 0
                },
                {
                    "sent": "Ann actually works quite well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, this type of network can be also used as a autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Which is useful for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So for example you do the same style of stacking greedy training and then put them together.",
                    "label": 0
                },
                {
                    "sent": "But in this case you can unroll the this.",
                    "label": 0
                },
                {
                    "sent": "Visible unit had hidden unit in this reverse direction, so essentially then it becomes the auto encoder motor encoder where you take the image and it converts into some hidden unit and then they sit in.",
                    "label": 0
                },
                {
                    "sent": "It is used for reconstructing the original image.",
                    "label": 0
                },
                {
                    "sent": "And if you put some bottleneck here, such as like small number of hidden unit in the middle, then this can be used as a dimensionality reduction and also can be useful for visualization as well.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it has been shown that through this type of auto encoding you can map the different class labels in much closer.",
                    "label": 0
                },
                {
                    "sent": "Closer vicinities compared to just a simple like linear mapping.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the visualization or projection of some.",
                    "label": 0
                },
                {
                    "sent": "This made small number of hidden units, so you see that same class examples are much more closer and also more separated from other class labels compared to just using the linear mapping.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also you can do some tuning up this with some additional objectives.",
                    "label": 0
                },
                {
                    "sent": "So basically here you can think about open coding as a one objective and also you can think about some kind of similarity between examples from the same class or different class.",
                    "label": 0
                },
                {
                    "sent": "So intuition is that you want the example from the same class to be close to each other and also you want the example from different class to be far away, so you can actually define some loss function that.",
                    "label": 0
                },
                {
                    "sent": "Encode that type of similarity in this middle layer and then actually it can also provide even better separation between different classes and also can be useful for classification as well.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about Convolutional network tomorrow, so I'm not going to go into details here, but you can basically also run model for some kind of.",
                    "label": 0
                },
                {
                    "sent": "Feature hierarchy for images and audio and different sensory modalities.",
                    "label": 0
                },
                {
                    "sent": "So here's one example.",
                    "label": 0
                },
                {
                    "sent": "So given the input image, you can learn some first level features.",
                    "label": 0
                },
                {
                    "sent": "That is encoding the image as a combination of edges, and then you can learn the second layer correlation, which includes some, maybe conjunction or combination of edges, and that can be used as a maybe higher level feature detector such as maybe corners and contours.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk more detail about this tomorrow, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I'm talking about some convolutional model, so it's not exactly just starting from Patch to this type of mapping, but maybe I'll talk more about this tomorrow, but in principle you can do.",
                    "label": 0
                },
                {
                    "sent": "You can just take the Patch and learn the first layers are same Patch size basis and the secondary it will be just some combination of these first layer patches, so it's a little bit different, but in principle, so very.",
                    "label": 0
                },
                {
                    "sent": "Similar idea, OK?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can also do greedy stacking up the IBM's for the speech data.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have some speech features such as MFC or spectrogram, and then users learn the first layer, second layer, and then so on and then stack the class table on top and then do some supervised backpropagation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is another example.",
                    "label": 0
                },
                {
                    "sent": "So in this case you do stacking and instead of doing just a classification on top, you actually put the hidden Markov model on this top layer.",
                    "label": 0
                },
                {
                    "sent": "So you can still do inference and learning.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that these features are learnable, so you can do some backpropagation to train this feature and so on, but essentially.",
                    "label": 0
                },
                {
                    "sent": "It is actually trained fairly easily so.",
                    "label": 0
                },
                {
                    "sent": "This type of model actually was used for making breakthroughs in speech recognition in 2012.",
                    "label": 1
                },
                {
                    "sent": "An around the time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are some other applications such as human pose or some applying to multimodal data or facial recognition and so on.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's actually fairly straightforward to apply this to many different scenarios.",
                    "label": 0
                },
                {
                    "sent": "So just as a remark, I just say that.",
                    "label": 0
                },
                {
                    "sent": "GBM is actually quite good generative model, although it's not probably state of the art, it can be used as a good model for learning distributed representation for handling some high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "And I talked fairly extensively about how this initialization VPN through the stacking of PBM's can be justifies in theory, but also maybe practical sense.",
                    "label": 0
                },
                {
                    "sent": "If you didn't get all the details, the take home message is that it can be also used as just initialization for the deep neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's actually known that for classification problem this DBN pretraining or GBM stacking can be still very useful.",
                    "label": 0
                },
                {
                    "sent": "Even these days when you don't have lots of huge amount of labeled data, OK so for the case of when you have like millions of labels such as image, net or some other equivalent large database then probably this pre training may not be useful.",
                    "label": 0
                },
                {
                    "sent": "But for for the case when you don't have lots of labels which may be actually the case when you maybe work on some new domains, such as maybe maybe medical or some health care problem, or maybe computational biology and so on, where you is not easy to get large amount of labels, then this type of stacking can be still very useful.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And also we talked about how this DBN can be viewed as some performing some probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See any questions here so far.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm pretty much done, but I'm yes.",
                    "label": 0
                },
                {
                    "sent": "So I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Set up the various.",
                    "label": 0
                },
                {
                    "sent": "Right, uh, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a very good question, so I think in practice people don't actually constrain the number of second layer units to be the same as the visible unit.",
                    "label": 0
                },
                {
                    "sent": "So people just use whatever arbitrary number of units for the second layer.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, theory doesn't hold in that type of situation.",
                    "label": 0
                },
                {
                    "sent": "But it still works in practice, so yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's another very interesting example of VPN that can be used for some probabilistic inference, so here.",
                    "label": 1
                },
                {
                    "sent": "For example, the problem is that you are given.",
                    "label": 0
                },
                {
                    "sent": "Some images but that are occluded, so you just provide the image where you only see the half of the image and then you are asked to info about the rest of the half.",
                    "label": 0
                },
                {
                    "sent": "So if you just use just a simple feedforward neural network, is is actually difficult to do some reasoning about this missing values but using the VPN.",
                    "label": 0
                },
                {
                    "sent": "If you just treat this as a probabilistic model, you can actually do some influence on this missing values, so this was done by Marco and runs at oh.",
                    "label": 0
                },
                {
                    "sent": "The CPI paper where basically you take the input image and then you just do feedforward infants and then you calculate this top layer hidden variables and then you.",
                    "label": 0
                },
                {
                    "sent": "You do generations so you just generate from this procedure so you will generate some reconstructed version of the space image an at that point.",
                    "label": 0
                },
                {
                    "sent": "It will kind of fill in some missing values in the first iteration.",
                    "label": 0
                },
                {
                    "sent": "And what they do is do this clamp for this observable, because you know, we know that these are just like observed.",
                    "label": 0
                },
                {
                    "sent": "So we just fix this, but you use the hallucinated version of this missing values as an input to the second iteration of bottom up influence.",
                    "label": 0
                },
                {
                    "sent": "So then that's the this third column, and then you repeat this bottom up and top down where you always clamp this bottom half an.",
                    "label": 0
                },
                {
                    "sent": "At the end.",
                    "label": 0
                },
                {
                    "sent": "You can basically hallucinate.",
                    "label": 0
                },
                {
                    "sent": "The rest of the half of the image and it actually gives quite decent results, so this is actually one example of how you can use the DBN as some probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So I should say that maybe further case where.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't have any ambiguity or missing value or some some confusing things in your data.",
                    "label": 0
                },
                {
                    "sent": "Then probably feedforward influences good enough.",
                    "label": 0
                },
                {
                    "sent": "So for example when you are just doing classification with noisy noisy listen, very unambiguous image, then people is probably fine.",
                    "label": 0
                },
                {
                    "sent": "And then just doing some pure backpropagation as a neural net is good enough when you have some kind of missing values or some uncertainties or.",
                    "label": 0
                },
                {
                    "sent": "Ambiguities in your observation?",
                    "label": 0
                },
                {
                    "sent": "Then this type of problems influence is actually useful thing.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I think that's actually a very good research question.",
                    "label": 0
                },
                {
                    "sent": "I don't think people really solve that question.",
                    "label": 0
                },
                {
                    "sent": "There are some work where, for example, using this type of model to generate some natural scene.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there are some some success where we can generate scenes or maybe small Patch like see 510.",
                    "label": 0
                },
                {
                    "sent": "Level image I guess Alan should have shown some examples of image generation, but for the case of really large large sized image where you have my hundreds of hundred 100 * 100 pixels, and if you want to generate some some fairly detailed high resolution scene of objects and some natural like.",
                    "label": 0
                },
                {
                    "sent": "Natural things, then I think that's actually an unsolved problem.",
                    "label": 0
                },
                {
                    "sent": "It's actually very hard problem, so generally speaking, generating image or generated model of image is actually not that easy so.",
                    "label": 0
                },
                {
                    "sent": "But maybe this type of model shows some promise that probably is also not the case that this is ultimately the right model to do.",
                    "label": 0
                },
                {
                    "sent": "I mean to you so people are still trying to develop better ways of doing this type of generative modeling these days.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me see I.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also yeah, since we have a few minutes left, I want to just talk a little bit about some slightly different way of.",
                    "label": 0
                },
                {
                    "sent": "Using deep are BMS for modeling I some distribution at the output space so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a kind of idea where.",
                    "label": 0
                },
                {
                    "sent": "Nowadays, I mean, we know that this type of RBM's or distributed representation is very useful for modeling a complex distribution.",
                    "label": 0
                },
                {
                    "sent": "And it has been very successful in dealing with complex input distributions, but when you want to work on some complex problems that deal with some out some output distribution that has large amounts of possibilities such as segmentation or detection for the case of computer vision problems, then maybe it's actually a good idea to actually also run our representation of these outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this type of problem, I mean, is known as a structural prediction problem.",
                    "label": 0
                },
                {
                    "sent": "So for example, there are so many examples, but for example, if you just consider A1 specific problem.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such as image segmentation.",
                    "label": 0
                },
                {
                    "sent": "This is how people solve the problem and maybe this also connects back to the undirected graphical model that are unexplained today.",
                    "label": 0
                },
                {
                    "sent": "So given the input image, you can basically construct some graph, some graphical model where the in this case you can consider the hidden nodes as as nodes as a sum label for this segment or some super pixel and you connect between this node.",
                    "label": 0
                },
                {
                    "sent": "Whenever these super pixels are adjacent an these label mode actually is connected to the feature, so some observation from the input image.",
                    "label": 0
                },
                {
                    "sent": "So you have some Singleton potential from the superpixel feature to this label.",
                    "label": 0
                },
                {
                    "sent": "And also there's some relationship between these adjacent labels.",
                    "label": 0
                },
                {
                    "sent": "So the intuition here is that you want to enforce some local consistency.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "If the two super pixels are adjacent, you want to assign same label for this superpixel.",
                    "label": 0
                },
                {
                    "sent": "However, this type of model, although it's actually very standard and it's actually one very popular model used in computer vision, called the paralyzed condition random field.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually infer some global consistency, and it's actually not that easy to do so.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by global consistency is some kind of global shape of this object, or in this case, is a face.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Is actually a very sensible thing to try to model this output space using RBM or some variant variant of PBM, so.",
                    "label": 0
                },
                {
                    "sent": "In this particular example, we can just think about GBM that's modeling this output space, so you can just treat this as a just pixel level image where you have some three type of outputs.",
                    "label": 0
                },
                {
                    "sent": "So you can think about one hot encoding, encoding, skin or hair or background, and so on.",
                    "label": 0
                },
                {
                    "sent": "And you can think about IBM that's modeling this stable space.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we actually just few things together, you can think about this.",
                    "label": 0
                },
                {
                    "sent": "Emma, another version of some hybrid graphical model where you are just stacking the conditional random field and putting the IBM on top.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this case, this turns out to be just the undirected model.",
                    "label": 0
                },
                {
                    "sent": "So since because the conditional random field is undirected and GBM is undirected, so we can just add the energy functions together.",
                    "label": 0
                },
                {
                    "sent": "So here the why is the label that we want to estimate given the image.",
                    "label": 0
                },
                {
                    "sent": "So we have energy terms coming from the conditional random field and also energy time coming from the PBM part, so that's.",
                    "label": 0
                },
                {
                    "sent": "Interesting here is that the PBM will basically infer some global consistency of this label space that's difficult to do.",
                    "label": 0
                },
                {
                    "sent": "So for the case of CRF.",
                    "label": 1
                },
                {
                    "sent": "So these are actually complementary.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out it turns out that this IBM part.",
                    "label": 0
                },
                {
                    "sent": "So if you visualize the hidden unit on top, it can actually encode some kind of pattern of this outputs such as maybe having appeared or long hair, or some direction of the Phase I mean, which way the faces?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking and so on and also if you just sample from this top layer, you can actually do some generation of these samples an they actually look quite decent so the top row shows the samples generated from the model.",
                    "label": 0
                },
                {
                    "sent": "The bottom row shows the closest match to the two.",
                    "label": 0
                },
                {
                    "sent": "Some examples in your training data.",
                    "label": 0
                },
                {
                    "sent": "So these are similar to each other but not identical, which means that the model is able to extrapolate from the training training.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data so that influence can be done actually quite straightforwardly.",
                    "label": 0
                },
                {
                    "sent": "So basically, since the hidden in it and this label units are latent in the inference time, we can do alternating Gibbs sampling or being filled in France.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you fix the H, then you can just calculate the top down influence from this hidden units, and that infers some global shape.",
                    "label": 0
                },
                {
                    "sent": "Constraint and then you can still do influence in the conditional random field.",
                    "label": 0
                },
                {
                    "sent": "So when you fix the H, then influencing the why is exactly the same as the original conditional random field.",
                    "label": 0
                },
                {
                    "sent": "An another iteration is to fix the wide variables and then do influence on the H and it is just the same inference as GBM that I just talked about.",
                    "label": 0
                },
                {
                    "sent": "It is just a sigmoid function of some W times this Y, so you can just repeat this alternating sampling or mean field an basically just adding this top layer doesn't really add much complexity in terms of the influence it is using the exactly the same inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "For the original.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RF but nice thing is they can actually infer some global consistency.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If we compare this type of model where we have conditional random field with RBM.",
                    "label": 0
                },
                {
                    "sent": "And if you see some baselines, such as just a logistic regression that only use a Singleton potential or the conditional random field that has paralyzed potential, we see that it makes some mistakes in the beginning and conditional random field tries to clean things up, but it actually doesn't successfully clean things up, because it only you don't need.",
                    "label": 0
                },
                {
                    "sent": "Consider some local consistency.",
                    "label": 0
                },
                {
                    "sent": "Whereas using this IBM as a popular as a shape prior and you can basically clean things up much better way 'cause it will try to penalize a lot when the segmentation looks actually unrealistic or it does if it doesn't look like a face shape and then it will.",
                    "label": 0
                },
                {
                    "sent": "It will incur lots of penalties.",
                    "label": 0
                },
                {
                    "sent": "So in the energy function.",
                    "label": 0
                },
                {
                    "sent": "So as a result it tries to find a more consistent.",
                    "label": 0
                },
                {
                    "sent": "Pattern that looks like an actual face.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some quantitative evaluations showing that this type of putting this top layer prior is a useful thing so.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, this is just one kind of way of using this RBM or distributed representation for modeling the output space, But this was maybe some anecdotal thing I wanted to talk about because we had a little bit of time, but maybe just let's go back to yeah, I'm basically done.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions I will be happy to answer and then also I'm happy to talk about this offline.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much, yeah.",
                    "label": 0
                }
            ]
        }
    }
}