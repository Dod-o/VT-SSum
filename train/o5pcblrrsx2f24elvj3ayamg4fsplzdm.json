{
    "id": "o5pcblrrsx2f24elvj3ayamg4fsplzdm",
    "title": "An Infinite Restricted Boltzmann Machine",
    "info": {
        "author": [
            "Marc-Alexandre C\u00f4t\u00e9, Universit\u00e9 de Sherbrooke"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_cote_boltzmann_machine/",
    "segmentation": [
        [
            "OK, so in this talk I will present you the work I've been doing with my supervisor golova shell.",
            "This is an extension on the restricted Boltzmann machine where we have an infinite number of hidden unit ends.",
            "The name, Infinite Resources Bozeman machine."
        ],
        [
            "And so we saw during the summer school alot of supervised learning.",
            "The best models were using a lot of label data for training, but this correcting those kind of data is really time consuming and we need someone to label.",
            "And it's maybe not available in all the men like for neuroimaging is some data labeled data is really hard to come by.",
            "And so we need some other expert order source of data.",
            "And as Russ mentioned it this morning, by far the largest."
        ],
        [
            "Sources, unlabeled data.",
            "So we just think about anything that has been generated by user, like you post a picture on Instagram or some tweets so that a lot of data coming and so we need some think about our algorithm that we use an new models that will be able to adapt to an input distribution that can change.",
            "So if there's a trend in the news, so probably the tweets that will come up.",
            "Will add this new information there, so we need the model to be able to adapt to that.",
            "And also it would be nice to have some sort of model that can wear the capacity of the model can grow during training.",
            "And this is exactly what I will speak."
        ],
        [
            "So the infinite IBM is basically an RBM where the capacity of the model is increasing during training.",
            "And this growing mechanism as you will, I will show you is obtained by simply modifying the energy function."
        ],
        [
            "So as we saw this morning, the RBM is generative stochastic neural network, which is composed here in of two.",
            "There's one visible visible units and the other one with hidden units.",
            "An is work.",
            "We only consider the binary case.",
            "And we can define an energy function that will map a scalar value to each possible configuration of the model, and we confirm that we can easily compute join the distribution there and Z is the partition function.",
            "So the first thing that we."
        ],
        [
            "Did here is to introduce the notion of ordering within the hidden layer an so, as you will see I will use a convention that the hidden units are going from the left to the right.",
            "So there's like an ordering.",
            "And to do that we added a new."
        ],
        [
            "Discrete random variable that will represent the number of hidden units that will contribute to the energy.",
            "And so if we look down at where in the energy function we need."
        ],
        [
            "Adapted to include that knew variable and what we see here is some where we only consider the Z. Firstly it in unit.",
            "And we also added a penalty term here.",
            "Better I, which represents basically the penalty of selecting that particular hidden unit an this penalty term is is part of the sum here so."
        ],
        [
            "As we select more hidden units definitely will goes up also.",
            "An that opportunity will go up, so now it was.",
            "So we add the value that is Z can take is from one to capital H. Capsulated is a finite number of hidden units.",
            "So now instead of having a finite number of the unit will take an infinite one.",
            "An everything will say basically the same, except that will make two assumptions.",
            "First that we all bias and weights associated to the Internet.",
            "It will be initialized to zero and that at any point during training will only have L hidden units that will have nonzero weights.",
            "And this those L IT units will be at the beginning of the hidden layer.",
            "Thanks to our ordering.",
            "An so."
        ],
        [
            "Yeah, OK, and the penalty term here.",
            "We need to be careful when we'll choose it.",
            "I will discuss about it in a few seconds, but we need to make sure that we pick one that.",
            "Will make our model will be well defined."
        ],
        [
            "An so useful function that we went working with PBM that we usually uses what we call the free energy, which is basically marginalized marginalization over the energies in the log coming and what we need to adapt it to also include RZ.",
            "And we we can see that we still have our summation there, that will only consider the ZZ in units.",
            "About right now.",
            "It's only the weight associated to this unit and we still have our penalty term.",
            "An so another thing that we need in order to be able to train this model, we need the conditional distribution of P of Z given V. So what this conditional distribution represent is basically how many feature doesn't model thinks it needs to be able to represent a given input.",
            "And we can see here and the denominator.",
            "We have an infinite sum an I will show you now how we can actually compute this."
        ],
        [
            "So the first thing that will do is split the sum in two part and so we have the first part that will go from Z = 1, two L and we have the rest of the summer.",
            "Here an inside the frenergy we add also some that will also split there.",
            "So we have the sum that would correspond to this term.",
            "Here the energy of V&L and we will have the rest of the summer and this last some here.",
            "An will it with some only hidden units or the weights and the bias of it and use that.",
            "Sorry, it will only consider the Internet that are after L, so we are the indexes L + 1 up to Zeke can be Infinity.",
            "So that means we can simplify here and that will give us stuff plus a 0.",
            "And now we that's now that we will make the addition of what kind of penalty we can use.",
            "And we decided to go with the stuff plus of the bias of a particular region unit an times a constant that will be an hyperparameter beta here.",
            "What it will do here with allow us to simplify?"
        ],
        [
            "The submission here to have a summation over a constant.",
            "Basically that only depends on the hyperparameters as we choose.",
            "An if we rewrite this infinite sum here we."
        ],
        [
            "In a clever way, we obtain Azure Metrics series here where the ratio over there will be between zero and one.",
            "If we make sure that our beta is greater than one.",
            "So that means that this sum converges and we can actually compute this conditional distribution.",
            "So with this.",
            "Initial distribution we can perform Gibbs sampling so as well."
        ],
        [
            "In GBM so, but we'll need the simple Z given the data 1st and after that we can simple are hidden units and are visible units.",
            "So we can."
        ],
        [
            "And use as similar approach As for the train RBM, which is a contrastive divergent or we can also use persistent CD's.",
            "We work too.",
            "Anne."
        ],
        [
            "We don't have to worry about having an infinite gradient because.",
            "We can show that the grison will be non zero only for the selected in units.",
            "Also, we can avoid having the model keep adding hidden units until the Infinity by simply adding some storm.",
            "Some amount of regularization on the weights of the.",
            "Updated units"
        ],
        [
            "So I will show you a video of training.",
            "So this is on the left the the.",
            "The filter associated in units being trained, so this is the epic one and we can see that the models will keep adding as it needs some hidden units and will see the filters share some common structure with the neighbors also, but they as the training will progress, will see that they will try to be different.",
            "But still a similar an under the right side is we are showing some negative sample that are produced by the Gibbs trains.",
            "Anne, what one thing that we observe when training our models is that having a different learning rate for each hidden units is really essential to make it work because the first hidden units with the filter assisted to the first news will receive more grades in more often because the model will choose them more often because they are at the beginning of the ordering.",
            "And we can see that that effect that also affect the last hidden units, which are a little bit less trained.",
            "Or they are undertrained.",
            "An so.",
            "We also play with the hyperparameter beta, which basically controls are our is the model to add a new Internet.",
            "So the larger the beta the order it is to add a new engine unit.",
            "But the model can still compensate for that penalty by just training more.",
            "So it basically will just will.",
            "It will require more epochs of training to achieve a comparative result.",
            "Come parable reason.",
            "And.",
            "So we."
        ],
        [
            "I run some experiments on Binarize Denison Caltech 101.",
            "Here we report the average negative log likelihood which we needed the partition function to compute those.",
            "We use an important sampling to estimate that partition function.",
            "We can see here that you can see that the we don't have better result in the PBM, but they are really competitive and we inside the margin of errors an.",
            "But the plus side is that we don't have to specify the number of hidden units.",
            "In advance an we have also an ordering effect in the feature, so that could be useful for certain tasks if you do some search you can start by looking at the first feature, because they are supposed to be the more prison in the data so the the more discriminative an.",
            "So I said."
        ],
        [
            "We can also visualize the conditional distribution of PFZ given V, which is really interesting to see what the model, how many units the model thing he needs depending on different example from the test set and the first sample here is simply random noise.",
            "Anne."
        ],
        [
            "So as future work we have some idea to reuse this dispersible for feedforward networks.",
            "We want to use it also for for search it can be binary search or we can implement a PBM with a softmax in.",
            "You need to have a more powerful search.",
            "Also, we have some idea of instead of having a flat representation of the hidden layer, we can maybe have it as a form of tree, so that could be useful for topic modeling.",
            "And this work also had been used for defining some word representation in the infinite Skip gram.",
            "So the code is available."
        ],
        [
            "On get up, if you want to reproduce the result an, there's also the paper on archive in.",
            "Recently we got publishing or computation."
        ],
        [
            "You're welcome, please, and thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this talk I will present you the work I've been doing with my supervisor golova shell.",
                    "label": 0
                },
                {
                    "sent": "This is an extension on the restricted Boltzmann machine where we have an infinite number of hidden unit ends.",
                    "label": 1
                },
                {
                    "sent": "The name, Infinite Resources Bozeman machine.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we saw during the summer school alot of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The best models were using a lot of label data for training, but this correcting those kind of data is really time consuming and we need someone to label.",
                    "label": 1
                },
                {
                    "sent": "And it's maybe not available in all the men like for neuroimaging is some data labeled data is really hard to come by.",
                    "label": 0
                },
                {
                    "sent": "And so we need some other expert order source of data.",
                    "label": 0
                },
                {
                    "sent": "And as Russ mentioned it this morning, by far the largest.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sources, unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So we just think about anything that has been generated by user, like you post a picture on Instagram or some tweets so that a lot of data coming and so we need some think about our algorithm that we use an new models that will be able to adapt to an input distribution that can change.",
                    "label": 0
                },
                {
                    "sent": "So if there's a trend in the news, so probably the tweets that will come up.",
                    "label": 0
                },
                {
                    "sent": "Will add this new information there, so we need the model to be able to adapt to that.",
                    "label": 0
                },
                {
                    "sent": "And also it would be nice to have some sort of model that can wear the capacity of the model can grow during training.",
                    "label": 1
                },
                {
                    "sent": "And this is exactly what I will speak.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the infinite IBM is basically an RBM where the capacity of the model is increasing during training.",
                    "label": 0
                },
                {
                    "sent": "And this growing mechanism as you will, I will show you is obtained by simply modifying the energy function.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as we saw this morning, the RBM is generative stochastic neural network, which is composed here in of two.",
                    "label": 0
                },
                {
                    "sent": "There's one visible visible units and the other one with hidden units.",
                    "label": 0
                },
                {
                    "sent": "An is work.",
                    "label": 0
                },
                {
                    "sent": "We only consider the binary case.",
                    "label": 0
                },
                {
                    "sent": "And we can define an energy function that will map a scalar value to each possible configuration of the model, and we confirm that we can easily compute join the distribution there and Z is the partition function.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did here is to introduce the notion of ordering within the hidden layer an so, as you will see I will use a convention that the hidden units are going from the left to the right.",
                    "label": 1
                },
                {
                    "sent": "So there's like an ordering.",
                    "label": 0
                },
                {
                    "sent": "And to do that we added a new.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discrete random variable that will represent the number of hidden units that will contribute to the energy.",
                    "label": 0
                },
                {
                    "sent": "And so if we look down at where in the energy function we need.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adapted to include that knew variable and what we see here is some where we only consider the Z. Firstly it in unit.",
                    "label": 0
                },
                {
                    "sent": "And we also added a penalty term here.",
                    "label": 0
                },
                {
                    "sent": "Better I, which represents basically the penalty of selecting that particular hidden unit an this penalty term is is part of the sum here so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we select more hidden units definitely will goes up also.",
                    "label": 0
                },
                {
                    "sent": "An that opportunity will go up, so now it was.",
                    "label": 0
                },
                {
                    "sent": "So we add the value that is Z can take is from one to capital H. Capsulated is a finite number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "So now instead of having a finite number of the unit will take an infinite one.",
                    "label": 0
                },
                {
                    "sent": "An everything will say basically the same, except that will make two assumptions.",
                    "label": 0
                },
                {
                    "sent": "First that we all bias and weights associated to the Internet.",
                    "label": 0
                },
                {
                    "sent": "It will be initialized to zero and that at any point during training will only have L hidden units that will have nonzero weights.",
                    "label": 0
                },
                {
                    "sent": "And this those L IT units will be at the beginning of the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Thanks to our ordering.",
                    "label": 0
                },
                {
                    "sent": "An so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, and the penalty term here.",
                    "label": 0
                },
                {
                    "sent": "We need to be careful when we'll choose it.",
                    "label": 0
                },
                {
                    "sent": "I will discuss about it in a few seconds, but we need to make sure that we pick one that.",
                    "label": 0
                },
                {
                    "sent": "Will make our model will be well defined.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An so useful function that we went working with PBM that we usually uses what we call the free energy, which is basically marginalized marginalization over the energies in the log coming and what we need to adapt it to also include RZ.",
                    "label": 1
                },
                {
                    "sent": "And we we can see that we still have our summation there, that will only consider the ZZ in units.",
                    "label": 0
                },
                {
                    "sent": "About right now.",
                    "label": 1
                },
                {
                    "sent": "It's only the weight associated to this unit and we still have our penalty term.",
                    "label": 1
                },
                {
                    "sent": "An so another thing that we need in order to be able to train this model, we need the conditional distribution of P of Z given V. So what this conditional distribution represent is basically how many feature doesn't model thinks it needs to be able to represent a given input.",
                    "label": 1
                },
                {
                    "sent": "And we can see here and the denominator.",
                    "label": 0
                },
                {
                    "sent": "We have an infinite sum an I will show you now how we can actually compute this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing that will do is split the sum in two part and so we have the first part that will go from Z = 1, two L and we have the rest of the summer.",
                    "label": 0
                },
                {
                    "sent": "Here an inside the frenergy we add also some that will also split there.",
                    "label": 0
                },
                {
                    "sent": "So we have the sum that would correspond to this term.",
                    "label": 0
                },
                {
                    "sent": "Here the energy of V&L and we will have the rest of the summer and this last some here.",
                    "label": 0
                },
                {
                    "sent": "An will it with some only hidden units or the weights and the bias of it and use that.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it will only consider the Internet that are after L, so we are the indexes L + 1 up to Zeke can be Infinity.",
                    "label": 0
                },
                {
                    "sent": "So that means we can simplify here and that will give us stuff plus a 0.",
                    "label": 0
                },
                {
                    "sent": "And now we that's now that we will make the addition of what kind of penalty we can use.",
                    "label": 0
                },
                {
                    "sent": "And we decided to go with the stuff plus of the bias of a particular region unit an times a constant that will be an hyperparameter beta here.",
                    "label": 0
                },
                {
                    "sent": "What it will do here with allow us to simplify?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The submission here to have a summation over a constant.",
                    "label": 0
                },
                {
                    "sent": "Basically that only depends on the hyperparameters as we choose.",
                    "label": 0
                },
                {
                    "sent": "An if we rewrite this infinite sum here we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a clever way, we obtain Azure Metrics series here where the ratio over there will be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "If we make sure that our beta is greater than one.",
                    "label": 0
                },
                {
                    "sent": "So that means that this sum converges and we can actually compute this conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So with this.",
                    "label": 0
                },
                {
                    "sent": "Initial distribution we can perform Gibbs sampling so as well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In GBM so, but we'll need the simple Z given the data 1st and after that we can simple are hidden units and are visible units.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And use as similar approach As for the train RBM, which is a contrastive divergent or we can also use persistent CD's.",
                    "label": 0
                },
                {
                    "sent": "We work too.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't have to worry about having an infinite gradient because.",
                    "label": 0
                },
                {
                    "sent": "We can show that the grison will be non zero only for the selected in units.",
                    "label": 0
                },
                {
                    "sent": "Also, we can avoid having the model keep adding hidden units until the Infinity by simply adding some storm.",
                    "label": 0
                },
                {
                    "sent": "Some amount of regularization on the weights of the.",
                    "label": 0
                },
                {
                    "sent": "Updated units",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will show you a video of training.",
                    "label": 0
                },
                {
                    "sent": "So this is on the left the the.",
                    "label": 0
                },
                {
                    "sent": "The filter associated in units being trained, so this is the epic one and we can see that the models will keep adding as it needs some hidden units and will see the filters share some common structure with the neighbors also, but they as the training will progress, will see that they will try to be different.",
                    "label": 0
                },
                {
                    "sent": "But still a similar an under the right side is we are showing some negative sample that are produced by the Gibbs trains.",
                    "label": 0
                },
                {
                    "sent": "Anne, what one thing that we observe when training our models is that having a different learning rate for each hidden units is really essential to make it work because the first hidden units with the filter assisted to the first news will receive more grades in more often because the model will choose them more often because they are at the beginning of the ordering.",
                    "label": 0
                },
                {
                    "sent": "And we can see that that effect that also affect the last hidden units, which are a little bit less trained.",
                    "label": 0
                },
                {
                    "sent": "Or they are undertrained.",
                    "label": 0
                },
                {
                    "sent": "An so.",
                    "label": 0
                },
                {
                    "sent": "We also play with the hyperparameter beta, which basically controls are our is the model to add a new Internet.",
                    "label": 0
                },
                {
                    "sent": "So the larger the beta the order it is to add a new engine unit.",
                    "label": 0
                },
                {
                    "sent": "But the model can still compensate for that penalty by just training more.",
                    "label": 0
                },
                {
                    "sent": "So it basically will just will.",
                    "label": 0
                },
                {
                    "sent": "It will require more epochs of training to achieve a comparative result.",
                    "label": 0
                },
                {
                    "sent": "Come parable reason.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I run some experiments on Binarize Denison Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "Here we report the average negative log likelihood which we needed the partition function to compute those.",
                    "label": 0
                },
                {
                    "sent": "We use an important sampling to estimate that partition function.",
                    "label": 0
                },
                {
                    "sent": "We can see here that you can see that the we don't have better result in the PBM, but they are really competitive and we inside the margin of errors an.",
                    "label": 0
                },
                {
                    "sent": "But the plus side is that we don't have to specify the number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "In advance an we have also an ordering effect in the feature, so that could be useful for certain tasks if you do some search you can start by looking at the first feature, because they are supposed to be the more prison in the data so the the more discriminative an.",
                    "label": 0
                },
                {
                    "sent": "So I said.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also visualize the conditional distribution of PFZ given V, which is really interesting to see what the model, how many units the model thing he needs depending on different example from the test set and the first sample here is simply random noise.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as future work we have some idea to reuse this dispersible for feedforward networks.",
                    "label": 0
                },
                {
                    "sent": "We want to use it also for for search it can be binary search or we can implement a PBM with a softmax in.",
                    "label": 0
                },
                {
                    "sent": "You need to have a more powerful search.",
                    "label": 0
                },
                {
                    "sent": "Also, we have some idea of instead of having a flat representation of the hidden layer, we can maybe have it as a form of tree, so that could be useful for topic modeling.",
                    "label": 0
                },
                {
                    "sent": "And this work also had been used for defining some word representation in the infinite Skip gram.",
                    "label": 0
                },
                {
                    "sent": "So the code is available.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On get up, if you want to reproduce the result an, there's also the paper on archive in.",
                    "label": 0
                },
                {
                    "sent": "Recently we got publishing or computation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're welcome, please, and thank you.",
                    "label": 0
                }
            ]
        }
    }
}