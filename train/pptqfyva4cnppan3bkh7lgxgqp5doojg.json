{
    "id": "pptqfyva4cnppan3bkh7lgxgqp5doojg",
    "title": "Introduction to Reinforcement Learning and Bayesian learning",
    "info": {
        "author": [
            "Mohammad Ghavamzadeh, Department of Computing Science, University of Alberta"
        ],
        "published": "June 22, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_ghavamzadeh_itrl/",
    "segmentation": [
        [
            "Placement, learning and Bayesian approach.",
            "It's going."
        ],
        [
            "Your very quick introduction.",
            "So sequential decision making under uncertainty is one of the fundamental problems in artificial intelligence.",
            "Questions such as how can I move around in the physical world like driving and navigation?",
            "How can I play and win a game, or how can I retrieve information over the web?",
            "How to do medical diagnosis and treatment, maximizing the throughput of the factory or optimizing the performance of a rescue team in an area hit by natural disaster?",
            "Are all examples of sequential decision making under uncertainty that arise in everyday life?"
        ],
        [
            "So reinforcement learning is a learning paradigm which is well suited to sequential decision making under uncertainty in reinforcement learning is a class of learning problems in which an agent interacts with an unfamiliar, dynamic and stochastic environment, and whose goal is to learn a policy to maximize some measure of its long-term performance.",
            "And the agents interaction with the environment can be modeled as a Markov decision process or MDP, or in the case that the state of the system may not be always completely observable to the agent as a partially observable Markov decision process or palm DP.",
            "So let's focus on MVP, the observer."
        ],
        [
            "Case for now.",
            "A Markov decision process, or MDP, is defined as a 5 cupo setup states X set of action A.",
            "The probability distribution or transition distribution P and which basically defines what's going to happen if I take action A in a state X, what's going to be the next state and the reward distribution?",
            "This is the general case that reward can be stochastic, so we have a reward distribution Q.",
            "And our X on a, which is the reward of taking action A in a state.",
            "X is a random variable distributed according to Q.",
            "And we also have an initial state distribution P0.",
            "And what is policy policy is a mapping from states to actions we call it deterministic policy or a mapping from a stage to distributions over actions, which is a stochastic policy."
        ],
        [
            "To see how best this is, an example to show you how we can formulate a problem as a Markov decision process.",
            "This is the game of backgammon, so you're a state at every time.",
            "Is the possible position of those.",
            "Sorry is the position of those 30 checkers on the board plus the outcome of the current rolled ice class, the person, the player who is to make the move.",
            "So that's the state.",
            "Your action is all the legal or permissible actions that are available to you.",
            "At every step and the very simple reward function for this game is 1 for win negative 14 lose for loss and 04 otherwise.",
            "So if you look at this game, we have 21 possible outcome for dice roll and we have about 20 legal move for each outcome of the dice which has a large branching factor.",
            "So only a very short.",
            "And very shallow look ahead is possible in this case."
        ],
        [
            "So just to give you a list of some applications of RLRL has had success in different domains in the past 1015 years, like the game of mccaman like the inventory management, dynamic channel allocation, elevator scheduling, a lot of application actually in RoboCop, RoboCop Soccer, many robotics application, right navigation by pedal walking.",
            "Grasping and switching between skills and the kind of famous application.",
            "By Andrew ingame the helicopter control and many more applications that you can find on this website at University of Michigan."
        ],
        [
            "Now let's see who we now.",
            "We know the goal is to find a good policy in order to maximize our performance.",
            "Now let's take a closer look at what how we achieve this goal and basically solve this learning problem.",
            "So, and these are some concepts that are being used in this tutorial, so it's good to know about those.",
            "Let's see what is value function.",
            "So when you function is the expected possibly discounted?",
            "This gamma is standing for discount factor.",
            "Expected return for a state or a state action pair, so this is the state value function and this is the extent action value function.",
            "So basically very simply."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Placement, learning and Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "It's going.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your very quick introduction.",
                    "label": 0
                },
                {
                    "sent": "So sequential decision making under uncertainty is one of the fundamental problems in artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "Questions such as how can I move around in the physical world like driving and navigation?",
                    "label": 1
                },
                {
                    "sent": "How can I play and win a game, or how can I retrieve information over the web?",
                    "label": 1
                },
                {
                    "sent": "How to do medical diagnosis and treatment, maximizing the throughput of the factory or optimizing the performance of a rescue team in an area hit by natural disaster?",
                    "label": 0
                },
                {
                    "sent": "Are all examples of sequential decision making under uncertainty that arise in everyday life?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So reinforcement learning is a learning paradigm which is well suited to sequential decision making under uncertainty in reinforcement learning is a class of learning problems in which an agent interacts with an unfamiliar, dynamic and stochastic environment, and whose goal is to learn a policy to maximize some measure of its long-term performance.",
                    "label": 1
                },
                {
                    "sent": "And the agents interaction with the environment can be modeled as a Markov decision process or MDP, or in the case that the state of the system may not be always completely observable to the agent as a partially observable Markov decision process or palm DP.",
                    "label": 0
                },
                {
                    "sent": "So let's focus on MVP, the observer.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case for now.",
                    "label": 0
                },
                {
                    "sent": "A Markov decision process, or MDP, is defined as a 5 cupo setup states X set of action A.",
                    "label": 1
                },
                {
                    "sent": "The probability distribution or transition distribution P and which basically defines what's going to happen if I take action A in a state X, what's going to be the next state and the reward distribution?",
                    "label": 0
                },
                {
                    "sent": "This is the general case that reward can be stochastic, so we have a reward distribution Q.",
                    "label": 0
                },
                {
                    "sent": "And our X on a, which is the reward of taking action A in a state.",
                    "label": 0
                },
                {
                    "sent": "X is a random variable distributed according to Q.",
                    "label": 0
                },
                {
                    "sent": "And we also have an initial state distribution P0.",
                    "label": 0
                },
                {
                    "sent": "And what is policy policy is a mapping from states to actions we call it deterministic policy or a mapping from a stage to distributions over actions, which is a stochastic policy.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see how best this is, an example to show you how we can formulate a problem as a Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "This is the game of backgammon, so you're a state at every time.",
                    "label": 0
                },
                {
                    "sent": "Is the possible position of those.",
                    "label": 0
                },
                {
                    "sent": "Sorry is the position of those 30 checkers on the board plus the outcome of the current rolled ice class, the person, the player who is to make the move.",
                    "label": 0
                },
                {
                    "sent": "So that's the state.",
                    "label": 0
                },
                {
                    "sent": "Your action is all the legal or permissible actions that are available to you.",
                    "label": 0
                },
                {
                    "sent": "At every step and the very simple reward function for this game is 1 for win negative 14 lose for loss and 04 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this game, we have 21 possible outcome for dice roll and we have about 20 legal move for each outcome of the dice which has a large branching factor.",
                    "label": 0
                },
                {
                    "sent": "So only a very short.",
                    "label": 0
                },
                {
                    "sent": "And very shallow look ahead is possible in this case.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give you a list of some applications of RLRL has had success in different domains in the past 1015 years, like the game of mccaman like the inventory management, dynamic channel allocation, elevator scheduling, a lot of application actually in RoboCop, RoboCop Soccer, many robotics application, right navigation by pedal walking.",
                    "label": 1
                },
                {
                    "sent": "Grasping and switching between skills and the kind of famous application.",
                    "label": 0
                },
                {
                    "sent": "By Andrew ingame the helicopter control and many more applications that you can find on this website at University of Michigan.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see who we now.",
                    "label": 0
                },
                {
                    "sent": "We know the goal is to find a good policy in order to maximize our performance.",
                    "label": 0
                },
                {
                    "sent": "Now let's take a closer look at what how we achieve this goal and basically solve this learning problem.",
                    "label": 0
                },
                {
                    "sent": "So, and these are some concepts that are being used in this tutorial, so it's good to know about those.",
                    "label": 0
                },
                {
                    "sent": "Let's see what is value function.",
                    "label": 1
                },
                {
                    "sent": "So when you function is the expected possibly discounted?",
                    "label": 0
                },
                {
                    "sent": "This gamma is standing for discount factor.",
                    "label": 0
                },
                {
                    "sent": "Expected return for a state or a state action pair, so this is the state value function and this is the extent action value function.",
                    "label": 1
                },
                {
                    "sent": "So basically very simply.",
                    "label": 0
                }
            ]
        }
    }
}