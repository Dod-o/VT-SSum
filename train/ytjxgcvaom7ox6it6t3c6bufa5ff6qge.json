{
    "id": "ytjxgcvaom7ox6it6t3c6bufa5ff6qge",
    "title": "Planning under Uncertainty Using Distributions over Posteriors",
    "info": {
        "author": [
            "Nicholas Roy, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_roy_puuudop/",
    "segmentation": [
        [
            "Thanks very much.",
            "So this is kind of interesting workshop because people from a couple of different backgrounds.",
            "Um, I wanted to start by talking a little bit.",
            "Just give you a motivating example of some work that my group has been doing in.",
            "Planning uncertainty."
        ],
        [
            "Particular on quadrotor helicopters, there's been a lot of interest in the last couple of years as these quadrotor helicopter platforms, which are about the size of I guess a dinner plate have become very freely available.",
            "If you buy cheap sensors, low weight, low power sensors like The Who Laser range Finder, you can use these for autonomous control navigation of the quad rotor.",
            "So for example this is just a short video of our helicopter exploring this data center building a map.",
            "This sort of technology that we've known about on ground vehicles for several years now.",
            "And really, you know by taking those algorithm technologies, slam, etc, putting them on the vehicle and being a little bit smart about things that you do, you can actually get the vehicles operating nicely of course."
        ],
        [
            "Quad rotor this is a map of this data center.",
            "It's a sort of thing that you'd expect from like a pioneer driving around.",
            "Of course, the quadrotor exists in three dimensions, and so you'd really."
        ],
        [
            "Like it to be able to do 3 dimensional operations as well.",
            "So for instance little 3 dimensional map of sorts at window or something like that and then fly through the window.",
            "This is all done purely autonomously.",
            "The kind of motivating scenario for these kinds of things is, say, your first responder responding to like 3 Mile Island disaster, and you use the helicopter to find a window, fly into a building and then map the building for the first responders and also go find the control panel and tell you whether or not the building is safe to enter.",
            "So that's the kind of problem that we're working on.",
            "ANWR."
        ],
        [
            "You know one of the things I'm going to advertise it about we can do is we can now fly everywhere pretty much anywhere.",
            "So we also flew the helicopter, not just indoors but also over MIT campus.",
            "And we built a map of MIT campus flying, sort of around starting from Master St through pass this data center around.",
            "There's a little bit of misalignment right here because we actually couldn't go over top for reasons that I don't quite remember.",
            "This is all GPS denied an so Abe had to hold the door open here.",
            "Anatomy of the student, How the other door on the other side.",
            "This quarter helicopter flies through the building out the other side and keeps on building a map of this.",
            "Courtyard here now the helicopter is interesting from a planning under uncertainty perspective, because even though we've got this great sensor which allows us to build Maps and things like that and build 3 dimensional models, it is a very limited field of view sensor.",
            "And so you've got to think very hard about the things that you're looking at in order to keep the vehicle stable.",
            "And if you're doing more tasks like target tracking or something like that where you don't necessarily know where the target is, a target is going to go in and out of the field of view of the vehicle.",
            "Then you've got to plan carefully, but the trajectory is that you take in order to keep yourself stable.",
            "And to keep you keep the target in a place where you're going to recover it."
        ],
        [
            "I'm not going to talk much about the technical challenges of actually doing this on the helicopter.",
            "I come from a ground vehicle background and so one of the hard things for me to learn was just how fast the dynamics are and how fast your estimate the velocity is of the helicopter.",
            "But the thing that's sort of the same on both ground vehicles.",
            "Navigating in GPS denied environments, and these helicopters is that you have to be smart about how you plan and so that takes us into the planning under uncertainty domain.",
            "Now I'm going to in the interest of time, I'm going to skip over sort of everything that has happened in the last 10 or 15 years in terms of planning under uncertainty and just get to the."
        ],
        [
            "What I think is sort of the most interesting thing in this come in the last couple of years, which is that if you know about planning or uncertainty, either from the control perspective than you think, Gee, linear, quadratic, Gaussian.",
            "If you know about it from the planning and sort of machine learning perspective, than you think about Palm, DP's partially observable Markov decision processes, and both of these techniques depend heavily on the idea of feedback from the environment, so I'm not going to talk much about the control perspective.",
            "I'm going to talk mostly from the Palm DP perspective, but the recent.",
            "Results have indicated that forward search building forward search and belief space is a really effective way to plan under uncertainty if you know a lot about Palm DP's and you know about building policy, dynamic programming Alpha vectors and I'm going to make a very strong statement which we can argue about later is that the day of Alpha vectors is over and it's time to think more about forward search.",
            "So let's just take this, just let me take it briefly through how forward search operates.",
            "In information space, so of course you know, search typically involves trees, and because we're planning under uncertainty, we're taking actions and we're getting observations.",
            "So in the case of quadrotor you know you are control and the quarter moves a little bit, and then it gets an observation about where it is in the environment.",
            "It gets an observation about where what the target is that it's tracking and so you can build this and or tree now like actions where you choosing one of these actions.",
            "So this is the OR part and then after an action you get observations and this is the end part.",
            "And because you don't know which observation you're going to get, you have to average over all possible observations.",
            "Those observations implicitly give rise to beliefs, and then you build this tree out to some depth, and then at the bottom of the tree you have a sequence of beliefs that a set of possible candidate beliefs.",
            "That's a result of the actions that you took in the observations that you receive, and you just do standard Bayesian filtering for what these beliefs might be."
        ],
        [
            "And then each of those beliefs has some kind of expected value.",
            "So."
        ],
        [
            "You compute the expected value for these beliefs and then each of those beliefs itself has a probability according to the observation that you took, and so you can propagate those beliefs back up the tree average over their likelihood, and then whichever action had the highest expected value at the root.",
            "Well, you choose that as your best policy.",
            "And this is not.",
            "Mark, I can't see the thing.",
            "Thanks.",
            "So this is, you know, by no means, particularly difficult algorithm, and the recent results by zhopa newsgroup at McGill, along with Brian shoved right.",
            "Laval have indicated this is a very very powerful technique for planning and complicated domains, which it typically don't do is you don't construct a policy which you do is you can start to plan.",
            "You build the planet, you choose an action, and then you repeat the process over and over again."
        ],
        [
            "The advantage of this is that focuses on the reachable set of beliefs.",
            "You know you're only searching from this one belief that you have right now, and you do that, and then you repeat over and over and over again.",
            "Now it might be the case that you're repeating a lot of work, but on the other hand I results from, I guess, about 20 years ago now from Demetrius, Persicus and Johnson.",
            "Others indicate that this is guaranteed to be a stable policy and or some relatively mild assumptions.",
            "Other advantages of forward search that you're doing the real work besides building the tree, is doing the propagation of the beliefs through the tree.",
            "And so this can be a very very expensive operation.",
            "But if you're living in a continuous domain or you have a factor discrete model and you can actually do that very, very efficiently in a way that other solution techniques don't lie to, take advantage of factor models.",
            "And the other thing is that we don't just have to do Palm DP's refugees.",
            "We could have all kinds of funky models in here with all kinds of I don't know Rao Blackwell's beliefs or something like that in here that gives measures of efficiency that we could take advantage of.",
            "Any other thing is that we don't even have to do expected reward down here.",
            "The leaves you could be doing information theoretic optimization or something like that.",
            "But the big problem with forward search is that it scales poorly with horizon length.",
            "The best forward search algorithms.",
            "Right now we're really not searching more than three or four levels deep, and that's just not going to cut it for certain problems."
        ],
        [
            "My hypothesis is that the big problem you know is that you've got these solution techniques, regardless of whether you're trying to optimize using a milk or you're trying to optimize using something else is that you're trying to compute what you're supposed to do is a feedback of the observation on every step.",
            "But there's many, many real world problems where you can expect the sequence of actions to basically be invariant to any sort of reasonable observations that you might get if you're driving through an environment you know you've got a map of the diamond isn't changing that frequently, and so you can plan chunks of actions forward in time.",
            "That are relatively insensitive to whatever observations you're going to get target tracking.",
            "If you don't see the target right now, well, the first thing to do is going to fly fine at Target, and you have a reasonable expectation you're not going to see anything for awhile, and this is true of other models as well."
        ],
        [
            "So the way that we achieve that is that what I'm going to say is I'm going to make my policy invariant to the observations for some period of time, and I'm going to do this for the idea of macro actions as I'm going to define a macro action as a fixed length sequence.",
            "I fixed like the open loop policy of a defined set of actions, so I'm going to choose the first section of the macro action I'm going to receive some observation.",
            "I don't know what what is ahead of time, and then I'm going to take another action.",
            "Then I'm going to also choose ahead of time.",
            "I'm not going to make it conditioned on.",
            "Circular object, whatever these observation was.",
            "Um?"
        ],
        [
            "And so I'm going to plan using this.",
            "See this sequence of actions.",
            "Here is just two as a single action or single macro action.",
            "I'm implicitly restricting the policy class because I can't if it's important that I react to whatever option they get here, I'm not going to be able to, but if my hypothesis is correct, then I actually don't need to condition on many things in the environment.",
            "And if I choose my macro actions correctly such that I'm only conditioning at the end of macro actions, then what is going to allow me to do?",
            "Is the search process is going to treat this as a single action, so if my search process can only go three or four ply?",
            "As most search processes right now can, if I have longer macro actions, I'm actually seemingly going 3 or 4 deep, but I'm actually going many, many more actions into the future, and that's going to be important for a number of problems."
        ],
        [
            "There is 1.",
            "There's a couple of technical challenges, so in terms of the forward search process, the first thing we do is we propagate the bullies forward.",
            "Depending on the action, the observations and that doesn't change, that's just as it was before.",
            "The only thing is is that I'm not branching on actions after every observation, but the other thing I have to do is to be able to propagate expected rewards back up.",
            "Now I have to the question is how do I actually compute this expect reward of these beliefs?",
            "It's each of these beliefs and expectation, but I remember I'm doing expectations not only over states but also over the observations I received down this."
        ],
        [
            "So one thing I could do is for a particular macro action, let's just look at one macro action at a time.",
            "I could.",
            "You know, for this particular action, I could just exhaustively enumerate all possible observations that I'd get, which is just as if I was doing previously in the forward search.",
            "And then I could propagate their rewards back up, but that's really slow and painful and I'm really trying to go forward in time if I'm really still paying the full cost of I've saved something on the branching of the actions.",
            "But I'm still paying the full cost of branching all the observations here just for the purposes of compute expected reward, and that's going to kill me.",
            "Another thing that."
        ],
        [
            "Could do is I could just sample possible observation sequences and I could do like a Monte Carlo integration over over that and that actually works surprisingly well.",
            "But there is one other thing that we could do, which is to implicitly notice.",
            "That the sequence of observations that I get along here for this particular macro action, first taking a one than a three, implicitly creates a distribution over beliefs at the end of the macro action."
        ],
        [
            "So I could draw the picture really like this where I have a macro actions A1A.",
            "Three and then whatever observations I get along here and implicitly define a distribution over this.",
            "Now if I'm generating this distribution by looking at the distribution of observation sequences, that doesn't help me in the slightest."
        ],
        [
            "But if I if my beliefs have a particular form, in particular, if they are Gaussian distributions.",
            "And in particular, if there are linear Gaussian.",
            "If there were results of a linear Gaussian process, which means that my dynamics in a continuous space are linear function of the action, the observations are also linear function of the state and I'm doing something like exact common filtering to compute the beliefs.",
            "Then I can actually compute in closed form a distribution over these beliefs in order one operation.",
            "I don't have to do the work of looking at all the options get along here.",
            "I can actually compute this very, very efficiently.",
            "Excuse me, I said order one.",
            "I meant order N. And as a length of the macro action, so just to be a little bit clear previously what we were doing is we were doing.",
            "You given a sequence of activations were competing a single posterior distribution for that particular action observation sequences.",
            "If I don't know the observation sequence ahead of time, I actually get a distribution over possible beliefs like that.",
            "And I have an exact parametric form for that distribution in this case, and if I don't have a linear Gaussian model, but I have like Bernoulli or something like that, this still approximations that allow me to approximate this distribution reasonably well.",
            "And since I have a parametric form for this distribution, I can compute the expected reward in closed form under some assumptions of the reward function."
        ],
        [
            "So how do I do the forward search process now?",
            "So one thing I could do is I could simply just do straight forward forward search.",
            "And you know, taken action, look at the posterior distribution of beliefs.",
            "Take a look at another sequence of macro actions and you know get out final posterior distribution over the beliefs at the end of each one of these macro actions each.",
            "So I'd have essentially what I'm doing is I'm chaining macro actions together in an open loop setting.",
            "I'm never conditioning on beliefs, that's a perfectly fine algorithm.",
            "In that it gives me a close form solution for the expected value of these macro actions and is guaranteed lower bound on the optimal value.",
            "However, because I'm never conditioning observations that turns out to be a really, really horrible sort of policy.",
            "I mean, it's a lower bound, but it's a long way from the lower back.",
            "What's the matter?",
            "What would you like me to do?",
            "Stretch it OIC.",
            "Is that better OK?"
        ],
        [
            "Alright, so one thing so I need to.",
            "I would like to be able to condition sometimes in observations and a natural place to do it is actually every time I get one of these sort of distributions over beliefs, so I don't have what I'd really like to be able to do is sort of partition this distribution over chunks of the belief space into places I'm going to take one action or another.",
            "I don't have a close form for that partitioning, so I'm going to sample.",
            "And I am going to do at this point a Monte Carlo integration over expected beliefs, sort of each one of these nodes where I'm going to do the optimization over each one of these sample belief separately.",
            "I no longer get a guarantee over the performance.",
            "I no longer have analytical calculation architecture award, but in practice it is actually a much better algorithm."
        ],
        [
            "In terms of computational complexity with you, don't use macros, macro actions, and just do straight forward forward search.",
            "Then your exponential in the horizon of your search.",
            "If you do use macro actions here you can see that all the terms are basically the total horizon length you want to search over, divided by the average length of your Mac corrections, and so we're getting substantial computational savings."
        ],
        [
            "So let's just look at one problem from the literature.",
            "This problem is due to.",
            "Tricks with every Simmons.",
            "It's essentially a scientific exploration problem where you gotta try to defy which rocks environment, or high value and sample them.",
            "Turns out this problem is super, super easy and no disrespect to trying to read, but it's actually probably not really worth studying as much studies.",
            "It's received in the literature, so we modified it.",
            "The big problem is that the value of information and the reward, or at the same place.",
            "So basically you just go and visit the rocks and you're done."
        ],
        [
            "So we modified it a little bit to put beacons in the environment that basically tell you the value of the rock, so you actually have to go visit the beacon and then go visit the rock there approx.",
            "In order to do well in this problem."
        ],
        [
            "If you take our modified problem and run it against our stop, which is the best palm DP solver Start Stop can't search far enough in the future that it has any idea that it has to visit the Beacons in order to get the value from the rocks.",
            "We do much better by being able to search far into the future.",
            "Having these macro actions that basically take us to the beacons take us to the rocks and the search only has to trade off which beacons in which rocks in order to visit."
        ],
        [
            "A whole pile of experimental comparisons which I'm not going to go through because I'm running out of time, but we do very, very well and our algorithm is an order of magnitude faster than other things that you might do in terms of forward search.",
            "Here this is sort of the best of the forward search algorithms that don't use macro actions, and we're in order of magnitude faster, and we're doing way better in terms of performance.",
            "I this in this case, this is a discrete problem and we are approximated with continuous values.",
            "In order to be able to take advantage of our trick of computing the expected over this distribution of beliefs so we do lose a little bit compared to the best possible thing that you might do, but we."
        ],
        [
            "Or faster.",
            "And the other thing is that we could scale to enormous problems because we have this continuous representation, because we're doing forward search, we can factor nicely, and so here we have a problem.",
            "That's a couple of orders, a couple of orders of magnitude larger than the largest problem that could be solved by standard palm DP solvers.",
            "And you know, it's slow now because 100 seconds where one before.",
            "But our fastest thing is only 60 times slower for much, much, much bigger problem."
        ],
        [
            "And the last thing I want to talk about is the target monitoring, where you actually now have multiple vehicles moving around an environment and you actually want to go figure."
        ],
        [
            "Where they are, we put this on just to close the loop on the helicopter.",
            "This is now a real world problem that's very continuous, relatively high dynamics and multi agent.",
            "It's factorizable which is nice because you have multiple vehicles you're going to start moving in a second and.",
            "This video is playing surprisingly slowly.",
            "'cause things do move faster than that.",
            "Even the helicopter mission.",
            "That which is weird."
        ],
        [
            "But let me skip ahead to the results.",
            "Is that were, you know, we could not solve this problem.",
            "We tried to use it, try to solve it using the best palm DP solvers.",
            "We worked with David Sues Group in Singapore to modify his code.",
            "They can't even load the transition matrices into memory, even though it's a factored representation.",
            "And so they certainly couldn't solve it.",
            "So we got a couple of heuristic algorithms from the literature, in particular, the MBR induce cut it out.",
            "And we're doing much better in terms of finding number of targets and our search times are sort of the same order of magnitude and it's able to deal with sort of the high dimensional continuous representation of the envir."
        ],
        [
            "There's a question about macro actions, but I'm not going to get into that."
        ],
        [
            "And I'll just wrap up and say that there's been a lot of work in the Palm DP literature and also increasing the controller and trying to push these horizons out as far as possible.",
            "Controllers down receding horizon control or coming up with good heuristic approximations.",
            "Palm DP literature has spent a lot of time trying to come up with better sampling algorithms in order to limit the space of samples that you're searching over in order to get around what Joel and Jeff and Sebastian called the Curse of History.",
            "My claim is that that's exactly the problem.",
            "To be working on, but sort of the wrong solution technique.",
            "What we really should be doing is searching as far into the future as we can.",
            "Can simply accepting the fact that not every policy needs to be considered if we prune our policy space and actually consider these open loop close to open loop fixed length macro actions then we would actually becomes possible to search in these large high dimensional domains there is this tradeoff between the depth of search and the number of conditional outcomes examined?",
            "I don't have anything to say about that, but I think this is the next obvious question to consider.",
            "And so I will stop there.",
            "Thank you very much.",
            "Left eye for few questions.",
            "Awesome.",
            "My corrections with different lengths.",
            "There's absolutely no reason why you couldn't, and the target tracking problem explicitly has macro actions of different links.",
            "Are proofs.",
            "Assume a fixed length of L in order to make the things workout nicely.",
            "If you had macro likes of varying lengths, the proofs might vary a little bit in terms of the complexity, but the algorithm itself will accept macro likes Max video length.",
            "The complexity of the problem in the construction of the macro actions which you didn't know because the space of my fractions is exponential in the Lane.",
            "More or less arbitrarily, you could forget about my crush and then track the macro action online.",
            "Like for example, the device and terministic function of the latest observations, including extraction.",
            "That way without.",
            "I completely agreed, with the exception of.",
            "I wouldn't concern matters that particular way, but you're right, you could so.",
            "But I think the most important thing you said is that you're right.",
            "We are hiding the complexity.",
            "That problem in the macro action that I talked about here and that if you really don't have a good set of macro actions, you're screwed.",
            "And what you'd really like to do is do it online.",
            "We do have a algorithm that actually could start them online.",
            "The nice thing about that is it gives you in anytime solution, so the limit of infinite in the limit of an infinite amount of computation.",
            "What you'd really like to do is converge to the full.",
            "The full search tree as if you done that from scratch.",
            "So what we currently do is we have a number of heuristics, including just random generation of macro actions, and we have the second question there.",
            "Is it?",
            "How do you take a current set of macro actions and improve it by one?",
            "And this is very related to the question of the question of addressing the verbal resolution letter.",
            "How do you take a current policy and split?",
            "The state space or extended the policy?",
            "That's a hard question.",
            "We don't have definitive answers on that, but you're absolutely, that is probably the right thing, is not stick with the current macro actions, but iteratively improvement given how much time you have left.",
            "Anymore questions.",
            "Is this sort of a certain approach?",
            "It is not.",
            "At least, it's certainly certainly certainty equivalent in the sense that we're assuming we know the model.",
            "There's no reason to assume that, it's just that we do know the models.",
            "In this case, if you don't know the model, it makes the belief propagation the expected value calculation harder.",
            "But we are.",
            "I guess you could view it as certainty equivalence along the macro actions is that you assume that no, that's not even true, because I'm computing the expected value along the macro action, so it's definitely not a certainty.",
            "Send it.",
            "Actually, somehow ignoring the noise and the optimal completion of the action.",
            "No, I'm not ignoring the noise because, again, I'm considering so certainly equivalent would assume that one sequence of things is going to happen, and I'm going to plan for that.",
            "And I'm going to take the reward of that, but it's not one sequence of thing I'm actually explicitly computing all possible observations along there, and I'm saying if I do this, what is the range of things that could happen, and what is expected value of that?",
            "So that's where it's different it I could see how it feels like certain equivalent in them committing up front to a particular set of things, but the only thing I'm committing up front to is a chunk of macro actions as opposed to changing the granularity of my policy, not of.",
            "How I I believe this article as much about evaluation is anything else.",
            "I'm not changing the evaluation from a traditional forward search.",
            "I'm gonna get.",
            "I'm not the type that you say.",
            "Hey I too uncertain I need to know.",
            "So this explicitly gives us the ability to do that.",
            "I didn't show the experiments.",
            "The results from those experiments.",
            "But we have experiments show exactly that.",
            "So that's a bank, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of interesting workshop because people from a couple of different backgrounds.",
                    "label": 0
                },
                {
                    "sent": "Um, I wanted to start by talking a little bit.",
                    "label": 0
                },
                {
                    "sent": "Just give you a motivating example of some work that my group has been doing in.",
                    "label": 0
                },
                {
                    "sent": "Planning uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Particular on quadrotor helicopters, there's been a lot of interest in the last couple of years as these quadrotor helicopter platforms, which are about the size of I guess a dinner plate have become very freely available.",
                    "label": 0
                },
                {
                    "sent": "If you buy cheap sensors, low weight, low power sensors like The Who Laser range Finder, you can use these for autonomous control navigation of the quad rotor.",
                    "label": 0
                },
                {
                    "sent": "So for example this is just a short video of our helicopter exploring this data center building a map.",
                    "label": 0
                },
                {
                    "sent": "This sort of technology that we've known about on ground vehicles for several years now.",
                    "label": 0
                },
                {
                    "sent": "And really, you know by taking those algorithm technologies, slam, etc, putting them on the vehicle and being a little bit smart about things that you do, you can actually get the vehicles operating nicely of course.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quad rotor this is a map of this data center.",
                    "label": 0
                },
                {
                    "sent": "It's a sort of thing that you'd expect from like a pioneer driving around.",
                    "label": 0
                },
                {
                    "sent": "Of course, the quadrotor exists in three dimensions, and so you'd really.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like it to be able to do 3 dimensional operations as well.",
                    "label": 0
                },
                {
                    "sent": "So for instance little 3 dimensional map of sorts at window or something like that and then fly through the window.",
                    "label": 0
                },
                {
                    "sent": "This is all done purely autonomously.",
                    "label": 0
                },
                {
                    "sent": "The kind of motivating scenario for these kinds of things is, say, your first responder responding to like 3 Mile Island disaster, and you use the helicopter to find a window, fly into a building and then map the building for the first responders and also go find the control panel and tell you whether or not the building is safe to enter.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of problem that we're working on.",
                    "label": 0
                },
                {
                    "sent": "ANWR.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know one of the things I'm going to advertise it about we can do is we can now fly everywhere pretty much anywhere.",
                    "label": 0
                },
                {
                    "sent": "So we also flew the helicopter, not just indoors but also over MIT campus.",
                    "label": 0
                },
                {
                    "sent": "And we built a map of MIT campus flying, sort of around starting from Master St through pass this data center around.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of misalignment right here because we actually couldn't go over top for reasons that I don't quite remember.",
                    "label": 0
                },
                {
                    "sent": "This is all GPS denied an so Abe had to hold the door open here.",
                    "label": 0
                },
                {
                    "sent": "Anatomy of the student, How the other door on the other side.",
                    "label": 0
                },
                {
                    "sent": "This quarter helicopter flies through the building out the other side and keeps on building a map of this.",
                    "label": 0
                },
                {
                    "sent": "Courtyard here now the helicopter is interesting from a planning under uncertainty perspective, because even though we've got this great sensor which allows us to build Maps and things like that and build 3 dimensional models, it is a very limited field of view sensor.",
                    "label": 0
                },
                {
                    "sent": "And so you've got to think very hard about the things that you're looking at in order to keep the vehicle stable.",
                    "label": 0
                },
                {
                    "sent": "And if you're doing more tasks like target tracking or something like that where you don't necessarily know where the target is, a target is going to go in and out of the field of view of the vehicle.",
                    "label": 0
                },
                {
                    "sent": "Then you've got to plan carefully, but the trajectory is that you take in order to keep yourself stable.",
                    "label": 0
                },
                {
                    "sent": "And to keep you keep the target in a place where you're going to recover it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to talk much about the technical challenges of actually doing this on the helicopter.",
                    "label": 0
                },
                {
                    "sent": "I come from a ground vehicle background and so one of the hard things for me to learn was just how fast the dynamics are and how fast your estimate the velocity is of the helicopter.",
                    "label": 0
                },
                {
                    "sent": "But the thing that's sort of the same on both ground vehicles.",
                    "label": 0
                },
                {
                    "sent": "Navigating in GPS denied environments, and these helicopters is that you have to be smart about how you plan and so that takes us into the planning under uncertainty domain.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to in the interest of time, I'm going to skip over sort of everything that has happened in the last 10 or 15 years in terms of planning under uncertainty and just get to the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I think is sort of the most interesting thing in this come in the last couple of years, which is that if you know about planning or uncertainty, either from the control perspective than you think, Gee, linear, quadratic, Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you know about it from the planning and sort of machine learning perspective, than you think about Palm, DP's partially observable Markov decision processes, and both of these techniques depend heavily on the idea of feedback from the environment, so I'm not going to talk much about the control perspective.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk mostly from the Palm DP perspective, but the recent.",
                    "label": 0
                },
                {
                    "sent": "Results have indicated that forward search building forward search and belief space is a really effective way to plan under uncertainty if you know a lot about Palm DP's and you know about building policy, dynamic programming Alpha vectors and I'm going to make a very strong statement which we can argue about later is that the day of Alpha vectors is over and it's time to think more about forward search.",
                    "label": 0
                },
                {
                    "sent": "So let's just take this, just let me take it briefly through how forward search operates.",
                    "label": 0
                },
                {
                    "sent": "In information space, so of course you know, search typically involves trees, and because we're planning under uncertainty, we're taking actions and we're getting observations.",
                    "label": 0
                },
                {
                    "sent": "So in the case of quadrotor you know you are control and the quarter moves a little bit, and then it gets an observation about where it is in the environment.",
                    "label": 0
                },
                {
                    "sent": "It gets an observation about where what the target is that it's tracking and so you can build this and or tree now like actions where you choosing one of these actions.",
                    "label": 0
                },
                {
                    "sent": "So this is the OR part and then after an action you get observations and this is the end part.",
                    "label": 0
                },
                {
                    "sent": "And because you don't know which observation you're going to get, you have to average over all possible observations.",
                    "label": 0
                },
                {
                    "sent": "Those observations implicitly give rise to beliefs, and then you build this tree out to some depth, and then at the bottom of the tree you have a sequence of beliefs that a set of possible candidate beliefs.",
                    "label": 0
                },
                {
                    "sent": "That's a result of the actions that you took in the observations that you receive, and you just do standard Bayesian filtering for what these beliefs might be.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then each of those beliefs has some kind of expected value.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You compute the expected value for these beliefs and then each of those beliefs itself has a probability according to the observation that you took, and so you can propagate those beliefs back up the tree average over their likelihood, and then whichever action had the highest expected value at the root.",
                    "label": 0
                },
                {
                    "sent": "Well, you choose that as your best policy.",
                    "label": 1
                },
                {
                    "sent": "And this is not.",
                    "label": 0
                },
                {
                    "sent": "Mark, I can't see the thing.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, by no means, particularly difficult algorithm, and the recent results by zhopa newsgroup at McGill, along with Brian shoved right.",
                    "label": 0
                },
                {
                    "sent": "Laval have indicated this is a very very powerful technique for planning and complicated domains, which it typically don't do is you don't construct a policy which you do is you can start to plan.",
                    "label": 0
                },
                {
                    "sent": "You build the planet, you choose an action, and then you repeat the process over and over again.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The advantage of this is that focuses on the reachable set of beliefs.",
                    "label": 0
                },
                {
                    "sent": "You know you're only searching from this one belief that you have right now, and you do that, and then you repeat over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "Now it might be the case that you're repeating a lot of work, but on the other hand I results from, I guess, about 20 years ago now from Demetrius, Persicus and Johnson.",
                    "label": 0
                },
                {
                    "sent": "Others indicate that this is guaranteed to be a stable policy and or some relatively mild assumptions.",
                    "label": 0
                },
                {
                    "sent": "Other advantages of forward search that you're doing the real work besides building the tree, is doing the propagation of the beliefs through the tree.",
                    "label": 0
                },
                {
                    "sent": "And so this can be a very very expensive operation.",
                    "label": 0
                },
                {
                    "sent": "But if you're living in a continuous domain or you have a factor discrete model and you can actually do that very, very efficiently in a way that other solution techniques don't lie to, take advantage of factor models.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that we don't just have to do Palm DP's refugees.",
                    "label": 0
                },
                {
                    "sent": "We could have all kinds of funky models in here with all kinds of I don't know Rao Blackwell's beliefs or something like that in here that gives measures of efficiency that we could take advantage of.",
                    "label": 0
                },
                {
                    "sent": "Any other thing is that we don't even have to do expected reward down here.",
                    "label": 0
                },
                {
                    "sent": "The leaves you could be doing information theoretic optimization or something like that.",
                    "label": 0
                },
                {
                    "sent": "But the big problem with forward search is that it scales poorly with horizon length.",
                    "label": 1
                },
                {
                    "sent": "The best forward search algorithms.",
                    "label": 0
                },
                {
                    "sent": "Right now we're really not searching more than three or four levels deep, and that's just not going to cut it for certain problems.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My hypothesis is that the big problem you know is that you've got these solution techniques, regardless of whether you're trying to optimize using a milk or you're trying to optimize using something else is that you're trying to compute what you're supposed to do is a feedback of the observation on every step.",
                    "label": 0
                },
                {
                    "sent": "But there's many, many real world problems where you can expect the sequence of actions to basically be invariant to any sort of reasonable observations that you might get if you're driving through an environment you know you've got a map of the diamond isn't changing that frequently, and so you can plan chunks of actions forward in time.",
                    "label": 0
                },
                {
                    "sent": "That are relatively insensitive to whatever observations you're going to get target tracking.",
                    "label": 0
                },
                {
                    "sent": "If you don't see the target right now, well, the first thing to do is going to fly fine at Target, and you have a reasonable expectation you're not going to see anything for awhile, and this is true of other models as well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way that we achieve that is that what I'm going to say is I'm going to make my policy invariant to the observations for some period of time, and I'm going to do this for the idea of macro actions as I'm going to define a macro action as a fixed length sequence.",
                    "label": 0
                },
                {
                    "sent": "I fixed like the open loop policy of a defined set of actions, so I'm going to choose the first section of the macro action I'm going to receive some observation.",
                    "label": 0
                },
                {
                    "sent": "I don't know what what is ahead of time, and then I'm going to take another action.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to also choose ahead of time.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to make it conditioned on.",
                    "label": 0
                },
                {
                    "sent": "Circular object, whatever these observation was.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I'm going to plan using this.",
                    "label": 1
                },
                {
                    "sent": "See this sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "Here is just two as a single action or single macro action.",
                    "label": 1
                },
                {
                    "sent": "I'm implicitly restricting the policy class because I can't if it's important that I react to whatever option they get here, I'm not going to be able to, but if my hypothesis is correct, then I actually don't need to condition on many things in the environment.",
                    "label": 0
                },
                {
                    "sent": "And if I choose my macro actions correctly such that I'm only conditioning at the end of macro actions, then what is going to allow me to do?",
                    "label": 0
                },
                {
                    "sent": "Is the search process is going to treat this as a single action, so if my search process can only go three or four ply?",
                    "label": 0
                },
                {
                    "sent": "As most search processes right now can, if I have longer macro actions, I'm actually seemingly going 3 or 4 deep, but I'm actually going many, many more actions into the future, and that's going to be important for a number of problems.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is 1.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of technical challenges, so in terms of the forward search process, the first thing we do is we propagate the bullies forward.",
                    "label": 0
                },
                {
                    "sent": "Depending on the action, the observations and that doesn't change, that's just as it was before.",
                    "label": 0
                },
                {
                    "sent": "The only thing is is that I'm not branching on actions after every observation, but the other thing I have to do is to be able to propagate expected rewards back up.",
                    "label": 0
                },
                {
                    "sent": "Now I have to the question is how do I actually compute this expect reward of these beliefs?",
                    "label": 0
                },
                {
                    "sent": "It's each of these beliefs and expectation, but I remember I'm doing expectations not only over states but also over the observations I received down this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing I could do is for a particular macro action, let's just look at one macro action at a time.",
                    "label": 0
                },
                {
                    "sent": "I could.",
                    "label": 0
                },
                {
                    "sent": "You know, for this particular action, I could just exhaustively enumerate all possible observations that I'd get, which is just as if I was doing previously in the forward search.",
                    "label": 1
                },
                {
                    "sent": "And then I could propagate their rewards back up, but that's really slow and painful and I'm really trying to go forward in time if I'm really still paying the full cost of I've saved something on the branching of the actions.",
                    "label": 0
                },
                {
                    "sent": "But I'm still paying the full cost of branching all the observations here just for the purposes of compute expected reward, and that's going to kill me.",
                    "label": 0
                },
                {
                    "sent": "Another thing that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could do is I could just sample possible observation sequences and I could do like a Monte Carlo integration over over that and that actually works surprisingly well.",
                    "label": 1
                },
                {
                    "sent": "But there is one other thing that we could do, which is to implicitly notice.",
                    "label": 0
                },
                {
                    "sent": "That the sequence of observations that I get along here for this particular macro action, first taking a one than a three, implicitly creates a distribution over beliefs at the end of the macro action.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I could draw the picture really like this where I have a macro actions A1A.",
                    "label": 0
                },
                {
                    "sent": "Three and then whatever observations I get along here and implicitly define a distribution over this.",
                    "label": 1
                },
                {
                    "sent": "Now if I'm generating this distribution by looking at the distribution of observation sequences, that doesn't help me in the slightest.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if I if my beliefs have a particular form, in particular, if they are Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "And in particular, if there are linear Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If there were results of a linear Gaussian process, which means that my dynamics in a continuous space are linear function of the action, the observations are also linear function of the state and I'm doing something like exact common filtering to compute the beliefs.",
                    "label": 0
                },
                {
                    "sent": "Then I can actually compute in closed form a distribution over these beliefs in order one operation.",
                    "label": 1
                },
                {
                    "sent": "I don't have to do the work of looking at all the options get along here.",
                    "label": 0
                },
                {
                    "sent": "I can actually compute this very, very efficiently.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, I said order one.",
                    "label": 0
                },
                {
                    "sent": "I meant order N. And as a length of the macro action, so just to be a little bit clear previously what we were doing is we were doing.",
                    "label": 0
                },
                {
                    "sent": "You given a sequence of activations were competing a single posterior distribution for that particular action observation sequences.",
                    "label": 0
                },
                {
                    "sent": "If I don't know the observation sequence ahead of time, I actually get a distribution over possible beliefs like that.",
                    "label": 0
                },
                {
                    "sent": "And I have an exact parametric form for that distribution in this case, and if I don't have a linear Gaussian model, but I have like Bernoulli or something like that, this still approximations that allow me to approximate this distribution reasonably well.",
                    "label": 0
                },
                {
                    "sent": "And since I have a parametric form for this distribution, I can compute the expected reward in closed form under some assumptions of the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do I do the forward search process now?",
                    "label": 1
                },
                {
                    "sent": "So one thing I could do is I could simply just do straight forward forward search.",
                    "label": 0
                },
                {
                    "sent": "And you know, taken action, look at the posterior distribution of beliefs.",
                    "label": 0
                },
                {
                    "sent": "Take a look at another sequence of macro actions and you know get out final posterior distribution over the beliefs at the end of each one of these macro actions each.",
                    "label": 1
                },
                {
                    "sent": "So I'd have essentially what I'm doing is I'm chaining macro actions together in an open loop setting.",
                    "label": 0
                },
                {
                    "sent": "I'm never conditioning on beliefs, that's a perfectly fine algorithm.",
                    "label": 0
                },
                {
                    "sent": "In that it gives me a close form solution for the expected value of these macro actions and is guaranteed lower bound on the optimal value.",
                    "label": 0
                },
                {
                    "sent": "However, because I'm never conditioning observations that turns out to be a really, really horrible sort of policy.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a lower bound, but it's a long way from the lower back.",
                    "label": 0
                },
                {
                    "sent": "What's the matter?",
                    "label": 0
                },
                {
                    "sent": "What would you like me to do?",
                    "label": 0
                },
                {
                    "sent": "Stretch it OIC.",
                    "label": 0
                },
                {
                    "sent": "Is that better OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so one thing so I need to.",
                    "label": 0
                },
                {
                    "sent": "I would like to be able to condition sometimes in observations and a natural place to do it is actually every time I get one of these sort of distributions over beliefs, so I don't have what I'd really like to be able to do is sort of partition this distribution over chunks of the belief space into places I'm going to take one action or another.",
                    "label": 1
                },
                {
                    "sent": "I don't have a close form for that partitioning, so I'm going to sample.",
                    "label": 0
                },
                {
                    "sent": "And I am going to do at this point a Monte Carlo integration over expected beliefs, sort of each one of these nodes where I'm going to do the optimization over each one of these sample belief separately.",
                    "label": 0
                },
                {
                    "sent": "I no longer get a guarantee over the performance.",
                    "label": 1
                },
                {
                    "sent": "I no longer have analytical calculation architecture award, but in practice it is actually a much better algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of computational complexity with you, don't use macros, macro actions, and just do straight forward forward search.",
                    "label": 1
                },
                {
                    "sent": "Then your exponential in the horizon of your search.",
                    "label": 0
                },
                {
                    "sent": "If you do use macro actions here you can see that all the terms are basically the total horizon length you want to search over, divided by the average length of your Mac corrections, and so we're getting substantial computational savings.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just look at one problem from the literature.",
                    "label": 0
                },
                {
                    "sent": "This problem is due to.",
                    "label": 0
                },
                {
                    "sent": "Tricks with every Simmons.",
                    "label": 0
                },
                {
                    "sent": "It's essentially a scientific exploration problem where you gotta try to defy which rocks environment, or high value and sample them.",
                    "label": 0
                },
                {
                    "sent": "Turns out this problem is super, super easy and no disrespect to trying to read, but it's actually probably not really worth studying as much studies.",
                    "label": 0
                },
                {
                    "sent": "It's received in the literature, so we modified it.",
                    "label": 0
                },
                {
                    "sent": "The big problem is that the value of information and the reward, or at the same place.",
                    "label": 0
                },
                {
                    "sent": "So basically you just go and visit the rocks and you're done.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we modified it a little bit to put beacons in the environment that basically tell you the value of the rock, so you actually have to go visit the beacon and then go visit the rock there approx.",
                    "label": 0
                },
                {
                    "sent": "In order to do well in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you take our modified problem and run it against our stop, which is the best palm DP solver Start Stop can't search far enough in the future that it has any idea that it has to visit the Beacons in order to get the value from the rocks.",
                    "label": 0
                },
                {
                    "sent": "We do much better by being able to search far into the future.",
                    "label": 0
                },
                {
                    "sent": "Having these macro actions that basically take us to the beacons take us to the rocks and the search only has to trade off which beacons in which rocks in order to visit.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A whole pile of experimental comparisons which I'm not going to go through because I'm running out of time, but we do very, very well and our algorithm is an order of magnitude faster than other things that you might do in terms of forward search.",
                    "label": 0
                },
                {
                    "sent": "Here this is sort of the best of the forward search algorithms that don't use macro actions, and we're in order of magnitude faster, and we're doing way better in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "I this in this case, this is a discrete problem and we are approximated with continuous values.",
                    "label": 0
                },
                {
                    "sent": "In order to be able to take advantage of our trick of computing the expected over this distribution of beliefs so we do lose a little bit compared to the best possible thing that you might do, but we.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or faster.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that we could scale to enormous problems because we have this continuous representation, because we're doing forward search, we can factor nicely, and so here we have a problem.",
                    "label": 0
                },
                {
                    "sent": "That's a couple of orders, a couple of orders of magnitude larger than the largest problem that could be solved by standard palm DP solvers.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's slow now because 100 seconds where one before.",
                    "label": 0
                },
                {
                    "sent": "But our fastest thing is only 60 times slower for much, much, much bigger problem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last thing I want to talk about is the target monitoring, where you actually now have multiple vehicles moving around an environment and you actually want to go figure.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where they are, we put this on just to close the loop on the helicopter.",
                    "label": 0
                },
                {
                    "sent": "This is now a real world problem that's very continuous, relatively high dynamics and multi agent.",
                    "label": 0
                },
                {
                    "sent": "It's factorizable which is nice because you have multiple vehicles you're going to start moving in a second and.",
                    "label": 0
                },
                {
                    "sent": "This video is playing surprisingly slowly.",
                    "label": 0
                },
                {
                    "sent": "'cause things do move faster than that.",
                    "label": 0
                },
                {
                    "sent": "Even the helicopter mission.",
                    "label": 0
                },
                {
                    "sent": "That which is weird.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let me skip ahead to the results.",
                    "label": 0
                },
                {
                    "sent": "Is that were, you know, we could not solve this problem.",
                    "label": 0
                },
                {
                    "sent": "We tried to use it, try to solve it using the best palm DP solvers.",
                    "label": 0
                },
                {
                    "sent": "We worked with David Sues Group in Singapore to modify his code.",
                    "label": 0
                },
                {
                    "sent": "They can't even load the transition matrices into memory, even though it's a factored representation.",
                    "label": 0
                },
                {
                    "sent": "And so they certainly couldn't solve it.",
                    "label": 0
                },
                {
                    "sent": "So we got a couple of heuristic algorithms from the literature, in particular, the MBR induce cut it out.",
                    "label": 0
                },
                {
                    "sent": "And we're doing much better in terms of finding number of targets and our search times are sort of the same order of magnitude and it's able to deal with sort of the high dimensional continuous representation of the envir.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a question about macro actions, but I'm not going to get into that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll just wrap up and say that there's been a lot of work in the Palm DP literature and also increasing the controller and trying to push these horizons out as far as possible.",
                    "label": 0
                },
                {
                    "sent": "Controllers down receding horizon control or coming up with good heuristic approximations.",
                    "label": 0
                },
                {
                    "sent": "Palm DP literature has spent a lot of time trying to come up with better sampling algorithms in order to limit the space of samples that you're searching over in order to get around what Joel and Jeff and Sebastian called the Curse of History.",
                    "label": 0
                },
                {
                    "sent": "My claim is that that's exactly the problem.",
                    "label": 0
                },
                {
                    "sent": "To be working on, but sort of the wrong solution technique.",
                    "label": 0
                },
                {
                    "sent": "What we really should be doing is searching as far into the future as we can.",
                    "label": 0
                },
                {
                    "sent": "Can simply accepting the fact that not every policy needs to be considered if we prune our policy space and actually consider these open loop close to open loop fixed length macro actions then we would actually becomes possible to search in these large high dimensional domains there is this tradeoff between the depth of search and the number of conditional outcomes examined?",
                    "label": 1
                },
                {
                    "sent": "I don't have anything to say about that, but I think this is the next obvious question to consider.",
                    "label": 0
                },
                {
                    "sent": "And so I will stop there.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Left eye for few questions.",
                    "label": 0
                },
                {
                    "sent": "Awesome.",
                    "label": 0
                },
                {
                    "sent": "My corrections with different lengths.",
                    "label": 0
                },
                {
                    "sent": "There's absolutely no reason why you couldn't, and the target tracking problem explicitly has macro actions of different links.",
                    "label": 0
                },
                {
                    "sent": "Are proofs.",
                    "label": 0
                },
                {
                    "sent": "Assume a fixed length of L in order to make the things workout nicely.",
                    "label": 0
                },
                {
                    "sent": "If you had macro likes of varying lengths, the proofs might vary a little bit in terms of the complexity, but the algorithm itself will accept macro likes Max video length.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the problem in the construction of the macro actions which you didn't know because the space of my fractions is exponential in the Lane.",
                    "label": 0
                },
                {
                    "sent": "More or less arbitrarily, you could forget about my crush and then track the macro action online.",
                    "label": 0
                },
                {
                    "sent": "Like for example, the device and terministic function of the latest observations, including extraction.",
                    "label": 0
                },
                {
                    "sent": "That way without.",
                    "label": 0
                },
                {
                    "sent": "I completely agreed, with the exception of.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't concern matters that particular way, but you're right, you could so.",
                    "label": 0
                },
                {
                    "sent": "But I think the most important thing you said is that you're right.",
                    "label": 0
                },
                {
                    "sent": "We are hiding the complexity.",
                    "label": 0
                },
                {
                    "sent": "That problem in the macro action that I talked about here and that if you really don't have a good set of macro actions, you're screwed.",
                    "label": 0
                },
                {
                    "sent": "And what you'd really like to do is do it online.",
                    "label": 0
                },
                {
                    "sent": "We do have a algorithm that actually could start them online.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about that is it gives you in anytime solution, so the limit of infinite in the limit of an infinite amount of computation.",
                    "label": 0
                },
                {
                    "sent": "What you'd really like to do is converge to the full.",
                    "label": 0
                },
                {
                    "sent": "The full search tree as if you done that from scratch.",
                    "label": 0
                },
                {
                    "sent": "So what we currently do is we have a number of heuristics, including just random generation of macro actions, and we have the second question there.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "How do you take a current set of macro actions and improve it by one?",
                    "label": 0
                },
                {
                    "sent": "And this is very related to the question of the question of addressing the verbal resolution letter.",
                    "label": 0
                },
                {
                    "sent": "How do you take a current policy and split?",
                    "label": 0
                },
                {
                    "sent": "The state space or extended the policy?",
                    "label": 0
                },
                {
                    "sent": "That's a hard question.",
                    "label": 0
                },
                {
                    "sent": "We don't have definitive answers on that, but you're absolutely, that is probably the right thing, is not stick with the current macro actions, but iteratively improvement given how much time you have left.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Is this sort of a certain approach?",
                    "label": 0
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "At least, it's certainly certainly certainty equivalent in the sense that we're assuming we know the model.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to assume that, it's just that we do know the models.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you don't know the model, it makes the belief propagation the expected value calculation harder.",
                    "label": 0
                },
                {
                    "sent": "But we are.",
                    "label": 0
                },
                {
                    "sent": "I guess you could view it as certainty equivalence along the macro actions is that you assume that no, that's not even true, because I'm computing the expected value along the macro action, so it's definitely not a certainty.",
                    "label": 0
                },
                {
                    "sent": "Send it.",
                    "label": 0
                },
                {
                    "sent": "Actually, somehow ignoring the noise and the optimal completion of the action.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not ignoring the noise because, again, I'm considering so certainly equivalent would assume that one sequence of things is going to happen, and I'm going to plan for that.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to take the reward of that, but it's not one sequence of thing I'm actually explicitly computing all possible observations along there, and I'm saying if I do this, what is the range of things that could happen, and what is expected value of that?",
                    "label": 0
                },
                {
                    "sent": "So that's where it's different it I could see how it feels like certain equivalent in them committing up front to a particular set of things, but the only thing I'm committing up front to is a chunk of macro actions as opposed to changing the granularity of my policy, not of.",
                    "label": 0
                },
                {
                    "sent": "How I I believe this article as much about evaluation is anything else.",
                    "label": 0
                },
                {
                    "sent": "I'm not changing the evaluation from a traditional forward search.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna get.",
                    "label": 0
                },
                {
                    "sent": "I'm not the type that you say.",
                    "label": 0
                },
                {
                    "sent": "Hey I too uncertain I need to know.",
                    "label": 0
                },
                {
                    "sent": "So this explicitly gives us the ability to do that.",
                    "label": 0
                },
                {
                    "sent": "I didn't show the experiments.",
                    "label": 0
                },
                {
                    "sent": "The results from those experiments.",
                    "label": 0
                },
                {
                    "sent": "But we have experiments show exactly that.",
                    "label": 0
                },
                {
                    "sent": "So that's a bank, thank you.",
                    "label": 0
                }
            ]
        }
    }
}