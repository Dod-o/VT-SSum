{
    "id": "3dn637x2oeu7gnxvbeyvemdyt6iksz3u",
    "title": "Provable Deterministic Leverage Score Sampling",
    "info": {
        "author": [
            "Christos Boutsidis, Yahoo! Research"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_boutsidis_score_sampling/",
    "segmentation": [
        [
            "Hello everybody, so I'm going to talk about provable deterministic leverage core sampling, which is essentially a method to solve a problem about matrix factorizations, which is called a column subset selection problem."
        ],
        [
            "Before defining the problem, let me introduce a singular value decomposition and this mostly serves a slide for notation.",
            "So M by N will be the matrix which we would like to decompose.",
            "K will be the target rank of our low rank factorization.",
            "And let's say we want to approximate the matrix.",
            "I consider the standard low rank approximation problem, which is we want to approximate the matrix A with a matrix X but has rank at most K and want to minimize the Frobenius norm of the difference a -- X. OK, the singular value, the composition, the composer matrix a into three matrices U Sigma An V transpose U&V are orthonormal matrices, Sigma is diagonal and so here are the matrices U sub K. And so you know Sigma, sub K and so on and so forth serve as the truncated factor.",
            "So for example you sub K is the is the first K columns of U, VK transpose are the first K rows of V transpose.",
            "So having said that, then the optimal solution to this low rank matrix approximation problem is the matrix AK, which is UK times Sigma K Times VK transpose."
        ],
        [
            "OK, So what is the problem with studying this talk?",
            "We would like to, given a matrix A and some sampling parameter C, We would like to choose see columns from our matrix such that projecting the matrix, say onto the span span by those columns which is CC pseudoinverse.",
            "We would like to minimize this, either the Frobenius or the spectral distance.",
            "So we call this.",
            "Column subset selection problem CSP, an abbreviation of it and you know if you think about it, this this is a solution to the CSP gives a low rank factorization to the matrix say as C * X where X is C. Pseudoinverse times a plus some error term and now the whole the whole thing here in this problem is to find those columns C such that to minimize the spectral or the Frobenius norm of this matrix.",
            "E."
        ],
        [
            "OK, so motivation.",
            "Why is that?",
            "You know why we want to solve this problem?",
            "In general, we want to design interpretable matrix factorizations and so one example is consider you have stock by date matrices.",
            "So for example, you know the columns of this matrix correspond to stocks.",
            "There also corresponds to different dates over a year.",
            "By applying algorithms for this problem into this matrix, essentially find the most important stocks in your portal.",
            "Follow, but this is only a small example by you know by by many more examples where you essentially use those algorithms to do feature selection in your data."
        ],
        [
            "OK, so this is a problem having studied over over many years actually within the numerical linear algebra community have been studying over the last 40 years, became popular within the theoretical computer science community, maybe 1015 years ago, and here are some representative example of for the Frobenius norm version of this problem.",
            "So the first the second column shows the number of columns that you need to select in this matrix C. In order to get the approximation which is shown in the third column, you know the first row corresponds to an algorithm which week bound.",
            "So this epsilon parameter is something small between zero and one.",
            "So here the approximation error depends on the Frobenius normal way, which can be very large.",
            "But then the other four rows correspond to algorithms where we have the so called relative error approximation.",
            "So here are the most important.",
            "Those are some representative algorithms giving sort of the state of the art bounds, especially the last two rows."
        ],
        [
            "So out of those algorithms, I would like to focus on one which was proposed by Ringness, Mahoning Motor Christian and is based on the so called lever at scores of the matrix.",
            "So the level scores will be something important for the rest of the talk.",
            "So let me define them carefully.",
            "Given the matrix A&M by N and some rank parameter Ki will be final ever at score for each column of a.",
            "And how I'm going to define it.",
            "I will take this matrix VK, which has N rows and K columns.",
            "So VK has the US columns.",
            "The top K right singular vectors of the matrix.",
            "And I'm going to define the level at score as the Euclidean norm of the Euclidean Norm Square of the eye roll of the Matrix VK.",
            "So every row of BK corresponds to one column of a.",
            "So that way you define.",
            "Level score for every column buffet.",
            "So given those leverage scores, here is a very simple algorithm to solve the column subset selection problem.",
            "Given the level at scores, you define the probability distribution and those probabilities are simply the level scores over K. Now you want to select for a given.",
            "See the sampling parameter you select randomly with probabilities that you just define.",
            "See columns from the Matrix, say.",
            "And it has been shown that if you select Cal OK over epsilon squared for some small epsilon columns, then you have a relative error approximation and recall that this a sub K matrix is the best rank K approximation, so CC should never say is a rank C matrix that has error at most one plus epsilon from the relative error approximation.",
            "OK, so this is a very well known randomized algorithm."
        ],
        [
            "Here is a deterministic algorithm which is sort of the analog of the randomized algorithm, and this algorithm was actually very old, proposed more than 40 years ago in I found a reference in some statistics Journal, and the idea is you have the matrix.",
            "Say you compute the level scores and now you just keep the see columns with the largest level at scores.",
            "You don't do a randomized sampling, you just.",
            "Rely on the columns with the largest level scores, so this algorithm works very well empirically, but there is no theoretical explanation about it, and this talk is about the contributions of our paper.",
            "The contribution is basically giving a theoretical explanation why this algorithm works."
        ],
        [
            "So you know this is the basic.",
            "The basic idea.",
            "You said you keep the final level scores and you keep the columns with the largest level at scores.",
            "And here is a slight modification of it essentially is presenting the same algorithm by introducing a new parameter which help us analyze the algorithm in a more meaningful way.",
            "And this parameter is this parameter Theta, and so again the algorithm is you compute this matrix BK, you compute the level scores.",
            "As I described before now, without loss of generality, let us assume that the columns are from one to NR order such that the leverage scores are in some nonincreasing order.",
            "So the first column corresponds to the largest level scores, and so on.",
            "And now we want to keep that see columns such that their sum, the sum of the level outscores exceeds this parameter Theta, and why we do this?",
            "I mean, this is the same as you just keep the columns with the largest level.",
            "Course, but I just want to introduce this parameter Theta in order to be able to quantify the error and the number of columns that you are, you should select in order to get a certain error.",
            "So this this data is essentially the mass.",
            "How much, how much energy of the level at scores you need to select OK, and we assume that this is given as input to the algorithm."
        ],
        [
            "Here is a basic the basic theorem, the basic result.",
            "If this Theta the parameter Theta is K minus epsilon, where epsilon is something small between zero and one K can be anything like 1000 whatever.",
            "Then if you select leverage scores whose sum is greater than Theta, then you get a relative error and you select the corresponding columns.",
            "You could get a relative error approximation to the best rank K approximation.",
            "So now, although this result is interesting, notice that.",
            "K is very large, epsilon can be very small so Theta would might be a sensory K. So if the level of scores are almost uniform then yes you will get this great one plus epsilon approximation.",
            "But in order to get this approximation things that intuitively the number of columns should essentially be closed to end in order to be able to guarantee that you're the selected level scores, have some who's who is greater than.",
            "Which is greater than 50, so this is interesting but not very useful.",
            "So then."
        ],
        [
            "The next question we asked is in under which cases this result gives something useful and the obvious thing to consider is when the leverage scores follow some sort of decay.",
            "In the paper, we studied one sort of decay, which is a power law, but I think that other other case can be also analyzed exponential decay.",
            "We haven't done in the paper though, so here is the result under this assumption.",
            "We assume that the level of scores follow a power law decay with exponent A sub K for technical reasons.",
            "We assume that this AK is greater than one, so we parameterize it as one plus ITA.",
            "That there is no.",
            "There is no particular reason that this a cake and be less than one.",
            "We just couldn't analyze this case, so perhaps it's doable, but we couldn't do it.",
            "So we assume that the level scores follow this power law decay with some exponent, one plus ETA, and then again the same algorithm under this assumption, can carabbia relative error approximation and can guarantee that the number of selected columns is.",
            "2K over epsilon two 1 / 1 / A sub K. So think about you know some some easy way to interpret this result is that think about this Atom goes close to one.",
            "So the exponent is essentially one.",
            "So then the number of columns is 2K over epsilon which is more or less.",
            "You cannot do better than that.",
            "There are some certain lower bounds which say that you cannot do better than K over epsilon.",
            "In order to get a relative error approximation."
        ],
        [
            "OK. OK, so having having introduced the.",
            "That power loss assumption.",
            "OK, this is having the power long assumption is great, gives some great relative error approximations, but the question is, is a power loss assumption realistic?",
            "In theory, yes.",
            "If the level scores follow power law, then yes, we have a great approximation.",
            "But what happens in real data?",
            "So the next thing is to experiment with a few real data.",
            "We've got a few large graphs and created adventurous matrices for those graphs.",
            "And some other data like future by.",
            "By object data and wanted to run this algorithm on those matrices.",
            "But and we tested the lever at scores of this made."
        ],
        [
            "This is we plotted the level scores and we wanted to see if they really follow a power law assumption.",
            "If they agree with the power low power, low assumption and the question is yes and most of the time they the exponent is also greater than one.",
            "Sometimes the exponent is less than one, but from the experiments which I'm not going to present here from the experiments also that this case where the exponent is less than one.",
            "This also gives some good results with this deterministic algorithm."
        ],
        [
            "OK, so then I want to present a couple of more experiments.",
            "So here we consider matrices M by N, where short and fat matrices.",
            "We experiment with different different numbers for the parameter K from K = 5, ten, 50 and 100 and we sort of created matrices for those dimensions and parameter K. We created matrices with.",
            "Different kind of leverage scores.",
            "So in the first row who created leverage scores that follow a power law with exponent half and in the 2nd row level at scores that follow a power law with Exponent 1.5.",
            "So in both cases and here are the approach we plot the approximation error, the spectral norm approximation error, the relative approximation error A minus pseudoinverse A over the optimal approximation error, so we expect this to be greater than one.",
            "Of course if K is.",
            "Less than C, if C is greater than K, the number of selected columns is greater than K. This can go great lower than one, and we sort of want to understand what is the difference between the two cases.",
            "So this magenta vertical line corresponds to the case where this relative error approximation equals one.",
            "Tells us how many columns we need to select in order to get a relative error approximation one, and by comparing the two cases, the corresponding upper and lower part for a specific K, we see that the 2nd and the 2nd row gives more accurate results in the sense that the number of selected columns need to be less than the number of selected columns.",
            "In the first case, and this indicates that.",
            "Sharper Ducation level scores give more accurate algorithms, which is basically predicted by our theory."
        ],
        [
            "So I have one more experiment which I would like to skip and."
        ],
        [
            "I would go to the conclusions.",
            "So basically what I described here.",
            "I described the column subset selection problem.",
            "There are a few methods for this problem.",
            "We focused on one approach which is sampling with respect to the level that scores.",
            "There is a nice randomized algorithm and nice theory about it which also gives.",
            "This algorithm is very accurate in practice.",
            "There is also a very nice deterministic algorithm.",
            "Just select the.",
            "Columns without largest corresponding level scores.",
            "There was no theorie about this algorithm and our contributions is giving a result which says that if the leverage scores follow power law decay, then this algorithm is essentially as accurate as the randomized algorithm."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody, so I'm going to talk about provable deterministic leverage core sampling, which is essentially a method to solve a problem about matrix factorizations, which is called a column subset selection problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before defining the problem, let me introduce a singular value decomposition and this mostly serves a slide for notation.",
                    "label": 1
                },
                {
                    "sent": "So M by N will be the matrix which we would like to decompose.",
                    "label": 0
                },
                {
                    "sent": "K will be the target rank of our low rank factorization.",
                    "label": 0
                },
                {
                    "sent": "And let's say we want to approximate the matrix.",
                    "label": 0
                },
                {
                    "sent": "I consider the standard low rank approximation problem, which is we want to approximate the matrix A with a matrix X but has rank at most K and want to minimize the Frobenius norm of the difference a -- X. OK, the singular value, the composition, the composer matrix a into three matrices U Sigma An V transpose U&V are orthonormal matrices, Sigma is diagonal and so here are the matrices U sub K. And so you know Sigma, sub K and so on and so forth serve as the truncated factor.",
                    "label": 0
                },
                {
                    "sent": "So for example you sub K is the is the first K columns of U, VK transpose are the first K rows of V transpose.",
                    "label": 1
                },
                {
                    "sent": "So having said that, then the optimal solution to this low rank matrix approximation problem is the matrix AK, which is UK times Sigma K Times VK transpose.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the problem with studying this talk?",
                    "label": 0
                },
                {
                    "sent": "We would like to, given a matrix A and some sampling parameter C, We would like to choose see columns from our matrix such that projecting the matrix, say onto the span span by those columns which is CC pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "We would like to minimize this, either the Frobenius or the spectral distance.",
                    "label": 0
                },
                {
                    "sent": "So we call this.",
                    "label": 0
                },
                {
                    "sent": "Column subset selection problem CSP, an abbreviation of it and you know if you think about it, this this is a solution to the CSP gives a low rank factorization to the matrix say as C * X where X is C. Pseudoinverse times a plus some error term and now the whole the whole thing here in this problem is to find those columns C such that to minimize the spectral or the Frobenius norm of this matrix.",
                    "label": 1
                },
                {
                    "sent": "E.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so motivation.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "You know why we want to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "In general, we want to design interpretable matrix factorizations and so one example is consider you have stock by date matrices.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know the columns of this matrix correspond to stocks.",
                    "label": 0
                },
                {
                    "sent": "There also corresponds to different dates over a year.",
                    "label": 0
                },
                {
                    "sent": "By applying algorithms for this problem into this matrix, essentially find the most important stocks in your portal.",
                    "label": 1
                },
                {
                    "sent": "Follow, but this is only a small example by you know by by many more examples where you essentially use those algorithms to do feature selection in your data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a problem having studied over over many years actually within the numerical linear algebra community have been studying over the last 40 years, became popular within the theoretical computer science community, maybe 1015 years ago, and here are some representative example of for the Frobenius norm version of this problem.",
                    "label": 1
                },
                {
                    "sent": "So the first the second column shows the number of columns that you need to select in this matrix C. In order to get the approximation which is shown in the third column, you know the first row corresponds to an algorithm which week bound.",
                    "label": 0
                },
                {
                    "sent": "So this epsilon parameter is something small between zero and one.",
                    "label": 1
                },
                {
                    "sent": "So here the approximation error depends on the Frobenius normal way, which can be very large.",
                    "label": 0
                },
                {
                    "sent": "But then the other four rows correspond to algorithms where we have the so called relative error approximation.",
                    "label": 1
                },
                {
                    "sent": "So here are the most important.",
                    "label": 0
                },
                {
                    "sent": "Those are some representative algorithms giving sort of the state of the art bounds, especially the last two rows.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So out of those algorithms, I would like to focus on one which was proposed by Ringness, Mahoning Motor Christian and is based on the so called lever at scores of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So the level scores will be something important for the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "So let me define them carefully.",
                    "label": 0
                },
                {
                    "sent": "Given the matrix A&M by N and some rank parameter Ki will be final ever at score for each column of a.",
                    "label": 0
                },
                {
                    "sent": "And how I'm going to define it.",
                    "label": 0
                },
                {
                    "sent": "I will take this matrix VK, which has N rows and K columns.",
                    "label": 0
                },
                {
                    "sent": "So VK has the US columns.",
                    "label": 0
                },
                {
                    "sent": "The top K right singular vectors of the matrix.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to define the level at score as the Euclidean norm of the Euclidean Norm Square of the eye roll of the Matrix VK.",
                    "label": 0
                },
                {
                    "sent": "So every row of BK corresponds to one column of a.",
                    "label": 0
                },
                {
                    "sent": "So that way you define.",
                    "label": 0
                },
                {
                    "sent": "Level score for every column buffet.",
                    "label": 0
                },
                {
                    "sent": "So given those leverage scores, here is a very simple algorithm to solve the column subset selection problem.",
                    "label": 0
                },
                {
                    "sent": "Given the level at scores, you define the probability distribution and those probabilities are simply the level scores over K. Now you want to select for a given.",
                    "label": 0
                },
                {
                    "sent": "See the sampling parameter you select randomly with probabilities that you just define.",
                    "label": 0
                },
                {
                    "sent": "See columns from the Matrix, say.",
                    "label": 0
                },
                {
                    "sent": "And it has been shown that if you select Cal OK over epsilon squared for some small epsilon columns, then you have a relative error approximation and recall that this a sub K matrix is the best rank K approximation, so CC should never say is a rank C matrix that has error at most one plus epsilon from the relative error approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very well known randomized algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a deterministic algorithm which is sort of the analog of the randomized algorithm, and this algorithm was actually very old, proposed more than 40 years ago in I found a reference in some statistics Journal, and the idea is you have the matrix.",
                    "label": 0
                },
                {
                    "sent": "Say you compute the level scores and now you just keep the see columns with the largest level at scores.",
                    "label": 0
                },
                {
                    "sent": "You don't do a randomized sampling, you just.",
                    "label": 0
                },
                {
                    "sent": "Rely on the columns with the largest level scores, so this algorithm works very well empirically, but there is no theoretical explanation about it, and this talk is about the contributions of our paper.",
                    "label": 1
                },
                {
                    "sent": "The contribution is basically giving a theoretical explanation why this algorithm works.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know this is the basic.",
                    "label": 0
                },
                {
                    "sent": "The basic idea.",
                    "label": 0
                },
                {
                    "sent": "You said you keep the final level scores and you keep the columns with the largest level at scores.",
                    "label": 0
                },
                {
                    "sent": "And here is a slight modification of it essentially is presenting the same algorithm by introducing a new parameter which help us analyze the algorithm in a more meaningful way.",
                    "label": 0
                },
                {
                    "sent": "And this parameter is this parameter Theta, and so again the algorithm is you compute this matrix BK, you compute the level scores.",
                    "label": 0
                },
                {
                    "sent": "As I described before now, without loss of generality, let us assume that the columns are from one to NR order such that the leverage scores are in some nonincreasing order.",
                    "label": 1
                },
                {
                    "sent": "So the first column corresponds to the largest level scores, and so on.",
                    "label": 0
                },
                {
                    "sent": "And now we want to keep that see columns such that their sum, the sum of the level outscores exceeds this parameter Theta, and why we do this?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is the same as you just keep the columns with the largest level.",
                    "label": 0
                },
                {
                    "sent": "Course, but I just want to introduce this parameter Theta in order to be able to quantify the error and the number of columns that you are, you should select in order to get a certain error.",
                    "label": 0
                },
                {
                    "sent": "So this this data is essentially the mass.",
                    "label": 0
                },
                {
                    "sent": "How much, how much energy of the level at scores you need to select OK, and we assume that this is given as input to the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a basic the basic theorem, the basic result.",
                    "label": 0
                },
                {
                    "sent": "If this Theta the parameter Theta is K minus epsilon, where epsilon is something small between zero and one K can be anything like 1000 whatever.",
                    "label": 0
                },
                {
                    "sent": "Then if you select leverage scores whose sum is greater than Theta, then you get a relative error and you select the corresponding columns.",
                    "label": 0
                },
                {
                    "sent": "You could get a relative error approximation to the best rank K approximation.",
                    "label": 0
                },
                {
                    "sent": "So now, although this result is interesting, notice that.",
                    "label": 0
                },
                {
                    "sent": "K is very large, epsilon can be very small so Theta would might be a sensory K. So if the level of scores are almost uniform then yes you will get this great one plus epsilon approximation.",
                    "label": 1
                },
                {
                    "sent": "But in order to get this approximation things that intuitively the number of columns should essentially be closed to end in order to be able to guarantee that you're the selected level scores, have some who's who is greater than.",
                    "label": 0
                },
                {
                    "sent": "Which is greater than 50, so this is interesting but not very useful.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next question we asked is in under which cases this result gives something useful and the obvious thing to consider is when the leverage scores follow some sort of decay.",
                    "label": 0
                },
                {
                    "sent": "In the paper, we studied one sort of decay, which is a power law, but I think that other other case can be also analyzed exponential decay.",
                    "label": 0
                },
                {
                    "sent": "We haven't done in the paper though, so here is the result under this assumption.",
                    "label": 0
                },
                {
                    "sent": "We assume that the level of scores follow a power law decay with exponent A sub K for technical reasons.",
                    "label": 1
                },
                {
                    "sent": "We assume that this AK is greater than one, so we parameterize it as one plus ITA.",
                    "label": 0
                },
                {
                    "sent": "That there is no.",
                    "label": 0
                },
                {
                    "sent": "There is no particular reason that this a cake and be less than one.",
                    "label": 0
                },
                {
                    "sent": "We just couldn't analyze this case, so perhaps it's doable, but we couldn't do it.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the level scores follow this power law decay with some exponent, one plus ETA, and then again the same algorithm under this assumption, can carabbia relative error approximation and can guarantee that the number of selected columns is.",
                    "label": 0
                },
                {
                    "sent": "2K over epsilon two 1 / 1 / A sub K. So think about you know some some easy way to interpret this result is that think about this Atom goes close to one.",
                    "label": 0
                },
                {
                    "sent": "So the exponent is essentially one.",
                    "label": 0
                },
                {
                    "sent": "So then the number of columns is 2K over epsilon which is more or less.",
                    "label": 0
                },
                {
                    "sent": "You cannot do better than that.",
                    "label": 0
                },
                {
                    "sent": "There are some certain lower bounds which say that you cannot do better than K over epsilon.",
                    "label": 0
                },
                {
                    "sent": "In order to get a relative error approximation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, so having having introduced the.",
                    "label": 0
                },
                {
                    "sent": "That power loss assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, this is having the power long assumption is great, gives some great relative error approximations, but the question is, is a power loss assumption realistic?",
                    "label": 0
                },
                {
                    "sent": "In theory, yes.",
                    "label": 0
                },
                {
                    "sent": "If the level scores follow power law, then yes, we have a great approximation.",
                    "label": 1
                },
                {
                    "sent": "But what happens in real data?",
                    "label": 0
                },
                {
                    "sent": "So the next thing is to experiment with a few real data.",
                    "label": 0
                },
                {
                    "sent": "We've got a few large graphs and created adventurous matrices for those graphs.",
                    "label": 0
                },
                {
                    "sent": "And some other data like future by.",
                    "label": 0
                },
                {
                    "sent": "By object data and wanted to run this algorithm on those matrices.",
                    "label": 0
                },
                {
                    "sent": "But and we tested the lever at scores of this made.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is we plotted the level scores and we wanted to see if they really follow a power law assumption.",
                    "label": 0
                },
                {
                    "sent": "If they agree with the power low power, low assumption and the question is yes and most of the time they the exponent is also greater than one.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the exponent is less than one, but from the experiments which I'm not going to present here from the experiments also that this case where the exponent is less than one.",
                    "label": 0
                },
                {
                    "sent": "This also gives some good results with this deterministic algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then I want to present a couple of more experiments.",
                    "label": 0
                },
                {
                    "sent": "So here we consider matrices M by N, where short and fat matrices.",
                    "label": 0
                },
                {
                    "sent": "We experiment with different different numbers for the parameter K from K = 5, ten, 50 and 100 and we sort of created matrices for those dimensions and parameter K. We created matrices with.",
                    "label": 1
                },
                {
                    "sent": "Different kind of leverage scores.",
                    "label": 1
                },
                {
                    "sent": "So in the first row who created leverage scores that follow a power law with exponent half and in the 2nd row level at scores that follow a power law with Exponent 1.5.",
                    "label": 0
                },
                {
                    "sent": "So in both cases and here are the approach we plot the approximation error, the spectral norm approximation error, the relative approximation error A minus pseudoinverse A over the optimal approximation error, so we expect this to be greater than one.",
                    "label": 0
                },
                {
                    "sent": "Of course if K is.",
                    "label": 0
                },
                {
                    "sent": "Less than C, if C is greater than K, the number of selected columns is greater than K. This can go great lower than one, and we sort of want to understand what is the difference between the two cases.",
                    "label": 0
                },
                {
                    "sent": "So this magenta vertical line corresponds to the case where this relative error approximation equals one.",
                    "label": 1
                },
                {
                    "sent": "Tells us how many columns we need to select in order to get a relative error approximation one, and by comparing the two cases, the corresponding upper and lower part for a specific K, we see that the 2nd and the 2nd row gives more accurate results in the sense that the number of selected columns need to be less than the number of selected columns.",
                    "label": 0
                },
                {
                    "sent": "In the first case, and this indicates that.",
                    "label": 0
                },
                {
                    "sent": "Sharper Ducation level scores give more accurate algorithms, which is basically predicted by our theory.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have one more experiment which I would like to skip and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would go to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "So basically what I described here.",
                    "label": 0
                },
                {
                    "sent": "I described the column subset selection problem.",
                    "label": 1
                },
                {
                    "sent": "There are a few methods for this problem.",
                    "label": 0
                },
                {
                    "sent": "We focused on one approach which is sampling with respect to the level that scores.",
                    "label": 0
                },
                {
                    "sent": "There is a nice randomized algorithm and nice theory about it which also gives.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is very accurate in practice.",
                    "label": 0
                },
                {
                    "sent": "There is also a very nice deterministic algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just select the.",
                    "label": 0
                },
                {
                    "sent": "Columns without largest corresponding level scores.",
                    "label": 1
                },
                {
                    "sent": "There was no theorie about this algorithm and our contributions is giving a result which says that if the leverage scores follow power law decay, then this algorithm is essentially as accurate as the randomized algorithm.",
                    "label": 0
                }
            ]
        }
    }
}