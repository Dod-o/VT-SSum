{
    "id": "to2u2ng4obp23jfpcisld32tz4fl4pv3",
    "title": "Deep Learning in Natural Language Processing",
    "info": {
        "author": [
            "Ronan Collobert, NEC Laboratories America, Inc.",
            "Jason Weston, Facebook"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/nips09_collobert_weston_dlnl/",
    "segmentation": [
        [
            "So.",
            "I just telling you a little story."
        ],
        [
            "Disclaimer this slide.",
            "Hello, I'm a support vector machine.",
            "And I think I know quite a bit about natural language processing.",
            "Hi, I'm in your network and I know a little bit about deep learning.",
            "You said Norway."
        ],
        [
            "A neural network, obviously.",
            "I never heard of it.",
            "I mean, what did well?",
            "Obviously you're too young.",
            "Network.",
            "So."
        ],
        [
            "In the beginning there was the discovery of the perceptual.",
            "The steps on I don't know.",
            "I don't know what it is.",
            "I mean is it to kind of mill or?",
            "No, if you take like a brain and you take a magnifying glass and look at the smallest thing you've got there.",
            "And stupidest thing you've got a single Europe oh you mean like this like kind of stupid linear model.",
            "Well, yeah, exactly but.",
            "Really, I don't trust it.",
            "I mean, I've been told it doesn't work well.",
            "Kind of sucks.",
            "That's what Minsky said."
        ],
        [
            "It made a bunch of people side, so they went on a quest to try and model nonlinearities.",
            "They tried all kinds of stuff like random projections to induce nonlinearity is adding nonlinear features to the inputs like products and features, and they even thought of kernels in 1964.",
            "I mean, I really don't know why it says they made this our life so complicated.",
            "You just take good features on good linear model and that's it.",
            "You know super vector machine.",
            "Well, people were still depressed."
        ],
        [
            "Until they discovered me, most lab sector.",
            "Yeah, I bet a pile of perciform.",
            "Well, they got excited."
        ],
        [
            "They were so excited they kept trying more and more things.",
            "Yeah, adding layers I bet."
        ],
        [
            "Well yeah, they try it.",
            "Time dilay your networks.",
            "What competitions as well yeah Oh yeah they tried recurrent urinary yeah but the legs.",
            "Yeah, even how do I also?",
            "Still people got scared."
        ],
        [
            "Yeah, not surprised about that.",
            "Reach the complexity of the open, intelligent, even universe brain.",
            "I wonder on the brain of who?"
        ],
        [
            "Well, it's not you 0.",
            "Just accept from universe there trying to roller itself, so just complex they decide what they do is too complex."
        ],
        [
            "Sir, that's something that's complex.",
            "In fact, something convex someone came up with a new perceptual network, but it's it's like this product formation.",
            "That's right network.",
            "I'm very proud of my father.",
            "Actually, he always say I'm cool on sexy.",
            "We liked your dog.",
            "You know, some people wondered if you weren't just a linear model.",
            "You know we scale notes.",
            "I can do everything."
        ],
        [
            "Well, life is convex and life is good.",
            "Yeah she lot of papers you know you can talk.",
            "But it didn't do everything they wanted."
        ],
        [
            "For example, some people they wanted to try and learn some good features and then.",
            "What for just the good features fitted to a linear model is fine.",
            "Yeah, could be fine if you knew what the good features were, but otherwise you might want to learn them.",
            "It's it's done.",
            "Coins come on.",
            "I mean it's not going to work well.",
            "People tried it anyway."
        ],
        [
            "The network like this actually and then you replace the hidden layer with eternal life and they let him cuddle and they put you at the end of it nonsense, it's nonconvex.",
            "Yeah, it looks a lot like me.",
            "But you know, it might be nonconvex, but still might be useful.",
            "Family so hold on a minute.",
            "I've got a lot of data.",
            "There are different tasks, maybe I can multitask these days here and share the features and none would be interesting.",
            "I think they did a lot of that with myself.",
            "You know, like I mean you cannot talk about that.",
            "I mean they did it a lot.",
            "It works very well.",
            "So even like in natural language processing.",
            "Come on.",
            "Network.",
            "Review again.",
            "And later we can just share, share some representation where we learn features, yeah, but it's nonconvex.",
            "Doesn't mean it isn't interesting."
        ],
        [
            "Then people also thought holding it.",
            "I got this unlabeled data.",
            "What no?",
            "I mean like it's positive support vector machine.",
            "It works so well you take on labels, you fit it.",
            "It's like a excellent.",
            "Yeah, your father was very proud of that.",
            "Yeah, I know and.",
            "Non convex loss on your head.",
            "Yeah come on.",
            "I mean many people tried to convexified that and it it works very fine so.",
            "Well, it might be quite late convexified.",
            "Yeah, just take a small data set, it works fine.",
            "But yeah, anyway if something is non convex it doesn't mean."
        ],
        [
            "Interesting.",
            "And then people thought I really want to get good results.",
            "What I should do?",
            "Yeah, as I always say they good features fitted too early, no model like me.",
            "Really good bread and you doing nonlinear transformation is the original features to some new features that we feed to year.",
            "Yeah, this looks like a two layer network where the first layer is this being human brain.",
            "Well, I'm multilayer network where the first layer is machine learning, not human.",
            "Ladies Watch were actually supposed to do.",
            "It's I think it's it's.",
            "It's again, nonconvex."
        ],
        [
            "And then scalability.",
            "Let's yeah, at least they made all book about me about that.",
            "I mean, it works so well.",
            "Well, they might have had to make a book about you because you're actually kind of slow, so they had to work very hard on that, yeah?",
            "The problem with this team.",
            "Support vectors in machine we tend to get fat.",
            "Try to fix that different ways.",
            "His few of them you could try and use plastic gradient descent works very well on call exponents, training, training me and your network.",
            "Learning which support vectors to include two removal is fat support vectors and that might turn into nonconvex optimization like to learn at work.",
            "That's done comics.",
            "I don't trust it.",
            "Or if you don't trust that, yeah, they're good features as I always say again and feed it to like a very nice support vector machine.",
            "Exactly, you could just go back to something like SVM, Lin, and training your linear CNN linear time and.",
            "You're good.",
            "Are you kind of just back to central?"
        ],
        [
            "I got an idea.",
            "Why don't I mean no one likes me?",
            "I'm in your network, why don't we just rename it deep network?",
            "And Justin thinks I'm pretty cool.",
            "I think I will always call you another network anyways.",
            "Well, Daniel.",
            "Others seem to be excited again because they're here in this room.",
            "Yeah, it's nonconvex."
        ],
        [
            "But slightly more seriously, putting all this together.",
            "We think of this.",
            "I think the network rethink your networks.",
            "Inflexible is different.",
            "We can put different modules, layers, losses and regularizers.",
            "We can do multitasking.",
            "Same supervised learning and learning hidden representations, all very elegantly within this framework and scalable too.",
            "So we're hoping that this might be a great tool for natural language processing."
        ],
        [
            "Yeah.",
            "I think we we had enough bad jokes.",
            "Wait for today.",
            "So.",
            "So before before we continue any further, I mean we would like to tell you what we want to do with NLP.",
            "You know we have all our secret goal.",
            "So in fact we want to have a conversation with like your computer will be alone and we.",
            "So of course we know it's not going to be easy, but at least we would be.",
            "We would like to be able to convert like a piece of English into like some computer or family structure that we will call the hidden representation, which we would like this hidden representation to somehow encode the syntax semantic of the piece of English we consider.",
            "So we are going to use like this kind of global hidden representation to basically train an natural language processing task and we are going to use like classical natural language processing task benchmark to see if, like you know this don't presentation are any good.",
            "That is if like maybe to check if the computer understands or not.",
            "So here we are really like starting natural language processing from scratch for getting like feature engineering.",
            "At least we try to minimize it.",
            "Add a using this hidden representation, which are quite global Indian.",
            "So we divide it like our talking four parts.",
            "The first one I think you had enough.",
            "It was about neural networks.",
            "Other Jason would say you have been brainwashed.",
            "The the second part is going to be about labeling the third part about retrieval and the last part about situated learning.",
            "So.",
            "The natural.",
            "Yeah, sorry.",
            "So the battle, the natural language processing fee."
        ],
        [
            "You know it has been, you know, existing since quite awhile and actually many tasks.",
            "I like task which involves tagging.",
            "So in the first part we are interesting in this task and the task."
        ],
        [
            "I will consider today I actually like.",
            "Those those four task, the first one is very simple.",
            "It's called like part of speech tagging is it's a syntactic task.",
            "You want to know if each word in the sentence is like a noun and adverb legit.",
            "If and so on, then you have something a bit more complicated with which is called the chunking.",
            "This time you want to get like syntactic constituent that is like chunk of ones and find out you know noun, phrase, verb phrase and so on and you have something a bit more semantic which is called named entity recognition.",
            "You want to know if like a award or like tree or chunk of world is like a person name, company name, location, name, external.",
            "And then there is like no most complicated task.",
            "I'm going to consider today, which is like semantic or labeling.",
            "So for this one you consider a verb before hand in the sentence.",
            "So for example, in this sentence John A deer born in the garden, there is only one version.",
            "So you consider 8 and you want to know what is acting on eight when it's on what it's exciting to hear.",
            "I mean, obviously John is acting on eight.",
            "It's acting on the upper.",
            "Where it's acting when it one is it's acting, I mean and so on.",
            "So you have many tags I got."
        ],
        [
            "And.",
            "As an LP's been existing since quite a while there is, it exists actually a lot of benchmarks we consider here benchmarks which are based on Wall Street Dollar and Reuters data set.",
            "Wall Street Journal is about like 1 million label off once for the taskbar.",
            "Interested in and for writers, it's about 200 K wants.",
            "And we followed exactly the setup proposed by the Co NLL.",
            "Competition challenges for chunking.",
            "It's in 2004.",
            "Any rights in 2003 and semantic horribly?",
            "And so it's a one in 2005.",
            "In for part of speech, we followed the exactly the setup proposed by total over in 2003.",
            "So many people like tried many systems on these benchmarks.",
            "On here I reported the main.",
            "I mean the top system I'm aware of.",
            "So when it's when it's about like winning a challenge, it's not surprising to find some people you know trying to win at all means, so they would like, for example, take extra data, you know, like an external labeled data set.",
            "Even like a I mean overtune like crazy.",
            "Each of their model I mean.",
            "When it's about comparison, we don't really care about that.",
            "We it doesn't bring any singular.",
            "If somebody like trains on something you know extra or so we consider as benchmark systems in this, the rest of the part of this talk.",
            "System which are really well established and system which like avoid using external labeled data.",
            "So I like put in bold all the system we're going to consider for each task as benchmark system in the rest of the talk.",
            "As a side note, and Owens and in 2005 used for any or some external data.",
            "But it was unlabeled data.",
            "And Koeman Punja Kanaka tour in 2005 used.",
            "Actually, extra bass trees, which were not provided by the challenge, so you have to keep that in mind."
        ],
        [
            "So natural language processing task quite complex task an if you want to solve a complex task, you have better through the complex system.",
            "So for me the really like 2 extreme choice to get the complex system.",
            "The first one is what I called large scale engineering, so I think we brainwashed you are better at the beginning about that.",
            "You basically like design, very good encrusted features that you know are good for your, for your task, and you feel that to a very classical, you know linear or simple learning algorithm so that actually works very well.",
            "But we are just not interested in that.",
            "We prefer, you know."
        ],
        [
            "The learning way.",
            "Instead we use like simple features that is like features which are related gently to your task, like in our case, it would be a text Jackie straight text and you try to design this time a quite complex system over this text features.",
            "I mean these text walls and.",
            "You hope that your system is going to implicitly learn the right features."
        ],
        [
            "So existing natural language processing system really tend to use, like you know, the 1st the 1st way.",
            "So they usually design encrusted features and feed it a feed them to like shallow classifier like a spotlight on machine.",
            "Sometimes there is a decoder on top or something, but it's often like that.",
            "So for tasks like button speech, ranking features are actually is really quite simple things like combination of words and labels.",
            "I mean it's OK, but for more complicated task it tends to be a bit more wild, in particular for semantical labeling.",
            "And actually it's sometimes even more complicated than that.",
            "People tend to like cascade features."
        ],
        [
            "So for semantic role labeling they would like first hand crafted features get from this uncrested features the part of speech tags from this part of speech tags they would like build up our street and then from this past three they would again handcraft which features and feed again these features to a new classifier and they would obtain.",
            "The tags are interested in.",
            "So.",
            "We think that."
        ],
        [
            "You know?",
            "Trying to do like some task specific engineering really like limits.",
            "The natural language processing scope because each time you have like a new task you need again to find new features and as the task tend to be more more complicated over the years you need to get you know more and more complicated features.",
            "So instead we would like to find some kind of unified hidden representation for text.",
            "Which would you know represent very well text and would would work for any kind of.",
            "Natural Language Processing task, which involves tagging an over these hidden or presentation.",
            "We would be able then to build some kind of unified natural language processing architecture.",
            "Without any external engineering.",
            "So we want to go towards this direction and to achieve this goal we are like going to start from scratch for get most of natural language processing knowledge.",
            "We are going to keep actually as the natural language processing benchmarks to see if we do, you know any good.",
            "But really, in a in this talk you have to remember or dogma which is avoid this specific engineering."
        ],
        [
            "So we are doing natural language processing.",
            "We need some kind of complicated system, something which would scale because we have a lot of data and something which would like fine somehow hidden or presentation.",
            "So I don't know if you know it rings a Bell in your head, but for me the obvious choice is like a normal network.",
            "So maybe you forgot about knowledge."
        ],
        [
            "Because it's quite old.",
            "But for me, and although it works just like a series of metrics, vector multiplications followed by some nonlinear ID.",
            "So in our case we use like a simple.",
            "I mean, it's like the hard version of the hyperbolic tangent, but you can use whatever you want.",
            "So you can, you know, stack this layers and basically at each layer you get.",
            "An increasing level of abstraction of your input data.",
            "The cool thing is that the.",
            "What we call the weights, the parameter of each layers are just trained and are trained by, you know something like gradient descent.",
            "So it looks OK like that because we would like then obtain like hidden representation in each layer.",
            "But the main problem here is how can we feed?",
            "You know text usually non networks understand like just you know number.",
            "So how can we feed the text?",
            "So.",
            "It's."
        ],
        [
            "Quite simple, you just like map simply.",
            "Next to like a feature space, you embed them into a feature space.",
            "So your one is like just like a 50 dimensional feature vector, nothing else But this feature vector, you know, is going to be trained.",
            "So.",
            "How do you define these features for the words and their handcrafted know, know, know, know, know, as I mean you map them to like a feature space.",
            "OK, and you train them by quite indecent.",
            "You really like?",
            "Train them the train?",
            "Yes our train."
        ],
        [
            "Yeah, it's an important point.",
            "They are trained.",
            "So the implementation is, like, you know, quite simple.",
            "It's basically a kind of look up table which has a lot of parameters like.",
            "It's a big matrix of the size, I mean the the size of blackler, feature size you want for the walls, for example 50 times the number of words you have in the dictionary.",
            "And basically the look up table just return 411, which is an index in some dictionary.",
            "It returns that the corresponding column in the big matrix, that's it.",
            "And you train, you know this parameters by gradient descent.",
            "So this is actually applicable to any kind of discrete features.",
            "For example, we could do stemming if we if we like, so you could apply that with the stem walls you could like use capitalization features.",
            "I mean, since I got all this kind of features, you can use this idea.",
            "Actually, this idea is quite all as old as well.",
            "It's more recent than our networks, but still it has been proposed in 2001 by Yoshua Bengio for like actually training a language model and it works very well.",
            "So what?"
        ],
        [
            "Once you know how to transform the words into vectors while you are already almost done, you just.",
            "You can actually, you know.",
            "I mean, you consider a sentence like here like the cat sat on the mat and suppose you want to tag the world on so you are going to take a window of once around this world of interest.",
            "So it's a fixed size window here.",
            "It's a window of size 5.",
            "And for each one in a window, you consider all your features of interest, and for each features on each word, you apply your lookup table idea.",
            "So you obtain for each one like the corresponding feature vectors.",
            "Once you've done that, you can just like concatenate, concatenate everything, and feed that to classical.",
            "You know neural network layers and you train the complete scene by back propagation, that's it.",
            "And surprisingly, it works well for many tasks like you know, part of speech, shrinking entity or join.",
            "Is it OK?",
            "The the only thing is that for task which like require long range dependencies like for example semantical labeling, you cannot expect this to work because phone number for semantic role labeling you are taking a world with respect.",
            "To affirm you chose before hand and there might be even like outside the window.",
            "So in this kind of task you know you cannot consider just a window of text, you have to consider the complete sentence.",
            "So instead."
        ],
        [
            "Well, just like consider the complete sentence.",
            "The plan then is that.",
            "A sentence might be of variable length, right?",
            "And there is actually like a quite common.",
            "No network architecture to deal with this kind of things.",
            "It's called like convolutions.",
            "Anne.",
            "Well, this time it's a bit like a generalization of the window approach you basically consider again a window of.",
            "Size weather right here at three you concatenate all the three vectors together and you apply your metrics.",
            "Atomic application which produce you like kind of local features.",
            "You shift.",
            "You do the same for the next three vectors.",
            "It gives you again like local features and so on.",
            "Come on, this was done in 1989.",
            "Why are you presenting the Synergy Tour in 2009?",
            "Because you forgot about it?",
            "Actually works very well for text, so so then.",
            "Then basically we are going to train again this parameter metrics as every other.",
            "You know non network layer.",
            "So once you have produced like this, you know local features.",
            "You still have, like a number of vectors, which is I mean which is like.",
            "Depending on the number of words in the sentence, so you still have to get rid of this.",
            "You know time dimension.",
            "So for doing this here we apply a Max overtime.",
            "So we try to capture the most interesting features overtime by just applying your Max over each features along time OK?",
            "And that's it.",
            "Once we've got that, we have now a fixed size feature vector which like represent or complete sentence, and we can we feed that to classical neural network layers.",
            "So we are going to do that actually for each word in the sentence.",
            "So we need to add some extra features which dealt with the network.",
            "Which word we are interested in and with respect to which verb we want to time, right?"
        ],
        [
            "So the network in that case looks like that it's again, I mean, this time you consider complete sentence for all one in the sentence you consider all the features you are interested in.",
            "You apply the look up table for all the all the words in the sentence you apply.",
            "Your convolutions gives you like local feature like tells you applied the Max of our time to get rid of the time dimension.",
            "And then you feel that the classical are layers and that's it."
        ],
        [
            "So we have to train this beast.",
            "We actually like use like.",
            "I mean we are.",
            "We are machine learner so we consider our training set right and we are going to.",
            "Interpret somehow or neural network outputs as priorities.",
            "So afterwards I will show you two ways to interpret.",
            "You know this network outputs as priorities.",
            "But once you have probabilities, you can just like maximum, maximize the log likelihood an apply stochastic gradient dissent over.",
            "You know the bomb, it also the network that are represented here by Theta.",
            "Can you just?",
            "Train legate, I mean people think usually that that there is a lot of tricks you know, but actually we consider only two classical tricks.",
            "We divide the learning rates by what is called the funding.",
            "But actually the learning rate is fixed.",
            "And we initialize also according to the opening, nothing else.",
            "So maybe you.",
            "What's this fan?",
            "And I've never heard of that.",
            "Yeah, good questions.",
            "It's I think it's internal gave this name.",
            "Actually it's like number of.",
            "Inputs we have in each layer basically OK, so somehow you normalize by the number of parameters you have in the layer.",
            "Input parameters you have anyway, so you'll know network is what kind of.",
            "I mean, if you look at it like you know like this, it's really like modules and actually we each module.",
            "Computes is it's there, but if with.",
            "I mean, each module has to compute is derivative with respect to the parameters right?",
            "And this can be done efficiently by what is called back propagation, which is just an application of the chain rule.",
            "So each module basically computes its derivative with respect to its own parameter.",
            "An it started with respect to its own inputs, and it's going to pass this directive to the next model, which I mean the previous model, which is going to use that then to compute its perative with respect to its own parameters.",
            "So the only problem here is how can we interpret non network output as probabilities while there is a very simple way that many people use actually which is."
        ],
        [
            "Usually cross entropy, quite iron.",
            "So you consider only output score of no network, so you have one output scope, clasper tag of interest in your in your task.",
            "And a well, you know, you pass, you pass it to the exponential just so it's positive.",
            "And you know my life with respect to all the classes.",
            "So you get the conditional probability.",
            "So here for some reasons I did find this quite handy operation which is logged and when I.",
            "But that will log.",
            "I basically get this log likelihood that I want to maximize.",
            "So you can see the log it as a kind of, you know, softmax if you like.",
            "So basically what you are doing here is that.",
            "You push up the score of the right class while you push down this course of all the other classes.",
            "And actually, at the extreme, if like the softmax is really on Max, you would personally like the score of the best class.",
            "I mean the best class according to the network.",
            "The problem with this criterion is that it doesn't take into account that we are dealing with like sentences.",
            "It's like just considering one world at the time, so it kind of sucks.",
            "So instead."
        ],
        [
            "We prefer to consider like the complete sentence.",
            "For each word in the sentence, you apply or no network is going to produce you score per tag in your NLP task.",
            "And here we consider an additional score which is going to be a score that you give when you jump from one level to 01.",
            "So we define then the score of a sentence with like T wants for path of tags.",
            "I to be like the sum of all the scores along this path plus this one is the transition scores right?",
            "And once again, we can apply the centric than before and interpret this core as a priority.",
            "But by just normalizing over all the passes, so before we were normalizing with respect to all the classes, this time we normalized with respect to all the paths.",
            "And basically it gives you this log likelihood that you want to make maximize.",
            "So once again you basically push up the score of the right path while pushing down the scores of all those of us at the extreme, if the if the law God is like just so Max, it's basically pushing down the score of the best path found by the network.",
            "The only problem is that this lohgad is now over all like all past, and it's actually very expensive to compute if you do it with your very naively.",
            "But Fortunately I mean it has been.",
            "Proposed a long time ago."
        ],
        [
            "There is something called the."
        ],
        [
            "For one algorithm which allow you to compute this logged in a recursive manner.",
            "You just have to consider this lohgad, but computed.",
            "On the bus up to the world T -- 1.",
            "So you suppose you have already this computation and by this simple regression you can just compute the log add overall the the bus up to the OPTI.",
            "Any and then you can just you know trained in our network.",
            "An backpropagating through this recursion using like the chain or as you did before falling, you know the normal layers, nothing, nothing different.",
            "It's a bit tricky mathematically, but it's OK. You can do it.",
            "You know it's important.",
            "So this is actually again quite all stuff.",
            "It's popular case of.",
            "What is called like graph transform networks or pose like by Leon Bottou, Yann Luca in 9798.",
            "And well, if you if you like, you can compare it to conditional random field.",
            "Which would be basically the linear version of this.",
            "You know for sequences.",
            "So here we are, really nonlinear and we do the same then condition or other field.",
            "We train it by stochastic gradient descent.",
            "But it.",
            "Yeah, as a side note, you know what influence you need to find the best path, and you do that by applying the Viterbi algorithm, which is basically the same algorithm data for one algorithm, But where you replace the log add by Max."
        ],
        [
            "Yeah, so well we can apply the you know our networks straight out of the box to task of interest.",
            "So we actually took the window approach for barter speech, barter speech chunking on any arm.",
            "The window was off size 5 on the network had like 300 hidden units.",
            "We use a convolutional approach for semantic role labeling.",
            "The kernel of the conversion was three and it had like 300 hidden units for the condition.",
            "An additional layer of biology, then units.",
            "We use both the World tag likelihood and the sentence style acute.",
            "And we fed as features was actually we took lower case words to limit the number of words in the dictionary and to keep the information of the case.",
            "We basically added like a capital letter features.",
            "So we just apply the network like that on all the task and you can see the results here.",
            "Compare with the benchmark systems in parallel accuracy for part of speech and is in F1 score for all other orders or task.",
            "And you can see that the government is, like you know, quite fair.",
            "Sometimes, yeah, come on guy.",
            "I admit when you use the whole sentence rather than the words are in performance is better, but it still sucks.",
            "Yeah, you're wasting my time.",
            "Yeah, I mean, you know I don't put the good results at the beginning of the talk, so actually it says I have to wait.",
            "Yeah?",
            "I mean you are quite right.",
            "It kind of sucks.",
            "Even so we are like in the ballpark of all the natural language processing existing on the planet.",
            "But yeah, you might wonder why it sucks.",
            "And actually if you think about it, while the capacity of like or network is mainly in.",
            "In the world features right.",
            "So the first question we we have in mind is, are we training this world?",
            "Features rates.",
            "So you should think more about it."
        ],
        [
            "Consider it sometimes like the cat sat on the mat.",
            "It should have the same kind of tight, then the feline sat on the net on the mat.",
            "Even sooner feline, you know, is rare compared to cat.",
            "Maybe it doesn't even exist.",
            "Actually in Wall Street Journal.",
            "So if the ones on buildings were close to each other cat and feline were close to each other in the world or buildings, then it would be really great because by country, by continuity the tax would be the same, right?",
            "So here I looked at like some random words in the dictionary underneath you have like.",
            "The rank in frequency of the world so small small number means the world is quite frequent on a large number means it is quite rare.",
            "And I reported the 10 closest wall according to the clinic Euclidean distance from the top one, and you can see, well, at least according to the Euclidean distance.",
            "The network doesn't seem to get anything, and it's really not surprising, because in fact, in Wall Street Journal there is about 1 million of wants and 15% of the most frequent world IP, like 90% of the time.",
            "So many words appear only like once or twice on 10 times.",
            "So you cannot expect to train a 50 dimensional vector with, like you know, a word which appeared few times."
        ],
        [
            "But Fortunately we have, like you know, a lot of unlabeled data available around here.",
            "You just take the web, and it's basically infinite.",
            "So in this part we are not.",
            "To liberate the.",
            "Unlabeled data in our system."
        ],
        [
            "If we had a box, I mean a black box.",
            "Which was able to answer to the question if a sentence is actually English on it, it would be really great because it would mean this black box is able to capture, you know, the syntax, the semantics or grammar of the language.",
            "So actually such, I mean some people try to build such black box.",
            "And yeah, like you're sure Benji on original Ducharme in 2001.",
            "Try to make what is called a language model.",
            "Which was.",
            "Predicting the probability of the next word in the sentence according to previous word in the sentence.",
            "In our case, we think that you know dealing with qualities.",
            "It's over complicated.",
            "You have.",
            "You know, normalization problems which are annoying and actually the entropy criterion they use tend to be determined mainly by the most frequent phrases so.",
            "Instead, instead, we prefer like to use a ranking criterion.",
            "So if I consider like network which is again going to be or window approach network.",
            "This network is going to give me a score score which says if my sentence is English or not.",
            "And I want the score of.",
            "Window of text in some copies to be larger with a certain margin than any other score for the same window of text, but where I replaced the middle one by some random wall.",
            "So basically the true piece of text coming from my copies is my positive example and my negative example is going to be this same window of text replaced in the middle by some Wonderwall.",
            "Again, I'm going to train this huge language model.",
            "By stochastic gradient descent I sent."
        ],
        [
            "And I'm going to train it on a lot of data.",
            "We actually first considered Wikipedia, which is which was about at the time, 600 million of words.",
            "Then we like considered an additional data set, which is called Reuters RC, one which added like 200 million of examples.",
            "It's a massive data set.",
            "You cannot expect you know to do classical training validation scheme like you know you train.",
            "You train on like a grid of parameters, and you're trying to find the right ones.",
            "I mean, here, it's impossible.",
            "It takes too much time, so instead we did like a bit like in biology.",
            "You like train.",
            "I mean you breed a couple of networks lines and you know every day you check or how is it going at you.",
            "Try to make decision according to some validation set to like stubble line or create a new one.",
            "And when we train our network like that, we train actually like the.",
            "The network over Wikipedia during about four weeks.",
            "With an increasing Detroit size and a lot of other things, and after four weeks we took the embeddings, it produced and we initialize the second language model that we trained, then over Wikipedia plus Reuters, and it was again three additional weeks of training."
        ],
        [
            "So after all this time we looked at the words embeddings and they look actually pretty great.",
            "The networks seem to really capture automatically.",
            "You know, some good semantic there, so we just like talk, this one.",
            "Embeddings initialized networks for the task you know, like part of speech, drinking, name, OT organization, and semantic role labeling and just rain as usual with just by initializing by this embeddings."
        ],
        [
            "Yeah.",
            "Beta factors that many times or is the amount of computation per day training.",
            "It's just domain.",
            "Are they expensive?",
            "I mean bye bye data.",
            "It's not that expensive because it's basically like 2 vector to matrix.",
            "Vector mitigation is small because it's so huge data set.",
            "I mean it's almost 1 billion of examples.",
            "You could actually apologize.",
            "It usually did some, you know.",
            "Try to parallelize it, but here we didn't want to mess with that.",
            "Basically, just like let it rain on one computer, we have time on today.",
            "The computers are actually faster than 10 years ago.",
            "So yeah, we applied this embeddings like to all natural language processing desk of interest.",
            "As you can see it gives like 4 for all task it gives really like a huge boost.",
            "We have an additional boost, mainly on name entity, organization.",
            "For the second I mean the larger language model.",
            "This is not really surprising because when we look at the world coverage we regain a lot with the second, the second language model with named entity recognition.",
            "So well, we thought we thought it was a good start.",
            "But still, you know we got this season or presentation which like seems to work pretty well on all these tasks.",
            "So now you know the next question is, why not training everything together and see what happens, right?",
            "So that's what we did.",
            "It's cool."
        ],
        [
            "Joint training."
        ],
        [
            "Just asking it has been used a lot in our networks.",
            "And you can find a good value for networks in like Rich Caruana disease in 97.",
            "So again, quite old stuff.",
            "Basically what we are doing here is that we are considering, for example two tasks.",
            "So yeah, all the networks for the two tasks and.",
            "So these networks can receive, you know, different inputs if they like, but the parameters are going to be shared.",
            "That is the bomb.",
            "It or this look up tables are the same level parameters, so these look up tables and the bomb.",
            "It also this first hidden layer are going to be the same as well.",
            "The rest of the layers are like kept, you know independent, but the first layers are we shared so."
        ],
        [
            "It rained the participants drinking on any altogether, and we had a look at what it.",
            "What what did this and basically what it does is that it gives a kind of significant boost for chunking, and linguist would tell you that it's not surprising because part of speech is often used as a good feature for drinking.",
            "For the other task, it doesn't change anything, basically.",
            "So.",
            "Now we have like quite reasonable results for all task, but of course you know there is always like."
        ],
        [
            "The temptation we.",
            "Yeah, I know we can always do like those of people you know.",
            "Try to see if you know if we plug this part of speech into this trunking.",
            "Does it help?",
            "Or if we put the drinking into the semantic webbing that help things like that.",
            "So that's what we did."
        ],
        [
            "We consider actually like stemming as good feature of our part of speech.",
            "It's often you know it's always to use actually in part of speech.",
            "So here we use only like the two car, 2 less characters of each one as an additional feature.",
            "We tried actually to use three or you know prefixes.",
            "If it was also exist, but you did not help.",
            "I mean it's mainly like the two characters which were important to less characters.",
            "We consider actually gazetteers.",
            "That is like a list of names.",
            "It's kind of cheating.",
            "I think that everybody does that, so we consider that foreign entity organization and there is like about eight dozen locations.",
            "Person name organizations given actually by the kernel challenge itself, so you know everybody else used this dataset.",
            "I mean, some use like.",
            "The guys at gym but we stick to this simple one.",
            "We also tried part of speech as a feature for drinking on the Monte organization.",
            "And drinking as a feature for semantic role labeling.",
            "Anna, you know?"
        ],
        [
            "The results it basically gives like a significant boost for all tasks.",
            "We are actually basically state of the art everywhere except for semantic role labeling, which is still not there.",
            "So I mean, it looks really good.",
            "The results in Italy kids because the networks is still training.",
            "It's the last result from this morning.",
            "Might have some results in like next week, but so."
        ],
        [
            "Yeah, this is an additional side because actually many people you know they don't trust non convex as you saw in the beginning of the talk so.",
            "I wanted to know, you know, the violence we had in these networks.",
            "So we train like 10 times each of the network for part of speech chunking on any other.",
            "And I reported here like the worst performance for each of them and the best one as well as the mean.",
            "And you can see there is a little bit of Mayans, but not that much.",
            "So of course we could.",
            "Then you know, try to average the 10 what networks we trained and things like that.",
            "But that's not our point here.",
            "So to reduce the variance in previous experiments, Yep.",
            "Is there rain with their registration?",
            "Yeah, yeah, exactly.",
            "It's a different organization.",
            "But which part is useful?",
            "Or both.",
            "You know it's stochastic, so I mean the training is stochastic, so the ordering of the data it might be different and initialization is like random, so it might be different.",
            "Always initialize randomly.",
            "Randomly according to the fun in you know, it's like it's a classical initialization.",
            "You just like to random numbers between I mean.",
            "Touch that.",
            "You you don't saturate too much, the hidden units basically.",
            "So yeah, exactly centered at zero and the variance is about.",
            "I mean the bit less than the.",
            "What would make the hyperbolic tangent such rights?",
            "So to reduce the variance in previous experiments, basically we use the same seed in all networks."
        ],
        [
            "Yeah, so is there any sexually or one less task which is like the semantical labeling where we are?",
            "As I say not quite there yet.",
            "And actually, people often say that passing is essential for.",
            "For semantic or labeling there, there are even like papers written about it.",
            "So on state of the art semantic here or labeling systems use actually several past reason.",
            "One I'm considering here as a benchmark used like I think 5 or 6 pass trees.",
            "So we decided.",
            "I mean, why not?",
            "You know, feeding laboratories to our network?",
            "So to doing so, we consider passing as a bus trip here, or we consider the bottom tags produced by by this Part 3, right?",
            "So that's what we call the level deal.",
            "The bottom tags actually correspond in fact to like chunk tags, except that what we call the bracketing.",
            "So when the tax starts and when the tag ends might differ from the chunking.",
            "Then basically we cut the last leaves of the bass string.",
            "An we consider the next button tags as you know additional features and then we cut again and we consider again the bottom tags as additional features.",
            "So we fed a pass or pass through like that by filling up to five levels of the past three.",
            "And we consider a."
        ],
        [
            "Hence the benchmark system and I'm sure you have to know that the benchmark system use six part three and they they also like provide a result with only one path 3 which is about three from the.",
            "Challenge.",
            "And you can see, well, we're already pretty pretty much there with.",
            "With like the level zero, I mean actually with that ranking we are pretty much there.",
            "But with levels or we we are going a bit over that and with more levels we get a little bit, but not that much.",
            "So Indian."
        ],
        [
            "We took, you know all these networks.",
            "And we did like a simple standalone.",
            "Implementations of all this.",
            "In that Journal, which processing task?",
            "So all these networks are basically mainly performing matrix vector multiplications, so we interface with BLAST.",
            "So we get some speed.",
            "And to summarize, or networks are using like lower case words.",
            "Plus the capitalization feature part of speech uses suffixes, not prefixes suffixes.",
            "Tracking users like part of speech tags name, entity organizer.",
            "Use like that here, and the same optical labeler use the levels of the bus tree that we actually trained to predict by, you know.",
            "Window Approach Network and actually the prediction was better than the chanak prediction.",
            "Or given by the challenge.",
            "Add A and then."
        ],
        [
            "So it's actually going pretty fast.",
            "We compared against existing system which are available so there are not many.",
            "Unfortunately we took here like the tutor and chance Ystem for part of speech.",
            "And you can see that we are, I mean very fast compared to them.",
            "Although I mean civil order of magnitude faster while we require much less memory, and it's basically the same for the semantic or label on the cool thing about that is, I mean it's very simple system compare."
        ],
        [
            "To what they are, it's completely standalone and I'm going to give you a demo so this system is going to be available soon.",
            "Actually mid January.",
            "So if you're interested in playing with it, you just send me an email.",
            "So I didn't lie to you, it's like really like few lines of see.",
            "And the computation is pretty easy because it's like completely, you know standalone.",
            "Sure, I'm going to use Blas just for fun.",
            "So it's also pretty fast to compile, basically done, and I'm going to feed like kind of sentence.",
            "And I have the tax right away so.",
            "You can actually try to compare with existing systems and you will see that it's really like day or night.",
            "So Indian I."
        ],
        [
            "I presented you like a kind of all purpose non network architecture for natural language processing task which involves tagging.",
            "It limits the task specific injuring by instead relying on like very large unlabeled data.",
            "And of course we do not plan to stop here now.",
            "We have like this hidden or presentation, but which are, you know, only limited towards.",
            "Basically we are now working on.",
            "Hidden opposition which are actually representing civil wars, so it's really the next step.",
            "So we received a bunch of critics on this this work, you know, for example, a common one is like why would I for get an LP expertise like for learning your neural network training skills.",
            "Well, as we said, natural language processing goals are really not limited to existing in natural language processing task.",
            "And if you stick to you know this always like finding new features.",
            "It's maybe a problem in the future when natural language processing tasks become more and more complicated.",
            "So we really think that task engineering excessive task engineering is really not desirable.",
            "We also like receive like critics like you know why using neural networks?",
            "Inoculated like 20 years ago.",
            "Well, sorry, but you know it scales on massive datasets.",
            "It's able to discover very, very nice hidden or presentation.",
            "And on top of that it's like all technology which is actually most of the technology which existed here that we use here existed in 97.",
            "So it's really like well proven technology.",
            "And the bottom line is, if we add started, you know, in 97 to train those networks with like vintage computers where we basically would be about finishing two days.",
            "So it was really the right time to start to train it, so that's it.",
            "So ice."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I just telling you a little story.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Disclaimer this slide.",
                    "label": 0
                },
                {
                    "sent": "Hello, I'm a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And I think I know quite a bit about natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Hi, I'm in your network and I know a little bit about deep learning.",
                    "label": 0
                },
                {
                    "sent": "You said Norway.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A neural network, obviously.",
                    "label": 0
                },
                {
                    "sent": "I never heard of it.",
                    "label": 0
                },
                {
                    "sent": "I mean, what did well?",
                    "label": 0
                },
                {
                    "sent": "Obviously you're too young.",
                    "label": 0
                },
                {
                    "sent": "Network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the beginning there was the discovery of the perceptual.",
                    "label": 1
                },
                {
                    "sent": "The steps on I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "I mean is it to kind of mill or?",
                    "label": 0
                },
                {
                    "sent": "No, if you take like a brain and you take a magnifying glass and look at the smallest thing you've got there.",
                    "label": 0
                },
                {
                    "sent": "And stupidest thing you've got a single Europe oh you mean like this like kind of stupid linear model.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, exactly but.",
                    "label": 0
                },
                {
                    "sent": "Really, I don't trust it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've been told it doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "Kind of sucks.",
                    "label": 0
                },
                {
                    "sent": "That's what Minsky said.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It made a bunch of people side, so they went on a quest to try and model nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "They tried all kinds of stuff like random projections to induce nonlinearity is adding nonlinear features to the inputs like products and features, and they even thought of kernels in 1964.",
                    "label": 1
                },
                {
                    "sent": "I mean, I really don't know why it says they made this our life so complicated.",
                    "label": 0
                },
                {
                    "sent": "You just take good features on good linear model and that's it.",
                    "label": 0
                },
                {
                    "sent": "You know super vector machine.",
                    "label": 1
                },
                {
                    "sent": "Well, people were still depressed.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Until they discovered me, most lab sector.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I bet a pile of perciform.",
                    "label": 0
                },
                {
                    "sent": "Well, they got excited.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They were so excited they kept trying more and more things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, adding layers I bet.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well yeah, they try it.",
                    "label": 0
                },
                {
                    "sent": "Time dilay your networks.",
                    "label": 0
                },
                {
                    "sent": "What competitions as well yeah Oh yeah they tried recurrent urinary yeah but the legs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, even how do I also?",
                    "label": 0
                },
                {
                    "sent": "Still people got scared.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, not surprised about that.",
                    "label": 0
                },
                {
                    "sent": "Reach the complexity of the open, intelligent, even universe brain.",
                    "label": 1
                },
                {
                    "sent": "I wonder on the brain of who?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's not you 0.",
                    "label": 0
                },
                {
                    "sent": "Just accept from universe there trying to roller itself, so just complex they decide what they do is too complex.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sir, that's something that's complex.",
                    "label": 0
                },
                {
                    "sent": "In fact, something convex someone came up with a new perceptual network, but it's it's like this product formation.",
                    "label": 1
                },
                {
                    "sent": "That's right network.",
                    "label": 0
                },
                {
                    "sent": "I'm very proud of my father.",
                    "label": 0
                },
                {
                    "sent": "Actually, he always say I'm cool on sexy.",
                    "label": 0
                },
                {
                    "sent": "We liked your dog.",
                    "label": 1
                },
                {
                    "sent": "You know, some people wondered if you weren't just a linear model.",
                    "label": 0
                },
                {
                    "sent": "You know we scale notes.",
                    "label": 0
                },
                {
                    "sent": "I can do everything.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, life is convex and life is good.",
                    "label": 0
                },
                {
                    "sent": "Yeah she lot of papers you know you can talk.",
                    "label": 0
                },
                {
                    "sent": "But it didn't do everything they wanted.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, some people they wanted to try and learn some good features and then.",
                    "label": 0
                },
                {
                    "sent": "What for just the good features fitted to a linear model is fine.",
                    "label": 0
                },
                {
                    "sent": "Yeah, could be fine if you knew what the good features were, but otherwise you might want to learn them.",
                    "label": 0
                },
                {
                    "sent": "It's it's done.",
                    "label": 0
                },
                {
                    "sent": "Coins come on.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not going to work well.",
                    "label": 0
                },
                {
                    "sent": "People tried it anyway.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The network like this actually and then you replace the hidden layer with eternal life and they let him cuddle and they put you at the end of it nonsense, it's nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it looks a lot like me.",
                    "label": 0
                },
                {
                    "sent": "But you know, it might be nonconvex, but still might be useful.",
                    "label": 0
                },
                {
                    "sent": "Family so hold on a minute.",
                    "label": 0
                },
                {
                    "sent": "I've got a lot of data.",
                    "label": 0
                },
                {
                    "sent": "There are different tasks, maybe I can multitask these days here and share the features and none would be interesting.",
                    "label": 0
                },
                {
                    "sent": "I think they did a lot of that with myself.",
                    "label": 0
                },
                {
                    "sent": "You know, like I mean you cannot talk about that.",
                    "label": 0
                },
                {
                    "sent": "I mean they did it a lot.",
                    "label": 0
                },
                {
                    "sent": "It works very well.",
                    "label": 0
                },
                {
                    "sent": "So even like in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Come on.",
                    "label": 0
                },
                {
                    "sent": "Network.",
                    "label": 0
                },
                {
                    "sent": "Review again.",
                    "label": 0
                },
                {
                    "sent": "And later we can just share, share some representation where we learn features, yeah, but it's nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Doesn't mean it isn't interesting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then people also thought holding it.",
                    "label": 0
                },
                {
                    "sent": "I got this unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "What no?",
                    "label": 0
                },
                {
                    "sent": "I mean like it's positive support vector machine.",
                    "label": 0
                },
                {
                    "sent": "It works so well you take on labels, you fit it.",
                    "label": 0
                },
                {
                    "sent": "It's like a excellent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, your father was very proud of that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know and.",
                    "label": 0
                },
                {
                    "sent": "Non convex loss on your head.",
                    "label": 0
                },
                {
                    "sent": "Yeah come on.",
                    "label": 0
                },
                {
                    "sent": "I mean many people tried to convexified that and it it works very fine so.",
                    "label": 0
                },
                {
                    "sent": "Well, it might be quite late convexified.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just take a small data set, it works fine.",
                    "label": 0
                },
                {
                    "sent": "But yeah, anyway if something is non convex it doesn't mean.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "And then people thought I really want to get good results.",
                    "label": 0
                },
                {
                    "sent": "What I should do?",
                    "label": 0
                },
                {
                    "sent": "Yeah, as I always say they good features fitted too early, no model like me.",
                    "label": 0
                },
                {
                    "sent": "Really good bread and you doing nonlinear transformation is the original features to some new features that we feed to year.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this looks like a two layer network where the first layer is this being human brain.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm multilayer network where the first layer is machine learning, not human.",
                    "label": 1
                },
                {
                    "sent": "Ladies Watch were actually supposed to do.",
                    "label": 0
                },
                {
                    "sent": "It's I think it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's again, nonconvex.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then scalability.",
                    "label": 0
                },
                {
                    "sent": "Let's yeah, at least they made all book about me about that.",
                    "label": 0
                },
                {
                    "sent": "I mean, it works so well.",
                    "label": 0
                },
                {
                    "sent": "Well, they might have had to make a book about you because you're actually kind of slow, so they had to work very hard on that, yeah?",
                    "label": 0
                },
                {
                    "sent": "The problem with this team.",
                    "label": 0
                },
                {
                    "sent": "Support vectors in machine we tend to get fat.",
                    "label": 0
                },
                {
                    "sent": "Try to fix that different ways.",
                    "label": 0
                },
                {
                    "sent": "His few of them you could try and use plastic gradient descent works very well on call exponents, training, training me and your network.",
                    "label": 1
                },
                {
                    "sent": "Learning which support vectors to include two removal is fat support vectors and that might turn into nonconvex optimization like to learn at work.",
                    "label": 0
                },
                {
                    "sent": "That's done comics.",
                    "label": 0
                },
                {
                    "sent": "I don't trust it.",
                    "label": 0
                },
                {
                    "sent": "Or if you don't trust that, yeah, they're good features as I always say again and feed it to like a very nice support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Exactly, you could just go back to something like SVM, Lin, and training your linear CNN linear time and.",
                    "label": 0
                },
                {
                    "sent": "You're good.",
                    "label": 0
                },
                {
                    "sent": "Are you kind of just back to central?",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I got an idea.",
                    "label": 0
                },
                {
                    "sent": "Why don't I mean no one likes me?",
                    "label": 0
                },
                {
                    "sent": "I'm in your network, why don't we just rename it deep network?",
                    "label": 0
                },
                {
                    "sent": "And Justin thinks I'm pretty cool.",
                    "label": 0
                },
                {
                    "sent": "I think I will always call you another network anyways.",
                    "label": 0
                },
                {
                    "sent": "Well, Daniel.",
                    "label": 0
                },
                {
                    "sent": "Others seem to be excited again because they're here in this room.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's nonconvex.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But slightly more seriously, putting all this together.",
                    "label": 1
                },
                {
                    "sent": "We think of this.",
                    "label": 0
                },
                {
                    "sent": "I think the network rethink your networks.",
                    "label": 0
                },
                {
                    "sent": "Inflexible is different.",
                    "label": 1
                },
                {
                    "sent": "We can put different modules, layers, losses and regularizers.",
                    "label": 1
                },
                {
                    "sent": "We can do multitasking.",
                    "label": 0
                },
                {
                    "sent": "Same supervised learning and learning hidden representations, all very elegantly within this framework and scalable too.",
                    "label": 1
                },
                {
                    "sent": "So we're hoping that this might be a great tool for natural language processing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I think we we had enough bad jokes.",
                    "label": 0
                },
                {
                    "sent": "Wait for today.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So before before we continue any further, I mean we would like to tell you what we want to do with NLP.",
                    "label": 0
                },
                {
                    "sent": "You know we have all our secret goal.",
                    "label": 0
                },
                {
                    "sent": "So in fact we want to have a conversation with like your computer will be alone and we.",
                    "label": 1
                },
                {
                    "sent": "So of course we know it's not going to be easy, but at least we would be.",
                    "label": 0
                },
                {
                    "sent": "We would like to be able to convert like a piece of English into like some computer or family structure that we will call the hidden representation, which we would like this hidden representation to somehow encode the syntax semantic of the piece of English we consider.",
                    "label": 0
                },
                {
                    "sent": "So we are going to use like this kind of global hidden representation to basically train an natural language processing task and we are going to use like classical natural language processing task benchmark to see if, like you know this don't presentation are any good.",
                    "label": 1
                },
                {
                    "sent": "That is if like maybe to check if the computer understands or not.",
                    "label": 0
                },
                {
                    "sent": "So here we are really like starting natural language processing from scratch for getting like feature engineering.",
                    "label": 0
                },
                {
                    "sent": "At least we try to minimize it.",
                    "label": 1
                },
                {
                    "sent": "Add a using this hidden representation, which are quite global Indian.",
                    "label": 0
                },
                {
                    "sent": "So we divide it like our talking four parts.",
                    "label": 1
                },
                {
                    "sent": "The first one I think you had enough.",
                    "label": 0
                },
                {
                    "sent": "It was about neural networks.",
                    "label": 0
                },
                {
                    "sent": "Other Jason would say you have been brainwashed.",
                    "label": 0
                },
                {
                    "sent": "The the second part is going to be about labeling the third part about retrieval and the last part about situated learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The natural.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "So the battle, the natural language processing fee.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know it has been, you know, existing since quite awhile and actually many tasks.",
                    "label": 0
                },
                {
                    "sent": "I like task which involves tagging.",
                    "label": 0
                },
                {
                    "sent": "So in the first part we are interesting in this task and the task.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will consider today I actually like.",
                    "label": 0
                },
                {
                    "sent": "Those those four task, the first one is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's called like part of speech tagging is it's a syntactic task.",
                    "label": 0
                },
                {
                    "sent": "You want to know if each word in the sentence is like a noun and adverb legit.",
                    "label": 0
                },
                {
                    "sent": "If and so on, then you have something a bit more complicated with which is called the chunking.",
                    "label": 0
                },
                {
                    "sent": "This time you want to get like syntactic constituent that is like chunk of ones and find out you know noun, phrase, verb phrase and so on and you have something a bit more semantic which is called named entity recognition.",
                    "label": 1
                },
                {
                    "sent": "You want to know if like a award or like tree or chunk of world is like a person name, company name, location, name, external.",
                    "label": 0
                },
                {
                    "sent": "And then there is like no most complicated task.",
                    "label": 0
                },
                {
                    "sent": "I'm going to consider today, which is like semantic or labeling.",
                    "label": 1
                },
                {
                    "sent": "So for this one you consider a verb before hand in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this sentence John A deer born in the garden, there is only one version.",
                    "label": 0
                },
                {
                    "sent": "So you consider 8 and you want to know what is acting on eight when it's on what it's exciting to hear.",
                    "label": 0
                },
                {
                    "sent": "I mean, obviously John is acting on eight.",
                    "label": 0
                },
                {
                    "sent": "It's acting on the upper.",
                    "label": 0
                },
                {
                    "sent": "Where it's acting when it one is it's acting, I mean and so on.",
                    "label": 0
                },
                {
                    "sent": "So you have many tags I got.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As an LP's been existing since quite a while there is, it exists actually a lot of benchmarks we consider here benchmarks which are based on Wall Street Dollar and Reuters data set.",
                    "label": 0
                },
                {
                    "sent": "Wall Street Journal is about like 1 million label off once for the taskbar.",
                    "label": 0
                },
                {
                    "sent": "Interested in and for writers, it's about 200 K wants.",
                    "label": 0
                },
                {
                    "sent": "And we followed exactly the setup proposed by the Co NLL.",
                    "label": 0
                },
                {
                    "sent": "Competition challenges for chunking.",
                    "label": 0
                },
                {
                    "sent": "It's in 2004.",
                    "label": 0
                },
                {
                    "sent": "Any rights in 2003 and semantic horribly?",
                    "label": 0
                },
                {
                    "sent": "And so it's a one in 2005.",
                    "label": 0
                },
                {
                    "sent": "In for part of speech, we followed the exactly the setup proposed by total over in 2003.",
                    "label": 0
                },
                {
                    "sent": "So many people like tried many systems on these benchmarks.",
                    "label": 0
                },
                {
                    "sent": "On here I reported the main.",
                    "label": 0
                },
                {
                    "sent": "I mean the top system I'm aware of.",
                    "label": 0
                },
                {
                    "sent": "So when it's when it's about like winning a challenge, it's not surprising to find some people you know trying to win at all means, so they would like, for example, take extra data, you know, like an external labeled data set.",
                    "label": 0
                },
                {
                    "sent": "Even like a I mean overtune like crazy.",
                    "label": 0
                },
                {
                    "sent": "Each of their model I mean.",
                    "label": 0
                },
                {
                    "sent": "When it's about comparison, we don't really care about that.",
                    "label": 0
                },
                {
                    "sent": "We it doesn't bring any singular.",
                    "label": 0
                },
                {
                    "sent": "If somebody like trains on something you know extra or so we consider as benchmark systems in this, the rest of the part of this talk.",
                    "label": 1
                },
                {
                    "sent": "System which are really well established and system which like avoid using external labeled data.",
                    "label": 1
                },
                {
                    "sent": "So I like put in bold all the system we're going to consider for each task as benchmark system in the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "As a side note, and Owens and in 2005 used for any or some external data.",
                    "label": 1
                },
                {
                    "sent": "But it was unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And Koeman Punja Kanaka tour in 2005 used.",
                    "label": 0
                },
                {
                    "sent": "Actually, extra bass trees, which were not provided by the challenge, so you have to keep that in mind.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So natural language processing task quite complex task an if you want to solve a complex task, you have better through the complex system.",
                    "label": 0
                },
                {
                    "sent": "So for me the really like 2 extreme choice to get the complex system.",
                    "label": 1
                },
                {
                    "sent": "The first one is what I called large scale engineering, so I think we brainwashed you are better at the beginning about that.",
                    "label": 0
                },
                {
                    "sent": "You basically like design, very good encrusted features that you know are good for your, for your task, and you feel that to a very classical, you know linear or simple learning algorithm so that actually works very well.",
                    "label": 0
                },
                {
                    "sent": "But we are just not interested in that.",
                    "label": 0
                },
                {
                    "sent": "We prefer, you know.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The learning way.",
                    "label": 0
                },
                {
                    "sent": "Instead we use like simple features that is like features which are related gently to your task, like in our case, it would be a text Jackie straight text and you try to design this time a quite complex system over this text features.",
                    "label": 0
                },
                {
                    "sent": "I mean these text walls and.",
                    "label": 0
                },
                {
                    "sent": "You hope that your system is going to implicitly learn the right features.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So existing natural language processing system really tend to use, like you know, the 1st the 1st way.",
                    "label": 0
                },
                {
                    "sent": "So they usually design encrusted features and feed it a feed them to like shallow classifier like a spotlight on machine.",
                    "label": 1
                },
                {
                    "sent": "Sometimes there is a decoder on top or something, but it's often like that.",
                    "label": 1
                },
                {
                    "sent": "So for tasks like button speech, ranking features are actually is really quite simple things like combination of words and labels.",
                    "label": 0
                },
                {
                    "sent": "I mean it's OK, but for more complicated task it tends to be a bit more wild, in particular for semantical labeling.",
                    "label": 0
                },
                {
                    "sent": "And actually it's sometimes even more complicated than that.",
                    "label": 0
                },
                {
                    "sent": "People tend to like cascade features.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for semantic role labeling they would like first hand crafted features get from this uncrested features the part of speech tags from this part of speech tags they would like build up our street and then from this past three they would again handcraft which features and feed again these features to a new classifier and they would obtain.",
                    "label": 1
                },
                {
                    "sent": "The tags are interested in.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We think that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Trying to do like some task specific engineering really like limits.",
                    "label": 0
                },
                {
                    "sent": "The natural language processing scope because each time you have like a new task you need again to find new features and as the task tend to be more more complicated over the years you need to get you know more and more complicated features.",
                    "label": 0
                },
                {
                    "sent": "So instead we would like to find some kind of unified hidden representation for text.",
                    "label": 1
                },
                {
                    "sent": "Which would you know represent very well text and would would work for any kind of.",
                    "label": 0
                },
                {
                    "sent": "Natural Language Processing task, which involves tagging an over these hidden or presentation.",
                    "label": 0
                },
                {
                    "sent": "We would be able then to build some kind of unified natural language processing architecture.",
                    "label": 0
                },
                {
                    "sent": "Without any external engineering.",
                    "label": 0
                },
                {
                    "sent": "So we want to go towards this direction and to achieve this goal we are like going to start from scratch for get most of natural language processing knowledge.",
                    "label": 1
                },
                {
                    "sent": "We are going to keep actually as the natural language processing benchmarks to see if we do, you know any good.",
                    "label": 0
                },
                {
                    "sent": "But really, in a in this talk you have to remember or dogma which is avoid this specific engineering.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are doing natural language processing.",
                    "label": 0
                },
                {
                    "sent": "We need some kind of complicated system, something which would scale because we have a lot of data and something which would like fine somehow hidden or presentation.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if you know it rings a Bell in your head, but for me the obvious choice is like a normal network.",
                    "label": 0
                },
                {
                    "sent": "So maybe you forgot about knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because it's quite old.",
                    "label": 0
                },
                {
                    "sent": "But for me, and although it works just like a series of metrics, vector multiplications followed by some nonlinear ID.",
                    "label": 0
                },
                {
                    "sent": "So in our case we use like a simple.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's like the hard version of the hyperbolic tangent, but you can use whatever you want.",
                    "label": 0
                },
                {
                    "sent": "So you can, you know, stack this layers and basically at each layer you get.",
                    "label": 1
                },
                {
                    "sent": "An increasing level of abstraction of your input data.",
                    "label": 1
                },
                {
                    "sent": "The cool thing is that the.",
                    "label": 0
                },
                {
                    "sent": "What we call the weights, the parameter of each layers are just trained and are trained by, you know something like gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So it looks OK like that because we would like then obtain like hidden representation in each layer.",
                    "label": 0
                },
                {
                    "sent": "But the main problem here is how can we feed?",
                    "label": 0
                },
                {
                    "sent": "You know text usually non networks understand like just you know number.",
                    "label": 1
                },
                {
                    "sent": "So how can we feed the text?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite simple, you just like map simply.",
                    "label": 0
                },
                {
                    "sent": "Next to like a feature space, you embed them into a feature space.",
                    "label": 0
                },
                {
                    "sent": "So your one is like just like a 50 dimensional feature vector, nothing else But this feature vector, you know, is going to be trained.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do you define these features for the words and their handcrafted know, know, know, know, know, as I mean you map them to like a feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, and you train them by quite indecent.",
                    "label": 0
                },
                {
                    "sent": "You really like?",
                    "label": 0
                },
                {
                    "sent": "Train them the train?",
                    "label": 0
                },
                {
                    "sent": "Yes our train.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, it's an important point.",
                    "label": 0
                },
                {
                    "sent": "They are trained.",
                    "label": 0
                },
                {
                    "sent": "So the implementation is, like, you know, quite simple.",
                    "label": 0
                },
                {
                    "sent": "It's basically a kind of look up table which has a lot of parameters like.",
                    "label": 0
                },
                {
                    "sent": "It's a big matrix of the size, I mean the the size of blackler, feature size you want for the walls, for example 50 times the number of words you have in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And basically the look up table just return 411, which is an index in some dictionary.",
                    "label": 1
                },
                {
                    "sent": "It returns that the corresponding column in the big matrix, that's it.",
                    "label": 0
                },
                {
                    "sent": "And you train, you know this parameters by gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So this is actually applicable to any kind of discrete features.",
                    "label": 0
                },
                {
                    "sent": "For example, we could do stemming if we if we like, so you could apply that with the stem walls you could like use capitalization features.",
                    "label": 0
                },
                {
                    "sent": "I mean, since I got all this kind of features, you can use this idea.",
                    "label": 0
                },
                {
                    "sent": "Actually, this idea is quite all as old as well.",
                    "label": 0
                },
                {
                    "sent": "It's more recent than our networks, but still it has been proposed in 2001 by Yoshua Bengio for like actually training a language model and it works very well.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once you know how to transform the words into vectors while you are already almost done, you just.",
                    "label": 0
                },
                {
                    "sent": "You can actually, you know.",
                    "label": 0
                },
                {
                    "sent": "I mean, you consider a sentence like here like the cat sat on the mat and suppose you want to tag the world on so you are going to take a window of once around this world of interest.",
                    "label": 1
                },
                {
                    "sent": "So it's a fixed size window here.",
                    "label": 0
                },
                {
                    "sent": "It's a window of size 5.",
                    "label": 1
                },
                {
                    "sent": "And for each one in a window, you consider all your features of interest, and for each features on each word, you apply your lookup table idea.",
                    "label": 0
                },
                {
                    "sent": "So you obtain for each one like the corresponding feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Once you've done that, you can just like concatenate, concatenate everything, and feed that to classical.",
                    "label": 0
                },
                {
                    "sent": "You know neural network layers and you train the complete scene by back propagation, that's it.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly, it works well for many tasks like you know, part of speech, shrinking entity or join.",
                    "label": 0
                },
                {
                    "sent": "Is it OK?",
                    "label": 0
                },
                {
                    "sent": "The the only thing is that for task which like require long range dependencies like for example semantical labeling, you cannot expect this to work because phone number for semantic role labeling you are taking a world with respect.",
                    "label": 1
                },
                {
                    "sent": "To affirm you chose before hand and there might be even like outside the window.",
                    "label": 0
                },
                {
                    "sent": "So in this kind of task you know you cannot consider just a window of text, you have to consider the complete sentence.",
                    "label": 0
                },
                {
                    "sent": "So instead.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, just like consider the complete sentence.",
                    "label": 0
                },
                {
                    "sent": "The plan then is that.",
                    "label": 0
                },
                {
                    "sent": "A sentence might be of variable length, right?",
                    "label": 0
                },
                {
                    "sent": "And there is actually like a quite common.",
                    "label": 0
                },
                {
                    "sent": "No network architecture to deal with this kind of things.",
                    "label": 0
                },
                {
                    "sent": "It's called like convolutions.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, this time it's a bit like a generalization of the window approach you basically consider again a window of.",
                    "label": 0
                },
                {
                    "sent": "Size weather right here at three you concatenate all the three vectors together and you apply your metrics.",
                    "label": 0
                },
                {
                    "sent": "Atomic application which produce you like kind of local features.",
                    "label": 0
                },
                {
                    "sent": "You shift.",
                    "label": 0
                },
                {
                    "sent": "You do the same for the next three vectors.",
                    "label": 0
                },
                {
                    "sent": "It gives you again like local features and so on.",
                    "label": 0
                },
                {
                    "sent": "Come on, this was done in 1989.",
                    "label": 0
                },
                {
                    "sent": "Why are you presenting the Synergy Tour in 2009?",
                    "label": 0
                },
                {
                    "sent": "Because you forgot about it?",
                    "label": 0
                },
                {
                    "sent": "Actually works very well for text, so so then.",
                    "label": 0
                },
                {
                    "sent": "Then basically we are going to train again this parameter metrics as every other.",
                    "label": 0
                },
                {
                    "sent": "You know non network layer.",
                    "label": 0
                },
                {
                    "sent": "So once you have produced like this, you know local features.",
                    "label": 0
                },
                {
                    "sent": "You still have, like a number of vectors, which is I mean which is like.",
                    "label": 0
                },
                {
                    "sent": "Depending on the number of words in the sentence, so you still have to get rid of this.",
                    "label": 0
                },
                {
                    "sent": "You know time dimension.",
                    "label": 0
                },
                {
                    "sent": "So for doing this here we apply a Max overtime.",
                    "label": 0
                },
                {
                    "sent": "So we try to capture the most interesting features overtime by just applying your Max over each features along time OK?",
                    "label": 1
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Once we've got that, we have now a fixed size feature vector which like represent or complete sentence, and we can we feed that to classical neural network layers.",
                    "label": 0
                },
                {
                    "sent": "So we are going to do that actually for each word in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So we need to add some extra features which dealt with the network.",
                    "label": 0
                },
                {
                    "sent": "Which word we are interested in and with respect to which verb we want to time, right?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the network in that case looks like that it's again, I mean, this time you consider complete sentence for all one in the sentence you consider all the features you are interested in.",
                    "label": 0
                },
                {
                    "sent": "You apply the look up table for all the all the words in the sentence you apply.",
                    "label": 0
                },
                {
                    "sent": "Your convolutions gives you like local feature like tells you applied the Max of our time to get rid of the time dimension.",
                    "label": 0
                },
                {
                    "sent": "And then you feel that the classical are layers and that's it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have to train this beast.",
                    "label": 0
                },
                {
                    "sent": "We actually like use like.",
                    "label": 0
                },
                {
                    "sent": "I mean we are.",
                    "label": 0
                },
                {
                    "sent": "We are machine learner so we consider our training set right and we are going to.",
                    "label": 1
                },
                {
                    "sent": "Interpret somehow or neural network outputs as priorities.",
                    "label": 1
                },
                {
                    "sent": "So afterwards I will show you two ways to interpret.",
                    "label": 1
                },
                {
                    "sent": "You know this network outputs as priorities.",
                    "label": 1
                },
                {
                    "sent": "But once you have probabilities, you can just like maximum, maximize the log likelihood an apply stochastic gradient dissent over.",
                    "label": 0
                },
                {
                    "sent": "You know the bomb, it also the network that are represented here by Theta.",
                    "label": 0
                },
                {
                    "sent": "Can you just?",
                    "label": 0
                },
                {
                    "sent": "Train legate, I mean people think usually that that there is a lot of tricks you know, but actually we consider only two classical tricks.",
                    "label": 1
                },
                {
                    "sent": "We divide the learning rates by what is called the funding.",
                    "label": 1
                },
                {
                    "sent": "But actually the learning rate is fixed.",
                    "label": 0
                },
                {
                    "sent": "And we initialize also according to the opening, nothing else.",
                    "label": 0
                },
                {
                    "sent": "So maybe you.",
                    "label": 0
                },
                {
                    "sent": "What's this fan?",
                    "label": 0
                },
                {
                    "sent": "And I've never heard of that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good questions.",
                    "label": 0
                },
                {
                    "sent": "It's I think it's internal gave this name.",
                    "label": 0
                },
                {
                    "sent": "Actually it's like number of.",
                    "label": 1
                },
                {
                    "sent": "Inputs we have in each layer basically OK, so somehow you normalize by the number of parameters you have in the layer.",
                    "label": 0
                },
                {
                    "sent": "Input parameters you have anyway, so you'll know network is what kind of.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at it like you know like this, it's really like modules and actually we each module.",
                    "label": 0
                },
                {
                    "sent": "Computes is it's there, but if with.",
                    "label": 0
                },
                {
                    "sent": "I mean, each module has to compute is derivative with respect to the parameters right?",
                    "label": 0
                },
                {
                    "sent": "And this can be done efficiently by what is called back propagation, which is just an application of the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So each module basically computes its derivative with respect to its own parameter.",
                    "label": 0
                },
                {
                    "sent": "An it started with respect to its own inputs, and it's going to pass this directive to the next model, which I mean the previous model, which is going to use that then to compute its perative with respect to its own parameters.",
                    "label": 0
                },
                {
                    "sent": "So the only problem here is how can we interpret non network output as probabilities while there is a very simple way that many people use actually which is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Usually cross entropy, quite iron.",
                    "label": 0
                },
                {
                    "sent": "So you consider only output score of no network, so you have one output scope, clasper tag of interest in your in your task.",
                    "label": 1
                },
                {
                    "sent": "And a well, you know, you pass, you pass it to the exponential just so it's positive.",
                    "label": 0
                },
                {
                    "sent": "And you know my life with respect to all the classes.",
                    "label": 0
                },
                {
                    "sent": "So you get the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So here for some reasons I did find this quite handy operation which is logged and when I.",
                    "label": 0
                },
                {
                    "sent": "But that will log.",
                    "label": 0
                },
                {
                    "sent": "I basically get this log likelihood that I want to maximize.",
                    "label": 0
                },
                {
                    "sent": "So you can see the log it as a kind of, you know, softmax if you like.",
                    "label": 1
                },
                {
                    "sent": "So basically what you are doing here is that.",
                    "label": 0
                },
                {
                    "sent": "You push up the score of the right class while you push down this course of all the other classes.",
                    "label": 0
                },
                {
                    "sent": "And actually, at the extreme, if like the softmax is really on Max, you would personally like the score of the best class.",
                    "label": 0
                },
                {
                    "sent": "I mean the best class according to the network.",
                    "label": 1
                },
                {
                    "sent": "The problem with this criterion is that it doesn't take into account that we are dealing with like sentences.",
                    "label": 0
                },
                {
                    "sent": "It's like just considering one world at the time, so it kind of sucks.",
                    "label": 0
                },
                {
                    "sent": "So instead.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We prefer to consider like the complete sentence.",
                    "label": 0
                },
                {
                    "sent": "For each word in the sentence, you apply or no network is going to produce you score per tag in your NLP task.",
                    "label": 0
                },
                {
                    "sent": "And here we consider an additional score which is going to be a score that you give when you jump from one level to 01.",
                    "label": 0
                },
                {
                    "sent": "So we define then the score of a sentence with like T wants for path of tags.",
                    "label": 0
                },
                {
                    "sent": "I to be like the sum of all the scores along this path plus this one is the transition scores right?",
                    "label": 0
                },
                {
                    "sent": "And once again, we can apply the centric than before and interpret this core as a priority.",
                    "label": 0
                },
                {
                    "sent": "But by just normalizing over all the passes, so before we were normalizing with respect to all the classes, this time we normalized with respect to all the paths.",
                    "label": 0
                },
                {
                    "sent": "And basically it gives you this log likelihood that you want to make maximize.",
                    "label": 0
                },
                {
                    "sent": "So once again you basically push up the score of the right path while pushing down the scores of all those of us at the extreme, if the if the law God is like just so Max, it's basically pushing down the score of the best path found by the network.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that this lohgad is now over all like all past, and it's actually very expensive to compute if you do it with your very naively.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately I mean it has been.",
                    "label": 0
                },
                {
                    "sent": "Proposed a long time ago.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is something called the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For one algorithm which allow you to compute this logged in a recursive manner.",
                    "label": 0
                },
                {
                    "sent": "You just have to consider this lohgad, but computed.",
                    "label": 0
                },
                {
                    "sent": "On the bus up to the world T -- 1.",
                    "label": 0
                },
                {
                    "sent": "So you suppose you have already this computation and by this simple regression you can just compute the log add overall the the bus up to the OPTI.",
                    "label": 0
                },
                {
                    "sent": "Any and then you can just you know trained in our network.",
                    "label": 0
                },
                {
                    "sent": "An backpropagating through this recursion using like the chain or as you did before falling, you know the normal layers, nothing, nothing different.",
                    "label": 1
                },
                {
                    "sent": "It's a bit tricky mathematically, but it's OK. You can do it.",
                    "label": 0
                },
                {
                    "sent": "You know it's important.",
                    "label": 0
                },
                {
                    "sent": "So this is actually again quite all stuff.",
                    "label": 0
                },
                {
                    "sent": "It's popular case of.",
                    "label": 0
                },
                {
                    "sent": "What is called like graph transform networks or pose like by Leon Bottou, Yann Luca in 9798.",
                    "label": 0
                },
                {
                    "sent": "And well, if you if you like, you can compare it to conditional random field.",
                    "label": 0
                },
                {
                    "sent": "Which would be basically the linear version of this.",
                    "label": 0
                },
                {
                    "sent": "You know for sequences.",
                    "label": 0
                },
                {
                    "sent": "So here we are, really nonlinear and we do the same then condition or other field.",
                    "label": 1
                },
                {
                    "sent": "We train it by stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "But it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as a side note, you know what influence you need to find the best path, and you do that by applying the Viterbi algorithm, which is basically the same algorithm data for one algorithm, But where you replace the log add by Max.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so well we can apply the you know our networks straight out of the box to task of interest.",
                    "label": 0
                },
                {
                    "sent": "So we actually took the window approach for barter speech, barter speech chunking on any arm.",
                    "label": 0
                },
                {
                    "sent": "The window was off size 5 on the network had like 300 hidden units.",
                    "label": 1
                },
                {
                    "sent": "We use a convolutional approach for semantic role labeling.",
                    "label": 1
                },
                {
                    "sent": "The kernel of the conversion was three and it had like 300 hidden units for the condition.",
                    "label": 0
                },
                {
                    "sent": "An additional layer of biology, then units.",
                    "label": 1
                },
                {
                    "sent": "We use both the World tag likelihood and the sentence style acute.",
                    "label": 0
                },
                {
                    "sent": "And we fed as features was actually we took lower case words to limit the number of words in the dictionary and to keep the information of the case.",
                    "label": 1
                },
                {
                    "sent": "We basically added like a capital letter features.",
                    "label": 0
                },
                {
                    "sent": "So we just apply the network like that on all the task and you can see the results here.",
                    "label": 0
                },
                {
                    "sent": "Compare with the benchmark systems in parallel accuracy for part of speech and is in F1 score for all other orders or task.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the government is, like you know, quite fair.",
                    "label": 0
                },
                {
                    "sent": "Sometimes, yeah, come on guy.",
                    "label": 0
                },
                {
                    "sent": "I admit when you use the whole sentence rather than the words are in performance is better, but it still sucks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're wasting my time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, you know I don't put the good results at the beginning of the talk, so actually it says I have to wait.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "I mean you are quite right.",
                    "label": 1
                },
                {
                    "sent": "It kind of sucks.",
                    "label": 0
                },
                {
                    "sent": "Even so we are like in the ballpark of all the natural language processing existing on the planet.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you might wonder why it sucks.",
                    "label": 0
                },
                {
                    "sent": "And actually if you think about it, while the capacity of like or network is mainly in.",
                    "label": 0
                },
                {
                    "sent": "In the world features right.",
                    "label": 0
                },
                {
                    "sent": "So the first question we we have in mind is, are we training this world?",
                    "label": 1
                },
                {
                    "sent": "Features rates.",
                    "label": 0
                },
                {
                    "sent": "So you should think more about it.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider it sometimes like the cat sat on the mat.",
                    "label": 1
                },
                {
                    "sent": "It should have the same kind of tight, then the feline sat on the net on the mat.",
                    "label": 1
                },
                {
                    "sent": "Even sooner feline, you know, is rare compared to cat.",
                    "label": 0
                },
                {
                    "sent": "Maybe it doesn't even exist.",
                    "label": 0
                },
                {
                    "sent": "Actually in Wall Street Journal.",
                    "label": 1
                },
                {
                    "sent": "So if the ones on buildings were close to each other cat and feline were close to each other in the world or buildings, then it would be really great because by country, by continuity the tax would be the same, right?",
                    "label": 0
                },
                {
                    "sent": "So here I looked at like some random words in the dictionary underneath you have like.",
                    "label": 0
                },
                {
                    "sent": "The rank in frequency of the world so small small number means the world is quite frequent on a large number means it is quite rare.",
                    "label": 1
                },
                {
                    "sent": "And I reported the 10 closest wall according to the clinic Euclidean distance from the top one, and you can see, well, at least according to the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "The network doesn't seem to get anything, and it's really not surprising, because in fact, in Wall Street Journal there is about 1 million of wants and 15% of the most frequent world IP, like 90% of the time.",
                    "label": 0
                },
                {
                    "sent": "So many words appear only like once or twice on 10 times.",
                    "label": 0
                },
                {
                    "sent": "So you cannot expect to train a 50 dimensional vector with, like you know, a word which appeared few times.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But Fortunately we have, like you know, a lot of unlabeled data available around here.",
                    "label": 1
                },
                {
                    "sent": "You just take the web, and it's basically infinite.",
                    "label": 0
                },
                {
                    "sent": "So in this part we are not.",
                    "label": 0
                },
                {
                    "sent": "To liberate the.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data in our system.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we had a box, I mean a black box.",
                    "label": 0
                },
                {
                    "sent": "Which was able to answer to the question if a sentence is actually English on it, it would be really great because it would mean this black box is able to capture, you know, the syntax, the semantics or grammar of the language.",
                    "label": 0
                },
                {
                    "sent": "So actually such, I mean some people try to build such black box.",
                    "label": 0
                },
                {
                    "sent": "And yeah, like you're sure Benji on original Ducharme in 2001.",
                    "label": 0
                },
                {
                    "sent": "Try to make what is called a language model.",
                    "label": 0
                },
                {
                    "sent": "Which was.",
                    "label": 0
                },
                {
                    "sent": "Predicting the probability of the next word in the sentence according to previous word in the sentence.",
                    "label": 1
                },
                {
                    "sent": "In our case, we think that you know dealing with qualities.",
                    "label": 0
                },
                {
                    "sent": "It's over complicated.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 1
                },
                {
                    "sent": "You know, normalization problems which are annoying and actually the entropy criterion they use tend to be determined mainly by the most frequent phrases so.",
                    "label": 1
                },
                {
                    "sent": "Instead, instead, we prefer like to use a ranking criterion.",
                    "label": 0
                },
                {
                    "sent": "So if I consider like network which is again going to be or window approach network.",
                    "label": 0
                },
                {
                    "sent": "This network is going to give me a score score which says if my sentence is English or not.",
                    "label": 0
                },
                {
                    "sent": "And I want the score of.",
                    "label": 0
                },
                {
                    "sent": "Window of text in some copies to be larger with a certain margin than any other score for the same window of text, but where I replaced the middle one by some random wall.",
                    "label": 0
                },
                {
                    "sent": "So basically the true piece of text coming from my copies is my positive example and my negative example is going to be this same window of text replaced in the middle by some Wonderwall.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm going to train this huge language model.",
                    "label": 0
                },
                {
                    "sent": "By stochastic gradient descent I sent.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to train it on a lot of data.",
                    "label": 0
                },
                {
                    "sent": "We actually first considered Wikipedia, which is which was about at the time, 600 million of words.",
                    "label": 1
                },
                {
                    "sent": "Then we like considered an additional data set, which is called Reuters RC, one which added like 200 million of examples.",
                    "label": 0
                },
                {
                    "sent": "It's a massive data set.",
                    "label": 0
                },
                {
                    "sent": "You cannot expect you know to do classical training validation scheme like you know you train.",
                    "label": 1
                },
                {
                    "sent": "You train on like a grid of parameters, and you're trying to find the right ones.",
                    "label": 0
                },
                {
                    "sent": "I mean, here, it's impossible.",
                    "label": 0
                },
                {
                    "sent": "It takes too much time, so instead we did like a bit like in biology.",
                    "label": 1
                },
                {
                    "sent": "You like train.",
                    "label": 1
                },
                {
                    "sent": "I mean you breed a couple of networks lines and you know every day you check or how is it going at you.",
                    "label": 1
                },
                {
                    "sent": "Try to make decision according to some validation set to like stubble line or create a new one.",
                    "label": 0
                },
                {
                    "sent": "And when we train our network like that, we train actually like the.",
                    "label": 0
                },
                {
                    "sent": "The network over Wikipedia during about four weeks.",
                    "label": 0
                },
                {
                    "sent": "With an increasing Detroit size and a lot of other things, and after four weeks we took the embeddings, it produced and we initialize the second language model that we trained, then over Wikipedia plus Reuters, and it was again three additional weeks of training.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after all this time we looked at the words embeddings and they look actually pretty great.",
                    "label": 0
                },
                {
                    "sent": "The networks seem to really capture automatically.",
                    "label": 0
                },
                {
                    "sent": "You know, some good semantic there, so we just like talk, this one.",
                    "label": 0
                },
                {
                    "sent": "Embeddings initialized networks for the task you know, like part of speech, drinking, name, OT organization, and semantic role labeling and just rain as usual with just by initializing by this embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Beta factors that many times or is the amount of computation per day training.",
                    "label": 0
                },
                {
                    "sent": "It's just domain.",
                    "label": 0
                },
                {
                    "sent": "Are they expensive?",
                    "label": 0
                },
                {
                    "sent": "I mean bye bye data.",
                    "label": 0
                },
                {
                    "sent": "It's not that expensive because it's basically like 2 vector to matrix.",
                    "label": 0
                },
                {
                    "sent": "Vector mitigation is small because it's so huge data set.",
                    "label": 0
                },
                {
                    "sent": "I mean it's almost 1 billion of examples.",
                    "label": 0
                },
                {
                    "sent": "You could actually apologize.",
                    "label": 0
                },
                {
                    "sent": "It usually did some, you know.",
                    "label": 0
                },
                {
                    "sent": "Try to parallelize it, but here we didn't want to mess with that.",
                    "label": 0
                },
                {
                    "sent": "Basically, just like let it rain on one computer, we have time on today.",
                    "label": 0
                },
                {
                    "sent": "The computers are actually faster than 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we applied this embeddings like to all natural language processing desk of interest.",
                    "label": 0
                },
                {
                    "sent": "As you can see it gives like 4 for all task it gives really like a huge boost.",
                    "label": 0
                },
                {
                    "sent": "We have an additional boost, mainly on name entity, organization.",
                    "label": 0
                },
                {
                    "sent": "For the second I mean the larger language model.",
                    "label": 0
                },
                {
                    "sent": "This is not really surprising because when we look at the world coverage we regain a lot with the second, the second language model with named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "So well, we thought we thought it was a good start.",
                    "label": 0
                },
                {
                    "sent": "But still, you know we got this season or presentation which like seems to work pretty well on all these tasks.",
                    "label": 0
                },
                {
                    "sent": "So now you know the next question is, why not training everything together and see what happens, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what we did.",
                    "label": 0
                },
                {
                    "sent": "It's cool.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Joint training.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just asking it has been used a lot in our networks.",
                    "label": 0
                },
                {
                    "sent": "And you can find a good value for networks in like Rich Caruana disease in 97.",
                    "label": 0
                },
                {
                    "sent": "So again, quite old stuff.",
                    "label": 0
                },
                {
                    "sent": "Basically what we are doing here is that we are considering, for example two tasks.",
                    "label": 0
                },
                {
                    "sent": "So yeah, all the networks for the two tasks and.",
                    "label": 0
                },
                {
                    "sent": "So these networks can receive, you know, different inputs if they like, but the parameters are going to be shared.",
                    "label": 0
                },
                {
                    "sent": "That is the bomb.",
                    "label": 0
                },
                {
                    "sent": "It or this look up tables are the same level parameters, so these look up tables and the bomb.",
                    "label": 0
                },
                {
                    "sent": "It also this first hidden layer are going to be the same as well.",
                    "label": 0
                },
                {
                    "sent": "The rest of the layers are like kept, you know independent, but the first layers are we shared so.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It rained the participants drinking on any altogether, and we had a look at what it.",
                    "label": 0
                },
                {
                    "sent": "What what did this and basically what it does is that it gives a kind of significant boost for chunking, and linguist would tell you that it's not surprising because part of speech is often used as a good feature for drinking.",
                    "label": 0
                },
                {
                    "sent": "For the other task, it doesn't change anything, basically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we have like quite reasonable results for all task, but of course you know there is always like.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The temptation we.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know we can always do like those of people you know.",
                    "label": 0
                },
                {
                    "sent": "Try to see if you know if we plug this part of speech into this trunking.",
                    "label": 0
                },
                {
                    "sent": "Does it help?",
                    "label": 0
                },
                {
                    "sent": "Or if we put the drinking into the semantic webbing that help things like that.",
                    "label": 0
                },
                {
                    "sent": "So that's what we did.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider actually like stemming as good feature of our part of speech.",
                    "label": 1
                },
                {
                    "sent": "It's often you know it's always to use actually in part of speech.",
                    "label": 0
                },
                {
                    "sent": "So here we use only like the two car, 2 less characters of each one as an additional feature.",
                    "label": 0
                },
                {
                    "sent": "We tried actually to use three or you know prefixes.",
                    "label": 0
                },
                {
                    "sent": "If it was also exist, but you did not help.",
                    "label": 1
                },
                {
                    "sent": "I mean it's mainly like the two characters which were important to less characters.",
                    "label": 0
                },
                {
                    "sent": "We consider actually gazetteers.",
                    "label": 0
                },
                {
                    "sent": "That is like a list of names.",
                    "label": 0
                },
                {
                    "sent": "It's kind of cheating.",
                    "label": 0
                },
                {
                    "sent": "I think that everybody does that, so we consider that foreign entity organization and there is like about eight dozen locations.",
                    "label": 0
                },
                {
                    "sent": "Person name organizations given actually by the kernel challenge itself, so you know everybody else used this dataset.",
                    "label": 0
                },
                {
                    "sent": "I mean, some use like.",
                    "label": 1
                },
                {
                    "sent": "The guys at gym but we stick to this simple one.",
                    "label": 0
                },
                {
                    "sent": "We also tried part of speech as a feature for drinking on the Monte organization.",
                    "label": 0
                },
                {
                    "sent": "And drinking as a feature for semantic role labeling.",
                    "label": 1
                },
                {
                    "sent": "Anna, you know?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results it basically gives like a significant boost for all tasks.",
                    "label": 0
                },
                {
                    "sent": "We are actually basically state of the art everywhere except for semantic role labeling, which is still not there.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it looks really good.",
                    "label": 0
                },
                {
                    "sent": "The results in Italy kids because the networks is still training.",
                    "label": 0
                },
                {
                    "sent": "It's the last result from this morning.",
                    "label": 0
                },
                {
                    "sent": "Might have some results in like next week, but so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this is an additional side because actually many people you know they don't trust non convex as you saw in the beginning of the talk so.",
                    "label": 0
                },
                {
                    "sent": "I wanted to know, you know, the violence we had in these networks.",
                    "label": 0
                },
                {
                    "sent": "So we train like 10 times each of the network for part of speech chunking on any other.",
                    "label": 0
                },
                {
                    "sent": "And I reported here like the worst performance for each of them and the best one as well as the mean.",
                    "label": 0
                },
                {
                    "sent": "And you can see there is a little bit of Mayans, but not that much.",
                    "label": 0
                },
                {
                    "sent": "So of course we could.",
                    "label": 0
                },
                {
                    "sent": "Then you know, try to average the 10 what networks we trained and things like that.",
                    "label": 0
                },
                {
                    "sent": "But that's not our point here.",
                    "label": 0
                },
                {
                    "sent": "So to reduce the variance in previous experiments, Yep.",
                    "label": 0
                },
                {
                    "sent": "Is there rain with their registration?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "It's a different organization.",
                    "label": 0
                },
                {
                    "sent": "But which part is useful?",
                    "label": 0
                },
                {
                    "sent": "Or both.",
                    "label": 0
                },
                {
                    "sent": "You know it's stochastic, so I mean the training is stochastic, so the ordering of the data it might be different and initialization is like random, so it might be different.",
                    "label": 0
                },
                {
                    "sent": "Always initialize randomly.",
                    "label": 0
                },
                {
                    "sent": "Randomly according to the fun in you know, it's like it's a classical initialization.",
                    "label": 0
                },
                {
                    "sent": "You just like to random numbers between I mean.",
                    "label": 0
                },
                {
                    "sent": "Touch that.",
                    "label": 0
                },
                {
                    "sent": "You you don't saturate too much, the hidden units basically.",
                    "label": 0
                },
                {
                    "sent": "So yeah, exactly centered at zero and the variance is about.",
                    "label": 0
                },
                {
                    "sent": "I mean the bit less than the.",
                    "label": 0
                },
                {
                    "sent": "What would make the hyperbolic tangent such rights?",
                    "label": 0
                },
                {
                    "sent": "So to reduce the variance in previous experiments, basically we use the same seed in all networks.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so is there any sexually or one less task which is like the semantical labeling where we are?",
                    "label": 0
                },
                {
                    "sent": "As I say not quite there yet.",
                    "label": 0
                },
                {
                    "sent": "And actually, people often say that passing is essential for.",
                    "label": 1
                },
                {
                    "sent": "For semantic or labeling there, there are even like papers written about it.",
                    "label": 1
                },
                {
                    "sent": "So on state of the art semantic here or labeling systems use actually several past reason.",
                    "label": 0
                },
                {
                    "sent": "One I'm considering here as a benchmark used like I think 5 or 6 pass trees.",
                    "label": 0
                },
                {
                    "sent": "So we decided.",
                    "label": 0
                },
                {
                    "sent": "I mean, why not?",
                    "label": 0
                },
                {
                    "sent": "You know, feeding laboratories to our network?",
                    "label": 1
                },
                {
                    "sent": "So to doing so, we consider passing as a bus trip here, or we consider the bottom tags produced by by this Part 3, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what we call the level deal.",
                    "label": 0
                },
                {
                    "sent": "The bottom tags actually correspond in fact to like chunk tags, except that what we call the bracketing.",
                    "label": 0
                },
                {
                    "sent": "So when the tax starts and when the tag ends might differ from the chunking.",
                    "label": 0
                },
                {
                    "sent": "Then basically we cut the last leaves of the bass string.",
                    "label": 0
                },
                {
                    "sent": "An we consider the next button tags as you know additional features and then we cut again and we consider again the bottom tags as additional features.",
                    "label": 0
                },
                {
                    "sent": "So we fed a pass or pass through like that by filling up to five levels of the past three.",
                    "label": 1
                },
                {
                    "sent": "And we consider a.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hence the benchmark system and I'm sure you have to know that the benchmark system use six part three and they they also like provide a result with only one path 3 which is about three from the.",
                    "label": 0
                },
                {
                    "sent": "Challenge.",
                    "label": 0
                },
                {
                    "sent": "And you can see, well, we're already pretty pretty much there with.",
                    "label": 0
                },
                {
                    "sent": "With like the level zero, I mean actually with that ranking we are pretty much there.",
                    "label": 0
                },
                {
                    "sent": "But with levels or we we are going a bit over that and with more levels we get a little bit, but not that much.",
                    "label": 0
                },
                {
                    "sent": "So Indian.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We took, you know all these networks.",
                    "label": 0
                },
                {
                    "sent": "And we did like a simple standalone.",
                    "label": 0
                },
                {
                    "sent": "Implementations of all this.",
                    "label": 0
                },
                {
                    "sent": "In that Journal, which processing task?",
                    "label": 0
                },
                {
                    "sent": "So all these networks are basically mainly performing matrix vector multiplications, so we interface with BLAST.",
                    "label": 0
                },
                {
                    "sent": "So we get some speed.",
                    "label": 0
                },
                {
                    "sent": "And to summarize, or networks are using like lower case words.",
                    "label": 1
                },
                {
                    "sent": "Plus the capitalization feature part of speech uses suffixes, not prefixes suffixes.",
                    "label": 0
                },
                {
                    "sent": "Tracking users like part of speech tags name, entity organizer.",
                    "label": 1
                },
                {
                    "sent": "Use like that here, and the same optical labeler use the levels of the bus tree that we actually trained to predict by, you know.",
                    "label": 0
                },
                {
                    "sent": "Window Approach Network and actually the prediction was better than the chanak prediction.",
                    "label": 0
                },
                {
                    "sent": "Or given by the challenge.",
                    "label": 0
                },
                {
                    "sent": "Add A and then.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's actually going pretty fast.",
                    "label": 0
                },
                {
                    "sent": "We compared against existing system which are available so there are not many.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately we took here like the tutor and chance Ystem for part of speech.",
                    "label": 0
                },
                {
                    "sent": "And you can see that we are, I mean very fast compared to them.",
                    "label": 0
                },
                {
                    "sent": "Although I mean civil order of magnitude faster while we require much less memory, and it's basically the same for the semantic or label on the cool thing about that is, I mean it's very simple system compare.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To what they are, it's completely standalone and I'm going to give you a demo so this system is going to be available soon.",
                    "label": 1
                },
                {
                    "sent": "Actually mid January.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in playing with it, you just send me an email.",
                    "label": 0
                },
                {
                    "sent": "So I didn't lie to you, it's like really like few lines of see.",
                    "label": 0
                },
                {
                    "sent": "And the computation is pretty easy because it's like completely, you know standalone.",
                    "label": 0
                },
                {
                    "sent": "Sure, I'm going to use Blas just for fun.",
                    "label": 0
                },
                {
                    "sent": "So it's also pretty fast to compile, basically done, and I'm going to feed like kind of sentence.",
                    "label": 0
                },
                {
                    "sent": "And I have the tax right away so.",
                    "label": 0
                },
                {
                    "sent": "You can actually try to compare with existing systems and you will see that it's really like day or night.",
                    "label": 0
                },
                {
                    "sent": "So Indian I.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I presented you like a kind of all purpose non network architecture for natural language processing task which involves tagging.",
                    "label": 1
                },
                {
                    "sent": "It limits the task specific injuring by instead relying on like very large unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And of course we do not plan to stop here now.",
                    "label": 1
                },
                {
                    "sent": "We have like this hidden or presentation, but which are, you know, only limited towards.",
                    "label": 0
                },
                {
                    "sent": "Basically we are now working on.",
                    "label": 0
                },
                {
                    "sent": "Hidden opposition which are actually representing civil wars, so it's really the next step.",
                    "label": 0
                },
                {
                    "sent": "So we received a bunch of critics on this this work, you know, for example, a common one is like why would I for get an LP expertise like for learning your neural network training skills.",
                    "label": 1
                },
                {
                    "sent": "Well, as we said, natural language processing goals are really not limited to existing in natural language processing task.",
                    "label": 0
                },
                {
                    "sent": "And if you stick to you know this always like finding new features.",
                    "label": 1
                },
                {
                    "sent": "It's maybe a problem in the future when natural language processing tasks become more and more complicated.",
                    "label": 0
                },
                {
                    "sent": "So we really think that task engineering excessive task engineering is really not desirable.",
                    "label": 1
                },
                {
                    "sent": "We also like receive like critics like you know why using neural networks?",
                    "label": 0
                },
                {
                    "sent": "Inoculated like 20 years ago.",
                    "label": 0
                },
                {
                    "sent": "Well, sorry, but you know it scales on massive datasets.",
                    "label": 0
                },
                {
                    "sent": "It's able to discover very, very nice hidden or presentation.",
                    "label": 0
                },
                {
                    "sent": "And on top of that it's like all technology which is actually most of the technology which existed here that we use here existed in 97.",
                    "label": 0
                },
                {
                    "sent": "So it's really like well proven technology.",
                    "label": 0
                },
                {
                    "sent": "And the bottom line is, if we add started, you know, in 97 to train those networks with like vintage computers where we basically would be about finishing two days.",
                    "label": 0
                },
                {
                    "sent": "So it was really the right time to start to train it, so that's it.",
                    "label": 0
                },
                {
                    "sent": "So ice.",
                    "label": 0
                }
            ]
        }
    }
}