{
    "id": "ilmnnz456yjd3oa5kbmjrody7cks5zfb",
    "title": "Data-dependent Prior PAC-Bayes Bounds: Empirical Study",
    "info": {
        "author": [
            "Emilio Parrado-Hernandez, Carlos III University of Madrid"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_parrado_hernandez_ddp/",
    "segmentation": [
        [
            "So John work with John Amiranda blossom sailing to complete this social network of researchers, mistakes and.",
            "This is the outline."
        ],
        [
            "Describe briefly experiments I conducted because our feeling was to get some insights about in which type of problems these bonds will work and what can we expect.",
            "About this this bounds from armor practitioner point of view.",
            "Then I will talk about the two algorithms that we are bonding, which is the regular VM and another version of VM that we call it a priority VM, which is which attempts to minimize explicitly explicitly the PAC Bayes bound.",
            "Then this next section is empirical results.",
            "Basically, how tight, how tight are the bounds we're achieving and how good are the models that we select because.",
            "We can also use this bounce to estimated to choose a good setting of C parameter for the SVM and also for the kernel parameter."
        ],
        [
            "So we will just five UCI datasets and will split in 80% of the data set for training at the 20% for testing, and some of the.",
            "Square we will learn a prior from part of the data will take half of this training data to run the prior.",
            "Also, we have agreed a 5 by 5 to select the parameter C for the SVM and the spread parameter of the Goshen kernel.",
            "And results I will show our bounce on the Gibbs classifier and then the error on this separate test set."
        ],
        [
            "So we are using the sum of data to construct the prior and we will just got some distributions 'cause they use in the Calverton Sissy to compute.",
            "So we need to fix the mean and covariance matrix of this priors an some of the presenters said before the common setting in the packet based bound is to use a zero mean Gaussian with an identity covariance matrix."
        ],
        [
            "So we've followed for strategies to select this meme of discussion distribution where I've called prior SVM is to use half of the data to learn this SVM, and just as the main distribution.",
            "Then another setting we call multiple prior is to use this prayer direction.",
            "Learn with half of the data and fix also priority number J of scalings.",
            "We will just end scalings an their place spherical Gaussian salon.",
            "This direction given by W the expectation prior is both silly and presented in the previous talk, and then something John also introducing his talk, which we call a mini SVM, is to speak.",
            "Reduce number of samples I will be using for 1664 and 128 samples and learn SVM with these sets.",
            "On average this SVM plan with this produce set of samples.",
            "Anne for cover."
        ],
        [
            "Matrix.",
            "In this we will just just do a strategist either take an identity matrix or what I call to prayer and is to stretch the covariance matrix along this prior.",
            "Learn with some of the."
        ],
        [
            "The training data.",
            "The algorithms said the first one is the common SVM and this Eater prior SVM starts from this WR that we learn with this half of the data and we want to run part of the posterior classifier.",
            "Minimizing this functional and we use this as a condition, so we want basically the prior to be close to the this prestigious part to be close to the prior one because.",
            "This has to do with the collisions between the two distributions, so our intuition is that if this is small we will get tide value of the pound.",
            "For this classifier, and also we hope that the test error that we get with this it appears VM is compara bulto the regular SPM."
        ],
        [
            "So this Saturday Feb 5 datasets are we reducing taking from the UCI reports three and what I wanted to show in this slide is how difficult it is to classify these two datasets.",
            "This five datasets, the most difficult one is this by moderate is good with the test error when we have tuned parameters using 10 fold cross validation is around 25% and this is a manuscript digits and this way from.",
            "Issue somehow also with the and this is spam filtering, so we have like more or less complicated problems an issue once."
        ],
        [
            "So this is the tightness of the bounds, how?",
            "And the amounts on the Gibbs classifier, so we see that the first one is the original, with Marina back based using prior with zero mean, and I don't take offense metrics.",
            "This one is when we learn the proper direction with half of the data and just 10 scalings.",
            "Here is when we stretch this covariance of the prior along the direction of this problem with half the data.",
            "This is the two priors that silly and just presented, and these are the results for this mini SVM prior when we use VM made with for sample 60 samples 64 and 128 samples so that its value of the one we get them when we have this prior land.",
            "Learn with half the data."
        ],
        [
            "If it's television, an SVM will try to bound the performance of the interpreters VM.",
            "In this easiest problem, we get significant reductions in the value of the bond of the biggest Gibbs classifier.",
            "Getting here results around 50.5.",
            "But with this other players that are learned not using half of the data, we get much higher values of the bound.",
            "So I wanted to check.",
            "Some explanation about how are we getting this pretty different results and I try to."
        ],
        [
            "To compute more less, they'll give an idea about the quality of the priors, calculating the error of their classifier that I said.",
            "So basically when we use half of the data to learn this prior, we get very accurate ones, and with this all the strategies that players have not so good.",
            "So what we think is actually happening is that if the price is good, it's worth.",
            "Just in that in the back base bound and we can get that results, but when there's not so good about just gets lost.",
            "And also some."
        ],
        [
            "Result about.",
            "If it's a good idea to use the bound to select the value of C and the value of the kernel parameter mean that we would explore this grid using the bound and take these two values where the bound is minimized and I'm comparing with two fold cross validation because in most of these settings.",
            "Or basically this so we need to try and two SVM's want to learn the prior and I want to fit the posterior part so that the complexity of the trend is more or less balance, and I think the results are more or less compatible in terms of the model selection.",
            "Is the different settings."
        ],
        [
            "And this is for the prior where the results have not so good as with the SVM's.",
            "Are they still compatible?",
            "But so here we can get tighter predictions using the bound, but the test error when we just about to do this model selection is not so good.",
            "With the SPMS."
        ],
        [
            "So just to wrap up.",
            "I've seen some results about strategies to select the prior for the PAC Bayes bound using some part of the data.",
            "I've seen my tuition that when we have a good prayer, we have a chance to get a really tight bound.",
            "And that we can get results compareable to use to focus validation to select the model?",
            "And.",
            "Also that in this other algorithm, the enterprise VM that we can get related predictions, but let's change of increasing the test error.",
            "That's it myself."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So John work with John Amiranda blossom sailing to complete this social network of researchers, mistakes and.",
                    "label": 0
                },
                {
                    "sent": "This is the outline.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe briefly experiments I conducted because our feeling was to get some insights about in which type of problems these bonds will work and what can we expect.",
                    "label": 0
                },
                {
                    "sent": "About this this bounds from armor practitioner point of view.",
                    "label": 0
                },
                {
                    "sent": "Then I will talk about the two algorithms that we are bonding, which is the regular VM and another version of VM that we call it a priority VM, which is which attempts to minimize explicitly explicitly the PAC Bayes bound.",
                    "label": 0
                },
                {
                    "sent": "Then this next section is empirical results.",
                    "label": 0
                },
                {
                    "sent": "Basically, how tight, how tight are the bounds we're achieving and how good are the models that we select because.",
                    "label": 0
                },
                {
                    "sent": "We can also use this bounce to estimated to choose a good setting of C parameter for the SVM and also for the kernel parameter.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will just five UCI datasets and will split in 80% of the data set for training at the 20% for testing, and some of the.",
                    "label": 0
                },
                {
                    "sent": "Square we will learn a prior from part of the data will take half of this training data to run the prior.",
                    "label": 0
                },
                {
                    "sent": "Also, we have agreed a 5 by 5 to select the parameter C for the SVM and the spread parameter of the Goshen kernel.",
                    "label": 0
                },
                {
                    "sent": "And results I will show our bounce on the Gibbs classifier and then the error on this separate test set.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are using the sum of data to construct the prior and we will just got some distributions 'cause they use in the Calverton Sissy to compute.",
                    "label": 0
                },
                {
                    "sent": "So we need to fix the mean and covariance matrix of this priors an some of the presenters said before the common setting in the packet based bound is to use a zero mean Gaussian with an identity covariance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've followed for strategies to select this meme of discussion distribution where I've called prior SVM is to use half of the data to learn this SVM, and just as the main distribution.",
                    "label": 1
                },
                {
                    "sent": "Then another setting we call multiple prior is to use this prayer direction.",
                    "label": 0
                },
                {
                    "sent": "Learn with half of the data and fix also priority number J of scalings.",
                    "label": 1
                },
                {
                    "sent": "We will just end scalings an their place spherical Gaussian salon.",
                    "label": 0
                },
                {
                    "sent": "This direction given by W the expectation prior is both silly and presented in the previous talk, and then something John also introducing his talk, which we call a mini SVM, is to speak.",
                    "label": 0
                },
                {
                    "sent": "Reduce number of samples I will be using for 1664 and 128 samples and learn SVM with these sets.",
                    "label": 0
                },
                {
                    "sent": "On average this SVM plan with this produce set of samples.",
                    "label": 0
                },
                {
                    "sent": "Anne for cover.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "In this we will just just do a strategist either take an identity matrix or what I call to prayer and is to stretch the covariance matrix along this prior.",
                    "label": 1
                },
                {
                    "sent": "Learn with some of the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The training data.",
                    "label": 0
                },
                {
                    "sent": "The algorithms said the first one is the common SVM and this Eater prior SVM starts from this WR that we learn with this half of the data and we want to run part of the posterior classifier.",
                    "label": 0
                },
                {
                    "sent": "Minimizing this functional and we use this as a condition, so we want basically the prior to be close to the this prestigious part to be close to the prior one because.",
                    "label": 0
                },
                {
                    "sent": "This has to do with the collisions between the two distributions, so our intuition is that if this is small we will get tide value of the pound.",
                    "label": 0
                },
                {
                    "sent": "For this classifier, and also we hope that the test error that we get with this it appears VM is compara bulto the regular SPM.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this Saturday Feb 5 datasets are we reducing taking from the UCI reports three and what I wanted to show in this slide is how difficult it is to classify these two datasets.",
                    "label": 0
                },
                {
                    "sent": "This five datasets, the most difficult one is this by moderate is good with the test error when we have tuned parameters using 10 fold cross validation is around 25% and this is a manuscript digits and this way from.",
                    "label": 0
                },
                {
                    "sent": "Issue somehow also with the and this is spam filtering, so we have like more or less complicated problems an issue once.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the tightness of the bounds, how?",
                    "label": 0
                },
                {
                    "sent": "And the amounts on the Gibbs classifier, so we see that the first one is the original, with Marina back based using prior with zero mean, and I don't take offense metrics.",
                    "label": 0
                },
                {
                    "sent": "This one is when we learn the proper direction with half of the data and just 10 scalings.",
                    "label": 0
                },
                {
                    "sent": "Here is when we stretch this covariance of the prior along the direction of this problem with half the data.",
                    "label": 0
                },
                {
                    "sent": "This is the two priors that silly and just presented, and these are the results for this mini SVM prior when we use VM made with for sample 60 samples 64 and 128 samples so that its value of the one we get them when we have this prior land.",
                    "label": 0
                },
                {
                    "sent": "Learn with half the data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it's television, an SVM will try to bound the performance of the interpreters VM.",
                    "label": 0
                },
                {
                    "sent": "In this easiest problem, we get significant reductions in the value of the bond of the biggest Gibbs classifier.",
                    "label": 0
                },
                {
                    "sent": "Getting here results around 50.5.",
                    "label": 0
                },
                {
                    "sent": "But with this other players that are learned not using half of the data, we get much higher values of the bound.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to check.",
                    "label": 0
                },
                {
                    "sent": "Some explanation about how are we getting this pretty different results and I try to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compute more less, they'll give an idea about the quality of the priors, calculating the error of their classifier that I said.",
                    "label": 0
                },
                {
                    "sent": "So basically when we use half of the data to learn this prior, we get very accurate ones, and with this all the strategies that players have not so good.",
                    "label": 0
                },
                {
                    "sent": "So what we think is actually happening is that if the price is good, it's worth.",
                    "label": 0
                },
                {
                    "sent": "Just in that in the back base bound and we can get that results, but when there's not so good about just gets lost.",
                    "label": 0
                },
                {
                    "sent": "And also some.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result about.",
                    "label": 0
                },
                {
                    "sent": "If it's a good idea to use the bound to select the value of C and the value of the kernel parameter mean that we would explore this grid using the bound and take these two values where the bound is minimized and I'm comparing with two fold cross validation because in most of these settings.",
                    "label": 0
                },
                {
                    "sent": "Or basically this so we need to try and two SVM's want to learn the prior and I want to fit the posterior part so that the complexity of the trend is more or less balance, and I think the results are more or less compatible in terms of the model selection.",
                    "label": 0
                },
                {
                    "sent": "Is the different settings.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is for the prior where the results have not so good as with the SVM's.",
                    "label": 0
                },
                {
                    "sent": "Are they still compatible?",
                    "label": 0
                },
                {
                    "sent": "But so here we can get tighter predictions using the bound, but the test error when we just about to do this model selection is not so good.",
                    "label": 0
                },
                {
                    "sent": "With the SPMS.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "I've seen some results about strategies to select the prior for the PAC Bayes bound using some part of the data.",
                    "label": 1
                },
                {
                    "sent": "I've seen my tuition that when we have a good prayer, we have a chance to get a really tight bound.",
                    "label": 0
                },
                {
                    "sent": "And that we can get results compareable to use to focus validation to select the model?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Also that in this other algorithm, the enterprise VM that we can get related predictions, but let's change of increasing the test error.",
                    "label": 0
                },
                {
                    "sent": "That's it myself.",
                    "label": 0
                }
            ]
        }
    }
}