{
    "id": "jaogt7my4wubopvzzihy2jb5cshbvary",
    "title": "Large-scale RLSC Learning Without Agony",
    "info": {
        "author": [
            "Wenye Li, Department of Computer Science and Engineering, Chinese University of Hong Kong"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Mathematics->Operations Research"
        ]
    },
    "url": "http://videolectures.net/icml07_wenye_lsrl/",
    "segmentation": [
        [
            "And our next speaker is going to be 1 Lee from the Chinese University of Hong Kong talking about large scale or else learning without agony.",
            "Everyone thanks for attending this presentation.",
            "Obviously from the Chinese University Hong Kong.",
            "This is joint work with Professor Kim Holien.",
            "Constantly on this represents our some work in applying the regularised least square classification models in solving classification problems."
        ],
        [
            "So we first give up brief introduction what we're cooking with the supervised learning problem, well associated with a number of training data which is specified by the input X and the output Y.",
            "And here we hope to find the function.",
            "We just specify the relationship between X&Y plus some noise white noise.",
            "So for high dimensional problems efficient approach is based either from wapnick classical theory on statistics, meaning or.",
            "Apologize idea of regularize learning.",
            "We came to this framework.",
            "The first is we define.",
            "We define a space of functions.",
            "We restrict our discussion to be within this function spaces.",
            "So this space is it's given by by linear expressions.",
            "But completion of linear impressions of kernel function kernel function example of kernel function is a Gaussian kernel such as a Gaussian kernel.",
            "So in this space we try to find function, the inner product is given is defined by.",
            "The kernel is typical way and in this function space we have to find the function F to minimize the regularizer.",
            "Minimalization problem, so here we have two parts.",
            "The first represents empirical error between the training and between the input and output.",
            "This part, this part is a measure of complexity of F, and gamma is a positive number.",
            "So basically we hope to find.",
            "Simple model that has more empirical error and we using this model with the why is specified by minus one or positive one and use for classification.",
            "We get it.",
            "It is called the so-called regularised least grain classification problem."
        ],
        [
            "Model.",
            "So because as we just said, it's the space we studied it, key generally has.",
            "The HK General has infinite dimensions.",
            "If we find some function in this space, we need to specify it means we need to specify an infinite number of parameters.",
            "How can we check how can we use such a model?",
            "Unfortunately, due to the cumulative.",
            "Presenter theorem either in 1971.",
            "Eight tells us that in using studying this regularizer minimization model, only a finite number of parameters are not zero, and all the other parameters will be 0 and thus make things tractable and but but this problem this theorem only tells us the form of the solution and how to get the values of C becomes our problem.",
            "The basic idea is to substitute this form back into this minimization problem.",
            "And after serving television we get through this problem and this is a positive definite linear equation.",
            "Here A is given by kernel matrix K plus some some regularization terms along the diagonal."
        ],
        [
            "So basically for this linear system is for small to medium sized problems we can use very fish in the servers.",
            "In fact we just apply the direct solvers such as elimination, but in fact because it is positive definite distance system, we generally use choice effect realization.",
            "It is very simple and efficient to use, but the problem is that all these direct solvers are almost time convexity almost.",
            "Om to power three and the space complexity is OM Square.",
            "How to solve it for large scale problems this becomes a major concern of our in our usage of IRC MoD."
        ],
        [
            "Ross Social communities problem.",
            "Previous work comes in two lines.",
            "The first is is some approximation methods.",
            "That is that instead of using all the training data, we tried to use some, we try to select a subset of the data instead of using all the other data.",
            "So this is some low rank approximation methods."
        ],
        [
            "But we are interested in this deterministic approach to use all the training data to find some solution an if the kernel is a linear kernel, dimensionality of data is not is not large.",
            "We can use this.",
            "Sherman Morrison Woodbury formula to calculate the inverse of a.",
            "But for general problems it does not work."
        ],
        [
            "So to to achieve a deterministic approach for general problems we need to find iterative solvers.",
            "Basically for iterative solvers to elements are vital.",
            "The iterative solvers are trying to find a solution by successive successively applies matrix vector multiplication until until modify the solution until the convergence.",
            "So two elements is that if we can improve the metrics.",
            "Vector multiplication speed?",
            "Surely we can improve the speed of of these iterative solvers.",
            "In fact, there is another element that is important.",
            "It is iterative scheme because we have different iterative solvers.",
            "So which iterative solver should we use?",
            "Generally, people prefer to use a computer gradient method to solve such a system, and many literatures in this along this line suggested to use.",
            "Gradient."
        ],
        [
            "So a brief introduction of correcting the color gradient method instead of tries to solve a sequel to wear directly.",
            "It studies minimization of quadratic objective function and it is equivalent to the to the solution of equals 12 in the algorithm converges.",
            "I.",
            "And that coupled with some incomplete trusted factorization as a pretty conditioner.",
            "The CG method is in fact the industry standard in solving such sparse positive definite linear systems."
        ],
        [
            "But the problem is that in machine learning our econometrics are not sparse.",
            "If we use a gotten kernel, no element in the matrix will be 0, so.",
            "We are facing with this system.",
            "I.",
            "So this is this is also on our motivation.",
            "CD is best suited for non sparse smart system.",
            "But for Ford in systems, where will it because for this system it is really difficult for us to find a good preconditioner.",
            "So if we apply the CG method naively, will it still gives the best performance so far?",
            "So this is our major concern.",
            "So basically our work comprises of two parts.",
            "The first part is empirical study.",
            "We will show that maybe some other method, some other iterative method work better than CG on this specific task.",
            "The other part is that we try to explain explain this algorithm this method.",
            "As a in fact, what we found is is a very."
        ],
        [
            "Asian of course.",
            "Title method in our task.",
            "In our experiments, the ghost item.",
            "This is a variant of both sides of method block version of Poseidon method and we found it works better in our applications and the second part of our work is that this is.",
            "Although this is existing algorithm, people generally studied its convergence properties by the analysis of spectral radius analysis.",
            "So, but in our work we will give another derivation of this algorithm.",
            "That might be that might not be quite well known.",
            "So with this derivation we will show which important factor is important to the convergence of this this Alpha.",
            "So here is some outlines.",
            "This algorithm not forget about the details with after the derivation this problem becomes evident.",
            "So you need not try to remember.",
            "I remember the details to memorize the details we want to hope to say that the major computation of this algorithm.",
            "Can be paralyzed."
        ],
        [
            "OK, so our basic idea after duration comes from the following.",
            "Instead of solving the matrix equation, is equals two, I will change it to a different between equivalent problem as the alternating projections in reproducing kernel Hilbert space.",
            "Now let's have the first observation.",
            "This observation can be easily to make for every positive definite matrix A.",
            "We can find some some kernel, some data X is an M by N matrix, so we can find some data that exists, some data in some space and some kernel defend on this space which satisfies this property.",
            "So this can be easily made can easily observed.",
            "So this is the first observation.",
            "Now we define we define.",
            "OK, OK chess.",
            "By associating with the kernel K and the data at the data set X in the usual way.",
            "So here this HK is different from what we discussed in the introduction part.",
            "So it is a finite dimension space.",
            "Oh, now we can see that finding a solution to the problem of EC equals two Y is equivalent to finding a solution in F in each case such that FX G = 2 YG for all the XG where G is from one to M. If we define the things the K in this way, it came this way, then we transform the problem of finding a C = 2 Y finding C to a problem of finding F in each key satisfies this property.",
            "Now we also divide because we hope to give divide and conquer approach.",
            "We could to divide the whole system that is also about some smaller systems.",
            "So we give first divide this X this X into several different subsets and accordingly with this we define the subspaces of HK that is associated with each XI.",
            "And the corresponding.",
            "We also define interpolation operator.",
            "In this way, now the key observation to our work is that.",
            "This observation, this interpolation operator in fact defense also garner production from each keto hi.",
            "So.",
            "Now it's PII is defined in this way.",
            "OK, we define it as an interpolation operator, borrowed term from RBF interpolation."
        ],
        [
            "Looking observation is that is made in approximation theory in some work says that this this interpolation operator in fact defensin orthogonal projection.",
            "OK, now with this.",
            "With this observation we can identify we can give a method in solving this linear equations."
        ],
        [
            "Because now we need to spend 1 slide to review the organ.",
            "Alternating projections in a general Hilbert space.",
            "The basic idea is the following.",
            "We have a general here over the space we have some subspaces of general of H. Now we have an arbitrary point.",
            "FF is the function in it.",
            "Now we have to we have to find the projection of F onto the intersection between each one and H2.",
            "How to get this intersection?",
            "When is that we project F onto each one and then we project P1F onto each tool and and successively we project.",
            "The path successively until and in the end we will get to the intersection between each one H2, so this is following mass ordinating projection.",
            "The convergence speed, uh visits alternating projection is at least lean here, but it is generally believed to be a pessimistic estimation.",
            "And in practice, it generally expects exibits quicker convergence speed than this linear."
        ],
        [
            "Nation.",
            "Now we can achieve the following domain decomposition approach in solving a C = 2 Y.",
            "In fact, this this approach is equivalent to the what we just said, the the variant of both sides or methods.",
            "The block version of measuring, because we hope to find F what F is, we have changed the problem.",
            "Solving AC equals 28 to find function F such that FX G = 2 YG for all for all XG.",
            "Now this is our requirement of F. With this requirement we first support, although we do not know what F is.",
            "We know its requirement is constrained.",
            "Now we first project of F onto this subspace.",
            "In fact this we will get P1F because if we project onto PH1 then this part will be P1F right?",
            "So it is equivalent set.",
            "We will get P1F.",
            "This is orthogonal projection.",
            "Of F onto H1 compliment.",
            "So how to compute P1F not remember."
        ],
        [
            "The PDF is defined here.",
            "This is an interpolation operation which has some requirement.",
            "On TV on the production.",
            "And this production is in fact defense."
        ],
        [
            "So gonna projection.",
            "So with this requirement we can compute.",
            "We can compute the exact form of P1.",
            "With the computers exact form of P1 and but with the information with the requirement on F and exact form on P1, we can compute the requirement on this Q1F if we know Q1F then the problem is solved because PF plus QF is equal to F. So but unfortunately with the observation of F and the exact form of PDF, we can only compute the.",
            "Requirement on Q1F, but similarly we can also project QF onto this subspace and we'll get the exact form on this part and the requirement on this part.",
            "And successively we can project continuously until we will reach the intersection between each one and H2.",
            "So in our case this is a zero function.",
            "And correspondingly, we can sum up all these projections, for example P1.",
            "This part, this part, this part, this part, this part, this part, and we will recover F. So this presents another, so we will get to a domain decomposition approach.",
            "But how to calculate P1F?",
            "It only involves to solve a small scale linear system.",
            "So it is tractable because we have divided systems into different small systems, we only need to solve a small system to get these P1F OK.",
            "Similarly for P2P MF.",
            "OK.",
            "So this is another derivation of the Gauss Seidel method.",
            "Of the of some algorithm, there's a variation of the Gulf side or method, and we generally in our work we suggest to use this to try this work.",
            "At least try this algorithm to solve the positive definite.",
            "Then systems linear systems in machine learning.",
            "No.",
            "Why we do we give this analysis?",
            "Because cosine method is well known to be convergent for these systems.",
            "Why do we can give this analysis?",
            "Cause from this example you can find that which is the most important factor in affecting the convergence speed of this Alpha.",
            "Obviously it is the angles between each one and H2.",
            "If each one H2 hours ago, now we only use two projections.",
            "Project here project.",
            "Here we will reach the the intersection so big cause we can divide each K into H1 and H2 according to our well.",
            "That means we can control how we can divide this.",
            "Divide the whole system into different subsystems so we can.",
            "Maybe we can try to maximize the angles between each one.",
            "Which two?",
            "We perform the division.",
            "In fact we can also do this."
        ],
        [
            "The next is some experiments.",
            "We use a test categorization problems as in test from the CMU test.",
            "Many group the three datasets, 20 newsgroups, 37 sectors and web data set.",
            "So the system the size of the system is around for this problem, it is 19,000, this is.",
            "1000 and this is over 4000.",
            "We use both linear kernel, Gaussian kernel in the test and all the necessary parameters.",
            "It is used in the experiment are selected by a cross validation and the right hand of the equation.",
            "Y equals the levels of data.",
            "And we found some better results of the good side or method over over."
        ],
        [
            "The city method.",
            "The black version of signed a measure, so we plot the CG in a blue color.",
            "Because the city is a direct objective of CD is different, so this is the number of iterations.",
            "This is the relative residuals.",
            "After each iteration we can see that CG might give some.",
            "You met regarded strange behavior of CG, but in fact it is so.",
            "Well, for cost for the Ghost Rider method it gives much smaller performance over the CG.",
            "So our major argument is that if we hope to use some some fewer iterations in getting such getting solution, so such such more traditional methods such as Ghost Rider method might be tried.",
            "In fact, we can also interleaved use CG and got side method, forgive them and took another.",
            "Paraments, but in our paper we may argue that CD or naive application of CG method might not be the best choice."
        ],
        [
            "So for the conclusion, we summarize our work.",
            "So first part is empirical study.",
            "We will show that some good side of maybe such type of method.",
            "Such old method can be used for such problems and the 2nd is that we try to analyze this to give analysis and might be new to that might not be well known and analyzing the important factors that affect convergence speed and the third is that.",
            "Include some some future work.",
            "In fact we keep to some results of conjugate gradient kernel config gradient method which is presented in this year, but we haven't give comparisons with this so far, and in fact we are also working on some.",
            "Some accelerations of the cosine method in applying this problem.",
            "Hopefully the results will be published in the future."
        ],
        [
            "Thanks.",
            "Questions.",
            "Soup to a how iframes incorrectly?",
            "You're solving quadratic objective without constraints.",
            "Adapt to these methods when you do have constraints such as we have many problems.",
            "In fact, for this problem, having considered the extension of with constraints.",
            "Until now I we haven't considered it yet.",
            "Any further questions?",
            "OK, so thank you speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our next speaker is going to be 1 Lee from the Chinese University of Hong Kong talking about large scale or else learning without agony.",
                    "label": 1
                },
                {
                    "sent": "Everyone thanks for attending this presentation.",
                    "label": 0
                },
                {
                    "sent": "Obviously from the Chinese University Hong Kong.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Professor Kim Holien.",
                    "label": 0
                },
                {
                    "sent": "Constantly on this represents our some work in applying the regularised least square classification models in solving classification problems.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we first give up brief introduction what we're cooking with the supervised learning problem, well associated with a number of training data which is specified by the input X and the output Y.",
                    "label": 0
                },
                {
                    "sent": "And here we hope to find the function.",
                    "label": 0
                },
                {
                    "sent": "We just specify the relationship between X&Y plus some noise white noise.",
                    "label": 0
                },
                {
                    "sent": "So for high dimensional problems efficient approach is based either from wapnick classical theory on statistics, meaning or.",
                    "label": 0
                },
                {
                    "sent": "Apologize idea of regularize learning.",
                    "label": 0
                },
                {
                    "sent": "We came to this framework.",
                    "label": 0
                },
                {
                    "sent": "The first is we define.",
                    "label": 0
                },
                {
                    "sent": "We define a space of functions.",
                    "label": 0
                },
                {
                    "sent": "We restrict our discussion to be within this function spaces.",
                    "label": 0
                },
                {
                    "sent": "So this space is it's given by by linear expressions.",
                    "label": 0
                },
                {
                    "sent": "But completion of linear impressions of kernel function kernel function example of kernel function is a Gaussian kernel such as a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "So in this space we try to find function, the inner product is given is defined by.",
                    "label": 0
                },
                {
                    "sent": "The kernel is typical way and in this function space we have to find the function F to minimize the regularizer.",
                    "label": 1
                },
                {
                    "sent": "Minimalization problem, so here we have two parts.",
                    "label": 0
                },
                {
                    "sent": "The first represents empirical error between the training and between the input and output.",
                    "label": 0
                },
                {
                    "sent": "This part, this part is a measure of complexity of F, and gamma is a positive number.",
                    "label": 0
                },
                {
                    "sent": "So basically we hope to find.",
                    "label": 0
                },
                {
                    "sent": "Simple model that has more empirical error and we using this model with the why is specified by minus one or positive one and use for classification.",
                    "label": 1
                },
                {
                    "sent": "We get it.",
                    "label": 0
                },
                {
                    "sent": "It is called the so-called regularised least grain classification problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So because as we just said, it's the space we studied it, key generally has.",
                    "label": 0
                },
                {
                    "sent": "The HK General has infinite dimensions.",
                    "label": 1
                },
                {
                    "sent": "If we find some function in this space, we need to specify it means we need to specify an infinite number of parameters.",
                    "label": 0
                },
                {
                    "sent": "How can we check how can we use such a model?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, due to the cumulative.",
                    "label": 0
                },
                {
                    "sent": "Presenter theorem either in 1971.",
                    "label": 0
                },
                {
                    "sent": "Eight tells us that in using studying this regularizer minimization model, only a finite number of parameters are not zero, and all the other parameters will be 0 and thus make things tractable and but but this problem this theorem only tells us the form of the solution and how to get the values of C becomes our problem.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is to substitute this form back into this minimization problem.",
                    "label": 0
                },
                {
                    "sent": "And after serving television we get through this problem and this is a positive definite linear equation.",
                    "label": 1
                },
                {
                    "sent": "Here A is given by kernel matrix K plus some some regularization terms along the diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically for this linear system is for small to medium sized problems we can use very fish in the servers.",
                    "label": 0
                },
                {
                    "sent": "In fact we just apply the direct solvers such as elimination, but in fact because it is positive definite distance system, we generally use choice effect realization.",
                    "label": 0
                },
                {
                    "sent": "It is very simple and efficient to use, but the problem is that all these direct solvers are almost time convexity almost.",
                    "label": 0
                },
                {
                    "sent": "Om to power three and the space complexity is OM Square.",
                    "label": 0
                },
                {
                    "sent": "How to solve it for large scale problems this becomes a major concern of our in our usage of IRC MoD.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ross Social communities problem.",
                    "label": 0
                },
                {
                    "sent": "Previous work comes in two lines.",
                    "label": 1
                },
                {
                    "sent": "The first is is some approximation methods.",
                    "label": 0
                },
                {
                    "sent": "That is that instead of using all the training data, we tried to use some, we try to select a subset of the data instead of using all the other data.",
                    "label": 1
                },
                {
                    "sent": "So this is some low rank approximation methods.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we are interested in this deterministic approach to use all the training data to find some solution an if the kernel is a linear kernel, dimensionality of data is not is not large.",
                    "label": 1
                },
                {
                    "sent": "We can use this.",
                    "label": 0
                },
                {
                    "sent": "Sherman Morrison Woodbury formula to calculate the inverse of a.",
                    "label": 1
                },
                {
                    "sent": "But for general problems it does not work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to to achieve a deterministic approach for general problems we need to find iterative solvers.",
                    "label": 0
                },
                {
                    "sent": "Basically for iterative solvers to elements are vital.",
                    "label": 0
                },
                {
                    "sent": "The iterative solvers are trying to find a solution by successive successively applies matrix vector multiplication until until modify the solution until the convergence.",
                    "label": 1
                },
                {
                    "sent": "So two elements is that if we can improve the metrics.",
                    "label": 0
                },
                {
                    "sent": "Vector multiplication speed?",
                    "label": 0
                },
                {
                    "sent": "Surely we can improve the speed of of these iterative solvers.",
                    "label": 0
                },
                {
                    "sent": "In fact, there is another element that is important.",
                    "label": 1
                },
                {
                    "sent": "It is iterative scheme because we have different iterative solvers.",
                    "label": 0
                },
                {
                    "sent": "So which iterative solver should we use?",
                    "label": 0
                },
                {
                    "sent": "Generally, people prefer to use a computer gradient method to solve such a system, and many literatures in this along this line suggested to use.",
                    "label": 0
                },
                {
                    "sent": "Gradient.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a brief introduction of correcting the color gradient method instead of tries to solve a sequel to wear directly.",
                    "label": 1
                },
                {
                    "sent": "It studies minimization of quadratic objective function and it is equivalent to the to the solution of equals 12 in the algorithm converges.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 1
                },
                {
                    "sent": "And that coupled with some incomplete trusted factorization as a pretty conditioner.",
                    "label": 0
                },
                {
                    "sent": "The CG method is in fact the industry standard in solving such sparse positive definite linear systems.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the problem is that in machine learning our econometrics are not sparse.",
                    "label": 1
                },
                {
                    "sent": "If we use a gotten kernel, no element in the matrix will be 0, so.",
                    "label": 0
                },
                {
                    "sent": "We are facing with this system.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this is this is also on our motivation.",
                    "label": 0
                },
                {
                    "sent": "CD is best suited for non sparse smart system.",
                    "label": 1
                },
                {
                    "sent": "But for Ford in systems, where will it because for this system it is really difficult for us to find a good preconditioner.",
                    "label": 0
                },
                {
                    "sent": "So if we apply the CG method naively, will it still gives the best performance so far?",
                    "label": 0
                },
                {
                    "sent": "So this is our major concern.",
                    "label": 0
                },
                {
                    "sent": "So basically our work comprises of two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part is empirical study.",
                    "label": 0
                },
                {
                    "sent": "We will show that maybe some other method, some other iterative method work better than CG on this specific task.",
                    "label": 0
                },
                {
                    "sent": "The other part is that we try to explain explain this algorithm this method.",
                    "label": 0
                },
                {
                    "sent": "As a in fact, what we found is is a very.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian of course.",
                    "label": 0
                },
                {
                    "sent": "Title method in our task.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, the ghost item.",
                    "label": 0
                },
                {
                    "sent": "This is a variant of both sides of method block version of Poseidon method and we found it works better in our applications and the second part of our work is that this is.",
                    "label": 0
                },
                {
                    "sent": "Although this is existing algorithm, people generally studied its convergence properties by the analysis of spectral radius analysis.",
                    "label": 0
                },
                {
                    "sent": "So, but in our work we will give another derivation of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "That might be that might not be quite well known.",
                    "label": 0
                },
                {
                    "sent": "So with this derivation we will show which important factor is important to the convergence of this this Alpha.",
                    "label": 0
                },
                {
                    "sent": "So here is some outlines.",
                    "label": 0
                },
                {
                    "sent": "This algorithm not forget about the details with after the derivation this problem becomes evident.",
                    "label": 0
                },
                {
                    "sent": "So you need not try to remember.",
                    "label": 0
                },
                {
                    "sent": "I remember the details to memorize the details we want to hope to say that the major computation of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Can be paralyzed.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our basic idea after duration comes from the following.",
                    "label": 0
                },
                {
                    "sent": "Instead of solving the matrix equation, is equals two, I will change it to a different between equivalent problem as the alternating projections in reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "Now let's have the first observation.",
                    "label": 0
                },
                {
                    "sent": "This observation can be easily to make for every positive definite matrix A.",
                    "label": 1
                },
                {
                    "sent": "We can find some some kernel, some data X is an M by N matrix, so we can find some data that exists, some data in some space and some kernel defend on this space which satisfies this property.",
                    "label": 0
                },
                {
                    "sent": "So this can be easily made can easily observed.",
                    "label": 0
                },
                {
                    "sent": "So this is the first observation.",
                    "label": 0
                },
                {
                    "sent": "Now we define we define.",
                    "label": 0
                },
                {
                    "sent": "OK, OK chess.",
                    "label": 0
                },
                {
                    "sent": "By associating with the kernel K and the data at the data set X in the usual way.",
                    "label": 0
                },
                {
                    "sent": "So here this HK is different from what we discussed in the introduction part.",
                    "label": 0
                },
                {
                    "sent": "So it is a finite dimension space.",
                    "label": 0
                },
                {
                    "sent": "Oh, now we can see that finding a solution to the problem of EC equals two Y is equivalent to finding a solution in F in each case such that FX G = 2 YG for all the XG where G is from one to M. If we define the things the K in this way, it came this way, then we transform the problem of finding a C = 2 Y finding C to a problem of finding F in each key satisfies this property.",
                    "label": 1
                },
                {
                    "sent": "Now we also divide because we hope to give divide and conquer approach.",
                    "label": 1
                },
                {
                    "sent": "We could to divide the whole system that is also about some smaller systems.",
                    "label": 0
                },
                {
                    "sent": "So we give first divide this X this X into several different subsets and accordingly with this we define the subspaces of HK that is associated with each XI.",
                    "label": 0
                },
                {
                    "sent": "And the corresponding.",
                    "label": 0
                },
                {
                    "sent": "We also define interpolation operator.",
                    "label": 0
                },
                {
                    "sent": "In this way, now the key observation to our work is that.",
                    "label": 0
                },
                {
                    "sent": "This observation, this interpolation operator in fact defense also garner production from each keto hi.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now it's PII is defined in this way.",
                    "label": 0
                },
                {
                    "sent": "OK, we define it as an interpolation operator, borrowed term from RBF interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking observation is that is made in approximation theory in some work says that this this interpolation operator in fact defensin orthogonal projection.",
                    "label": 1
                },
                {
                    "sent": "OK, now with this.",
                    "label": 0
                },
                {
                    "sent": "With this observation we can identify we can give a method in solving this linear equations.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because now we need to spend 1 slide to review the organ.",
                    "label": 0
                },
                {
                    "sent": "Alternating projections in a general Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is the following.",
                    "label": 0
                },
                {
                    "sent": "We have a general here over the space we have some subspaces of general of H. Now we have an arbitrary point.",
                    "label": 0
                },
                {
                    "sent": "FF is the function in it.",
                    "label": 0
                },
                {
                    "sent": "Now we have to we have to find the projection of F onto the intersection between each one and H2.",
                    "label": 0
                },
                {
                    "sent": "How to get this intersection?",
                    "label": 0
                },
                {
                    "sent": "When is that we project F onto each one and then we project P1F onto each tool and and successively we project.",
                    "label": 0
                },
                {
                    "sent": "The path successively until and in the end we will get to the intersection between each one H2, so this is following mass ordinating projection.",
                    "label": 0
                },
                {
                    "sent": "The convergence speed, uh visits alternating projection is at least lean here, but it is generally believed to be a pessimistic estimation.",
                    "label": 1
                },
                {
                    "sent": "And in practice, it generally expects exibits quicker convergence speed than this linear.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "Now we can achieve the following domain decomposition approach in solving a C = 2 Y.",
                    "label": 0
                },
                {
                    "sent": "In fact, this this approach is equivalent to the what we just said, the the variant of both sides or methods.",
                    "label": 0
                },
                {
                    "sent": "The block version of measuring, because we hope to find F what F is, we have changed the problem.",
                    "label": 0
                },
                {
                    "sent": "Solving AC equals 28 to find function F such that FX G = 2 YG for all for all XG.",
                    "label": 0
                },
                {
                    "sent": "Now this is our requirement of F. With this requirement we first support, although we do not know what F is.",
                    "label": 0
                },
                {
                    "sent": "We know its requirement is constrained.",
                    "label": 0
                },
                {
                    "sent": "Now we first project of F onto this subspace.",
                    "label": 0
                },
                {
                    "sent": "In fact this we will get P1F because if we project onto PH1 then this part will be P1F right?",
                    "label": 0
                },
                {
                    "sent": "So it is equivalent set.",
                    "label": 0
                },
                {
                    "sent": "We will get P1F.",
                    "label": 0
                },
                {
                    "sent": "This is orthogonal projection.",
                    "label": 0
                },
                {
                    "sent": "Of F onto H1 compliment.",
                    "label": 0
                },
                {
                    "sent": "So how to compute P1F not remember.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The PDF is defined here.",
                    "label": 0
                },
                {
                    "sent": "This is an interpolation operation which has some requirement.",
                    "label": 0
                },
                {
                    "sent": "On TV on the production.",
                    "label": 0
                },
                {
                    "sent": "And this production is in fact defense.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So gonna projection.",
                    "label": 0
                },
                {
                    "sent": "So with this requirement we can compute.",
                    "label": 0
                },
                {
                    "sent": "We can compute the exact form of P1.",
                    "label": 0
                },
                {
                    "sent": "With the computers exact form of P1 and but with the information with the requirement on F and exact form on P1, we can compute the requirement on this Q1F if we know Q1F then the problem is solved because PF plus QF is equal to F. So but unfortunately with the observation of F and the exact form of PDF, we can only compute the.",
                    "label": 0
                },
                {
                    "sent": "Requirement on Q1F, but similarly we can also project QF onto this subspace and we'll get the exact form on this part and the requirement on this part.",
                    "label": 0
                },
                {
                    "sent": "And successively we can project continuously until we will reach the intersection between each one and H2.",
                    "label": 0
                },
                {
                    "sent": "So in our case this is a zero function.",
                    "label": 0
                },
                {
                    "sent": "And correspondingly, we can sum up all these projections, for example P1.",
                    "label": 0
                },
                {
                    "sent": "This part, this part, this part, this part, this part, this part, and we will recover F. So this presents another, so we will get to a domain decomposition approach.",
                    "label": 1
                },
                {
                    "sent": "But how to calculate P1F?",
                    "label": 0
                },
                {
                    "sent": "It only involves to solve a small scale linear system.",
                    "label": 0
                },
                {
                    "sent": "So it is tractable because we have divided systems into different small systems, we only need to solve a small system to get these P1F OK.",
                    "label": 0
                },
                {
                    "sent": "Similarly for P2P MF.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is another derivation of the Gauss Seidel method.",
                    "label": 0
                },
                {
                    "sent": "Of the of some algorithm, there's a variation of the Gulf side or method, and we generally in our work we suggest to use this to try this work.",
                    "label": 0
                },
                {
                    "sent": "At least try this algorithm to solve the positive definite.",
                    "label": 0
                },
                {
                    "sent": "Then systems linear systems in machine learning.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Why we do we give this analysis?",
                    "label": 0
                },
                {
                    "sent": "Because cosine method is well known to be convergent for these systems.",
                    "label": 0
                },
                {
                    "sent": "Why do we can give this analysis?",
                    "label": 0
                },
                {
                    "sent": "Cause from this example you can find that which is the most important factor in affecting the convergence speed of this Alpha.",
                    "label": 0
                },
                {
                    "sent": "Obviously it is the angles between each one and H2.",
                    "label": 0
                },
                {
                    "sent": "If each one H2 hours ago, now we only use two projections.",
                    "label": 0
                },
                {
                    "sent": "Project here project.",
                    "label": 0
                },
                {
                    "sent": "Here we will reach the the intersection so big cause we can divide each K into H1 and H2 according to our well.",
                    "label": 0
                },
                {
                    "sent": "That means we can control how we can divide this.",
                    "label": 0
                },
                {
                    "sent": "Divide the whole system into different subsystems so we can.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can try to maximize the angles between each one.",
                    "label": 0
                },
                {
                    "sent": "Which two?",
                    "label": 0
                },
                {
                    "sent": "We perform the division.",
                    "label": 0
                },
                {
                    "sent": "In fact we can also do this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next is some experiments.",
                    "label": 0
                },
                {
                    "sent": "We use a test categorization problems as in test from the CMU test.",
                    "label": 0
                },
                {
                    "sent": "Many group the three datasets, 20 newsgroups, 37 sectors and web data set.",
                    "label": 0
                },
                {
                    "sent": "So the system the size of the system is around for this problem, it is 19,000, this is.",
                    "label": 0
                },
                {
                    "sent": "1000 and this is over 4000.",
                    "label": 0
                },
                {
                    "sent": "We use both linear kernel, Gaussian kernel in the test and all the necessary parameters.",
                    "label": 0
                },
                {
                    "sent": "It is used in the experiment are selected by a cross validation and the right hand of the equation.",
                    "label": 0
                },
                {
                    "sent": "Y equals the levels of data.",
                    "label": 0
                },
                {
                    "sent": "And we found some better results of the good side or method over over.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The city method.",
                    "label": 0
                },
                {
                    "sent": "The black version of signed a measure, so we plot the CG in a blue color.",
                    "label": 0
                },
                {
                    "sent": "Because the city is a direct objective of CD is different, so this is the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "This is the relative residuals.",
                    "label": 0
                },
                {
                    "sent": "After each iteration we can see that CG might give some.",
                    "label": 0
                },
                {
                    "sent": "You met regarded strange behavior of CG, but in fact it is so.",
                    "label": 0
                },
                {
                    "sent": "Well, for cost for the Ghost Rider method it gives much smaller performance over the CG.",
                    "label": 0
                },
                {
                    "sent": "So our major argument is that if we hope to use some some fewer iterations in getting such getting solution, so such such more traditional methods such as Ghost Rider method might be tried.",
                    "label": 0
                },
                {
                    "sent": "In fact, we can also interleaved use CG and got side method, forgive them and took another.",
                    "label": 0
                },
                {
                    "sent": "Paraments, but in our paper we may argue that CD or naive application of CG method might not be the best choice.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the conclusion, we summarize our work.",
                    "label": 0
                },
                {
                    "sent": "So first part is empirical study.",
                    "label": 0
                },
                {
                    "sent": "We will show that some good side of maybe such type of method.",
                    "label": 0
                },
                {
                    "sent": "Such old method can be used for such problems and the 2nd is that we try to analyze this to give analysis and might be new to that might not be well known and analyzing the important factors that affect convergence speed and the third is that.",
                    "label": 0
                },
                {
                    "sent": "Include some some future work.",
                    "label": 0
                },
                {
                    "sent": "In fact we keep to some results of conjugate gradient kernel config gradient method which is presented in this year, but we haven't give comparisons with this so far, and in fact we are also working on some.",
                    "label": 0
                },
                {
                    "sent": "Some accelerations of the cosine method in applying this problem.",
                    "label": 0
                },
                {
                    "sent": "Hopefully the results will be published in the future.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Soup to a how iframes incorrectly?",
                    "label": 0
                },
                {
                    "sent": "You're solving quadratic objective without constraints.",
                    "label": 0
                },
                {
                    "sent": "Adapt to these methods when you do have constraints such as we have many problems.",
                    "label": 0
                },
                {
                    "sent": "In fact, for this problem, having considered the extension of with constraints.",
                    "label": 0
                },
                {
                    "sent": "Until now I we haven't considered it yet.",
                    "label": 0
                },
                {
                    "sent": "Any further questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you speaker again.",
                    "label": 0
                }
            ]
        }
    }
}