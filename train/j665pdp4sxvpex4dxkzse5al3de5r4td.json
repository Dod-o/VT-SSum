{
    "id": "j665pdp4sxvpex4dxkzse5al3de5r4td",
    "title": "Cluster stability and robust optimization",
    "info": {
        "author": [
            "Joachim M. Buhmann, Institute of Computational Science, ETH Zurich"
        ],
        "published": "July 28, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/srmc07_buhmann_joachim_csr/",
    "segmentation": [
        [
            "What I would like to do is to make the same disclaimer says Natty did I'm.",
            "I'm basically not presenting specific results.",
            "I'm presenting a strategy which I think might relate the issue.",
            "The relative specific issues of class stability to the much broader issue that we have an optimization problem and this might be typically a combinatorial optimization problem.",
            "It's defined by random variables because you have noisy measurements to specify your instance.",
            "And then you would like to know how precisely should you approximate such."
        ],
        [
            "Such such a problem.",
            "So we have seen this, you know, coming back to.",
            "Two clustering and spending a couple of moments on on what what I was setting for clustering is I think, and that's why I insisted on the separation between the modeling issue and the model order selection issue I I. I would like the the designer of a clustering algorithm to pay attention when when writing down the cost function to all aspects he or she would like to emphasize in data analysis.",
            "So the cost function has to capture it if you go.",
            "If you get lousy results from the clustering point of view, then you should reconsider your model attempt and I think it's very valuable.",
            "To separate the modeling issues from the from the sampling issues, from how well can you actually determine a solution as a learning generalization problem, and there is a modeling problem and we worked for quite some time on the modeling problem and have various solutions to that, but as far as is I would say when we used instability measures for determining the number of clusters, we never wanted to fix the modeling problem.",
            "We have anecdotal evidence that if you have a strong misspecification of your model with respect to your data.",
            "So, for example, if you think this should be connected components rather than these blobs, or here on the other side side, you want to save blobs and you have a situation like this, then you get in many cases you get instability becausw the underlying data distribution has some features.",
            "Which the algorithm cannot capture an by algorithm I mean the cost function including the optimization principle.",
            "So this since the algorithm most likely will not conspire with nature in a malicious way to pretend something is super stable and nevertheless you get a lousy clustering value, a cost value, you will see a little bit of instability.",
            "But I have strong doubts that you can solve the modeling problem.",
            "And maybe one has to we have to talk about this even further.",
            "But I I have strong doubts that you will find a metatheory for clustering which captures all the properties you would like to have for class string.",
            "And automatically removes the burden from the designer of a data analysis problem to specify models.",
            "So these automatic you know this machine which automatically spits out theories about your data and and then can sort out with a meta principle like learnability or stability.",
            "Which of these models are kosher?",
            "I have my doubts that this will work now to some degree the support vector machines try to intend that because.",
            "You you have this kernel, the kernel has many degrees of freedom.",
            "You go to very high dimensional space.",
            "It's basically unlearnable there and then you introduce the margin and you solve their trade off.",
            "But nevertheless I I think this is a dream which which which?",
            "Which I would would not subscribe to.",
            "This is the dream that I'm trying to promote or anything, but I'm saying is that look at this example, yes?",
            "I.",
            "If you just apply stability and you take a came in sideways and then it will be more stable on the left, picture that on there and it will be unstable on the right hand picture.",
            "No no, but I make an F fine transformation.",
            "I stretch it like this then it will be stable here too.",
            "Let's the problem.",
            "So so so.",
            "So K means gives you probably much higher costs here than here, even if you normalize it to the variance.",
            "Annual fix your scale so that you actually can compare apples with peers.",
            "But apples and oranges, but I you know that's the problem with all the negative results you can come up with as long.",
            "I'm only frightened if these negative results.",
            "It can attack.",
            "A meta principle like like like stability measurements when we believe it's the right model and nevertheless it will fail.",
            "That is a critical test.",
            "Now if the model is misspecified then then we have this problem that that that OK we get the wrong answer but we get the wrong answer because we didn't work hard enough on the modeling issue and we tried to put the burden on on sort of a meta frame theory.",
            "Reminds me of my supervisor and he told me that he never had a problem that if you work hard enough you can solve it.",
            "So I asked him what about vehicles and pee so he said, oh I didn't work hard enough.",
            "So you say it whenever well, Hilbert also had that opinion about.",
            "Look hard enough in specifying the no no no here, here, here you get here.",
            "With clustering you have a special special way of of of modeling your data.",
            "You look for indicator functions and so on, but but I have I.",
            "My gut feeling is that if you if you are not specifying it, an if you in an agnostic way would like to get structure out of it, either it boils down to something like minimum description length type principles which have other problems or or or or you have you have an infinite hypothesis class and then you have a search problem.",
            "You know in some ways or another it it gets unbounded and it reminds me in 1996 there was this.",
            "Final discussion at the edge of Workshop and David Mackay was just advocating specify your prior make it broad enough and you will.",
            "You can learn it and then Steve or Mahendra in the last row and that like the lecture Hall, was asking, what is the prior on Dirac's equation because it was a physics lecture Hall and on the restroom there was the direct equation and I think it makes the point clear if your hypothesis class becomes.",
            "Infinite in, in, in in the VC dimension Sense, then even Bayesian procedures have to be tuned very carefully and and otherwise they get inconsistent.",
            "And and here we might have the same problem.",
            "OK, so so so for applications I think you have to put emphasis on the."
        ],
        [
            "Modeling problem, and that's where the crux is.",
            "So, So what would I like to do?",
            "I told you already a little bit about my view on clustering.",
            "I started in in 96 with with with thinking about what what learning means in clustering we had this.",
            "I came up with this this notion of empirical risk approximation then then there was there was a deadlock.",
            "In improving.",
            "Properties of it.",
            "There's some anecdotal evidence that that the principle might do something useful.",
            "Then we came up with these stability approach going in the completely different direction by basically saying let's assume you have an algorithm.",
            "The algorithm gives you a partitioning back.",
            "Let's measure how good that partitioning is under fluctuations, and that was an attempt, at least from my point of view, to separate the signal in a partitioning when you randomly draw a partitioning from data from the noise.",
            "Now they are different.",
            "We discussed that here.",
            "They are different sources for for these noise effects like algorithmic instability, sampling instability, you can list them.",
            "But that was the attempt.",
            "Unfortunately, for this measure you can.",
            "You cannot show too many things and right now.",
            "You know?",
            "Already at least showed some properties for the asymptotics, and I think this is a promising direction, but I would like to go to advocate.",
            "Actually a research program in a different direction.",
            "I think we have particular properties in clustering, which you usually don't have in classification, and which relat."
        ],
        [
            "And the learning problem back to what, what, what, what I did earlier when we looked at statistical mechanics approaches for learning.",
            "So we have measurements.",
            "I have some objects.",
            "Oh, I have some data XI would like to distinguish between the objects on the data because the clustering for me is a partitioning of the objects.",
            "Now if you talk about K means and the Euclidean setting, then usually you identify the vector which characterize the object with the object itself.",
            "But you might have actually a matrix of the similarities, or even a more complicated data structure when you think in terms of lines as we saw it at least once here as a proposal.",
            "Then the data are much more complicated, but the objects are simple and I think this is important because the optimization.",
            "So what do we?",
            "What would we like to get?",
            "We would like to get the data partitioning C and it Maps the objects to the set one 2K.",
            "And the hypothesis class Cardinal EC is the May the space of all possible mappings.",
            "So the VC dimension is infinite because at least I have K to the end.",
            "Different mappings or something of that order.",
            "This is not the space where sea of owners, but it's the space of the mapping where the mapping seen it or yes.",
            "No, the these these these, these are assignment functions.",
            "They basically tell me what is the cluster for a particular object of these functions.",
            "End of telegraphic, see yes.",
            "Because you don't have.",
            "We can talk about that any any other variants of the dimension which are designed for multiple.",
            "OK, that's the representation issue most most of the time in the earlier 10 years ago I was writing these clustering cost functions of these Boolean variables.",
            "MI new equals one.",
            "When you assign object I to cluster new.",
            "Now you know computer scientists like to have these functions C because it's more compact.",
            "Also, it smells not so much like statistical mechanics and that that helps some people.",
            "OK, so then we have the modeling issue.",
            "There is a function R which Maps the data space and these hypothesis to our plus.",
            "So I have I have my data whatever they are.",
            "I have a hypothesis and I tell you what this value is.",
            "Now this cost function has to be chosen very carefully because is shy pointed out this number might not tell you anything and there is a clear distinction between classification.",
            "And clustering because in clustering and you will see it on the next transparency.",
            "You might have unbounded losses if you can define something like a loss.",
            "So the scale here is important, whereas in classification the scale is already fixed between zero and one, it's just your losses.",
            "So you're saying you have a cost function which for each individual data point and its label computer cost value, yes.",
            "Because I think interesting, it's often much more complicated, so we often don't have a cost function where for each individual data point, independent of all the other data points we can assign across.",
            "OK, you could do this in quantization for community can do that, but if you have some measure of oh, you mean this form here, what do you refer to?",
            "So you're the overall cost of their clustering is something like the sum of the individual costs on your data points?",
            "No, but but you know if you have, for example, correlation clustering.",
            "All what I'm saying is I have some functions are oh, which which tell me how expensive it is to to assign the object O to a particular cluster.",
            "But I'm not saying that this assignment this random variable is independent of all the others.",
            "I say so.",
            "Even though you might, so I think it is something like a global big data structure, so it's not like.",
            "You can't just look in a local environment somehow, OK?",
            "Value in like an incentive cost.",
            "So you need to install it adjusting.",
            "It's not the summer.",
            "OK, forget this is not necessary here, it's not necessary.",
            "Yeah no.",
            "Optimization.",
            "We can do something like.",
            "You can't just say we going to local environment and optimize something here as we can investigation.",
            "We have to look at the global thing.",
            "No.",
            "No, I disagree.",
            "A lot of divide and conquer algorithms are written for many for many combinatorial optimization problems.",
            "So.",
            "So although you have a global cost, even if I'm not able to write it as a sum over individual objects, you have a.",
            "You have a just a function here.",
            "What you have to assume that in some sense all these individual contributions of the same order of magnitude.",
            "If you can't do that if you're for example you have these one data point very far out.",
            "And this data point has more incurs more costs when when wrongly assigned than any other data, then all the other N -- 1 data points together.",
            "Then you have still a clustering problem, but you no longer have a setting where you can learn anything because you cannot make any predictions.",
            "One cluster is the diameter.",
            "Yeah.",
            "I I haven't thought about these types of clustering principles so much, but I I don't think it's a it's a. Yeah, yeah yeah, but some of in class or distances have.",
            "Sure, can they be?",
            "This is some overall points because it's a function of pairs of points, not of individual.",
            "OKOK you know I have different functions are over here.",
            "So.",
            "Yeah, sure.",
            "OK OK yeah.",
            "So.",
            "So what what I also would like to say, this is certainly not the most general way we found already out that this notation is disturbing.",
            "So yeah, I should remove it.",
            "But there are also algorithms out there which basically gives you back.",
            "Which gives you back a partition which are doing something good, but which cannot be written as as minimizing some costs.",
            "So and in the back of our head we should still consider that that we also would like to measure the stability or measure some properties of these types of algorithms and.",
            "In the end, we would like to get also for these."
        ],
        [
            "So these algorithms are learning scenario.",
            "So so when you when you look at K means then you have this nice cost function in 4K means you can actually go quite a long way with with the analysis because conditioned on the wise these axes become independent and then you have these loss functions.",
            "Here these are independent random variables so you can do the usual game and it works out but it works out up to the point.",
            "That that you look at these loss function he ran, it's quadratic, so it's unbounded and you need different types of bounding techniques, and in particular you have to commit to a scale.",
            "So so your remark with respect to scale is exactly related to the unboundedness of the loss function.",
            "Now we also a second remark in classification one has seen that learning curves depend very strongly on assumptions on the on the on the distribution, the whole game of doing statistical mechanics.",
            "For learning curves and seeing these phase transition was related to the fact that particular distributions give you structure, and since we're looking for structure, I question whether whether in a strong sense we can ever achieve a distribution independent result for clustering, because in some sense the structure defines a scale.",
            "The scale defines at least a subset of potential distributions and we have to generalize the notion of distribution dependence.",
            "So that's what I want to say in that respect.",
            "So the optimal clustering solution here is the minimum.",
            "It's the it's the nearest neighbour assignments.",
            "If the guys are fixed, if the guys are not fixed and you let them depend on, see the definition of device, then you get a relatively ugly combinatorial optimization problem which is no longer polynomial in the assignments, but which becomes a rational function in the assignments.",
            "Then it's getting a little bit more complicated.",
            "But there are also other.",
            "Things that you have to consider.",
            "One is the hypothesis class.",
            "So if you look in in books like the book Form Legosi.",
            "Then you will see that they consider the vector quantization case, and in the vector quantization case you know that that the nearest neighbor assignments is the best, but you can do so, you have no uncertainty there, and the only uncertainty is where should you place your voice if you have the vector quantization case, you have finite VC, dimension it so learnable as long as soon as you actually allow yourself to assign the data point.",
            "Either to the next or to the second next cluster you get already in hypothesis class, which has two to the N different possibilities.",
            "And that is exactly the mechanism which allows you in the asymptotic limit.",
            "When you have, when you have infinite amount of data.",
            "Still toe to toe to entertain.",
            "A number of different interpretations for your class string.",
            "Because you might learn your centroids when you get an infinite amount of data, but you cannot learn an individual assignment of data to clusters.",
            "And so, so if these are the variables which you optimize by an algorithm and you and you are not assuming like in vector quantization that you have hard assignments and you always use the nearest neighbor rule, then you get you get a situation where you have to come up with a new notion of what the solution to a learning problem is.",
            "Trying to optimize your over the optimization is see an X.",
            "If I write it like this because why is a strict function of C?",
            "In the mixture model, but you are optimizing other centroids and these unobservable variables.",
            "No.",
            "Help petitions is.",
            "So we just optimize just rolled optimization objective just over the century, integrating over the labels.",
            "Yes, but then you are not.",
            "Then you are not.",
            "How do you want to integrate them out?",
            "What model are you assuming for them maximum entropy?",
            "Difference in model over the labels are so we can to join model over the labels and centers, yes.",
            "But but but you are, you OK, but you admit that that every pair of labels and centroids would be a different hypothesis.",
            "And for example, a Gibbs sampler is exploring these hypothesis.",
            "Looking like this, I'm going to look at the marginal only over the centuries, so now my my my model is just deciding objective two centroids OK and give me Central.",
            "It's I'll tell you what the but still fun, but for mix for mixture models you would still estimate the probabilities of assigning data point OKOKOKOKOKOK.",
            "For example, you might get rid of this problem in the context of clustering.",
            "You are not getting rid of this problem.",
            "When I tell you, now apply your insights from learning and generalization to ATSP problem where I give you the travel time between cities and they are uncertain.",
            "And you should tell me what what, what, what, what, what a reasonable traveler actually does.",
            "He he or she minimizes the expected travel times and it's not committing to the optimal travel time on one sample of of times.",
            "So I don't know.",
            "I'm not sure computer scientist speak something, but still if I just have the centroids, I can still tell you the expected and I'm keeping the centrics.",
            "My mother is just the centric, but you know when they answer queries like this form, I can use the expectations with respect to those centroids.",
            "I don't have to too.",
            "I'm not saying that I'm going to be assigning each point to the map century, I'm just saying that I'm not.",
            "I'm not worrying about the assignments at the moment.",
            "We model just misses the centroid, but you have to.",
            "Somehow you have to explore what is.",
            "What is the probability of assigning a particular data point to a particular centroid, and these values are either fixed by the model and then only the centroids are your problem or they.",
            "Might be variable, and if you want to learn them then then you get this explosion of.",
            "Because it's not the binary function.",
            "I mean if if I if the number of clusters is not predetermined, that's right, then it's not a bandwidth.",
            "Then what's relevant here is scale set.",
            "Is that yeah, fetch header in dimensional, yeah.",
            "Is there any I can find with certain a confidence, some approximation?",
            "On the value of approximation and give you final sample.",
            "But that is not the point I wanted.",
            "I wanted to help you.",
            "OK so you would like to remove this statement.",
            "This problem is not solvable.",
            "No, no.",
            "What I'm advocating is the following.",
            "Learnability in the sense that you give back an individual function, which is the result of your analysis, is is the traditional notion of a solution to a learning problem.",
            "What you could also say is you have way too many.",
            "You have way too many degrees of freedom in your hypothesis class you would like to keep these degrees of freedom because they give you flexibility.",
            "So you relax your notion of a solution by saying there is a set of possible.",
            "Explanations and these set is somehow characterized.",
            "Then we have a representation problem, but this is ultimately your solution.",
            "So if you use a Gibbs sampler for optimizing any kind of costs which are empirically defined, and you stop that Gibbs sampler at a finite temperature.",
            "Then this Gibbs sampler gives you zillions of different solutions they all capture in as an ensemble, some properties of your data, and that's what I would like to keep.",
            "So so I would like to keep a finite set of hypothesis which are compatible with with my data now.",
            "But you are suggesting is this is a misspecified problem.",
            "You have too many degrees of freedom before you even optimize commit to a particular model like the people did this vector quantization by saying the assignment has to be the nearest neighbor assignment.",
            "Then you have a well specified problem and you get an unique answer.",
            "What I'm saying is you pay a price for that.",
            "You don't have any more flexibility in the assignments, which apparently seem to be important when we talk about clustering, because the cluster point at the boundary.",
            "Is not.",
            "This 100% generated by by by the nearest.",
            "Neighbor OK yeah I I I since you are a theoretician, an I'm only trained physicist getting paid as a computer scientist.",
            "You know this much better than me, so I am not committing to that.",
            "What I'm saying is the notion of what people had in physics, that you have macroscopic variables in microscopic variables.",
            "This notion directly comes into play here.",
            "The microscopic variables are this assignment variables and they cannot be specified.",
            "In the macroscopic variables in the asymptotic limit, get a direct distribution when they're properly defined, and then you can learn them.",
            "That does not mean that you should throw away the microscopic variables, and I bet there are certainly some some relations to this random variables, because that's basically what you want."
        ],
        [
            "See what you get.",
            "OK, I I I."
        ],
        [
            "I told you already about this problem.",
            "So.",
            "Damn it, what is this?",
            "OK, I wanted to.",
            "I wanted to come to this one so so this might be a partition you learn for clustering.",
            "I I now colored the data points so that I actually can trace them.",
            "So this is the second sample if you just use your cluster solution and this is your second sample, you see that a lot of data outside and so.",
            "So the cluster structure is not not nicely sort of mapped to it.",
            "The colors refer to three Gaussians from which I have chosen this data.",
            "So obviously the shape of these clusters.",
            "Is too complex to be determined by the algorithm for these type of data.",
            "And that's an instability we would like to you."
        ],
        [
            "Like to detect.",
            "OK, so so this is now the essential picture, which which I would like to discuss and the rest is is basically something which which which I did a couple of years ago.",
            "We haven't and solution space and for clustering this solution space has a size K to the N and I have a solution space for which I which I consider here for a trainings instance and this is the solution space for the test instance.",
            "So I have two instances which I consider.",
            "It's a two instance scenario.",
            "Now what you do is you find the empirical risk minimization solution noted by the X here and then you have.",
            "Then you have a set of of gamma close functions which which are gamma close to this.",
            "Here now for this you have to define a distance in terms of your clustering costs.",
            "For this solution space for the space of assignments.",
            "If you use K means, for example, the distance from the nearest neighbor assignment is also exactly the costs of a particular clustering.",
            "So in this in this, in the case where you have one function which is a complete complete minner and on all other functions on all other loss functions, then that's only true for the distance to the to the minimizer.",
            "Then the costs are identical to.",
            "To the distance.",
            "So so this set here are all costs which are less than gamma verse in terms of my criterion than the best empirical minimizer.",
            "So when you now consider.",
            "These functions here and you and you would like to know how you apply such a partitioning to a second instance.",
            "Then it's not really clear how to do that, and I guess almost independent Lee shy and and we came up with the notion of an extension.",
            "You did that in your subsampling case and we did it in the context that that we actually were considering pairwise clusterings normalized cut.",
            "For proteins and image applications, and it was absolutely unclear how I actually how I take a segmentation of one image and apply that to another image.",
            "We are semantically I have the same objects in there.",
            "I have a Tiger in there in a tree about the Tiger is in one image lying on the on on one of the branches of that of the tree and the other one it jumps from the tree.",
            "So how would you actually copy such such a segmentation?",
            "And if you don't know how to copy that then then the whole notion of generalization becomes meaningless.",
            "So there you have to find such a 5 function here.",
            "Now this actually might be distorted in this new space.",
            "Here what you can also then consider is if you take your test instance.",
            "And you and you.",
            "And you do an empirical optimization on the test instance, something which you learn and normally can't do but.",
            "But if I give you the second image, you just perform that, then you might get this.",
            "This circle, which is partially overlapping with the image of this solutions when mapped to the new one.",
            "And instability somehow measures how how different are.",
            "Are these sets?",
            "At least that's what you would expect if I generalize my solution to a new instance and it has a large overlap with what I would do with an approximation on that new instance, then I would consider it stable.",
            "Then obviously the noise hasn't hasn't bothered me too much.",
            "So, so that is that is the issue at the level of of of instances.",
            "No here, for example, I assumed that the empirical minimizer on my training data is actually not in the epsilon approximation or gamma approximation on the test data.",
            "It it it lies somewhere out here.",
            "So what what is missing in that picture in that picture is missing and and now I I I would like to refer to your knowledge about information theory and coding theory.",
            "What I would like to so far we have we have two instances X1 and X2.",
            "These are my datasets.",
            "Which why are my clustering cost function define my optimization problem.",
            "So I have two optimization problems and I assume that these two data.",
            "Coming from a souls.",
            "You know they in some sense that I should be talking about the same same problems which are only different because of fluctuations.",
            "So, so this defines an instance.",
            "An optimization in this defines an optimization instance, this space.",
            "Is much much larger usually than this space.",
            "So for example, this space in clustering is K to the N. This phase in correlation clustering.",
            "Since I have these correlation matrices as possible optimization instances is.",
            "Is if I have binary correlations, is 2 to the N ^2?",
            "So my instance space cardinality is 2 to the N squared and my solution space.",
            "I think I called it, you see is is K to the end.",
            "So so, So what I have to cope with this that my variation?",
            "Comes up here in the instant space.",
            "I perturb my data.",
            "But it's some process here and then I get a test instance on which I would like to test my learn solution.",
            "So since this space is much larger, I don't really know which of the degrees of freedom in that space are actually relevant to change a solution down here.",
            "So what I have to find is I have to find an equivalence class partitioning of that space in huge equivalence classes, which then are so different that they actually make a difference in the solutions.",
            "And this is an issue here because the solution space is tiny compared to my instant space, so a lot of degrees of freedom in the instance space are completely irrelevant.",
            "A four for determining a solution.",
            "So the idea I think, which one could draw from from from information theory, is.",
            "That you try to find such spheres in the solution space.",
            "Bless you.",
            "And you cover your solution space with these fields.",
            "You define all functions which are inside such as fear as statistically equivalent.",
            "You can't distinguish between them, but if you go from one sphere to the next sphere, then you say.",
            "Now I have a statistically different function I should care about.",
            "Then you ask yourself, OK, how can I now using this feels down here.",
            "It's basically an epsilon covering type of argument.",
            "How can I use this?",
            "Feels here to partition that space up there and there I would like to to refer to information theory where you basically have a codebook, you take a codebook vector, you transmit that codebook vector.",
            "The codebook vector is then corrupted, and then you compare, which is an optimization this codebook vector.",
            "The received the received message vector with your codebook and you do a minimization.",
            "So you have the instance space, but for every instance you have a different solution space, because then the space of clustering of this particular sample, right?",
            "Yes.",
            "You want to map now.",
            "That's.",
            "Subsets of that solution spaces back in place this place, but you have.",
            "How do you do it?",
            "It was the same solution, yes, yes, yes you get.",
            "You get as many instances as you consider.",
            "You get potentially different solution spaces, but since I assume that I have a function Phi, I know how to map them to each other and if we talk about the whole business in Euclidean spaces and I have K means, then this file is the identity matrix or this fine might be a very simple function like the nearest neighbour assignment function.",
            "Parameters.",
            "Oh yes, the the equivalence class of the of the assignments.",
            "The whole.",
            "Infinite yeah yes.",
            "OK.",
            "The bottom part of the figure I think you assume.",
            "Gamma closeness in terms of costs, yes.",
            "At least informally related to instability so, but yes.",
            "OK yes yes, yes this is this is this is the point which I wanted to make in the beginning that if you have if you have one clustering which is in costs.",
            "Pill object clearly better than any other clustering.",
            "You have 'em in a runt of it.",
            "Then if I consider that Minner and as an anchor point, which is the empirical risk minimizer, that's that X.",
            "Then cost differences of any other point in here too.",
            "That is also a distance.",
            "Incense in the sense of an L1 loss.",
            "Now if I take a point here in a point here and I calculate the distance in the L1 sense, this for sure is no longer a difference in energies.",
            "So, so it the notion of having of having a distance measure in the solution space and considering cost differences only coincide when you are very close to the global minimizer.",
            "Sort of sort of the last ball, which you can define around the global minimizer also gives you.",
            "Up to maybe some pathologies of your of your cost function.",
            "It gives you a bound on the distance if you are far away you have these.",
            "You have these energy layers.",
            "These cost layers where you have functions which are very far apart from each other.",
            "But they have the same costs, so their cost differences are zero.",
            "So So what you have to do is you basically have to consider feels around prototypical new minimizers of other problems of other clustering problems, which in some sense are equivalent to this clustering problem.",
            "So my proposal is that you start, you take your clustering, propose you click the clustering cost function, you permute the indices.",
            "So you basically set problems all over here in your solution space and then you determine the size of these spheres as an approximation quantity in such a way.",
            "That when you apply the perturbation due to your sampling draws that you stay in the same field.",
            "And this would give you something like in information theory.",
            "This is called the mutual information, because it gives you the sphere covering of Hamming space.",
            "Here you would have something like approximation capacity, because if I if this perturbation in my instance space is huge, then for sure I can only fill in a few balls here, otherwise I will already go from one bowl to the next one.",
            "If there is no noise in here.",
            "Any problem will be different because my deterministic algorithm, if it runs well enough, will allow me to find that solution.",
            "So how do you do that now?",
            "Now basically you have to calculate.",
            "If I take this problem, which is my starting problem.",
            "I convert this problem by permuting the indices of my objects in a random way.",
            "I calculate a new problem which has a minimizer here.",
            "Then I have to I have to basically now calculate the overlap of these tools, spheres and I would expect like in information theory that this overlap is the size of the of this field time.",
            "Diameter grows, you get basically a threshold phenomenon.",
            "Either they're completely overlapping or they're completely distinct, and this threshold phenomenon comes from the fact that you have these.",
            "Exponentially many degrees of freedom from the assignment variable, so the statistical mechanics trick, which is basically a large deviation phenomenon, helps you to make clear how you have to sample that.",
            "Now what is?",
            "What is an additional benefit from from a computer science point of view?",
            "This is sort of a principle for approximation.",
            "Now now if you if you, I'm probably running out of time.",
            "If you if you use approximation algorithms in the setting of approximation theoretical computer science, you have no guarantee that this algorithm gives you, in any sense a typical solution back.",
            "This algorithm just gives you a solution back which is below your threshold.",
            "Nothing else is proven.",
            "However, we know from applications that in costs accepting a solution which is a factor of two worlds, then the best one is basically being out of the game.",
            "Now in clustering we have very strong indications that these approximation factors sometimes are above the highest phase transition.",
            "So basically only throwing all the clusters in the same in the same doing all the objects in the same cluster.",
            "Is already a solution which would be admissible from the approximation bar, so we really have to rely on typical solutions which we sample and not on solutions which are in some sense just artificial.",
            "Now information theory tells you how to get these typical solutions because you get this, you get this set of solutions back.",
            "You can average over it and and.",
            "And and and then you and then you.",
            "And then you can sample from such a solution.",
            "All belief propagation algorithms essentially do some kind of averaging over this over this.",
            "Overly set.",
            "OK, so so this is the picture I have in mind.",
            "Are there any questions?",
            "Because what is on the rest of the slide?"
        ],
        [
            "This is what I did in terms of calculations, so this is a summary so, So what I would like to do is you you treat, but this is now more specific for clustering you did you.",
            "You treat your data partition as a key area code.",
            "Then you will communicate and that's important.",
            "You communicate instances.",
            "Because the instances are perturbed and not the not my code.",
            "You have to.",
            "You have to weave into that communication process the optimization which has to be done by the by the by the receiver.",
            "And if you have a very delicate optimization problem, then receiver cannot reconstruct the OR identify the instance from a whole codebook of different instances, which the sender might have used.",
            "Then you have to use a crude approximation level.",
            "I communicate with you by sending you an instance.",
            "OV OV.",
            "Basically we basically draw from that Oracle.",
            "I get an instance, you get an instance so that these instances generated by the same source.",
            "And I have to and I have to.",
            "I have to specify now my approximation level in such a way that you have a chance to identify these souls.",
            "Because this source.",
            "This is one of the codebook problems where we draw two instances.",
            "Here there might be another codebook problem here.",
            "Write another book.",
            "Be a code problem, and so on.",
            "You might have many different types of code book problems because we we partition this space in equivalence classes.",
            "Now, now somebody.",
            "Are you saying you are trying to transmit a number between one and K to me?",
            "No, I I I the message would be that that you have to communicate an index for one of these codebook problems here.",
            "Communicating to me an index and index, yes, but you at the end get a problem to be optimized.",
            "No, that is that that is, that is, that is a number between one and two to the NR where are defined some kind of rate.",
            "It's basically it's basically the number of spheres which we can place on the solution space in such a way that if I if somebody maliciously perturbs of a problem, I don't know exactly what problem you got anymore because the perturbation is already large enough that I might confuse it with another problem.",
            "It's like like you get the vector we want to talk about the same codebook vector.",
            "Unfortunately you have the codebook vector as a send and I have.",
            "Only a noisy copy of it.",
            "Rating mechanism.",
            "We both we both.",
            "We sort of get from an Oracle.",
            "The problem generating mechanism.",
            "Yeah, data in problem is identical because I we community commit to a clustering problem.",
            "The instance is defined by the data and the optimization is weaved into that.",
            "Approximation here.",
            "So so if you do that, then you relat the combinatorial quantity.",
            "How many spheres can you place with a certain distance in the distance comes from how you write down your cost function.",
            "You leave that that covering problem.",
            "Into a bound on on the optimization.",
            "So, so why does that?",
            "Anything has to do with annealing or these types of algorithms, or the temperature which which you don't understand.",
            "It has something to do because if I define this set, I basically write down.",
            "If I want to know what is the cardinality of of C gamma one, I write it as a sum over all my my partitions.",
            "And then I have a theater.",
            "R. Min minus.",
            "I'll see.",
            "Plus, so as long as my costs.",
            "Are smaller than the minimal costs plus an approximation quantity gamma.",
            "I count this function, otherwise I discard that function.",
            "So if you write down this.",
            "Then what you will get is you will get and you didn't want to sample from that set from the tsetse gamma.",
            "You get the Gibbs distribution as the sampling procedure, which is maximum entropy on that.",
            "So the temperature is just the LaGrange parameter to ensure this threshold.",
            "To ensure this radius here.",
            "It's nothing more than that and it has to come into the game because we have to commit to equality.",
            "So the modeling part comes into the play when you define the distance of these fields which cover your solution space.",
            "OK, I guess.",
            "Let me just go over this."
        ],
        [
            "So in the end, what I would like to get this, you know relation like this.",
            "There is something like the entropy.",
            "There is something like the energy the energy is is is the is the the size of that maximal radius of that sphere because basically all the solutions are at the boundary of that sphere.",
            "So the average energy over the sphere is the average energy over the over the shell.",
            "The entropy counts how many functions I have there.",
            "An if I differentiate this this this combinatorial quantity with respect to the radius of my sphere, then I get the inverse temperature.",
            "That's exactly the relationship which you know from statistical mechanics.",
            "So the temperature is nothing but the combinatorial quantity over States and these states have in physics and energy semantics.",
            "In optimization they have an optimization semantics and so on.",
            "Now large deviation bounds.",
            "If you put them together, but I would expect that it depends on the cardinality of your space, which you can use for defining problems divided by the cardinality of of.",
            "Of Of your sphere.",
            "So if gamma is large, this will be large.",
            "Then I will have a small factor.",
            "Here the inverse temperature has to be small.",
            "The temperature has to be large, so it's exactly this tradeoff between between small noise being able of having small spheres to cover your solution space and and then being able to approximate to a very small level of.",
            "Precision, but these algorithms, which are basically in the spirit of Gibbs sampling, would guarantee you that you get typical solution with a very low probability.",
            "You get an atypical solution.",
            "And that's what you live on in these approximations.",
            "OK, so that's the."
        ],
        [
            "Picture which.",
            "Which I know so so I have no time.",
            "OK, so so why is that useful?",
            "Why do we actually care about the better approximation technique than what we have so far?",
            "Well, a typical problem which which comes from image analysis is the following.",
            "You have different sort of different scales in image analysis.",
            "There's the data scale just defined by the resolution.",
            "There is and this is now a segmentation problem in some sense of segmentation.",
            "Problem is especially ordered clustering problem where you have a situation like you you described you cannot so easily pull out an object and write it.",
            "Write the costs as a sum of independent components.",
            "OK, you have another scale, and that's the approximation scale.",
            "How value should approximate a solution here?",
            "And then you have the model order.",
            "The scale.",
            "How many classes should you use now?",
            "Why is the approximation?",
            "Of offer of of a set of solutions by a set of solution solution to the model to the, to the to the model model or the problem.",
            "This segmentation, as this segmentation?",
            "Comes out as an interpretation of a Let's let's go to here.",
            "This is a segmentation in two different classes.",
            "You see the segment up here in the segment down here.",
            "The colors indicate the assignment of a Pixel 21 cluster.",
            "So the fact that you have different colors in here but totally randomly discribed only means that the clustering algorithm has basically two clusters, one in the upper part for the Sky in running the lower part for the rest.",
            "Now between the different clusters.",
            "You do not distinguish here.",
            "This is a frozen instance of the Gibbs sampling.",
            "If you run it longer then you get a completely different picture.",
            "You still get all the yellow stuff up here in all the blueish greenish stuff down here.",
            "But but but otherwise all these segmentations are independent and the result is from using such a maximum entropy type procedure that if I now calculate the means in color space for my for my for my distribution down here, I have three.",
            "I have three Gaussians because we model this Wisconsin and have three girls since down here, which which basically have.",
            "The same mean.",
            "So I three times a degenerate distribution.",
            "If I reduce my temperature, I see how structure is unfolding.",
            "I see more and more of the combinatorial structure in my data.",
            "And here this is very low temperature, so here you get the five.",
            "So this is the optimization scale.",
            "The Model order scale.",
            "We don't know how to couple these scales.",
            "People have no clue they have zillions of different heuristics, but there is no principled way of doing that.",
            "Such a theory, which which tells you something about robust optimization which comes from from a quantization of your solution space, which then is mapped to the instant space to tell you what are significantly different problems, so that you actually should provide representation, power or representation degrees of freedom to describe them.",
            "Such a theorie would couple these data space, which basically tells you something about the fluctuations.",
            "To the to the, to the, to the to the cost approximation scale, which basically tells you how your interpretation pays attention to the fluctuations here and to the model order scale.",
            "How you control within the setting of a cost function the degrees of freedom.",
            "And that's what we need in practice, because that is a very relevant problem.",
            "And then there comes the other observation.",
            "When the problem is well formulated, like in this case, or in this case, usually it's extremely simple to find the solution.",
            "So the algorithmic complexities also weaved into that because.",
            "When I go from small spheres to very large spheres, I basically have a.",
            "A continuity method which which which takes a discrete optimization problem and by averaging their problem embedded in a family of simpler problem up to when I have only one cluster, then I have a entropy driven convex optimization problem which basically gives me the meaningless and so that all classes are the same."
        ],
        [
            "Hi this is the rapper.",
            "OK. Well, we didn't settle on this one.",
            "I think I think that the frame theorie which we are missing also in in clustering should give you the optimal tradeoff between stability and informative ITI.",
            "But if I if I have, if I have a hypothesis class, we are potentially I have 100 clusters, But I have an averaging procedure such that the degrees of freedom for these hundred clusters, because it will be a typical, never really express and I commit only to a set of solutions which I treat them as equivalently so.",
            "So there would be a Bayesian view how you could superimpose that.",
            "Then I would have automatically solve this tradeoff between informative itean stability.",
            "And and the breakpoint when I have too much noise in my problem instance generation process and I want to approximate two precisely is basically that that that people who draw the same problem.",
            "But with two instances from a soul from a problem, source will come up with different solutions and they will decide they are statistically different.",
            "Because not the empirical minimizer is the solution.",
            "The set coming with the empirical minimizer is the solution, and there will be no overlap between these two solution sets anymore.",
            "OK, so thank you.",
            "Tell it's just an idea.",
            "Actions regarding this.",
            "Everybody is sick, so.",
            "So let me let me just take the stage and thank the organizers for bringing it together.",
            "It's also the last the last talk.",
            "I guess there is still a afternoon brainstorming session, but people probably will will diffuse quickly.",
            "Yeah, I hope.",
            "I hope that that the expectations are met at least partially and.",
            "Next, deadlines are approaching.",
            "Well, actually I I would have I. I would hope that that this is not sort of deadline driven research because I think machine learning really has to tell a fundamental lesson to computer science and it's related to the fact that that if you neglect noise, you might conceptually have simpler problems, like in optimization.",
            "But you also start asking questions which become unsolvable and at least for the practice, it does not make a difference.",
            "So, so pay attention to this variability and then you will gain automatically relevance from practice because everybody is an engineer is usually able to do measurements and give you some uncertainty values on their measurements.",
            "So identifying these variability of the instances is usually something where you can quite easily commit to in discussions.",
            "The modeling problem is not solved.",
            "But but at least this this will give give some ideas."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I would like to do is to make the same disclaimer says Natty did I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm basically not presenting specific results.",
                    "label": 0
                },
                {
                    "sent": "I'm presenting a strategy which I think might relate the issue.",
                    "label": 0
                },
                {
                    "sent": "The relative specific issues of class stability to the much broader issue that we have an optimization problem and this might be typically a combinatorial optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's defined by random variables because you have noisy measurements to specify your instance.",
                    "label": 0
                },
                {
                    "sent": "And then you would like to know how precisely should you approximate such.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such such a problem.",
                    "label": 0
                },
                {
                    "sent": "So we have seen this, you know, coming back to.",
                    "label": 0
                },
                {
                    "sent": "Two clustering and spending a couple of moments on on what what I was setting for clustering is I think, and that's why I insisted on the separation between the modeling issue and the model order selection issue I I. I would like the the designer of a clustering algorithm to pay attention when when writing down the cost function to all aspects he or she would like to emphasize in data analysis.",
                    "label": 0
                },
                {
                    "sent": "So the cost function has to capture it if you go.",
                    "label": 0
                },
                {
                    "sent": "If you get lousy results from the clustering point of view, then you should reconsider your model attempt and I think it's very valuable.",
                    "label": 0
                },
                {
                    "sent": "To separate the modeling issues from the from the sampling issues, from how well can you actually determine a solution as a learning generalization problem, and there is a modeling problem and we worked for quite some time on the modeling problem and have various solutions to that, but as far as is I would say when we used instability measures for determining the number of clusters, we never wanted to fix the modeling problem.",
                    "label": 0
                },
                {
                    "sent": "We have anecdotal evidence that if you have a strong misspecification of your model with respect to your data.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you think this should be connected components rather than these blobs, or here on the other side side, you want to save blobs and you have a situation like this, then you get in many cases you get instability becausw the underlying data distribution has some features.",
                    "label": 0
                },
                {
                    "sent": "Which the algorithm cannot capture an by algorithm I mean the cost function including the optimization principle.",
                    "label": 0
                },
                {
                    "sent": "So this since the algorithm most likely will not conspire with nature in a malicious way to pretend something is super stable and nevertheless you get a lousy clustering value, a cost value, you will see a little bit of instability.",
                    "label": 0
                },
                {
                    "sent": "But I have strong doubts that you can solve the modeling problem.",
                    "label": 0
                },
                {
                    "sent": "And maybe one has to we have to talk about this even further.",
                    "label": 0
                },
                {
                    "sent": "But I I have strong doubts that you will find a metatheory for clustering which captures all the properties you would like to have for class string.",
                    "label": 0
                },
                {
                    "sent": "And automatically removes the burden from the designer of a data analysis problem to specify models.",
                    "label": 0
                },
                {
                    "sent": "So these automatic you know this machine which automatically spits out theories about your data and and then can sort out with a meta principle like learnability or stability.",
                    "label": 0
                },
                {
                    "sent": "Which of these models are kosher?",
                    "label": 0
                },
                {
                    "sent": "I have my doubts that this will work now to some degree the support vector machines try to intend that because.",
                    "label": 0
                },
                {
                    "sent": "You you have this kernel, the kernel has many degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "You go to very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's basically unlearnable there and then you introduce the margin and you solve their trade off.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless I I think this is a dream which which which?",
                    "label": 0
                },
                {
                    "sent": "Which I would would not subscribe to.",
                    "label": 0
                },
                {
                    "sent": "This is the dream that I'm trying to promote or anything, but I'm saying is that look at this example, yes?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "If you just apply stability and you take a came in sideways and then it will be more stable on the left, picture that on there and it will be unstable on the right hand picture.",
                    "label": 0
                },
                {
                    "sent": "No no, but I make an F fine transformation.",
                    "label": 0
                },
                {
                    "sent": "I stretch it like this then it will be stable here too.",
                    "label": 0
                },
                {
                    "sent": "Let's the problem.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "So K means gives you probably much higher costs here than here, even if you normalize it to the variance.",
                    "label": 0
                },
                {
                    "sent": "Annual fix your scale so that you actually can compare apples with peers.",
                    "label": 0
                },
                {
                    "sent": "But apples and oranges, but I you know that's the problem with all the negative results you can come up with as long.",
                    "label": 0
                },
                {
                    "sent": "I'm only frightened if these negative results.",
                    "label": 0
                },
                {
                    "sent": "It can attack.",
                    "label": 0
                },
                {
                    "sent": "A meta principle like like like stability measurements when we believe it's the right model and nevertheless it will fail.",
                    "label": 0
                },
                {
                    "sent": "That is a critical test.",
                    "label": 0
                },
                {
                    "sent": "Now if the model is misspecified then then we have this problem that that that OK we get the wrong answer but we get the wrong answer because we didn't work hard enough on the modeling issue and we tried to put the burden on on sort of a meta frame theory.",
                    "label": 0
                },
                {
                    "sent": "Reminds me of my supervisor and he told me that he never had a problem that if you work hard enough you can solve it.",
                    "label": 0
                },
                {
                    "sent": "So I asked him what about vehicles and pee so he said, oh I didn't work hard enough.",
                    "label": 0
                },
                {
                    "sent": "So you say it whenever well, Hilbert also had that opinion about.",
                    "label": 0
                },
                {
                    "sent": "Look hard enough in specifying the no no no here, here, here you get here.",
                    "label": 0
                },
                {
                    "sent": "With clustering you have a special special way of of of modeling your data.",
                    "label": 0
                },
                {
                    "sent": "You look for indicator functions and so on, but but I have I.",
                    "label": 0
                },
                {
                    "sent": "My gut feeling is that if you if you are not specifying it, an if you in an agnostic way would like to get structure out of it, either it boils down to something like minimum description length type principles which have other problems or or or or you have you have an infinite hypothesis class and then you have a search problem.",
                    "label": 0
                },
                {
                    "sent": "You know in some ways or another it it gets unbounded and it reminds me in 1996 there was this.",
                    "label": 0
                },
                {
                    "sent": "Final discussion at the edge of Workshop and David Mackay was just advocating specify your prior make it broad enough and you will.",
                    "label": 0
                },
                {
                    "sent": "You can learn it and then Steve or Mahendra in the last row and that like the lecture Hall, was asking, what is the prior on Dirac's equation because it was a physics lecture Hall and on the restroom there was the direct equation and I think it makes the point clear if your hypothesis class becomes.",
                    "label": 0
                },
                {
                    "sent": "Infinite in, in, in in the VC dimension Sense, then even Bayesian procedures have to be tuned very carefully and and otherwise they get inconsistent.",
                    "label": 0
                },
                {
                    "sent": "And and here we might have the same problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so for applications I think you have to put emphasis on the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modeling problem, and that's where the crux is.",
                    "label": 0
                },
                {
                    "sent": "So, So what would I like to do?",
                    "label": 0
                },
                {
                    "sent": "I told you already a little bit about my view on clustering.",
                    "label": 1
                },
                {
                    "sent": "I started in in 96 with with with thinking about what what learning means in clustering we had this.",
                    "label": 1
                },
                {
                    "sent": "I came up with this this notion of empirical risk approximation then then there was there was a deadlock.",
                    "label": 0
                },
                {
                    "sent": "In improving.",
                    "label": 0
                },
                {
                    "sent": "Properties of it.",
                    "label": 0
                },
                {
                    "sent": "There's some anecdotal evidence that that the principle might do something useful.",
                    "label": 0
                },
                {
                    "sent": "Then we came up with these stability approach going in the completely different direction by basically saying let's assume you have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm gives you a partitioning back.",
                    "label": 0
                },
                {
                    "sent": "Let's measure how good that partitioning is under fluctuations, and that was an attempt, at least from my point of view, to separate the signal in a partitioning when you randomly draw a partitioning from data from the noise.",
                    "label": 0
                },
                {
                    "sent": "Now they are different.",
                    "label": 0
                },
                {
                    "sent": "We discussed that here.",
                    "label": 0
                },
                {
                    "sent": "They are different sources for for these noise effects like algorithmic instability, sampling instability, you can list them.",
                    "label": 0
                },
                {
                    "sent": "But that was the attempt.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, for this measure you can.",
                    "label": 0
                },
                {
                    "sent": "You cannot show too many things and right now.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Already at least showed some properties for the asymptotics, and I think this is a promising direction, but I would like to go to advocate.",
                    "label": 0
                },
                {
                    "sent": "Actually a research program in a different direction.",
                    "label": 0
                },
                {
                    "sent": "I think we have particular properties in clustering, which you usually don't have in classification, and which relat.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the learning problem back to what, what, what, what I did earlier when we looked at statistical mechanics approaches for learning.",
                    "label": 0
                },
                {
                    "sent": "So we have measurements.",
                    "label": 0
                },
                {
                    "sent": "I have some objects.",
                    "label": 0
                },
                {
                    "sent": "Oh, I have some data XI would like to distinguish between the objects on the data because the clustering for me is a partitioning of the objects.",
                    "label": 0
                },
                {
                    "sent": "Now if you talk about K means and the Euclidean setting, then usually you identify the vector which characterize the object with the object itself.",
                    "label": 0
                },
                {
                    "sent": "But you might have actually a matrix of the similarities, or even a more complicated data structure when you think in terms of lines as we saw it at least once here as a proposal.",
                    "label": 0
                },
                {
                    "sent": "Then the data are much more complicated, but the objects are simple and I think this is important because the optimization.",
                    "label": 0
                },
                {
                    "sent": "So what do we?",
                    "label": 0
                },
                {
                    "sent": "What would we like to get?",
                    "label": 0
                },
                {
                    "sent": "We would like to get the data partitioning C and it Maps the objects to the set one 2K.",
                    "label": 0
                },
                {
                    "sent": "And the hypothesis class Cardinal EC is the May the space of all possible mappings.",
                    "label": 1
                },
                {
                    "sent": "So the VC dimension is infinite because at least I have K to the end.",
                    "label": 0
                },
                {
                    "sent": "Different mappings or something of that order.",
                    "label": 0
                },
                {
                    "sent": "This is not the space where sea of owners, but it's the space of the mapping where the mapping seen it or yes.",
                    "label": 0
                },
                {
                    "sent": "No, the these these these, these are assignment functions.",
                    "label": 1
                },
                {
                    "sent": "They basically tell me what is the cluster for a particular object of these functions.",
                    "label": 0
                },
                {
                    "sent": "End of telegraphic, see yes.",
                    "label": 0
                },
                {
                    "sent": "Because you don't have.",
                    "label": 0
                },
                {
                    "sent": "We can talk about that any any other variants of the dimension which are designed for multiple.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the representation issue most most of the time in the earlier 10 years ago I was writing these clustering cost functions of these Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "MI new equals one.",
                    "label": 0
                },
                {
                    "sent": "When you assign object I to cluster new.",
                    "label": 0
                },
                {
                    "sent": "Now you know computer scientists like to have these functions C because it's more compact.",
                    "label": 0
                },
                {
                    "sent": "Also, it smells not so much like statistical mechanics and that that helps some people.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we have the modeling issue.",
                    "label": 0
                },
                {
                    "sent": "There is a function R which Maps the data space and these hypothesis to our plus.",
                    "label": 0
                },
                {
                    "sent": "So I have I have my data whatever they are.",
                    "label": 0
                },
                {
                    "sent": "I have a hypothesis and I tell you what this value is.",
                    "label": 0
                },
                {
                    "sent": "Now this cost function has to be chosen very carefully because is shy pointed out this number might not tell you anything and there is a clear distinction between classification.",
                    "label": 0
                },
                {
                    "sent": "And clustering because in clustering and you will see it on the next transparency.",
                    "label": 0
                },
                {
                    "sent": "You might have unbounded losses if you can define something like a loss.",
                    "label": 1
                },
                {
                    "sent": "So the scale here is important, whereas in classification the scale is already fixed between zero and one, it's just your losses.",
                    "label": 0
                },
                {
                    "sent": "So you're saying you have a cost function which for each individual data point and its label computer cost value, yes.",
                    "label": 0
                },
                {
                    "sent": "Because I think interesting, it's often much more complicated, so we often don't have a cost function where for each individual data point, independent of all the other data points we can assign across.",
                    "label": 0
                },
                {
                    "sent": "OK, you could do this in quantization for community can do that, but if you have some measure of oh, you mean this form here, what do you refer to?",
                    "label": 0
                },
                {
                    "sent": "So you're the overall cost of their clustering is something like the sum of the individual costs on your data points?",
                    "label": 0
                },
                {
                    "sent": "No, but but you know if you have, for example, correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "All what I'm saying is I have some functions are oh, which which tell me how expensive it is to to assign the object O to a particular cluster.",
                    "label": 0
                },
                {
                    "sent": "But I'm not saying that this assignment this random variable is independent of all the others.",
                    "label": 0
                },
                {
                    "sent": "I say so.",
                    "label": 0
                },
                {
                    "sent": "Even though you might, so I think it is something like a global big data structure, so it's not like.",
                    "label": 0
                },
                {
                    "sent": "You can't just look in a local environment somehow, OK?",
                    "label": 0
                },
                {
                    "sent": "Value in like an incentive cost.",
                    "label": 0
                },
                {
                    "sent": "So you need to install it adjusting.",
                    "label": 0
                },
                {
                    "sent": "It's not the summer.",
                    "label": 0
                },
                {
                    "sent": "OK, forget this is not necessary here, it's not necessary.",
                    "label": 0
                },
                {
                    "sent": "Yeah no.",
                    "label": 0
                },
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "We can do something like.",
                    "label": 0
                },
                {
                    "sent": "You can't just say we going to local environment and optimize something here as we can investigation.",
                    "label": 0
                },
                {
                    "sent": "We have to look at the global thing.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, I disagree.",
                    "label": 0
                },
                {
                    "sent": "A lot of divide and conquer algorithms are written for many for many combinatorial optimization problems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So although you have a global cost, even if I'm not able to write it as a sum over individual objects, you have a.",
                    "label": 0
                },
                {
                    "sent": "You have a just a function here.",
                    "label": 0
                },
                {
                    "sent": "What you have to assume that in some sense all these individual contributions of the same order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "If you can't do that if you're for example you have these one data point very far out.",
                    "label": 0
                },
                {
                    "sent": "And this data point has more incurs more costs when when wrongly assigned than any other data, then all the other N -- 1 data points together.",
                    "label": 0
                },
                {
                    "sent": "Then you have still a clustering problem, but you no longer have a setting where you can learn anything because you cannot make any predictions.",
                    "label": 0
                },
                {
                    "sent": "One cluster is the diameter.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I I haven't thought about these types of clustering principles so much, but I I don't think it's a it's a. Yeah, yeah yeah, but some of in class or distances have.",
                    "label": 0
                },
                {
                    "sent": "Sure, can they be?",
                    "label": 0
                },
                {
                    "sent": "This is some overall points because it's a function of pairs of points, not of individual.",
                    "label": 0
                },
                {
                    "sent": "OKOK you know I have different functions are over here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "OK OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what what I also would like to say, this is certainly not the most general way we found already out that this notation is disturbing.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I should remove it.",
                    "label": 0
                },
                {
                    "sent": "But there are also algorithms out there which basically gives you back.",
                    "label": 0
                },
                {
                    "sent": "Which gives you back a partition which are doing something good, but which cannot be written as as minimizing some costs.",
                    "label": 0
                },
                {
                    "sent": "So and in the back of our head we should still consider that that we also would like to measure the stability or measure some properties of these types of algorithms and.",
                    "label": 0
                },
                {
                    "sent": "In the end, we would like to get also for these.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these algorithms are learning scenario.",
                    "label": 0
                },
                {
                    "sent": "So so when you when you look at K means then you have this nice cost function in 4K means you can actually go quite a long way with with the analysis because conditioned on the wise these axes become independent and then you have these loss functions.",
                    "label": 0
                },
                {
                    "sent": "Here these are independent random variables so you can do the usual game and it works out but it works out up to the point.",
                    "label": 0
                },
                {
                    "sent": "That that you look at these loss function he ran, it's quadratic, so it's unbounded and you need different types of bounding techniques, and in particular you have to commit to a scale.",
                    "label": 0
                },
                {
                    "sent": "So so your remark with respect to scale is exactly related to the unboundedness of the loss function.",
                    "label": 0
                },
                {
                    "sent": "Now we also a second remark in classification one has seen that learning curves depend very strongly on assumptions on the on the on the distribution, the whole game of doing statistical mechanics.",
                    "label": 0
                },
                {
                    "sent": "For learning curves and seeing these phase transition was related to the fact that particular distributions give you structure, and since we're looking for structure, I question whether whether in a strong sense we can ever achieve a distribution independent result for clustering, because in some sense the structure defines a scale.",
                    "label": 0
                },
                {
                    "sent": "The scale defines at least a subset of potential distributions and we have to generalize the notion of distribution dependence.",
                    "label": 0
                },
                {
                    "sent": "So that's what I want to say in that respect.",
                    "label": 0
                },
                {
                    "sent": "So the optimal clustering solution here is the minimum.",
                    "label": 1
                },
                {
                    "sent": "It's the it's the nearest neighbour assignments.",
                    "label": 0
                },
                {
                    "sent": "If the guys are fixed, if the guys are not fixed and you let them depend on, see the definition of device, then you get a relatively ugly combinatorial optimization problem which is no longer polynomial in the assignments, but which becomes a rational function in the assignments.",
                    "label": 0
                },
                {
                    "sent": "Then it's getting a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "But there are also other.",
                    "label": 0
                },
                {
                    "sent": "Things that you have to consider.",
                    "label": 1
                },
                {
                    "sent": "One is the hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "So if you look in in books like the book Form Legosi.",
                    "label": 0
                },
                {
                    "sent": "Then you will see that they consider the vector quantization case, and in the vector quantization case you know that that the nearest neighbor assignments is the best, but you can do so, you have no uncertainty there, and the only uncertainty is where should you place your voice if you have the vector quantization case, you have finite VC, dimension it so learnable as long as soon as you actually allow yourself to assign the data point.",
                    "label": 0
                },
                {
                    "sent": "Either to the next or to the second next cluster you get already in hypothesis class, which has two to the N different possibilities.",
                    "label": 0
                },
                {
                    "sent": "And that is exactly the mechanism which allows you in the asymptotic limit.",
                    "label": 0
                },
                {
                    "sent": "When you have, when you have infinite amount of data.",
                    "label": 0
                },
                {
                    "sent": "Still toe to toe to entertain.",
                    "label": 0
                },
                {
                    "sent": "A number of different interpretations for your class string.",
                    "label": 0
                },
                {
                    "sent": "Because you might learn your centroids when you get an infinite amount of data, but you cannot learn an individual assignment of data to clusters.",
                    "label": 0
                },
                {
                    "sent": "And so, so if these are the variables which you optimize by an algorithm and you and you are not assuming like in vector quantization that you have hard assignments and you always use the nearest neighbor rule, then you get you get a situation where you have to come up with a new notion of what the solution to a learning problem is.",
                    "label": 0
                },
                {
                    "sent": "Trying to optimize your over the optimization is see an X.",
                    "label": 0
                },
                {
                    "sent": "If I write it like this because why is a strict function of C?",
                    "label": 0
                },
                {
                    "sent": "In the mixture model, but you are optimizing other centroids and these unobservable variables.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Help petitions is.",
                    "label": 0
                },
                {
                    "sent": "So we just optimize just rolled optimization objective just over the century, integrating over the labels.",
                    "label": 0
                },
                {
                    "sent": "Yes, but then you are not.",
                    "label": 0
                },
                {
                    "sent": "Then you are not.",
                    "label": 0
                },
                {
                    "sent": "How do you want to integrate them out?",
                    "label": 0
                },
                {
                    "sent": "What model are you assuming for them maximum entropy?",
                    "label": 0
                },
                {
                    "sent": "Difference in model over the labels are so we can to join model over the labels and centers, yes.",
                    "label": 0
                },
                {
                    "sent": "But but but you are, you OK, but you admit that that every pair of labels and centroids would be a different hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And for example, a Gibbs sampler is exploring these hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Looking like this, I'm going to look at the marginal only over the centuries, so now my my my model is just deciding objective two centroids OK and give me Central.",
                    "label": 0
                },
                {
                    "sent": "It's I'll tell you what the but still fun, but for mix for mixture models you would still estimate the probabilities of assigning data point OKOKOKOKOKOK.",
                    "label": 0
                },
                {
                    "sent": "For example, you might get rid of this problem in the context of clustering.",
                    "label": 0
                },
                {
                    "sent": "You are not getting rid of this problem.",
                    "label": 0
                },
                {
                    "sent": "When I tell you, now apply your insights from learning and generalization to ATSP problem where I give you the travel time between cities and they are uncertain.",
                    "label": 0
                },
                {
                    "sent": "And you should tell me what what, what, what, what, what a reasonable traveler actually does.",
                    "label": 0
                },
                {
                    "sent": "He he or she minimizes the expected travel times and it's not committing to the optimal travel time on one sample of of times.",
                    "label": 0
                },
                {
                    "sent": "So I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure computer scientist speak something, but still if I just have the centroids, I can still tell you the expected and I'm keeping the centrics.",
                    "label": 0
                },
                {
                    "sent": "My mother is just the centric, but you know when they answer queries like this form, I can use the expectations with respect to those centroids.",
                    "label": 0
                },
                {
                    "sent": "I don't have to too.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that I'm going to be assigning each point to the map century, I'm just saying that I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not worrying about the assignments at the moment.",
                    "label": 0
                },
                {
                    "sent": "We model just misses the centroid, but you have to.",
                    "label": 0
                },
                {
                    "sent": "Somehow you have to explore what is.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of assigning a particular data point to a particular centroid, and these values are either fixed by the model and then only the centroids are your problem or they.",
                    "label": 0
                },
                {
                    "sent": "Might be variable, and if you want to learn them then then you get this explosion of.",
                    "label": 0
                },
                {
                    "sent": "Because it's not the binary function.",
                    "label": 0
                },
                {
                    "sent": "I mean if if I if the number of clusters is not predetermined, that's right, then it's not a bandwidth.",
                    "label": 0
                },
                {
                    "sent": "Then what's relevant here is scale set.",
                    "label": 0
                },
                {
                    "sent": "Is that yeah, fetch header in dimensional, yeah.",
                    "label": 0
                },
                {
                    "sent": "Is there any I can find with certain a confidence, some approximation?",
                    "label": 0
                },
                {
                    "sent": "On the value of approximation and give you final sample.",
                    "label": 0
                },
                {
                    "sent": "But that is not the point I wanted.",
                    "label": 0
                },
                {
                    "sent": "I wanted to help you.",
                    "label": 0
                },
                {
                    "sent": "OK so you would like to remove this statement.",
                    "label": 0
                },
                {
                    "sent": "This problem is not solvable.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "What I'm advocating is the following.",
                    "label": 0
                },
                {
                    "sent": "Learnability in the sense that you give back an individual function, which is the result of your analysis, is is the traditional notion of a solution to a learning problem.",
                    "label": 0
                },
                {
                    "sent": "What you could also say is you have way too many.",
                    "label": 0
                },
                {
                    "sent": "You have way too many degrees of freedom in your hypothesis class you would like to keep these degrees of freedom because they give you flexibility.",
                    "label": 0
                },
                {
                    "sent": "So you relax your notion of a solution by saying there is a set of possible.",
                    "label": 0
                },
                {
                    "sent": "Explanations and these set is somehow characterized.",
                    "label": 0
                },
                {
                    "sent": "Then we have a representation problem, but this is ultimately your solution.",
                    "label": 0
                },
                {
                    "sent": "So if you use a Gibbs sampler for optimizing any kind of costs which are empirically defined, and you stop that Gibbs sampler at a finite temperature.",
                    "label": 0
                },
                {
                    "sent": "Then this Gibbs sampler gives you zillions of different solutions they all capture in as an ensemble, some properties of your data, and that's what I would like to keep.",
                    "label": 0
                },
                {
                    "sent": "So so I would like to keep a finite set of hypothesis which are compatible with with my data now.",
                    "label": 0
                },
                {
                    "sent": "But you are suggesting is this is a misspecified problem.",
                    "label": 0
                },
                {
                    "sent": "You have too many degrees of freedom before you even optimize commit to a particular model like the people did this vector quantization by saying the assignment has to be the nearest neighbor assignment.",
                    "label": 0
                },
                {
                    "sent": "Then you have a well specified problem and you get an unique answer.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is you pay a price for that.",
                    "label": 0
                },
                {
                    "sent": "You don't have any more flexibility in the assignments, which apparently seem to be important when we talk about clustering, because the cluster point at the boundary.",
                    "label": 0
                },
                {
                    "sent": "Is not.",
                    "label": 0
                },
                {
                    "sent": "This 100% generated by by by the nearest.",
                    "label": 0
                },
                {
                    "sent": "Neighbor OK yeah I I I since you are a theoretician, an I'm only trained physicist getting paid as a computer scientist.",
                    "label": 0
                },
                {
                    "sent": "You know this much better than me, so I am not committing to that.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is the notion of what people had in physics, that you have macroscopic variables in microscopic variables.",
                    "label": 0
                },
                {
                    "sent": "This notion directly comes into play here.",
                    "label": 0
                },
                {
                    "sent": "The microscopic variables are this assignment variables and they cannot be specified.",
                    "label": 0
                },
                {
                    "sent": "In the macroscopic variables in the asymptotic limit, get a direct distribution when they're properly defined, and then you can learn them.",
                    "label": 0
                },
                {
                    "sent": "That does not mean that you should throw away the microscopic variables, and I bet there are certainly some some relations to this random variables, because that's basically what you want.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what you get.",
                    "label": 0
                },
                {
                    "sent": "OK, I I I.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I told you already about this problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Damn it, what is this?",
                    "label": 0
                },
                {
                    "sent": "OK, I wanted to.",
                    "label": 0
                },
                {
                    "sent": "I wanted to come to this one so so this might be a partition you learn for clustering.",
                    "label": 0
                },
                {
                    "sent": "I I now colored the data points so that I actually can trace them.",
                    "label": 0
                },
                {
                    "sent": "So this is the second sample if you just use your cluster solution and this is your second sample, you see that a lot of data outside and so.",
                    "label": 0
                },
                {
                    "sent": "So the cluster structure is not not nicely sort of mapped to it.",
                    "label": 0
                },
                {
                    "sent": "The colors refer to three Gaussians from which I have chosen this data.",
                    "label": 0
                },
                {
                    "sent": "So obviously the shape of these clusters.",
                    "label": 0
                },
                {
                    "sent": "Is too complex to be determined by the algorithm for these type of data.",
                    "label": 0
                },
                {
                    "sent": "And that's an instability we would like to you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like to detect.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is now the essential picture, which which I would like to discuss and the rest is is basically something which which which I did a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "We haven't and solution space and for clustering this solution space has a size K to the N and I have a solution space for which I which I consider here for a trainings instance and this is the solution space for the test instance.",
                    "label": 0
                },
                {
                    "sent": "So I have two instances which I consider.",
                    "label": 0
                },
                {
                    "sent": "It's a two instance scenario.",
                    "label": 1
                },
                {
                    "sent": "Now what you do is you find the empirical risk minimization solution noted by the X here and then you have.",
                    "label": 0
                },
                {
                    "sent": "Then you have a set of of gamma close functions which which are gamma close to this.",
                    "label": 0
                },
                {
                    "sent": "Here now for this you have to define a distance in terms of your clustering costs.",
                    "label": 0
                },
                {
                    "sent": "For this solution space for the space of assignments.",
                    "label": 0
                },
                {
                    "sent": "If you use K means, for example, the distance from the nearest neighbor assignment is also exactly the costs of a particular clustering.",
                    "label": 0
                },
                {
                    "sent": "So in this in this, in the case where you have one function which is a complete complete minner and on all other functions on all other loss functions, then that's only true for the distance to the to the minimizer.",
                    "label": 0
                },
                {
                    "sent": "Then the costs are identical to.",
                    "label": 0
                },
                {
                    "sent": "To the distance.",
                    "label": 0
                },
                {
                    "sent": "So so this set here are all costs which are less than gamma verse in terms of my criterion than the best empirical minimizer.",
                    "label": 0
                },
                {
                    "sent": "So when you now consider.",
                    "label": 0
                },
                {
                    "sent": "These functions here and you and you would like to know how you apply such a partitioning to a second instance.",
                    "label": 0
                },
                {
                    "sent": "Then it's not really clear how to do that, and I guess almost independent Lee shy and and we came up with the notion of an extension.",
                    "label": 0
                },
                {
                    "sent": "You did that in your subsampling case and we did it in the context that that we actually were considering pairwise clusterings normalized cut.",
                    "label": 0
                },
                {
                    "sent": "For proteins and image applications, and it was absolutely unclear how I actually how I take a segmentation of one image and apply that to another image.",
                    "label": 0
                },
                {
                    "sent": "We are semantically I have the same objects in there.",
                    "label": 0
                },
                {
                    "sent": "I have a Tiger in there in a tree about the Tiger is in one image lying on the on on one of the branches of that of the tree and the other one it jumps from the tree.",
                    "label": 0
                },
                {
                    "sent": "So how would you actually copy such such a segmentation?",
                    "label": 0
                },
                {
                    "sent": "And if you don't know how to copy that then then the whole notion of generalization becomes meaningless.",
                    "label": 0
                },
                {
                    "sent": "So there you have to find such a 5 function here.",
                    "label": 0
                },
                {
                    "sent": "Now this actually might be distorted in this new space.",
                    "label": 0
                },
                {
                    "sent": "Here what you can also then consider is if you take your test instance.",
                    "label": 0
                },
                {
                    "sent": "And you and you.",
                    "label": 0
                },
                {
                    "sent": "And you do an empirical optimization on the test instance, something which you learn and normally can't do but.",
                    "label": 0
                },
                {
                    "sent": "But if I give you the second image, you just perform that, then you might get this.",
                    "label": 0
                },
                {
                    "sent": "This circle, which is partially overlapping with the image of this solutions when mapped to the new one.",
                    "label": 0
                },
                {
                    "sent": "And instability somehow measures how how different are.",
                    "label": 0
                },
                {
                    "sent": "Are these sets?",
                    "label": 0
                },
                {
                    "sent": "At least that's what you would expect if I generalize my solution to a new instance and it has a large overlap with what I would do with an approximation on that new instance, then I would consider it stable.",
                    "label": 0
                },
                {
                    "sent": "Then obviously the noise hasn't hasn't bothered me too much.",
                    "label": 0
                },
                {
                    "sent": "So, so that is that is the issue at the level of of of instances.",
                    "label": 0
                },
                {
                    "sent": "No here, for example, I assumed that the empirical minimizer on my training data is actually not in the epsilon approximation or gamma approximation on the test data.",
                    "label": 0
                },
                {
                    "sent": "It it it lies somewhere out here.",
                    "label": 0
                },
                {
                    "sent": "So what what is missing in that picture in that picture is missing and and now I I I would like to refer to your knowledge about information theory and coding theory.",
                    "label": 0
                },
                {
                    "sent": "What I would like to so far we have we have two instances X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "These are my datasets.",
                    "label": 0
                },
                {
                    "sent": "Which why are my clustering cost function define my optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So I have two optimization problems and I assume that these two data.",
                    "label": 0
                },
                {
                    "sent": "Coming from a souls.",
                    "label": 0
                },
                {
                    "sent": "You know they in some sense that I should be talking about the same same problems which are only different because of fluctuations.",
                    "label": 0
                },
                {
                    "sent": "So, so this defines an instance.",
                    "label": 0
                },
                {
                    "sent": "An optimization in this defines an optimization instance, this space.",
                    "label": 0
                },
                {
                    "sent": "Is much much larger usually than this space.",
                    "label": 0
                },
                {
                    "sent": "So for example, this space in clustering is K to the N. This phase in correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "Since I have these correlation matrices as possible optimization instances is.",
                    "label": 0
                },
                {
                    "sent": "Is if I have binary correlations, is 2 to the N ^2?",
                    "label": 1
                },
                {
                    "sent": "So my instance space cardinality is 2 to the N squared and my solution space.",
                    "label": 0
                },
                {
                    "sent": "I think I called it, you see is is K to the end.",
                    "label": 0
                },
                {
                    "sent": "So so, So what I have to cope with this that my variation?",
                    "label": 0
                },
                {
                    "sent": "Comes up here in the instant space.",
                    "label": 0
                },
                {
                    "sent": "I perturb my data.",
                    "label": 0
                },
                {
                    "sent": "But it's some process here and then I get a test instance on which I would like to test my learn solution.",
                    "label": 0
                },
                {
                    "sent": "So since this space is much larger, I don't really know which of the degrees of freedom in that space are actually relevant to change a solution down here.",
                    "label": 0
                },
                {
                    "sent": "So what I have to find is I have to find an equivalence class partitioning of that space in huge equivalence classes, which then are so different that they actually make a difference in the solutions.",
                    "label": 0
                },
                {
                    "sent": "And this is an issue here because the solution space is tiny compared to my instant space, so a lot of degrees of freedom in the instance space are completely irrelevant.",
                    "label": 0
                },
                {
                    "sent": "A four for determining a solution.",
                    "label": 0
                },
                {
                    "sent": "So the idea I think, which one could draw from from from information theory, is.",
                    "label": 0
                },
                {
                    "sent": "That you try to find such spheres in the solution space.",
                    "label": 0
                },
                {
                    "sent": "Bless you.",
                    "label": 0
                },
                {
                    "sent": "And you cover your solution space with these fields.",
                    "label": 0
                },
                {
                    "sent": "You define all functions which are inside such as fear as statistically equivalent.",
                    "label": 0
                },
                {
                    "sent": "You can't distinguish between them, but if you go from one sphere to the next sphere, then you say.",
                    "label": 0
                },
                {
                    "sent": "Now I have a statistically different function I should care about.",
                    "label": 0
                },
                {
                    "sent": "Then you ask yourself, OK, how can I now using this feels down here.",
                    "label": 0
                },
                {
                    "sent": "It's basically an epsilon covering type of argument.",
                    "label": 0
                },
                {
                    "sent": "How can I use this?",
                    "label": 0
                },
                {
                    "sent": "Feels here to partition that space up there and there I would like to to refer to information theory where you basically have a codebook, you take a codebook vector, you transmit that codebook vector.",
                    "label": 0
                },
                {
                    "sent": "The codebook vector is then corrupted, and then you compare, which is an optimization this codebook vector.",
                    "label": 0
                },
                {
                    "sent": "The received the received message vector with your codebook and you do a minimization.",
                    "label": 0
                },
                {
                    "sent": "So you have the instance space, but for every instance you have a different solution space, because then the space of clustering of this particular sample, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You want to map now.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 1
                },
                {
                    "sent": "Subsets of that solution spaces back in place this place, but you have.",
                    "label": 0
                },
                {
                    "sent": "How do you do it?",
                    "label": 0
                },
                {
                    "sent": "It was the same solution, yes, yes, yes you get.",
                    "label": 0
                },
                {
                    "sent": "You get as many instances as you consider.",
                    "label": 0
                },
                {
                    "sent": "You get potentially different solution spaces, but since I assume that I have a function Phi, I know how to map them to each other and if we talk about the whole business in Euclidean spaces and I have K means, then this file is the identity matrix or this fine might be a very simple function like the nearest neighbour assignment function.",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, the the equivalence class of the of the assignments.",
                    "label": 0
                },
                {
                    "sent": "The whole.",
                    "label": 0
                },
                {
                    "sent": "Infinite yeah yes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The bottom part of the figure I think you assume.",
                    "label": 0
                },
                {
                    "sent": "Gamma closeness in terms of costs, yes.",
                    "label": 0
                },
                {
                    "sent": "At least informally related to instability so, but yes.",
                    "label": 0
                },
                {
                    "sent": "OK yes yes, yes this is this is this is the point which I wanted to make in the beginning that if you have if you have one clustering which is in costs.",
                    "label": 0
                },
                {
                    "sent": "Pill object clearly better than any other clustering.",
                    "label": 0
                },
                {
                    "sent": "You have 'em in a runt of it.",
                    "label": 0
                },
                {
                    "sent": "Then if I consider that Minner and as an anchor point, which is the empirical risk minimizer, that's that X.",
                    "label": 0
                },
                {
                    "sent": "Then cost differences of any other point in here too.",
                    "label": 0
                },
                {
                    "sent": "That is also a distance.",
                    "label": 0
                },
                {
                    "sent": "Incense in the sense of an L1 loss.",
                    "label": 0
                },
                {
                    "sent": "Now if I take a point here in a point here and I calculate the distance in the L1 sense, this for sure is no longer a difference in energies.",
                    "label": 0
                },
                {
                    "sent": "So, so it the notion of having of having a distance measure in the solution space and considering cost differences only coincide when you are very close to the global minimizer.",
                    "label": 0
                },
                {
                    "sent": "Sort of sort of the last ball, which you can define around the global minimizer also gives you.",
                    "label": 0
                },
                {
                    "sent": "Up to maybe some pathologies of your of your cost function.",
                    "label": 0
                },
                {
                    "sent": "It gives you a bound on the distance if you are far away you have these.",
                    "label": 0
                },
                {
                    "sent": "You have these energy layers.",
                    "label": 0
                },
                {
                    "sent": "These cost layers where you have functions which are very far apart from each other.",
                    "label": 0
                },
                {
                    "sent": "But they have the same costs, so their cost differences are zero.",
                    "label": 0
                },
                {
                    "sent": "So So what you have to do is you basically have to consider feels around prototypical new minimizers of other problems of other clustering problems, which in some sense are equivalent to this clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So my proposal is that you start, you take your clustering, propose you click the clustering cost function, you permute the indices.",
                    "label": 0
                },
                {
                    "sent": "So you basically set problems all over here in your solution space and then you determine the size of these spheres as an approximation quantity in such a way.",
                    "label": 0
                },
                {
                    "sent": "That when you apply the perturbation due to your sampling draws that you stay in the same field.",
                    "label": 0
                },
                {
                    "sent": "And this would give you something like in information theory.",
                    "label": 0
                },
                {
                    "sent": "This is called the mutual information, because it gives you the sphere covering of Hamming space.",
                    "label": 0
                },
                {
                    "sent": "Here you would have something like approximation capacity, because if I if this perturbation in my instance space is huge, then for sure I can only fill in a few balls here, otherwise I will already go from one bowl to the next one.",
                    "label": 0
                },
                {
                    "sent": "If there is no noise in here.",
                    "label": 0
                },
                {
                    "sent": "Any problem will be different because my deterministic algorithm, if it runs well enough, will allow me to find that solution.",
                    "label": 0
                },
                {
                    "sent": "So how do you do that now?",
                    "label": 0
                },
                {
                    "sent": "Now basically you have to calculate.",
                    "label": 0
                },
                {
                    "sent": "If I take this problem, which is my starting problem.",
                    "label": 0
                },
                {
                    "sent": "I convert this problem by permuting the indices of my objects in a random way.",
                    "label": 0
                },
                {
                    "sent": "I calculate a new problem which has a minimizer here.",
                    "label": 0
                },
                {
                    "sent": "Then I have to I have to basically now calculate the overlap of these tools, spheres and I would expect like in information theory that this overlap is the size of the of this field time.",
                    "label": 0
                },
                {
                    "sent": "Diameter grows, you get basically a threshold phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Either they're completely overlapping or they're completely distinct, and this threshold phenomenon comes from the fact that you have these.",
                    "label": 0
                },
                {
                    "sent": "Exponentially many degrees of freedom from the assignment variable, so the statistical mechanics trick, which is basically a large deviation phenomenon, helps you to make clear how you have to sample that.",
                    "label": 1
                },
                {
                    "sent": "Now what is?",
                    "label": 0
                },
                {
                    "sent": "What is an additional benefit from from a computer science point of view?",
                    "label": 0
                },
                {
                    "sent": "This is sort of a principle for approximation.",
                    "label": 0
                },
                {
                    "sent": "Now now if you if you, I'm probably running out of time.",
                    "label": 0
                },
                {
                    "sent": "If you if you use approximation algorithms in the setting of approximation theoretical computer science, you have no guarantee that this algorithm gives you, in any sense a typical solution back.",
                    "label": 0
                },
                {
                    "sent": "This algorithm just gives you a solution back which is below your threshold.",
                    "label": 0
                },
                {
                    "sent": "Nothing else is proven.",
                    "label": 0
                },
                {
                    "sent": "However, we know from applications that in costs accepting a solution which is a factor of two worlds, then the best one is basically being out of the game.",
                    "label": 0
                },
                {
                    "sent": "Now in clustering we have very strong indications that these approximation factors sometimes are above the highest phase transition.",
                    "label": 0
                },
                {
                    "sent": "So basically only throwing all the clusters in the same in the same doing all the objects in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "Is already a solution which would be admissible from the approximation bar, so we really have to rely on typical solutions which we sample and not on solutions which are in some sense just artificial.",
                    "label": 0
                },
                {
                    "sent": "Now information theory tells you how to get these typical solutions because you get this, you get this set of solutions back.",
                    "label": 0
                },
                {
                    "sent": "You can average over it and and.",
                    "label": 0
                },
                {
                    "sent": "And and and then you and then you.",
                    "label": 0
                },
                {
                    "sent": "And then you can sample from such a solution.",
                    "label": 0
                },
                {
                    "sent": "All belief propagation algorithms essentially do some kind of averaging over this over this.",
                    "label": 0
                },
                {
                    "sent": "Overly set.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is the picture I have in mind.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Because what is on the rest of the slide?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what I did in terms of calculations, so this is a summary so, So what I would like to do is you you treat, but this is now more specific for clustering you did you.",
                    "label": 0
                },
                {
                    "sent": "You treat your data partition as a key area code.",
                    "label": 1
                },
                {
                    "sent": "Then you will communicate and that's important.",
                    "label": 0
                },
                {
                    "sent": "You communicate instances.",
                    "label": 1
                },
                {
                    "sent": "Because the instances are perturbed and not the not my code.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You have to weave into that communication process the optimization which has to be done by the by the by the receiver.",
                    "label": 0
                },
                {
                    "sent": "And if you have a very delicate optimization problem, then receiver cannot reconstruct the OR identify the instance from a whole codebook of different instances, which the sender might have used.",
                    "label": 0
                },
                {
                    "sent": "Then you have to use a crude approximation level.",
                    "label": 0
                },
                {
                    "sent": "I communicate with you by sending you an instance.",
                    "label": 0
                },
                {
                    "sent": "OV OV.",
                    "label": 0
                },
                {
                    "sent": "Basically we basically draw from that Oracle.",
                    "label": 0
                },
                {
                    "sent": "I get an instance, you get an instance so that these instances generated by the same source.",
                    "label": 0
                },
                {
                    "sent": "And I have to and I have to.",
                    "label": 0
                },
                {
                    "sent": "I have to specify now my approximation level in such a way that you have a chance to identify these souls.",
                    "label": 0
                },
                {
                    "sent": "Because this source.",
                    "label": 0
                },
                {
                    "sent": "This is one of the codebook problems where we draw two instances.",
                    "label": 0
                },
                {
                    "sent": "Here there might be another codebook problem here.",
                    "label": 0
                },
                {
                    "sent": "Write another book.",
                    "label": 0
                },
                {
                    "sent": "Be a code problem, and so on.",
                    "label": 0
                },
                {
                    "sent": "You might have many different types of code book problems because we we partition this space in equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "Now, now somebody.",
                    "label": 0
                },
                {
                    "sent": "Are you saying you are trying to transmit a number between one and K to me?",
                    "label": 0
                },
                {
                    "sent": "No, I I I the message would be that that you have to communicate an index for one of these codebook problems here.",
                    "label": 0
                },
                {
                    "sent": "Communicating to me an index and index, yes, but you at the end get a problem to be optimized.",
                    "label": 0
                },
                {
                    "sent": "No, that is that that is, that is, that is a number between one and two to the NR where are defined some kind of rate.",
                    "label": 0
                },
                {
                    "sent": "It's basically it's basically the number of spheres which we can place on the solution space in such a way that if I if somebody maliciously perturbs of a problem, I don't know exactly what problem you got anymore because the perturbation is already large enough that I might confuse it with another problem.",
                    "label": 0
                },
                {
                    "sent": "It's like like you get the vector we want to talk about the same codebook vector.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately you have the codebook vector as a send and I have.",
                    "label": 0
                },
                {
                    "sent": "Only a noisy copy of it.",
                    "label": 0
                },
                {
                    "sent": "Rating mechanism.",
                    "label": 0
                },
                {
                    "sent": "We both we both.",
                    "label": 0
                },
                {
                    "sent": "We sort of get from an Oracle.",
                    "label": 0
                },
                {
                    "sent": "The problem generating mechanism.",
                    "label": 0
                },
                {
                    "sent": "Yeah, data in problem is identical because I we community commit to a clustering problem.",
                    "label": 0
                },
                {
                    "sent": "The instance is defined by the data and the optimization is weaved into that.",
                    "label": 0
                },
                {
                    "sent": "Approximation here.",
                    "label": 0
                },
                {
                    "sent": "So so if you do that, then you relat the combinatorial quantity.",
                    "label": 0
                },
                {
                    "sent": "How many spheres can you place with a certain distance in the distance comes from how you write down your cost function.",
                    "label": 0
                },
                {
                    "sent": "You leave that that covering problem.",
                    "label": 0
                },
                {
                    "sent": "Into a bound on on the optimization.",
                    "label": 0
                },
                {
                    "sent": "So, so why does that?",
                    "label": 0
                },
                {
                    "sent": "Anything has to do with annealing or these types of algorithms, or the temperature which which you don't understand.",
                    "label": 0
                },
                {
                    "sent": "It has something to do because if I define this set, I basically write down.",
                    "label": 0
                },
                {
                    "sent": "If I want to know what is the cardinality of of C gamma one, I write it as a sum over all my my partitions.",
                    "label": 0
                },
                {
                    "sent": "And then I have a theater.",
                    "label": 0
                },
                {
                    "sent": "R. Min minus.",
                    "label": 0
                },
                {
                    "sent": "I'll see.",
                    "label": 0
                },
                {
                    "sent": "Plus, so as long as my costs.",
                    "label": 0
                },
                {
                    "sent": "Are smaller than the minimal costs plus an approximation quantity gamma.",
                    "label": 0
                },
                {
                    "sent": "I count this function, otherwise I discard that function.",
                    "label": 0
                },
                {
                    "sent": "So if you write down this.",
                    "label": 0
                },
                {
                    "sent": "Then what you will get is you will get and you didn't want to sample from that set from the tsetse gamma.",
                    "label": 0
                },
                {
                    "sent": "You get the Gibbs distribution as the sampling procedure, which is maximum entropy on that.",
                    "label": 0
                },
                {
                    "sent": "So the temperature is just the LaGrange parameter to ensure this threshold.",
                    "label": 0
                },
                {
                    "sent": "To ensure this radius here.",
                    "label": 0
                },
                {
                    "sent": "It's nothing more than that and it has to come into the game because we have to commit to equality.",
                    "label": 0
                },
                {
                    "sent": "So the modeling part comes into the play when you define the distance of these fields which cover your solution space.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess.",
                    "label": 0
                },
                {
                    "sent": "Let me just go over this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the end, what I would like to get this, you know relation like this.",
                    "label": 0
                },
                {
                    "sent": "There is something like the entropy.",
                    "label": 0
                },
                {
                    "sent": "There is something like the energy the energy is is is the is the the size of that maximal radius of that sphere because basically all the solutions are at the boundary of that sphere.",
                    "label": 0
                },
                {
                    "sent": "So the average energy over the sphere is the average energy over the over the shell.",
                    "label": 0
                },
                {
                    "sent": "The entropy counts how many functions I have there.",
                    "label": 0
                },
                {
                    "sent": "An if I differentiate this this this combinatorial quantity with respect to the radius of my sphere, then I get the inverse temperature.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the relationship which you know from statistical mechanics.",
                    "label": 0
                },
                {
                    "sent": "So the temperature is nothing but the combinatorial quantity over States and these states have in physics and energy semantics.",
                    "label": 0
                },
                {
                    "sent": "In optimization they have an optimization semantics and so on.",
                    "label": 0
                },
                {
                    "sent": "Now large deviation bounds.",
                    "label": 0
                },
                {
                    "sent": "If you put them together, but I would expect that it depends on the cardinality of your space, which you can use for defining problems divided by the cardinality of of.",
                    "label": 0
                },
                {
                    "sent": "Of Of your sphere.",
                    "label": 0
                },
                {
                    "sent": "So if gamma is large, this will be large.",
                    "label": 0
                },
                {
                    "sent": "Then I will have a small factor.",
                    "label": 0
                },
                {
                    "sent": "Here the inverse temperature has to be small.",
                    "label": 0
                },
                {
                    "sent": "The temperature has to be large, so it's exactly this tradeoff between between small noise being able of having small spheres to cover your solution space and and then being able to approximate to a very small level of.",
                    "label": 0
                },
                {
                    "sent": "Precision, but these algorithms, which are basically in the spirit of Gibbs sampling, would guarantee you that you get typical solution with a very low probability.",
                    "label": 0
                },
                {
                    "sent": "You get an atypical solution.",
                    "label": 0
                },
                {
                    "sent": "And that's what you live on in these approximations.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Picture which.",
                    "label": 0
                },
                {
                    "sent": "Which I know so so I have no time.",
                    "label": 0
                },
                {
                    "sent": "OK, so so why is that useful?",
                    "label": 0
                },
                {
                    "sent": "Why do we actually care about the better approximation technique than what we have so far?",
                    "label": 0
                },
                {
                    "sent": "Well, a typical problem which which comes from image analysis is the following.",
                    "label": 0
                },
                {
                    "sent": "You have different sort of different scales in image analysis.",
                    "label": 1
                },
                {
                    "sent": "There's the data scale just defined by the resolution.",
                    "label": 0
                },
                {
                    "sent": "There is and this is now a segmentation problem in some sense of segmentation.",
                    "label": 0
                },
                {
                    "sent": "Problem is especially ordered clustering problem where you have a situation like you you described you cannot so easily pull out an object and write it.",
                    "label": 0
                },
                {
                    "sent": "Write the costs as a sum of independent components.",
                    "label": 0
                },
                {
                    "sent": "OK, you have another scale, and that's the approximation scale.",
                    "label": 0
                },
                {
                    "sent": "How value should approximate a solution here?",
                    "label": 0
                },
                {
                    "sent": "And then you have the model order.",
                    "label": 1
                },
                {
                    "sent": "The scale.",
                    "label": 0
                },
                {
                    "sent": "How many classes should you use now?",
                    "label": 0
                },
                {
                    "sent": "Why is the approximation?",
                    "label": 0
                },
                {
                    "sent": "Of offer of of a set of solutions by a set of solution solution to the model to the, to the to the model model or the problem.",
                    "label": 0
                },
                {
                    "sent": "This segmentation, as this segmentation?",
                    "label": 0
                },
                {
                    "sent": "Comes out as an interpretation of a Let's let's go to here.",
                    "label": 0
                },
                {
                    "sent": "This is a segmentation in two different classes.",
                    "label": 0
                },
                {
                    "sent": "You see the segment up here in the segment down here.",
                    "label": 0
                },
                {
                    "sent": "The colors indicate the assignment of a Pixel 21 cluster.",
                    "label": 0
                },
                {
                    "sent": "So the fact that you have different colors in here but totally randomly discribed only means that the clustering algorithm has basically two clusters, one in the upper part for the Sky in running the lower part for the rest.",
                    "label": 0
                },
                {
                    "sent": "Now between the different clusters.",
                    "label": 0
                },
                {
                    "sent": "You do not distinguish here.",
                    "label": 0
                },
                {
                    "sent": "This is a frozen instance of the Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "If you run it longer then you get a completely different picture.",
                    "label": 0
                },
                {
                    "sent": "You still get all the yellow stuff up here in all the blueish greenish stuff down here.",
                    "label": 0
                },
                {
                    "sent": "But but but otherwise all these segmentations are independent and the result is from using such a maximum entropy type procedure that if I now calculate the means in color space for my for my for my distribution down here, I have three.",
                    "label": 0
                },
                {
                    "sent": "I have three Gaussians because we model this Wisconsin and have three girls since down here, which which basically have.",
                    "label": 0
                },
                {
                    "sent": "The same mean.",
                    "label": 0
                },
                {
                    "sent": "So I three times a degenerate distribution.",
                    "label": 0
                },
                {
                    "sent": "If I reduce my temperature, I see how structure is unfolding.",
                    "label": 0
                },
                {
                    "sent": "I see more and more of the combinatorial structure in my data.",
                    "label": 0
                },
                {
                    "sent": "And here this is very low temperature, so here you get the five.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization scale.",
                    "label": 0
                },
                {
                    "sent": "The Model order scale.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to couple these scales.",
                    "label": 0
                },
                {
                    "sent": "People have no clue they have zillions of different heuristics, but there is no principled way of doing that.",
                    "label": 0
                },
                {
                    "sent": "Such a theory, which which tells you something about robust optimization which comes from from a quantization of your solution space, which then is mapped to the instant space to tell you what are significantly different problems, so that you actually should provide representation, power or representation degrees of freedom to describe them.",
                    "label": 0
                },
                {
                    "sent": "Such a theorie would couple these data space, which basically tells you something about the fluctuations.",
                    "label": 0
                },
                {
                    "sent": "To the to the, to the, to the to the cost approximation scale, which basically tells you how your interpretation pays attention to the fluctuations here and to the model order scale.",
                    "label": 0
                },
                {
                    "sent": "How you control within the setting of a cost function the degrees of freedom.",
                    "label": 1
                },
                {
                    "sent": "And that's what we need in practice, because that is a very relevant problem.",
                    "label": 0
                },
                {
                    "sent": "And then there comes the other observation.",
                    "label": 0
                },
                {
                    "sent": "When the problem is well formulated, like in this case, or in this case, usually it's extremely simple to find the solution.",
                    "label": 0
                },
                {
                    "sent": "So the algorithmic complexities also weaved into that because.",
                    "label": 0
                },
                {
                    "sent": "When I go from small spheres to very large spheres, I basically have a.",
                    "label": 0
                },
                {
                    "sent": "A continuity method which which which takes a discrete optimization problem and by averaging their problem embedded in a family of simpler problem up to when I have only one cluster, then I have a entropy driven convex optimization problem which basically gives me the meaningless and so that all classes are the same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi this is the rapper.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, we didn't settle on this one.",
                    "label": 0
                },
                {
                    "sent": "I think I think that the frame theorie which we are missing also in in clustering should give you the optimal tradeoff between stability and informative ITI.",
                    "label": 0
                },
                {
                    "sent": "But if I if I have, if I have a hypothesis class, we are potentially I have 100 clusters, But I have an averaging procedure such that the degrees of freedom for these hundred clusters, because it will be a typical, never really express and I commit only to a set of solutions which I treat them as equivalently so.",
                    "label": 0
                },
                {
                    "sent": "So there would be a Bayesian view how you could superimpose that.",
                    "label": 0
                },
                {
                    "sent": "Then I would have automatically solve this tradeoff between informative itean stability.",
                    "label": 0
                },
                {
                    "sent": "And and the breakpoint when I have too much noise in my problem instance generation process and I want to approximate two precisely is basically that that that people who draw the same problem.",
                    "label": 0
                },
                {
                    "sent": "But with two instances from a soul from a problem, source will come up with different solutions and they will decide they are statistically different.",
                    "label": 0
                },
                {
                    "sent": "Because not the empirical minimizer is the solution.",
                    "label": 0
                },
                {
                    "sent": "The set coming with the empirical minimizer is the solution, and there will be no overlap between these two solution sets anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "Tell it's just an idea.",
                    "label": 0
                },
                {
                    "sent": "Actions regarding this.",
                    "label": 0
                },
                {
                    "sent": "Everybody is sick, so.",
                    "label": 0
                },
                {
                    "sent": "So let me let me just take the stage and thank the organizers for bringing it together.",
                    "label": 0
                },
                {
                    "sent": "It's also the last the last talk.",
                    "label": 0
                },
                {
                    "sent": "I guess there is still a afternoon brainstorming session, but people probably will will diffuse quickly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I hope.",
                    "label": 0
                },
                {
                    "sent": "I hope that that the expectations are met at least partially and.",
                    "label": 0
                },
                {
                    "sent": "Next, deadlines are approaching.",
                    "label": 0
                },
                {
                    "sent": "Well, actually I I would have I. I would hope that that this is not sort of deadline driven research because I think machine learning really has to tell a fundamental lesson to computer science and it's related to the fact that that if you neglect noise, you might conceptually have simpler problems, like in optimization.",
                    "label": 0
                },
                {
                    "sent": "But you also start asking questions which become unsolvable and at least for the practice, it does not make a difference.",
                    "label": 0
                },
                {
                    "sent": "So, so pay attention to this variability and then you will gain automatically relevance from practice because everybody is an engineer is usually able to do measurements and give you some uncertainty values on their measurements.",
                    "label": 0
                },
                {
                    "sent": "So identifying these variability of the instances is usually something where you can quite easily commit to in discussions.",
                    "label": 0
                },
                {
                    "sent": "The modeling problem is not solved.",
                    "label": 0
                },
                {
                    "sent": "But but at least this this will give give some ideas.",
                    "label": 0
                }
            ]
        }
    }
}