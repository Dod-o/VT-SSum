{
    "id": "xwadebn3yfjeridnpnsct7pxxd36a5rm",
    "title": "Boosting statistical network inference by incorporating prior knowledge from multiple sources",
    "info": {
        "author": [
            "Paurush Praveen, Bonn-Aachen International Center for Information Technology, University of Bonn"
        ],
        "published": "Oct. 23, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/mlsb2012_praveen_boosting/",
    "segmentation": [
        [
            "I'm going to talk about the method that that we recently developed and we are growing are currently updating it further to confirm to get the quantitative quantified prior from the existing biological knowledge.",
            "The knowledge that we already have in biological databases or in terms of ontological terms.",
            "So this is about the stories about that."
        ],
        [
            "So in representation first I will.",
            "I would like to tell you what inspired us to do this study and then followed by the approach.",
            "What approaches do we use to integrate different knowledge sources into one quantified fired prior and then we talk about how we tested whether our method works correctly and validated it.",
            "And finally I will talk about the application."
        ],
        [
            "So as we know, we are very much into creating networks out of high throughput data.",
            "We have a lot of data microarray data.",
            "We generate data and then we try to use our problem.",
            "Probabilistic methods like Mission, Network to create networks out of it.",
            "But there is a problem with this.",
            "The micro read the high throughput data we have.",
            "We have a lot of noise and the sample size is quite low.",
            "Because of this, what we face in the networks created we have very high false positive and false negatives.",
            "So we have to overcome somehow this false positive and false negatives."
        ],
        [
            "To overcome this, it has been proposed by many people that if we try to integrate data from information knowledge from different databases and like literature, ontologies an interaction databases like we have HPHP ID, string etc.",
            "If we somehow tried to get all this information together and put it in our learning method, then our method probably will work better.",
            "So we need a quantified prior.",
            "From existing biological knowledge and if we once we try to use this prayer into our learning method, it is very, very hard.",
            "It is expected that it will reduce the false positives and false negatives and at the same time it was or it will also reach a network which is more closer to the real biological world and explains the biological phenomenon in a better way."
        ],
        [
            "But we have some problems here.",
            "If we try to integrate data from different sources, what we have the issue is that the data is very hetrogenous.",
            "That means we have different kinds of data.",
            "We have interaction data we have in Toronto logical information.",
            "We have literature information.",
            "So these are completely diverse, very different kind of data and integrating them together is difficult.",
            "The next issue that comes up is.",
            "We have missing information.",
            "Alot of annotation is missing the info Internet interaction.",
            "Different interaction databases are not complete so we have a lot of lot of holes in our knowledge which we need to fill.",
            "And the next thing will be numerical handlings by numerical handling.",
            "I would say as I as I mentioned earlier, we need a quantified prior, but for this we need a quantified knowledge as well.",
            "How to get that quantified knowledge?"
        ],
        [
            "So before I start with them with my approach is I would like to explain what exactly how we want to use our prayer.",
            "We all know the base through here and we need a product to learn the network some priority information to learn the network.",
            "If we considered for a network that every edge is independent of each other and then the overall prior for the network will be the product of each edge in the network.",
            "It was proposed that this kind of prior was proposed by my foolish and coworkers as a function of the end of the individual individual edges here.",
            "What up rival do approval of learning the network from data prior will try to bias the networks in the infinite, the network being inferred towards to consider the already known interaction.",
            "It will also learn new edges, but it will also see if there's some edges are really existing.",
            "They are known in the biological world."
        ],
        [
            "So what is our idea?",
            "So our idea is first to assemble different knowledge sources.",
            "Like what kind of sources can we use for our quantified priors?",
            "What are the sources of information once we have assembled these sources of information, we need to get the edge confidence we need to get some numbers because we need finally a quantified press.",
            "So we need an edge conference.",
            "That's what we call it an edge confidence for every interaction in the network.",
            "After getting this edge confidence, we finally we need to combine all these edge confidences from different sources into one quantified prior.",
            "Once we have a quantified prior, we can test, validate and finally go for an application."
        ],
        [
            "But what sources should we consider?",
            "There can be many sources.",
            "Second, how to get these confidence interval confidence numbers?",
            "What confidence for each interaction?",
            "How to get these numbers?",
            "And finally how to combine all these information?",
            "All this knowledge into one prior."
        ],
        [
            "So let's go with the first thing.",
            "What information sources we consider here.",
            "So we consider interaction databases.",
            "Of course, like Patrick Common string HP ID for as they provide the interaction between different proteins, binary interactions.",
            "Then we consider Gene ontologies we consider if some terms in Gene ontology similar have high degree of similarity.",
            "They're more likely that we have more confidence in the edge that can occur between them.",
            "Next we consider the similarity in domains of the proteins.",
            "So we and we also consider the interacting domain information.",
            "If we have a pair of protein they will have we can form from every domains present in each of the protein.",
            "We can form pairs and if these domains this pair of domain can really interact with each other and finally we can also consider the interactions that are already mentioned in the literature."
        ],
        [
            "So, but once we have the sources, we need to see how to find the edge confidence.",
            "So for any kind of interaction graph, we take the data the data from the interaction databases and for this right now what we're using is inverse of the shortest path length.",
            "So if this path is very short, the confidence is higher.",
            "If the path is long, the confidence is low.",
            "But now we're trying to update it to the graph diffusion kernel.",
            "But in the current presentation I will be showing the results from this first part, that is the short spot length.",
            "The next next thing is the ontological terms.",
            "So we try to compute the similarities in the associated GO terms, mainly biological processes here.",
            "Which was proposed by Lynn in 1989 and next is the protein domain data.",
            "So for protein domain data we have domain signature interpret domain signature for each protein and we tried to find the similarities across these signatures.",
            "And finally, the other form is interacting domain data.",
            "As I mentioned earlier.",
            "So we have pairs of proteins and reform all the possible combinations of the domains present in them, and we try to find how many of them are really can really interact.",
            "We have a database for that called domain database.",
            "We use this to search the frequency relative frequency for this."
        ],
        [
            "Now I will go once I have the sources assembled together, then I have the confidence intervals confidence numbers.",
            "Then I go how to combine these all these different sources into one quantified prior.",
            "We here propose two different models for this for this and the first one we call it a Latin factor model or elephant in short.",
            "For this model we have an assumption that all the knowledge, whatever we have in biology comes from a really from a true network, which we don't know yet, but it actually arises from a true network.",
            "Which we call here 5.",
            "So and the if this information sat together, they originate from this fight.",
            "There very highly correlated.",
            "These information sources are highly correlated.",
            "They originate from this true network.",
            "Once we have this, this true network is a Latin variable for us and we try to learn this truth but unknown network using our information sources that we assembled in.",
            "In the final and the previous stages.",
            "And we compute you compute this.",
            "Find the true network like this without using Bashan rule."
        ],
        [
            "Now there are some other things that I would like to tell about this model.",
            "We we have an assumption that the information in the knowledge sources follows a beta distribution.",
            "As you can see in this, this is the beta distribution here.",
            "And the log likelihood for the for the network is given by this equation here.",
            "So we have we consider it to be a beta distribution as you see here with the shape parameters Alpha and beta.",
            "To learn the overall prior network, what we use is a special kind of MCMC, Markov chain Monte Carlo approach using Metropolitan Hastings algorithm.",
            "And it is a nested MCMC.",
            "I will explain the nested in this slide self first.",
            "What kind of operation do we use to change change the network every time during the answer steps?",
            "These are the essence of steps here like edge insertion, deletion and reversal.",
            "So we should try to insert a new edge or delete an existing edge or reverse the direction of the edge.",
            "So we start from an arbitrary network and we do these operations too.",
            "Damn to maintain this nested approach in the sense that we modify our Alpha beta on every iteration after certain iteration, we modify change our Alpha.",
            "Beta values is also leads to a faster convergence.",
            "Of the MCMC algorithm."
        ],
        [
            "The next approach is quite called noisy or model, so it's based on the noisy or logic.",
            "And the model looks in terms.",
            "It looks a bit different from the previous model, but it says that we have causes the information sources that we collected in the previous stages are the causes and leading to a network to quantify the quantified value of the edge confidences, which is fine.",
            "Again here, this is the effect.",
            "So it says that the probability of each cost to be sufficient to prove the effect is independent of the presence of others.",
            "This leads to that when we try to combine these all these causes into an effect.",
            "It follows a noisy or function and then the overall network can be given as the situation here.",
            "So overall prior knowledge is our effect here.",
            "And finally, what we get the cause that causes, as I mentioned before, is the confidence in each edge of the network."
        ],
        [
            "So we apply our noisy or model on all the sources of information to get overall prior.",
            "And.",
            "Yeah, so once we have our our prior knowledge both the both the method sell LFM that in factor model as well as the noisy or method.",
            "What we get is a confidence, confidence value in the edges which ranges between zero and one.",
            "And here are the simulation results.",
            "So better before going into the simulations I would like to explain how we really simulated the things here.",
            "So for this what we did, we took care graph and we did a random walk over this cake graphs for a fixed number of nodes most depend on depending on the type of simulations we want to perform.",
            "Once we had a network we tried to.",
            "Create information sources for each of these networks with with its with certain values of shape para meters.",
            "As I mentioned, it will be a beat up beta distribution, so with a different set of Alpha beta values we tried to simulate the day and the information sources.",
            "Once we have the information sources, we blind out our original network and then with these information sources we try to create the original network and then we try to find the sensitivity and specificity.",
            "For comparisons, we use the independent prior approach, which believes that every source is independent of each other, so the overall overall prior will be product of all the information sources.",
            "So this is our results here what we have."
        ],
        [
            "So what we observe if there if the informations are highly correlated, our method perform quite well as we can see here and with A3 and beta 3, the correlations are quite high and at the same time we also observe here that their performance is better in terms of sensitivity and specificity."
        ],
        [
            "The next thing is that what if we have low information content?",
            "It is very.",
            "It will be very common in the real biological world that we have very low information content.",
            "So we observe what we did, we we, we, we analyze it like if we have high information content like the information value that that we have is very low.",
            "And we see that the independent parameter doesn't work that well, but our methods.",
            "Give a fed fairly good results even with the high content they perform better."
        ],
        [
            "Now what we see again that we observed that at two in 2 two of two of these instances, the noisier, the noisy or model was outperformed by the independent parameter.",
            "But The thing is here what we see.",
            "These have very high confidence, very high bird hide, range of data, confidence data.",
            "What we have in the information sources.",
            "This leads to this leads to better performance of noisy or models.",
            "The independent parameter models here.",
            "And even in but noisy or even in case of weak information, it performs well.",
            "And overall to say to mention, I would say that.",
            "The Latin factor model that we developed was outperforming the independent prior method throughout the test."
        ],
        [
            "Now we go towards the how many sources can be used.",
            "There should be a break point where the the number of sources we can use will will have a limit.",
            "So we tried with different number of sources.",
            "As you can see here.",
            "So it's from starting from one source which is just learn the network from one source up to six sources and we found that our networks perform better with higher number of sources as well and the other the method with which we are comparing.",
            "Here the independent parameter.",
            "Doesn't perform if we increase our number of sources further."
        ],
        [
            "So we can see this here and the graphs that the red ones then independent parameter performance diminishes here, whereas the our models continue to perform better even with high and low everywhere.",
            "They perform better than the independent independent power."
        ],
        [
            "And this is finally the network size.",
            "What we observe in the simulation that the network size is not.",
            "Is not affecting here that much, but still I would mention that LFM noisy or continue to perform better than the independent parameters."
        ],
        [
            "Next, finally, what we did, we tried to simulate the real kick pathway with the real information sources and what we found.",
            "This was very interesting that we have very low information content in the real biological world and also a lot of missing information.",
            "So when we use our independent prior method it fails.",
            "It cannot handle their missing information, it cannot if the information is very low because we multiply that numerical underflow is caused and so it rejects everything.",
            "For every probability thresholds so, and therefore we have a sensitivity close to zero and specific specificity one, but our methods can handle this missing information and the low values of information, and they perform fairly well here.",
            "And in fact there with the size of what we can observe here, the Latin factor model improves because it has more data points to learn from."
        ],
        [
            "And finally, next thing what we did, we try to reconstruct Bashan, network relation, network using a cake, a cyclic subgraph.",
            "And this is our original subgraph.",
            "We try to limit it to make it the make the job computationally feasible.",
            "We try to limit our search.",
            "So what we observe we create we took us, took a graph and we generate a different number of samples for it starting from 5 up to 1000 samples of data set and then we use our prior knowledge to reconstruct this network with the guitar with the data set generated, what we found that the one with the prior reaches this level of 400 more than 400 samples even with the.",
            "Just 55 number of samples so it outputs simply outperformed.",
            "It performs better when we have some prior.",
            "This is what it proves in terms of specificity.",
            "If we observe, it also maintains a fairly good specificity here."
        ],
        [
            "And finally, once we are done, we applied it on a real data set.",
            "This breast cancer data set from one here and what we observed here.",
            "This is the network that we have from metalcore, kind of established network and then we tried to create this this way using this data set of 37 genes without prayer we could get some of the some of the edges here, but not a lot.",
            "And."
        ],
        [
            "Then we tried to include our prior and what we observe that we can get more of the edges there.",
            "We can explains the real biology involved better in our when we use a prior.",
            "So what we can see it explains we just focus here on one of the modules.",
            "This ESR one module we see that all these cross edges were identified when we use a prior but when we don't use a pair these edges were not identified at the same time.",
            "This C4 family of proteins was identified.",
            "Identified close to this Esri module as here as well.",
            "You can see, but here it was quite far from this module."
        ],
        [
            "So in summary, I would like to say that we propose two approaches to get probabilistic knowledge into integrate knowledge from different sources biological sources probabilistically.",
            "And which can help in network inference, better network inference.",
            "It can use heterogeneous sources of biological information and knowledge.",
            "It is flexible with the number of sources we can use.",
            "A greater number of sources of information and also handles the missing information and shows good agreement with the known networks, and of course, it also improves the network reconstruction using Bayesian network reconstruction errors."
        ],
        [
            "I would like to thank my supervisor, professor, Foolish, and group members that Institute.",
            "And the funding agencies."
        ],
        [
            "These are my defenses."
        ],
        [
            "Where you can visit the posters we have here.",
            "Thank you.",
            "Thank you very much.",
            "We have time for a few questions that was on.",
            "I was interesting talk.",
            "I read a paper that came out in the annals of statistics this year on this problem.",
            "They were combining multiple datasets and on simulations their findings agree with a large part of what you said.",
            "They found on a real problem of identifying transcription factor targets that actually, rather than combining as many data sources as possible into your prior, their performance was best when they choose the.",
            "Best, I just use that it can you can you sympathize with that?",
            "Or is that not in agreement with what you found?",
            "Can you come again with the question what exactly want to know?",
            "A similar study found that rather than using as many different sources of information as possible, OK, it was advantageous just to pick the best source of information.",
            "Instead of all.",
            "So what's the best source of information?",
            "First of all, I would like to know this, what do you call a best source of information?",
            "Well, the source that performs best under this source study, yeah, but it is difficult here we can try to integrate different number of sources and every source.",
            "There is something more and something lacking.",
            "So we suppose you integrated two additional sources which were just noise.",
            "That's going to reduce your performance.",
            "So in a sense you could ask how much does it hurt to include additional pieces which don't matter.",
            "I mean, how robust is it against number of sources?",
            "How many sources can be might be not informative, yes.",
            "That depends on what kind of information do we.",
            "Because if we have a lot of noise, it will be a problem, but it can handle the missing sources.",
            "Of course to some extent it can handle noise.",
            "What I have seen so far, the data are quite the information sources that we're using are quite established once, so we didn't face this problem yet.",
            "But maybe if you have very noisy source it can break.",
            "Thank you.",
            "So I was.",
            "I was wondering about the independence of the cake.",
            "Annotation and the data sources which you are using.",
            "I mean So what?",
            "How did you make sure that it's not?",
            "Yeah.",
            "Yeah, this is this is very interesting.",
            "So what we tried to do?",
            "We avoid cake for our knowledge sources at all because we're finally using KEGG pathways to simulate if we use cake again, it will be biased.",
            "So we try to avoid completely right.",
            "But cake is based on some other datasets which might be using right?",
            "So of course it is based on so we need a standard.",
            "So we also simulated some other datasets and they are working quite well without RAM.",
            "Not just not can we.",
            "In fact we also did with random.",
            "Networks, but I did not mention the random network thing here and it performed well with the random networks as well so.",
            "OK.",
            "I did not mention it here because of the time constraints, but it works with the random network as well.",
            "Good anymore questions.",
            "So here this one.",
            "You said that you used the links in the.",
            "Yeah, the lean similarity score for genealogy, right?",
            "This is what you used, but this is based, let's say another textual evidence coming from word net.",
            "So it's based more in, you know, statistical.",
            "Let's say not, not not that much related to biology or Department for example, or something like that.",
            "My question would be why didn't you use some specifically Gene Ontology measures?",
            "For example, there is a reason one from Bunk.",
            "In 2003 that we're modeling based on on June Ontology.",
            "Yeah, I'm in Pacific tradition to use other methods as well.",
            "As of now we were using this Lynn method and it was working quite well, but it will be good suggestion to use other methods as well so you didn't test any other measures.",
            "We tried some other measures but this Lynn was working quite well.",
            "We tried the other similarity measures there, but this Lin was working quite well.",
            "So so you choose because Lynn is an average similarity measure so it's based on the entropy as well.",
            "Far from the article position of the term.",
            "In the ontology.",
            "So this kind of entropy.",
            "How did you exactly calculate it?",
            "We have a method that computes the Lynn similarities between 2 gene annotations, so it gives me the gene similarities based on the ontological information and we use these similarities.",
            "Yeah, but not corpus.",
            "Did you use only the gene ontology as a corpus so you didn't use any kind of probabilities in pub Med literature?",
            "Something not so, only gene ontology and another question.",
            "You said that you use a lot of sources that for each one produces a different score that has a different range.",
            "So you brought you normalized everyone about the simulation studies.",
            "Try to simulate with different Alpha and shape parameters, and we try to observe the data range, what data ranges it produces to understand what's the relation between the information content, the data range and our performance of our models.",
            "Yeah, my question would be they have different ranges, its shores provide similarity.",
            "Let's say from different ranges.",
            "Did you just normalize it or did you feel that was for simulation study so it's normalized so it was simple normalization from zero to 1 so that you could apply the noisy or or you didn't do any logistic regression stuff?",
            "No nothing.",
            "So just simple normalization.",
            "OK, thank you very much.",
            "OK, so we're perfectly in time for the break, so let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about the method that that we recently developed and we are growing are currently updating it further to confirm to get the quantitative quantified prior from the existing biological knowledge.",
                    "label": 0
                },
                {
                    "sent": "The knowledge that we already have in biological databases or in terms of ontological terms.",
                    "label": 0
                },
                {
                    "sent": "So this is about the stories about that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in representation first I will.",
                    "label": 0
                },
                {
                    "sent": "I would like to tell you what inspired us to do this study and then followed by the approach.",
                    "label": 0
                },
                {
                    "sent": "What approaches do we use to integrate different knowledge sources into one quantified fired prior and then we talk about how we tested whether our method works correctly and validated it.",
                    "label": 0
                },
                {
                    "sent": "And finally I will talk about the application.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we know, we are very much into creating networks out of high throughput data.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of data microarray data.",
                    "label": 0
                },
                {
                    "sent": "We generate data and then we try to use our problem.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic methods like Mission, Network to create networks out of it.",
                    "label": 0
                },
                {
                    "sent": "But there is a problem with this.",
                    "label": 0
                },
                {
                    "sent": "The micro read the high throughput data we have.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of noise and the sample size is quite low.",
                    "label": 1
                },
                {
                    "sent": "Because of this, what we face in the networks created we have very high false positive and false negatives.",
                    "label": 1
                },
                {
                    "sent": "So we have to overcome somehow this false positive and false negatives.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To overcome this, it has been proposed by many people that if we try to integrate data from information knowledge from different databases and like literature, ontologies an interaction databases like we have HPHP ID, string etc.",
                    "label": 0
                },
                {
                    "sent": "If we somehow tried to get all this information together and put it in our learning method, then our method probably will work better.",
                    "label": 0
                },
                {
                    "sent": "So we need a quantified prior.",
                    "label": 0
                },
                {
                    "sent": "From existing biological knowledge and if we once we try to use this prayer into our learning method, it is very, very hard.",
                    "label": 1
                },
                {
                    "sent": "It is expected that it will reduce the false positives and false negatives and at the same time it was or it will also reach a network which is more closer to the real biological world and explains the biological phenomenon in a better way.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we have some problems here.",
                    "label": 0
                },
                {
                    "sent": "If we try to integrate data from different sources, what we have the issue is that the data is very hetrogenous.",
                    "label": 0
                },
                {
                    "sent": "That means we have different kinds of data.",
                    "label": 0
                },
                {
                    "sent": "We have interaction data we have in Toronto logical information.",
                    "label": 0
                },
                {
                    "sent": "We have literature information.",
                    "label": 0
                },
                {
                    "sent": "So these are completely diverse, very different kind of data and integrating them together is difficult.",
                    "label": 0
                },
                {
                    "sent": "The next issue that comes up is.",
                    "label": 0
                },
                {
                    "sent": "We have missing information.",
                    "label": 0
                },
                {
                    "sent": "Alot of annotation is missing the info Internet interaction.",
                    "label": 0
                },
                {
                    "sent": "Different interaction databases are not complete so we have a lot of lot of holes in our knowledge which we need to fill.",
                    "label": 0
                },
                {
                    "sent": "And the next thing will be numerical handlings by numerical handling.",
                    "label": 1
                },
                {
                    "sent": "I would say as I as I mentioned earlier, we need a quantified prior, but for this we need a quantified knowledge as well.",
                    "label": 0
                },
                {
                    "sent": "How to get that quantified knowledge?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I start with them with my approach is I would like to explain what exactly how we want to use our prayer.",
                    "label": 0
                },
                {
                    "sent": "We all know the base through here and we need a product to learn the network some priority information to learn the network.",
                    "label": 0
                },
                {
                    "sent": "If we considered for a network that every edge is independent of each other and then the overall prior for the network will be the product of each edge in the network.",
                    "label": 0
                },
                {
                    "sent": "It was proposed that this kind of prior was proposed by my foolish and coworkers as a function of the end of the individual individual edges here.",
                    "label": 0
                },
                {
                    "sent": "What up rival do approval of learning the network from data prior will try to bias the networks in the infinite, the network being inferred towards to consider the already known interaction.",
                    "label": 0
                },
                {
                    "sent": "It will also learn new edges, but it will also see if there's some edges are really existing.",
                    "label": 0
                },
                {
                    "sent": "They are known in the biological world.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is our idea?",
                    "label": 0
                },
                {
                    "sent": "So our idea is first to assemble different knowledge sources.",
                    "label": 1
                },
                {
                    "sent": "Like what kind of sources can we use for our quantified priors?",
                    "label": 0
                },
                {
                    "sent": "What are the sources of information once we have assembled these sources of information, we need to get the edge confidence we need to get some numbers because we need finally a quantified press.",
                    "label": 0
                },
                {
                    "sent": "So we need an edge conference.",
                    "label": 0
                },
                {
                    "sent": "That's what we call it an edge confidence for every interaction in the network.",
                    "label": 0
                },
                {
                    "sent": "After getting this edge confidence, we finally we need to combine all these edge confidences from different sources into one quantified prior.",
                    "label": 1
                },
                {
                    "sent": "Once we have a quantified prior, we can test, validate and finally go for an application.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what sources should we consider?",
                    "label": 0
                },
                {
                    "sent": "There can be many sources.",
                    "label": 0
                },
                {
                    "sent": "Second, how to get these confidence interval confidence numbers?",
                    "label": 0
                },
                {
                    "sent": "What confidence for each interaction?",
                    "label": 0
                },
                {
                    "sent": "How to get these numbers?",
                    "label": 1
                },
                {
                    "sent": "And finally how to combine all these information?",
                    "label": 1
                },
                {
                    "sent": "All this knowledge into one prior.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's go with the first thing.",
                    "label": 0
                },
                {
                    "sent": "What information sources we consider here.",
                    "label": 0
                },
                {
                    "sent": "So we consider interaction databases.",
                    "label": 0
                },
                {
                    "sent": "Of course, like Patrick Common string HP ID for as they provide the interaction between different proteins, binary interactions.",
                    "label": 0
                },
                {
                    "sent": "Then we consider Gene ontologies we consider if some terms in Gene ontology similar have high degree of similarity.",
                    "label": 0
                },
                {
                    "sent": "They're more likely that we have more confidence in the edge that can occur between them.",
                    "label": 1
                },
                {
                    "sent": "Next we consider the similarity in domains of the proteins.",
                    "label": 1
                },
                {
                    "sent": "So we and we also consider the interacting domain information.",
                    "label": 0
                },
                {
                    "sent": "If we have a pair of protein they will have we can form from every domains present in each of the protein.",
                    "label": 0
                },
                {
                    "sent": "We can form pairs and if these domains this pair of domain can really interact with each other and finally we can also consider the interactions that are already mentioned in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but once we have the sources, we need to see how to find the edge confidence.",
                    "label": 0
                },
                {
                    "sent": "So for any kind of interaction graph, we take the data the data from the interaction databases and for this right now what we're using is inverse of the shortest path length.",
                    "label": 0
                },
                {
                    "sent": "So if this path is very short, the confidence is higher.",
                    "label": 0
                },
                {
                    "sent": "If the path is long, the confidence is low.",
                    "label": 0
                },
                {
                    "sent": "But now we're trying to update it to the graph diffusion kernel.",
                    "label": 1
                },
                {
                    "sent": "But in the current presentation I will be showing the results from this first part, that is the short spot length.",
                    "label": 0
                },
                {
                    "sent": "The next next thing is the ontological terms.",
                    "label": 1
                },
                {
                    "sent": "So we try to compute the similarities in the associated GO terms, mainly biological processes here.",
                    "label": 1
                },
                {
                    "sent": "Which was proposed by Lynn in 1989 and next is the protein domain data.",
                    "label": 0
                },
                {
                    "sent": "So for protein domain data we have domain signature interpret domain signature for each protein and we tried to find the similarities across these signatures.",
                    "label": 1
                },
                {
                    "sent": "And finally, the other form is interacting domain data.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So we have pairs of proteins and reform all the possible combinations of the domains present in them, and we try to find how many of them are really can really interact.",
                    "label": 0
                },
                {
                    "sent": "We have a database for that called domain database.",
                    "label": 0
                },
                {
                    "sent": "We use this to search the frequency relative frequency for this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I will go once I have the sources assembled together, then I have the confidence intervals confidence numbers.",
                    "label": 0
                },
                {
                    "sent": "Then I go how to combine these all these different sources into one quantified prior.",
                    "label": 0
                },
                {
                    "sent": "We here propose two different models for this for this and the first one we call it a Latin factor model or elephant in short.",
                    "label": 0
                },
                {
                    "sent": "For this model we have an assumption that all the knowledge, whatever we have in biology comes from a really from a true network, which we don't know yet, but it actually arises from a true network.",
                    "label": 0
                },
                {
                    "sent": "Which we call here 5.",
                    "label": 0
                },
                {
                    "sent": "So and the if this information sat together, they originate from this fight.",
                    "label": 0
                },
                {
                    "sent": "There very highly correlated.",
                    "label": 0
                },
                {
                    "sent": "These information sources are highly correlated.",
                    "label": 0
                },
                {
                    "sent": "They originate from this true network.",
                    "label": 1
                },
                {
                    "sent": "Once we have this, this true network is a Latin variable for us and we try to learn this truth but unknown network using our information sources that we assembled in.",
                    "label": 1
                },
                {
                    "sent": "In the final and the previous stages.",
                    "label": 1
                },
                {
                    "sent": "And we compute you compute this.",
                    "label": 0
                },
                {
                    "sent": "Find the true network like this without using Bashan rule.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are some other things that I would like to tell about this model.",
                    "label": 0
                },
                {
                    "sent": "We we have an assumption that the information in the knowledge sources follows a beta distribution.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this, this is the beta distribution here.",
                    "label": 0
                },
                {
                    "sent": "And the log likelihood for the for the network is given by this equation here.",
                    "label": 1
                },
                {
                    "sent": "So we have we consider it to be a beta distribution as you see here with the shape parameters Alpha and beta.",
                    "label": 1
                },
                {
                    "sent": "To learn the overall prior network, what we use is a special kind of MCMC, Markov chain Monte Carlo approach using Metropolitan Hastings algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it is a nested MCMC.",
                    "label": 0
                },
                {
                    "sent": "I will explain the nested in this slide self first.",
                    "label": 0
                },
                {
                    "sent": "What kind of operation do we use to change change the network every time during the answer steps?",
                    "label": 0
                },
                {
                    "sent": "These are the essence of steps here like edge insertion, deletion and reversal.",
                    "label": 0
                },
                {
                    "sent": "So we should try to insert a new edge or delete an existing edge or reverse the direction of the edge.",
                    "label": 0
                },
                {
                    "sent": "So we start from an arbitrary network and we do these operations too.",
                    "label": 0
                },
                {
                    "sent": "Damn to maintain this nested approach in the sense that we modify our Alpha beta on every iteration after certain iteration, we modify change our Alpha.",
                    "label": 0
                },
                {
                    "sent": "Beta values is also leads to a faster convergence.",
                    "label": 0
                },
                {
                    "sent": "Of the MCMC algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next approach is quite called noisy or model, so it's based on the noisy or logic.",
                    "label": 0
                },
                {
                    "sent": "And the model looks in terms.",
                    "label": 0
                },
                {
                    "sent": "It looks a bit different from the previous model, but it says that we have causes the information sources that we collected in the previous stages are the causes and leading to a network to quantify the quantified value of the edge confidences, which is fine.",
                    "label": 0
                },
                {
                    "sent": "Again here, this is the effect.",
                    "label": 1
                },
                {
                    "sent": "So it says that the probability of each cost to be sufficient to prove the effect is independent of the presence of others.",
                    "label": 1
                },
                {
                    "sent": "This leads to that when we try to combine these all these causes into an effect.",
                    "label": 1
                },
                {
                    "sent": "It follows a noisy or function and then the overall network can be given as the situation here.",
                    "label": 1
                },
                {
                    "sent": "So overall prior knowledge is our effect here.",
                    "label": 0
                },
                {
                    "sent": "And finally, what we get the cause that causes, as I mentioned before, is the confidence in each edge of the network.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we apply our noisy or model on all the sources of information to get overall prior.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so once we have our our prior knowledge both the both the method sell LFM that in factor model as well as the noisy or method.",
                    "label": 0
                },
                {
                    "sent": "What we get is a confidence, confidence value in the edges which ranges between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And here are the simulation results.",
                    "label": 0
                },
                {
                    "sent": "So better before going into the simulations I would like to explain how we really simulated the things here.",
                    "label": 0
                },
                {
                    "sent": "So for this what we did, we took care graph and we did a random walk over this cake graphs for a fixed number of nodes most depend on depending on the type of simulations we want to perform.",
                    "label": 0
                },
                {
                    "sent": "Once we had a network we tried to.",
                    "label": 0
                },
                {
                    "sent": "Create information sources for each of these networks with with its with certain values of shape para meters.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, it will be a beat up beta distribution, so with a different set of Alpha beta values we tried to simulate the day and the information sources.",
                    "label": 0
                },
                {
                    "sent": "Once we have the information sources, we blind out our original network and then with these information sources we try to create the original network and then we try to find the sensitivity and specificity.",
                    "label": 0
                },
                {
                    "sent": "For comparisons, we use the independent prior approach, which believes that every source is independent of each other, so the overall overall prior will be product of all the information sources.",
                    "label": 0
                },
                {
                    "sent": "So this is our results here what we have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we observe if there if the informations are highly correlated, our method perform quite well as we can see here and with A3 and beta 3, the correlations are quite high and at the same time we also observe here that their performance is better in terms of sensitivity and specificity.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next thing is that what if we have low information content?",
                    "label": 0
                },
                {
                    "sent": "It is very.",
                    "label": 0
                },
                {
                    "sent": "It will be very common in the real biological world that we have very low information content.",
                    "label": 0
                },
                {
                    "sent": "So we observe what we did, we we, we, we analyze it like if we have high information content like the information value that that we have is very low.",
                    "label": 0
                },
                {
                    "sent": "And we see that the independent parameter doesn't work that well, but our methods.",
                    "label": 0
                },
                {
                    "sent": "Give a fed fairly good results even with the high content they perform better.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what we see again that we observed that at two in 2 two of two of these instances, the noisier, the noisy or model was outperformed by the independent parameter.",
                    "label": 0
                },
                {
                    "sent": "But The thing is here what we see.",
                    "label": 0
                },
                {
                    "sent": "These have very high confidence, very high bird hide, range of data, confidence data.",
                    "label": 0
                },
                {
                    "sent": "What we have in the information sources.",
                    "label": 0
                },
                {
                    "sent": "This leads to this leads to better performance of noisy or models.",
                    "label": 0
                },
                {
                    "sent": "The independent parameter models here.",
                    "label": 0
                },
                {
                    "sent": "And even in but noisy or even in case of weak information, it performs well.",
                    "label": 1
                },
                {
                    "sent": "And overall to say to mention, I would say that.",
                    "label": 1
                },
                {
                    "sent": "The Latin factor model that we developed was outperforming the independent prior method throughout the test.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we go towards the how many sources can be used.",
                    "label": 0
                },
                {
                    "sent": "There should be a break point where the the number of sources we can use will will have a limit.",
                    "label": 0
                },
                {
                    "sent": "So we tried with different number of sources.",
                    "label": 1
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "So it's from starting from one source which is just learn the network from one source up to six sources and we found that our networks perform better with higher number of sources as well and the other the method with which we are comparing.",
                    "label": 0
                },
                {
                    "sent": "Here the independent parameter.",
                    "label": 1
                },
                {
                    "sent": "Doesn't perform if we increase our number of sources further.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see this here and the graphs that the red ones then independent parameter performance diminishes here, whereas the our models continue to perform better even with high and low everywhere.",
                    "label": 0
                },
                {
                    "sent": "They perform better than the independent independent power.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is finally the network size.",
                    "label": 0
                },
                {
                    "sent": "What we observe in the simulation that the network size is not.",
                    "label": 0
                },
                {
                    "sent": "Is not affecting here that much, but still I would mention that LFM noisy or continue to perform better than the independent parameters.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, finally, what we did, we tried to simulate the real kick pathway with the real information sources and what we found.",
                    "label": 0
                },
                {
                    "sent": "This was very interesting that we have very low information content in the real biological world and also a lot of missing information.",
                    "label": 0
                },
                {
                    "sent": "So when we use our independent prior method it fails.",
                    "label": 0
                },
                {
                    "sent": "It cannot handle their missing information, it cannot if the information is very low because we multiply that numerical underflow is caused and so it rejects everything.",
                    "label": 0
                },
                {
                    "sent": "For every probability thresholds so, and therefore we have a sensitivity close to zero and specific specificity one, but our methods can handle this missing information and the low values of information, and they perform fairly well here.",
                    "label": 0
                },
                {
                    "sent": "And in fact there with the size of what we can observe here, the Latin factor model improves because it has more data points to learn from.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, next thing what we did, we try to reconstruct Bashan, network relation, network using a cake, a cyclic subgraph.",
                    "label": 0
                },
                {
                    "sent": "And this is our original subgraph.",
                    "label": 0
                },
                {
                    "sent": "We try to limit it to make it the make the job computationally feasible.",
                    "label": 0
                },
                {
                    "sent": "We try to limit our search.",
                    "label": 0
                },
                {
                    "sent": "So what we observe we create we took us, took a graph and we generate a different number of samples for it starting from 5 up to 1000 samples of data set and then we use our prior knowledge to reconstruct this network with the guitar with the data set generated, what we found that the one with the prior reaches this level of 400 more than 400 samples even with the.",
                    "label": 0
                },
                {
                    "sent": "Just 55 number of samples so it outputs simply outperformed.",
                    "label": 1
                },
                {
                    "sent": "It performs better when we have some prior.",
                    "label": 0
                },
                {
                    "sent": "This is what it proves in terms of specificity.",
                    "label": 1
                },
                {
                    "sent": "If we observe, it also maintains a fairly good specificity here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, once we are done, we applied it on a real data set.",
                    "label": 0
                },
                {
                    "sent": "This breast cancer data set from one here and what we observed here.",
                    "label": 1
                },
                {
                    "sent": "This is the network that we have from metalcore, kind of established network and then we tried to create this this way using this data set of 37 genes without prayer we could get some of the some of the edges here, but not a lot.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we tried to include our prior and what we observe that we can get more of the edges there.",
                    "label": 0
                },
                {
                    "sent": "We can explains the real biology involved better in our when we use a prior.",
                    "label": 0
                },
                {
                    "sent": "So what we can see it explains we just focus here on one of the modules.",
                    "label": 0
                },
                {
                    "sent": "This ESR one module we see that all these cross edges were identified when we use a prior but when we don't use a pair these edges were not identified at the same time.",
                    "label": 0
                },
                {
                    "sent": "This C4 family of proteins was identified.",
                    "label": 0
                },
                {
                    "sent": "Identified close to this Esri module as here as well.",
                    "label": 0
                },
                {
                    "sent": "You can see, but here it was quite far from this module.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, I would like to say that we propose two approaches to get probabilistic knowledge into integrate knowledge from different sources biological sources probabilistically.",
                    "label": 0
                },
                {
                    "sent": "And which can help in network inference, better network inference.",
                    "label": 1
                },
                {
                    "sent": "It can use heterogeneous sources of biological information and knowledge.",
                    "label": 1
                },
                {
                    "sent": "It is flexible with the number of sources we can use.",
                    "label": 1
                },
                {
                    "sent": "A greater number of sources of information and also handles the missing information and shows good agreement with the known networks, and of course, it also improves the network reconstruction using Bayesian network reconstruction errors.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like to thank my supervisor, professor, Foolish, and group members that Institute.",
                    "label": 0
                },
                {
                    "sent": "And the funding agencies.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are my defenses.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where you can visit the posters we have here.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "We have time for a few questions that was on.",
                    "label": 0
                },
                {
                    "sent": "I was interesting talk.",
                    "label": 0
                },
                {
                    "sent": "I read a paper that came out in the annals of statistics this year on this problem.",
                    "label": 0
                },
                {
                    "sent": "They were combining multiple datasets and on simulations their findings agree with a large part of what you said.",
                    "label": 0
                },
                {
                    "sent": "They found on a real problem of identifying transcription factor targets that actually, rather than combining as many data sources as possible into your prior, their performance was best when they choose the.",
                    "label": 0
                },
                {
                    "sent": "Best, I just use that it can you can you sympathize with that?",
                    "label": 0
                },
                {
                    "sent": "Or is that not in agreement with what you found?",
                    "label": 0
                },
                {
                    "sent": "Can you come again with the question what exactly want to know?",
                    "label": 0
                },
                {
                    "sent": "A similar study found that rather than using as many different sources of information as possible, OK, it was advantageous just to pick the best source of information.",
                    "label": 0
                },
                {
                    "sent": "Instead of all.",
                    "label": 0
                },
                {
                    "sent": "So what's the best source of information?",
                    "label": 0
                },
                {
                    "sent": "First of all, I would like to know this, what do you call a best source of information?",
                    "label": 0
                },
                {
                    "sent": "Well, the source that performs best under this source study, yeah, but it is difficult here we can try to integrate different number of sources and every source.",
                    "label": 0
                },
                {
                    "sent": "There is something more and something lacking.",
                    "label": 0
                },
                {
                    "sent": "So we suppose you integrated two additional sources which were just noise.",
                    "label": 0
                },
                {
                    "sent": "That's going to reduce your performance.",
                    "label": 0
                },
                {
                    "sent": "So in a sense you could ask how much does it hurt to include additional pieces which don't matter.",
                    "label": 0
                },
                {
                    "sent": "I mean, how robust is it against number of sources?",
                    "label": 0
                },
                {
                    "sent": "How many sources can be might be not informative, yes.",
                    "label": 0
                },
                {
                    "sent": "That depends on what kind of information do we.",
                    "label": 0
                },
                {
                    "sent": "Because if we have a lot of noise, it will be a problem, but it can handle the missing sources.",
                    "label": 0
                },
                {
                    "sent": "Of course to some extent it can handle noise.",
                    "label": 0
                },
                {
                    "sent": "What I have seen so far, the data are quite the information sources that we're using are quite established once, so we didn't face this problem yet.",
                    "label": 0
                },
                {
                    "sent": "But maybe if you have very noisy source it can break.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So I was.",
                    "label": 0
                },
                {
                    "sent": "I was wondering about the independence of the cake.",
                    "label": 0
                },
                {
                    "sent": "Annotation and the data sources which you are using.",
                    "label": 0
                },
                {
                    "sent": "I mean So what?",
                    "label": 0
                },
                {
                    "sent": "How did you make sure that it's not?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is this is very interesting.",
                    "label": 0
                },
                {
                    "sent": "So what we tried to do?",
                    "label": 0
                },
                {
                    "sent": "We avoid cake for our knowledge sources at all because we're finally using KEGG pathways to simulate if we use cake again, it will be biased.",
                    "label": 0
                },
                {
                    "sent": "So we try to avoid completely right.",
                    "label": 0
                },
                {
                    "sent": "But cake is based on some other datasets which might be using right?",
                    "label": 0
                },
                {
                    "sent": "So of course it is based on so we need a standard.",
                    "label": 0
                },
                {
                    "sent": "So we also simulated some other datasets and they are working quite well without RAM.",
                    "label": 0
                },
                {
                    "sent": "Not just not can we.",
                    "label": 0
                },
                {
                    "sent": "In fact we also did with random.",
                    "label": 0
                },
                {
                    "sent": "Networks, but I did not mention the random network thing here and it performed well with the random networks as well so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I did not mention it here because of the time constraints, but it works with the random network as well.",
                    "label": 0
                },
                {
                    "sent": "Good anymore questions.",
                    "label": 0
                },
                {
                    "sent": "So here this one.",
                    "label": 0
                },
                {
                    "sent": "You said that you used the links in the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the lean similarity score for genealogy, right?",
                    "label": 0
                },
                {
                    "sent": "This is what you used, but this is based, let's say another textual evidence coming from word net.",
                    "label": 0
                },
                {
                    "sent": "So it's based more in, you know, statistical.",
                    "label": 0
                },
                {
                    "sent": "Let's say not, not not that much related to biology or Department for example, or something like that.",
                    "label": 0
                },
                {
                    "sent": "My question would be why didn't you use some specifically Gene Ontology measures?",
                    "label": 0
                },
                {
                    "sent": "For example, there is a reason one from Bunk.",
                    "label": 0
                },
                {
                    "sent": "In 2003 that we're modeling based on on June Ontology.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm in Pacific tradition to use other methods as well.",
                    "label": 0
                },
                {
                    "sent": "As of now we were using this Lynn method and it was working quite well, but it will be good suggestion to use other methods as well so you didn't test any other measures.",
                    "label": 0
                },
                {
                    "sent": "We tried some other measures but this Lynn was working quite well.",
                    "label": 0
                },
                {
                    "sent": "We tried the other similarity measures there, but this Lin was working quite well.",
                    "label": 0
                },
                {
                    "sent": "So so you choose because Lynn is an average similarity measure so it's based on the entropy as well.",
                    "label": 0
                },
                {
                    "sent": "Far from the article position of the term.",
                    "label": 0
                },
                {
                    "sent": "In the ontology.",
                    "label": 0
                },
                {
                    "sent": "So this kind of entropy.",
                    "label": 0
                },
                {
                    "sent": "How did you exactly calculate it?",
                    "label": 0
                },
                {
                    "sent": "We have a method that computes the Lynn similarities between 2 gene annotations, so it gives me the gene similarities based on the ontological information and we use these similarities.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but not corpus.",
                    "label": 0
                },
                {
                    "sent": "Did you use only the gene ontology as a corpus so you didn't use any kind of probabilities in pub Med literature?",
                    "label": 0
                },
                {
                    "sent": "Something not so, only gene ontology and another question.",
                    "label": 0
                },
                {
                    "sent": "You said that you use a lot of sources that for each one produces a different score that has a different range.",
                    "label": 0
                },
                {
                    "sent": "So you brought you normalized everyone about the simulation studies.",
                    "label": 0
                },
                {
                    "sent": "Try to simulate with different Alpha and shape parameters, and we try to observe the data range, what data ranges it produces to understand what's the relation between the information content, the data range and our performance of our models.",
                    "label": 0
                },
                {
                    "sent": "Yeah, my question would be they have different ranges, its shores provide similarity.",
                    "label": 0
                },
                {
                    "sent": "Let's say from different ranges.",
                    "label": 0
                },
                {
                    "sent": "Did you just normalize it or did you feel that was for simulation study so it's normalized so it was simple normalization from zero to 1 so that you could apply the noisy or or you didn't do any logistic regression stuff?",
                    "label": 0
                },
                {
                    "sent": "No nothing.",
                    "label": 0
                },
                {
                    "sent": "So just simple normalization.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're perfectly in time for the break, so let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}