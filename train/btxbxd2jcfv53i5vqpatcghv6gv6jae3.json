{
    "id": "btxbxd2jcfv53i5vqpatcghv6gv6jae3",
    "title": "Bayesian Nonparametrics",
    "info": {
        "author": [
            "Yee Whye Teh, Department of Statistics, University of Oxford"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2011_teh_nonparametrics/",
    "segmentation": [
        [
            "Today I'll be talking about Bayesian Nonparametrics, and I guess I'll start out."
        ],
        [
            "With a bit of introduction to Bayesian machine learning, first I guess in the last week you have heard a lot about various Bayesian approaches to machine learning and statistics, so this is a bit of a recapping us."
        ],
        [
            "So let's start off with probabilistic modeling.",
            "Well, as we all know, a machine learning is all about data, and we know that data has lots of different has lots of noise and uncertainty and so forth.",
            "an A big part of learning from data is about handling or being able to reason with the noise in the data and the idea of probabilistic modeling is that the language of probability theory is kind of a very rich language to express all of these uncertainties about.",
            "About data and an allows us to.",
            "Two reason coherently.",
            "About this data and the solve models that we use to express this uncertainties about data called probabilistic models.",
            "OK, so we'll see a number of these examples later, and I think probabilistic models has been very successful, especially in the last 10 years in in both statistics and machine learning.",
            "And the reason is because, well, there's kind of a few reasons.",
            "The first one is that it allows us to.",
            "Count like Visualize our models of the data.",
            "At the high level, without actually worrying about all the little details of.",
            "About the model.",
            "Another reason is that we can build very complex models from lots of simple parts and we can do both of this using the using the language of graphical models and which which I guess you're probably familiar with.",
            "And another reason is that it allows us to separate out the modeling questions from the computational questions, right?",
            "So it allows the modeler to bill to come up with a model and then it allow somebody else to build to derive algorithms for doing inference and learning in the models.",
            "Given the models that the modeler has come up with, and I think this separation of the modeling question from the algorithmic questions is very important.",
            "Basically because.",
            "Well, there's a few reasons.",
            "One of the reasons is that people who know their domains well may not be the same people who can program well, and the people who can program well may not be able to know biology of linguistics domain well, so that allows us a separation of efforts into the modeling, self efforts and the algorithmics of effort, and this allows the field to advance a lot faster.",
            "Another reason for this.",
            "Separation of modeling and algorithm questions is because.",
            "Um?",
            "When you do the your modeling of your data, you like to to think about this statistical questions without having to worry about all the different approximations that you have to do to get the model to work on your data.",
            "And this is very nice because it allows you to think about you know in the ideal situation what should be done then that should be separated from what could be done on your data so that you can so that if it works, if your algorithms will.",
            "Or doesn't work.",
            "You can count like localize and figure out where is it that it's gone wrong.",
            "Right?"
        ],
        [
            "So what is probabilistic modeling?",
            "Basically, a probabilistic model is simply a joint distribution over a set of random variables.",
            "OK, so here's our joints distribution and will typically be parameterized by some parameter Theta.",
            "And typically we can think about this probabilistic models as generative models.",
            "So the idea is that we can postulate of particulare generative process that could have led to the observations that we actually do observe.",
            "Right so.",
            "Um?",
            "Alright OK, let me see so.",
            "And the idea is that with this joint distribution you can then do things like inference, which is to basically estimate what were the likely states.",
            "Uh, Varun, observed random variables.",
            "Given the states of our observed data.",
            "So so this allows us to infer what we did not observe.",
            "And of course, this conditional distribution of the unobserved variables, given the observed once, is simply given by base rule, right?",
            "So this is just the ratio of the joint distribution divided by the marginal distribution of the observed data.",
            "And of course, we don't typically know the values fault for the parameters for model an would like to learn about those as well."
        ],
        [
            "Um so.",
            "The process of finding the lightly settings of the parameters which could have led to our data is called learning, and this is typically done by maximum likelihood, right?",
            "So we like to find the parameter setting which gives highest, probably D to our observations.",
            "There's lots of different things that you could do with probabilistic models.",
            "You could do things like prediction, so prediction this is can be phrased as basically computing the conditional distribution of some UN observed.",
            "Data from the observed ones.",
            "So you can think of this as your test set, and that's as your training set.",
            "We can do classification, so if you have a bunch of different models, each of them indexed by C. Ann given by a different parameter, then you could decide given a test data item which class it belongs to by basically assigning it to the to the category or to the class that gives it highest probability under its probabilistic model.",
            "So and in addition to this, of course there's lots of different other things that you could do.",
            "I guess this a bit.",
            "These are things like visualization, so you would like to visualize.",
            "The generative process that kind of could have led to your data.",
            "This is also very useful in things like interpreting the data and summarizing the data and so forth.",
            "And as I said before, there's this separation of the modeling question from the algorithm questions.",
            "An one of the nice things will probably stay models is that there are lots of different.",
            "There are lots of standard ways in which you could derive algorithms for your models, so you can derive inference algorithms, learning algorithms and so forth, and this algorithms things like you know the expectation propagation.",
            "The expectation maximization algorithm junction tree.",
            "There's variational inference, expectation propagation, Markov chain, Monte Carlo sampling, and so forth.",
            "So this algorithms are pretty standard an given a model is relatively straightforward to to derive an algorithm for doing inference or learning in your model.",
            "So one thing we should I which is I always feel."
        ],
        [
            "In a bit weird about probabilistic models, is that given our observed data X.",
            "Here we have two things that we don't know about.",
            "We have our latent variables, Y&R parameters, Theta, right and.",
            "If you look about this.",
            "The way in which we estimate the unobserved latent variables is called."
        ],
        [
            "Difference from the way we estimate the parameters right?",
            "So one is just doing one is just computing the conditional distribution while the other one is trying to do maximum likelihood.",
            "And always I always find that a bit strange because they should basically be the same processes both of them about the process of inferring what we did not observe from what we did observe."
        ],
        [
            "And basically the Bayesian modeling approach.",
            "Does that does both of these both inference and learning in exactly the same way, and to do that?",
            "Of course you will need a prior distribution over your parameters, so this is our prior distribution over parameters so that we can estimate both the unobserved latent variables and the and observed parameters in the same process, which is simply to compute the posterior distribution over the latent variables and parameters together.",
            "Given the observed data, so the posterior distribution here is going to be again given by base rule, which is now going to be a.",
            "A ratio between the joint distribution over the all the random variables, whether observed or unobserved, an also with the parameters.",
            "OK, and then of course this in the denominator here will have the marginal distribution of simply the observed variables, where we have marginalized out both the parameters and the latent variables.",
            "Again, we can do things like prediction and classification and so forth, so prediction would correspond to computing the conditional distribution of a new test item given your training sets X one to XN.",
            "And this would be of course just be basically integral of the conditional probability of the test item given the parameters where we've integrated up the parameters with respect to its posterior distribution distribution.",
            "So this is just.",
            "Follows from the from probability theory and classification would be kind of similar, so we would like to compute the conditional probability of the test item given the data set corresponding to the C class.",
            "Where we have again marginalized out the parameters corresponding to the CS class.",
            "So one thing to note is that this prior distribution is is very useful because it allows us to regularize our learning.",
            "So I guess you guys have all know that when you do maximum likelihood learning then you have to be careful not to over fit to your data.",
            "Write an one way in which you could address overfitting is by regularising your parameters, and one way we should think about this prior distribution here.",
            "Is basically as a way of of regularising your parameters.",
            "So basically, if you look at the posterior distribution of the parameters given our observed data is going to be a ratio between the joint between the observed data and the parameters divided by the marginal.",
            "And if you look at this joint.",
            "Distribution here it will be high if the particular setting of the parameters.",
            "Gives high probability of to the observed data and has high probability under the prior an.",
            "Basically we can think of this product here as a tradeoff in finding good parameter settings that fits both the prior well as well as the data will.",
            "Something else which I like to say is that in the Bayesian way of looking at things, there's really no difference between inference and learning, so both of them correspond to computing the conditional distribution of some unobserved random variable given the observed data.",
            "And I think this is quite important, especially when you start looking into more complex models and and for this models that there's often not much distinction between what is a latent variable and what is a parameter, they're all simply random variables that you did not observe."
        ],
        [
            "Right, So what are some examples of probabilistic models?",
            "I guess the there's a very large class of probabilistic models called graphical models which are very useful in terms of reasoning about.",
            "Uncertainty in this.",
            "In our data.",
            "In graphical models can be visualized in this graphs, right?",
            "And basically each note of the graph correspond to a random variable and each edge basically correspond to a direct dependence of 1 variable on another variable, an importantly.",
            "If you don't.",
            "If there's no edge connecting to random variables, that corresponds to actually conditional independence.",
            "Between the random variables.",
            "So in this graph, for example, it's because there's no age between earthquake and burglar.",
            "That means that whether an earthquake happens or not, or whether a burglar comes into your house or not, are independent events.",
            "Um, yeah.",
            "So this is the Asian Network and this is the alarm network and I guess they're pretty.",
            "Popular simple graphical models to illustrate about these things are."
        ],
        [
            "OK, so I'll show you a few more examples, so this example for model based clustering is will actually return to this.",
            "A few times over both today and tomorrow.",
            "And this is a model for clustering.",
            "So what is clustering?",
            "Clustering is the idea that given a data set which consists of heterogeneous of.",
            "Uh, data items.",
            "Basically you can think of the data items as coming from different sources.",
            "Then we like to cluster the data items into the different clusters, each of which are kind of homogeneous within themselves and their and the different clusters are different.",
            "So to visualize.",
            "So if the blue dots here are our data items, then we would like to cluster them into three different clusters, which corresponds to well.",
            "Um, three different types of data items that we observe, right?",
            "And this is a model.",
            "And we can take a model based approach to clustering.",
            "An IS model based in the sense that we are going to postulate a particular model for the data items in each cluster.",
            "So the model kind of goes as follows.",
            "Where can I assume so?",
            "XI is going to be data item I.",
            "We're going to assume that there's some latent variables that I which corresponds to the UN observed indicator variable, which tells us which cluster this data belongs to.",
            "And we're going to assume the following generative model.",
            "So for every data item, we're first going to decide which cluster that data item belongs to by sampling from a discrete distribution with prior given by \u03c0.",
            "So Pi here is basically a vector PI12 Pi K with Pi K being the prior probability that cluster K is respond, ISM is the prior probability that data item I belongs to cluster K. And given that we've postulated that.",
            "Data item I belongs to cluster KC.",
            "Then the.",
            "That this so is that is that question.",
            "Oh, it's just a discrete distribution.",
            "Yeah, well, it's a discrete distribution with probabilities given by \u03c0, so.",
            "The probability that that I takes on value one is going to buy one.",
            "And the probability that that I takes on value 2 is going to be \u03c0 two, and the probability that that I takes on value K is going to be pikey.",
            "Um, I would.",
            "Yeah, so people typically call that multinomial in machine learning, but it's actually a bit confusing because a multinomial distribution actually is actually a richer set of distributions then this discrete distributions.",
            "OK, I wanted to make that distinction, typically in the multinomial distribution you have a pie, but you also have.",
            "Like a number of independent draws from that distribution.",
            "Um, right so.",
            "Yeah, so in machine learning typically people call it multinomial.",
            "Right, so that I is a postulated latent variable which tells us which cluster XI belongs to.",
            "And given that we know that I so given the given that we know which cluster data item I belongs to, we can now postulate that there's some distribution over data items that belongs to that cluster, so that distribution is going to be given by this F distribution, and it's going to be parameterized by Theta zed I.",
            "So if that I take some value 3, then this is going to be Theta star tree and this can be the parameter.",
            "Responding to corresponding to the third cluster, an F of Theta star tree is going to be the distribution over data items under the third cluster.",
            "And being Bayesian, of course we would like to place priors over both sets of parameters that we have here and the typical prior for the mixing proportions.",
            "Pie is what's called richly distribution, and typically we might assume such a symmetric directly distribution where the parameters are all the same and there's going to be a hyperparameter Alpha, and we can't do this division by K. So the idea is that this is Alpha corresponds to to the basically the strength of the prior an.",
            "If you take this vector and you normalize it, then it's going to be 1 / K is going to be a vector where every entry is simply 1 / K. And that corresponds to the prior mean of \u03c0.",
            "Then we might assume that there's a prior over the cluster parameters given by H, so here's our generative process and this is a graphical models which represents that generative process.",
            "We have this plate notation here.",
            "Anna Plate basically just means that we take that part of the sub graph of the graphical model and just replicated a certain number of times.",
            "So in this case, can you guys see down here?",
            "No OK yeah, alright so this plate.",
            "Here is I equals to 12 N OK and what that says is that we're going to replicate this two random variables N times and going to call them.",
            "I basically we're going to index them by I, which is going to take on values 12 N. Displayed here will be replicated it K times because we assume that there are big key clusters in our model, each of them having a parameter.",
            "Theta Starkey.",
            "Right, so we'll come back to this later, so the idea now is that we will only observe the excise and we'd like to compute the conditional distribution of both the latent variables which indicate which cluster each data item belongs to, as well as the posterior distribution over the parameters of the model.",
            "OK."
        ],
        [
            "Um?",
            "It's just like to show you a few examples of graphical models and of probabilistic models.",
            "So the second example here is a hidden Markov model, and this is basically a dynamic generalization of the mixture model.",
            "So, and this is a very popular model for time series, and basically it assumes that the our time series, which is now going to be X1 to X Tao is generated in the following way.",
            "It assumes that there's some sequence of latent variables which we do not observe.",
            "And we assume that there's some Markov dependence of the latent variables.",
            "So the idea of this Markov dependencies that issues that is going to depend on the previous zed.",
            "And given the value of the previous set, the the value of the next zed variable is going to be independent of all that variables that comes in the past.",
            "OK, so this is the assumption of Markov property.",
            "Basically the future is independent of the past given the present.",
            "So we assume that there's a Markov chain over our set variables, and then each of our observations dangles off one of the variables.",
            "So X2 depends on that too.",
            "And this is of course a Bayesian hidden Markov models, because I've kind of drawn in the parameters of the model as well.",
            "Pi K Here is going to be a vector of length big K, and it's going to give us the transition probability going out from state key.",
            "And Theta star K is going to be the admission distribute.",
            "Is the parameter for the admission distribution if you're in.",
            "If you are in states K. Um, OK."
        ],
        [
            "Just another example, here's another example which has real world applications is for collaborative filtering, so this in this problem of collaborative filtering you have a number of users, so think of Netflix you have a number of users, you have a number of products or movies.",
            "And for certain subset of users and movies, you might have ratings RIJ, which is basically the rating that that user I assigns to movie J.",
            "So if the user really likes the movie then he would say that RJ is equals to five and if the user really dislikes the movie then he will assign RJ equals to one.",
            "OK, so we have a between one start a 5 star rating.",
            "And the problem here is that given a certain subset of this observed ratings would like to predict how.",
            "How much does a particular user like a particular movie over all the observed user movie pairs?",
            "And we can assume.",
            "And here's a very popular probabilistic model for solving collaborative filtering.",
            "The idea is the following, so we're going to assume that every user is going to have some.",
            "Some features which are UN observed and this is a vector in the Euclidean space which basically describes what sort of movies does the user like.",
            "Corresponding to every movie, we're also going to assume that there's a probability vector which describes what type of movie that movie is.",
            "And we.",
            "Postulate that the rating that user I gives to movie J.",
            "It's going to be Gaussian distributed with a mean given by the DOT product between the user feature and the movie feature, so that if the two feature vectors kind of line up together.",
            "So what that says is that the soft movies that the user likes is very similar to the sort of movie that movie is.",
            "And of course, we're going to assume that there's some variance around this mean so that we can handle.",
            "All the randomness in the ratings of the of the of the users.",
            "So yeah, so RJ is going to be Gaussian distributed with mean given by this dot product and the variance given basic more square.",
            "And you can now I forgot to to describe to tell you what is the prior over this features and the prior over Sigma Square.",
            "But you can imagine that you know these features might be given priors which are Gaussian and this feature is also Gaussian.",
            "Well for Sigma Square you might use a gamma or it or invest, prior.",
            "And given our ratings, we can now again compute the posterior distribution over user features an movie features.",
            "And that basically kind of solves the problem for us because with the user features and product features we can.",
            "We can estimate how much does each user likes each movie.",
            "According to this model, of course, one thing to note about this is that.",
            "We are not just Catholic D dimensional vector, right?",
            "So we're all kind of complex beings.",
            "We like certain movies.",
            "For some reason we don't like certain movies for some reason, and really this is simply a very simple.",
            "This is simply a model which makes very strong simplifying assumptions about the generative process.",
            "That that gave us the movie ratings, but still is a very useful model.",
            "And you can count test out this model on bye bye.",
            "Um, entering this into the Netflix competition and so forth, right?",
            "So, you could kind of test it by looking at how well it predicts ratings, which which it doesn't.",
            "It hasn't observed, and this does really well.",
            "So yeah, so I like to kind of emphasize that of course all models are wrong, But some models are useful.",
            "OK, so this is a phrase by.",
            "A famous statistician that I forgot the name of."
        ],
        [
            "Right so so that's Bayesian machine learning.",
            "Of course, today and tomorrow I would like to talk about a particular class of Bayesian models called Bayesian nonparametric models.",
            "And if you are interested in this, there's a book on Basinal parametrics, edited by shot at all.",
            "That just came out last year."
        ],
        [
            "OK, so let's start off by curve telling by saying what is a nonparametric model.",
            "OK, so basically I can tell you what it is not.",
            "It's basically not a parametric model, So what is a parametric model?",
            "A parametric model is basically a model with a certain fixed number of parameters.",
            "So for example, a Gaussian is a parametric model because it has a mean and variance.",
            "It has two parameters and that's it.",
            "OK.",
            "So basically a nonparametric model is not a parametric model.",
            "Not in the sense that it doesn't have any parameters, but rather in the sense that it has.",
            "Either an infinite number of parameters or a number of parameters that changes with the amount of data that is observed.",
            "A man is in that sense that it is not a parametric model, somewhat different way of viewing.",
            "What is a nonparametric model is that it's a family of distributions that is dense in some large space.",
            "OK, I'll come back to this a bit later.",
            "So the idea, yeah, come into this later.",
            "So why are we interested in doing Bayesian nonparametrics?"
        ],
        [
            "Um, so let's imagine the following.",
            "So imagine that you're trying to do classification.",
            "So in classification you have some inputs and you have some outputs.",
            "And you may have a space of very large space of hypothesis which basically causes corresponds to the space of all smooth functions from input space to output space.",
            "So your output could be.",
            "If it's binary, then it's just consists of either zero or one, but your input could be, say, a Euclidean space.",
            "So this is the space of all possible smooth functions from your input space to your output space, and there might be some true underlying function which you like to learn about.",
            "OK, so this function is the function that actually tells you for every input what should be the true output.",
            "If you do something like in neural networks or linear regression or something, then basically you're assuming a parametric model of this space."
        ],
        [
            "And what is a model and model is basically a very tiny subset of this space that can be parameterized by your model.",
            "Um?",
            "And if you're being Bayesian about this, you might put a prior on on this very small space.",
            "Um?",
            "And it's very unlikely that the true function will actually be in this space is very unlikely that your true function is, say, a linear function, because you know many in many real world type of data sets is a lot more complexity involved.",
            "So maybe what you might do is that you might consider a whole sequence of models of increasing complexity, each of which tries to capture more and more of this space, so they're kind of growing in size, so you might."
        ],
        [
            "Consider a model of.",
            "This is a model and it might be nested in another model which.",
            "Is able to parameterise a larger part of the space."
        ],
        [
            "And maybe another one that's going to prioritize a larger part of the space.",
            "But if you assume that your model is a parametric model, is very unlikely that your parametric model will be able to parameterise this whole space of continuous and smooth functions from input to output, so it's very unlikely that your true function will ever be part of your model space.",
            "OK, So what is the idea of basing on prime tricks?",
            "There are kind of two ideas here.",
            "The first idea is that we would like to come up with a model that is so large and so."
        ],
        [
            "Reach that, it can parameterise all of your hypothesis space.",
            "OK, so you'd like to cover.",
            "The word is that you have a model with large coverage, like to cover this whole space of continuous and smooth functions.",
            "Over all of this.",
            "So that's the first idea, and the reason why we want to do this is that we would like to have certain guarantees which allows us to show that as we see more and more data that our model will converge to the true.",
            "To the true function.",
            "Um?",
            "OK, so and the second idea of Bayesian or Private Ricks is that this is of course a very very large spaces, typically infinite dimensional.",
            "In the case of smooth functions will be possibly uncountably many dimensions, right?",
            "So this is a very large space, so how can we actually build?",
            "How can we?",
            "How are we able to do efficient learning in this space?",
            "And the only way in which we could do efficient learning is that we would like to impose certain modeling assumptions or structures on our model.",
            "And the idea here is that we'd like to place high probability two models, which we think are likely.",
            "An low probability to models that we think are unlikely OK.",
            "So this account, like the things are here.",
            "So for example, in the case of classification, we might place high probability on functions which are smoother.",
            "Or maybe which might have might be described more or less using a a basis consisting of a small number of basis functions.",
            "So we'd like to give high probability to small functions an lower probability.",
            "Two functions which are not as smooth, which are kind of.",
            "I'm kind of thinking about this function says on the outside of this space.",
            "And this is a very important part of basic nonparametrics because it allows us to basically make certain assumptions about our problem, and that allows us to learn more efficiently.",
            "OK so to be I guess I'll kind of go through a few of this reason."
        ],
        [
            "More specifically, OK.",
            "So the first reason well?",
            "That was kind of a high level picture, so tell you a bit more about the specific reasons why you might want to do basic nonparametrics.",
            "The first reason is that.",
            "Um?",
            "It's kind of very nice way to do to actually get away from model selection and model averaging.",
            "OK, so I guess if you've tried to do any to solve any real problems with machine learning.",
            "You often find that trying to do model selection or model averaging is very expensive, right, so?",
            "You know, if you're trying to do clustering, how many clusters are there?",
            "If you're trying to do?",
            "Classification how many basis functions should you use and to do that you need to do like cross validation or some sort of Bayesian way of doing model selection and all of them are expensive because it allows.",
            "It basically requires you to run.",
            "A series of different models on the same data set and then to decide which one to select.",
            "And the reason why people do model selection or model averaging is because it's kind of used to prevent basically overfitting or underfitting.",
            "You want to find a model of the right size given your data set.",
            "So the idea is that in a Bayesian model, if you kind of specify it well, then it should not overfit anyways, because if you think about what is overfitting, it's when you do something like maximum likelihood fitting of your parameters.",
            "An you come up with some single parameter right?",
            "And that parameters kind of fit it to your data.",
            "But in the Bayesian approach, you never actually fit anything to your to your data.",
            "You always just compute the posterior distribution over your parameters or over your latent variables.",
            "And because you're not fitting anything to your data, you're not.",
            "You will not be overfitting through your data either, right?",
            "So that solves the problem of overfitting.",
            "How about underfitting?",
            "So the idea of basing on Parametrics is that if you use a very large model then you won't under fit either.",
            "Right, so that so that leads us to Bayesian nonparametric models which will not overfit all underfit.",
            "So the idea is that we instead of looking at a series of models and then trying to decide which one to use, we simply look at one very large model and then we do Bayesian inference in that model.",
            "That count like allows us to not overfit to our data."
        ],
        [
            "OK, so reason #2 as I talked about just now, large function spaces so.",
            "In many machine learning applications, the object that we're often interested in learning about comes from some very large space.",
            "So in the case of classification or regression, we might want to work with the space of all possible smooth functions.",
            "In the case of density estimation here, you might want to work with the space of all possible smooth densities.",
            "An innocence, it's kind of easier to think about learning in this large function spaces.",
            "If you think about.",
            "Basically learning this.",
            "Objects this infinite dimensional objects themselves, rather than trying to learn about parameters of a parametric model which consists of a small part of this space.",
            "So this leads automatically to Bayesian non parametric models.",
            "So they're basically models over this very large function spaces.",
            "The idea is that we would like to place a prior over all possible functions and then do posterior inference in this space of all possible functions."
        ],
        [
            "So this recently.",
            "24 recently.",
            "So this is kind of follow up from the model selection model averaging problem question.",
            "In many graphical models are in actually many models.",
            "We are often interested in learning structures.",
            "Certain structures from data, so we might want to learn what is the structure of a graphical model given observed variables at the leaves of the model.",
            "We might be interested in learning the structure of a tree given observations at the leaves of the tree.",
            "OK, so this is a tree of I guess everyday objects.",
            "Down here.",
            "OK and to to work with this off.",
            "A structure is what we will need is a Bayesian.",
            "Prior over this combinatorial structures.",
            "And, uh, nonparametric models are kind of.",
            "A particular way of assigning priors over this community structures, and they're typically there are sometimes.",
            "Interestingly enough, actually end up simpler than the parametric models.",
            "And yeah."
        ],
        [
            "OK, finally the fourth reason is that basically the field of stochastic processes and probability theory have come up with lots of different interesting stochastic processes with lots of interesting properties.",
            "And.",
            "By count looking at what are the stochastic processes out there?",
            "We can find lots of interesting models with interesting properties.",
            "So I will talk about a number of this today and tomorrow.",
            "So we have properties like projectivity exchange ability.",
            "We have properties like power law properties.",
            "OK, there's another typo this repayment your.",
            "Yep, so the idea is that.",
            "In the last few years we've come.",
            "Being able to come up with models which have interesting properties which we wouldn't have discovered if we did not look into basing on primary models.",
            "And these models are.",
            "Often use very useful and you will see a certain number as some number of examples of these models which were able to do very well compared to the.",
            "A parametric models."
        ],
        [
            "So before I finish this section, I'd like to say a few words.",
            "So as I said before nonparametric models.",
            "Doesn't mean that they don't have parameters, it just means that they are not parametric models, so they still have parameters.",
            "They just still have.",
            "They just have an infinite number of parameters.",
            "Or maybe they may have a number of parameters that grows with the amount of data.",
            "Um?",
            "Another point which I'd like to make is that there's no free lunch.",
            "OK, you have to pay for your lunch here, right?",
            "At least your supervisor has to pay for lunch here and in machine learning.",
            "That count means that basically you cannot do any learning without having to put in some effort into making assumptions about your data about how your data came about.",
            "Um?",
            "An nonparametric models.",
            "It's not that they don't make assumptions, they still make assumptions, but it's just that they are made in such a way that they are bit less constrained by your assumptions.",
            "Um Ann, I'd like to say a word here about kind of Bayesian way of doing machine learning as opposed to a non basin waste.",
            "So the nice thing with a Bayesian approach to machine learning is that.",
            "We are being very honest about what sort of assumptions we made in coming up with our model because we have to express everything into in this joint distribution over parameters and latent variables, and that joint distribution actually.",
            "Makes explicit all the assumptions that we make about our data generating process.",
            "And that's kind of nice because it allows us to be honest about what we did, and it allows us to think more clearly about what were the assumptions that work and what were the assumptions that don't work.",
            "And finally, I'd like to say that so.",
            "Um, so many models can be nonparametric in some sense.",
            "An parametric in some other sense, and these are perhaps a better name for this are semiparametric models.",
            "So they're kind of like half half really.",
            "So the idea is that there are some parts of the model which we liked to make nonparametric.",
            "But then there are some other parts which we know a lot about and we can make very strong assumptions that you know they should come from some parametric form."
        ],
        [
            "So what are the self things that people work on in basic nonparametrics?",
            "Well there's the problem of count developing kind of novel classes of nonparametric models with that may have some interesting properties that may be suitable for modeling certain types of data.",
            "There's the problem of developing novel algorithms that can efficiently cope with this infinitely many parameters in our non parametric model.",
            "And then finally, for the theorists among you, there's lots of interesting work to be done in developing the theory, which allows us to show whether a certain class of nonparametric models will converge to the true distribution or function if given enough data, and how quickly does this convergence occur?",
            "So if your model you can do number one.",
            "If your computer scientist could do #2, and if you are probably the theorist or something you can do the military."
        ],
        [
            "So.",
            "Um, there's been lots of previous tutorials on based on Parametric's already, I've listed some of this here and you can find some on video lectures as well, and there are a number of review papers and so forth on based on prometrics.",
            "So for this tutorial I'll concentrate particularly on the Dirichlet process.",
            "Basically because it's kind of a cornerstone of based on parametric San would be nice to be able to understand it, kind of.",
            "In a bit more detail than just knowing what it is at a high level and then of course we'll look into the various extensions and generalizations of the judicial process, including things like payment, your processes, hierarchical directly, processes, hierarchical pineal processes, random partitions and so forth.",
            "Are there any questions about this powder?",
            "Tutorial.",
            "Yes.",
            "Convert structure.",
            "Something similar.",
            "Yeah.",
            "Reason we love somebody or something else is more like the prior is simpler.",
            "So."
        ],
        [
            "Let me see.",
            "I might come back to this a bit later, actually, because we'll be talking about random trees actually tomorrow.",
            "But the idea is that.",
            "If I give you this data set, you may come up with some prior over this data set OK, But if I then told you that hey, there's this something else which I haven't really told you about.",
            "So would that change your model in a typical parametric model that might change your model.",
            "Just by me saying that, oh, that's you know, there's three other objects which I hadn't told you about.",
            "I'll tell you about them tomorrow, right?",
            "An by come making this nonparametric prior assumptions it.",
            "We can come up with models which would not be affected by this of additional data items which were not observed.",
            "OK, and that kind of simplifies your inference process.",
            "OK."
        ],
        [
            "So I guess I'll just continue on with your issue."
        ],
        [
            "Assess as I said, them directly.",
            "Processes are cornerstone of modern Bayesian nonparametrics.",
            "And it has been actually rediscovered many, many times before, as basically the infinite limit of finite mixture models.",
            "Even in machine learning it was rediscovered twice and it statistiques many times.",
            "And formally, they were defined and they were given this name of a directly process by focusing back in 1973, about close to 40 years ago now.",
            "As basically a distribution over measures.",
            "I guess in the next 2 slides.",
            "Actually, not in the next 2 slides it a little bit further down.",
            "I'll be telling you what are measures before I can tell you what are distributions over measures.",
            "Right?",
            "Um, an richley process being kind of this very basically a Canonical example of a nonparametric model.",
            "It does kind of like different ways in which you could derive it.",
            "Ann, it's kind of a special case of lots of different processes and will be looking at will be deriving the directly process actually in three different ways.",
            "The as the infinite limit of Gibbs sampler for a finite mixture model and using the Chinese restaurant process an using the stick breaking construction."
        ],
        [
            "So let's start off with this infinite limit of finite mixture models."
        ],
        [
            "I guess the simplest recall that we had this model, this model for model based clustering, which is typically called a finite mixture model.",
            "An is finite in the sense that there's a finite number of of mixture components or clusters.",
            "And it's a mixture model in that you assume that the data that you observe is a mixture of different, basically heterogeneous populations.",
            "Right?",
            "Right, and so just to recall for you.",
            "The generative model is that we have some indicator variables that I which tells us which cluster XI belongs to.",
            "And it can take on one of K values, K = 1, two big K. The mixing proportion is pie with a prior given by Additionally.",
            "And again we have for each cluster K we have a parameter.",
            "Theta, Starkey, and we have a prior over Theta Star K. Given by H. I have people here who has seen directly distribution before.",
            "Who has not seen directly distributions?",
            "Quite a lot.",
            "OK so.",
            "I'll spend a few slides on them up.",
            "Oh by H. What I mean is a prior over the parameters of that cluster.",
            "So F is going to be your your cluster distribution and H is your prior so.",
            "In the case of a mixture of Gaussians, OK. F would be a Gaussian parameterized by a mean and variance, and that theater will be vector of length two is it will be the mean and the variance, and H is going to be a prior over your mean annual variance.",
            "So you might think that maybe the mean.",
            "I expect it to be lying close to zero OK, and the variance would be approximately one.",
            "OK, so those are counted priors on your mean annual variance.",
            "OK."
        ],
        [
            "So richly distribution is basically a.",
            "Distribution on the K dimensional probability simplex.",
            "So this is basically the set of all vectors Pi such that each entry of the vector is non negative and it sums to one.",
            "Typically you can."
        ],
        [
            "Think of this as a.",
            "Maybe I should draw this?",
            "You have a 3 dimensional space.",
            "OK, so this is your.",
            "\u03a0 one Pi 2\u03c0 three, and this is a space which basically cause bonds to something like this.",
            "Basically, it's this plane which passes through.",
            "100 010 N 001 so it's a plane in that basically.",
            "Every point of this probability simplex is going to be a vector of length tree, each of which is each entry is positive or is not negative and they have to sum to one.",
            "So it's spent by this tree vectors.",
            "Basically it's all convex combinations of this tree vectors.",
            "OK."
        ],
        [
            "Um?",
            "And Additionally, distribution is a distribution on this space and it has a density given by this thing.",
            "OK, so this here is going to be the basically is the normalization constants for the density.",
            "And over here is the part of the density, which does depend on pie, and it's basically a product over K equals to one to big K of Pi K race today.",
            "Some exponents minus one, so this exponents are the parameters of the delay distribution.",
            "And this gamma functions are basically what they are called gamma functions and they are given by this integral.",
            "Basically, that's the normalization constants.",
            "So the Richland distributions are used all over the place in probabilistic modeling and the main reason for that is basically that is the standard distribution for probability vectors an.",
            "This is due to the fact that they are conjugate to the multinomial.",
            "Distribution and particularly conjugate to discrete distributions."
        ],
        [
            "So just to visualize what this dense, this looks like OK if the so.",
            "Here's the Dirichlet density down here.",
            "Where I've count generalize this to one where Alpha here is a vector where every entry is positive so that Alpha there's a typo should be distribute, K goes to one to big K of Pi K race to the Alpha K -- 1.",
            "And when Alpha is.",
            "A vector of ones.",
            "So there's a vector of ones.",
            "Then we see that this part here is going to be Pike, a race to the 1 -- 1.",
            "And that's just going to be pickey raised to the zero, and this is going to be one.",
            "And what that says is that the direction is going to be a uniform distribution over the probability simplex.",
            "When Alpha is greater than one.",
            "So for example if it's 222.",
            "The Dirichlet distribution is going to be a unimodal distribution with a with mean given by.",
            "Basically this vector normalized OK, so the mean is going to be at 1/3 one third, one third, and the total mass of the Alpha here basically describes how concentrated the mode is around the moon.",
            "Um?",
            "So when Alpha is 555, then we see that the distribution is more concentrated around its mean.",
            "Yeah, OK an if the Alpha vector is not symmetric, so for example is 255.",
            "Then the mode gets shifted off the center of the probability simplex to one of the edges or to a corner here, and more Interestingly, if the alphas are less than one, then we see that instead of a unimodal distribution we have a multimodal distribution where we have high probability around the corners and low probability in the center of the probability simplex.",
            "So that just is a visualization what the traditional distribution is."
        ],
        [
            "Right, so as I said, the traditional distribution is kind of all over the place in probabilistic modeling, mainly because it's conjugate to the multinomial right so?",
            "Let's see how this conjugacy work.",
            "If you look at the joint distribution over the prior over \u03c0.",
            "Add the cluster indicator variables that OK, so that's the prior.",
            "That's the conditional probability of each that I given Pi.",
            "So the priors given by this Dirichlet.",
            "The conditional probabilities we could collect up into a term like this.",
            "So basically every time that I take some value K, then we're going to have a Pi K contribution.",
            "An NK is the number of that ice that take on value K, so we can.",
            "So if there are NK is that I stay on value K. Then we basically have Pike a race to the NK Times and this is over K from one to beat K. And you can see that the conditional probability of the zed vector here.",
            "Add in the prior prior on Pi.",
            "They cannot have a similar functional form, and if you actually multiply these two probabilities together, you get the joint probability is going to be this normalization times product key equals one to big K of pikey.",
            "Race to the Alpha divided by K plus N K -- 1.",
            "Right, and so if you normalize this.",
            "So that you get the posterior distribution of Pi given set, then you see that the posterior distribution will also be a Dirichlet.",
            "OK, but with parameters updated by NK.",
            "OK um.",
            "While the marginal distribution over these at vector here, you can compute it by basically taking the ratio of this probabilities so basically.",
            "Yeah, so if you take the.",
            "Our joints divided by the posterior.",
            "You get the marginal probability.",
            "And the fact that I can write down both of these equations says that both the posterior and the marginal distributions are tractable and can be computed easily, and this is what's meant by conjugacy here.",
            "Basically, it allows us to compute the posterior, easily, allows us to compute the marginal easily."
        ],
        [
            "So coming back to our finite mixture model, we can now make use of this conjugacy to actually derive a Gibbs sampler for this, so I guess Peter Green has talked quite a bit about Gibbs sampling already on Thursday and Friday, right?",
            "So this should be easy for you.",
            "So if you do give sampling, So what we'd like to do is to basically compute.",
            "Posterior over the parameters and the latent variables given observed that I observed variables XI and Gibbs sampler is basically a Markov chain Monte Carlo sampler, in which we update each unobserved variables by computing its conditional distribution given everything else sampling from that conditional distribution and then repeating this overall unobserved variables and just repeating this until you've converged to the posterior.",
            "So you can.",
            "We can derive the conditional distribution of that I.",
            "Given all the other variables and you can see you can derive that the conditional probability of that I taking on value K. It's going to be proportional to two terms.",
            "It's going to be proportional to pikey, which is the basically the conditional prior.",
            "That said, I take some value K times conditional likelihood term, which tells us how likely is it that we observe XI given that it belongs to cluster K. And if you normalize this, you are going to get the basically the responsibility of cluster K for data item I.",
            "And this is the conditional distribution for every eye.",
            "For every key you can compute this and you can update each of those at ice easily by just doing that.",
            "And given the sad eyes, we would like to also get the conditional distributions of the parameters as well, and you can see that for the pies because of the Dirichlet multinomial conjugacy, the conditional distribution of Pi given all the other random variables is going to be directly as well.",
            "It's going to be given by this directly with updated parameters.",
            "While the conditional distribution of theater of data K given all the other variables is going to be the prior times likelihood of all the data items that is currently assigned to cluster key.",
            "Turns out that this skip samplers actually not as efficient as another Gibbs sampler where called collapsed Gibbs sampler and this keeps sampler works by basically taking this model, marginalizing out Pi an theater, and then just updating the the only variables there left, which is The Jets.",
            "And if you look at the collapsed Gibbs sampler, basically the conditional probability of that I given all the other variables is.",
            "Of this form, again, we have a conditional prior times the conditional likelihood.",
            "Where the conditional prior has changed from paikea to this form here because we've integrated a pikey.",
            "With integrated out the Pi vector and this conditional prior is kind of quite intuitive.",
            "It just says that.",
            "It's going to be proportional to Alpha divided by K. So that's coming from the prior on \u03c0.",
            "Plus the number of data items that were currently assigned to cluster K If you don't count data item I itself.",
            "So the idea is that a cluster that has lots of different data items assigned to it will have a higher probability of being of taking responsibility for this data item I. OK. And then we also have a conditional.",
            "Likelihood of data item XI, given that it's assigned to cluster K and this term here is basically given by the predictive probability of XI given.",
            "Given all the data items that work, there are currently assigned to cluster key except for data item I and you can compute this in the following way.",
            "And if you assume that the prior distribution age is conjugate to our F distribution, then this can be computed efficiently as well, just as the Dirichlet multinomial conjugacy allows us to compute all the probabilities efficiently.",
            "If you assume that H is conjugate to F, we can also assume that this can be computed efficiently too, so this gives us gifts, collapse keeps updates for these at I variables integrated out both \u03c0 and Theta from this model.",
            "OK.",
            "So as I said."
        ],
        [
            "We can obtain the delay distribution by basically taking this infinite limit of Gibbs sampler in particularly of this collapsed Gibbs sampler.",
            "Well, perhaps I should say why we might want to do this.",
            "OK, so we might want to do this because.",
            "Because we may not want to make any assumption about the number of clusters in our data set, so it could be that when we do clustering as we see more and more data, we might see more and more clusters and the more data.",
            "Items we see the more clusters there might be in them.",
            "OK, So what that says is that in the true underlying generative process that might not be a fixed finite number of clusters.",
            "There really is an infinite number of clusters.",
            "Is just that, given a certain small data set, you only see a finite number of of them.",
            "So this by taking this infinite limit of K going to Infinity.",
            "We are basically making that assumption that there really is an infinite number of clusters.",
            "That generated our data, but it's just that you know, given that if you observe only a finite number of data items, you only.",
            "See less than 100 number of clusters, see.",
            "So before we take this, K goes to Infinity limit.",
            "We can imagine that there's kind of a very large value for key.",
            "So in particular, if K is very very large then it will be.",
            "Much larger than N. But an here is going to be our data set size.",
            "Basically there will only be an data items in our datasets.",
            "An IF K is much larger than N, then we know that there are at most N clusters which will be occupied in the sense that end clusters are going to be.",
            "Associated with the end date items OK.",
            "Typically there will be less than N clusters associated with our data, but there can be at most N because every data item can only be assigned to at most one cluster.",
            "Right?",
            "So we know that because if K is much larger than N, most of these K clusters will be basically empty, right?",
            "There won't be assigned any data items at all, right?",
            "So they're empty and we can kind of lump them all together just to make our computations more efficient.",
            "And if you do that, then the conditional distributions for collapsed Gibbs sampler will look like this.",
            "So for a cluster K. Which this have some data item assigned to it?",
            "Then NK will be positive and the conditional probability of that I equals 2K is going to be again this conditional prior.",
            "This is unchanged times the.",
            "Conditional likelihood of XI given all the other data items assigned to cluster K. Will also have the conditional probability of.",
            "Data item I being assigned to some cluster which is not.",
            "Associated with any other data item and you can compute that by basically looking at basically summing over all of this empty clusters, right?",
            "So for each of the empty clusters, we're going to have a conditional prior of Alpha divided by K. Divided by N -- 1 plus Alpha and then we have a conditional likelihood which is basically the probability of XI given that there are no other data items being assigned to that cluster.",
            "Yes.",
            "Key star is going to be the number of clusters which have some data item.",
            "Assigned to them.",
            "OK, so these are the non empty clusters, so that K -- K style is the number of empty clusters, right?",
            "So when you sum over those probabilities, you're going to get came.",
            "In this case here, yes.",
            "I'll come to that later actually yeah.",
            "So basically Alpha is going to be related to what you expect.",
            "The number of clusters you expect to see in your data set.",
            "I'll come to that in I don't know if 10 slides or something.",
            "Basically, the larger Alpha is, the more clusters we tend to expect to see.",
            "So we know that K star is the number of occupied clusters and it will be at most NK.",
            "An N is going to be much smaller than K. If K is very very large, right?",
            "So now we can take Kate to go to Infinity because in these two equations here.",
            "When K goes to Infinity, we see that Alpha divided by K is going to go to 0.",
            "So that's going to go to 0.",
            "K -- K Star divided by K is going to go to one because case that is upper bounded by N&K is going to Infinity right?",
            "So this time this ratio here is going to go to one."
        ],
        [
            "So that's going to go to 1A times one is this Alpha, so this is.",
            "The infinite limit of the collapsed Gibbs sampler and basically this gives us a collapsed Gibbs sampler for an infinite mixture model with an infinite number of clusters.",
            "And you can actually implement this and it will work very well OK."
        ],
        [
            "But actually it kind of doesn't make sense as the model itself doesn't make sense.",
            "And the reason is because if you look at any particular cluster in here, it will be assigned basically a. I prior probability of 0 being.",
            "That would be a basically yeah, so the probability of that particular cluster being assigned to explain a particular data item will be basically 0.",
            "And basically the reason is because if you look at the prior, the prior is a symmetric Dirichlet, so the prior probability of cluster K being assigned a particular data item will be just one over Big K and one over Big K is going to go to zero as big K goes to Infinity.",
            "And basically.",
            "The.",
            "Basically the the math that we can't have to go through to actually derive the judicial process are basically kind of better ways of kind of making this infinite limit construction precise.",
            "Like what does it mean?",
            "What does this infinite limit mean when we take big K to go to Infinity?",
            "OK, and this two ways are basically different ways of looking at the original process, kind of.",
            "Better ways of making this precise?",
            "And basically we can think of a Dirichlet process as a basically an infinite dimensional directly distribution.",
            "Basically is the think of it as the prior is the limit of the Dirichlet prior in our.",
            "Over the mix it mixing proportions as the number of clusters goes to Infinity."
        ],
        [
            "OK, so I guess I'll move on.",
            "To define what is additional process?",
            "Before I do so, perhaps I would actually spend 2 slides on some probability theory.",
            "Does everybody here know about what is?",
            "What is probability theory or what is a measure?",
            "Anybody reason OK and who doesn't know?",
            "I guess.",
            "Yes, OK, that's fine.",
            "I am explaining everything.",
            "I just like to see."
        ],
        [
            "So this is just to give you a little bit of flavor of what is measure theory and what is probability theory.",
            "Measure theory is actually a theory of how you measure things, and the South measures that you think about is things like for example, you know.",
            "Given a set of the real line, what is its length?",
            "So given, say, you know a stick of this length, what is its length given a state of this length, what is length may be given a an object like this?",
            "What is this volume?",
            "Or what is this mess?",
            "So this account, like a theory of how you measure things and solve things that we like to measure, basically subsets of some space.",
            "So the space that we're going to work with.",
            "Is going to be theater.",
            "And the set of subsets which are going to be measurable in this space.",
            "It's going to be given by Sigma OK, Big Sigma.",
            "An basically this Sigma here is going to be called a Sigma algebra.",
            "And what if this is simply a family of subsets which are which can be measured in our theory?",
            "Anne Anne, what are the subsets that could be measured?",
            "Firstly we have to assume that we can at least measure something.",
            "OK, so that this Sigma cannot be empty.",
            "In particular, we can assume that the empty set is measurable.",
            "We know that you know the volume of the empty set is going to be 0 or the length of an empty set is going to be 0.",
            "If we say that.",
            "If a is going to be a measurable set, so if you can measure a.",
            "Then we can pretty much assume that the complement of a should be measurable as well.",
            "So if you can measure the whole set, we can measure a, then the the.",
            "The measure of the complement of A is going to be the measure of the whole set minus the measure of a right?",
            "So that's says that if A is measurable, then its complement is measurable.",
            "If we have a sequence of measurable sets, A1A two and so forth, then a good assumption is to assume that the Union of this sequence is also going to be measurable.",
            "OK. And what is a measure?",
            "A measure is simply a function that takes.",
            "As input, this measurable sets so it's going to be a function from the Sigma algebra to the positive rail line.",
            "And basically mu of a is going to be the length of a or the volume of a.",
            "And of course, the measure of the empty set is going to be 0.",
            "OK, if we have a sequence of measurable sets and they are disjoint, then it's kind of a good assumption to make that the measure of the Union would be the sum of the measures of the individual sets.",
            "That's this thing, and a probability measure is 1 where basically the measure of the whole space here is going to be one.",
            "OK, so in probability theory measure.",
            "So this subsets are going to be events and a measure of the event is basically going to be associated with the probability of that event.",
            "Of that random event, write an if we have disjoint events, so we have events for which at most one of them will occur, then the probability of the Union of the events will be.",
            "Of course the sum of the probabilities of the events.",
            "We don't really need to worry about things which are not measurable actually because in machine learning we never really deal with non measurable things anyways.",
            "So everything we consider here will be measurable."
        ],
        [
            "Right so.",
            "So that tells us what are probabilities, what events, events are measurable sets, what are probabilities that simply the measure of this manageable sets?",
            "How about random variables so a random variable is basically a function is in fact a deterministic function in this theory of probability theory.",
            "So a random variable F is going to be a function from a measurable space to another measurable space.",
            "Anne, we need to have.",
            "Some properties on Earth.",
            "Basically it has to be measurable.",
            "And one way in which you could think about this is that.",
            "If you think about how you might implement a random number generator right so?",
            "For example, you can implement a random number generator for at exponential random variables by doing the following right in Matlab anyways, you can first generate a uniform random variable on 01 and then just pass it through the.",
            "A function which is negative long of that variable.",
            "So.",
            "So and.",
            "Ah.",
            "A random variable which is which has a distribution given by an exponential distribution with parameter of one.",
            "We can generate a sample from this random variable by sampling U from a uniform distribution.",
            "On 01 and then setting X to be minus log of you.",
            "OK, and you can work out that if you generate such a uniform random variable and then pass it through the minus log of you, then this X here is going to be random because US random.",
            "And that the distribution of Axia will be just exponential distribution with the rates of 1.",
            "So if you think about what this function does.",
            "It takes us.",
            "It is a deterministic function, so I've just written it down.",
            "It's deterministic in the sense that what we do to this U is fixed, but what makes it random is that it has this has.",
            "As in, put some random number itself so we can think of this random variable X here as basically a deterministic functions of some other random variable, and basically that's what's happening here.",
            "A random variable X is going to be a fixed function measurable function X from some space to another space an we have basically put all the randomness of this random variable X into this probability space of.",
            "Theater.",
            "Right?",
            "Yeah, so I guess that's what this is saying.",
            "And then finally, what is the stochastic process?",
            "So Additionally process is called the Richland process because it is a stochastic process.",
            "Ann is stochastic.",
            "Process is simply a collection of random variables.",
            "OK, so.",
            "So it's a collection of random variables XI where I is belongs to some index sets an the only difference between what a stochastic processes an what say is a is a graphical model.",
            "Is that a stochastic process?",
            "This index set I can be infinite and can be uncountably infinite, so we could describe the joint distribution over an uncountably many random variables under stochastic process, and you can't need this.",
            "Because if you think about.",
            "A function write a function is basically.",
            "For every input you have some output, right?",
            "Anna Random function is in a sense a stochastic process, because we can think of a random function.",
            "As a set of random variables, one for every input.",
            "And this input is going to be indexed by its input space, which could be something like are out of them or something."
        ],
        [
            "OK.",
            "So is that clear for probability theory?",
            "It's just cause simple domain thing which I like to you to take away from this is that basically.",
            "Measure is is this a function itself, right?",
            "Imagine this function which tells us for every measurable set, what is it.",
            "Length or what is its volume or what is its probability?"
        ],
        [
            "OK.",
            "So now we can define what's original.",
            "Process is basically going to be a random probability measure.",
            "So what is a random probability measure is simply a random function right from the measurable sets to the positive rail line.",
            "And it has to have the following property so.",
            "If this is our space here.",
            "Then if you take any partition of the space, so if A1 to a big key is a partition of the space in that their union is going to be all of the space.",
            "And they're gonna be disjoint OK?",
            "Then if you consider this vector here, OK.",
            "Which basically G of A1 is going to be the probability assigned to this part of the space G of a two is going to be the probability assigned to that part of the space and so forth.",
            "So this has to be a probability vector here GG of a one to JFK right?",
            "Because?",
            "G is going to be a probability measure, so the probability assigned to the whole space has to be 1, right?",
            "So.",
            "This has to be a probably probably the vector, and if G is a random probability measure, then this vector here has to be random.",
            "So basically we can think of this.",
            "G is a, it's a random probability measure in the sense that you can think of.",
            "It has to has to have a total mass of 1 and it has.",
            "To put a certain amount of mass in each of these parts of the space, and it could be random in terms of how much mass it puts into each part of the space.",
            "And what we are saying is that the amount of master it places until each of these parts of the space as a sum to one.",
            "And it has to be nonnegative an it's random an we assume that.",
            "Basically, this random probability vector here has a prior which is given by a Dirichlet.",
            "And that rich lips can be parameterized by Alpha and by H. Alpha is just going to be a positive scalar.",
            "An H is a.",
            "Is another measure so that each of a 12H of a K is going to be a vector that sums to one.",
            "OK.",
            "Right, so those are the parameters.",
            "OK. How much time will come running out?"
        ],
        [
            "So.",
            "Yep, so Alpha is so these are the two parameters.",
            "Alpha H. Typically the Alpha parameters is is called the strength parameter or the mass parameter or sometimes called the concentration parameter H is called the base distribution.",
            "You can workout that.",
            "Given a measurable subset A, then G of a is going to be a random variable taking values between zero and one.",
            "And you can work out there that the expectation of G of a is just going to be H of a.",
            "So what that says is that the base distribution H here is basically the mean of the directly process.",
            "Um?",
            "And you can also workout the variance of this random variable G of A.",
            "IS has a form which gives which is given by this and what that says is that the strength parameter Alpha is.",
            "Basically you can think of it as an inverse variance, right?",
            "So the larger Alpha is then the smaller this variance.",
            "Yeah, and if Alpha is very small then this variance is going to be large."
        ],
        [
            "OK.",
            "So, um."
        ],
        [
            "Actually, I think I have a. I can show you a demo, so here's comfort a little demo that.",
            "Show she was a draw from a directional process.",
            "So.",
            "We know that, so this is going to be additional process where the space is going to be one and the total area of this thing here is going to be the total probability assigned to the whole space.",
            "And of course that total probability has to be one.",
            "So this is basically a square on 01201.",
            "So it's a it's a random probability measure, right so?",
            "We can ask how much of the mass is going to be on the in between zero to 0.5 and how much of the mass is going to be between 0.5 to one and the amount of mass is going to be random, so we can sample from that.",
            "So weird.",
            "OK, I actually did that twice, so we can ask what's the total mass between zero to 0.25 once the total mass between 0.2525 to 0.5 was the total mass.",
            "Here was a total mess here.",
            "And that's going to be random, so the amount of mass we assigned to each of this is going to be random angwe.",
            "In this particular draw, we place most of the mess in here a bit here.",
            "A bit here and very little in here.",
            "Now we can take each of these little segments here and ask and split into four different segments and say which?",
            "Of the total mass assigned to this whole segment, how much do you want to place into one of these four segments?",
            "You can do this and you can see that it assigns.",
            "Kind of this mess into this little bit here and this bit.",
            "You know it's tough.",
            "Your account will basically iteratively.",
            "Placing the mass into different parts of the space such that the total mass that we begin with is one OK. Ann is a random process so that this gives us a random probability measure.",
            "Yes.",
            "Pins.",
            "Yes, that's right.",
            "Yeah, it was the area of the bar actually, yeah.",
            "Oh, so that the background bars are basically the.",
            "The G when the when the partition of the spaces cost.",
            "So we start off with a with a cost partition of the space which is the whole space itself and we know that there's a total probability of 1 assigned to that.",
            "Whole space.",
            "And then when we re find the partition of the space we get smaller and smaller parts.",
            "OK, and in each of these parts we have a random probability of a random amount of probability mass assigned to that part of the space, and that amount is going to be given by the area.",
            "I still didn't get it.",
            "Um?",
            "What is comfort?",
            "An iterative process, right?",
            "So you can't take the partition and refine it, and you ask how much mass you want to assign to the more refined partition.",
            "In the things in the background, we just haven't decided how much of the mass is going to be assigned to each of the more finer partitions.",
            "We just know that the total mass has to be a certain amount and account this visualizing that total mass.",
            "Could be that yes.",
            "This one.",
            "After that.",
            "10 yeah so.",
            "OK, sure.",
            "We can try that.",
            "It says it somehow.",
            "OK, so we have a probability measure right?",
            "So it has to assign a total mass of 1 to the whole space.",
            "That's going to be the the total mess.",
            "Um?",
            "The next step is we're going to take this space.",
            "We're going to partition it into four quarters, and then we ask how much mass does it want to assign to each of the four quarters?",
            "OK, and if you sample that from a directly.",
            "Where can I get something like this?",
            "Where now it has decided that the total mass in here is going to be this amount given by the area.",
            "The total mass in here is going to be given by this area and basically this thing in the background is just the previous step of the iteration is not really in there is just to help you visualize what's happening.",
            "The next step would be you take each of these quarters an you further partition it into four quarters and you ask how much mass does it have to be?",
            "Does it want to place in each of those four quarters?",
            "And the total mass assigned to the four quarters of this first quarter here has to sum up to the total mass that's already decided at this point.",
            "One of the one of the steps is that right you split into two at your complex person.",
            "That's what the yeah, that's right.",
            "Yeah, that's right.",
            "It's just that somehow, for some reason when I plug this thing into this projector every keypress I do is it gets translated into two key presses.",
            "Yes.",
            "Sample distribution but when you refine the partition, you're not getting independent sample from the refined position, right?",
            "It's not independent.",
            "Process for sampling from the replying partition to get it to match up with the previous refinement.",
            "Is it that you just take a Jewish way process that's independently defined on the partition?",
            "And is the mass attacks in the previous chapter we just recursively against?",
            "That's right.",
            "So basically when we when we want to decide how much when we take this.",
            "Part of the space and we cut it into four parts and we ask how much mass it wants to be.",
            "It wants to place in each of the four parts.",
            "That's precisely what we do.",
            "We actually sample from originally and then multiply that vector in.",
            "So that the total mass stays as it is and this actually comes from a very deep.",
            "Reason for further drilling.",
            "Basically a richly process is basically.",
            "It's basically the amount of mass it places on different parts of the space are basically independent, and the only assumption is that they have to sum to one.",
            "Besides this single assumption, everything is basically independent, which is why we could do this.",
            "OK.",
            "Right, so now we're going to take each of this split in, split into four, and then decide how much mass it wants to place into each of the four.",
            "So for this particular project for this I don't know, probably Alpha.",
            "It goes to five or something like that.",
            "I don't remember.",
            "I could look take a look.",
            "Yeah, OK. Um?",
            "Do this now, we can yeah so.",
            "Yep, so the original bit.",
            "Which is the total mass given by this thing by the rectangle at the back here OK?",
            "Which we can't quite see now is now going to.",
            "That is, total mass is going to be spread up in between these four segments and we have higher probability here and here and lower probability in here.",
            "And we can repeat this.",
            "And you can see what happens, OK?",
            "OK, and this is basically a sample from a directly process, right?",
            "It's a probability measure, so it has to be the total mass of all of these spikes has to sum to one.",
            "And basically the fact that it looks like this spiky objects says that it's an atomic distribution.",
            "I'll come back to that later.",
            "Yes, example is the basis.",
            "Yes, that's right.",
            "The base distribution is uniform.",
            "And if you do this multiple times.",
            "Then you will get a different draw from the judicial process, right?",
            "I can do this.",
            "OK, so that's a bit different, right?",
            "Every time is going to look at like an atomic distribution, but it's going to be random.",
            "Well, I guess I'm kind of out of time.",
            "I should take a break.",
            "Will come back in 20 minutes I guess, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'll be talking about Bayesian Nonparametrics, and I guess I'll start out.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a bit of introduction to Bayesian machine learning, first I guess in the last week you have heard a lot about various Bayesian approaches to machine learning and statistics, so this is a bit of a recapping us.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start off with probabilistic modeling.",
                    "label": 0
                },
                {
                    "sent": "Well, as we all know, a machine learning is all about data, and we know that data has lots of different has lots of noise and uncertainty and so forth.",
                    "label": 1
                },
                {
                    "sent": "an A big part of learning from data is about handling or being able to reason with the noise in the data and the idea of probabilistic modeling is that the language of probability theory is kind of a very rich language to express all of these uncertainties about.",
                    "label": 0
                },
                {
                    "sent": "About data and an allows us to.",
                    "label": 0
                },
                {
                    "sent": "Two reason coherently.",
                    "label": 0
                },
                {
                    "sent": "About this data and the solve models that we use to express this uncertainties about data called probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll see a number of these examples later, and I think probabilistic models has been very successful, especially in the last 10 years in in both statistics and machine learning.",
                    "label": 0
                },
                {
                    "sent": "And the reason is because, well, there's kind of a few reasons.",
                    "label": 0
                },
                {
                    "sent": "The first one is that it allows us to.",
                    "label": 0
                },
                {
                    "sent": "Count like Visualize our models of the data.",
                    "label": 0
                },
                {
                    "sent": "At the high level, without actually worrying about all the little details of.",
                    "label": 0
                },
                {
                    "sent": "About the model.",
                    "label": 0
                },
                {
                    "sent": "Another reason is that we can build very complex models from lots of simple parts and we can do both of this using the using the language of graphical models and which which I guess you're probably familiar with.",
                    "label": 0
                },
                {
                    "sent": "And another reason is that it allows us to separate out the modeling questions from the computational questions, right?",
                    "label": 0
                },
                {
                    "sent": "So it allows the modeler to bill to come up with a model and then it allow somebody else to build to derive algorithms for doing inference and learning in the models.",
                    "label": 0
                },
                {
                    "sent": "Given the models that the modeler has come up with, and I think this separation of the modeling question from the algorithmic questions is very important.",
                    "label": 0
                },
                {
                    "sent": "Basically because.",
                    "label": 0
                },
                {
                    "sent": "Well, there's a few reasons.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons is that people who know their domains well may not be the same people who can program well, and the people who can program well may not be able to know biology of linguistics domain well, so that allows us a separation of efforts into the modeling, self efforts and the algorithmics of effort, and this allows the field to advance a lot faster.",
                    "label": 1
                },
                {
                    "sent": "Another reason for this.",
                    "label": 0
                },
                {
                    "sent": "Separation of modeling and algorithm questions is because.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "When you do the your modeling of your data, you like to to think about this statistical questions without having to worry about all the different approximations that you have to do to get the model to work on your data.",
                    "label": 0
                },
                {
                    "sent": "And this is very nice because it allows you to think about you know in the ideal situation what should be done then that should be separated from what could be done on your data so that you can so that if it works, if your algorithms will.",
                    "label": 0
                },
                {
                    "sent": "Or doesn't work.",
                    "label": 0
                },
                {
                    "sent": "You can count like localize and figure out where is it that it's gone wrong.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is probabilistic modeling?",
                    "label": 0
                },
                {
                    "sent": "Basically, a probabilistic model is simply a joint distribution over a set of random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so here's our joints distribution and will typically be parameterized by some parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "And typically we can think about this probabilistic models as generative models.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we can postulate of particulare generative process that could have led to the observations that we actually do observe.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Alright OK, let me see so.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that with this joint distribution you can then do things like inference, which is to basically estimate what were the likely states.",
                    "label": 0
                },
                {
                    "sent": "Uh, Varun, observed random variables.",
                    "label": 1
                },
                {
                    "sent": "Given the states of our observed data.",
                    "label": 0
                },
                {
                    "sent": "So so this allows us to infer what we did not observe.",
                    "label": 0
                },
                {
                    "sent": "And of course, this conditional distribution of the unobserved variables, given the observed once, is simply given by base rule, right?",
                    "label": 0
                },
                {
                    "sent": "So this is just the ratio of the joint distribution divided by the marginal distribution of the observed data.",
                    "label": 0
                },
                {
                    "sent": "And of course, we don't typically know the values fault for the parameters for model an would like to learn about those as well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "The process of finding the lightly settings of the parameters which could have led to our data is called learning, and this is typically done by maximum likelihood, right?",
                    "label": 1
                },
                {
                    "sent": "So we like to find the parameter setting which gives highest, probably D to our observations.",
                    "label": 0
                },
                {
                    "sent": "There's lots of different things that you could do with probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "You could do things like prediction, so prediction this is can be phrased as basically computing the conditional distribution of some UN observed.",
                    "label": 0
                },
                {
                    "sent": "Data from the observed ones.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as your test set, and that's as your training set.",
                    "label": 0
                },
                {
                    "sent": "We can do classification, so if you have a bunch of different models, each of them indexed by C. Ann given by a different parameter, then you could decide given a test data item which class it belongs to by basically assigning it to the to the category or to the class that gives it highest probability under its probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So and in addition to this, of course there's lots of different other things that you could do.",
                    "label": 0
                },
                {
                    "sent": "I guess this a bit.",
                    "label": 0
                },
                {
                    "sent": "These are things like visualization, so you would like to visualize.",
                    "label": 0
                },
                {
                    "sent": "The generative process that kind of could have led to your data.",
                    "label": 0
                },
                {
                    "sent": "This is also very useful in things like interpreting the data and summarizing the data and so forth.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, there's this separation of the modeling question from the algorithm questions.",
                    "label": 0
                },
                {
                    "sent": "An one of the nice things will probably stay models is that there are lots of different.",
                    "label": 0
                },
                {
                    "sent": "There are lots of standard ways in which you could derive algorithms for your models, so you can derive inference algorithms, learning algorithms and so forth, and this algorithms things like you know the expectation propagation.",
                    "label": 1
                },
                {
                    "sent": "The expectation maximization algorithm junction tree.",
                    "label": 0
                },
                {
                    "sent": "There's variational inference, expectation propagation, Markov chain, Monte Carlo sampling, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this algorithms are pretty standard an given a model is relatively straightforward to to derive an algorithm for doing inference or learning in your model.",
                    "label": 0
                },
                {
                    "sent": "So one thing we should I which is I always feel.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a bit weird about probabilistic models, is that given our observed data X.",
                    "label": 1
                },
                {
                    "sent": "Here we have two things that we don't know about.",
                    "label": 0
                },
                {
                    "sent": "We have our latent variables, Y&R parameters, Theta, right and.",
                    "label": 1
                },
                {
                    "sent": "If you look about this.",
                    "label": 1
                },
                {
                    "sent": "The way in which we estimate the unobserved latent variables is called.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference from the way we estimate the parameters right?",
                    "label": 0
                },
                {
                    "sent": "So one is just doing one is just computing the conditional distribution while the other one is trying to do maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And always I always find that a bit strange because they should basically be the same processes both of them about the process of inferring what we did not observe from what we did observe.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically the Bayesian modeling approach.",
                    "label": 0
                },
                {
                    "sent": "Does that does both of these both inference and learning in exactly the same way, and to do that?",
                    "label": 1
                },
                {
                    "sent": "Of course you will need a prior distribution over your parameters, so this is our prior distribution over parameters so that we can estimate both the unobserved latent variables and the and observed parameters in the same process, which is simply to compute the posterior distribution over the latent variables and parameters together.",
                    "label": 0
                },
                {
                    "sent": "Given the observed data, so the posterior distribution here is going to be again given by base rule, which is now going to be a.",
                    "label": 0
                },
                {
                    "sent": "A ratio between the joint distribution over the all the random variables, whether observed or unobserved, an also with the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, and then of course this in the denominator here will have the marginal distribution of simply the observed variables, where we have marginalized out both the parameters and the latent variables.",
                    "label": 0
                },
                {
                    "sent": "Again, we can do things like prediction and classification and so forth, so prediction would correspond to computing the conditional distribution of a new test item given your training sets X one to XN.",
                    "label": 0
                },
                {
                    "sent": "And this would be of course just be basically integral of the conditional probability of the test item given the parameters where we've integrated up the parameters with respect to its posterior distribution distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                },
                {
                    "sent": "Follows from the from probability theory and classification would be kind of similar, so we would like to compute the conditional probability of the test item given the data set corresponding to the C class.",
                    "label": 0
                },
                {
                    "sent": "Where we have again marginalized out the parameters corresponding to the CS class.",
                    "label": 0
                },
                {
                    "sent": "So one thing to note is that this prior distribution is is very useful because it allows us to regularize our learning.",
                    "label": 0
                },
                {
                    "sent": "So I guess you guys have all know that when you do maximum likelihood learning then you have to be careful not to over fit to your data.",
                    "label": 0
                },
                {
                    "sent": "Write an one way in which you could address overfitting is by regularising your parameters, and one way we should think about this prior distribution here.",
                    "label": 0
                },
                {
                    "sent": "Is basically as a way of of regularising your parameters.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you look at the posterior distribution of the parameters given our observed data is going to be a ratio between the joint between the observed data and the parameters divided by the marginal.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this joint.",
                    "label": 0
                },
                {
                    "sent": "Distribution here it will be high if the particular setting of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Gives high probability of to the observed data and has high probability under the prior an.",
                    "label": 0
                },
                {
                    "sent": "Basically we can think of this product here as a tradeoff in finding good parameter settings that fits both the prior well as well as the data will.",
                    "label": 0
                },
                {
                    "sent": "Something else which I like to say is that in the Bayesian way of looking at things, there's really no difference between inference and learning, so both of them correspond to computing the conditional distribution of some unobserved random variable given the observed data.",
                    "label": 0
                },
                {
                    "sent": "And I think this is quite important, especially when you start looking into more complex models and and for this models that there's often not much distinction between what is a latent variable and what is a parameter, they're all simply random variables that you did not observe.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what are some examples of probabilistic models?",
                    "label": 0
                },
                {
                    "sent": "I guess the there's a very large class of probabilistic models called graphical models which are very useful in terms of reasoning about.",
                    "label": 0
                },
                {
                    "sent": "Uncertainty in this.",
                    "label": 0
                },
                {
                    "sent": "In our data.",
                    "label": 0
                },
                {
                    "sent": "In graphical models can be visualized in this graphs, right?",
                    "label": 1
                },
                {
                    "sent": "And basically each note of the graph correspond to a random variable and each edge basically correspond to a direct dependence of 1 variable on another variable, an importantly.",
                    "label": 0
                },
                {
                    "sent": "If you don't.",
                    "label": 0
                },
                {
                    "sent": "If there's no edge connecting to random variables, that corresponds to actually conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Between the random variables.",
                    "label": 0
                },
                {
                    "sent": "So in this graph, for example, it's because there's no age between earthquake and burglar.",
                    "label": 0
                },
                {
                    "sent": "That means that whether an earthquake happens or not, or whether a burglar comes into your house or not, are independent events.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is the Asian Network and this is the alarm network and I guess they're pretty.",
                    "label": 0
                },
                {
                    "sent": "Popular simple graphical models to illustrate about these things are.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll show you a few more examples, so this example for model based clustering is will actually return to this.",
                    "label": 0
                },
                {
                    "sent": "A few times over both today and tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And this is a model for clustering.",
                    "label": 0
                },
                {
                    "sent": "So what is clustering?",
                    "label": 0
                },
                {
                    "sent": "Clustering is the idea that given a data set which consists of heterogeneous of.",
                    "label": 0
                },
                {
                    "sent": "Uh, data items.",
                    "label": 0
                },
                {
                    "sent": "Basically you can think of the data items as coming from different sources.",
                    "label": 0
                },
                {
                    "sent": "Then we like to cluster the data items into the different clusters, each of which are kind of homogeneous within themselves and their and the different clusters are different.",
                    "label": 0
                },
                {
                    "sent": "So to visualize.",
                    "label": 0
                },
                {
                    "sent": "So if the blue dots here are our data items, then we would like to cluster them into three different clusters, which corresponds to well.",
                    "label": 0
                },
                {
                    "sent": "Um, three different types of data items that we observe, right?",
                    "label": 0
                },
                {
                    "sent": "And this is a model.",
                    "label": 0
                },
                {
                    "sent": "And we can take a model based approach to clustering.",
                    "label": 0
                },
                {
                    "sent": "An IS model based in the sense that we are going to postulate a particular model for the data items in each cluster.",
                    "label": 0
                },
                {
                    "sent": "So the model kind of goes as follows.",
                    "label": 0
                },
                {
                    "sent": "Where can I assume so?",
                    "label": 0
                },
                {
                    "sent": "XI is going to be data item I.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that there's some latent variables that I which corresponds to the UN observed indicator variable, which tells us which cluster this data belongs to.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume the following generative model.",
                    "label": 0
                },
                {
                    "sent": "So for every data item, we're first going to decide which cluster that data item belongs to by sampling from a discrete distribution with prior given by \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So Pi here is basically a vector PI12 Pi K with Pi K being the prior probability that cluster K is respond, ISM is the prior probability that data item I belongs to cluster K. And given that we've postulated that.",
                    "label": 0
                },
                {
                    "sent": "Data item I belongs to cluster KC.",
                    "label": 1
                },
                {
                    "sent": "Then the.",
                    "label": 0
                },
                {
                    "sent": "That this so is that is that question.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's just a discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, it's a discrete distribution with probabilities given by \u03c0, so.",
                    "label": 0
                },
                {
                    "sent": "The probability that that I takes on value one is going to buy one.",
                    "label": 0
                },
                {
                    "sent": "And the probability that that I takes on value 2 is going to be \u03c0 two, and the probability that that I takes on value K is going to be pikey.",
                    "label": 0
                },
                {
                    "sent": "Um, I would.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so people typically call that multinomial in machine learning, but it's actually a bit confusing because a multinomial distribution actually is actually a richer set of distributions then this discrete distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, I wanted to make that distinction, typically in the multinomial distribution you have a pie, but you also have.",
                    "label": 0
                },
                {
                    "sent": "Like a number of independent draws from that distribution.",
                    "label": 0
                },
                {
                    "sent": "Um, right so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in machine learning typically people call it multinomial.",
                    "label": 0
                },
                {
                    "sent": "Right, so that I is a postulated latent variable which tells us which cluster XI belongs to.",
                    "label": 0
                },
                {
                    "sent": "And given that we know that I so given the given that we know which cluster data item I belongs to, we can now postulate that there's some distribution over data items that belongs to that cluster, so that distribution is going to be given by this F distribution, and it's going to be parameterized by Theta zed I.",
                    "label": 0
                },
                {
                    "sent": "So if that I take some value 3, then this is going to be Theta star tree and this can be the parameter.",
                    "label": 0
                },
                {
                    "sent": "Responding to corresponding to the third cluster, an F of Theta star tree is going to be the distribution over data items under the third cluster.",
                    "label": 0
                },
                {
                    "sent": "And being Bayesian, of course we would like to place priors over both sets of parameters that we have here and the typical prior for the mixing proportions.",
                    "label": 0
                },
                {
                    "sent": "Pie is what's called richly distribution, and typically we might assume such a symmetric directly distribution where the parameters are all the same and there's going to be a hyperparameter Alpha, and we can't do this division by K. So the idea is that this is Alpha corresponds to to the basically the strength of the prior an.",
                    "label": 0
                },
                {
                    "sent": "If you take this vector and you normalize it, then it's going to be 1 / K is going to be a vector where every entry is simply 1 / K. And that corresponds to the prior mean of \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Then we might assume that there's a prior over the cluster parameters given by H, so here's our generative process and this is a graphical models which represents that generative process.",
                    "label": 0
                },
                {
                    "sent": "We have this plate notation here.",
                    "label": 0
                },
                {
                    "sent": "Anna Plate basically just means that we take that part of the sub graph of the graphical model and just replicated a certain number of times.",
                    "label": 0
                },
                {
                    "sent": "So in this case, can you guys see down here?",
                    "label": 0
                },
                {
                    "sent": "No OK yeah, alright so this plate.",
                    "label": 0
                },
                {
                    "sent": "Here is I equals to 12 N OK and what that says is that we're going to replicate this two random variables N times and going to call them.",
                    "label": 0
                },
                {
                    "sent": "I basically we're going to index them by I, which is going to take on values 12 N. Displayed here will be replicated it K times because we assume that there are big key clusters in our model, each of them having a parameter.",
                    "label": 0
                },
                {
                    "sent": "Theta Starkey.",
                    "label": 0
                },
                {
                    "sent": "Right, so we'll come back to this later, so the idea now is that we will only observe the excise and we'd like to compute the conditional distribution of both the latent variables which indicate which cluster each data item belongs to, as well as the posterior distribution over the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's just like to show you a few examples of graphical models and of probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "So the second example here is a hidden Markov model, and this is basically a dynamic generalization of the mixture model.",
                    "label": 1
                },
                {
                    "sent": "So, and this is a very popular model for time series, and basically it assumes that the our time series, which is now going to be X1 to X Tao is generated in the following way.",
                    "label": 1
                },
                {
                    "sent": "It assumes that there's some sequence of latent variables which we do not observe.",
                    "label": 0
                },
                {
                    "sent": "And we assume that there's some Markov dependence of the latent variables.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this Markov dependencies that issues that is going to depend on the previous zed.",
                    "label": 0
                },
                {
                    "sent": "And given the value of the previous set, the the value of the next zed variable is going to be independent of all that variables that comes in the past.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the assumption of Markov property.",
                    "label": 0
                },
                {
                    "sent": "Basically the future is independent of the past given the present.",
                    "label": 0
                },
                {
                    "sent": "So we assume that there's a Markov chain over our set variables, and then each of our observations dangles off one of the variables.",
                    "label": 0
                },
                {
                    "sent": "So X2 depends on that too.",
                    "label": 0
                },
                {
                    "sent": "And this is of course a Bayesian hidden Markov models, because I've kind of drawn in the parameters of the model as well.",
                    "label": 0
                },
                {
                    "sent": "Pi K Here is going to be a vector of length big K, and it's going to give us the transition probability going out from state key.",
                    "label": 0
                },
                {
                    "sent": "And Theta star K is going to be the admission distribute.",
                    "label": 0
                },
                {
                    "sent": "Is the parameter for the admission distribution if you're in.",
                    "label": 0
                },
                {
                    "sent": "If you are in states K. Um, OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just another example, here's another example which has real world applications is for collaborative filtering, so this in this problem of collaborative filtering you have a number of users, so think of Netflix you have a number of users, you have a number of products or movies.",
                    "label": 0
                },
                {
                    "sent": "And for certain subset of users and movies, you might have ratings RIJ, which is basically the rating that that user I assigns to movie J.",
                    "label": 0
                },
                {
                    "sent": "So if the user really likes the movie then he would say that RJ is equals to five and if the user really dislikes the movie then he will assign RJ equals to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a between one start a 5 star rating.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is that given a certain subset of this observed ratings would like to predict how.",
                    "label": 1
                },
                {
                    "sent": "How much does a particular user like a particular movie over all the observed user movie pairs?",
                    "label": 0
                },
                {
                    "sent": "And we can assume.",
                    "label": 1
                },
                {
                    "sent": "And here's a very popular probabilistic model for solving collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following, so we're going to assume that every user is going to have some.",
                    "label": 0
                },
                {
                    "sent": "Some features which are UN observed and this is a vector in the Euclidean space which basically describes what sort of movies does the user like.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to every movie, we're also going to assume that there's a probability vector which describes what type of movie that movie is.",
                    "label": 1
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Postulate that the rating that user I gives to movie J.",
                    "label": 0
                },
                {
                    "sent": "It's going to be Gaussian distributed with a mean given by the DOT product between the user feature and the movie feature, so that if the two feature vectors kind of line up together.",
                    "label": 0
                },
                {
                    "sent": "So what that says is that the soft movies that the user likes is very similar to the sort of movie that movie is.",
                    "label": 0
                },
                {
                    "sent": "And of course, we're going to assume that there's some variance around this mean so that we can handle.",
                    "label": 0
                },
                {
                    "sent": "All the randomness in the ratings of the of the of the users.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so RJ is going to be Gaussian distributed with mean given by this dot product and the variance given basic more square.",
                    "label": 0
                },
                {
                    "sent": "And you can now I forgot to to describe to tell you what is the prior over this features and the prior over Sigma Square.",
                    "label": 0
                },
                {
                    "sent": "But you can imagine that you know these features might be given priors which are Gaussian and this feature is also Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Well for Sigma Square you might use a gamma or it or invest, prior.",
                    "label": 0
                },
                {
                    "sent": "And given our ratings, we can now again compute the posterior distribution over user features an movie features.",
                    "label": 0
                },
                {
                    "sent": "And that basically kind of solves the problem for us because with the user features and product features we can.",
                    "label": 1
                },
                {
                    "sent": "We can estimate how much does each user likes each movie.",
                    "label": 0
                },
                {
                    "sent": "According to this model, of course, one thing to note about this is that.",
                    "label": 0
                },
                {
                    "sent": "We are not just Catholic D dimensional vector, right?",
                    "label": 0
                },
                {
                    "sent": "So we're all kind of complex beings.",
                    "label": 0
                },
                {
                    "sent": "We like certain movies.",
                    "label": 0
                },
                {
                    "sent": "For some reason we don't like certain movies for some reason, and really this is simply a very simple.",
                    "label": 0
                },
                {
                    "sent": "This is simply a model which makes very strong simplifying assumptions about the generative process.",
                    "label": 0
                },
                {
                    "sent": "That that gave us the movie ratings, but still is a very useful model.",
                    "label": 0
                },
                {
                    "sent": "And you can count test out this model on bye bye.",
                    "label": 0
                },
                {
                    "sent": "Um, entering this into the Netflix competition and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So, you could kind of test it by looking at how well it predicts ratings, which which it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It hasn't observed, and this does really well.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I like to kind of emphasize that of course all models are wrong, But some models are useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a phrase by.",
                    "label": 0
                },
                {
                    "sent": "A famous statistician that I forgot the name of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so so that's Bayesian machine learning.",
                    "label": 0
                },
                {
                    "sent": "Of course, today and tomorrow I would like to talk about a particular class of Bayesian models called Bayesian nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "And if you are interested in this, there's a book on Basinal parametrics, edited by shot at all.",
                    "label": 0
                },
                {
                    "sent": "That just came out last year.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start off by curve telling by saying what is a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically I can tell you what it is not.",
                    "label": 0
                },
                {
                    "sent": "It's basically not a parametric model, So what is a parametric model?",
                    "label": 1
                },
                {
                    "sent": "A parametric model is basically a model with a certain fixed number of parameters.",
                    "label": 0
                },
                {
                    "sent": "So for example, a Gaussian is a parametric model because it has a mean and variance.",
                    "label": 0
                },
                {
                    "sent": "It has two parameters and that's it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically a nonparametric model is not a parametric model.",
                    "label": 0
                },
                {
                    "sent": "Not in the sense that it doesn't have any parameters, but rather in the sense that it has.",
                    "label": 0
                },
                {
                    "sent": "Either an infinite number of parameters or a number of parameters that changes with the amount of data that is observed.",
                    "label": 0
                },
                {
                    "sent": "A man is in that sense that it is not a parametric model, somewhat different way of viewing.",
                    "label": 0
                },
                {
                    "sent": "What is a nonparametric model is that it's a family of distributions that is dense in some large space.",
                    "label": 1
                },
                {
                    "sent": "OK, I'll come back to this a bit later.",
                    "label": 0
                },
                {
                    "sent": "So the idea, yeah, come into this later.",
                    "label": 0
                },
                {
                    "sent": "So why are we interested in doing Bayesian nonparametrics?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, so let's imagine the following.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you're trying to do classification.",
                    "label": 0
                },
                {
                    "sent": "So in classification you have some inputs and you have some outputs.",
                    "label": 0
                },
                {
                    "sent": "And you may have a space of very large space of hypothesis which basically causes corresponds to the space of all smooth functions from input space to output space.",
                    "label": 0
                },
                {
                    "sent": "So your output could be.",
                    "label": 0
                },
                {
                    "sent": "If it's binary, then it's just consists of either zero or one, but your input could be, say, a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So this is the space of all possible smooth functions from your input space to your output space, and there might be some true underlying function which you like to learn about.",
                    "label": 0
                },
                {
                    "sent": "OK, so this function is the function that actually tells you for every input what should be the true output.",
                    "label": 0
                },
                {
                    "sent": "If you do something like in neural networks or linear regression or something, then basically you're assuming a parametric model of this space.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what is a model and model is basically a very tiny subset of this space that can be parameterized by your model.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And if you're being Bayesian about this, you might put a prior on on this very small space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it's very unlikely that the true function will actually be in this space is very unlikely that your true function is, say, a linear function, because you know many in many real world type of data sets is a lot more complexity involved.",
                    "label": 0
                },
                {
                    "sent": "So maybe what you might do is that you might consider a whole sequence of models of increasing complexity, each of which tries to capture more and more of this space, so they're kind of growing in size, so you might.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider a model of.",
                    "label": 0
                },
                {
                    "sent": "This is a model and it might be nested in another model which.",
                    "label": 0
                },
                {
                    "sent": "Is able to parameterise a larger part of the space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And maybe another one that's going to prioritize a larger part of the space.",
                    "label": 0
                },
                {
                    "sent": "But if you assume that your model is a parametric model, is very unlikely that your parametric model will be able to parameterise this whole space of continuous and smooth functions from input to output, so it's very unlikely that your true function will ever be part of your model space.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the idea of basing on prime tricks?",
                    "label": 0
                },
                {
                    "sent": "There are kind of two ideas here.",
                    "label": 0
                },
                {
                    "sent": "The first idea is that we would like to come up with a model that is so large and so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reach that, it can parameterise all of your hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "OK, so you'd like to cover.",
                    "label": 0
                },
                {
                    "sent": "The word is that you have a model with large coverage, like to cover this whole space of continuous and smooth functions.",
                    "label": 0
                },
                {
                    "sent": "Over all of this.",
                    "label": 0
                },
                {
                    "sent": "So that's the first idea, and the reason why we want to do this is that we would like to have certain guarantees which allows us to show that as we see more and more data that our model will converge to the true.",
                    "label": 0
                },
                {
                    "sent": "To the true function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so and the second idea of Bayesian or Private Ricks is that this is of course a very very large spaces, typically infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "In the case of smooth functions will be possibly uncountably many dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a very large space, so how can we actually build?",
                    "label": 0
                },
                {
                    "sent": "How can we?",
                    "label": 0
                },
                {
                    "sent": "How are we able to do efficient learning in this space?",
                    "label": 0
                },
                {
                    "sent": "And the only way in which we could do efficient learning is that we would like to impose certain modeling assumptions or structures on our model.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that we'd like to place high probability two models, which we think are likely.",
                    "label": 0
                },
                {
                    "sent": "An low probability to models that we think are unlikely OK.",
                    "label": 0
                },
                {
                    "sent": "So this account, like the things are here.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the case of classification, we might place high probability on functions which are smoother.",
                    "label": 0
                },
                {
                    "sent": "Or maybe which might have might be described more or less using a a basis consisting of a small number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to give high probability to small functions an lower probability.",
                    "label": 0
                },
                {
                    "sent": "Two functions which are not as smooth, which are kind of.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of thinking about this function says on the outside of this space.",
                    "label": 0
                },
                {
                    "sent": "And this is a very important part of basic nonparametrics because it allows us to basically make certain assumptions about our problem, and that allows us to learn more efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK so to be I guess I'll kind of go through a few of this reason.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More specifically, OK.",
                    "label": 0
                },
                {
                    "sent": "So the first reason well?",
                    "label": 0
                },
                {
                    "sent": "That was kind of a high level picture, so tell you a bit more about the specific reasons why you might want to do basic nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "The first reason is that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's kind of very nice way to do to actually get away from model selection and model averaging.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess if you've tried to do any to solve any real problems with machine learning.",
                    "label": 0
                },
                {
                    "sent": "You often find that trying to do model selection or model averaging is very expensive, right, so?",
                    "label": 0
                },
                {
                    "sent": "You know, if you're trying to do clustering, how many clusters are there?",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do?",
                    "label": 0
                },
                {
                    "sent": "Classification how many basis functions should you use and to do that you need to do like cross validation or some sort of Bayesian way of doing model selection and all of them are expensive because it allows.",
                    "label": 0
                },
                {
                    "sent": "It basically requires you to run.",
                    "label": 0
                },
                {
                    "sent": "A series of different models on the same data set and then to decide which one to select.",
                    "label": 0
                },
                {
                    "sent": "And the reason why people do model selection or model averaging is because it's kind of used to prevent basically overfitting or underfitting.",
                    "label": 1
                },
                {
                    "sent": "You want to find a model of the right size given your data set.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that in a Bayesian model, if you kind of specify it well, then it should not overfit anyways, because if you think about what is overfitting, it's when you do something like maximum likelihood fitting of your parameters.",
                    "label": 0
                },
                {
                    "sent": "An you come up with some single parameter right?",
                    "label": 0
                },
                {
                    "sent": "And that parameters kind of fit it to your data.",
                    "label": 0
                },
                {
                    "sent": "But in the Bayesian approach, you never actually fit anything to your to your data.",
                    "label": 0
                },
                {
                    "sent": "You always just compute the posterior distribution over your parameters or over your latent variables.",
                    "label": 0
                },
                {
                    "sent": "And because you're not fitting anything to your data, you're not.",
                    "label": 0
                },
                {
                    "sent": "You will not be overfitting through your data either, right?",
                    "label": 0
                },
                {
                    "sent": "So that solves the problem of overfitting.",
                    "label": 0
                },
                {
                    "sent": "How about underfitting?",
                    "label": 1
                },
                {
                    "sent": "So the idea of basing on Parametrics is that if you use a very large model then you won't under fit either.",
                    "label": 0
                },
                {
                    "sent": "Right, so that so that leads us to Bayesian nonparametric models which will not overfit all underfit.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that we instead of looking at a series of models and then trying to decide which one to use, we simply look at one very large model and then we do Bayesian inference in that model.",
                    "label": 0
                },
                {
                    "sent": "That count like allows us to not overfit to our data.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so reason #2 as I talked about just now, large function spaces so.",
                    "label": 1
                },
                {
                    "sent": "In many machine learning applications, the object that we're often interested in learning about comes from some very large space.",
                    "label": 0
                },
                {
                    "sent": "So in the case of classification or regression, we might want to work with the space of all possible smooth functions.",
                    "label": 0
                },
                {
                    "sent": "In the case of density estimation here, you might want to work with the space of all possible smooth densities.",
                    "label": 0
                },
                {
                    "sent": "An innocence, it's kind of easier to think about learning in this large function spaces.",
                    "label": 0
                },
                {
                    "sent": "If you think about.",
                    "label": 0
                },
                {
                    "sent": "Basically learning this.",
                    "label": 0
                },
                {
                    "sent": "Objects this infinite dimensional objects themselves, rather than trying to learn about parameters of a parametric model which consists of a small part of this space.",
                    "label": 0
                },
                {
                    "sent": "So this leads automatically to Bayesian non parametric models.",
                    "label": 1
                },
                {
                    "sent": "So they're basically models over this very large function spaces.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we would like to place a prior over all possible functions and then do posterior inference in this space of all possible functions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this recently.",
                    "label": 0
                },
                {
                    "sent": "24 recently.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of follow up from the model selection model averaging problem question.",
                    "label": 0
                },
                {
                    "sent": "In many graphical models are in actually many models.",
                    "label": 0
                },
                {
                    "sent": "We are often interested in learning structures.",
                    "label": 0
                },
                {
                    "sent": "Certain structures from data, so we might want to learn what is the structure of a graphical model given observed variables at the leaves of the model.",
                    "label": 0
                },
                {
                    "sent": "We might be interested in learning the structure of a tree given observations at the leaves of the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a tree of I guess everyday objects.",
                    "label": 0
                },
                {
                    "sent": "Down here.",
                    "label": 0
                },
                {
                    "sent": "OK and to to work with this off.",
                    "label": 0
                },
                {
                    "sent": "A structure is what we will need is a Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Prior over this combinatorial structures.",
                    "label": 1
                },
                {
                    "sent": "And, uh, nonparametric models are kind of.",
                    "label": 0
                },
                {
                    "sent": "A particular way of assigning priors over this community structures, and they're typically there are sometimes.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, actually end up simpler than the parametric models.",
                    "label": 1
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, finally the fourth reason is that basically the field of stochastic processes and probability theory have come up with lots of different interesting stochastic processes with lots of interesting properties.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "By count looking at what are the stochastic processes out there?",
                    "label": 0
                },
                {
                    "sent": "We can find lots of interesting models with interesting properties.",
                    "label": 1
                },
                {
                    "sent": "So I will talk about a number of this today and tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So we have properties like projectivity exchange ability.",
                    "label": 0
                },
                {
                    "sent": "We have properties like power law properties.",
                    "label": 0
                },
                {
                    "sent": "OK, there's another typo this repayment your.",
                    "label": 0
                },
                {
                    "sent": "Yep, so the idea is that.",
                    "label": 0
                },
                {
                    "sent": "In the last few years we've come.",
                    "label": 0
                },
                {
                    "sent": "Being able to come up with models which have interesting properties which we wouldn't have discovered if we did not look into basing on primary models.",
                    "label": 0
                },
                {
                    "sent": "And these models are.",
                    "label": 0
                },
                {
                    "sent": "Often use very useful and you will see a certain number as some number of examples of these models which were able to do very well compared to the.",
                    "label": 0
                },
                {
                    "sent": "A parametric models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I finish this section, I'd like to say a few words.",
                    "label": 0
                },
                {
                    "sent": "So as I said before nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "Doesn't mean that they don't have parameters, it just means that they are not parametric models, so they still have parameters.",
                    "label": 1
                },
                {
                    "sent": "They just still have.",
                    "label": 0
                },
                {
                    "sent": "They just have an infinite number of parameters.",
                    "label": 1
                },
                {
                    "sent": "Or maybe they may have a number of parameters that grows with the amount of data.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Another point which I'd like to make is that there's no free lunch.",
                    "label": 0
                },
                {
                    "sent": "OK, you have to pay for your lunch here, right?",
                    "label": 0
                },
                {
                    "sent": "At least your supervisor has to pay for lunch here and in machine learning.",
                    "label": 0
                },
                {
                    "sent": "That count means that basically you cannot do any learning without having to put in some effort into making assumptions about your data about how your data came about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "An nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "It's not that they don't make assumptions, they still make assumptions, but it's just that they are made in such a way that they are bit less constrained by your assumptions.",
                    "label": 0
                },
                {
                    "sent": "Um Ann, I'd like to say a word here about kind of Bayesian way of doing machine learning as opposed to a non basin waste.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing with a Bayesian approach to machine learning is that.",
                    "label": 0
                },
                {
                    "sent": "We are being very honest about what sort of assumptions we made in coming up with our model because we have to express everything into in this joint distribution over parameters and latent variables, and that joint distribution actually.",
                    "label": 0
                },
                {
                    "sent": "Makes explicit all the assumptions that we make about our data generating process.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of nice because it allows us to be honest about what we did, and it allows us to think more clearly about what were the assumptions that work and what were the assumptions that don't work.",
                    "label": 1
                },
                {
                    "sent": "And finally, I'd like to say that so.",
                    "label": 0
                },
                {
                    "sent": "Um, so many models can be nonparametric in some sense.",
                    "label": 0
                },
                {
                    "sent": "An parametric in some other sense, and these are perhaps a better name for this are semiparametric models.",
                    "label": 0
                },
                {
                    "sent": "So they're kind of like half half really.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that there are some parts of the model which we liked to make nonparametric.",
                    "label": 0
                },
                {
                    "sent": "But then there are some other parts which we know a lot about and we can make very strong assumptions that you know they should come from some parametric form.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the self things that people work on in basic nonparametrics?",
                    "label": 0
                },
                {
                    "sent": "Well there's the problem of count developing kind of novel classes of nonparametric models with that may have some interesting properties that may be suitable for modeling certain types of data.",
                    "label": 1
                },
                {
                    "sent": "There's the problem of developing novel algorithms that can efficiently cope with this infinitely many parameters in our non parametric model.",
                    "label": 0
                },
                {
                    "sent": "And then finally, for the theorists among you, there's lots of interesting work to be done in developing the theory, which allows us to show whether a certain class of nonparametric models will converge to the true distribution or function if given enough data, and how quickly does this convergence occur?",
                    "label": 0
                },
                {
                    "sent": "So if your model you can do number one.",
                    "label": 0
                },
                {
                    "sent": "If your computer scientist could do #2, and if you are probably the theorist or something you can do the military.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um, there's been lots of previous tutorials on based on Parametric's already, I've listed some of this here and you can find some on video lectures as well, and there are a number of review papers and so forth on based on prometrics.",
                    "label": 0
                },
                {
                    "sent": "So for this tutorial I'll concentrate particularly on the Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "Basically because it's kind of a cornerstone of based on parametric San would be nice to be able to understand it, kind of.",
                    "label": 0
                },
                {
                    "sent": "In a bit more detail than just knowing what it is at a high level and then of course we'll look into the various extensions and generalizations of the judicial process, including things like payment, your processes, hierarchical directly, processes, hierarchical pineal processes, random partitions and so forth.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about this powder?",
                    "label": 0
                },
                {
                    "sent": "Tutorial.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Convert structure.",
                    "label": 0
                },
                {
                    "sent": "Something similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Reason we love somebody or something else is more like the prior is simpler.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "I might come back to this a bit later, actually, because we'll be talking about random trees actually tomorrow.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that.",
                    "label": 0
                },
                {
                    "sent": "If I give you this data set, you may come up with some prior over this data set OK, But if I then told you that hey, there's this something else which I haven't really told you about.",
                    "label": 0
                },
                {
                    "sent": "So would that change your model in a typical parametric model that might change your model.",
                    "label": 0
                },
                {
                    "sent": "Just by me saying that, oh, that's you know, there's three other objects which I hadn't told you about.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about them tomorrow, right?",
                    "label": 0
                },
                {
                    "sent": "An by come making this nonparametric prior assumptions it.",
                    "label": 0
                },
                {
                    "sent": "We can come up with models which would not be affected by this of additional data items which were not observed.",
                    "label": 0
                },
                {
                    "sent": "OK, and that kind of simplifies your inference process.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I guess I'll just continue on with your issue.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assess as I said, them directly.",
                    "label": 0
                },
                {
                    "sent": "Processes are cornerstone of modern Bayesian nonparametrics.",
                    "label": 1
                },
                {
                    "sent": "And it has been actually rediscovered many, many times before, as basically the infinite limit of finite mixture models.",
                    "label": 1
                },
                {
                    "sent": "Even in machine learning it was rediscovered twice and it statistiques many times.",
                    "label": 0
                },
                {
                    "sent": "And formally, they were defined and they were given this name of a directly process by focusing back in 1973, about close to 40 years ago now.",
                    "label": 0
                },
                {
                    "sent": "As basically a distribution over measures.",
                    "label": 0
                },
                {
                    "sent": "I guess in the next 2 slides.",
                    "label": 0
                },
                {
                    "sent": "Actually, not in the next 2 slides it a little bit further down.",
                    "label": 0
                },
                {
                    "sent": "I'll be telling you what are measures before I can tell you what are distributions over measures.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Um, an richley process being kind of this very basically a Canonical example of a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "It does kind of like different ways in which you could derive it.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's kind of a special case of lots of different processes and will be looking at will be deriving the directly process actually in three different ways.",
                    "label": 0
                },
                {
                    "sent": "The as the infinite limit of Gibbs sampler for a finite mixture model and using the Chinese restaurant process an using the stick breaking construction.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start off with this infinite limit of finite mixture models.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess the simplest recall that we had this model, this model for model based clustering, which is typically called a finite mixture model.",
                    "label": 1
                },
                {
                    "sent": "An is finite in the sense that there's a finite number of of mixture components or clusters.",
                    "label": 0
                },
                {
                    "sent": "And it's a mixture model in that you assume that the data that you observe is a mixture of different, basically heterogeneous populations.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right, and so just to recall for you.",
                    "label": 0
                },
                {
                    "sent": "The generative model is that we have some indicator variables that I which tells us which cluster XI belongs to.",
                    "label": 0
                },
                {
                    "sent": "And it can take on one of K values, K = 1, two big K. The mixing proportion is pie with a prior given by Additionally.",
                    "label": 1
                },
                {
                    "sent": "And again we have for each cluster K we have a parameter.",
                    "label": 1
                },
                {
                    "sent": "Theta, Starkey, and we have a prior over Theta Star K. Given by H. I have people here who has seen directly distribution before.",
                    "label": 0
                },
                {
                    "sent": "Who has not seen directly distributions?",
                    "label": 0
                },
                {
                    "sent": "Quite a lot.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "I'll spend a few slides on them up.",
                    "label": 0
                },
                {
                    "sent": "Oh by H. What I mean is a prior over the parameters of that cluster.",
                    "label": 0
                },
                {
                    "sent": "So F is going to be your your cluster distribution and H is your prior so.",
                    "label": 0
                },
                {
                    "sent": "In the case of a mixture of Gaussians, OK. F would be a Gaussian parameterized by a mean and variance, and that theater will be vector of length two is it will be the mean and the variance, and H is going to be a prior over your mean annual variance.",
                    "label": 0
                },
                {
                    "sent": "So you might think that maybe the mean.",
                    "label": 0
                },
                {
                    "sent": "I expect it to be lying close to zero OK, and the variance would be approximately one.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are counted priors on your mean annual variance.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So richly distribution is basically a.",
                    "label": 0
                },
                {
                    "sent": "Distribution on the K dimensional probability simplex.",
                    "label": 1
                },
                {
                    "sent": "So this is basically the set of all vectors Pi such that each entry of the vector is non negative and it sums to one.",
                    "label": 0
                },
                {
                    "sent": "Typically you can.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of this as a.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should draw this?",
                    "label": 0
                },
                {
                    "sent": "You have a 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is your.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 one Pi 2\u03c0 three, and this is a space which basically cause bonds to something like this.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's this plane which passes through.",
                    "label": 0
                },
                {
                    "sent": "100 010 N 001 so it's a plane in that basically.",
                    "label": 0
                },
                {
                    "sent": "Every point of this probability simplex is going to be a vector of length tree, each of which is each entry is positive or is not negative and they have to sum to one.",
                    "label": 0
                },
                {
                    "sent": "So it's spent by this tree vectors.",
                    "label": 0
                },
                {
                    "sent": "Basically it's all convex combinations of this tree vectors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And Additionally, distribution is a distribution on this space and it has a density given by this thing.",
                    "label": 1
                },
                {
                    "sent": "OK, so this here is going to be the basically is the normalization constants for the density.",
                    "label": 0
                },
                {
                    "sent": "And over here is the part of the density, which does depend on pie, and it's basically a product over K equals to one to big K of Pi K race today.",
                    "label": 0
                },
                {
                    "sent": "Some exponents minus one, so this exponents are the parameters of the delay distribution.",
                    "label": 0
                },
                {
                    "sent": "And this gamma functions are basically what they are called gamma functions and they are given by this integral.",
                    "label": 0
                },
                {
                    "sent": "Basically, that's the normalization constants.",
                    "label": 0
                },
                {
                    "sent": "So the Richland distributions are used all over the place in probabilistic modeling and the main reason for that is basically that is the standard distribution for probability vectors an.",
                    "label": 1
                },
                {
                    "sent": "This is due to the fact that they are conjugate to the multinomial.",
                    "label": 0
                },
                {
                    "sent": "Distribution and particularly conjugate to discrete distributions.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to visualize what this dense, this looks like OK if the so.",
                    "label": 0
                },
                {
                    "sent": "Here's the Dirichlet density down here.",
                    "label": 0
                },
                {
                    "sent": "Where I've count generalize this to one where Alpha here is a vector where every entry is positive so that Alpha there's a typo should be distribute, K goes to one to big K of Pi K race to the Alpha K -- 1.",
                    "label": 0
                },
                {
                    "sent": "And when Alpha is.",
                    "label": 0
                },
                {
                    "sent": "A vector of ones.",
                    "label": 0
                },
                {
                    "sent": "So there's a vector of ones.",
                    "label": 0
                },
                {
                    "sent": "Then we see that this part here is going to be Pike, a race to the 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "And that's just going to be pickey raised to the zero, and this is going to be one.",
                    "label": 0
                },
                {
                    "sent": "And what that says is that the direction is going to be a uniform distribution over the probability simplex.",
                    "label": 0
                },
                {
                    "sent": "When Alpha is greater than one.",
                    "label": 0
                },
                {
                    "sent": "So for example if it's 222.",
                    "label": 0
                },
                {
                    "sent": "The Dirichlet distribution is going to be a unimodal distribution with a with mean given by.",
                    "label": 0
                },
                {
                    "sent": "Basically this vector normalized OK, so the mean is going to be at 1/3 one third, one third, and the total mass of the Alpha here basically describes how concentrated the mode is around the moon.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So when Alpha is 555, then we see that the distribution is more concentrated around its mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK an if the Alpha vector is not symmetric, so for example is 255.",
                    "label": 0
                },
                {
                    "sent": "Then the mode gets shifted off the center of the probability simplex to one of the edges or to a corner here, and more Interestingly, if the alphas are less than one, then we see that instead of a unimodal distribution we have a multimodal distribution where we have high probability around the corners and low probability in the center of the probability simplex.",
                    "label": 0
                },
                {
                    "sent": "So that just is a visualization what the traditional distribution is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so as I said, the traditional distribution is kind of all over the place in probabilistic modeling, mainly because it's conjugate to the multinomial right so?",
                    "label": 0
                },
                {
                    "sent": "Let's see how this conjugacy work.",
                    "label": 0
                },
                {
                    "sent": "If you look at the joint distribution over the prior over \u03c0.",
                    "label": 1
                },
                {
                    "sent": "Add the cluster indicator variables that OK, so that's the prior.",
                    "label": 0
                },
                {
                    "sent": "That's the conditional probability of each that I given Pi.",
                    "label": 0
                },
                {
                    "sent": "So the priors given by this Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "The conditional probabilities we could collect up into a term like this.",
                    "label": 0
                },
                {
                    "sent": "So basically every time that I take some value K, then we're going to have a Pi K contribution.",
                    "label": 0
                },
                {
                    "sent": "An NK is the number of that ice that take on value K, so we can.",
                    "label": 0
                },
                {
                    "sent": "So if there are NK is that I stay on value K. Then we basically have Pike a race to the NK Times and this is over K from one to beat K. And you can see that the conditional probability of the zed vector here.",
                    "label": 0
                },
                {
                    "sent": "Add in the prior prior on Pi.",
                    "label": 0
                },
                {
                    "sent": "They cannot have a similar functional form, and if you actually multiply these two probabilities together, you get the joint probability is going to be this normalization times product key equals one to big K of pikey.",
                    "label": 0
                },
                {
                    "sent": "Race to the Alpha divided by K plus N K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Right, and so if you normalize this.",
                    "label": 1
                },
                {
                    "sent": "So that you get the posterior distribution of Pi given set, then you see that the posterior distribution will also be a Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "OK, but with parameters updated by NK.",
                    "label": 1
                },
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "While the marginal distribution over these at vector here, you can compute it by basically taking the ratio of this probabilities so basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you take the.",
                    "label": 0
                },
                {
                    "sent": "Our joints divided by the posterior.",
                    "label": 0
                },
                {
                    "sent": "You get the marginal probability.",
                    "label": 0
                },
                {
                    "sent": "And the fact that I can write down both of these equations says that both the posterior and the marginal distributions are tractable and can be computed easily, and this is what's meant by conjugacy here.",
                    "label": 0
                },
                {
                    "sent": "Basically, it allows us to compute the posterior, easily, allows us to compute the marginal easily.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So coming back to our finite mixture model, we can now make use of this conjugacy to actually derive a Gibbs sampler for this, so I guess Peter Green has talked quite a bit about Gibbs sampling already on Thursday and Friday, right?",
                    "label": 0
                },
                {
                    "sent": "So this should be easy for you.",
                    "label": 0
                },
                {
                    "sent": "So if you do give sampling, So what we'd like to do is to basically compute.",
                    "label": 0
                },
                {
                    "sent": "Posterior over the parameters and the latent variables given observed that I observed variables XI and Gibbs sampler is basically a Markov chain Monte Carlo sampler, in which we update each unobserved variables by computing its conditional distribution given everything else sampling from that conditional distribution and then repeating this overall unobserved variables and just repeating this until you've converged to the posterior.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "We can derive the conditional distribution of that I.",
                    "label": 0
                },
                {
                    "sent": "Given all the other variables and you can see you can derive that the conditional probability of that I taking on value K. It's going to be proportional to two terms.",
                    "label": 0
                },
                {
                    "sent": "It's going to be proportional to pikey, which is the basically the conditional prior.",
                    "label": 0
                },
                {
                    "sent": "That said, I take some value K times conditional likelihood term, which tells us how likely is it that we observe XI given that it belongs to cluster K. And if you normalize this, you are going to get the basically the responsibility of cluster K for data item I.",
                    "label": 0
                },
                {
                    "sent": "And this is the conditional distribution for every eye.",
                    "label": 0
                },
                {
                    "sent": "For every key you can compute this and you can update each of those at ice easily by just doing that.",
                    "label": 0
                },
                {
                    "sent": "And given the sad eyes, we would like to also get the conditional distributions of the parameters as well, and you can see that for the pies because of the Dirichlet multinomial conjugacy, the conditional distribution of Pi given all the other random variables is going to be directly as well.",
                    "label": 0
                },
                {
                    "sent": "It's going to be given by this directly with updated parameters.",
                    "label": 0
                },
                {
                    "sent": "While the conditional distribution of theater of data K given all the other variables is going to be the prior times likelihood of all the data items that is currently assigned to cluster key.",
                    "label": 0
                },
                {
                    "sent": "Turns out that this skip samplers actually not as efficient as another Gibbs sampler where called collapsed Gibbs sampler and this keeps sampler works by basically taking this model, marginalizing out Pi an theater, and then just updating the the only variables there left, which is The Jets.",
                    "label": 1
                },
                {
                    "sent": "And if you look at the collapsed Gibbs sampler, basically the conditional probability of that I given all the other variables is.",
                    "label": 0
                },
                {
                    "sent": "Of this form, again, we have a conditional prior times the conditional likelihood.",
                    "label": 0
                },
                {
                    "sent": "Where the conditional prior has changed from paikea to this form here because we've integrated a pikey.",
                    "label": 0
                },
                {
                    "sent": "With integrated out the Pi vector and this conditional prior is kind of quite intuitive.",
                    "label": 0
                },
                {
                    "sent": "It just says that.",
                    "label": 0
                },
                {
                    "sent": "It's going to be proportional to Alpha divided by K. So that's coming from the prior on \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Plus the number of data items that were currently assigned to cluster K If you don't count data item I itself.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that a cluster that has lots of different data items assigned to it will have a higher probability of being of taking responsibility for this data item I. OK. And then we also have a conditional.",
                    "label": 0
                },
                {
                    "sent": "Likelihood of data item XI, given that it's assigned to cluster K and this term here is basically given by the predictive probability of XI given.",
                    "label": 0
                },
                {
                    "sent": "Given all the data items that work, there are currently assigned to cluster key except for data item I and you can compute this in the following way.",
                    "label": 0
                },
                {
                    "sent": "And if you assume that the prior distribution age is conjugate to our F distribution, then this can be computed efficiently as well, just as the Dirichlet multinomial conjugacy allows us to compute all the probabilities efficiently.",
                    "label": 1
                },
                {
                    "sent": "If you assume that H is conjugate to F, we can also assume that this can be computed efficiently too, so this gives us gifts, collapse keeps updates for these at I variables integrated out both \u03c0 and Theta from this model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as I said.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can obtain the delay distribution by basically taking this infinite limit of Gibbs sampler in particularly of this collapsed Gibbs sampler.",
                    "label": 1
                },
                {
                    "sent": "Well, perhaps I should say why we might want to do this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we might want to do this because.",
                    "label": 0
                },
                {
                    "sent": "Because we may not want to make any assumption about the number of clusters in our data set, so it could be that when we do clustering as we see more and more data, we might see more and more clusters and the more data.",
                    "label": 0
                },
                {
                    "sent": "Items we see the more clusters there might be in them.",
                    "label": 0
                },
                {
                    "sent": "OK, So what that says is that in the true underlying generative process that might not be a fixed finite number of clusters.",
                    "label": 0
                },
                {
                    "sent": "There really is an infinite number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Is just that, given a certain small data set, you only see a finite number of of them.",
                    "label": 0
                },
                {
                    "sent": "So this by taking this infinite limit of K going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We are basically making that assumption that there really is an infinite number of clusters.",
                    "label": 0
                },
                {
                    "sent": "That generated our data, but it's just that you know, given that if you observe only a finite number of data items, you only.",
                    "label": 0
                },
                {
                    "sent": "See less than 100 number of clusters, see.",
                    "label": 0
                },
                {
                    "sent": "So before we take this, K goes to Infinity limit.",
                    "label": 1
                },
                {
                    "sent": "We can imagine that there's kind of a very large value for key.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if K is very very large then it will be.",
                    "label": 0
                },
                {
                    "sent": "Much larger than N. But an here is going to be our data set size.",
                    "label": 0
                },
                {
                    "sent": "Basically there will only be an data items in our datasets.",
                    "label": 0
                },
                {
                    "sent": "An IF K is much larger than N, then we know that there are at most N clusters which will be occupied in the sense that end clusters are going to be.",
                    "label": 0
                },
                {
                    "sent": "Associated with the end date items OK.",
                    "label": 0
                },
                {
                    "sent": "Typically there will be less than N clusters associated with our data, but there can be at most N because every data item can only be assigned to at most one cluster.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we know that because if K is much larger than N, most of these K clusters will be basically empty, right?",
                    "label": 0
                },
                {
                    "sent": "There won't be assigned any data items at all, right?",
                    "label": 0
                },
                {
                    "sent": "So they're empty and we can kind of lump them all together just to make our computations more efficient.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, then the conditional distributions for collapsed Gibbs sampler will look like this.",
                    "label": 0
                },
                {
                    "sent": "So for a cluster K. Which this have some data item assigned to it?",
                    "label": 0
                },
                {
                    "sent": "Then NK will be positive and the conditional probability of that I equals 2K is going to be again this conditional prior.",
                    "label": 0
                },
                {
                    "sent": "This is unchanged times the.",
                    "label": 0
                },
                {
                    "sent": "Conditional likelihood of XI given all the other data items assigned to cluster K. Will also have the conditional probability of.",
                    "label": 0
                },
                {
                    "sent": "Data item I being assigned to some cluster which is not.",
                    "label": 0
                },
                {
                    "sent": "Associated with any other data item and you can compute that by basically looking at basically summing over all of this empty clusters, right?",
                    "label": 0
                },
                {
                    "sent": "So for each of the empty clusters, we're going to have a conditional prior of Alpha divided by K. Divided by N -- 1 plus Alpha and then we have a conditional likelihood which is basically the probability of XI given that there are no other data items being assigned to that cluster.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Key star is going to be the number of clusters which have some data item.",
                    "label": 0
                },
                {
                    "sent": "Assigned to them.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the non empty clusters, so that K -- K style is the number of empty clusters, right?",
                    "label": 0
                },
                {
                    "sent": "So when you sum over those probabilities, you're going to get came.",
                    "label": 0
                },
                {
                    "sent": "In this case here, yes.",
                    "label": 0
                },
                {
                    "sent": "I'll come to that later actually yeah.",
                    "label": 0
                },
                {
                    "sent": "So basically Alpha is going to be related to what you expect.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters you expect to see in your data set.",
                    "label": 0
                },
                {
                    "sent": "I'll come to that in I don't know if 10 slides or something.",
                    "label": 0
                },
                {
                    "sent": "Basically, the larger Alpha is, the more clusters we tend to expect to see.",
                    "label": 0
                },
                {
                    "sent": "So we know that K star is the number of occupied clusters and it will be at most NK.",
                    "label": 0
                },
                {
                    "sent": "An N is going to be much smaller than K. If K is very very large, right?",
                    "label": 0
                },
                {
                    "sent": "So now we can take Kate to go to Infinity because in these two equations here.",
                    "label": 0
                },
                {
                    "sent": "When K goes to Infinity, we see that Alpha divided by K is going to go to 0.",
                    "label": 0
                },
                {
                    "sent": "So that's going to go to 0.",
                    "label": 0
                },
                {
                    "sent": "K -- K Star divided by K is going to go to one because case that is upper bounded by N&K is going to Infinity right?",
                    "label": 0
                },
                {
                    "sent": "So this time this ratio here is going to go to one.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's going to go to 1A times one is this Alpha, so this is.",
                    "label": 0
                },
                {
                    "sent": "The infinite limit of the collapsed Gibbs sampler and basically this gives us a collapsed Gibbs sampler for an infinite mixture model with an infinite number of clusters.",
                    "label": 1
                },
                {
                    "sent": "And you can actually implement this and it will work very well OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But actually it kind of doesn't make sense as the model itself doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "And the reason is because if you look at any particular cluster in here, it will be assigned basically a. I prior probability of 0 being.",
                    "label": 0
                },
                {
                    "sent": "That would be a basically yeah, so the probability of that particular cluster being assigned to explain a particular data item will be basically 0.",
                    "label": 0
                },
                {
                    "sent": "And basically the reason is because if you look at the prior, the prior is a symmetric Dirichlet, so the prior probability of cluster K being assigned a particular data item will be just one over Big K and one over Big K is going to go to zero as big K goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And basically.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Basically the the math that we can't have to go through to actually derive the judicial process are basically kind of better ways of kind of making this infinite limit construction precise.",
                    "label": 1
                },
                {
                    "sent": "Like what does it mean?",
                    "label": 0
                },
                {
                    "sent": "What does this infinite limit mean when we take big K to go to Infinity?",
                    "label": 0
                },
                {
                    "sent": "OK, and this two ways are basically different ways of looking at the original process, kind of.",
                    "label": 0
                },
                {
                    "sent": "Better ways of making this precise?",
                    "label": 0
                },
                {
                    "sent": "And basically we can think of a Dirichlet process as a basically an infinite dimensional directly distribution.",
                    "label": 1
                },
                {
                    "sent": "Basically is the think of it as the prior is the limit of the Dirichlet prior in our.",
                    "label": 0
                },
                {
                    "sent": "Over the mix it mixing proportions as the number of clusters goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I guess I'll move on.",
                    "label": 0
                },
                {
                    "sent": "To define what is additional process?",
                    "label": 0
                },
                {
                    "sent": "Before I do so, perhaps I would actually spend 2 slides on some probability theory.",
                    "label": 0
                },
                {
                    "sent": "Does everybody here know about what is?",
                    "label": 0
                },
                {
                    "sent": "What is probability theory or what is a measure?",
                    "label": 0
                },
                {
                    "sent": "Anybody reason OK and who doesn't know?",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, that's fine.",
                    "label": 0
                },
                {
                    "sent": "I am explaining everything.",
                    "label": 0
                },
                {
                    "sent": "I just like to see.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just to give you a little bit of flavor of what is measure theory and what is probability theory.",
                    "label": 0
                },
                {
                    "sent": "Measure theory is actually a theory of how you measure things, and the South measures that you think about is things like for example, you know.",
                    "label": 0
                },
                {
                    "sent": "Given a set of the real line, what is its length?",
                    "label": 0
                },
                {
                    "sent": "So given, say, you know a stick of this length, what is its length given a state of this length, what is length may be given a an object like this?",
                    "label": 0
                },
                {
                    "sent": "What is this volume?",
                    "label": 0
                },
                {
                    "sent": "Or what is this mess?",
                    "label": 0
                },
                {
                    "sent": "So this account, like a theory of how you measure things and solve things that we like to measure, basically subsets of some space.",
                    "label": 0
                },
                {
                    "sent": "So the space that we're going to work with.",
                    "label": 0
                },
                {
                    "sent": "Is going to be theater.",
                    "label": 0
                },
                {
                    "sent": "And the set of subsets which are going to be measurable in this space.",
                    "label": 0
                },
                {
                    "sent": "It's going to be given by Sigma OK, Big Sigma.",
                    "label": 0
                },
                {
                    "sent": "An basically this Sigma here is going to be called a Sigma algebra.",
                    "label": 0
                },
                {
                    "sent": "And what if this is simply a family of subsets which are which can be measured in our theory?",
                    "label": 1
                },
                {
                    "sent": "Anne Anne, what are the subsets that could be measured?",
                    "label": 0
                },
                {
                    "sent": "Firstly we have to assume that we can at least measure something.",
                    "label": 0
                },
                {
                    "sent": "OK, so that this Sigma cannot be empty.",
                    "label": 0
                },
                {
                    "sent": "In particular, we can assume that the empty set is measurable.",
                    "label": 0
                },
                {
                    "sent": "We know that you know the volume of the empty set is going to be 0 or the length of an empty set is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "If we say that.",
                    "label": 0
                },
                {
                    "sent": "If a is going to be a measurable set, so if you can measure a.",
                    "label": 0
                },
                {
                    "sent": "Then we can pretty much assume that the complement of a should be measurable as well.",
                    "label": 0
                },
                {
                    "sent": "So if you can measure the whole set, we can measure a, then the the.",
                    "label": 0
                },
                {
                    "sent": "The measure of the complement of A is going to be the measure of the whole set minus the measure of a right?",
                    "label": 0
                },
                {
                    "sent": "So that's says that if A is measurable, then its complement is measurable.",
                    "label": 0
                },
                {
                    "sent": "If we have a sequence of measurable sets, A1A two and so forth, then a good assumption is to assume that the Union of this sequence is also going to be measurable.",
                    "label": 0
                },
                {
                    "sent": "OK. And what is a measure?",
                    "label": 1
                },
                {
                    "sent": "A measure is simply a function that takes.",
                    "label": 1
                },
                {
                    "sent": "As input, this measurable sets so it's going to be a function from the Sigma algebra to the positive rail line.",
                    "label": 0
                },
                {
                    "sent": "And basically mu of a is going to be the length of a or the volume of a.",
                    "label": 1
                },
                {
                    "sent": "And of course, the measure of the empty set is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, if we have a sequence of measurable sets and they are disjoint, then it's kind of a good assumption to make that the measure of the Union would be the sum of the measures of the individual sets.",
                    "label": 0
                },
                {
                    "sent": "That's this thing, and a probability measure is 1 where basically the measure of the whole space here is going to be one.",
                    "label": 0
                },
                {
                    "sent": "OK, so in probability theory measure.",
                    "label": 0
                },
                {
                    "sent": "So this subsets are going to be events and a measure of the event is basically going to be associated with the probability of that event.",
                    "label": 0
                },
                {
                    "sent": "Of that random event, write an if we have disjoint events, so we have events for which at most one of them will occur, then the probability of the Union of the events will be.",
                    "label": 0
                },
                {
                    "sent": "Of course the sum of the probabilities of the events.",
                    "label": 0
                },
                {
                    "sent": "We don't really need to worry about things which are not measurable actually because in machine learning we never really deal with non measurable things anyways.",
                    "label": 0
                },
                {
                    "sent": "So everything we consider here will be measurable.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So that tells us what are probabilities, what events, events are measurable sets, what are probabilities that simply the measure of this manageable sets?",
                    "label": 0
                },
                {
                    "sent": "How about random variables so a random variable is basically a function is in fact a deterministic function in this theory of probability theory.",
                    "label": 1
                },
                {
                    "sent": "So a random variable F is going to be a function from a measurable space to another measurable space.",
                    "label": 0
                },
                {
                    "sent": "Anne, we need to have.",
                    "label": 0
                },
                {
                    "sent": "Some properties on Earth.",
                    "label": 0
                },
                {
                    "sent": "Basically it has to be measurable.",
                    "label": 0
                },
                {
                    "sent": "And one way in which you could think about this is that.",
                    "label": 0
                },
                {
                    "sent": "If you think about how you might implement a random number generator right so?",
                    "label": 0
                },
                {
                    "sent": "For example, you can implement a random number generator for at exponential random variables by doing the following right in Matlab anyways, you can first generate a uniform random variable on 01 and then just pass it through the.",
                    "label": 0
                },
                {
                    "sent": "A function which is negative long of that variable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So and.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "A random variable which is which has a distribution given by an exponential distribution with parameter of one.",
                    "label": 0
                },
                {
                    "sent": "We can generate a sample from this random variable by sampling U from a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "On 01 and then setting X to be minus log of you.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can work out that if you generate such a uniform random variable and then pass it through the minus log of you, then this X here is going to be random because US random.",
                    "label": 0
                },
                {
                    "sent": "And that the distribution of Axia will be just exponential distribution with the rates of 1.",
                    "label": 0
                },
                {
                    "sent": "So if you think about what this function does.",
                    "label": 0
                },
                {
                    "sent": "It takes us.",
                    "label": 0
                },
                {
                    "sent": "It is a deterministic function, so I've just written it down.",
                    "label": 0
                },
                {
                    "sent": "It's deterministic in the sense that what we do to this U is fixed, but what makes it random is that it has this has.",
                    "label": 0
                },
                {
                    "sent": "As in, put some random number itself so we can think of this random variable X here as basically a deterministic functions of some other random variable, and basically that's what's happening here.",
                    "label": 0
                },
                {
                    "sent": "A random variable X is going to be a fixed function measurable function X from some space to another space an we have basically put all the randomness of this random variable X into this probability space of.",
                    "label": 1
                },
                {
                    "sent": "Theater.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I guess that's what this is saying.",
                    "label": 1
                },
                {
                    "sent": "And then finally, what is the stochastic process?",
                    "label": 0
                },
                {
                    "sent": "So Additionally process is called the Richland process because it is a stochastic process.",
                    "label": 0
                },
                {
                    "sent": "Ann is stochastic.",
                    "label": 0
                },
                {
                    "sent": "Process is simply a collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So it's a collection of random variables XI where I is belongs to some index sets an the only difference between what a stochastic processes an what say is a is a graphical model.",
                    "label": 1
                },
                {
                    "sent": "Is that a stochastic process?",
                    "label": 0
                },
                {
                    "sent": "This index set I can be infinite and can be uncountably infinite, so we could describe the joint distribution over an uncountably many random variables under stochastic process, and you can't need this.",
                    "label": 1
                },
                {
                    "sent": "Because if you think about.",
                    "label": 0
                },
                {
                    "sent": "A function write a function is basically.",
                    "label": 0
                },
                {
                    "sent": "For every input you have some output, right?",
                    "label": 0
                },
                {
                    "sent": "Anna Random function is in a sense a stochastic process, because we can think of a random function.",
                    "label": 0
                },
                {
                    "sent": "As a set of random variables, one for every input.",
                    "label": 0
                },
                {
                    "sent": "And this input is going to be indexed by its input space, which could be something like are out of them or something.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So is that clear for probability theory?",
                    "label": 0
                },
                {
                    "sent": "It's just cause simple domain thing which I like to you to take away from this is that basically.",
                    "label": 0
                },
                {
                    "sent": "Measure is is this a function itself, right?",
                    "label": 0
                },
                {
                    "sent": "Imagine this function which tells us for every measurable set, what is it.",
                    "label": 0
                },
                {
                    "sent": "Length or what is its volume or what is its probability?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now we can define what's original.",
                    "label": 0
                },
                {
                    "sent": "Process is basically going to be a random probability measure.",
                    "label": 1
                },
                {
                    "sent": "So what is a random probability measure is simply a random function right from the measurable sets to the positive rail line.",
                    "label": 1
                },
                {
                    "sent": "And it has to have the following property so.",
                    "label": 0
                },
                {
                    "sent": "If this is our space here.",
                    "label": 0
                },
                {
                    "sent": "Then if you take any partition of the space, so if A1 to a big key is a partition of the space in that their union is going to be all of the space.",
                    "label": 0
                },
                {
                    "sent": "And they're gonna be disjoint OK?",
                    "label": 0
                },
                {
                    "sent": "Then if you consider this vector here, OK.",
                    "label": 1
                },
                {
                    "sent": "Which basically G of A1 is going to be the probability assigned to this part of the space G of a two is going to be the probability assigned to that part of the space and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this has to be a probability vector here GG of a one to JFK right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "G is going to be a probability measure, so the probability assigned to the whole space has to be 1, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This has to be a probably probably the vector, and if G is a random probability measure, then this vector here has to be random.",
                    "label": 0
                },
                {
                    "sent": "So basically we can think of this.",
                    "label": 0
                },
                {
                    "sent": "G is a, it's a random probability measure in the sense that you can think of.",
                    "label": 0
                },
                {
                    "sent": "It has to has to have a total mass of 1 and it has.",
                    "label": 0
                },
                {
                    "sent": "To put a certain amount of mass in each of these parts of the space, and it could be random in terms of how much mass it puts into each part of the space.",
                    "label": 0
                },
                {
                    "sent": "And what we are saying is that the amount of master it places until each of these parts of the space as a sum to one.",
                    "label": 0
                },
                {
                    "sent": "And it has to be nonnegative an it's random an we assume that.",
                    "label": 0
                },
                {
                    "sent": "Basically, this random probability vector here has a prior which is given by a Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "And that rich lips can be parameterized by Alpha and by H. Alpha is just going to be a positive scalar.",
                    "label": 0
                },
                {
                    "sent": "An H is a.",
                    "label": 0
                },
                {
                    "sent": "Is another measure so that each of a 12H of a K is going to be a vector that sums to one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so those are the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK. How much time will come running out?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yep, so Alpha is so these are the two parameters.",
                    "label": 0
                },
                {
                    "sent": "Alpha H. Typically the Alpha parameters is is called the strength parameter or the mass parameter or sometimes called the concentration parameter H is called the base distribution.",
                    "label": 0
                },
                {
                    "sent": "You can workout that.",
                    "label": 0
                },
                {
                    "sent": "Given a measurable subset A, then G of a is going to be a random variable taking values between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And you can work out there that the expectation of G of a is just going to be H of a.",
                    "label": 0
                },
                {
                    "sent": "So what that says is that the base distribution H here is basically the mean of the directly process.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And you can also workout the variance of this random variable G of A.",
                    "label": 0
                },
                {
                    "sent": "IS has a form which gives which is given by this and what that says is that the strength parameter Alpha is.",
                    "label": 0
                },
                {
                    "sent": "Basically you can think of it as an inverse variance, right?",
                    "label": 0
                },
                {
                    "sent": "So the larger Alpha is then the smaller this variance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and if Alpha is very small then this variance is going to be large.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, I think I have a. I can show you a demo, so here's comfort a little demo that.",
                    "label": 0
                },
                {
                    "sent": "Show she was a draw from a directional process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know that, so this is going to be additional process where the space is going to be one and the total area of this thing here is going to be the total probability assigned to the whole space.",
                    "label": 0
                },
                {
                    "sent": "And of course that total probability has to be one.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a square on 01201.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a random probability measure, right so?",
                    "label": 0
                },
                {
                    "sent": "We can ask how much of the mass is going to be on the in between zero to 0.5 and how much of the mass is going to be between 0.5 to one and the amount of mass is going to be random, so we can sample from that.",
                    "label": 0
                },
                {
                    "sent": "So weird.",
                    "label": 0
                },
                {
                    "sent": "OK, I actually did that twice, so we can ask what's the total mass between zero to 0.25 once the total mass between 0.2525 to 0.5 was the total mass.",
                    "label": 0
                },
                {
                    "sent": "Here was a total mess here.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be random, so the amount of mass we assigned to each of this is going to be random angwe.",
                    "label": 0
                },
                {
                    "sent": "In this particular draw, we place most of the mess in here a bit here.",
                    "label": 0
                },
                {
                    "sent": "A bit here and very little in here.",
                    "label": 0
                },
                {
                    "sent": "Now we can take each of these little segments here and ask and split into four different segments and say which?",
                    "label": 0
                },
                {
                    "sent": "Of the total mass assigned to this whole segment, how much do you want to place into one of these four segments?",
                    "label": 0
                },
                {
                    "sent": "You can do this and you can see that it assigns.",
                    "label": 0
                },
                {
                    "sent": "Kind of this mess into this little bit here and this bit.",
                    "label": 0
                },
                {
                    "sent": "You know it's tough.",
                    "label": 0
                },
                {
                    "sent": "Your account will basically iteratively.",
                    "label": 0
                },
                {
                    "sent": "Placing the mass into different parts of the space such that the total mass that we begin with is one OK. Ann is a random process so that this gives us a random probability measure.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Pins.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it was the area of the bar actually, yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh, so that the background bars are basically the.",
                    "label": 0
                },
                {
                    "sent": "The G when the when the partition of the spaces cost.",
                    "label": 0
                },
                {
                    "sent": "So we start off with a with a cost partition of the space which is the whole space itself and we know that there's a total probability of 1 assigned to that.",
                    "label": 0
                },
                {
                    "sent": "Whole space.",
                    "label": 0
                },
                {
                    "sent": "And then when we re find the partition of the space we get smaller and smaller parts.",
                    "label": 0
                },
                {
                    "sent": "OK, and in each of these parts we have a random probability of a random amount of probability mass assigned to that part of the space, and that amount is going to be given by the area.",
                    "label": 0
                },
                {
                    "sent": "I still didn't get it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What is comfort?",
                    "label": 0
                },
                {
                    "sent": "An iterative process, right?",
                    "label": 0
                },
                {
                    "sent": "So you can't take the partition and refine it, and you ask how much mass you want to assign to the more refined partition.",
                    "label": 0
                },
                {
                    "sent": "In the things in the background, we just haven't decided how much of the mass is going to be assigned to each of the more finer partitions.",
                    "label": 0
                },
                {
                    "sent": "We just know that the total mass has to be a certain amount and account this visualizing that total mass.",
                    "label": 0
                },
                {
                    "sent": "Could be that yes.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "After that.",
                    "label": 0
                },
                {
                    "sent": "10 yeah so.",
                    "label": 0
                },
                {
                    "sent": "OK, sure.",
                    "label": 0
                },
                {
                    "sent": "We can try that.",
                    "label": 0
                },
                {
                    "sent": "It says it somehow.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a probability measure right?",
                    "label": 0
                },
                {
                    "sent": "So it has to assign a total mass of 1 to the whole space.",
                    "label": 1
                },
                {
                    "sent": "That's going to be the the total mess.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The next step is we're going to take this space.",
                    "label": 0
                },
                {
                    "sent": "We're going to partition it into four quarters, and then we ask how much mass does it want to assign to each of the four quarters?",
                    "label": 0
                },
                {
                    "sent": "OK, and if you sample that from a directly.",
                    "label": 0
                },
                {
                    "sent": "Where can I get something like this?",
                    "label": 0
                },
                {
                    "sent": "Where now it has decided that the total mass in here is going to be this amount given by the area.",
                    "label": 0
                },
                {
                    "sent": "The total mass in here is going to be given by this area and basically this thing in the background is just the previous step of the iteration is not really in there is just to help you visualize what's happening.",
                    "label": 0
                },
                {
                    "sent": "The next step would be you take each of these quarters an you further partition it into four quarters and you ask how much mass does it have to be?",
                    "label": 0
                },
                {
                    "sent": "Does it want to place in each of those four quarters?",
                    "label": 0
                },
                {
                    "sent": "And the total mass assigned to the four quarters of this first quarter here has to sum up to the total mass that's already decided at this point.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the steps is that right you split into two at your complex person.",
                    "label": 0
                },
                {
                    "sent": "That's what the yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "It's just that somehow, for some reason when I plug this thing into this projector every keypress I do is it gets translated into two key presses.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sample distribution but when you refine the partition, you're not getting independent sample from the refined position, right?",
                    "label": 0
                },
                {
                    "sent": "It's not independent.",
                    "label": 0
                },
                {
                    "sent": "Process for sampling from the replying partition to get it to match up with the previous refinement.",
                    "label": 0
                },
                {
                    "sent": "Is it that you just take a Jewish way process that's independently defined on the partition?",
                    "label": 0
                },
                {
                    "sent": "And is the mass attacks in the previous chapter we just recursively against?",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "So basically when we when we want to decide how much when we take this.",
                    "label": 0
                },
                {
                    "sent": "Part of the space and we cut it into four parts and we ask how much mass it wants to be.",
                    "label": 0
                },
                {
                    "sent": "It wants to place in each of the four parts.",
                    "label": 0
                },
                {
                    "sent": "That's precisely what we do.",
                    "label": 0
                },
                {
                    "sent": "We actually sample from originally and then multiply that vector in.",
                    "label": 0
                },
                {
                    "sent": "So that the total mass stays as it is and this actually comes from a very deep.",
                    "label": 0
                },
                {
                    "sent": "Reason for further drilling.",
                    "label": 0
                },
                {
                    "sent": "Basically a richly process is basically.",
                    "label": 0
                },
                {
                    "sent": "It's basically the amount of mass it places on different parts of the space are basically independent, and the only assumption is that they have to sum to one.",
                    "label": 0
                },
                {
                    "sent": "Besides this single assumption, everything is basically independent, which is why we could do this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so now we're going to take each of this split in, split into four, and then decide how much mass it wants to place into each of the four.",
                    "label": 0
                },
                {
                    "sent": "So for this particular project for this I don't know, probably Alpha.",
                    "label": 0
                },
                {
                    "sent": "It goes to five or something like that.",
                    "label": 0
                },
                {
                    "sent": "I don't remember.",
                    "label": 0
                },
                {
                    "sent": "I could look take a look.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Do this now, we can yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yep, so the original bit.",
                    "label": 0
                },
                {
                    "sent": "Which is the total mass given by this thing by the rectangle at the back here OK?",
                    "label": 0
                },
                {
                    "sent": "Which we can't quite see now is now going to.",
                    "label": 0
                },
                {
                    "sent": "That is, total mass is going to be spread up in between these four segments and we have higher probability here and here and lower probability in here.",
                    "label": 0
                },
                {
                    "sent": "And we can repeat this.",
                    "label": 1
                },
                {
                    "sent": "And you can see what happens, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, and this is basically a sample from a directly process, right?",
                    "label": 0
                },
                {
                    "sent": "It's a probability measure, so it has to be the total mass of all of these spikes has to sum to one.",
                    "label": 0
                },
                {
                    "sent": "And basically the fact that it looks like this spiky objects says that it's an atomic distribution.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that later.",
                    "label": 0
                },
                {
                    "sent": "Yes, example is the basis.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "The base distribution is uniform.",
                    "label": 0
                },
                {
                    "sent": "And if you do this multiple times.",
                    "label": 0
                },
                {
                    "sent": "Then you will get a different draw from the judicial process, right?",
                    "label": 0
                },
                {
                    "sent": "I can do this.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a bit different, right?",
                    "label": 0
                },
                {
                    "sent": "Every time is going to look at like an atomic distribution, but it's going to be random.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess I'm kind of out of time.",
                    "label": 0
                },
                {
                    "sent": "I should take a break.",
                    "label": 0
                },
                {
                    "sent": "Will come back in 20 minutes I guess, thanks.",
                    "label": 0
                }
            ]
        }
    }
}