{
    "id": "xrgdheiu4c5hp3nrdjvfstexqzf6od7q",
    "title": "Probabilistic Relaxation Labeling by Fokker-Planck Diffusion on a Graph",
    "info": {
        "author": [
            "Edwin Hancock, Department of Computer Science, University of York"
        ],
        "published": "July 11, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/gbr07_hancock_prl/",
    "segmentation": [
        [
            "So this is is some ways related work.",
            "It's about diffusion and it's work I've been doing with with Hong Fang Wang and the aim is to see how we could formulate probabilistic relaxation as a Fokker Planck diffusion process on a graph.",
            "So in the previous talk, what I was was really talking about was smoothing pixel values using diffusion process and this work.",
            "What I want to do is to try to take the process up to a kind of a symbolic level.",
            "And relaxation labeling is a really classic technique in pattern recognition.",
            "It was introduced in the early 1970s by Rosenfeld Hooker as a way of iteratively assigning labels to objects using compatibility relationships, and there's been a lot of work on the process since.",
            "Particularly looking at issues of optimization issues of how you model the process in terms of Bayesian or belief propagation process.",
            "And I I worked on this with Joseph Kittler in the mid 1980s when I moved into the field.",
            "And so I thought it was time to go back to this and see whether we could use some kernel methods of heat kernel techniques to try to reinterpret the method."
        ],
        [
            "So the outline of the talk is is like this.",
            "I'll start by introducing the method, then I'll talk a little bit about probabilistic relaxation labeling and then talk about how one can view.",
            "Might be able to view relaxation labeling this diffusion process on a graph, and then provide some experiments and some discussion."
        ],
        [
            "So this is kind of an overview of what we're trying to do.",
            "The idea is to exploit the similarities between diffusion process and relaxation labeling, develop a new iterative process for consistent labeling.",
            "So the problem we're dealing with here is trying to assign labels to objects where the objects have a topology arrangement and where we know that there are certain consistency constraints that apply to the assignment of labels to adjacent objects.",
            "What I'm going to do is is to show how the classical relaxation process is the formulation in terms of these ingredients, objects, labels, arrangement topology and consistency relations.",
            "How we can represent this compactly in terms of support graph and in this support graph, what we're going to to have nodes that represent possible assignments of labels to objects?",
            "And then the the weights between those nodes are going to to represent the degree of compatibility of the assignment of a particular pair of labels to a particular pair of objects.",
            "And once I have this this graph structure, what I'm going to do is rather like the previous talk.",
            "I'm going to try to set up a random walk or diffusion on the graph and show how we can use this diffusion to evolve a state vector.",
            "That state vector represents represents the probabilities on the object label nodes in the graph.",
            "And what I'll do is I'll show how this this process can be modeled using the Fokker Planck equation, and then what I'll do is I'll provide relatively limited amount of evaluation of this method on the problem of data clustering."
        ],
        [
            "So just to give you a little view of what the classical probabilistic relaxation labeling algorithm is.",
            "It was first introduced by Rosenfeld Hookah in the early 1970s.",
            "Is not the first relaxation algorithm, the first relaxation algorithm can really be traced back to the work of waltz.",
            "Waltzes algorithm works in a purely discrete manner by trying to remove inconsistent label assignments to objects that does this using a constraint filtering algorithm.",
            "So in the vaults algorithm, the representation is purely discrete.",
            "But Rosenfeld Hooker tried to extend this idea of waltz to a continuous domain, where each possible assignment of a label to an object is represented by a probability.",
            "And then what they developed was a very simple way of updating, iteratively updating these probabilities in a manner which incorporated compatibility's concerning adjacent label assignments and propagated evidence for the label assignment, iteratively, more globally in the network that was under analysis."
        ],
        [
            "So relaxation labeling aims to assign a consistent and ambiguous set of labels to given set of objects.",
            "It's important ingredient is to rely on contextual information and this is provided by the topology of object arrangement and sets of compatibility relations between label assignments on object configurations.",
            "Typically pairs of objects.",
            "And it does this what it tries to do is to use an evidence combining formula to update the label probabilities and propagate label consistency constraint.",
            "So the evidence combining formula is local.",
            "What it does is eventually it propagates the effects of label consistency constraints globally.",
            "And they of course, the problem with all iterative process is is that in order ready to be rendered effective, it needs good initialization.",
            "We have to have a reasonable guess at initial label probability assignment in order to get a reasonable answer at the end of the day."
        ],
        [
            "So here are the.",
            "Really, the formulas of relaxation labeling the the common feature to all of them is a probability update formula.",
            "What we do is we update the probability that object J takes on label Omega J at iteration K plus one using the information about the labeling at the previous iteration K. And normally it takes the form of a multiplying the previous probabilities by support function S and then in order to make the objects we're dealing with probabilities, we divide out by normalizing term.",
            "Now the original support function developed by Rosenfeld Hookah takes on the following form what it does is, it multiplies the probability of the label assignment of the neighboring object by support function, which tells us about the degree of support between the labels on adjacent objects.",
            "Now in some work with Joseph Hitler in 1985, I developed a rather more complicated form of this support function that took the form of a product of sums and.",
            "This too can be substituted into this formula.",
            "So that's that's the classical method.",
            "What it tries to do is to update object label assignments are represent in terms of extra probabilities using a support function S support function S combines information about evidence and label compatibility on adjacent objects."
        ],
        [
            "So what I'd like to do in this talk talk first of all, is to try to set this so we have another new.",
            "News Flash is to show how this process can be set up in a graph setting graph theoretical setting so that we can then start to use some of the apparatus of.",
            "Random walks on graphs and diffusion processes to emulate the relaxation process."
        ],
        [
            "So to do this, what we're going to do is to set up a support graph.",
            "The nodes in the support graph represent possible object labels assignments, so each node in the graph is a possible assignment to another of the label to an object.",
            "And the edges represent label compatibility.",
            "So what we're going to do is is to set up the support graph, and then what we want to do is to establish a random walk on this graph, and the idea is that probability visiting a node is the property under this random walk.",
            "Is the probability of object label assignment, so this is the state vector of the process.",
            "And what we want to do is to evolve this state vector of the random up with time to update the probabilities and combine evidence in much the same way that relaxation labeling does."
        ],
        [
            "So.",
            "This is a more formal statement of how we compute this support graph is just a choice.",
            "It's an intuitive choice rather than a sort of theoretically justified design, but it's based on intuition about relaxation processes and what people have tried previously.",
            "So the node set of our graph is going to be the Cartesian product of the set of objects X and the set of labels Omega, and then what we want to do is to find a weight between the edge connecting a pair of nodes, and we want to assign this weight in such a way as to reflect evidence for the object label assignments at the nodes that are being connected.",
            "The approximate proximity of the nodes to each other, and then the compatibility of the labels.",
            "On the nodes.",
            "So this is how we do it.",
            "We first of all take the probabilities of labels.",
            "Omega, Rye and Omega, J or Dodge.",
            "Ixion Jay.",
            "We multiply those two probabilities together.",
            "Then we we consider the positions of the nodes in J and we multiply the product of the probabilities by weight, which indicates whether or not the nodes are in proximity to each other.",
            "That way it can either be 01 if we have the data already in the form of a graph, or it may simply be approximately weight function.",
            "If we're dealing with real value data points and then we have a compatibility coefficient which tells us the compatibility of having.",
            "Label Omega on Object J and label obj object J.",
            "So typically this this compatibility coefficient is going to be one if there's a compatibility between those label assignments is going to be 0 if they're incompatible.",
            "So that's the edge weight assignment we make this assignment if the object J is in the neighborhood of object I and 0 otherwise, so that gives us a effectively a weighted adjacency matrix for which encompases first of all evidence for the object label assignments.",
            "Secondly, information about proximity of the pair of nodes and Thirdly information about the compatibility of the labels.",
            "So.",
            "Later on I'll show you how we can assign these compatibility's are generally assigned using knowledge of the label domain and they can be either assigned using probabilistic model or in the case of this worker heuristic method based on our expectations about the labeling problem.",
            "Proximity weight W is set using the interpoint distance between the data and we're going to use a Gaussian distribution on the distances."
        ],
        [
            "So this is just a sort of representation of the setup."
        ],
        [
            "As a graph.",
            "So what I want to do is is now to consider a random walk on the graph structure and I want to use this.",
            "The diffusion equation for this random walk to evolve the state vector for the object label probabilities with time."
        ],
        [
            "So this is just a sort of a note about diffusion process is.",
            "Diffusion process is a, Markov is 1 which is composed of Markov random variables that are correlated and index by time.",
            "We use the Markov property here that for a collection of random variables indexed by time T given the current value of the variable, the future is independent of the variables passed and there a lot of examples of using these sort of process is to do all sorts of things in computer vision and pattern recognition, and I've listed some of them here.",
            "So the idea in this work is is to use a diffusion process to model random walk on the support graph."
        ],
        [
            "So this is the the Fokker Planck equation.",
            "If P is that the probability distribution for the system at time T in terms of the random variables X, then it's effectively a differential operator.",
            "As shown there.",
            "So what I what I want to do is to is just to set this the model of this diffusion operator up using the adjacency structure that I've shown previously.",
            "Um?",
            "So I think that's that's what I want to say about that."
        ],
        [
            "So the idea now is to use spectral graph theory to try to solve the Fokker Planck equation."
        ],
        [
            "Using using the spectrum of the operator.",
            "So what I've done here is I've set up Laplacian matrix from the adjacency matrix.",
            "Then I've also defined a transition probability.",
            "Matrix, which is just the weight matrix divided by the by the degree matrix.",
            "So this is basically the matrix setting of what I'm doing."
        ],
        [
            "And then what we want to do is to.",
            "Take the transition matrix and becausw, usually in relaxation labeling compatibility is symmetric.",
            "What I want to do is to symmetrize the transition matrix to describe the evolution of the random walk in a way which which is which is symmetric.",
            "So the choice we've adopted here to get a symmetric version of the transition matrix is to pre multiply the transition matrix by its own transpose, and then the operator for the random walk is just the identity matrix minus the symmetric transition matrix.",
            "So what I've done here is is written down the solution of the Fokker Planck equation with the.",
            "With operator F is very similar to the solution of the heat equation, which I showed earlier.",
            "Probability of being in the probability state vector at time T is just the exponential of the operator F multiplied by time and then exponentiated times initial state vector.",
            "And again, what we can do is we can, by computing the spectrum of F which I've shown here, we can write the solution of the.",
            "The state vector dot down in terms of the spectrum of the operator F. So P at T is just you.",
            "The eigenvector matrix of F times the exponential of tee times Lambda times new transpose times initial state state vector.",
            "So this is effectively they are statement of relaxation labeling.",
            "Now we're doing this, we're updating the probability vector P of T, which in this interpretation is the state vector of the random walk on the support graph.",
            "We're updating that the time and then the operator which represents the compatibility's of labels on objects is F and that's just found by subtracting symmetrised version of P. It's given in terms of the the white matrix on the graph from the identity matrix.",
            "So that's that's the that's a state."
        ],
        [
            "To the problem.",
            "Anne."
        ],
        [
            "And what I've done now is, we've we've applied this to some relatively simple data classification problems, and I'll show you the results of those."
        ],
        [
            "So what we've done is we have tried to write down incompatibilities in terms of pairs, triples, and quartets of objects.",
            "And the compatibility matrices that we use are given here.",
            "So these what we're doing is we're trying to label in this configurations of 1.",
            "Of two, three, and four different objects.",
            "The idea is that the labels here correspond to different clusters in the data and the compatibility's for neighboring data points to take on the different labels.",
            "So in this case we have two labels on a pair of objects.",
            "The probability that we get the same label on adjacent objects is given by 1.",
            "And the probability that we have different labels on adjacent objects when we have effectively two clusters in the data is .3 and these different examples extend this to different numbers of labels."
        ],
        [
            "So here are the datasets we've tried this on, so the first problem involves assigning two labels to two clusters, the second one three labels to three clusters.",
            "Again, here we have three labels to three clusters, and I think these two are full labels to four clusters."
        ],
        [
            "So in the next slide, what we have.",
            "The what I've done here is I've plotted the.",
            "Of the fraction of correct assignments as a function of iteration number.",
            "And what I've done here is is to do this for different numbers of.",
            "Initially correct assignments.",
            "So what we do is we start with 10% initially, correct?",
            "And then go up to 75% initially incorrect and these are.",
            "These are the performance curves in all cases the relaxation operator improves the performance with iteration number and even when the fraction of initial errors is quite large."
        ],
        [
            "And these are some of the more.",
            "More complicated datasets, the two last ones, again with few.",
            "Exception here we seem to monitor, monotonically improve the performance of the labeling.",
            "So that's that's really."
        ],
        [
            "All I want to to say what we've done is we've provided new development of probabilistic relaxation into graph spectral setting.",
            "We view relaxation labeling as a diffusion process on the graph, and this diffusion process combines evidence and propagates constraints globally.",
            "What it means is that some extent we have a kernel interpretation of relaxation labeling.",
            "From this approach, we've done some a few experiments only a limited number of experiments on classification tasks, and these seem to suggest that the method improves the accuracy of the labeling with iteration number.",
            "And what we're doing at the moment is looking at number of more challenging applications, particularly image labeling and segmentation, and speaker recognition, to see whether we can establish in a deeper way the performance of the method."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is is some ways related work.",
                    "label": 0
                },
                {
                    "sent": "It's about diffusion and it's work I've been doing with with Hong Fang Wang and the aim is to see how we could formulate probabilistic relaxation as a Fokker Planck diffusion process on a graph.",
                    "label": 1
                },
                {
                    "sent": "So in the previous talk, what I was was really talking about was smoothing pixel values using diffusion process and this work.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is to try to take the process up to a kind of a symbolic level.",
                    "label": 0
                },
                {
                    "sent": "And relaxation labeling is a really classic technique in pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "It was introduced in the early 1970s by Rosenfeld Hooker as a way of iteratively assigning labels to objects using compatibility relationships, and there's been a lot of work on the process since.",
                    "label": 0
                },
                {
                    "sent": "Particularly looking at issues of optimization issues of how you model the process in terms of Bayesian or belief propagation process.",
                    "label": 0
                },
                {
                    "sent": "And I I worked on this with Joseph Kittler in the mid 1980s when I moved into the field.",
                    "label": 0
                },
                {
                    "sent": "And so I thought it was time to go back to this and see whether we could use some kernel methods of heat kernel techniques to try to reinterpret the method.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of the talk is is like this.",
                    "label": 0
                },
                {
                    "sent": "I'll start by introducing the method, then I'll talk a little bit about probabilistic relaxation labeling and then talk about how one can view.",
                    "label": 1
                },
                {
                    "sent": "Might be able to view relaxation labeling this diffusion process on a graph, and then provide some experiments and some discussion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is kind of an overview of what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "The idea is to exploit the similarities between diffusion process and relaxation labeling, develop a new iterative process for consistent labeling.",
                    "label": 1
                },
                {
                    "sent": "So the problem we're dealing with here is trying to assign labels to objects where the objects have a topology arrangement and where we know that there are certain consistency constraints that apply to the assignment of labels to adjacent objects.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is is to show how the classical relaxation process is the formulation in terms of these ingredients, objects, labels, arrangement topology and consistency relations.",
                    "label": 0
                },
                {
                    "sent": "How we can represent this compactly in terms of support graph and in this support graph, what we're going to to have nodes that represent possible assignments of labels to objects?",
                    "label": 0
                },
                {
                    "sent": "And then the the weights between those nodes are going to to represent the degree of compatibility of the assignment of a particular pair of labels to a particular pair of objects.",
                    "label": 0
                },
                {
                    "sent": "And once I have this this graph structure, what I'm going to do is rather like the previous talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to set up a random walk or diffusion on the graph and show how we can use this diffusion to evolve a state vector.",
                    "label": 0
                },
                {
                    "sent": "That state vector represents represents the probabilities on the object label nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "And what I'll do is I'll show how this this process can be modeled using the Fokker Planck equation, and then what I'll do is I'll provide relatively limited amount of evaluation of this method on the problem of data clustering.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give you a little view of what the classical probabilistic relaxation labeling algorithm is.",
                    "label": 0
                },
                {
                    "sent": "It was first introduced by Rosenfeld Hookah in the early 1970s.",
                    "label": 1
                },
                {
                    "sent": "Is not the first relaxation algorithm, the first relaxation algorithm can really be traced back to the work of waltz.",
                    "label": 0
                },
                {
                    "sent": "Waltzes algorithm works in a purely discrete manner by trying to remove inconsistent label assignments to objects that does this using a constraint filtering algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in the vaults algorithm, the representation is purely discrete.",
                    "label": 0
                },
                {
                    "sent": "But Rosenfeld Hooker tried to extend this idea of waltz to a continuous domain, where each possible assignment of a label to an object is represented by a probability.",
                    "label": 0
                },
                {
                    "sent": "And then what they developed was a very simple way of updating, iteratively updating these probabilities in a manner which incorporated compatibility's concerning adjacent label assignments and propagated evidence for the label assignment, iteratively, more globally in the network that was under analysis.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So relaxation labeling aims to assign a consistent and ambiguous set of labels to given set of objects.",
                    "label": 1
                },
                {
                    "sent": "It's important ingredient is to rely on contextual information and this is provided by the topology of object arrangement and sets of compatibility relations between label assignments on object configurations.",
                    "label": 1
                },
                {
                    "sent": "Typically pairs of objects.",
                    "label": 0
                },
                {
                    "sent": "And it does this what it tries to do is to use an evidence combining formula to update the label probabilities and propagate label consistency constraint.",
                    "label": 0
                },
                {
                    "sent": "So the evidence combining formula is local.",
                    "label": 0
                },
                {
                    "sent": "What it does is eventually it propagates the effects of label consistency constraints globally.",
                    "label": 0
                },
                {
                    "sent": "And they of course, the problem with all iterative process is is that in order ready to be rendered effective, it needs good initialization.",
                    "label": 0
                },
                {
                    "sent": "We have to have a reasonable guess at initial label probability assignment in order to get a reasonable answer at the end of the day.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the.",
                    "label": 0
                },
                {
                    "sent": "Really, the formulas of relaxation labeling the the common feature to all of them is a probability update formula.",
                    "label": 0
                },
                {
                    "sent": "What we do is we update the probability that object J takes on label Omega J at iteration K plus one using the information about the labeling at the previous iteration K. And normally it takes the form of a multiplying the previous probabilities by support function S and then in order to make the objects we're dealing with probabilities, we divide out by normalizing term.",
                    "label": 0
                },
                {
                    "sent": "Now the original support function developed by Rosenfeld Hookah takes on the following form what it does is, it multiplies the probability of the label assignment of the neighboring object by support function, which tells us about the degree of support between the labels on adjacent objects.",
                    "label": 0
                },
                {
                    "sent": "Now in some work with Joseph Hitler in 1985, I developed a rather more complicated form of this support function that took the form of a product of sums and.",
                    "label": 0
                },
                {
                    "sent": "This too can be substituted into this formula.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the classical method.",
                    "label": 0
                },
                {
                    "sent": "What it tries to do is to update object label assignments are represent in terms of extra probabilities using a support function S support function S combines information about evidence and label compatibility on adjacent objects.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'd like to do in this talk talk first of all, is to try to set this so we have another new.",
                    "label": 0
                },
                {
                    "sent": "News Flash is to show how this process can be set up in a graph setting graph theoretical setting so that we can then start to use some of the apparatus of.",
                    "label": 1
                },
                {
                    "sent": "Random walks on graphs and diffusion processes to emulate the relaxation process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do this, what we're going to do is to set up a support graph.",
                    "label": 1
                },
                {
                    "sent": "The nodes in the support graph represent possible object labels assignments, so each node in the graph is a possible assignment to another of the label to an object.",
                    "label": 0
                },
                {
                    "sent": "And the edges represent label compatibility.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is is to set up the support graph, and then what we want to do is to establish a random walk on this graph, and the idea is that probability visiting a node is the property under this random walk.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of object label assignment, so this is the state vector of the process.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is to evolve this state vector of the random up with time to update the probabilities and combine evidence in much the same way that relaxation labeling does.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a more formal statement of how we compute this support graph is just a choice.",
                    "label": 0
                },
                {
                    "sent": "It's an intuitive choice rather than a sort of theoretically justified design, but it's based on intuition about relaxation processes and what people have tried previously.",
                    "label": 0
                },
                {
                    "sent": "So the node set of our graph is going to be the Cartesian product of the set of objects X and the set of labels Omega, and then what we want to do is to find a weight between the edge connecting a pair of nodes, and we want to assign this weight in such a way as to reflect evidence for the object label assignments at the nodes that are being connected.",
                    "label": 1
                },
                {
                    "sent": "The approximate proximity of the nodes to each other, and then the compatibility of the labels.",
                    "label": 0
                },
                {
                    "sent": "On the nodes.",
                    "label": 0
                },
                {
                    "sent": "So this is how we do it.",
                    "label": 0
                },
                {
                    "sent": "We first of all take the probabilities of labels.",
                    "label": 0
                },
                {
                    "sent": "Omega, Rye and Omega, J or Dodge.",
                    "label": 0
                },
                {
                    "sent": "Ixion Jay.",
                    "label": 0
                },
                {
                    "sent": "We multiply those two probabilities together.",
                    "label": 0
                },
                {
                    "sent": "Then we we consider the positions of the nodes in J and we multiply the product of the probabilities by weight, which indicates whether or not the nodes are in proximity to each other.",
                    "label": 0
                },
                {
                    "sent": "That way it can either be 01 if we have the data already in the form of a graph, or it may simply be approximately weight function.",
                    "label": 0
                },
                {
                    "sent": "If we're dealing with real value data points and then we have a compatibility coefficient which tells us the compatibility of having.",
                    "label": 0
                },
                {
                    "sent": "Label Omega on Object J and label obj object J.",
                    "label": 0
                },
                {
                    "sent": "So typically this this compatibility coefficient is going to be one if there's a compatibility between those label assignments is going to be 0 if they're incompatible.",
                    "label": 0
                },
                {
                    "sent": "So that's the edge weight assignment we make this assignment if the object J is in the neighborhood of object I and 0 otherwise, so that gives us a effectively a weighted adjacency matrix for which encompases first of all evidence for the object label assignments.",
                    "label": 0
                },
                {
                    "sent": "Secondly, information about proximity of the pair of nodes and Thirdly information about the compatibility of the labels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Later on I'll show you how we can assign these compatibility's are generally assigned using knowledge of the label domain and they can be either assigned using probabilistic model or in the case of this worker heuristic method based on our expectations about the labeling problem.",
                    "label": 0
                },
                {
                    "sent": "Proximity weight W is set using the interpoint distance between the data and we're going to use a Gaussian distribution on the distances.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a sort of representation of the setup.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a graph.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is is now to consider a random walk on the graph structure and I want to use this.",
                    "label": 1
                },
                {
                    "sent": "The diffusion equation for this random walk to evolve the state vector for the object label probabilities with time.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just a sort of a note about diffusion process is.",
                    "label": 0
                },
                {
                    "sent": "Diffusion process is a, Markov is 1 which is composed of Markov random variables that are correlated and index by time.",
                    "label": 1
                },
                {
                    "sent": "We use the Markov property here that for a collection of random variables indexed by time T given the current value of the variable, the future is independent of the variables passed and there a lot of examples of using these sort of process is to do all sorts of things in computer vision and pattern recognition, and I've listed some of them here.",
                    "label": 1
                },
                {
                    "sent": "So the idea in this work is is to use a diffusion process to model random walk on the support graph.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the the Fokker Planck equation.",
                    "label": 0
                },
                {
                    "sent": "If P is that the probability distribution for the system at time T in terms of the random variables X, then it's effectively a differential operator.",
                    "label": 0
                },
                {
                    "sent": "As shown there.",
                    "label": 0
                },
                {
                    "sent": "So what I what I want to do is to is just to set this the model of this diffusion operator up using the adjacency structure that I've shown previously.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I think that's that's what I want to say about that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea now is to use spectral graph theory to try to solve the Fokker Planck equation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using using the spectrum of the operator.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here is I've set up Laplacian matrix from the adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "Then I've also defined a transition probability.",
                    "label": 0
                },
                {
                    "sent": "Matrix, which is just the weight matrix divided by the by the degree matrix.",
                    "label": 1
                },
                {
                    "sent": "So this is basically the matrix setting of what I'm doing.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then what we want to do is to.",
                    "label": 0
                },
                {
                    "sent": "Take the transition matrix and becausw, usually in relaxation labeling compatibility is symmetric.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is to symmetrize the transition matrix to describe the evolution of the random walk in a way which which is which is symmetric.",
                    "label": 0
                },
                {
                    "sent": "So the choice we've adopted here to get a symmetric version of the transition matrix is to pre multiply the transition matrix by its own transpose, and then the operator for the random walk is just the identity matrix minus the symmetric transition matrix.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here is is written down the solution of the Fokker Planck equation with the.",
                    "label": 1
                },
                {
                    "sent": "With operator F is very similar to the solution of the heat equation, which I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "Probability of being in the probability state vector at time T is just the exponential of the operator F multiplied by time and then exponentiated times initial state vector.",
                    "label": 0
                },
                {
                    "sent": "And again, what we can do is we can, by computing the spectrum of F which I've shown here, we can write the solution of the.",
                    "label": 0
                },
                {
                    "sent": "The state vector dot down in terms of the spectrum of the operator F. So P at T is just you.",
                    "label": 0
                },
                {
                    "sent": "The eigenvector matrix of F times the exponential of tee times Lambda times new transpose times initial state state vector.",
                    "label": 0
                },
                {
                    "sent": "So this is effectively they are statement of relaxation labeling.",
                    "label": 0
                },
                {
                    "sent": "Now we're doing this, we're updating the probability vector P of T, which in this interpretation is the state vector of the random walk on the support graph.",
                    "label": 0
                },
                {
                    "sent": "We're updating that the time and then the operator which represents the compatibility's of labels on objects is F and that's just found by subtracting symmetrised version of P. It's given in terms of the the white matrix on the graph from the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the that's a state.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the problem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I've done now is, we've we've applied this to some relatively simple data classification problems, and I'll show you the results of those.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we've done is we have tried to write down incompatibilities in terms of pairs, triples, and quartets of objects.",
                    "label": 0
                },
                {
                    "sent": "And the compatibility matrices that we use are given here.",
                    "label": 0
                },
                {
                    "sent": "So these what we're doing is we're trying to label in this configurations of 1.",
                    "label": 0
                },
                {
                    "sent": "Of two, three, and four different objects.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the labels here correspond to different clusters in the data and the compatibility's for neighboring data points to take on the different labels.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have two labels on a pair of objects.",
                    "label": 0
                },
                {
                    "sent": "The probability that we get the same label on adjacent objects is given by 1.",
                    "label": 0
                },
                {
                    "sent": "And the probability that we have different labels on adjacent objects when we have effectively two clusters in the data is .3 and these different examples extend this to different numbers of labels.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the datasets we've tried this on, so the first problem involves assigning two labels to two clusters, the second one three labels to three clusters.",
                    "label": 0
                },
                {
                    "sent": "Again, here we have three labels to three clusters, and I think these two are full labels to four clusters.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the next slide, what we have.",
                    "label": 0
                },
                {
                    "sent": "The what I've done here is I've plotted the.",
                    "label": 0
                },
                {
                    "sent": "Of the fraction of correct assignments as a function of iteration number.",
                    "label": 0
                },
                {
                    "sent": "And what I've done here is is to do this for different numbers of.",
                    "label": 0
                },
                {
                    "sent": "Initially correct assignments.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we start with 10% initially, correct?",
                    "label": 0
                },
                {
                    "sent": "And then go up to 75% initially incorrect and these are.",
                    "label": 0
                },
                {
                    "sent": "These are the performance curves in all cases the relaxation operator improves the performance with iteration number and even when the fraction of initial errors is quite large.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are some of the more.",
                    "label": 0
                },
                {
                    "sent": "More complicated datasets, the two last ones, again with few.",
                    "label": 0
                },
                {
                    "sent": "Exception here we seem to monitor, monotonically improve the performance of the labeling.",
                    "label": 0
                },
                {
                    "sent": "So that's that's really.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All I want to to say what we've done is we've provided new development of probabilistic relaxation into graph spectral setting.",
                    "label": 1
                },
                {
                    "sent": "We view relaxation labeling as a diffusion process on the graph, and this diffusion process combines evidence and propagates constraints globally.",
                    "label": 0
                },
                {
                    "sent": "What it means is that some extent we have a kernel interpretation of relaxation labeling.",
                    "label": 0
                },
                {
                    "sent": "From this approach, we've done some a few experiments only a limited number of experiments on classification tasks, and these seem to suggest that the method improves the accuracy of the labeling with iteration number.",
                    "label": 0
                },
                {
                    "sent": "And what we're doing at the moment is looking at number of more challenging applications, particularly image labeling and segmentation, and speaker recognition, to see whether we can establish in a deeper way the performance of the method.",
                    "label": 0
                }
            ]
        }
    }
}