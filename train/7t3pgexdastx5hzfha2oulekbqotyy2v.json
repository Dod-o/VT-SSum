{
    "id": "7t3pgexdastx5hzfha2oulekbqotyy2v",
    "title": "Future Information Minimization as PAC Bayes regularization in Reinforcement Learning",
    "info": {
        "author": [
            "Naftali Tishby, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_tishby_refinement/",
    "segmentation": [
        [
            "Alright, good evening.",
            "When a student of yours is asking you for doing a favor and give a talk in the workshop organizers, and I usually say yes, but when I saw the list of speakers, I really got scared because I'm.",
            "I don't feel as comfortable with such a hardcore theoreticians, but.",
            "Since this is the last talk of the day, it's going to be somewhat of an entertainment, so take it on the lighter side and so essentially I am.",
            "What I want to talk about is some sort of.",
            "Combination of.",
            "Model order selection with part of the function that you really try to optimize.",
            "So in some sense the motivation is not model selection.",
            "The motivation is to do reinforces learning correctly, but it turns out that if you do it correctly, you also self regularize the problem in some sense and and so so."
        ],
        [
            "It's going to be connected to the topic of this workshop only in the second half of my of my.",
            "Talk, which I hope to get squeezed into the time so I don't have to introduce here.",
            "I suppose anything about MDP's and Palm.",
            "DP's just want to make sure that the notation is more or less settled so we talk about States and actions and model which condition probabilities an rewards and then when we have a palm DP we usually assume that.",
            "The states are not directly observed and another probability distribution, another channel.",
            "If you want which is denoted here by Sigma in addition to \u03c0, which is the usual policy which obscures somehow the states to the agent or Organism, whatever it is.",
            "And of course this is completely standard, even taken from all the standard textbooks.",
            "The goal of reinforcement learning at least a planning problem, is to optimize and.",
            "Reward or to find the policy which.",
            "Local policy local map from the state or from the.",
            "Belief states which are going to be some stochastic probability distribution over the States and with respect to the action and of course we want to optimize the expected future reward or the value, discounted or not, discounted out.",
            "I mentioned this issue of discounting."
        ],
        [
            "So of course, we all know this is taken from Sutton Barto.",
            "This is the standard model of thinking about an agent interacting with an environment, and this is actually in the fully observed, or the MDP case, and we have a model change we have states or transitions, and we have rewards and everything is rather simple and.",
            "Of course, the way I think about it this is."
        ],
        [
            "OK, the way we usually solve it again, the planning problem in the finite infinite of both infinite infinite rise by infant horizon, it becomes a lot a lot more transparent to formulate it this way you just.",
            "Define a Bellman equation which is some sort of a self consistency of the expected future reward with some discount and essentially it tells you that the value at your current state is the average of the value of the next state average of all possible next states average respect to the policy and the model of the transitions.",
            "Plus the local reward.",
            "This essentially some sort of continuity equation if you want to, or linear equation in the policy.",
            "It's also given the policy is a linear equation in the value, so it has many many ways of thinking about it.",
            "Of course usually what we do is just iterate the policy iteration, iterate between \u03c0 and VU, do sweep over states, estimate V given pie, and then look for the greedy pie or the deterministic pie and.",
            "We know that the whole thing converges eventually, even though the problem is not convex, but there's a nice converging proof of this terministic algorithm.",
            "Was I missing this in this formulation is this?",
            "Actions and behavior.",
            "In general we don't care only about rewards, we care about other things.",
            "In particular, if I."
        ],
        [
            "They know if I if I let look at this I graphical model for the.",
            "I call it the perception action cycle, but it can be just a sensory motor or or agent environment or whatever you want.",
            "So it's a nice way of thinking about those variables in discrete time, the environment or the world here is denoted by W. It's a Markov chain and the memory of the mind or the mental states of the of the Organism.",
            "We can just think about the invest.",
            "W is also a Markov chain, and these two Markov chains are usually linked together.",
            "Locali by observations and actions which make them just just a larger markets team of all these four variables.",
            "But the point is that we know only some of the variables and we try to optimize respect to the others.",
            "But in order to quantify the problem, usually what we do is.",
            "We add so I think about I want to think about it in some sort of information theoretic processes, some sort of an equilibrium.",
            "Think about a very very long chain like this, where both Markov chains are in some sense stationary, which is of course an approximation which is only going to give us something useful if we can beyond it can go beyond it eventually.",
            "But what is nice about this particular graphical model is that.",
            "You see, immediately that the standard reinforcement learning, which is essentially attaching some sort of reward to the transition between the world state from WW2 plus one through observation and action.",
            "This is incomplete in some sense because there is another triangle here, which is what's actually happening, mentale or or in the belief states of the Organism when it moves from empty to empty plus one in terms of quantifiers and what I would like to argue both on.",
            "Some philosophical grounds if you want and just for the completion of this symmetric graph, there should be something here as well.",
            "And which really is associated with the change of knowledge or change of belief that the Organism or the internal states of the Organism has, with respect to the environment.",
            "So this is the way I think about about the complete story of Palm DP, at least in equilibrium.",
            "There is this internal external local coastal reward associated with with the environment transitions.",
            "And there is an internal reward which we have to identify somehow.",
            "Which is associated with.",
            "The information gain or the change of the distribution or the belief states.",
            "That the mind the mind on the memory has about the world at about the actions.",
            "And of course, I want to think about these two things.",
            "The observations of the world as some sort of perception of perceptual channel.",
            "If you want and the actions or predictions and actions that the Organism is making on the environment as some sort of predicted channel.",
            "And these two channels are going.",
            "As those of you heard me several times this week already and know that they are important in my in my game, but it's not the focus of this talk.",
            "So these two channels are really actually balanced.",
            "I believe in some sort of optimal behavior.",
            "So what I want to do is to generalize somehow the Bellman equation in a way that will include.",
            "This information gain and at the same time I will also gain some regularization of the problem and and actually model order complexity control, which I find interesting.",
            "So I want to solve the real problem.",
            "I mean what actually happens in terms of this internal reward of organisms?",
            "But I also want to cast it into a mathematical problem which is well defined in terms of learning theory.",
            "So usually when you do something right, these things happen together.",
            "The right thing is also mathematically correct.",
            "But not always so.",
            "Anne.",
            "You can actually.",
            "I mean they can philosophize on what is really the meaning of this internal reward.",
            "But just remember that.",
            "We have intrinsic motivations.",
            "We do things not only because we get our external food or money or whatever it is that you are rewarded with.",
            "But also we are sick knowledge we have.",
            "There's something called information picking pick up, information gathering and our behavior is only governed by both processes, by by the need to acquire true reward.",
            "And by curiosity, if you want to buy the net over exploration and this should somehow.",
            "Take care of this need to know more about the world.",
            "If I'm right.",
            "OK, so in order to actually make sense of this graph, that actually if again, for those of you who know what I'm after, I'm I'm really thinking about many cycles like this.",
            "Not one thing to think about true behavior of organisms or two agents.",
            "They usually interact with the environment on multiple scales starting from very short times of milliseconds all the way.",
            "Two years, maybe more that.",
            "Essentially, as long as we plan things, as long as we remember things, we have interaction with the environment, so these are multiple scales and multi multiple timescales, but the fact that there are many of them can allow me to use the standard asymptotic techniques of information theory if you want and think about the typical distributions of such a long Bayesian network.",
            "So think about it as something which was for a long time and I'm going to optimize those local probabilities such that.",
            "The whole thing will be typical, which means that I have a variational optimization problem which is really very simple here.",
            "Just minimize information subject to everything that you know or maximize entropy.",
            "Subject everything that you constrain in the problem.",
            "Alright, so."
        ],
        [
            "The first thing I."
        ],
        [
            "To do is we need to bring together these two gentlemen.",
            "Call Chen and Rachel Bill, and I'm not quite sure that ever met, but they could have and they should have because they were they were thinking about similar problems.",
            "Shannon is of course known mostly for his information theory, but he actually spent a lot of time, especially in the 50s and 60s, trying to solve a major problems like this, and I'm sure actually he actually wrote very, very nice papers like chess game and other other.",
            "Game theoretic or?",
            "We really knew quite a lot about dynamic programming, just didn't then formulate it in precise, precise ways that Bellman did so anyway, so."
        ],
        [
            "If you look again at the MDP from an information theoretic perspective, it's actually quite straightforward to see that I can think about Trajectory's on MVP.",
            "And about the description of projectors on MDP, some sort of code, I mean, so you can think about all the other decisions being binary.",
            "This is not a big deal and I can describe.",
            "What is the shortest, any any any path on this on this graph can be described by the shortest number of bits, which I can think of is a code word.",
            "And of course I can concatenate trajectories and I can think about can have all sorts of properties like the Kraft inequality and so on and so forth, But this is rather trivial, so the other thing.",
            "OK, so I really want to somehow link this notion of information optimality with the notion of control theoretic optimality, or the bellman optimality, and I think it's completely.",
            "Straight forward to see this."
        ],
        [
            "So think about this type of game of interaction with the world as an exchange of decisions and choices.",
            "Well move player move and so on.",
            "And if I simply look and I could actually play with you, all sorts of formal game and put some axioms on what type of measure of information I should put here, which really satisfied this action.",
            "But it's rather easy to see that it's I don't have that many choices.",
            "If I want this type of recursion to hold.",
            "So whatever my measure of information at the state S with respect to some goal and the goal can be to get home or the gold can be to accumulate.",
            "The maximum reward on the way or whatever it is, then this information.",
            "The way it is defined by Shannon, I mean the neutral information is really factorizes in the sense that only on a Markov chain on MDP, and also for the palm DP, but slightly more complicated to writing in the nice the nice way, which is nothing more than the chain rule of information or the activity of the log with respect to product of distributions, and essentially so that my knowledge about the goal at the state St is going to be the average in all of the goal with PIE.",
            "At state SD plus one respected the goal average over all possible S T + 1 plus what I'm gaining in between.",
            "So this is simply the Bell.",
            "My question.",
            "I'm cheating here a little bit, but we'll see in a minute that is actually useful cheat.",
            "So and the proof of this is really."
        ],
        [
            "The straightforward in the context on MVP.",
            "Remember that I'm not learning anything now, I'm just trying to describe this flow of information between the environment and the agent in an equilibrium where nothing is changed in terms of learning, it's not yet learning.",
            "So in order to really make the analogy with with the Bellman equation complete.",
            "And for many other reasons, I actually think about the quantity which I called the complete analogy with the cost to go.",
            "The value to go in control.",
            "I call it the information to go.",
            "And you too.",
            "Very simple, intuitive reasons.",
            "I did find this information to go as the average conditional average of all the future States and actions conditioned on my current state and action, just like the Q in the MVP of the probability of all the future state and action given condition of my car divided by some sort of prior.",
            "Which in this case is the agnostic prior.",
            "What I mean by this is is really assume complete independence between the States and the actions.",
            "And of course you can put a much more sophisticated prior, but this is nice because then everything factorizes.",
            "And of course I can simply write this information to go and forget for a second about the issues of convergence.",
            "This, as it is defined, is actually imposed if I'm thinking about Infinite Horizon.",
            "So if you how to discount these things, it's a tricky question, but at this point think about finite horizon and it just makes things easy.",
            "So this information to go at time T is again the average with respect to the next transition of my state of the information to go at time T + 1.",
            "But now I have an explicit form from this.",
            "For this information gain and I colored it is blue and red systematically, where blue is associated with the information that I get from the environment in the transition.",
            "So I I do something, I act at state St and then the information that the environment responds.",
            "Respond by moving to SD plus one.",
            "This can be if you want an interaction by asking a question and getting an answer, or this can be a measurement, or if you are living in a completely deterministic environment then this has no information, essentially because.",
            "It's going to be one for the true States and zero for the other states, but any stochastic environment distance.",
            "This is some sort of information gain.",
            "Of course, if I'm asking the right questions and this can be very useful.",
            "This time is even simplify.",
            "Understand it's simply the information that I need to know before I act.",
            "Essentially, of course, without the averaging, this is still not still not necessarily positive quantity.",
            "Only after average them I get something which looks like an information and this is simply going to be the capacity of my controller.",
            "If you want how many bits I actually need to send from the state to the.",
            "To perform the action.",
            "So this is a very natural measure of of complexity, complexity, the control complexity order, predictive capacity after averaging, and this is some sort of information that I gained by the responses of the environment.",
            "Cause this is slightly.",
            "Impose at this point in order to really make sense of it, I need to move to the to the palm DP setting.",
            "Believe me that you can do it, just makes it adding it's adding a lot more terms depending on this Sigma and the transition of the memory and so on.",
            "So I keep the notation simple for this talk and talk only about MDP.",
            "And believe me that the property is done in a very similar way, although of course the tractability of the algorithm is very different.",
            "So I guess an explicit expression for this information gain as essentially two specific of what I call information gains need information in the control and the information supplied by the environment.",
            "By the way, if you are worrying about the signs here and you should, because this looks strange.",
            "I mean, why should I pay something by being more stochastic?",
            "I mean, but remember that this is the entropy if you want to.",
            "Information that I get's response to my action.",
            "So just like encoding, if you want to ask good questions, you should maximize the entropy of the answer of the reply.",
            "So by minimizing this eventually I'm going to maximize the conditional entropy and so this is very good.",
            "This is precisely behaving in the most informative way, respecting the environment's response and minimizing this is simply.",
            "Try to be as least committed as possible about your actions.",
            "OK so this is simple.",
            "I have essentially this.",
            "Information gain explicitly stated in terms of my unknown policy and in general in terms of the unknown.",
            "Ception observation channel as well, and maybe the way I'm gonna pull up my memory."
        ],
        [
            "So what is the first thing that you can see immediately?",
            "Is that indeed you unify.",
            "Classical questions and information theory, which are essentially entropy reduction problems like like finding the coin of 20 question games or things like this.",
            "So you can think about about the Huffman coding.",
            "If you want, which essentially an interaction algorithm is a Bellman equation which is actually very nice because you know of course that an optimal code is a tree where every subtree is also optimal, so you do it backward recursion, and this is precisely the mirror image of the.",
            "Optimality on codes.",
            "And of course you can think about it as some sort of movement on a simplex.",
            "Their belief states again, there's a huge computational issue here which I have to explain.",
            "I mean, this is an infinite state space.",
            "Of course you have all the all the problems that we have with belief states in general, but conceptually it's very nice.",
            "I mean, you just solve this Bellman equation.",
            "This information equation and you actually solve coding theoretic problems and now.",
            "But now you can put these two things together so there's a long list of things that you can now.",
            "Put bring from information theory and in writing the control theoretic."
        ],
        [
            "Setting, for example the Huffman coding.",
            "As I already said, essentially just an entropy reduction algorithm and everything.",
            "So in terms of this information, again, just compare it to some sort of ignorant uniform distribution.",
            "Eventually it boils down to something like the entropy at the level of the tree is the average over the lower level of the tree plus the entropy gain, which is precisely how much entropy you got in your question or in this in the in the tree junction.",
            "So it's very important, of course.",
            "Quite obvious, of course, may take a turning it into an equation."
        ],
        [
            "Different story I can cast into things like hypothesis testing so sequential about this.",
            "Testing is nothing like nothing but a game of information gained of adding information gained from examples.",
            "You know that eventually make a decision.",
            "I said a little more about it on Monday and my tutorial, but that should be obvious here.",
            "You can.",
            "Look at classical problems like Kelly Gambolling, in which is a nice example of where information is used, not for just for coding for other things and cast it into this as well.",
            "So actually it's a good."
        ],
        [
            "Unified formalism.",
            "So now I want to go back to the Bellman equation or the the valuable."
        ],
        [
            "Xavier and of course now I have this dual part of the story I had.",
            "I have the original Bellman equation for the.",
            "State transitions of the world as we always do, and I have this information bellman, like equation, they look exactly the same.",
            "So of course they're not the same, because if they were the same could gain anything from this.",
            "While here, the rewards are numbers which are completely arbitrary in some sense.",
            "In the fixed here, the rewards are functions in a nonlinear functions of my probability distribution and therefore.",
            "Although they look the same, they are not the same equation, but they have, so I have to prove something about convergence and other things.",
            "It's not obvious, but in principle when I see such equations, the first thing we want to do is to combine them.",
            "For example, asking what is the minimal information to go or the mirror information that you need to know about the future.",
            "Subject to some constraint on the value.",
            "If you think about it in terms of planning, what is the simplest possible way in terms of decisions that I have to make in order to get home?",
            "It's actually much better than the shortest and the fastest in many ways, yes.",
            "Information.",
            "I agree with you, but I know my space is sufficient services.",
            "Yes, I agree with you that in the MVP case it's somewhat funny.",
            "For the funding fee is really interesting, but actually even from the MDP it gives us something interesting, although in the only thing which is really unnecessary in the MDP is that the environment is not really telling me anything.",
            "I know the state, so those observations depart the blue part of my information gain is in some sense needless, but in the end it because you agree with me that there's actually a lot of information, and if I really want to combine this information gathering with the road gathering behaviors, I need both of them, so it's just so this is one place where I'm cheating.",
            "The other places where I'm cheating is that really I assumed?",
            "Independence between the steps and of course, which means that I get the same amount of information at every from every example frame.",
            "Remove my my cycle as you force.",
            "No, this cannot be true, because if you learn just like in hypothesis testing for example, so the information gained images, just log likelihood ratio.",
            "But when you learn you already know what's going to happen and actually gain less than that.",
            "But this type of treating is actually useful because this really allows me to put these two things on the same footing.",
            "So while this reward is really accumulated and you know this is this, not it's not.",
            "It should be really accumulating because.",
            "You need some sort of discounting to make those things work, but here is without a disco visit.",
            "This car, never mind the discount is actually telling you that the road in the next step is not going to be exactly the same.",
            "Now in your planning as rewards right now, here I have no discounting.",
            "And this is something which bothered me for a long time.",
            "I mean, how do you really discount information?",
            "But but you, all of you know enough about learning to know that.",
            "The label now and the label 10 steps on now no, I know I'm not don't have the same value to you and it depends of course what you already know about the version stays.",
            "This has a lot to do with active learning as many other things, I mean.",
            "So if I give you 2 access and which one to label, you will choose the one which will get the maximum row.",
            "If you want the maximum information gain or the maximum.",
            "Further ahead in terms of reduction of the hypothesis space and but you know that eventually when you accumulate those informations that you learn through learning is never expensive, it grows.",
            "Either logarithmically in the finite dimensional case or it can grow like a sub linear function.",
            "But the fact that it's not growing extensively is really very important.",
            "This is a fundamental aspect about learning for all of us.",
            "OK, so I have these two type of Bellman equations, but for now I just want to play the name game which is solving them together and the major things to think about it.",
            "OK I want to design my planner on my GPS planner if you want to give me the simplest possible way home.",
            "Under some constraint on the value, which mean I want to get to Madrid, but in less than 10 hours OK, something like this so, but otherwise I wanted to be the simplest possible with the least committed future in some sense.",
            "And this commit futures to mean the minimum number of assumptions.",
            "Compute assumptions that I actually make about the future, which is it's very simple.",
            "You just want to minimize this information to go."
        ],
        [
            "OK, so of course you can do this."
        ],
        [
            "Mathematics is trivial, but I still show it to you.",
            "You just combine these two functions with some sort of LaGrange multiplier.",
            "So this is the information to go, and this is I want to minimize this and under constraint on this and those from some reason avoid about the fact that I didn't put the value of the constraint.",
            "I hope you know that I can always get rid of it and move all my parameters to the garage multiplier, so forget about this is really constrained optimization, But this has some nice formal properties.",
            "First of all, it looks very much like entropy and energy, so we'll call it free energy.",
            "But of course not has almost nothing to do with the thermodynamic free energy, except that it's exactly the same style of optimization.",
            "This plays the role of an entropy disposed world for energy, and you're going to get similar things.",
            "And of course everything is within a Bellman equation, so I have a recursion the same recursion again for this linear combination of information and cost and.",
            "It just looks a little longer, but I believe it's even much longer in the Palm DP case, but it's a very similar structure.",
            "Even from this Bellman equation alone, you already get some interesting properties in quantities which are not entirely trivial.",
            "For example, there is a connection.",
            "In this equilibrium flow of information with the environment between the capacity of your perception, the capacity of your behavior and the cost and its link.",
            "So these three quantities sum to 0.",
            "There's something nicer for some.",
            "Some rule.",
            "Of course, this is without discounting of information yet and yes.",
            "I'm sure that these two terms are playing the role of energy and entropy.",
            "I'm not sure I'm just making the analogy and just.",
            "See that they're both energy and the fluctuations which actually generated in the entropy is still missing and would come in from the finite sample size considerations.",
            "I don't think that entropy is a finite sample size affect.",
            "Entropy is inextensive quantity and find a finite sized are corrections to the entropy, their their succession dissipation.",
            "Also other things which are not including not included in this formulation fluctuations trigger essentially the magnitude of the entropy and the value of information in the value of.",
            "Of the value and the information seems to be both target functions in this community, so the both these two quantities.",
            "The way I formulated here.",
            "Just for you, I mean, these are extensive quantities.",
            "They grow linearly with my time and they ignore the fluctuations.",
            "You're absolutely right.",
            "If I really want to be serious, I have to talk about learning.",
            "Which is the deviation from this equilibrium in some sense.",
            "I changed my model.",
            "This is a much more interesting question and it's going to.",
            "It has a lot of similarities with the fluctuation dissipation theorem.",
            "Yes, I get into production and I get in production.",
            "I get other things, but it's not this theory and this is just for the physicist in OK. Now this is very simple.",
            "I mean this is what I want to do here is to look at this as some sort of information.",
            "Of course it's an information theoretic quantity, but it's also as of course most of you notice already.",
            "It's also a regular regularizer, pacbase bound on my learning.",
            "And so that's so the technical thing is that I can minimize this very easily, of course, because of the fact that my information gain is a long linear function of my policy, and the other thing that I want to optimize, I'm going to get a nontrivial solution to this equation."
        ],
        [
            "So you just forget about some of these details.",
            "This is just rewriting it in a way which looks more familiar to physicists and some sort of a gift distribution.",
            "But eventually the answer is here you can optimize this subject to the assumption of Bellman optimality.",
            "Recursively in exactly the same way that you do for Internet programming in general.",
            "You optimize it in a backward manner, and of course we have to do this forward, backward in order to converge eventually to unique solution and you can prove also in a minute a global convergence with algorithm for any finite better.",
            "Remember, better was a LaGrange multiplier associated with the value, so it's very much like the the temperature in thermal dynamics and what you get is that the optimal policy is soft, soft, Max type of of policies.",
            "Stochastic in general, until as long as better is finite and the optimal policy is simply exponential in this free energy.",
            "But in a sense it's just a slight generalization of most most people do anyway when they do softmax policies, which is just exponential in the queue.",
            "In the next, this is the simplest, simplest kind of softmax.",
            "Here you actually get the information.",
            "Term also affects the fuzziness of your solution.",
            "This, by the way, this these three questions, of course implicit and in order to solve them, you need to iterate them.",
            "But this is precisely that.",
            "There are multiple out algorithm in information theory, or we know that this is converging very quickly to the fixed point, and there are no it's convex and there's no problem with local Optima, and so OK, so you can solve this problem.",
            "Essentially it's reiterating two equations.",
            "The optimal policy which replaces the greedy policy search in the in the in the usual policy iteration and the free energy, which is solved by the this Bellman equation.",
            "Ordinary programming in a backward flip on the state."
        ],
        [
            "Now the only thing to be said about it, but the solution is actually obvious.",
            "And So what I get here is some sort of a value of information type of theorem.",
            "It tells me that.",
            "Let's say I'm solving a simple problem like this maze.",
            "It's a very simple maze.",
            "Let's say that every action can take me in one of eight directions.",
            "When you go to.",
            "Better Infinity means that you put all your only the value is important information is cost less.",
            "Then of course you go back to the standard RL and when you decrease better or go to higher higher stochastic city you get usually this type of upward concave curve which is very much like the rate distortion function but in reverse which tells you how many what is the minimum number of bits that you need to assume in your trajectory in order to get a certain value.",
            "Only points below this curve achievable in principle.",
            "The curve is the optimal limit and be above the curve is unachievable, so this is a classical value of information curve as far as I'm concerned because what is the minimal value at the given the maximum value to give information or the minimal information at the given value.",
            "Of course the two things are completely dual to each other and you can prove monotonicity here, as long as you're playing inside.",
            "Inside the training there's no issue of learning yet.",
            "But it's already nice about this.",
            "That festival in many problems you can really compress the complexity of the fallen significantly by moving from, let's say, in this case.",
            "It is very simple case.",
            "You can compress by a factor of three or four day.",
            "The number of bits almost without losing any value.",
            "But beyond that I mean.",
            "So here I need a lot less decisions here then I need there, but the value decreases only slightly.",
            "Of course we can make it more more precise and I also get that.",
            "The algorithm somehow identifies the points on my trajectory.",
            "Let's say these two 3 two corners here, where my decisions have to be much more simplistic, much more precise.",
            "So these are really the points where you have to worry about knowing your future precisely and the other places are not as important and this type of segmentation of the future into chunks between which you have to be very precise, is actually a very interesting problem on its own.",
            "Was trying to say something about it earlier this morning, another workshop.",
            "This is essentially what is eventually going to give me some sort of hierarchical description of my plans, but forget about."
        ],
        [
            "Right now, so here is just another simpler example.",
            "Imagine that you have a maze like this.",
            "There is a wide Rd here and narrow short shortcut here.",
            "And of course, in order to find this."
        ],
        [
            "Scott, you actually need to be more specific about the information, let's say so in low temperature and high temperature you your agents will do something like this.",
            "I mean the chances of them getting into this narrow Rd where is very very small.",
            "But when you."
        ],
        [
            "When you increase the better, let's say here.",
            "If this is very high, better the chances of them to find the shortcut is very very high, and notice by the way, that this is going to give you some."
        ],
        [
            "The phase transition in behavior, so the information curve would go stickley's until you find more or less the shortcut and then it more or less has saturates.",
            "So it's really nice way again of exploring the structure of your problem.",
            "OK, so."
        ],
        [
            "This is just another slightly more interesting example.",
            "Now I really want."
        ],
        [
            "Move to the to the issue at hand, which is really regularization and model order selection.",
            "So first of all, we could prove this.",
            "This work was done, but my way with Ohatchee, Miranda, Newton, Rubin an what I think all of you know by now so.",
            "Essentially, we can easily prove essentially just by repeating the classical convergence proof of of the user lorelle, that this type of transformation on my free energy has a unique solution.",
            "And here we actually ignore the information term that she was worried about, doesn't it doesn't help anything in there in the MVP case.",
            "So for the MDP you have a global convergence of this algorithm, finite beta by the way for infinite better than the usual case.",
            "This requires some sort of tricky limit, but I'm actually happier.",
            "It seems that with finite better, you don't only gain some sort of saving information, you also the proof of convergence is much more."
        ],
        [
            "Bust.",
            "Cause if we find out better things are convex unlike the case of deterministic policy where you may have two equal equally likely solutions which are not the same.",
            "So now we actually take another look into this information gain.",
            "And of course it doesn't take too much for anyone here to realize that this information to go term is nothing backed.",
            "the PAC Bayes bound in a sense.",
            "I mean the posterior to prior KL, which is the complexity of my pack base, is precisely the time that I added and wanted to minimize in order to achieve this.",
            "If you want cognitive goal of minimizing information about the future and of course.",
            "Now you can play the same game again and see OK. We have this really celebrated theorem of McAllister I."
        ],
        [
            "I just plug in my information to go.",
            "The probability of all the future conditioned by the prior and I put the prior to be again this stupid prior of independence independent action and and states.",
            "But you can of course do much more sophisticated priors here.",
            "And of course the complexity of the class is directly related to the description of your report is given the problem.",
            "This is my intuition of the PAC Bayes theorem.",
            "Essentially it's nothing more than the classical outcome razor result.",
            "If you want just stated in terms of information measures so.",
            "This is really bit the description of airport's class, and in this case it's an extensive quantity.",
            "It's only it's going with T, so you have to talk about bound rate or information rate and so on.",
            "But now you can prove the following theorem.",
            "Now imagine that there's actually learning, so let's at least in the simplest possible way.",
            "We took here something like episodes in this game.",
            "In this maze finite episodes, which give you some sort of fluctuations over the reward, so you don't really know the rewards.",
            "And then you compare your essentially the expected reward to the priority board and all things here, and then the whole thing boils down to learning theoretic setting which really makes that gives this value of information curve.",
            "This is the in training curve and this is the outer training curve out.",
            "So here I am learning my reward function.",
            "I can do the same for the model itself is a little more tricky based on samples or episodes.",
            "So it's the simplest possible scenario that we could think of.",
            "In love learning in terms of.",
            "Episodes through through a maze level through an MVP and of course what you always get that with any finite number of samples.",
            "There's an optimum.",
            "Which really optimizes this bond.",
            "So optimizing this by simply minimizing the mutual information subject to something else, which is the expected reward.",
            "And if you try to get beyond this, you see that your expected rewards going to decrease above a certain number of bits.",
            "So essentially this is.",
            "This is nice because we started with some attempt to understand cognition and understand information gathering.",
            "And we ended up with a nice application of the PAC Bayes bound to and you see that the bound is not just abound, it's part of the story is an essential component of learning."
        ],
        [
            "OK, so.",
            "The optimal tradeoff between future and value is obtained at some finite better, which is directly related to the optimization of the complexity bound.",
            "It's not precisely model order selection, but of course now I can play with different models and plug plug exactly in the same the same type of bound.",
            "I know when I get these two type of stories parallel to each other, you optimize the complexity and you.",
            "Make your.",
            "Information gathering information accumulation information gain story.",
            "More involved.",
            "Depending on the structure of your model.",
            "OK, so I think I'm more or less out of time, but actually way beyond my time but.",
            "And I just want to hint at what we do now, with with Palm DP's which is a lot more interesting of course.",
            "So first of all we can."
        ],
        [
            "It turns out that you can write.",
            "I have how much 5 minutes, 10 minutes, 10 minutes we actually don't want to say too much about it, but I'll say a little bit."
        ],
        [
            "So we can.",
            "Revealing some of the things in the oven, specially when you take it online afterwards.",
            "So essentially what we know how to do and this is work of another student who folks mainly.",
            "Essentially, we know how to rewrite the whole thing in apondi setting and and and essentially this information gathering.",
            "They can be done also sequentially, so this is also not very surprising, so you know that when you have exact sufficient statistics, let's say they can always be written in an online way.",
            "I mean additive way, just like in the airport.",
            "This testing scenario you can always add one quantity which accumulates the information in every example and forget about the rest just like just like you do with averages or with variances and other things.",
            "In the Gaussian case, let's say or in there.",
            "For this testing case you simply add the log likelihood ratio of the current example, forget about the rest.",
            "What we know how to do now is to generalize it to the cases where you don't have exact sufficient statistics and.",
            "And again, so you have those information capacities as in some sense as controlling of the complexity, but here.",
            "I'm giving up on being exact sufficient statistic, but I don't give up on having an exponential form, which means again that I have additive online ways of doing this.",
            "Statistics approximate sophistic calculation.",
            "So believe me that there's something like a dual Bellman equation.",
            "It's not permanent question because I'm not averaging over the future.",
            "I'm summing the past so there's no average.",
            "But when you do it enough, you have you gain those accumulated statistics.",
            "So this is what I called.",
            "The online was sequential information bottleneck, but you don't have to buy this particular graph an.",
            "No, there's actually 1 interesting case, which is just a cennetig condition that we already know words.",
            "Of course in the case of a Gaussian environment.",
            "And linear systems.",
            "You get back the common the common filter formulation, or if you want an LPR formulation, depends how you think of control theory, so there's also some sanity condition.",
            "Everything boils down again to the Gaussian environment.",
            "The Gaussian bottleneck is solvable exactly, and and then the linear system case you get back Coleman essentially just as a simple reduction of this, so this is nice, and I could probably tell you more about it.",
            "What happened beyond the Gaussian case?",
            "And beyond the Gaussian case, as I said already earlier this week, we can extend it using kernels.",
            "And and anything that, after after embedded embedding with the kernel, can become more more Gaussian can be attracted in this in this in this story.",
            "So essentially here we really have some plan that I hope next year or something I'd be able to really completely tell you, but so the kernel I be something that is actually a very trivial thing.",
            "You just embed the past and the future of environment using some sort of nonlinear function, which makes it more Gaussian.",
            "Test this in various ways anyway, so this is essentially all I wanted to say here and then."
        ],
        [
            "I'm usually surprised to finish in time so.",
            "Essentially what we've been trying to do here is to put on the same formal footing information theory in control theory, and with with it some sort of close from theory of perception and action, which is a very ambitious goal, but it's actually doable, I think.",
            "And of course we use for this.",
            "Yeah, OK, so those conclusions are not Franklin tally conclusions for this talk with.",
            "We use for this essentially some very simple properties, information information theoretic quantities we ignored.",
            "This is the kind of cheating that I was doing.",
            "I ignore the fact that I have learning and played the game, that there is some sort of an equilibrium, which means the systems are fixed, but the information flows are regular and I think we have a more or less complete description of this equilibrium as setting which is, as I said, a constant flow of information and the next step of course is to embed learning into it and we just started to do it with this.",
            "Belmont Park base bound and see what happens when you deviate from equilibrium and really one of the nicest analogies that you have here is when you think about discounting.",
            "So I never liked the way you do discounting in our rail, because this exponential discounting is mathematically very convenient, but I needed some more fundamental way of doing it.",
            "If you think about the information discounting, it becomes much more interesting.",
            "So again, information discounting is directly related to learning directly related to how much you are willing to pay for the label of this example in two days from now, not now.",
            "And this is a sub linear function.",
            "This sub linear immediately if I want the two equations to be consistent with each other.",
            "These two functions have to go together and the only way I know how to do it now is by changing the notion of time mentale.",
            "So essentially, if I think about.",
            "My planning time notice Chronicle time, but this information time.",
            "The same amount of bits per unit time.",
            "Then everything makes perfect sense, so I know 10 bits about the next hour and maybe 10 another 10 days about the next day and another 10 days about the next month and so on.",
            "It's not that bad, but maybe so.",
            "So essentially I should plan in terms of number of decisions I have to make and not in terms of the time that actually passed from state to state.",
            "Think about your smart tuition stick about the first second.",
            "It actually links together, so the fact that information usually grows logarithmically logarithmically with the number of a number of samples is directly linked with the exponential exponential discounting of reward.",
            "So these two are completely consistent with the channel.",
            "If it doesn't grow logarithmically, which means that we don't have number finite parameter space or something like a nonparametric model, this immediately reflects on known exponential discounting.",
            "Which is sometimes known as hyperbolic discounting or things like this, which actually turns out to be important in biology as well.",
            "So this type of crazy modification of time makes perfect sense and is also a way of getting rid of the computational difficulty of Pompey's.",
            "So this is really to be done.",
            "I think.",
            "I'll stop here.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, good evening.",
                    "label": 0
                },
                {
                    "sent": "When a student of yours is asking you for doing a favor and give a talk in the workshop organizers, and I usually say yes, but when I saw the list of speakers, I really got scared because I'm.",
                    "label": 0
                },
                {
                    "sent": "I don't feel as comfortable with such a hardcore theoreticians, but.",
                    "label": 0
                },
                {
                    "sent": "Since this is the last talk of the day, it's going to be somewhat of an entertainment, so take it on the lighter side and so essentially I am.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk about is some sort of.",
                    "label": 0
                },
                {
                    "sent": "Combination of.",
                    "label": 0
                },
                {
                    "sent": "Model order selection with part of the function that you really try to optimize.",
                    "label": 0
                },
                {
                    "sent": "So in some sense the motivation is not model selection.",
                    "label": 0
                },
                {
                    "sent": "The motivation is to do reinforces learning correctly, but it turns out that if you do it correctly, you also self regularize the problem in some sense and and so so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's going to be connected to the topic of this workshop only in the second half of my of my.",
                    "label": 0
                },
                {
                    "sent": "Talk, which I hope to get squeezed into the time so I don't have to introduce here.",
                    "label": 0
                },
                {
                    "sent": "I suppose anything about MDP's and Palm.",
                    "label": 0
                },
                {
                    "sent": "DP's just want to make sure that the notation is more or less settled so we talk about States and actions and model which condition probabilities an rewards and then when we have a palm DP we usually assume that.",
                    "label": 0
                },
                {
                    "sent": "The states are not directly observed and another probability distribution, another channel.",
                    "label": 0
                },
                {
                    "sent": "If you want which is denoted here by Sigma in addition to \u03c0, which is the usual policy which obscures somehow the states to the agent or Organism, whatever it is.",
                    "label": 0
                },
                {
                    "sent": "And of course this is completely standard, even taken from all the standard textbooks.",
                    "label": 0
                },
                {
                    "sent": "The goal of reinforcement learning at least a planning problem, is to optimize and.",
                    "label": 0
                },
                {
                    "sent": "Reward or to find the policy which.",
                    "label": 0
                },
                {
                    "sent": "Local policy local map from the state or from the.",
                    "label": 0
                },
                {
                    "sent": "Belief states which are going to be some stochastic probability distribution over the States and with respect to the action and of course we want to optimize the expected future reward or the value, discounted or not, discounted out.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this issue of discounting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course, we all know this is taken from Sutton Barto.",
                    "label": 0
                },
                {
                    "sent": "This is the standard model of thinking about an agent interacting with an environment, and this is actually in the fully observed, or the MDP case, and we have a model change we have states or transitions, and we have rewards and everything is rather simple and.",
                    "label": 0
                },
                {
                    "sent": "Of course, the way I think about it this is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the way we usually solve it again, the planning problem in the finite infinite of both infinite infinite rise by infant horizon, it becomes a lot a lot more transparent to formulate it this way you just.",
                    "label": 0
                },
                {
                    "sent": "Define a Bellman equation which is some sort of a self consistency of the expected future reward with some discount and essentially it tells you that the value at your current state is the average of the value of the next state average of all possible next states average respect to the policy and the model of the transitions.",
                    "label": 0
                },
                {
                    "sent": "Plus the local reward.",
                    "label": 0
                },
                {
                    "sent": "This essentially some sort of continuity equation if you want to, or linear equation in the policy.",
                    "label": 0
                },
                {
                    "sent": "It's also given the policy is a linear equation in the value, so it has many many ways of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "Of course usually what we do is just iterate the policy iteration, iterate between \u03c0 and VU, do sweep over states, estimate V given pie, and then look for the greedy pie or the deterministic pie and.",
                    "label": 0
                },
                {
                    "sent": "We know that the whole thing converges eventually, even though the problem is not convex, but there's a nice converging proof of this terministic algorithm.",
                    "label": 0
                },
                {
                    "sent": "Was I missing this in this formulation is this?",
                    "label": 0
                },
                {
                    "sent": "Actions and behavior.",
                    "label": 0
                },
                {
                    "sent": "In general we don't care only about rewards, we care about other things.",
                    "label": 0
                },
                {
                    "sent": "In particular, if I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They know if I if I let look at this I graphical model for the.",
                    "label": 1
                },
                {
                    "sent": "I call it the perception action cycle, but it can be just a sensory motor or or agent environment or whatever you want.",
                    "label": 0
                },
                {
                    "sent": "So it's a nice way of thinking about those variables in discrete time, the environment or the world here is denoted by W. It's a Markov chain and the memory of the mind or the mental states of the of the Organism.",
                    "label": 0
                },
                {
                    "sent": "We can just think about the invest.",
                    "label": 0
                },
                {
                    "sent": "W is also a Markov chain, and these two Markov chains are usually linked together.",
                    "label": 0
                },
                {
                    "sent": "Locali by observations and actions which make them just just a larger markets team of all these four variables.",
                    "label": 0
                },
                {
                    "sent": "But the point is that we know only some of the variables and we try to optimize respect to the others.",
                    "label": 0
                },
                {
                    "sent": "But in order to quantify the problem, usually what we do is.",
                    "label": 0
                },
                {
                    "sent": "We add so I think about I want to think about it in some sort of information theoretic processes, some sort of an equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Think about a very very long chain like this, where both Markov chains are in some sense stationary, which is of course an approximation which is only going to give us something useful if we can beyond it can go beyond it eventually.",
                    "label": 0
                },
                {
                    "sent": "But what is nice about this particular graphical model is that.",
                    "label": 0
                },
                {
                    "sent": "You see, immediately that the standard reinforcement learning, which is essentially attaching some sort of reward to the transition between the world state from WW2 plus one through observation and action.",
                    "label": 0
                },
                {
                    "sent": "This is incomplete in some sense because there is another triangle here, which is what's actually happening, mentale or or in the belief states of the Organism when it moves from empty to empty plus one in terms of quantifiers and what I would like to argue both on.",
                    "label": 0
                },
                {
                    "sent": "Some philosophical grounds if you want and just for the completion of this symmetric graph, there should be something here as well.",
                    "label": 0
                },
                {
                    "sent": "And which really is associated with the change of knowledge or change of belief that the Organism or the internal states of the Organism has, with respect to the environment.",
                    "label": 0
                },
                {
                    "sent": "So this is the way I think about about the complete story of Palm DP, at least in equilibrium.",
                    "label": 0
                },
                {
                    "sent": "There is this internal external local coastal reward associated with with the environment transitions.",
                    "label": 0
                },
                {
                    "sent": "And there is an internal reward which we have to identify somehow.",
                    "label": 0
                },
                {
                    "sent": "Which is associated with.",
                    "label": 1
                },
                {
                    "sent": "The information gain or the change of the distribution or the belief states.",
                    "label": 0
                },
                {
                    "sent": "That the mind the mind on the memory has about the world at about the actions.",
                    "label": 1
                },
                {
                    "sent": "And of course, I want to think about these two things.",
                    "label": 0
                },
                {
                    "sent": "The observations of the world as some sort of perception of perceptual channel.",
                    "label": 0
                },
                {
                    "sent": "If you want and the actions or predictions and actions that the Organism is making on the environment as some sort of predicted channel.",
                    "label": 0
                },
                {
                    "sent": "And these two channels are going.",
                    "label": 0
                },
                {
                    "sent": "As those of you heard me several times this week already and know that they are important in my in my game, but it's not the focus of this talk.",
                    "label": 0
                },
                {
                    "sent": "So these two channels are really actually balanced.",
                    "label": 0
                },
                {
                    "sent": "I believe in some sort of optimal behavior.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is to generalize somehow the Bellman equation in a way that will include.",
                    "label": 0
                },
                {
                    "sent": "This information gain and at the same time I will also gain some regularization of the problem and and actually model order complexity control, which I find interesting.",
                    "label": 0
                },
                {
                    "sent": "So I want to solve the real problem.",
                    "label": 0
                },
                {
                    "sent": "I mean what actually happens in terms of this internal reward of organisms?",
                    "label": 0
                },
                {
                    "sent": "But I also want to cast it into a mathematical problem which is well defined in terms of learning theory.",
                    "label": 0
                },
                {
                    "sent": "So usually when you do something right, these things happen together.",
                    "label": 0
                },
                {
                    "sent": "The right thing is also mathematically correct.",
                    "label": 0
                },
                {
                    "sent": "But not always so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You can actually.",
                    "label": 0
                },
                {
                    "sent": "I mean they can philosophize on what is really the meaning of this internal reward.",
                    "label": 0
                },
                {
                    "sent": "But just remember that.",
                    "label": 0
                },
                {
                    "sent": "We have intrinsic motivations.",
                    "label": 0
                },
                {
                    "sent": "We do things not only because we get our external food or money or whatever it is that you are rewarded with.",
                    "label": 0
                },
                {
                    "sent": "But also we are sick knowledge we have.",
                    "label": 0
                },
                {
                    "sent": "There's something called information picking pick up, information gathering and our behavior is only governed by both processes, by by the need to acquire true reward.",
                    "label": 0
                },
                {
                    "sent": "And by curiosity, if you want to buy the net over exploration and this should somehow.",
                    "label": 0
                },
                {
                    "sent": "Take care of this need to know more about the world.",
                    "label": 0
                },
                {
                    "sent": "If I'm right.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to actually make sense of this graph, that actually if again, for those of you who know what I'm after, I'm I'm really thinking about many cycles like this.",
                    "label": 0
                },
                {
                    "sent": "Not one thing to think about true behavior of organisms or two agents.",
                    "label": 0
                },
                {
                    "sent": "They usually interact with the environment on multiple scales starting from very short times of milliseconds all the way.",
                    "label": 0
                },
                {
                    "sent": "Two years, maybe more that.",
                    "label": 0
                },
                {
                    "sent": "Essentially, as long as we plan things, as long as we remember things, we have interaction with the environment, so these are multiple scales and multi multiple timescales, but the fact that there are many of them can allow me to use the standard asymptotic techniques of information theory if you want and think about the typical distributions of such a long Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "So think about it as something which was for a long time and I'm going to optimize those local probabilities such that.",
                    "label": 0
                },
                {
                    "sent": "The whole thing will be typical, which means that I have a variational optimization problem which is really very simple here.",
                    "label": 0
                },
                {
                    "sent": "Just minimize information subject to everything that you know or maximize entropy.",
                    "label": 0
                },
                {
                    "sent": "Subject everything that you constrain in the problem.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first thing I.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is we need to bring together these two gentlemen.",
                    "label": 0
                },
                {
                    "sent": "Call Chen and Rachel Bill, and I'm not quite sure that ever met, but they could have and they should have because they were they were thinking about similar problems.",
                    "label": 0
                },
                {
                    "sent": "Shannon is of course known mostly for his information theory, but he actually spent a lot of time, especially in the 50s and 60s, trying to solve a major problems like this, and I'm sure actually he actually wrote very, very nice papers like chess game and other other.",
                    "label": 0
                },
                {
                    "sent": "Game theoretic or?",
                    "label": 0
                },
                {
                    "sent": "We really knew quite a lot about dynamic programming, just didn't then formulate it in precise, precise ways that Bellman did so anyway, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look again at the MDP from an information theoretic perspective, it's actually quite straightforward to see that I can think about Trajectory's on MVP.",
                    "label": 0
                },
                {
                    "sent": "And about the description of projectors on MDP, some sort of code, I mean, so you can think about all the other decisions being binary.",
                    "label": 0
                },
                {
                    "sent": "This is not a big deal and I can describe.",
                    "label": 0
                },
                {
                    "sent": "What is the shortest, any any any path on this on this graph can be described by the shortest number of bits, which I can think of is a code word.",
                    "label": 0
                },
                {
                    "sent": "And of course I can concatenate trajectories and I can think about can have all sorts of properties like the Kraft inequality and so on and so forth, But this is rather trivial, so the other thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so I really want to somehow link this notion of information optimality with the notion of control theoretic optimality, or the bellman optimality, and I think it's completely.",
                    "label": 0
                },
                {
                    "sent": "Straight forward to see this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So think about this type of game of interaction with the world as an exchange of decisions and choices.",
                    "label": 0
                },
                {
                    "sent": "Well move player move and so on.",
                    "label": 0
                },
                {
                    "sent": "And if I simply look and I could actually play with you, all sorts of formal game and put some axioms on what type of measure of information I should put here, which really satisfied this action.",
                    "label": 0
                },
                {
                    "sent": "But it's rather easy to see that it's I don't have that many choices.",
                    "label": 0
                },
                {
                    "sent": "If I want this type of recursion to hold.",
                    "label": 0
                },
                {
                    "sent": "So whatever my measure of information at the state S with respect to some goal and the goal can be to get home or the gold can be to accumulate.",
                    "label": 0
                },
                {
                    "sent": "The maximum reward on the way or whatever it is, then this information.",
                    "label": 0
                },
                {
                    "sent": "The way it is defined by Shannon, I mean the neutral information is really factorizes in the sense that only on a Markov chain on MDP, and also for the palm DP, but slightly more complicated to writing in the nice the nice way, which is nothing more than the chain rule of information or the activity of the log with respect to product of distributions, and essentially so that my knowledge about the goal at the state St is going to be the average in all of the goal with PIE.",
                    "label": 0
                },
                {
                    "sent": "At state SD plus one respected the goal average over all possible S T + 1 plus what I'm gaining in between.",
                    "label": 0
                },
                {
                    "sent": "So this is simply the Bell.",
                    "label": 0
                },
                {
                    "sent": "My question.",
                    "label": 0
                },
                {
                    "sent": "I'm cheating here a little bit, but we'll see in a minute that is actually useful cheat.",
                    "label": 0
                },
                {
                    "sent": "So and the proof of this is really.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The straightforward in the context on MVP.",
                    "label": 0
                },
                {
                    "sent": "Remember that I'm not learning anything now, I'm just trying to describe this flow of information between the environment and the agent in an equilibrium where nothing is changed in terms of learning, it's not yet learning.",
                    "label": 0
                },
                {
                    "sent": "So in order to really make the analogy with with the Bellman equation complete.",
                    "label": 0
                },
                {
                    "sent": "And for many other reasons, I actually think about the quantity which I called the complete analogy with the cost to go.",
                    "label": 0
                },
                {
                    "sent": "The value to go in control.",
                    "label": 0
                },
                {
                    "sent": "I call it the information to go.",
                    "label": 0
                },
                {
                    "sent": "And you too.",
                    "label": 0
                },
                {
                    "sent": "Very simple, intuitive reasons.",
                    "label": 0
                },
                {
                    "sent": "I did find this information to go as the average conditional average of all the future States and actions conditioned on my current state and action, just like the Q in the MVP of the probability of all the future state and action given condition of my car divided by some sort of prior.",
                    "label": 0
                },
                {
                    "sent": "Which in this case is the agnostic prior.",
                    "label": 0
                },
                {
                    "sent": "What I mean by this is is really assume complete independence between the States and the actions.",
                    "label": 0
                },
                {
                    "sent": "And of course you can put a much more sophisticated prior, but this is nice because then everything factorizes.",
                    "label": 0
                },
                {
                    "sent": "And of course I can simply write this information to go and forget for a second about the issues of convergence.",
                    "label": 0
                },
                {
                    "sent": "This, as it is defined, is actually imposed if I'm thinking about Infinite Horizon.",
                    "label": 0
                },
                {
                    "sent": "So if you how to discount these things, it's a tricky question, but at this point think about finite horizon and it just makes things easy.",
                    "label": 0
                },
                {
                    "sent": "So this information to go at time T is again the average with respect to the next transition of my state of the information to go at time T + 1.",
                    "label": 0
                },
                {
                    "sent": "But now I have an explicit form from this.",
                    "label": 0
                },
                {
                    "sent": "For this information gain and I colored it is blue and red systematically, where blue is associated with the information that I get from the environment in the transition.",
                    "label": 0
                },
                {
                    "sent": "So I I do something, I act at state St and then the information that the environment responds.",
                    "label": 0
                },
                {
                    "sent": "Respond by moving to SD plus one.",
                    "label": 0
                },
                {
                    "sent": "This can be if you want an interaction by asking a question and getting an answer, or this can be a measurement, or if you are living in a completely deterministic environment then this has no information, essentially because.",
                    "label": 0
                },
                {
                    "sent": "It's going to be one for the true States and zero for the other states, but any stochastic environment distance.",
                    "label": 0
                },
                {
                    "sent": "This is some sort of information gain.",
                    "label": 0
                },
                {
                    "sent": "Of course, if I'm asking the right questions and this can be very useful.",
                    "label": 0
                },
                {
                    "sent": "This time is even simplify.",
                    "label": 0
                },
                {
                    "sent": "Understand it's simply the information that I need to know before I act.",
                    "label": 0
                },
                {
                    "sent": "Essentially, of course, without the averaging, this is still not still not necessarily positive quantity.",
                    "label": 0
                },
                {
                    "sent": "Only after average them I get something which looks like an information and this is simply going to be the capacity of my controller.",
                    "label": 0
                },
                {
                    "sent": "If you want how many bits I actually need to send from the state to the.",
                    "label": 0
                },
                {
                    "sent": "To perform the action.",
                    "label": 0
                },
                {
                    "sent": "So this is a very natural measure of of complexity, complexity, the control complexity order, predictive capacity after averaging, and this is some sort of information that I gained by the responses of the environment.",
                    "label": 0
                },
                {
                    "sent": "Cause this is slightly.",
                    "label": 0
                },
                {
                    "sent": "Impose at this point in order to really make sense of it, I need to move to the to the palm DP setting.",
                    "label": 0
                },
                {
                    "sent": "Believe me that you can do it, just makes it adding it's adding a lot more terms depending on this Sigma and the transition of the memory and so on.",
                    "label": 0
                },
                {
                    "sent": "So I keep the notation simple for this talk and talk only about MDP.",
                    "label": 0
                },
                {
                    "sent": "And believe me that the property is done in a very similar way, although of course the tractability of the algorithm is very different.",
                    "label": 0
                },
                {
                    "sent": "So I guess an explicit expression for this information gain as essentially two specific of what I call information gains need information in the control and the information supplied by the environment.",
                    "label": 0
                },
                {
                    "sent": "By the way, if you are worrying about the signs here and you should, because this looks strange.",
                    "label": 0
                },
                {
                    "sent": "I mean, why should I pay something by being more stochastic?",
                    "label": 0
                },
                {
                    "sent": "I mean, but remember that this is the entropy if you want to.",
                    "label": 0
                },
                {
                    "sent": "Information that I get's response to my action.",
                    "label": 0
                },
                {
                    "sent": "So just like encoding, if you want to ask good questions, you should maximize the entropy of the answer of the reply.",
                    "label": 0
                },
                {
                    "sent": "So by minimizing this eventually I'm going to maximize the conditional entropy and so this is very good.",
                    "label": 0
                },
                {
                    "sent": "This is precisely behaving in the most informative way, respecting the environment's response and minimizing this is simply.",
                    "label": 0
                },
                {
                    "sent": "Try to be as least committed as possible about your actions.",
                    "label": 0
                },
                {
                    "sent": "OK so this is simple.",
                    "label": 0
                },
                {
                    "sent": "I have essentially this.",
                    "label": 0
                },
                {
                    "sent": "Information gain explicitly stated in terms of my unknown policy and in general in terms of the unknown.",
                    "label": 0
                },
                {
                    "sent": "Ception observation channel as well, and maybe the way I'm gonna pull up my memory.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the first thing that you can see immediately?",
                    "label": 1
                },
                {
                    "sent": "Is that indeed you unify.",
                    "label": 0
                },
                {
                    "sent": "Classical questions and information theory, which are essentially entropy reduction problems like like finding the coin of 20 question games or things like this.",
                    "label": 0
                },
                {
                    "sent": "So you can think about about the Huffman coding.",
                    "label": 0
                },
                {
                    "sent": "If you want, which essentially an interaction algorithm is a Bellman equation which is actually very nice because you know of course that an optimal code is a tree where every subtree is also optimal, so you do it backward recursion, and this is precisely the mirror image of the.",
                    "label": 0
                },
                {
                    "sent": "Optimality on codes.",
                    "label": 1
                },
                {
                    "sent": "And of course you can think about it as some sort of movement on a simplex.",
                    "label": 0
                },
                {
                    "sent": "Their belief states again, there's a huge computational issue here which I have to explain.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is an infinite state space.",
                    "label": 0
                },
                {
                    "sent": "Of course you have all the all the problems that we have with belief states in general, but conceptually it's very nice.",
                    "label": 0
                },
                {
                    "sent": "I mean, you just solve this Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "This information equation and you actually solve coding theoretic problems and now.",
                    "label": 0
                },
                {
                    "sent": "But now you can put these two things together so there's a long list of things that you can now.",
                    "label": 0
                },
                {
                    "sent": "Put bring from information theory and in writing the control theoretic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting, for example the Huffman coding.",
                    "label": 1
                },
                {
                    "sent": "As I already said, essentially just an entropy reduction algorithm and everything.",
                    "label": 0
                },
                {
                    "sent": "So in terms of this information, again, just compare it to some sort of ignorant uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Eventually it boils down to something like the entropy at the level of the tree is the average over the lower level of the tree plus the entropy gain, which is precisely how much entropy you got in your question or in this in the in the tree junction.",
                    "label": 1
                },
                {
                    "sent": "So it's very important, of course.",
                    "label": 0
                },
                {
                    "sent": "Quite obvious, of course, may take a turning it into an equation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different story I can cast into things like hypothesis testing so sequential about this.",
                    "label": 0
                },
                {
                    "sent": "Testing is nothing like nothing but a game of information gained of adding information gained from examples.",
                    "label": 0
                },
                {
                    "sent": "You know that eventually make a decision.",
                    "label": 0
                },
                {
                    "sent": "I said a little more about it on Monday and my tutorial, but that should be obvious here.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Look at classical problems like Kelly Gambolling, in which is a nice example of where information is used, not for just for coding for other things and cast it into this as well.",
                    "label": 0
                },
                {
                    "sent": "So actually it's a good.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unified formalism.",
                    "label": 0
                },
                {
                    "sent": "So now I want to go back to the Bellman equation or the the valuable.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Xavier and of course now I have this dual part of the story I had.",
                    "label": 0
                },
                {
                    "sent": "I have the original Bellman equation for the.",
                    "label": 0
                },
                {
                    "sent": "State transitions of the world as we always do, and I have this information bellman, like equation, they look exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So of course they're not the same, because if they were the same could gain anything from this.",
                    "label": 0
                },
                {
                    "sent": "While here, the rewards are numbers which are completely arbitrary in some sense.",
                    "label": 0
                },
                {
                    "sent": "In the fixed here, the rewards are functions in a nonlinear functions of my probability distribution and therefore.",
                    "label": 0
                },
                {
                    "sent": "Although they look the same, they are not the same equation, but they have, so I have to prove something about convergence and other things.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious, but in principle when I see such equations, the first thing we want to do is to combine them.",
                    "label": 0
                },
                {
                    "sent": "For example, asking what is the minimal information to go or the mirror information that you need to know about the future.",
                    "label": 0
                },
                {
                    "sent": "Subject to some constraint on the value.",
                    "label": 0
                },
                {
                    "sent": "If you think about it in terms of planning, what is the simplest possible way in terms of decisions that I have to make in order to get home?",
                    "label": 0
                },
                {
                    "sent": "It's actually much better than the shortest and the fastest in many ways, yes.",
                    "label": 0
                },
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "I agree with you, but I know my space is sufficient services.",
                    "label": 0
                },
                {
                    "sent": "Yes, I agree with you that in the MVP case it's somewhat funny.",
                    "label": 0
                },
                {
                    "sent": "For the funding fee is really interesting, but actually even from the MDP it gives us something interesting, although in the only thing which is really unnecessary in the MDP is that the environment is not really telling me anything.",
                    "label": 0
                },
                {
                    "sent": "I know the state, so those observations depart the blue part of my information gain is in some sense needless, but in the end it because you agree with me that there's actually a lot of information, and if I really want to combine this information gathering with the road gathering behaviors, I need both of them, so it's just so this is one place where I'm cheating.",
                    "label": 0
                },
                {
                    "sent": "The other places where I'm cheating is that really I assumed?",
                    "label": 0
                },
                {
                    "sent": "Independence between the steps and of course, which means that I get the same amount of information at every from every example frame.",
                    "label": 0
                },
                {
                    "sent": "Remove my my cycle as you force.",
                    "label": 0
                },
                {
                    "sent": "No, this cannot be true, because if you learn just like in hypothesis testing for example, so the information gained images, just log likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "But when you learn you already know what's going to happen and actually gain less than that.",
                    "label": 0
                },
                {
                    "sent": "But this type of treating is actually useful because this really allows me to put these two things on the same footing.",
                    "label": 0
                },
                {
                    "sent": "So while this reward is really accumulated and you know this is this, not it's not.",
                    "label": 0
                },
                {
                    "sent": "It should be really accumulating because.",
                    "label": 0
                },
                {
                    "sent": "You need some sort of discounting to make those things work, but here is without a disco visit.",
                    "label": 0
                },
                {
                    "sent": "This car, never mind the discount is actually telling you that the road in the next step is not going to be exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Now in your planning as rewards right now, here I have no discounting.",
                    "label": 0
                },
                {
                    "sent": "And this is something which bothered me for a long time.",
                    "label": 0
                },
                {
                    "sent": "I mean, how do you really discount information?",
                    "label": 0
                },
                {
                    "sent": "But but you, all of you know enough about learning to know that.",
                    "label": 0
                },
                {
                    "sent": "The label now and the label 10 steps on now no, I know I'm not don't have the same value to you and it depends of course what you already know about the version stays.",
                    "label": 0
                },
                {
                    "sent": "This has a lot to do with active learning as many other things, I mean.",
                    "label": 0
                },
                {
                    "sent": "So if I give you 2 access and which one to label, you will choose the one which will get the maximum row.",
                    "label": 0
                },
                {
                    "sent": "If you want the maximum information gain or the maximum.",
                    "label": 0
                },
                {
                    "sent": "Further ahead in terms of reduction of the hypothesis space and but you know that eventually when you accumulate those informations that you learn through learning is never expensive, it grows.",
                    "label": 0
                },
                {
                    "sent": "Either logarithmically in the finite dimensional case or it can grow like a sub linear function.",
                    "label": 0
                },
                {
                    "sent": "But the fact that it's not growing extensively is really very important.",
                    "label": 0
                },
                {
                    "sent": "This is a fundamental aspect about learning for all of us.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have these two type of Bellman equations, but for now I just want to play the name game which is solving them together and the major things to think about it.",
                    "label": 0
                },
                {
                    "sent": "OK I want to design my planner on my GPS planner if you want to give me the simplest possible way home.",
                    "label": 0
                },
                {
                    "sent": "Under some constraint on the value, which mean I want to get to Madrid, but in less than 10 hours OK, something like this so, but otherwise I wanted to be the simplest possible with the least committed future in some sense.",
                    "label": 0
                },
                {
                    "sent": "And this commit futures to mean the minimum number of assumptions.",
                    "label": 0
                },
                {
                    "sent": "Compute assumptions that I actually make about the future, which is it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You just want to minimize this information to go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so of course you can do this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mathematics is trivial, but I still show it to you.",
                    "label": 0
                },
                {
                    "sent": "You just combine these two functions with some sort of LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "So this is the information to go, and this is I want to minimize this and under constraint on this and those from some reason avoid about the fact that I didn't put the value of the constraint.",
                    "label": 0
                },
                {
                    "sent": "I hope you know that I can always get rid of it and move all my parameters to the garage multiplier, so forget about this is really constrained optimization, But this has some nice formal properties.",
                    "label": 0
                },
                {
                    "sent": "First of all, it looks very much like entropy and energy, so we'll call it free energy.",
                    "label": 0
                },
                {
                    "sent": "But of course not has almost nothing to do with the thermodynamic free energy, except that it's exactly the same style of optimization.",
                    "label": 0
                },
                {
                    "sent": "This plays the role of an entropy disposed world for energy, and you're going to get similar things.",
                    "label": 0
                },
                {
                    "sent": "And of course everything is within a Bellman equation, so I have a recursion the same recursion again for this linear combination of information and cost and.",
                    "label": 0
                },
                {
                    "sent": "It just looks a little longer, but I believe it's even much longer in the Palm DP case, but it's a very similar structure.",
                    "label": 0
                },
                {
                    "sent": "Even from this Bellman equation alone, you already get some interesting properties in quantities which are not entirely trivial.",
                    "label": 0
                },
                {
                    "sent": "For example, there is a connection.",
                    "label": 0
                },
                {
                    "sent": "In this equilibrium flow of information with the environment between the capacity of your perception, the capacity of your behavior and the cost and its link.",
                    "label": 0
                },
                {
                    "sent": "So these three quantities sum to 0.",
                    "label": 0
                },
                {
                    "sent": "There's something nicer for some.",
                    "label": 0
                },
                {
                    "sent": "Some rule.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is without discounting of information yet and yes.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that these two terms are playing the role of energy and entropy.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I'm just making the analogy and just.",
                    "label": 0
                },
                {
                    "sent": "See that they're both energy and the fluctuations which actually generated in the entropy is still missing and would come in from the finite sample size considerations.",
                    "label": 0
                },
                {
                    "sent": "I don't think that entropy is a finite sample size affect.",
                    "label": 0
                },
                {
                    "sent": "Entropy is inextensive quantity and find a finite sized are corrections to the entropy, their their succession dissipation.",
                    "label": 0
                },
                {
                    "sent": "Also other things which are not including not included in this formulation fluctuations trigger essentially the magnitude of the entropy and the value of information in the value of.",
                    "label": 1
                },
                {
                    "sent": "Of the value and the information seems to be both target functions in this community, so the both these two quantities.",
                    "label": 0
                },
                {
                    "sent": "The way I formulated here.",
                    "label": 0
                },
                {
                    "sent": "Just for you, I mean, these are extensive quantities.",
                    "label": 0
                },
                {
                    "sent": "They grow linearly with my time and they ignore the fluctuations.",
                    "label": 0
                },
                {
                    "sent": "You're absolutely right.",
                    "label": 0
                },
                {
                    "sent": "If I really want to be serious, I have to talk about learning.",
                    "label": 0
                },
                {
                    "sent": "Which is the deviation from this equilibrium in some sense.",
                    "label": 0
                },
                {
                    "sent": "I changed my model.",
                    "label": 0
                },
                {
                    "sent": "This is a much more interesting question and it's going to.",
                    "label": 0
                },
                {
                    "sent": "It has a lot of similarities with the fluctuation dissipation theorem.",
                    "label": 0
                },
                {
                    "sent": "Yes, I get into production and I get in production.",
                    "label": 0
                },
                {
                    "sent": "I get other things, but it's not this theory and this is just for the physicist in OK. Now this is very simple.",
                    "label": 0
                },
                {
                    "sent": "I mean this is what I want to do here is to look at this as some sort of information.",
                    "label": 0
                },
                {
                    "sent": "Of course it's an information theoretic quantity, but it's also as of course most of you notice already.",
                    "label": 1
                },
                {
                    "sent": "It's also a regular regularizer, pacbase bound on my learning.",
                    "label": 0
                },
                {
                    "sent": "And so that's so the technical thing is that I can minimize this very easily, of course, because of the fact that my information gain is a long linear function of my policy, and the other thing that I want to optimize, I'm going to get a nontrivial solution to this equation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you just forget about some of these details.",
                    "label": 0
                },
                {
                    "sent": "This is just rewriting it in a way which looks more familiar to physicists and some sort of a gift distribution.",
                    "label": 0
                },
                {
                    "sent": "But eventually the answer is here you can optimize this subject to the assumption of Bellman optimality.",
                    "label": 0
                },
                {
                    "sent": "Recursively in exactly the same way that you do for Internet programming in general.",
                    "label": 0
                },
                {
                    "sent": "You optimize it in a backward manner, and of course we have to do this forward, backward in order to converge eventually to unique solution and you can prove also in a minute a global convergence with algorithm for any finite better.",
                    "label": 0
                },
                {
                    "sent": "Remember, better was a LaGrange multiplier associated with the value, so it's very much like the the temperature in thermal dynamics and what you get is that the optimal policy is soft, soft, Max type of of policies.",
                    "label": 0
                },
                {
                    "sent": "Stochastic in general, until as long as better is finite and the optimal policy is simply exponential in this free energy.",
                    "label": 0
                },
                {
                    "sent": "But in a sense it's just a slight generalization of most most people do anyway when they do softmax policies, which is just exponential in the queue.",
                    "label": 0
                },
                {
                    "sent": "In the next, this is the simplest, simplest kind of softmax.",
                    "label": 0
                },
                {
                    "sent": "Here you actually get the information.",
                    "label": 0
                },
                {
                    "sent": "Term also affects the fuzziness of your solution.",
                    "label": 0
                },
                {
                    "sent": "This, by the way, this these three questions, of course implicit and in order to solve them, you need to iterate them.",
                    "label": 0
                },
                {
                    "sent": "But this is precisely that.",
                    "label": 0
                },
                {
                    "sent": "There are multiple out algorithm in information theory, or we know that this is converging very quickly to the fixed point, and there are no it's convex and there's no problem with local Optima, and so OK, so you can solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's reiterating two equations.",
                    "label": 0
                },
                {
                    "sent": "The optimal policy which replaces the greedy policy search in the in the in the usual policy iteration and the free energy, which is solved by the this Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Ordinary programming in a backward flip on the state.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the only thing to be said about it, but the solution is actually obvious.",
                    "label": 0
                },
                {
                    "sent": "And So what I get here is some sort of a value of information type of theorem.",
                    "label": 0
                },
                {
                    "sent": "It tells me that.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm solving a simple problem like this maze.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple maze.",
                    "label": 0
                },
                {
                    "sent": "Let's say that every action can take me in one of eight directions.",
                    "label": 0
                },
                {
                    "sent": "When you go to.",
                    "label": 0
                },
                {
                    "sent": "Better Infinity means that you put all your only the value is important information is cost less.",
                    "label": 0
                },
                {
                    "sent": "Then of course you go back to the standard RL and when you decrease better or go to higher higher stochastic city you get usually this type of upward concave curve which is very much like the rate distortion function but in reverse which tells you how many what is the minimum number of bits that you need to assume in your trajectory in order to get a certain value.",
                    "label": 0
                },
                {
                    "sent": "Only points below this curve achievable in principle.",
                    "label": 0
                },
                {
                    "sent": "The curve is the optimal limit and be above the curve is unachievable, so this is a classical value of information curve as far as I'm concerned because what is the minimal value at the given the maximum value to give information or the minimal information at the given value.",
                    "label": 0
                },
                {
                    "sent": "Of course the two things are completely dual to each other and you can prove monotonicity here, as long as you're playing inside.",
                    "label": 0
                },
                {
                    "sent": "Inside the training there's no issue of learning yet.",
                    "label": 0
                },
                {
                    "sent": "But it's already nice about this.",
                    "label": 0
                },
                {
                    "sent": "That festival in many problems you can really compress the complexity of the fallen significantly by moving from, let's say, in this case.",
                    "label": 0
                },
                {
                    "sent": "It is very simple case.",
                    "label": 0
                },
                {
                    "sent": "You can compress by a factor of three or four day.",
                    "label": 0
                },
                {
                    "sent": "The number of bits almost without losing any value.",
                    "label": 0
                },
                {
                    "sent": "But beyond that I mean.",
                    "label": 0
                },
                {
                    "sent": "So here I need a lot less decisions here then I need there, but the value decreases only slightly.",
                    "label": 0
                },
                {
                    "sent": "Of course we can make it more more precise and I also get that.",
                    "label": 0
                },
                {
                    "sent": "The algorithm somehow identifies the points on my trajectory.",
                    "label": 0
                },
                {
                    "sent": "Let's say these two 3 two corners here, where my decisions have to be much more simplistic, much more precise.",
                    "label": 0
                },
                {
                    "sent": "So these are really the points where you have to worry about knowing your future precisely and the other places are not as important and this type of segmentation of the future into chunks between which you have to be very precise, is actually a very interesting problem on its own.",
                    "label": 0
                },
                {
                    "sent": "Was trying to say something about it earlier this morning, another workshop.",
                    "label": 0
                },
                {
                    "sent": "This is essentially what is eventually going to give me some sort of hierarchical description of my plans, but forget about.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now, so here is just another simpler example.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you have a maze like this.",
                    "label": 0
                },
                {
                    "sent": "There is a wide Rd here and narrow short shortcut here.",
                    "label": 0
                },
                {
                    "sent": "And of course, in order to find this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scott, you actually need to be more specific about the information, let's say so in low temperature and high temperature you your agents will do something like this.",
                    "label": 0
                },
                {
                    "sent": "I mean the chances of them getting into this narrow Rd where is very very small.",
                    "label": 0
                },
                {
                    "sent": "But when you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you increase the better, let's say here.",
                    "label": 0
                },
                {
                    "sent": "If this is very high, better the chances of them to find the shortcut is very very high, and notice by the way, that this is going to give you some.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The phase transition in behavior, so the information curve would go stickley's until you find more or less the shortcut and then it more or less has saturates.",
                    "label": 0
                },
                {
                    "sent": "So it's really nice way again of exploring the structure of your problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just another slightly more interesting example.",
                    "label": 0
                },
                {
                    "sent": "Now I really want.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to the to the issue at hand, which is really regularization and model order selection.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we could prove this.",
                    "label": 0
                },
                {
                    "sent": "This work was done, but my way with Ohatchee, Miranda, Newton, Rubin an what I think all of you know by now so.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we can easily prove essentially just by repeating the classical convergence proof of of the user lorelle, that this type of transformation on my free energy has a unique solution.",
                    "label": 0
                },
                {
                    "sent": "And here we actually ignore the information term that she was worried about, doesn't it doesn't help anything in there in the MVP case.",
                    "label": 0
                },
                {
                    "sent": "So for the MDP you have a global convergence of this algorithm, finite beta by the way for infinite better than the usual case.",
                    "label": 0
                },
                {
                    "sent": "This requires some sort of tricky limit, but I'm actually happier.",
                    "label": 0
                },
                {
                    "sent": "It seems that with finite better, you don't only gain some sort of saving information, you also the proof of convergence is much more.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bust.",
                    "label": 0
                },
                {
                    "sent": "Cause if we find out better things are convex unlike the case of deterministic policy where you may have two equal equally likely solutions which are not the same.",
                    "label": 0
                },
                {
                    "sent": "So now we actually take another look into this information gain.",
                    "label": 0
                },
                {
                    "sent": "And of course it doesn't take too much for anyone here to realize that this information to go term is nothing backed.",
                    "label": 0
                },
                {
                    "sent": "the PAC Bayes bound in a sense.",
                    "label": 0
                },
                {
                    "sent": "I mean the posterior to prior KL, which is the complexity of my pack base, is precisely the time that I added and wanted to minimize in order to achieve this.",
                    "label": 0
                },
                {
                    "sent": "If you want cognitive goal of minimizing information about the future and of course.",
                    "label": 0
                },
                {
                    "sent": "Now you can play the same game again and see OK. We have this really celebrated theorem of McAllister I.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just plug in my information to go.",
                    "label": 0
                },
                {
                    "sent": "The probability of all the future conditioned by the prior and I put the prior to be again this stupid prior of independence independent action and and states.",
                    "label": 0
                },
                {
                    "sent": "But you can of course do much more sophisticated priors here.",
                    "label": 0
                },
                {
                    "sent": "And of course the complexity of the class is directly related to the description of your report is given the problem.",
                    "label": 0
                },
                {
                    "sent": "This is my intuition of the PAC Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's nothing more than the classical outcome razor result.",
                    "label": 0
                },
                {
                    "sent": "If you want just stated in terms of information measures so.",
                    "label": 0
                },
                {
                    "sent": "This is really bit the description of airport's class, and in this case it's an extensive quantity.",
                    "label": 0
                },
                {
                    "sent": "It's only it's going with T, so you have to talk about bound rate or information rate and so on.",
                    "label": 0
                },
                {
                    "sent": "But now you can prove the following theorem.",
                    "label": 0
                },
                {
                    "sent": "Now imagine that there's actually learning, so let's at least in the simplest possible way.",
                    "label": 0
                },
                {
                    "sent": "We took here something like episodes in this game.",
                    "label": 0
                },
                {
                    "sent": "In this maze finite episodes, which give you some sort of fluctuations over the reward, so you don't really know the rewards.",
                    "label": 0
                },
                {
                    "sent": "And then you compare your essentially the expected reward to the priority board and all things here, and then the whole thing boils down to learning theoretic setting which really makes that gives this value of information curve.",
                    "label": 0
                },
                {
                    "sent": "This is the in training curve and this is the outer training curve out.",
                    "label": 0
                },
                {
                    "sent": "So here I am learning my reward function.",
                    "label": 0
                },
                {
                    "sent": "I can do the same for the model itself is a little more tricky based on samples or episodes.",
                    "label": 0
                },
                {
                    "sent": "So it's the simplest possible scenario that we could think of.",
                    "label": 0
                },
                {
                    "sent": "In love learning in terms of.",
                    "label": 0
                },
                {
                    "sent": "Episodes through through a maze level through an MVP and of course what you always get that with any finite number of samples.",
                    "label": 0
                },
                {
                    "sent": "There's an optimum.",
                    "label": 0
                },
                {
                    "sent": "Which really optimizes this bond.",
                    "label": 0
                },
                {
                    "sent": "So optimizing this by simply minimizing the mutual information subject to something else, which is the expected reward.",
                    "label": 0
                },
                {
                    "sent": "And if you try to get beyond this, you see that your expected rewards going to decrease above a certain number of bits.",
                    "label": 0
                },
                {
                    "sent": "So essentially this is.",
                    "label": 0
                },
                {
                    "sent": "This is nice because we started with some attempt to understand cognition and understand information gathering.",
                    "label": 0
                },
                {
                    "sent": "And we ended up with a nice application of the PAC Bayes bound to and you see that the bound is not just abound, it's part of the story is an essential component of learning.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The optimal tradeoff between future and value is obtained at some finite better, which is directly related to the optimization of the complexity bound.",
                    "label": 0
                },
                {
                    "sent": "It's not precisely model order selection, but of course now I can play with different models and plug plug exactly in the same the same type of bound.",
                    "label": 0
                },
                {
                    "sent": "I know when I get these two type of stories parallel to each other, you optimize the complexity and you.",
                    "label": 0
                },
                {
                    "sent": "Make your.",
                    "label": 0
                },
                {
                    "sent": "Information gathering information accumulation information gain story.",
                    "label": 0
                },
                {
                    "sent": "More involved.",
                    "label": 0
                },
                {
                    "sent": "Depending on the structure of your model.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'm more or less out of time, but actually way beyond my time but.",
                    "label": 0
                },
                {
                    "sent": "And I just want to hint at what we do now, with with Palm DP's which is a lot more interesting of course.",
                    "label": 0
                },
                {
                    "sent": "So first of all we can.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that you can write.",
                    "label": 0
                },
                {
                    "sent": "I have how much 5 minutes, 10 minutes, 10 minutes we actually don't want to say too much about it, but I'll say a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "Revealing some of the things in the oven, specially when you take it online afterwards.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we know how to do and this is work of another student who folks mainly.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we know how to rewrite the whole thing in apondi setting and and and essentially this information gathering.",
                    "label": 0
                },
                {
                    "sent": "They can be done also sequentially, so this is also not very surprising, so you know that when you have exact sufficient statistics, let's say they can always be written in an online way.",
                    "label": 0
                },
                {
                    "sent": "I mean additive way, just like in the airport.",
                    "label": 0
                },
                {
                    "sent": "This testing scenario you can always add one quantity which accumulates the information in every example and forget about the rest just like just like you do with averages or with variances and other things.",
                    "label": 0
                },
                {
                    "sent": "In the Gaussian case, let's say or in there.",
                    "label": 0
                },
                {
                    "sent": "For this testing case you simply add the log likelihood ratio of the current example, forget about the rest.",
                    "label": 0
                },
                {
                    "sent": "What we know how to do now is to generalize it to the cases where you don't have exact sufficient statistics and.",
                    "label": 0
                },
                {
                    "sent": "And again, so you have those information capacities as in some sense as controlling of the complexity, but here.",
                    "label": 0
                },
                {
                    "sent": "I'm giving up on being exact sufficient statistic, but I don't give up on having an exponential form, which means again that I have additive online ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "Statistics approximate sophistic calculation.",
                    "label": 0
                },
                {
                    "sent": "So believe me that there's something like a dual Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "It's not permanent question because I'm not averaging over the future.",
                    "label": 0
                },
                {
                    "sent": "I'm summing the past so there's no average.",
                    "label": 0
                },
                {
                    "sent": "But when you do it enough, you have you gain those accumulated statistics.",
                    "label": 0
                },
                {
                    "sent": "So this is what I called.",
                    "label": 0
                },
                {
                    "sent": "The online was sequential information bottleneck, but you don't have to buy this particular graph an.",
                    "label": 0
                },
                {
                    "sent": "No, there's actually 1 interesting case, which is just a cennetig condition that we already know words.",
                    "label": 0
                },
                {
                    "sent": "Of course in the case of a Gaussian environment.",
                    "label": 0
                },
                {
                    "sent": "And linear systems.",
                    "label": 0
                },
                {
                    "sent": "You get back the common the common filter formulation, or if you want an LPR formulation, depends how you think of control theory, so there's also some sanity condition.",
                    "label": 0
                },
                {
                    "sent": "Everything boils down again to the Gaussian environment.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian bottleneck is solvable exactly, and and then the linear system case you get back Coleman essentially just as a simple reduction of this, so this is nice, and I could probably tell you more about it.",
                    "label": 0
                },
                {
                    "sent": "What happened beyond the Gaussian case?",
                    "label": 0
                },
                {
                    "sent": "And beyond the Gaussian case, as I said already earlier this week, we can extend it using kernels.",
                    "label": 0
                },
                {
                    "sent": "And and anything that, after after embedded embedding with the kernel, can become more more Gaussian can be attracted in this in this in this story.",
                    "label": 0
                },
                {
                    "sent": "So essentially here we really have some plan that I hope next year or something I'd be able to really completely tell you, but so the kernel I be something that is actually a very trivial thing.",
                    "label": 0
                },
                {
                    "sent": "You just embed the past and the future of environment using some sort of nonlinear function, which makes it more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Test this in various ways anyway, so this is essentially all I wanted to say here and then.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm usually surprised to finish in time so.",
                    "label": 0
                },
                {
                    "sent": "Essentially what we've been trying to do here is to put on the same formal footing information theory in control theory, and with with it some sort of close from theory of perception and action, which is a very ambitious goal, but it's actually doable, I think.",
                    "label": 0
                },
                {
                    "sent": "And of course we use for this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so those conclusions are not Franklin tally conclusions for this talk with.",
                    "label": 0
                },
                {
                    "sent": "We use for this essentially some very simple properties, information information theoretic quantities we ignored.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of cheating that I was doing.",
                    "label": 0
                },
                {
                    "sent": "I ignore the fact that I have learning and played the game, that there is some sort of an equilibrium, which means the systems are fixed, but the information flows are regular and I think we have a more or less complete description of this equilibrium as setting which is, as I said, a constant flow of information and the next step of course is to embed learning into it and we just started to do it with this.",
                    "label": 0
                },
                {
                    "sent": "Belmont Park base bound and see what happens when you deviate from equilibrium and really one of the nicest analogies that you have here is when you think about discounting.",
                    "label": 0
                },
                {
                    "sent": "So I never liked the way you do discounting in our rail, because this exponential discounting is mathematically very convenient, but I needed some more fundamental way of doing it.",
                    "label": 0
                },
                {
                    "sent": "If you think about the information discounting, it becomes much more interesting.",
                    "label": 0
                },
                {
                    "sent": "So again, information discounting is directly related to learning directly related to how much you are willing to pay for the label of this example in two days from now, not now.",
                    "label": 0
                },
                {
                    "sent": "And this is a sub linear function.",
                    "label": 0
                },
                {
                    "sent": "This sub linear immediately if I want the two equations to be consistent with each other.",
                    "label": 0
                },
                {
                    "sent": "These two functions have to go together and the only way I know how to do it now is by changing the notion of time mentale.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if I think about.",
                    "label": 0
                },
                {
                    "sent": "My planning time notice Chronicle time, but this information time.",
                    "label": 0
                },
                {
                    "sent": "The same amount of bits per unit time.",
                    "label": 0
                },
                {
                    "sent": "Then everything makes perfect sense, so I know 10 bits about the next hour and maybe 10 another 10 days about the next day and another 10 days about the next month and so on.",
                    "label": 0
                },
                {
                    "sent": "It's not that bad, but maybe so.",
                    "label": 0
                },
                {
                    "sent": "So essentially I should plan in terms of number of decisions I have to make and not in terms of the time that actually passed from state to state.",
                    "label": 0
                },
                {
                    "sent": "Think about your smart tuition stick about the first second.",
                    "label": 0
                },
                {
                    "sent": "It actually links together, so the fact that information usually grows logarithmically logarithmically with the number of a number of samples is directly linked with the exponential exponential discounting of reward.",
                    "label": 0
                },
                {
                    "sent": "So these two are completely consistent with the channel.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't grow logarithmically, which means that we don't have number finite parameter space or something like a nonparametric model, this immediately reflects on known exponential discounting.",
                    "label": 0
                },
                {
                    "sent": "Which is sometimes known as hyperbolic discounting or things like this, which actually turns out to be important in biology as well.",
                    "label": 0
                },
                {
                    "sent": "So this type of crazy modification of time makes perfect sense and is also a way of getting rid of the computational difficulty of Pompey's.",
                    "label": 0
                },
                {
                    "sent": "So this is really to be done.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I'll stop here.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}