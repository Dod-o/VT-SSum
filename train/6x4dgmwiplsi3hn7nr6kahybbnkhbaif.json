{
    "id": "6x4dgmwiplsi3hn7nr6kahybbnkhbaif",
    "title": "Mondrian forests: Efficient random forests for streaming data via Bayesian nonparametrics",
    "info": {
        "author": [
            "Yee Whye Teh, Department of Statistics, University of Oxford"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Information Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_teh_mondrian_forests/",
    "segmentation": [
        [
            "This is joint work with my student Balaji Lakshminarayanan, Dan, Roy and Cambridge was going to Toronto and myself and I'd like to and this is talk about.",
            "I think a very cute little idea that we've had for trying to do supervised learning on online streaming data.",
            "OK, so he."
        ],
        [
            "The background, here's a outline.",
            "Sorry, so I'll start by kind of like motivating the approach that we want to take and then the proposal that we are going to make is what we call modern forest.",
            "I'll tell you what they are later and then it should experiments and conclusions.",
            "OK."
        ],
        [
            "Right, so as I said, well, we're interest."
        ],
        [
            "And in here supervised learning, so we have a data set X with some labels.",
            "Y&Y could be either discrete value, in which case will."
        ],
        [
            "To do classification or where or if it's real value, we're trying to to do regression here, and the goal is of course to predict Y star given X star, which is a test data point."
        ],
        [
            "And as the title of the talk indicated, the thing that we're interested in here."
        ],
        [
            "Is to use use what I call random forest, which is a very large, very popular class of supervised learning methods, which I guess hasn't been mentioned yet.",
            "But it is one of the ones that almost everybody uses.",
            "So if you go to some of the machine learning Kaggle competitions for example, basically everybody uses those.",
            "Right, I think I'm not sure whether everybody knows what random forests are.",
            "I think most people do, but I will go through over the next few slides, but basically they produce state of the art results for lots of real prediction tasks, and they're also very robust, and they're very parallelizable and computationally efficient, which is why people use it so much.",
            "Right so."
        ],
        [
            "What random forests are is that they are basically collections of decision trees, so I first have to tell you what decision trees are, and here's just a little cartoon that is very simple things.",
            "OK, so a decision tree is.",
            "Basically you can think of it as a hierarchical axis aligned binary partition of our input space.",
            "So here's our partition of into the space where the cuts here are basically axis aligned an another way in which you could visualize this partitioning is basically as a tree of.",
            "Of tests that you could do on your input vector in which each test looks at a single entry of your vector, and you say whether it's larger than or smaller than a particular value.",
            "If it's larger good on this site.",
            "If it's smaller, go down that side.",
            "You go down all the way down to the tree, and at the leaves of the tree we basically give a prediction for what class or what.",
            "Response value we should predict on that part of the space, OK?",
            "Right, I'm going to use T2 to indicate basically the tree, which in this case is basically the list of the structure along with the nodes of the tree along with the feature ID's, which is the feature value that we're going to look at in this test plus the location of the splits and then of course we also need to have parameters which describe the conditional distribution of each class at each part of the rectangular block of our space here.",
            "So that's a decision tree and decision trees are very nice.",
            "They're very interpretable, they have very old method.",
            "Of course they don't generalize very well.",
            "You can imagine that this axis aligned cuts are just not very sensible types of decision boundaries."
        ],
        [
            "What people have been doing is to say that instead of using a single decision tree to to make a prediction, what we should do is to average our predictions over IID draws from some distribution over decision trees.",
            "So our predictive probability for why star given exercises some average over M decision trees.",
            "In this case, where assuming that decision trees are independent, draws from some distribution.",
            "OK."
        ],
        [
            "And basically by combining multiple decision trees, this significantly improves performance.",
            "And this is I would like to say a few things here.",
            "So this is a method for doing."
        ],
        [
            "Model combination on sambol learning and this is not a Bayesian model averaging technique.",
            "OK, so basically in Bayesian model averaging what you what you might do is to say that OK, I have some prior over this gentries computer posterior distribution draw samples from the posterior and average over the posterior.",
            "In fact you could actually.",
            "Try that an.",
            "Actually it doesn't work as well as simply averaging over some distribution over decision trees.",
            "And basically you could understand that as basically this is a technique for doing variance reduction rather than bias reduction.",
            "Right, OK, so what's up distributions over decision trees that have people looked at the Sky?"
        ],
        [
            "Two pretty standard techniques.",
            "One is Leo Breiman's Random Forest, which is the original idea, I guess, is where the term came from, and the idea here is that there's kind of two ingredients to introduce randomness in our construction of the decision trees.",
            "The first one is bagging, which is basically we would like to.",
            "Construct 3 sample versions of our datasets with replacement.",
            "So this introduces basically variability in our data set that we are training each decision tree on and the other way in which we introduce variabilities.",
            "Randomly subsample features out of the deep possible features that we could look at at each point.",
            "The Gentry an.",
            "We choose the best split among the subsample features, optimizing overall possible split locations in that sub sampled features.",
            "So that's a brightness, random false, and then there's another version called extremely randomized trees in which rather than sampling, say K features and optimizing over where we're going to cut along each of the key features, we actually sample K pairs of feature IDs and locations, and we optimize over this K pairs.",
            "At each point of the decision tree an.",
            "Also there's no bagging as well, so one we had extreme of this extremely randomized trees, which is maybe you could call it an extremely extremely randomized tree is Erta, where instead of sampling K feature ID location path, we simply sample one of them, in which case there's no decision.",
            "There's no choice that we have to make among this K, feature ID, location pairs.",
            "And another strange thing with this is of course, since we don't have to decide among these K path, we do not even need to look at the labels themselves at all.",
            "An amazingly this thing works quite well.",
            "We'll see some figures later."
        ],
        [
            "And basically OK. Yep, so some advantages of random forests.",
            "They give very good predictive performance.",
            "They also very robust.",
            "They're very easy to train and very fast train and can be trained in parallel an there's kind of no over fitting really.",
            "The.",
            "The problem which we like to address in this talk is actually 1.",
            "One point in which they don't.",
            "Worked very well, which is that it's not easy to kind of generalize random forests construction methods to work on datasets which are streaming and increasing overtime.",
            "So one very naive way in which you could do is to basically take batch offline learning, random forest learning algorithm an periodically retrain this based on all the data that you have so far.",
            "OK, of course that can be very slow and people have proposed online random forest variants.",
            "These two papers here, but the problem with this is that they are both computationally inefficient as well as data inefficient as well.",
            "They don't give very good predictions.",
            "OK, an hour proposal.",
            "Here is what we call modern forest, which is combining ideas of random forests with ideas from basin nonparametric, which is what's called a monitoring process.",
            "And what this leads to is actually a method that can work in online.",
            "Away and actually it operates in both batch and online modes, and the interesting thing is that the online version actually has exactly the same performance.",
            "In fact it gives exactly the same predictions as the offline batch method, but you can actually train this online in a very efficient manner, and it's also data efficient as well."
        ],
        [
            "Right, So what I'm on run processes?"
        ],
        [
            "So, so it's named after Pete Mondrian's paintings.",
            "OK, so this is one of his paintings we obtained from Wikipedia.",
            "Basically looks like axis aligned.",
            "Box of input space and we use this so an.",
            "Yeah, so this is basically a stochastic process over binary hierarchical axis aligned partitions.",
            "OK."
        ],
        [
            "So I think it's probably easiest to describe it in a cartoon.",
            "The generative process for a modern process.",
            "OK, so it has a parameter Lambda which basically governs the complexity of the partitions that you get, and it has another two parameters in or in the case in which you have a D dimensional space, you have a deeper matters which basically describes the rectangle over which we are going to create an access lines hierarchical partition of and the generative process works as follows.",
            "We first draw Delta Epsilon, which is from exponential distribution with a rate given by basically the length of this interval plus the length of that interval, OK. An if this.",
            "If this Delta is greater than Lambda, we're going to stop.",
            "So basically we return this as a draw from our monitoring process without any cuts.",
            "But if it's less than Lambda, then we're going to cut somewhere in this rectangle, and the choice for where we're going to cut it goes as follows.",
            "We're first going to pick a dimension to cut with probability proportional to the.",
            "Basically, the length of the interval corresponding to that dimension.",
            "And then, once we've chosen a dimension, will choose a location uniformly from the interval.",
            "So this defines a cut of our space, and we basically recurse on both sides.",
            "So recurse on the left.",
            "We might get something like that, and on the right we might get something like that.",
            "Of course, we have to reduce the complexity parameter because we've kind of.",
            "Created one note of our tree.",
            "Right, so what's so interesting about this stochastic process where you can show is that it has."
        ],
        [
            "Very nice self consistency property which is the following.",
            "So let's imagine that we simulate.",
            "A tree hierarchical partitioning of this rectangle.",
            "And let's suppose that we have a smaller rectangle within this larger rectangle, OK?"
        ],
        [
            "And you could imagine taking this partition T and restricting it to this smaller rectangle.",
            "So if the previous T."
        ],
        [
            "Is random then this restriction of T to this smaller rectangle is going to be random as well.",
            "You could ask what is the distribution of this partition over this smaller rectangle?",
            "And it turns out that it's also a modern process run on the smaller rectangle.",
            "OK, so this is the nice property of this which we're going to use for online learning.",
            "You can actually flip this around and ask instead of asking what happens under the restriction, you could expand the space out larger, and in fact because of this self consistency."
        ],
        [
            "Operti, you could actually construct a well defined extension of this hierarchical partition to what a hierarchical partition of R square OK. Search that for any finite.",
            "Rectangle of space.",
            "The restriction to that rectangle is exactly the monitoring process that I've just described.",
            "Right, so that's the monitoring process."
        ],
        [
            "So how do we use this to build online decision trees?",
            "Well, basically we already have our hierarchical partitioning of our space.",
            "Um?",
            "What we do is we are going to use this.",
            "Modern processes apply over decision trees conditioned on X, where X is our input vectors where basically the range of the monitoring process is given by basically the range of the data.",
            "Because of this self consistency property, this is equivalent.",
            "You can think of it as equivalent to a prior over trees defined over over RB.",
            "That is in fact independent of X and the only dependence on X that we get is that actually this distribution over decision trees given X is simply the restriction of this independent tree into this.",
            "Space, which is of course whose range is dependent on X."
        ],
        [
            "And how do we use this for online learning?",
            "Basically, as our data set grows as X grows, the range of our data is going to grow and we simply unveil Decision tree on a larger range as our OK as a data set grows.",
            "And the conditional and this unveiling is basically follows from basically a conditional conditional monitoring process.",
            "Right because of this self consistency property, the distribution of the trees in both the offline and the online construction are exactly the same OK and also the order of the data points does not matter.",
            "OK, so he."
        ],
        [
            "There's a little cartoon that kind of just describes what's going on.",
            "OK, so let's suppose that we start off with the datasets with two points, so A&B and of course in a 2 dimensional space.",
            "This is the range of our data given by the grey rectangle, and let's suppose that we've decided that within this Gray rectangle we're going to have a decision tree, which is basically have a horizontal cut at that value.",
            "And let's suppose now that we we observe a new data point, C&C lights up here.",
            "So the first thing we have to do is to extend the range of monitor entry to the larger range.",
            "OK, so we're going to go from this little Gray rectangle to this larger green rectangle.",
            "An in making that extension we have to make now a decision which is that.",
            "If.",
            "OK, before we see see we see that there was a cut here.",
            "OK, so now we have to make a decision.",
            "Was this cut in fact the.",
            "Extend all the way across.",
            "Or maybe there's actually another cut of this space which we haven't seen but actually kind of.",
            "It precedes this.",
            "Cut here, OK?",
            "An unless we can make that decision.",
            "In fact it's very simple.",
            "Distribution, which I'm not going to describe.",
            "But basically, let's imagine that, OK, let's make it.",
            "We made a decision, which is that actually there was a cut above this one.",
            "So basically there was a note of this decision tree above this note here, OK?",
            "And let's see, let's say that we observe a third data point.",
            "And let's say that the data point D is around here.",
            "Then of course it is to the left of this cut, so we kind of.",
            "We know that D is going to appear somewhere in this part of the tree, so we're going to recurse on this part of the tree, and on this part we had a tree that is observed only on this little rectangle.",
            "So the first thing we have to do is to extend that range.",
            "Again, we have to extend the range from here up to here.",
            "And again we have to make a decision.",
            "Was there a cut above this one, or in fact that this cut was the.",
            "Is kind of at the top here of this decision tree.",
            "Let's suppose that we make that decision and again we recurse down to this part of the space.",
            "And again we make a decision.",
            "Let's say that there's another cut here that gives us another node.",
            "With the in here, so that's just a cartoon for what happens during the construction of this conditional modern tree.",
            "Right so."
        ],
        [
            "A few key differences between modern forests, an existing online random for.",
            "So what I've just described is we're going to use this monitoring process as this distribution over decision trees an for our data set for constructing random files we have to draw multiple ID samples from this distribution, and we can do this multiple times and that gives us our modern forest.",
            "OK, so the key differences are the following, so the first one is that the splits do not extend to regions of the space that we have not made any observations we only make.",
            "This place only extend over the part of the space that we do observe.",
            "New splits can be introduced anywhere in the tree.",
            "If you look at the construction here, we started off with this decision tree and then we introduced a split above this, which is quite different from other online decision tree construction algorithms, in which they always construct splits lower down the tree.",
            "OK, and another part of the process of the model that they haven't really described is that we also have a self consistent hierarchical Bayesian prior on the leaf parameters based on the hierarchical Pitman Yor process.",
            "Right anyway, so that's a very simple idea.",
            "So let's see how well does it work on.",
            "On some experiments we looked at a few datasets here of yes.",
            "Yep.",
            "So how do you?",
            "How do you combine all the trees?",
            "Each tree is an independent draw from this modern process, and you do this simple averaging.",
            "You do simple averaging, so it's not Bayesian.",
            "Posterior averaging is actually prior averaging.",
            "If you think about the modern processors apply over decision trees, which is weird, and it does better than posterior averaging.",
            "It's not."
        ],
        [
            "Bayesian."
        ],
        [
            "Always so here's a few datasets with varying dimensionality, number of classes, training data set and and tested.",
            "The set.",
            "Of course, this is just a.",
            "A batch of datasets, right?",
            "So there's no online nature to this, so we what we do is we take each data set and we split into 100 million batches and we show the Model 1 mini batch at a time so that the model is basically seeing an increasing subset of our datasets.",
            "And what we'd like to know is what the computational complexity of tracking this increasing sized datasets, as well as how well does the online learning algorithm do?",
            "And we compare this against the state of the art.",
            "Basically, we have batch offline methods, breiman's random forests, the extremely extremely randomized tree, and only extremely randomized tree, as well as the online random forests of suffering at all, which showed.",
            "Basically.",
            "The two online random Force methods work about the same."
        ],
        [
            "OK, so here's results on one of the datasets.",
            "So.",
            "Here's sorry on the Y axis.",
            "Here is basically the test accuracy of the different methods.",
            "And on the X axis of this left plot, we're basically seeing an increasing fraction of our training data.",
            "While on the right plot is the amount of time that it takes to construct to process that part of the data set.",
            "And modern forest is the blue curve, which is basically.",
            "In among the other various offline random forests methods, OK, basically it works just as well as the state of the offline methods, while the online random forests of Safari.",
            "Actually there's a lot worse.",
            "It's kind of as much lower predictive accuracy.",
            "OK.",
            "If you look at computational costs, it's much more efficient, so this curve now is looking at computational cost in terms of time versus accuracy, and we see that the blue curve is to the left of the of the black and the red curves, which is the.",
            "Extremely randomized trees.",
            "We didn't have the random forest curve up here.",
            "Brightness random forest because it's actually we used scikit learn which is a different.",
            "Implementation in a different language, so it's a bit hard to compare computational time cost.",
            "Yep.",
            "Right, so it's very data efficient.",
            "It's very close to offline methods and significantly outperforms the online methods and is much faster than all the other methods.",
            "At."
        ],
        [
            "And this actually the same self conclusion holds across the different datasets.",
            "So there's another data set which is a handwritten digit recognition datasets.",
            "Again, we see that the blue curve is to the left and basically doing just as well as the as the offline methods.",
            "So I think that Rebecca had a very nice talk about.",
            "Partly talking about kind of regrets of online methods.",
            "So in a sense, this online method has no regret relative to the corresponding offline method.",
            "It's basically exactly the same.",
            "OK, so satellite the same conclusions.",
            "The first data set actually is a somewhat different conclusion in which."
        ],
        [
            "If you look at the modern forest, it actually does a lot worse than the corresponding.",
            "Then some of the.",
            "Offline random forest construction."
        ],
        [
            "Algorithms and the reason for this is that.",
            "Basically this is a data set in which the dimensionality is much higher than has lots of irrelevant features, and because of the way we construct our modern trees, it doesn't ever look at the labels, so it often picks features which are.",
            "Irrelevant as power, the decision tree and of course you don't expect it to work that well and you can compare this against the CRT one.",
            "Predictions and the account like pretty similar.",
            "If you do an initial preprocessing in which you only in which we first decide which are the relevant features, and then we run our various random forest algorithms, we get predictions which are basically again state of the art as well.",
            "So basically this is the.",
            "The main failure mode of the of the random forest method.",
            "OK."
        ],
        [
            "So that's it in can."
        ],
        [
            "Vision is a very simple idea in which we in which we have.",
            "A method for doing online random forests that works in exactly the same way whether it's offline or online, so it's computationally very fast.",
            "It gives very good predictive accuracies.",
            "And it's very simple to do to implement in terms of future.",
            "What would like to look into is basically what happens if we have high dimensional data with lots of irrelevant, irrelevant features.",
            "Can we somehow identify the irrelevant and relevant features in an online manner?",
            "OK.",
            "Anyway, so thank you.",
            "We have archive paper this as a paper on archive as well as source code in Python, so if you'd like to try this, go ahead.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with my student Balaji Lakshminarayanan, Dan, Roy and Cambridge was going to Toronto and myself and I'd like to and this is talk about.",
                    "label": 0
                },
                {
                    "sent": "I think a very cute little idea that we've had for trying to do supervised learning on online streaming data.",
                    "label": 0
                },
                {
                    "sent": "OK, so he.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The background, here's a outline.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so I'll start by kind of like motivating the approach that we want to take and then the proposal that we are going to make is what we call modern forest.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you what they are later and then it should experiments and conclusions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so as I said, well, we're interest.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in here supervised learning, so we have a data set X with some labels.",
                    "label": 0
                },
                {
                    "sent": "Y&Y could be either discrete value, in which case will.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do classification or where or if it's real value, we're trying to to do regression here, and the goal is of course to predict Y star given X star, which is a test data point.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as the title of the talk indicated, the thing that we're interested in here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to use use what I call random forest, which is a very large, very popular class of supervised learning methods, which I guess hasn't been mentioned yet.",
                    "label": 1
                },
                {
                    "sent": "But it is one of the ones that almost everybody uses.",
                    "label": 0
                },
                {
                    "sent": "So if you go to some of the machine learning Kaggle competitions for example, basically everybody uses those.",
                    "label": 0
                },
                {
                    "sent": "Right, I think I'm not sure whether everybody knows what random forests are.",
                    "label": 0
                },
                {
                    "sent": "I think most people do, but I will go through over the next few slides, but basically they produce state of the art results for lots of real prediction tasks, and they're also very robust, and they're very parallelizable and computationally efficient, which is why people use it so much.",
                    "label": 1
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What random forests are is that they are basically collections of decision trees, so I first have to tell you what decision trees are, and here's just a little cartoon that is very simple things.",
                    "label": 0
                },
                {
                    "sent": "OK, so a decision tree is.",
                    "label": 0
                },
                {
                    "sent": "Basically you can think of it as a hierarchical axis aligned binary partition of our input space.",
                    "label": 0
                },
                {
                    "sent": "So here's our partition of into the space where the cuts here are basically axis aligned an another way in which you could visualize this partitioning is basically as a tree of.",
                    "label": 0
                },
                {
                    "sent": "Of tests that you could do on your input vector in which each test looks at a single entry of your vector, and you say whether it's larger than or smaller than a particular value.",
                    "label": 0
                },
                {
                    "sent": "If it's larger good on this site.",
                    "label": 0
                },
                {
                    "sent": "If it's smaller, go down that side.",
                    "label": 0
                },
                {
                    "sent": "You go down all the way down to the tree, and at the leaves of the tree we basically give a prediction for what class or what.",
                    "label": 0
                },
                {
                    "sent": "Response value we should predict on that part of the space, OK?",
                    "label": 0
                },
                {
                    "sent": "Right, I'm going to use T2 to indicate basically the tree, which in this case is basically the list of the structure along with the nodes of the tree along with the feature ID's, which is the feature value that we're going to look at in this test plus the location of the splits and then of course we also need to have parameters which describe the conditional distribution of each class at each part of the rectangular block of our space here.",
                    "label": 0
                },
                {
                    "sent": "So that's a decision tree and decision trees are very nice.",
                    "label": 0
                },
                {
                    "sent": "They're very interpretable, they have very old method.",
                    "label": 0
                },
                {
                    "sent": "Of course they don't generalize very well.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that this axis aligned cuts are just not very sensible types of decision boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What people have been doing is to say that instead of using a single decision tree to to make a prediction, what we should do is to average our predictions over IID draws from some distribution over decision trees.",
                    "label": 0
                },
                {
                    "sent": "So our predictive probability for why star given exercises some average over M decision trees.",
                    "label": 0
                },
                {
                    "sent": "In this case, where assuming that decision trees are independent, draws from some distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically by combining multiple decision trees, this significantly improves performance.",
                    "label": 0
                },
                {
                    "sent": "And this is I would like to say a few things here.",
                    "label": 0
                },
                {
                    "sent": "So this is a method for doing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model combination on sambol learning and this is not a Bayesian model averaging technique.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically in Bayesian model averaging what you what you might do is to say that OK, I have some prior over this gentries computer posterior distribution draw samples from the posterior and average over the posterior.",
                    "label": 0
                },
                {
                    "sent": "In fact you could actually.",
                    "label": 0
                },
                {
                    "sent": "Try that an.",
                    "label": 0
                },
                {
                    "sent": "Actually it doesn't work as well as simply averaging over some distribution over decision trees.",
                    "label": 0
                },
                {
                    "sent": "And basically you could understand that as basically this is a technique for doing variance reduction rather than bias reduction.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so what's up distributions over decision trees that have people looked at the Sky?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two pretty standard techniques.",
                    "label": 0
                },
                {
                    "sent": "One is Leo Breiman's Random Forest, which is the original idea, I guess, is where the term came from, and the idea here is that there's kind of two ingredients to introduce randomness in our construction of the decision trees.",
                    "label": 0
                },
                {
                    "sent": "The first one is bagging, which is basically we would like to.",
                    "label": 0
                },
                {
                    "sent": "Construct 3 sample versions of our datasets with replacement.",
                    "label": 0
                },
                {
                    "sent": "So this introduces basically variability in our data set that we are training each decision tree on and the other way in which we introduce variabilities.",
                    "label": 0
                },
                {
                    "sent": "Randomly subsample features out of the deep possible features that we could look at at each point.",
                    "label": 0
                },
                {
                    "sent": "The Gentry an.",
                    "label": 0
                },
                {
                    "sent": "We choose the best split among the subsample features, optimizing overall possible split locations in that sub sampled features.",
                    "label": 1
                },
                {
                    "sent": "So that's a brightness, random false, and then there's another version called extremely randomized trees in which rather than sampling, say K features and optimizing over where we're going to cut along each of the key features, we actually sample K pairs of feature IDs and locations, and we optimize over this K pairs.",
                    "label": 0
                },
                {
                    "sent": "At each point of the decision tree an.",
                    "label": 0
                },
                {
                    "sent": "Also there's no bagging as well, so one we had extreme of this extremely randomized trees, which is maybe you could call it an extremely extremely randomized tree is Erta, where instead of sampling K feature ID location path, we simply sample one of them, in which case there's no decision.",
                    "label": 0
                },
                {
                    "sent": "There's no choice that we have to make among this K, feature ID, location pairs.",
                    "label": 0
                },
                {
                    "sent": "And another strange thing with this is of course, since we don't have to decide among these K path, we do not even need to look at the labels themselves at all.",
                    "label": 0
                },
                {
                    "sent": "An amazingly this thing works quite well.",
                    "label": 0
                },
                {
                    "sent": "We'll see some figures later.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically OK. Yep, so some advantages of random forests.",
                    "label": 1
                },
                {
                    "sent": "They give very good predictive performance.",
                    "label": 0
                },
                {
                    "sent": "They also very robust.",
                    "label": 0
                },
                {
                    "sent": "They're very easy to train and very fast train and can be trained in parallel an there's kind of no over fitting really.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The problem which we like to address in this talk is actually 1.",
                    "label": 0
                },
                {
                    "sent": "One point in which they don't.",
                    "label": 0
                },
                {
                    "sent": "Worked very well, which is that it's not easy to kind of generalize random forests construction methods to work on datasets which are streaming and increasing overtime.",
                    "label": 0
                },
                {
                    "sent": "So one very naive way in which you could do is to basically take batch offline learning, random forest learning algorithm an periodically retrain this based on all the data that you have so far.",
                    "label": 0
                },
                {
                    "sent": "OK, of course that can be very slow and people have proposed online random forest variants.",
                    "label": 0
                },
                {
                    "sent": "These two papers here, but the problem with this is that they are both computationally inefficient as well as data inefficient as well.",
                    "label": 0
                },
                {
                    "sent": "They don't give very good predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, an hour proposal.",
                    "label": 0
                },
                {
                    "sent": "Here is what we call modern forest, which is combining ideas of random forests with ideas from basin nonparametric, which is what's called a monitoring process.",
                    "label": 0
                },
                {
                    "sent": "And what this leads to is actually a method that can work in online.",
                    "label": 0
                },
                {
                    "sent": "Away and actually it operates in both batch and online modes, and the interesting thing is that the online version actually has exactly the same performance.",
                    "label": 0
                },
                {
                    "sent": "In fact it gives exactly the same predictions as the offline batch method, but you can actually train this online in a very efficient manner, and it's also data efficient as well.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, So what I'm on run processes?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so it's named after Pete Mondrian's paintings.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one of his paintings we obtained from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Basically looks like axis aligned.",
                    "label": 0
                },
                {
                    "sent": "Box of input space and we use this so an.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is basically a stochastic process over binary hierarchical axis aligned partitions.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think it's probably easiest to describe it in a cartoon.",
                    "label": 0
                },
                {
                    "sent": "The generative process for a modern process.",
                    "label": 1
                },
                {
                    "sent": "OK, so it has a parameter Lambda which basically governs the complexity of the partitions that you get, and it has another two parameters in or in the case in which you have a D dimensional space, you have a deeper matters which basically describes the rectangle over which we are going to create an access lines hierarchical partition of and the generative process works as follows.",
                    "label": 0
                },
                {
                    "sent": "We first draw Delta Epsilon, which is from exponential distribution with a rate given by basically the length of this interval plus the length of that interval, OK. An if this.",
                    "label": 0
                },
                {
                    "sent": "If this Delta is greater than Lambda, we're going to stop.",
                    "label": 0
                },
                {
                    "sent": "So basically we return this as a draw from our monitoring process without any cuts.",
                    "label": 0
                },
                {
                    "sent": "But if it's less than Lambda, then we're going to cut somewhere in this rectangle, and the choice for where we're going to cut it goes as follows.",
                    "label": 0
                },
                {
                    "sent": "We're first going to pick a dimension to cut with probability proportional to the.",
                    "label": 0
                },
                {
                    "sent": "Basically, the length of the interval corresponding to that dimension.",
                    "label": 0
                },
                {
                    "sent": "And then, once we've chosen a dimension, will choose a location uniformly from the interval.",
                    "label": 1
                },
                {
                    "sent": "So this defines a cut of our space, and we basically recurse on both sides.",
                    "label": 1
                },
                {
                    "sent": "So recurse on the left.",
                    "label": 0
                },
                {
                    "sent": "We might get something like that, and on the right we might get something like that.",
                    "label": 0
                },
                {
                    "sent": "Of course, we have to reduce the complexity parameter because we've kind of.",
                    "label": 0
                },
                {
                    "sent": "Created one note of our tree.",
                    "label": 0
                },
                {
                    "sent": "Right, so what's so interesting about this stochastic process where you can show is that it has.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very nice self consistency property which is the following.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine that we simulate.",
                    "label": 0
                },
                {
                    "sent": "A tree hierarchical partitioning of this rectangle.",
                    "label": 0
                },
                {
                    "sent": "And let's suppose that we have a smaller rectangle within this larger rectangle, OK?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you could imagine taking this partition T and restricting it to this smaller rectangle.",
                    "label": 0
                },
                {
                    "sent": "So if the previous T.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is random then this restriction of T to this smaller rectangle is going to be random as well.",
                    "label": 1
                },
                {
                    "sent": "You could ask what is the distribution of this partition over this smaller rectangle?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's also a modern process run on the smaller rectangle.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the nice property of this which we're going to use for online learning.",
                    "label": 0
                },
                {
                    "sent": "You can actually flip this around and ask instead of asking what happens under the restriction, you could expand the space out larger, and in fact because of this self consistency.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operti, you could actually construct a well defined extension of this hierarchical partition to what a hierarchical partition of R square OK. Search that for any finite.",
                    "label": 0
                },
                {
                    "sent": "Rectangle of space.",
                    "label": 0
                },
                {
                    "sent": "The restriction to that rectangle is exactly the monitoring process that I've just described.",
                    "label": 1
                },
                {
                    "sent": "Right, so that's the monitoring process.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we use this to build online decision trees?",
                    "label": 0
                },
                {
                    "sent": "Well, basically we already have our hierarchical partitioning of our space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What we do is we are going to use this.",
                    "label": 0
                },
                {
                    "sent": "Modern processes apply over decision trees conditioned on X, where X is our input vectors where basically the range of the monitoring process is given by basically the range of the data.",
                    "label": 1
                },
                {
                    "sent": "Because of this self consistency property, this is equivalent.",
                    "label": 1
                },
                {
                    "sent": "You can think of it as equivalent to a prior over trees defined over over RB.",
                    "label": 0
                },
                {
                    "sent": "That is in fact independent of X and the only dependence on X that we get is that actually this distribution over decision trees given X is simply the restriction of this independent tree into this.",
                    "label": 0
                },
                {
                    "sent": "Space, which is of course whose range is dependent on X.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how do we use this for online learning?",
                    "label": 0
                },
                {
                    "sent": "Basically, as our data set grows as X grows, the range of our data is going to grow and we simply unveil Decision tree on a larger range as our OK as a data set grows.",
                    "label": 0
                },
                {
                    "sent": "And the conditional and this unveiling is basically follows from basically a conditional conditional monitoring process.",
                    "label": 0
                },
                {
                    "sent": "Right because of this self consistency property, the distribution of the trees in both the offline and the online construction are exactly the same OK and also the order of the data points does not matter.",
                    "label": 0
                },
                {
                    "sent": "OK, so he.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a little cartoon that kind of just describes what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's suppose that we start off with the datasets with two points, so A&B and of course in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is the range of our data given by the grey rectangle, and let's suppose that we've decided that within this Gray rectangle we're going to have a decision tree, which is basically have a horizontal cut at that value.",
                    "label": 0
                },
                {
                    "sent": "And let's suppose now that we we observe a new data point, C&C lights up here.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we have to do is to extend the range of monitor entry to the larger range.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to go from this little Gray rectangle to this larger green rectangle.",
                    "label": 0
                },
                {
                    "sent": "An in making that extension we have to make now a decision which is that.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "OK, before we see see we see that there was a cut here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have to make a decision.",
                    "label": 0
                },
                {
                    "sent": "Was this cut in fact the.",
                    "label": 0
                },
                {
                    "sent": "Extend all the way across.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there's actually another cut of this space which we haven't seen but actually kind of.",
                    "label": 0
                },
                {
                    "sent": "It precedes this.",
                    "label": 0
                },
                {
                    "sent": "Cut here, OK?",
                    "label": 0
                },
                {
                    "sent": "An unless we can make that decision.",
                    "label": 0
                },
                {
                    "sent": "In fact it's very simple.",
                    "label": 0
                },
                {
                    "sent": "Distribution, which I'm not going to describe.",
                    "label": 0
                },
                {
                    "sent": "But basically, let's imagine that, OK, let's make it.",
                    "label": 0
                },
                {
                    "sent": "We made a decision, which is that actually there was a cut above this one.",
                    "label": 0
                },
                {
                    "sent": "So basically there was a note of this decision tree above this note here, OK?",
                    "label": 0
                },
                {
                    "sent": "And let's see, let's say that we observe a third data point.",
                    "label": 0
                },
                {
                    "sent": "And let's say that the data point D is around here.",
                    "label": 1
                },
                {
                    "sent": "Then of course it is to the left of this cut, so we kind of.",
                    "label": 0
                },
                {
                    "sent": "We know that D is going to appear somewhere in this part of the tree, so we're going to recurse on this part of the tree, and on this part we had a tree that is observed only on this little rectangle.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we have to do is to extend that range.",
                    "label": 0
                },
                {
                    "sent": "Again, we have to extend the range from here up to here.",
                    "label": 0
                },
                {
                    "sent": "And again we have to make a decision.",
                    "label": 0
                },
                {
                    "sent": "Was there a cut above this one, or in fact that this cut was the.",
                    "label": 0
                },
                {
                    "sent": "Is kind of at the top here of this decision tree.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that we make that decision and again we recurse down to this part of the space.",
                    "label": 0
                },
                {
                    "sent": "And again we make a decision.",
                    "label": 0
                },
                {
                    "sent": "Let's say that there's another cut here that gives us another node.",
                    "label": 0
                },
                {
                    "sent": "With the in here, so that's just a cartoon for what happens during the construction of this conditional modern tree.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few key differences between modern forests, an existing online random for.",
                    "label": 1
                },
                {
                    "sent": "So what I've just described is we're going to use this monitoring process as this distribution over decision trees an for our data set for constructing random files we have to draw multiple ID samples from this distribution, and we can do this multiple times and that gives us our modern forest.",
                    "label": 0
                },
                {
                    "sent": "OK, so the key differences are the following, so the first one is that the splits do not extend to regions of the space that we have not made any observations we only make.",
                    "label": 0
                },
                {
                    "sent": "This place only extend over the part of the space that we do observe.",
                    "label": 0
                },
                {
                    "sent": "New splits can be introduced anywhere in the tree.",
                    "label": 1
                },
                {
                    "sent": "If you look at the construction here, we started off with this decision tree and then we introduced a split above this, which is quite different from other online decision tree construction algorithms, in which they always construct splits lower down the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, and another part of the process of the model that they haven't really described is that we also have a self consistent hierarchical Bayesian prior on the leaf parameters based on the hierarchical Pitman Yor process.",
                    "label": 0
                },
                {
                    "sent": "Right anyway, so that's a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "So let's see how well does it work on.",
                    "label": 0
                },
                {
                    "sent": "On some experiments we looked at a few datasets here of yes.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you combine all the trees?",
                    "label": 0
                },
                {
                    "sent": "Each tree is an independent draw from this modern process, and you do this simple averaging.",
                    "label": 0
                },
                {
                    "sent": "You do simple averaging, so it's not Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Posterior averaging is actually prior averaging.",
                    "label": 0
                },
                {
                    "sent": "If you think about the modern processors apply over decision trees, which is weird, and it does better than posterior averaging.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Always so here's a few datasets with varying dimensionality, number of classes, training data set and and tested.",
                    "label": 1
                },
                {
                    "sent": "The set.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is just a.",
                    "label": 0
                },
                {
                    "sent": "A batch of datasets, right?",
                    "label": 1
                },
                {
                    "sent": "So there's no online nature to this, so we what we do is we take each data set and we split into 100 million batches and we show the Model 1 mini batch at a time so that the model is basically seeing an increasing subset of our datasets.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to know is what the computational complexity of tracking this increasing sized datasets, as well as how well does the online learning algorithm do?",
                    "label": 0
                },
                {
                    "sent": "And we compare this against the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Basically, we have batch offline methods, breiman's random forests, the extremely extremely randomized tree, and only extremely randomized tree, as well as the online random forests of suffering at all, which showed.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "The two online random Force methods work about the same.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's results on one of the datasets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's sorry on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here is basically the test accuracy of the different methods.",
                    "label": 1
                },
                {
                    "sent": "And on the X axis of this left plot, we're basically seeing an increasing fraction of our training data.",
                    "label": 1
                },
                {
                    "sent": "While on the right plot is the amount of time that it takes to construct to process that part of the data set.",
                    "label": 0
                },
                {
                    "sent": "And modern forest is the blue curve, which is basically.",
                    "label": 0
                },
                {
                    "sent": "In among the other various offline random forests methods, OK, basically it works just as well as the state of the offline methods, while the online random forests of Safari.",
                    "label": 0
                },
                {
                    "sent": "Actually there's a lot worse.",
                    "label": 0
                },
                {
                    "sent": "It's kind of as much lower predictive accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you look at computational costs, it's much more efficient, so this curve now is looking at computational cost in terms of time versus accuracy, and we see that the blue curve is to the left of the of the black and the red curves, which is the.",
                    "label": 0
                },
                {
                    "sent": "Extremely randomized trees.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the random forest curve up here.",
                    "label": 0
                },
                {
                    "sent": "Brightness random forest because it's actually we used scikit learn which is a different.",
                    "label": 0
                },
                {
                    "sent": "Implementation in a different language, so it's a bit hard to compare computational time cost.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's very data efficient.",
                    "label": 0
                },
                {
                    "sent": "It's very close to offline methods and significantly outperforms the online methods and is much faster than all the other methods.",
                    "label": 1
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this actually the same self conclusion holds across the different datasets.",
                    "label": 0
                },
                {
                    "sent": "So there's another data set which is a handwritten digit recognition datasets.",
                    "label": 0
                },
                {
                    "sent": "Again, we see that the blue curve is to the left and basically doing just as well as the as the offline methods.",
                    "label": 0
                },
                {
                    "sent": "So I think that Rebecca had a very nice talk about.",
                    "label": 0
                },
                {
                    "sent": "Partly talking about kind of regrets of online methods.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, this online method has no regret relative to the corresponding offline method.",
                    "label": 0
                },
                {
                    "sent": "It's basically exactly the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so satellite the same conclusions.",
                    "label": 0
                },
                {
                    "sent": "The first data set actually is a somewhat different conclusion in which.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the modern forest, it actually does a lot worse than the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Then some of the.",
                    "label": 0
                },
                {
                    "sent": "Offline random forest construction.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms and the reason for this is that.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a data set in which the dimensionality is much higher than has lots of irrelevant features, and because of the way we construct our modern trees, it doesn't ever look at the labels, so it often picks features which are.",
                    "label": 0
                },
                {
                    "sent": "Irrelevant as power, the decision tree and of course you don't expect it to work that well and you can compare this against the CRT one.",
                    "label": 0
                },
                {
                    "sent": "Predictions and the account like pretty similar.",
                    "label": 0
                },
                {
                    "sent": "If you do an initial preprocessing in which you only in which we first decide which are the relevant features, and then we run our various random forest algorithms, we get predictions which are basically again state of the art as well.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the.",
                    "label": 0
                },
                {
                    "sent": "The main failure mode of the of the random forest method.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's it in can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vision is a very simple idea in which we in which we have.",
                    "label": 0
                },
                {
                    "sent": "A method for doing online random forests that works in exactly the same way whether it's offline or online, so it's computationally very fast.",
                    "label": 0
                },
                {
                    "sent": "It gives very good predictive accuracies.",
                    "label": 0
                },
                {
                    "sent": "And it's very simple to do to implement in terms of future.",
                    "label": 0
                },
                {
                    "sent": "What would like to look into is basically what happens if we have high dimensional data with lots of irrelevant, irrelevant features.",
                    "label": 1
                },
                {
                    "sent": "Can we somehow identify the irrelevant and relevant features in an online manner?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so thank you.",
                    "label": 0
                },
                {
                    "sent": "We have archive paper this as a paper on archive as well as source code in Python, so if you'd like to try this, go ahead.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}