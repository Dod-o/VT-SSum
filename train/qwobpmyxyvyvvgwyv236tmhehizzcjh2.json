{
    "id": "qwobpmyxyvyvvgwyv236tmhehizzcjh2",
    "title": "Efficient Policy Construction for MDPs Represented in Probabilistic PDDL",
    "info": {
        "author": [
            "Boris Lesner, Laboratoire GREYC, University of Caen Basse-Normandie"
        ],
        "published": "July 21, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/icaps2011_lesner_probabilistic/",
    "segmentation": [
        [
            "So."
        ],
        [
            "I first give a small introduction about PDL and what we aim to do.",
            "Then I will go to the.",
            "All the stuff that on policing construction today that we I will introduce new concept concept for action value from this section values.",
            "And then give some examples and results.",
            "Now I also speak about policy of it."
        ],
        [
            "Problems.",
            "The first observation is that PDL actions can be used to represent Markov decision processes in a compact way, so the.",
            "The question is how to compute policies given those PPD elections.",
            "I'm mostly working on the discounted infinite terrorism case.",
            "What you usually do if you have some PDL MVP is there to translate the PDL into GPS and then choose your favorite soldier like Spell.",
            "She wants optimality or some approximate one if you want.",
            "What we propose years to use the PDL section because it's compact also and this allows to avoid the cost of the translation which might not be always costly, but.",
            "We we try to remove this step and also PDL is nice for unlegal correlated effects sometimes easier to define your prob."
        ],
        [
            "And we respect.",
            "So we focus on grounded PDL so the step valuable some Boolean variables.",
            "I understand updates are defined like a basic effects just means that you an effect is just enforce the value of some valuables in the Interstate to get the next date.",
            "And then also for compact value function representation which use a DDS.",
            "Which is quite common in in fact, all MVP's, and they also efficient operators on."
        ],
        [
            "Dysfunction.",
            "So a quick look at PDL and action is.",
            "Made us two components.",
            "The precondition I won't talk much about them, but.",
            "You have also the effects, so the effects are Rick defined recursively.",
            "You can force the value of some variable X.",
            "You can add some amount of reward.",
            "You have also conditional effects, which says some effect will apply only in state that satisfies some formula.",
            "You have also conjunctive effects, which basically means that a set of effects will all play at the same time.",
            "So does effects must be consistent to.",
            "You can't have any effect which say sets extra true and another one which is extra fault.",
            "They must be consistent.",
            "You have also probability probabilistic effects.",
            "That means you have a set of effects on each one can occur with some probability.",
            "So with this probability effects just when you're given a state you don't have one, only one successor state, but with the state, you have a distribution over over next over effects on the reward.",
            "That's the exact."
        ],
        [
            "Know about action."
        ],
        [
            "Values.",
            "So what we classically assume, and it's quite natural is the is the frame assumption.",
            "When you compute the action value, you just say that the variables not modified by your action will remain the same in next state.",
            "It's what we have to do for MVP's.",
            "Not if you.",
            "I doesn't make this assumption.",
            "You can say OK, my accent, change some variables, but I don't make any assumption on all the other variable will be changed in next date.",
            "So this gave gave us the notion of frameless action values.",
            "So those promise section values FEV take two states.",
            "The current state S and some assignment to the next variables.",
            "It's not.",
            "It's not quite a stable.",
            "Still, you will draw your effect distribution on the current state, but the effects alkali on extra.",
            "Down if you have.",
            "If you have these functions, you can still compute the regular action value function the Q values."
        ],
        [
            "So why do we need to that value?",
            "In fact, it's useful to handle conjunctive effects.",
            "Um, the key idea is that when you have this conjunctive effect is just to compute the value of the first effect that give you a newer value.",
            "You have forced the value the valuable changed by the 1st effect and then you don't say nothing about Olney.",
            "Other variables will evolve in next date, so you just take this a value of Y1, compute you have on top of that you compute the value of Y2 and then repeat.",
            "As we will see, we can do some other things with values because.",
            "They still can express every possible future other valuable."
        ],
        [
            "No, or the computation of values is based on backup tools.",
            "That's given some value or just value function.",
            "You can have a rule that for each kind of PPL effect will be willing to construct your value.",
            "For this fact we will see that each rules can be expressed with a few a DDS operators."
        ],
        [
            "So I will not give the algorithm in detail, I will.",
            "I prefer to do that."
        ],
        [
            "In an example.",
            "So now.",
            "Assume you have this action effect we say.",
            "Uh, I'd want to do here and then when X is false, Z will be true in next date with probability point 3X will be false.",
            "Otherwise we why will be true.",
            "As I assume you have some initial value function VOK as in Agda, so so did lines when the variable is true and that doesn't lines when the variable is false.",
            "The first things you need to do when you want to compute one iteration.",
            "Is to say that your value function represent next date.",
            "So you prime variables, meaning the without the values in next state and apply the discount factor.",
            "Here Gamma is the discount is .8, it's just.",
            "To be."
        ],
        [
            "So then you apply the first effect.",
            "So basically applying you hear what effect just adding the amount of 1.",
            "So here with a DD with just one 2."
        ],
        [
            "Tweet.",
            "No, we have a when axis falls, Z will be true that we will first focus on the value of Z and then we will.",
            "Applying the conditional effect.",
            "So we have this previous value function which already accounts for the increase of reward.",
            "And then what we want to do is to say that Z will always be through next state.",
            "So with ideas you can do that with the cofactor operation with basically replace Z prime with this true branch.",
            "Yeah, because you will be true.",
            "So we just remove the and but for instance.",
            "And we have this new one."
        ],
        [
            "Fight.",
            "So for the complete conditional effect must change it.",
            "So Z must.",
            "You must take the value of Z only if X is false in the current state.",
            "So here.",
            "We add dependency on the current state variable in the current state, it's X again.",
            "We can do that with a simple.",
            "If then else Aded operation which basically says that.",
            "1X error is false.",
            "Just point to the diagram where we use the value of Z.",
            "Otherwise just refer to the diagram with the previous value, just it changed nothing.",
            "So again, we get this compact value function."
        ],
        [
            "I know for a probabilistic effect it's quite simple.",
            "You just compute your value for the effects.",
            "X false and why?",
            "True, it's not shown here and then you just make a weighted sum again.",
            "You can do that with a DDS.",
            "And then again to obtain the value for a for the all action effect.",
            "So it basically says, you know, for example if X is true in current states, maybe it could be false or true in next date and if X will be false and Z will be true.",
            "I have a. I expect for OK."
        ],
        [
            "This is too much information we we need the frame assumption here.",
            "And what we need to say that OK next next date variables will take the value.",
            "The same truth value as in the current state.",
            "So we just forced the value of underlying variables to be prime variables.",
            "Again, you can express that with some aded operation.",
            "Well, he's quite not come back.",
            "Basically, says you must make each prime variable equivalent to an plain one and then take out the prime variables.",
            "We can do that with some specific algorithm, which just makes one traversal of the diagram, and then once you have forced the equivalence between the variables, you end up with the classical action value."
        ],
        [
            "So since we can compute the actual values and just now easy to adapt value iteration.",
            "We propose the algorithm ABAB, which is rule based action back, which was done on the roles that show me for.",
            "And just simply simply value iteration.",
            "Except we first.",
            "For exception, we compute the value, transform it into Q value, then maximize author action and then extract the policy."
        ],
        [
            "So some experiments are about."
        ],
        [
            "So we compared it.",
            "So on finding optimal policies, this in this country terrorism on some IPC benchmarks and we compared it to.",
            "To spell after taking a translation to deviance.",
            "So we compared against two different version of spreads.",
            "One that compute the complete transition matrix on the complication diagram and another one which one by one we just.",
            "User each DB and at the time it doesn't compute the full transition matrix.",
            "So on this domain social is rescue.",
            "Albab seems to work really fine.",
            "But what's interesting is that.",
            "The two versions of spells differ quite significant significantly.",
            "Now, one by one is better because on this domain you have many independent conditions.",
            "Basically the you have many action will say OK, One X1 is through, do something and when X2 is true is true do something and it adds a lot of.",
            "Lot of combinations in it, so that's why, for example, if you build a complete transition matrix, it go, it goes bigger."
        ],
        [
            "But then.",
            "Other results now you have the drive domains, which are some kind of traffic intersections program.",
            "Down there we see that.",
            "Maybe yes, quite some trouble, no is not is the worst over there and spelled behaves gets quite well with that.",
            "What's interesting in this domain is that the effects of actions have a specific structure.",
            "They're they look like like deviance, actually just a conjunction of probabilistic unconditional effects.",
            "You have this very strict structure and you have really complex actions, but only."
        ],
        [
            "Tree actions.",
            "And a PC you have a drive.",
            "Unrolled domains which have the same effects, but the actions are just split into smaller runs on them.",
            "Album becomes to work better with still.",
            "Oh on the biggest instance's someday."
        ],
        [
            "Settings.",
            "But then when you look at the definition of the effect, you can see that you can rearrange them to write them much more compactly, and we can see that airbags know exploit the compactness of defects."
        ],
        [
            "How we speak about."
        ],
        [
            "Policy resume so just some idea for future Rockingham.",
            "Suppose you have some policy of value function for this policy and action and A and money modified action aprime which is quite similar to way and you also have the value of A and we ask the question are we compute value for apron given a value of eight without restarting from scratch?",
            "For example it could happen maybe in reinforcement learning.",
            "Also."
        ],
        [
            "Later.",
            "So for example, you can add an effect, he just simply back up a conjunctive effect.",
            "You just compute the.",
            "Value of E on top of the value of a.",
            "You can also modify the reward function even with the conditional rewards.",
            "Which is simply A plus on the times with a DDS.",
            "You can also revise the relative probabilities of effects, for example.",
            "We can say under some condition you will occur with probability P, otherwise not nothing will happen, and then you can change the probability P to some other quality Q. I'm just some.",
            "A few 80D operations.",
            "You can go even further if you have again some effect E, which probably TPM with the otherwise you have E prime and if he on a prime are consistent you can also do some revision needs a bit more complex but still you don't need to recompute it from scratch."
        ],
        [
            "So to conclude, now now we the the key contribution I think is not the value iteration algorithm per say, but mostly the concept of values.",
            "Well, they can Adelie you to find the action value directly from.",
            "PPD element effects reactions.",
            "And it's not always good, but it works well when the effects are small.",
            "When you have non exclusive condition you have to.",
            "We have to work a little more on what's making the algorithm better, and they also exploit the the compactness of a DDS.",
            "As we saw, you have some possibilities for revising policies.",
            "And the next step is maybe yeah.",
            "Take a look at probabilistic planning because with PDL use of another.",
            "The initial state information on the goal state information and maybe we can use that too.",
            "Avoid with surgical old state space and maybe reduce the search.",
            "Yeah, it could be quite straightforward to use African style approximations, which basically approximate the ideas.",
            "And also I think it would be nice to try it with the affine engineers.",
            "Because the probabilistic effects are could benefit from from this structure."
        ],
        [
            "So thank you if you have questions.",
            "Some questions.",
            "So one question I had is I struggled to understand really.",
            "You know when it's really better than, say, normal spot.",
            "So you say when you have.",
            "Compact effects an OK excuse me function, what do you really mean by compact effects?",
            "Maybe with respect to some normal form, you know, like one in the night, so not relative to some normal form.",
            "In fact normal form.",
            "Not that compact, and so you must rather make these small as possible.",
            "To be with you, you're fine with conjunctions, So what is really bothering you?",
            "Making your uncomplicated with respect to other algorithms it so?",
            "For example, if if the problem is a natural definition with DBMS, so it's not good because it's not made for DBMS, but works much really better with that.",
            "But sometimes there are translation is.",
            "Quite dumb, quite difficult to do, but it's still polynomial size, but.",
            "And so it really depends on the program and we have to OK.",
            "So basically whenever it's complicated to do the deviant translation that you might, you might offer some.",
            "So would be good to characterize, yeah?",
            "Other questions.",
            "OK, thank you for."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I first give a small introduction about PDL and what we aim to do.",
                    "label": 0
                },
                {
                    "sent": "Then I will go to the.",
                    "label": 0
                },
                {
                    "sent": "All the stuff that on policing construction today that we I will introduce new concept concept for action value from this section values.",
                    "label": 0
                },
                {
                    "sent": "And then give some examples and results.",
                    "label": 0
                },
                {
                    "sent": "Now I also speak about policy of it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "The first observation is that PDL actions can be used to represent Markov decision processes in a compact way, so the.",
                    "label": 1
                },
                {
                    "sent": "The question is how to compute policies given those PPD elections.",
                    "label": 1
                },
                {
                    "sent": "I'm mostly working on the discounted infinite terrorism case.",
                    "label": 0
                },
                {
                    "sent": "What you usually do if you have some PDL MVP is there to translate the PDL into GPS and then choose your favorite soldier like Spell.",
                    "label": 0
                },
                {
                    "sent": "She wants optimality or some approximate one if you want.",
                    "label": 0
                },
                {
                    "sent": "What we propose years to use the PDL section because it's compact also and this allows to avoid the cost of the translation which might not be always costly, but.",
                    "label": 1
                },
                {
                    "sent": "We we try to remove this step and also PDL is nice for unlegal correlated effects sometimes easier to define your prob.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we respect.",
                    "label": 0
                },
                {
                    "sent": "So we focus on grounded PDL so the step valuable some Boolean variables.",
                    "label": 0
                },
                {
                    "sent": "I understand updates are defined like a basic effects just means that you an effect is just enforce the value of some valuables in the Interstate to get the next date.",
                    "label": 0
                },
                {
                    "sent": "And then also for compact value function representation which use a DDS.",
                    "label": 1
                },
                {
                    "sent": "Which is quite common in in fact, all MVP's, and they also efficient operators on.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dysfunction.",
                    "label": 0
                },
                {
                    "sent": "So a quick look at PDL and action is.",
                    "label": 0
                },
                {
                    "sent": "Made us two components.",
                    "label": 0
                },
                {
                    "sent": "The precondition I won't talk much about them, but.",
                    "label": 0
                },
                {
                    "sent": "You have also the effects, so the effects are Rick defined recursively.",
                    "label": 1
                },
                {
                    "sent": "You can force the value of some variable X.",
                    "label": 1
                },
                {
                    "sent": "You can add some amount of reward.",
                    "label": 0
                },
                {
                    "sent": "You have also conditional effects, which says some effect will apply only in state that satisfies some formula.",
                    "label": 0
                },
                {
                    "sent": "You have also conjunctive effects, which basically means that a set of effects will all play at the same time.",
                    "label": 1
                },
                {
                    "sent": "So does effects must be consistent to.",
                    "label": 0
                },
                {
                    "sent": "You can't have any effect which say sets extra true and another one which is extra fault.",
                    "label": 1
                },
                {
                    "sent": "They must be consistent.",
                    "label": 0
                },
                {
                    "sent": "You have also probability probabilistic effects.",
                    "label": 0
                },
                {
                    "sent": "That means you have a set of effects on each one can occur with some probability.",
                    "label": 0
                },
                {
                    "sent": "So with this probability effects just when you're given a state you don't have one, only one successor state, but with the state, you have a distribution over over next over effects on the reward.",
                    "label": 0
                },
                {
                    "sent": "That's the exact.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Know about action.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "So what we classically assume, and it's quite natural is the is the frame assumption.",
                    "label": 1
                },
                {
                    "sent": "When you compute the action value, you just say that the variables not modified by your action will remain the same in next state.",
                    "label": 1
                },
                {
                    "sent": "It's what we have to do for MVP's.",
                    "label": 0
                },
                {
                    "sent": "Not if you.",
                    "label": 0
                },
                {
                    "sent": "I doesn't make this assumption.",
                    "label": 0
                },
                {
                    "sent": "You can say OK, my accent, change some variables, but I don't make any assumption on all the other variable will be changed in next date.",
                    "label": 0
                },
                {
                    "sent": "So this gave gave us the notion of frameless action values.",
                    "label": 1
                },
                {
                    "sent": "So those promise section values FEV take two states.",
                    "label": 0
                },
                {
                    "sent": "The current state S and some assignment to the next variables.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not quite a stable.",
                    "label": 0
                },
                {
                    "sent": "Still, you will draw your effect distribution on the current state, but the effects alkali on extra.",
                    "label": 0
                },
                {
                    "sent": "Down if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have these functions, you can still compute the regular action value function the Q values.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why do we need to that value?",
                    "label": 0
                },
                {
                    "sent": "In fact, it's useful to handle conjunctive effects.",
                    "label": 0
                },
                {
                    "sent": "Um, the key idea is that when you have this conjunctive effect is just to compute the value of the first effect that give you a newer value.",
                    "label": 0
                },
                {
                    "sent": "You have forced the value the valuable changed by the 1st effect and then you don't say nothing about Olney.",
                    "label": 0
                },
                {
                    "sent": "Other variables will evolve in next date, so you just take this a value of Y1, compute you have on top of that you compute the value of Y2 and then repeat.",
                    "label": 0
                },
                {
                    "sent": "As we will see, we can do some other things with values because.",
                    "label": 0
                },
                {
                    "sent": "They still can express every possible future other valuable.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, or the computation of values is based on backup tools.",
                    "label": 0
                },
                {
                    "sent": "That's given some value or just value function.",
                    "label": 0
                },
                {
                    "sent": "You can have a rule that for each kind of PPL effect will be willing to construct your value.",
                    "label": 1
                },
                {
                    "sent": "For this fact we will see that each rules can be expressed with a few a DDS operators.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will not give the algorithm in detail, I will.",
                    "label": 0
                },
                {
                    "sent": "I prefer to do that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In an example.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Assume you have this action effect we say.",
                    "label": 1
                },
                {
                    "sent": "Uh, I'd want to do here and then when X is false, Z will be true in next date with probability point 3X will be false.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we why will be true.",
                    "label": 0
                },
                {
                    "sent": "As I assume you have some initial value function VOK as in Agda, so so did lines when the variable is true and that doesn't lines when the variable is false.",
                    "label": 0
                },
                {
                    "sent": "The first things you need to do when you want to compute one iteration.",
                    "label": 1
                },
                {
                    "sent": "Is to say that your value function represent next date.",
                    "label": 0
                },
                {
                    "sent": "So you prime variables, meaning the without the values in next state and apply the discount factor.",
                    "label": 0
                },
                {
                    "sent": "Here Gamma is the discount is .8, it's just.",
                    "label": 0
                },
                {
                    "sent": "To be.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then you apply the first effect.",
                    "label": 0
                },
                {
                    "sent": "So basically applying you hear what effect just adding the amount of 1.",
                    "label": 0
                },
                {
                    "sent": "So here with a DD with just one 2.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tweet.",
                    "label": 0
                },
                {
                    "sent": "No, we have a when axis falls, Z will be true that we will first focus on the value of Z and then we will.",
                    "label": 0
                },
                {
                    "sent": "Applying the conditional effect.",
                    "label": 0
                },
                {
                    "sent": "So we have this previous value function which already accounts for the increase of reward.",
                    "label": 0
                },
                {
                    "sent": "And then what we want to do is to say that Z will always be through next state.",
                    "label": 0
                },
                {
                    "sent": "So with ideas you can do that with the cofactor operation with basically replace Z prime with this true branch.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because you will be true.",
                    "label": 0
                },
                {
                    "sent": "So we just remove the and but for instance.",
                    "label": 0
                },
                {
                    "sent": "And we have this new one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fight.",
                    "label": 0
                },
                {
                    "sent": "So for the complete conditional effect must change it.",
                    "label": 0
                },
                {
                    "sent": "So Z must.",
                    "label": 0
                },
                {
                    "sent": "You must take the value of Z only if X is false in the current state.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "We add dependency on the current state variable in the current state, it's X again.",
                    "label": 0
                },
                {
                    "sent": "We can do that with a simple.",
                    "label": 0
                },
                {
                    "sent": "If then else Aded operation which basically says that.",
                    "label": 0
                },
                {
                    "sent": "1X error is false.",
                    "label": 0
                },
                {
                    "sent": "Just point to the diagram where we use the value of Z.",
                    "label": 0
                },
                {
                    "sent": "Otherwise just refer to the diagram with the previous value, just it changed nothing.",
                    "label": 0
                },
                {
                    "sent": "So again, we get this compact value function.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know for a probabilistic effect it's quite simple.",
                    "label": 1
                },
                {
                    "sent": "You just compute your value for the effects.",
                    "label": 0
                },
                {
                    "sent": "X false and why?",
                    "label": 0
                },
                {
                    "sent": "True, it's not shown here and then you just make a weighted sum again.",
                    "label": 0
                },
                {
                    "sent": "You can do that with a DDS.",
                    "label": 0
                },
                {
                    "sent": "And then again to obtain the value for a for the all action effect.",
                    "label": 0
                },
                {
                    "sent": "So it basically says, you know, for example if X is true in current states, maybe it could be false or true in next date and if X will be false and Z will be true.",
                    "label": 0
                },
                {
                    "sent": "I have a. I expect for OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is too much information we we need the frame assumption here.",
                    "label": 0
                },
                {
                    "sent": "And what we need to say that OK next next date variables will take the value.",
                    "label": 0
                },
                {
                    "sent": "The same truth value as in the current state.",
                    "label": 0
                },
                {
                    "sent": "So we just forced the value of underlying variables to be prime variables.",
                    "label": 0
                },
                {
                    "sent": "Again, you can express that with some aded operation.",
                    "label": 0
                },
                {
                    "sent": "Well, he's quite not come back.",
                    "label": 0
                },
                {
                    "sent": "Basically, says you must make each prime variable equivalent to an plain one and then take out the prime variables.",
                    "label": 0
                },
                {
                    "sent": "We can do that with some specific algorithm, which just makes one traversal of the diagram, and then once you have forced the equivalence between the variables, you end up with the classical action value.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So since we can compute the actual values and just now easy to adapt value iteration.",
                    "label": 1
                },
                {
                    "sent": "We propose the algorithm ABAB, which is rule based action back, which was done on the roles that show me for.",
                    "label": 1
                },
                {
                    "sent": "And just simply simply value iteration.",
                    "label": 0
                },
                {
                    "sent": "Except we first.",
                    "label": 0
                },
                {
                    "sent": "For exception, we compute the value, transform it into Q value, then maximize author action and then extract the policy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some experiments are about.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compared it.",
                    "label": 0
                },
                {
                    "sent": "So on finding optimal policies, this in this country terrorism on some IPC benchmarks and we compared it to.",
                    "label": 0
                },
                {
                    "sent": "To spell after taking a translation to deviance.",
                    "label": 0
                },
                {
                    "sent": "So we compared against two different version of spreads.",
                    "label": 0
                },
                {
                    "sent": "One that compute the complete transition matrix on the complication diagram and another one which one by one we just.",
                    "label": 0
                },
                {
                    "sent": "User each DB and at the time it doesn't compute the full transition matrix.",
                    "label": 0
                },
                {
                    "sent": "So on this domain social is rescue.",
                    "label": 0
                },
                {
                    "sent": "Albab seems to work really fine.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting is that.",
                    "label": 0
                },
                {
                    "sent": "The two versions of spells differ quite significant significantly.",
                    "label": 0
                },
                {
                    "sent": "Now, one by one is better because on this domain you have many independent conditions.",
                    "label": 0
                },
                {
                    "sent": "Basically the you have many action will say OK, One X1 is through, do something and when X2 is true is true do something and it adds a lot of.",
                    "label": 0
                },
                {
                    "sent": "Lot of combinations in it, so that's why, for example, if you build a complete transition matrix, it go, it goes bigger.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But then.",
                    "label": 0
                },
                {
                    "sent": "Other results now you have the drive domains, which are some kind of traffic intersections program.",
                    "label": 1
                },
                {
                    "sent": "Down there we see that.",
                    "label": 0
                },
                {
                    "sent": "Maybe yes, quite some trouble, no is not is the worst over there and spelled behaves gets quite well with that.",
                    "label": 0
                },
                {
                    "sent": "What's interesting in this domain is that the effects of actions have a specific structure.",
                    "label": 0
                },
                {
                    "sent": "They're they look like like deviance, actually just a conjunction of probabilistic unconditional effects.",
                    "label": 0
                },
                {
                    "sent": "You have this very strict structure and you have really complex actions, but only.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree actions.",
                    "label": 0
                },
                {
                    "sent": "And a PC you have a drive.",
                    "label": 0
                },
                {
                    "sent": "Unrolled domains which have the same effects, but the actions are just split into smaller runs on them.",
                    "label": 0
                },
                {
                    "sent": "Album becomes to work better with still.",
                    "label": 0
                },
                {
                    "sent": "Oh on the biggest instance's someday.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Settings.",
                    "label": 0
                },
                {
                    "sent": "But then when you look at the definition of the effect, you can see that you can rearrange them to write them much more compactly, and we can see that airbags know exploit the compactness of defects.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we speak about.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Policy resume so just some idea for future Rockingham.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have some policy of value function for this policy and action and A and money modified action aprime which is quite similar to way and you also have the value of A and we ask the question are we compute value for apron given a value of eight without restarting from scratch?",
                    "label": 1
                },
                {
                    "sent": "For example it could happen maybe in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can add an effect, he just simply back up a conjunctive effect.",
                    "label": 0
                },
                {
                    "sent": "You just compute the.",
                    "label": 0
                },
                {
                    "sent": "Value of E on top of the value of a.",
                    "label": 0
                },
                {
                    "sent": "You can also modify the reward function even with the conditional rewards.",
                    "label": 0
                },
                {
                    "sent": "Which is simply A plus on the times with a DDS.",
                    "label": 0
                },
                {
                    "sent": "You can also revise the relative probabilities of effects, for example.",
                    "label": 0
                },
                {
                    "sent": "We can say under some condition you will occur with probability P, otherwise not nothing will happen, and then you can change the probability P to some other quality Q. I'm just some.",
                    "label": 0
                },
                {
                    "sent": "A few 80D operations.",
                    "label": 0
                },
                {
                    "sent": "You can go even further if you have again some effect E, which probably TPM with the otherwise you have E prime and if he on a prime are consistent you can also do some revision needs a bit more complex but still you don't need to recompute it from scratch.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, now now we the the key contribution I think is not the value iteration algorithm per say, but mostly the concept of values.",
                    "label": 0
                },
                {
                    "sent": "Well, they can Adelie you to find the action value directly from.",
                    "label": 0
                },
                {
                    "sent": "PPD element effects reactions.",
                    "label": 0
                },
                {
                    "sent": "And it's not always good, but it works well when the effects are small.",
                    "label": 0
                },
                {
                    "sent": "When you have non exclusive condition you have to.",
                    "label": 1
                },
                {
                    "sent": "We have to work a little more on what's making the algorithm better, and they also exploit the the compactness of a DDS.",
                    "label": 1
                },
                {
                    "sent": "As we saw, you have some possibilities for revising policies.",
                    "label": 0
                },
                {
                    "sent": "And the next step is maybe yeah.",
                    "label": 0
                },
                {
                    "sent": "Take a look at probabilistic planning because with PDL use of another.",
                    "label": 0
                },
                {
                    "sent": "The initial state information on the goal state information and maybe we can use that too.",
                    "label": 0
                },
                {
                    "sent": "Avoid with surgical old state space and maybe reduce the search.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it could be quite straightforward to use African style approximations, which basically approximate the ideas.",
                    "label": 0
                },
                {
                    "sent": "And also I think it would be nice to try it with the affine engineers.",
                    "label": 0
                },
                {
                    "sent": "Because the probabilistic effects are could benefit from from this structure.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you if you have questions.",
                    "label": 1
                },
                {
                    "sent": "Some questions.",
                    "label": 0
                },
                {
                    "sent": "So one question I had is I struggled to understand really.",
                    "label": 0
                },
                {
                    "sent": "You know when it's really better than, say, normal spot.",
                    "label": 0
                },
                {
                    "sent": "So you say when you have.",
                    "label": 1
                },
                {
                    "sent": "Compact effects an OK excuse me function, what do you really mean by compact effects?",
                    "label": 0
                },
                {
                    "sent": "Maybe with respect to some normal form, you know, like one in the night, so not relative to some normal form.",
                    "label": 0
                },
                {
                    "sent": "In fact normal form.",
                    "label": 0
                },
                {
                    "sent": "Not that compact, and so you must rather make these small as possible.",
                    "label": 0
                },
                {
                    "sent": "To be with you, you're fine with conjunctions, So what is really bothering you?",
                    "label": 0
                },
                {
                    "sent": "Making your uncomplicated with respect to other algorithms it so?",
                    "label": 0
                },
                {
                    "sent": "For example, if if the problem is a natural definition with DBMS, so it's not good because it's not made for DBMS, but works much really better with that.",
                    "label": 0
                },
                {
                    "sent": "But sometimes there are translation is.",
                    "label": 1
                },
                {
                    "sent": "Quite dumb, quite difficult to do, but it's still polynomial size, but.",
                    "label": 0
                },
                {
                    "sent": "And so it really depends on the program and we have to OK.",
                    "label": 0
                },
                {
                    "sent": "So basically whenever it's complicated to do the deviant translation that you might, you might offer some.",
                    "label": 0
                },
                {
                    "sent": "So would be good to characterize, yeah?",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for.",
                    "label": 0
                }
            ]
        }
    }
}