{
    "id": "v5ppt2jqwpkmlcavo7fpnqp34g45rjum",
    "title": "Scalable Histograms on Large Probabilistic Datase Nonlinear Classification",
    "info": {
        "author": [
            "Mingwang Tang, School of Computing, University of Utah"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_tang_scalable_histograms/",
    "segmentation": [
        [
            "This paper is a giant work with my advisor, Philly."
        ],
        [
            "So nowadays to demand mean system of facing new challenges from large scale data and distributed source, an uncertainty with data.",
            "For example, in this figure we can show that the leader could be generally from multiple devices like the phone and the iPad and the servers and also in the sensor networkers.",
            "Sensor readings are inherently noisy and they are generated from digital sources and data integration system often times.",
            "Party matches produced, so didn't synopsis techniques became very important and fundamental for managing large probabilistic data.",
            "So in this paper particularly, we care about how to build a scalable histogram on large public data."
        ],
        [
            "So first let's review how to build a we optimal histogram determines decide.",
            "So given a frequency vector way on a domain or end here that we I the frequency item I.",
            "As the figure shows below and the space Project B.",
            "So the goal is trying to find out a cycle bar keyed and with each bracket the frequency of each item will be approximated by a constant representatives.",
            "So clearly we can find out like the optimal be barkette such side the access the error between the.",
            "Original data distribution and marketing property could be minimized and this turn out could be finished by dynamic programming approach using begin squared time and a bunch of processing method has been proposed for such kind of histogram."
        ],
        [
            "And moving to property database, we know that a property database characterized property distribution of numerous possible world.",
            "For example, this blue circle represent of frequency distribution impossible, possible world one and we can show another part."
        ],
        [
            "Which I wrapped by the triangles which have a different frequency distribution.",
            "So now the frequency vector becames a random frequency vector, so each frequency has the other distribution across a possible world."
        ],
        [
            "So to describe the frequency distribution, so we're going to introduce two probability data models of the first one, the tuple model.",
            "So the top of model, each top hole going to specify a site of mutual exclusion, excluding choice.",
            "Here, that IJK is drawn from the item domain and the PJC is associated probability that this DJ going to take this value TJK for example, these two 41 specified that.",
            "It's going to take item value one with property of thereupon two.",
            "It's also could take item three with proper leadership on three and."
        ],
        [
            "We can show that this item appear in different two posts, which means that when we instantiate this property debate into a possible world when counting the number of times it's appearing that possible world will be the frequency of this item."
        ],
        [
            "And another one is the value model.",
            "So instead of.",
            "Spiting by a batch of items in a tuple so this value model actually specify the frequency distribution of single item.",
            "OK, so now."
        ],
        [
            "We see that this, for example 2.3 is only specified.",
            "The frequency value of item 3.",
            "Now the values is not from John from, not from the item, but from the frequency domain."
        ],
        [
            "OK, so back to the histogram on property data.",
            "So one problem we have to consider is how to combine the histogram from each possible world.",
            "And how to compute this efficient without explicitly instantiate the property database so Graham has a proposed way of building the expectation based histogram in hazer ICD paper.",
            "Basically the goal is try to minimize the expected histogram.",
            "Across the all the possible world."
        ],
        [
            "And this optimal Hickman also could be derived by a dynamic programming approach in being squared time.",
            "And he showed that the minimal bucket error for a bucket boundary from S2, E could be achieved by fighting this bucket drop into the falling value, and in order to compute this single Buckley error in constant time, we need to precompute this prefix of rates or A&BA.",
            "Clearly this optimal method or market construction does not scalable, since it had a quadratic running time in terms of underlying domain.",
            "So."
        ],
        [
            "So.",
            "We proposed process mastered of building histogram based on the partition and merge principle.",
            "So the idea is quite simple."
        ],
        [
            "So how in the partition pays we're going to divide this domain into equal size sub domain?",
            "And within each sub domain.",
            "We're going to build working to build a local optimal be barking so with am sub domain, we're going to up."
        ],
        [
            "Would be embarked in the merging phase.",
            "And each bracketed revenue T could be represented by a weighted frequency, which means that.",
            "The weight of this park.",
            "It is simply the last of the bracket it covered from the partition phase."
        ],
        [
            "Based on this awaited pro, can see we can compute the final B bar kit."
        ],
        [
            "And we show that the payment method actually provided a square root N approximate ratio compared to the optimal histogram.",
            "Provided by the state with art and the running time has been reduced and we show that we can further improve the running time using a recursive merging.",
            "So basically instead we partition the domain into 10 to the into M to the power of our sub domain instead of I'm domain and using our iterations we can produce a final be bucket.",
            "In each iteration the domain sites will be reduced by a factor or I'm.",
            "Then we show that it's recursive master provide provided on 10 to the power of our two approximate ratio.",
            "And in practice, these two methods actually always produce a very close to optimal approximation quality in our experiment."
        ],
        [
            "We also extended this idea to distributed and parallel citing."
        ],
        [
            "So basically we partition the domain D intubated chunks so that we can provide in each chunk independent in parallel and four tuples in each chunk.",
            "We're providing them in a streaming way for each topic.",
            "For each tuple, we're going to compute key value pair for the value model.",
            "For example, we're going to use the item value as a key, and values will be expectation of the iPod and each partition apply square and.",
            "Based off a hash function, we're going to send this key value pair to assassinate sub domain."
        ],
        [
            "When when the septum and collected all the keyboard appears, it could reconstruct the idea rate and based on those Abby arrays we can compute the local optimal histogram."
        ],
        [
            "For each sub domain, an emerging fields, we have more values for the recursive version.",
            "The merging side from each iteration could be handled in dependently and of course by doing this we have further reduced running time and also we bring some communication cost.",
            "So basically the communication in the partition phase has dominates the Community communication costs for the tuple model is going to take.",
            "Bingo, bingo, bit and bass and formatting mode is going to take legal advice."
        ],
        [
            "So next we.",
            "Reduce how to reduce the communication cost using sampling tasks techniques.",
            "So when we are observing this adyeri we can see that.",
            "Each re element of A or B could be could be view at rank or some of the rank of the value of G like the AJ.",
            "The ECJ is the prefix sum of values that as a as a item item value that's less than J.",
            "So basically this rank of J in this array."
        ],
        [
            "So the idea is quite simple.",
            "Each data chunk we just compute this expectation over A and this."
        ],
        [
            "Rate this every element could be view as the counter at each item, right?",
            "Alright, for example, this 5 means we have 5 items 3.",
            "And we can conceptually unfold these 3 into 5 copies an."
        ],
        [
            "We sample each copy with probably the P and this is going to generate quantile sample quantile sample and based on this contact sample we can reconstruct array A and or B and for each element we guarantee that the error will be multi multi and manners epsilon N."
        ],
        [
            "And for the moving to the Top Model, since became the more complicated we need to compute some.",
            "Second, appropriately moment for a dynamic changing.",
            "Range seared the J going to iterate throughout the values from SK to Ek Wichita Sub domain and we know that MSMS sketch could be used to estimate the app to four fixed domain.",
            "So our idea like we're going to partition this sub domain into multiple intervals in a hierarchical structure and then we build AMS for each interval now given.",
            "Ecurie intro S&E.",
            "We're going to find out a Canonical cover for this Curie range, and then we use the app to for the intervals within this Canonical cover to approximate the app to occur in range."
        ],
        [
            "So moving to the experiment experiment."
        ],
        [
            "Experiment Part A which generates the.",
            "The supermodel and value model data set using the client ID field or 1998 World Cup website and also another decide we uses atmosphere measurements.",
            "Did aside from the samples project and the default experimental parameters are the number of market is 400 and the domain side for the regular decide handled K Anne for the large data is 600 K and the number of for the recursive merging method adapts is 2."
        ],
        [
            "So we see in this figure at the domain site increase of the master.",
            "How are increasing running time particularly?",
            "For the optimal method on the date sites, you could hide the key with.",
            "It's going to running around like 13 hours and for the recursive merging method is going to take only several 100 seconds."
        ],
        [
            "We also see that approximate ratio is actually quite close to the optimal histogram."
        ],
        [
            "And moving to the large side, we see that our parallel and recursive parallel recursive merging method provides a very good performance.",
            "When the domain had reached reached 1 million, we see that it's still going to need only several 100 seconds.",
            "So."
        ],
        [
            "To conclude, we have proposed a new well Opossum masterful constructing his scalable histogram of large public data and the quality.",
            "Of the approximate histogram almost as good as optimal, in practice, we also extend our technique to district in parallel sightings and the future work we can consider to study, and probably the probabilistic histogram with PDF at the market development team and handle his ground with other error metric.",
            "Thank you."
        ],
        [
            "Now I'm open to the questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This paper is a giant work with my advisor, Philly.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So nowadays to demand mean system of facing new challenges from large scale data and distributed source, an uncertainty with data.",
                    "label": 1
                },
                {
                    "sent": "For example, in this figure we can show that the leader could be generally from multiple devices like the phone and the iPad and the servers and also in the sensor networkers.",
                    "label": 0
                },
                {
                    "sent": "Sensor readings are inherently noisy and they are generated from digital sources and data integration system often times.",
                    "label": 1
                },
                {
                    "sent": "Party matches produced, so didn't synopsis techniques became very important and fundamental for managing large probabilistic data.",
                    "label": 0
                },
                {
                    "sent": "So in this paper particularly, we care about how to build a scalable histogram on large public data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let's review how to build a we optimal histogram determines decide.",
                    "label": 0
                },
                {
                    "sent": "So given a frequency vector way on a domain or end here that we I the frequency item I.",
                    "label": 1
                },
                {
                    "sent": "As the figure shows below and the space Project B.",
                    "label": 0
                },
                {
                    "sent": "So the goal is trying to find out a cycle bar keyed and with each bracket the frequency of each item will be approximated by a constant representatives.",
                    "label": 0
                },
                {
                    "sent": "So clearly we can find out like the optimal be barkette such side the access the error between the.",
                    "label": 0
                },
                {
                    "sent": "Original data distribution and marketing property could be minimized and this turn out could be finished by dynamic programming approach using begin squared time and a bunch of processing method has been proposed for such kind of histogram.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And moving to property database, we know that a property database characterized property distribution of numerous possible world.",
                    "label": 0
                },
                {
                    "sent": "For example, this blue circle represent of frequency distribution impossible, possible world one and we can show another part.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I wrapped by the triangles which have a different frequency distribution.",
                    "label": 0
                },
                {
                    "sent": "So now the frequency vector becames a random frequency vector, so each frequency has the other distribution across a possible world.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to describe the frequency distribution, so we're going to introduce two probability data models of the first one, the tuple model.",
                    "label": 1
                },
                {
                    "sent": "So the top of model, each top hole going to specify a site of mutual exclusion, excluding choice.",
                    "label": 1
                },
                {
                    "sent": "Here, that IJK is drawn from the item domain and the PJC is associated probability that this DJ going to take this value TJK for example, these two 41 specified that.",
                    "label": 0
                },
                {
                    "sent": "It's going to take item value one with property of thereupon two.",
                    "label": 0
                },
                {
                    "sent": "It's also could take item three with proper leadership on three and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that this item appear in different two posts, which means that when we instantiate this property debate into a possible world when counting the number of times it's appearing that possible world will be the frequency of this item.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another one is the value model.",
                    "label": 0
                },
                {
                    "sent": "So instead of.",
                    "label": 0
                },
                {
                    "sent": "Spiting by a batch of items in a tuple so this value model actually specify the frequency distribution of single item.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see that this, for example 2.3 is only specified.",
                    "label": 0
                },
                {
                    "sent": "The frequency value of item 3.",
                    "label": 0
                },
                {
                    "sent": "Now the values is not from John from, not from the item, but from the frequency domain.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so back to the histogram on property data.",
                    "label": 0
                },
                {
                    "sent": "So one problem we have to consider is how to combine the histogram from each possible world.",
                    "label": 0
                },
                {
                    "sent": "And how to compute this efficient without explicitly instantiate the property database so Graham has a proposed way of building the expectation based histogram in hazer ICD paper.",
                    "label": 1
                },
                {
                    "sent": "Basically the goal is try to minimize the expected histogram.",
                    "label": 1
                },
                {
                    "sent": "Across the all the possible world.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this optimal Hickman also could be derived by a dynamic programming approach in being squared time.",
                    "label": 0
                },
                {
                    "sent": "And he showed that the minimal bucket error for a bucket boundary from S2, E could be achieved by fighting this bucket drop into the falling value, and in order to compute this single Buckley error in constant time, we need to precompute this prefix of rates or A&BA.",
                    "label": 1
                },
                {
                    "sent": "Clearly this optimal method or market construction does not scalable, since it had a quadratic running time in terms of underlying domain.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We proposed process mastered of building histogram based on the partition and merge principle.",
                    "label": 1
                },
                {
                    "sent": "So the idea is quite simple.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how in the partition pays we're going to divide this domain into equal size sub domain?",
                    "label": 1
                },
                {
                    "sent": "And within each sub domain.",
                    "label": 1
                },
                {
                    "sent": "We're going to build working to build a local optimal be barking so with am sub domain, we're going to up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would be embarked in the merging phase.",
                    "label": 0
                },
                {
                    "sent": "And each bracketed revenue T could be represented by a weighted frequency, which means that.",
                    "label": 1
                },
                {
                    "sent": "The weight of this park.",
                    "label": 0
                },
                {
                    "sent": "It is simply the last of the bracket it covered from the partition phase.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on this awaited pro, can see we can compute the final B bar kit.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we show that the payment method actually provided a square root N approximate ratio compared to the optimal histogram.",
                    "label": 0
                },
                {
                    "sent": "Provided by the state with art and the running time has been reduced and we show that we can further improve the running time using a recursive merging.",
                    "label": 0
                },
                {
                    "sent": "So basically instead we partition the domain into 10 to the into M to the power of our sub domain instead of I'm domain and using our iterations we can produce a final be bucket.",
                    "label": 0
                },
                {
                    "sent": "In each iteration the domain sites will be reduced by a factor or I'm.",
                    "label": 1
                },
                {
                    "sent": "Then we show that it's recursive master provide provided on 10 to the power of our two approximate ratio.",
                    "label": 0
                },
                {
                    "sent": "And in practice, these two methods actually always produce a very close to optimal approximation quality in our experiment.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also extended this idea to distributed and parallel citing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically we partition the domain D intubated chunks so that we can provide in each chunk independent in parallel and four tuples in each chunk.",
                    "label": 0
                },
                {
                    "sent": "We're providing them in a streaming way for each topic.",
                    "label": 0
                },
                {
                    "sent": "For each tuple, we're going to compute key value pair for the value model.",
                    "label": 0
                },
                {
                    "sent": "For example, we're going to use the item value as a key, and values will be expectation of the iPod and each partition apply square and.",
                    "label": 0
                },
                {
                    "sent": "Based off a hash function, we're going to send this key value pair to assassinate sub domain.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When when the septum and collected all the keyboard appears, it could reconstruct the idea rate and based on those Abby arrays we can compute the local optimal histogram.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For each sub domain, an emerging fields, we have more values for the recursive version.",
                    "label": 0
                },
                {
                    "sent": "The merging side from each iteration could be handled in dependently and of course by doing this we have further reduced running time and also we bring some communication cost.",
                    "label": 0
                },
                {
                    "sent": "So basically the communication in the partition phase has dominates the Community communication costs for the tuple model is going to take.",
                    "label": 1
                },
                {
                    "sent": "Bingo, bingo, bit and bass and formatting mode is going to take legal advice.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next we.",
                    "label": 0
                },
                {
                    "sent": "Reduce how to reduce the communication cost using sampling tasks techniques.",
                    "label": 0
                },
                {
                    "sent": "So when we are observing this adyeri we can see that.",
                    "label": 0
                },
                {
                    "sent": "Each re element of A or B could be could be view at rank or some of the rank of the value of G like the AJ.",
                    "label": 0
                },
                {
                    "sent": "The ECJ is the prefix sum of values that as a as a item item value that's less than J.",
                    "label": 0
                },
                {
                    "sent": "So basically this rank of J in this array.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is quite simple.",
                    "label": 0
                },
                {
                    "sent": "Each data chunk we just compute this expectation over A and this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rate this every element could be view as the counter at each item, right?",
                    "label": 0
                },
                {
                    "sent": "Alright, for example, this 5 means we have 5 items 3.",
                    "label": 0
                },
                {
                    "sent": "And we can conceptually unfold these 3 into 5 copies an.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We sample each copy with probably the P and this is going to generate quantile sample quantile sample and based on this contact sample we can reconstruct array A and or B and for each element we guarantee that the error will be multi multi and manners epsilon N.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the moving to the Top Model, since became the more complicated we need to compute some.",
                    "label": 0
                },
                {
                    "sent": "Second, appropriately moment for a dynamic changing.",
                    "label": 0
                },
                {
                    "sent": "Range seared the J going to iterate throughout the values from SK to Ek Wichita Sub domain and we know that MSMS sketch could be used to estimate the app to four fixed domain.",
                    "label": 0
                },
                {
                    "sent": "So our idea like we're going to partition this sub domain into multiple intervals in a hierarchical structure and then we build AMS for each interval now given.",
                    "label": 0
                },
                {
                    "sent": "Ecurie intro S&E.",
                    "label": 0
                },
                {
                    "sent": "We're going to find out a Canonical cover for this Curie range, and then we use the app to for the intervals within this Canonical cover to approximate the app to occur in range.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So moving to the experiment experiment.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment Part A which generates the.",
                    "label": 0
                },
                {
                    "sent": "The supermodel and value model data set using the client ID field or 1998 World Cup website and also another decide we uses atmosphere measurements.",
                    "label": 1
                },
                {
                    "sent": "Did aside from the samples project and the default experimental parameters are the number of market is 400 and the domain side for the regular decide handled K Anne for the large data is 600 K and the number of for the recursive merging method adapts is 2.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see in this figure at the domain site increase of the master.",
                    "label": 0
                },
                {
                    "sent": "How are increasing running time particularly?",
                    "label": 0
                },
                {
                    "sent": "For the optimal method on the date sites, you could hide the key with.",
                    "label": 0
                },
                {
                    "sent": "It's going to running around like 13 hours and for the recursive merging method is going to take only several 100 seconds.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also see that approximate ratio is actually quite close to the optimal histogram.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And moving to the large side, we see that our parallel and recursive parallel recursive merging method provides a very good performance.",
                    "label": 0
                },
                {
                    "sent": "When the domain had reached reached 1 million, we see that it's still going to need only several 100 seconds.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we have proposed a new well Opossum masterful constructing his scalable histogram of large public data and the quality.",
                    "label": 0
                },
                {
                    "sent": "Of the approximate histogram almost as good as optimal, in practice, we also extend our technique to district in parallel sightings and the future work we can consider to study, and probably the probabilistic histogram with PDF at the market development team and handle his ground with other error metric.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm open to the questions.",
                    "label": 0
                }
            ]
        }
    }
}