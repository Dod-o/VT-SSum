{
    "id": "guyzhn74zs5434ojvjxtrkdfrybey5wx",
    "title": "Learning Kernels via Margin-and-Radius Ratios",
    "info": {
        "author": [
            "Kun Gai, Department of Automation, Tsinghua University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_gai_lkm/",
    "segmentation": [
        [
            "I'm Congo and title of my work work is learning kernels, fair margin ratio, radio ratings, ratios most exist?"
        ],
        [
            "Synchrony, Massage employees a large margin principle.",
            "However, consider if we enlarge the scaling of a kernel, the margin of SM within the kernel will also increase, so there are scaling problems in most instead of merging based formulations.",
            "In linear combination kernel cases, normal constraint on combination coefficients needs to be enforced.",
            "However, different types of normal constraints with different data size, so it is very difficult for users to select a suitable one.",
            "The different initial scalings of basis kernels."
        ],
        [
            "Sorry, two different results.",
            "So we want to find a more reasonable, reasonable way to measure the goodness of kernel.",
            "Some theoretical results say that the generalization error bound of kernel learning for both linear and nonlinear cases depends on the ratio between the margin and readings and minimal enclosing ball of data in the feature space motivated by this result, we present new scaling invariant formulations or corner learning.",
            "Are we present the radiance of minimal, including full of data in the feature space and the first term in our objective function involves a large ratio between the margin and the readings of minimal enclosing ball in the second term is a huge loss."
        ],
        [
            "Some other formulations also consider the readings of minimal enclosing ball for corner learning.",
            "However, the formulations.",
            "Women's soft margin versions still have the scaling problems.",
            "We prove that our formulation formulation is invariant to scaling of kernels in both linear and nonlinear cases, and in linear combination.",
            "Kernel cases are formulation formulation is also invariant to normal construct.",
            "On the combination coefficients.",
            "And also invariant to this initial scalings of basis kernels.",
            "So in our proposed kernel learning problem, the main difficult difficulty is to how to optimize, optimize, optimize it.",
            "And we."
        ],
        [
            "We show that our proposed kernel learning problem can be transformed to a tree level optimization problem.",
            "The objective function itself is a bilevel optimal value function, so the desk is theorem which is widely used in previous kernel learning methods is not enough.",
            "Here we propose a new theorem of the differential ability of multi level optimal optimal value functions, and by applying our theorem we show that.",
            "The objective function of our proposed kernel learning problem is differentiable, and we use standard gradient based algorithm for our kernel learning problem and experiments validates that our method outperforms other state of art corner learning approaches.",
            "In most benchmarking sites and welcome to our poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm Congo and title of my work work is learning kernels, fair margin ratio, radio ratings, ratios most exist?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Synchrony, Massage employees a large margin principle.",
                    "label": 1
                },
                {
                    "sent": "However, consider if we enlarge the scaling of a kernel, the margin of SM within the kernel will also increase, so there are scaling problems in most instead of merging based formulations.",
                    "label": 1
                },
                {
                    "sent": "In linear combination kernel cases, normal constraint on combination coefficients needs to be enforced.",
                    "label": 1
                },
                {
                    "sent": "However, different types of normal constraints with different data size, so it is very difficult for users to select a suitable one.",
                    "label": 0
                },
                {
                    "sent": "The different initial scalings of basis kernels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, two different results.",
                    "label": 0
                },
                {
                    "sent": "So we want to find a more reasonable, reasonable way to measure the goodness of kernel.",
                    "label": 0
                },
                {
                    "sent": "Some theoretical results say that the generalization error bound of kernel learning for both linear and nonlinear cases depends on the ratio between the margin and readings and minimal enclosing ball of data in the feature space motivated by this result, we present new scaling invariant formulations or corner learning.",
                    "label": 1
                },
                {
                    "sent": "Are we present the radiance of minimal, including full of data in the feature space and the first term in our objective function involves a large ratio between the margin and the readings of minimal enclosing ball in the second term is a huge loss.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some other formulations also consider the readings of minimal enclosing ball for corner learning.",
                    "label": 0
                },
                {
                    "sent": "However, the formulations.",
                    "label": 0
                },
                {
                    "sent": "Women's soft margin versions still have the scaling problems.",
                    "label": 1
                },
                {
                    "sent": "We prove that our formulation formulation is invariant to scaling of kernels in both linear and nonlinear cases, and in linear combination.",
                    "label": 1
                },
                {
                    "sent": "Kernel cases are formulation formulation is also invariant to normal construct.",
                    "label": 1
                },
                {
                    "sent": "On the combination coefficients.",
                    "label": 0
                },
                {
                    "sent": "And also invariant to this initial scalings of basis kernels.",
                    "label": 0
                },
                {
                    "sent": "So in our proposed kernel learning problem, the main difficult difficulty is to how to optimize, optimize, optimize it.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show that our proposed kernel learning problem can be transformed to a tree level optimization problem.",
                    "label": 1
                },
                {
                    "sent": "The objective function itself is a bilevel optimal value function, so the desk is theorem which is widely used in previous kernel learning methods is not enough.",
                    "label": 1
                },
                {
                    "sent": "Here we propose a new theorem of the differential ability of multi level optimal optimal value functions, and by applying our theorem we show that.",
                    "label": 0
                },
                {
                    "sent": "The objective function of our proposed kernel learning problem is differentiable, and we use standard gradient based algorithm for our kernel learning problem and experiments validates that our method outperforms other state of art corner learning approaches.",
                    "label": 1
                },
                {
                    "sent": "In most benchmarking sites and welcome to our poster.",
                    "label": 0
                }
            ]
        }
    }
}