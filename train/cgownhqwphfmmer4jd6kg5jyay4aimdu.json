{
    "id": "cgownhqwphfmmer4jd6kg5jyay4aimdu",
    "title": "Parallel Gibbs Sampling for Hierarchical Dirichlet Processes via Gamma Processes Equivalence",
    "info": {
        "author": [
            "Dehua Cheng, Computer Science Department, University of Southern California"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_cheng_gibbs_sampling/",
    "segmentation": [
        [
            "Hello everyone, my name is Dasha and I'm here to talk about our work on the parallel sampling algorithm.",
            "This is joint work with my advisor, yellow and."
        ],
        [
            "The topic model is for example the letter individually allocation and the hierarchical direction process.",
            "Is there are powerful models that can handle the rich relational structures in the data.",
            "However, they are not able to.",
            "Handle the large scale of data.",
            "And in this big data error, this actually caused some serious issues.",
            "On the other hand, the parallel computing paradigm is have been the most promising solutions to address this type of issues and.",
            "Therefore, the power difference algorithm are worth investigating.",
            "Today we're going to focus on the parallel sampling algorithms for LBA and HTP and compare with variational inference algorithms.",
            "I think the sampling algorithm usually provides a more accurate inference results."
        ],
        [
            "There are a lot of remarkable works to improve the naive sampling algorithms.",
            "For example, the remarkable collapsed Gibbs sampler, which they integrate out some of the variables to improve the mixing rate of the chain, and also there are several parallel sampling algorithms for parallel sampling algorithms.",
            "It's very important to explore the conditional independency in the model and for example based on the.",
            "Graph coloring scheme.",
            "There is.",
            "There's a simple chromatic parallel Gibbs sampler.",
            "And also there are some hog wild type of parallel center that they actually ignored some of the dependency between the variables.",
            "A more elegant approach is trying to introduce new auxiliary variables to create additional conditional independency to help with parallel sampling.",
            "For example, there's a remarkable work by Williamson and they have proposed an exact parallel sampling algorithm for HDP.",
            "Where they actually introduced the auxiliary variable for each word in the document and conditioned on those words, the model becomes several independent HTTPS.",
            "So that means that we can use the existing sampling algorithm for HTP to perform a parallel sampling.",
            "When we look back at this existing approaches, we can see there's the tradeoff there.",
            "If we want to improve the mixing rate, usually we need to do variable elimination.",
            "And this elimination operation usually will introduce new clicks, which is work against the parallel sampling.",
            "On the other hand, if you want to introduce new auxiliary variable to create new rooms for designing the parallel sampling algorithm.",
            "But this will most likely to harm the mixing rates.",
            "Whether we are trying to do is we're trying to avoid this type of trade off and our approach is.",
            "We will study an equivalent model where the will replace the triturate multinomial hierarchy in the original model by gamma person hierarchy.",
            "This new equivalent model is actually a minor variation to the model called negative binomial process.",
            "And what it's done?",
            "It's instead of adding new variables or deleting the existing variable.",
            "We replaced some of the variables.",
            "And we are hoping that this this different perspective will lead to an efficient parallel sampling algorithm."
        ],
        [
            "Now let's take a look at the equivalent model.",
            "Here is only a substructure of the original model, and we only look at the one document now.",
            "Tehran is the length of this document and X or the observation.",
            "The word of this document and Z is the topic assignment for the world and \u03c0 is the topic vector.",
            "What we do next is we will pre group the word bias topics know here you can see that in sub K is actually the number of word that belongs to topic K in this document.",
            "And after we have this in sub K, we can generate the word for topic K all at once.",
            "And note that given the length of the document an and given the topic vector Pi, this in some K actually follows a multinomial distribution.",
            "And also the topic vector Pie itself follows the rich late distribution.",
            "And this is Richard multinomial hierarchy we are talking about.",
            "What we do is we will replace this hierarchy with the gamma person hierarchy.",
            "And now \u03c0 prime is a gamma random variable, and we can reconstruct the pie pie vector by normalizing over this \u03c0 prime.",
            "The equivalency is established by the following fact that first for directed random variable we can obtain it by a normalizing several independent gamma random variables.",
            "And also if we have a bunch of random variable which condition on their sum, it actually becomes a multinomial distribution.",
            "What we have for this equipment model is that there's more conditional independency, especially between the topics as shown by this comparison on the right hand side on the left hand side, it's actually.",
            "A different view of the original model where we actually drop the plate representation and we group all the observations together and we write out those vectors explicitly."
        ],
        [
            "On the right hand side, it's equivalent model and we can see that there is indeed this core advantage that equivalent model have more conditional independency, especially between the topics.",
            "And this is great for parallel inference.",
            "And also since we only replace the sum of the variable, there's a no inflation on a number of variables.",
            "This is quite may be good for the mixing rates.",
            "And to be optimistic, we can see that by this transformation we get the conditional independence for free.",
            "And we're hoping to get a parallel sampling algorithm that can update the parameters for each topics for each topics in parallel.",
            "However, that's just too good to be true, and there's some bad news ahead."
        ],
        [
            "The first one is after we observe the documents.",
            "Some restrictions, for example all in sub K are linked together now because they need some to the length of the document N which is just a fixed number.",
            "When we see the observation.",
            "And also.",
            "There's a more serious issue because recall that when I describe the equivalent model with that, we generate the word for each topics all at once.",
            "This actually implies that the word in the equivalent model are pre grouped by the topics, which is definitely not true in the real word documents.",
            "And this group information is far from trivial, and basically it's.",
            "It's a goal of our inference task.",
            "So it's impossible to obtain it before hand.",
            "So with this equivalent model, those difficulties does not despair.",
            "They just hiding somewhere else.",
            "And Luckily we have some effective approach to handle these two bottlenecks."
        ],
        [
            "Recall that those two bottlenecks are all coming from the observations.",
            "So instead of doing sampling directly on the operation itself, we are trying to sampling from the empirical distribution of the of the observation and what we do is we will create a new document by resampling on the original documents.",
            "And since the new document the length of the new document are flexible, so we have observed the length, the length of the document that's relaxing the first bottleneck.",
            "And also simultaneously we will assign the topic to the word we sampled.",
            "And so now we will also have the group information, and therefore we have relaxed both bottlenecks or at once, and more specifically we trying trying to do is that for each, for each document we will create a collection of word for each topics and we're trying to update those collections and their parameters in parallel.",
            "And the result is a sampling algorithm is actually an approximate parallel sampling algorithms and approximation are coming from two sources.",
            "One is there some approximation in the resampling process and also we have applied the upper bound on the number of topics."
        ],
        [
            "Here, let me describe the key step of our approach, which is resampling and simultaneously topic assignment.",
            "Let's take one document and topic K as example.",
            "As initial stage we will store the original document in the stack and then we will update those collections by Atul.",
            "Proposed updates one is the add a word to this collection.",
            "Others delete a word from this collection and there's two update with.",
            "Equal probability.",
            "When we add word to this collection, will first pop word from the stack and trying to add it to this collection.",
            "The acceptance rate is calculated similar as in the reversible jump MCMC.",
            "If it fails, we just push it back to the stack.",
            "I wouldn't delete in a word, it's just."
        ],
        [
            "Similar.",
            "We will randomly choose the work from the collection and just trying to delete it.",
            "Other variables are quite easy to update because most of them has contributed priors."
        ],
        [
            "Here is a overview of our algorithm.",
            "For each, for each topics we will first fix those fixed other parameters and focusing on update those individual collections.",
            "And this is also updating the FK for them.",
            "After doing this for several rounds and for all the documents we will fix this collections and update the other parameters.",
            "For example the Theta Pi prime an orphan.",
            "And then start a new iteration.",
            "You can see that in the outer loop, our population is actually over the number of topics, which is quite different than the other product sampling approaches there.",
            "Parallelization is usually over over documents, which is actually better because usually we have more documents, but Fortunately we can also do this parallel update over documents in the inner loop."
        ],
        [
            "Now let's see some experiment experimental results.",
            "This is on the NIPS test set which is small scale datasets with just over 2000 documents.",
            "We choose this data set so that all the baselines can converge within a reasonable amount of time.",
            "And this table contains a convergence time and perplexity in the end.",
            "We see that thing is actually a hog wild type of parallel sampling algorithm.",
            "You can see that although it's a convergence time, speedup scales quite well with the increase of the number of processors.",
            "But it gives more and more inaccurate inference results.",
            "In contrast, our algorithm GPP actually scales well in terms of both accuracy and speed up."
        ],
        [
            "We also experimented on the larger data set.",
            "This is a New York time data set and our algorithm gives better results within the given amount of time.",
            "And also we have experimented on Bitcoin blog data set we collected and here are the learned latent topics.",
            "You can see that's actually quite interpretable.",
            "For example, the topic one actually corresponding to the introduction or description of the Bitcoin.",
            "And the topic story is actually the reaction of Chinese government towards the rise of Bitcoin."
        ],
        [
            "I think in conclusion, based on this different perspective, our algorithm actually provides a better tradeoff between the scalability, speed and accuracy.",
            "And as the future work, I think that it's good to design some smarter scheduling techniques to further improve the scalability.",
            "And also it's quite natural.",
            "Think about modified this algorithms for online learning scenario.",
            "And indeed, we think that's worthwhile to investigating other similar type of model to apply this type of approaches.",
            "And."
        ],
        [
            "Thank you everyone and I can't."
        ],
        [
            "Questions now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Dasha and I'm here to talk about our work on the parallel sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my advisor, yellow and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The topic model is for example the letter individually allocation and the hierarchical direction process.",
                    "label": 0
                },
                {
                    "sent": "Is there are powerful models that can handle the rich relational structures in the data.",
                    "label": 0
                },
                {
                    "sent": "However, they are not able to.",
                    "label": 0
                },
                {
                    "sent": "Handle the large scale of data.",
                    "label": 0
                },
                {
                    "sent": "And in this big data error, this actually caused some serious issues.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, the parallel computing paradigm is have been the most promising solutions to address this type of issues and.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the power difference algorithm are worth investigating.",
                    "label": 0
                },
                {
                    "sent": "Today we're going to focus on the parallel sampling algorithms for LBA and HTP and compare with variational inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "I think the sampling algorithm usually provides a more accurate inference results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are a lot of remarkable works to improve the naive sampling algorithms.",
                    "label": 0
                },
                {
                    "sent": "For example, the remarkable collapsed Gibbs sampler, which they integrate out some of the variables to improve the mixing rate of the chain, and also there are several parallel sampling algorithms for parallel sampling algorithms.",
                    "label": 1
                },
                {
                    "sent": "It's very important to explore the conditional independency in the model and for example based on the.",
                    "label": 0
                },
                {
                    "sent": "Graph coloring scheme.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 1
                },
                {
                    "sent": "There's a simple chromatic parallel Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "And also there are some hog wild type of parallel center that they actually ignored some of the dependency between the variables.",
                    "label": 0
                },
                {
                    "sent": "A more elegant approach is trying to introduce new auxiliary variables to create additional conditional independency to help with parallel sampling.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a remarkable work by Williamson and they have proposed an exact parallel sampling algorithm for HDP.",
                    "label": 0
                },
                {
                    "sent": "Where they actually introduced the auxiliary variable for each word in the document and conditioned on those words, the model becomes several independent HTTPS.",
                    "label": 0
                },
                {
                    "sent": "So that means that we can use the existing sampling algorithm for HTP to perform a parallel sampling.",
                    "label": 0
                },
                {
                    "sent": "When we look back at this existing approaches, we can see there's the tradeoff there.",
                    "label": 0
                },
                {
                    "sent": "If we want to improve the mixing rate, usually we need to do variable elimination.",
                    "label": 0
                },
                {
                    "sent": "And this elimination operation usually will introduce new clicks, which is work against the parallel sampling.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you want to introduce new auxiliary variable to create new rooms for designing the parallel sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "But this will most likely to harm the mixing rates.",
                    "label": 0
                },
                {
                    "sent": "Whether we are trying to do is we're trying to avoid this type of trade off and our approach is.",
                    "label": 1
                },
                {
                    "sent": "We will study an equivalent model where the will replace the triturate multinomial hierarchy in the original model by gamma person hierarchy.",
                    "label": 0
                },
                {
                    "sent": "This new equivalent model is actually a minor variation to the model called negative binomial process.",
                    "label": 0
                },
                {
                    "sent": "And what it's done?",
                    "label": 0
                },
                {
                    "sent": "It's instead of adding new variables or deleting the existing variable.",
                    "label": 0
                },
                {
                    "sent": "We replaced some of the variables.",
                    "label": 0
                },
                {
                    "sent": "And we are hoping that this this different perspective will lead to an efficient parallel sampling algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's take a look at the equivalent model.",
                    "label": 1
                },
                {
                    "sent": "Here is only a substructure of the original model, and we only look at the one document now.",
                    "label": 0
                },
                {
                    "sent": "Tehran is the length of this document and X or the observation.",
                    "label": 0
                },
                {
                    "sent": "The word of this document and Z is the topic assignment for the world and \u03c0 is the topic vector.",
                    "label": 0
                },
                {
                    "sent": "What we do next is we will pre group the word bias topics know here you can see that in sub K is actually the number of word that belongs to topic K in this document.",
                    "label": 0
                },
                {
                    "sent": "And after we have this in sub K, we can generate the word for topic K all at once.",
                    "label": 0
                },
                {
                    "sent": "And note that given the length of the document an and given the topic vector Pi, this in some K actually follows a multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "And also the topic vector Pie itself follows the rich late distribution.",
                    "label": 0
                },
                {
                    "sent": "And this is Richard multinomial hierarchy we are talking about.",
                    "label": 0
                },
                {
                    "sent": "What we do is we will replace this hierarchy with the gamma person hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And now \u03c0 prime is a gamma random variable, and we can reconstruct the pie pie vector by normalizing over this \u03c0 prime.",
                    "label": 0
                },
                {
                    "sent": "The equivalency is established by the following fact that first for directed random variable we can obtain it by a normalizing several independent gamma random variables.",
                    "label": 0
                },
                {
                    "sent": "And also if we have a bunch of random variable which condition on their sum, it actually becomes a multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "What we have for this equipment model is that there's more conditional independency, especially between the topics as shown by this comparison on the right hand side on the left hand side, it's actually.",
                    "label": 0
                },
                {
                    "sent": "A different view of the original model where we actually drop the plate representation and we group all the observations together and we write out those vectors explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the right hand side, it's equivalent model and we can see that there is indeed this core advantage that equivalent model have more conditional independency, especially between the topics.",
                    "label": 0
                },
                {
                    "sent": "And this is great for parallel inference.",
                    "label": 1
                },
                {
                    "sent": "And also since we only replace the sum of the variable, there's a no inflation on a number of variables.",
                    "label": 0
                },
                {
                    "sent": "This is quite may be good for the mixing rates.",
                    "label": 0
                },
                {
                    "sent": "And to be optimistic, we can see that by this transformation we get the conditional independence for free.",
                    "label": 1
                },
                {
                    "sent": "And we're hoping to get a parallel sampling algorithm that can update the parameters for each topics for each topics in parallel.",
                    "label": 0
                },
                {
                    "sent": "However, that's just too good to be true, and there's some bad news ahead.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one is after we observe the documents.",
                    "label": 1
                },
                {
                    "sent": "Some restrictions, for example all in sub K are linked together now because they need some to the length of the document N which is just a fixed number.",
                    "label": 1
                },
                {
                    "sent": "When we see the observation.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                },
                {
                    "sent": "There's a more serious issue because recall that when I describe the equivalent model with that, we generate the word for each topics all at once.",
                    "label": 0
                },
                {
                    "sent": "This actually implies that the word in the equivalent model are pre grouped by the topics, which is definitely not true in the real word documents.",
                    "label": 1
                },
                {
                    "sent": "And this group information is far from trivial, and basically it's.",
                    "label": 0
                },
                {
                    "sent": "It's a goal of our inference task.",
                    "label": 0
                },
                {
                    "sent": "So it's impossible to obtain it before hand.",
                    "label": 0
                },
                {
                    "sent": "So with this equivalent model, those difficulties does not despair.",
                    "label": 0
                },
                {
                    "sent": "They just hiding somewhere else.",
                    "label": 0
                },
                {
                    "sent": "And Luckily we have some effective approach to handle these two bottlenecks.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recall that those two bottlenecks are all coming from the observations.",
                    "label": 0
                },
                {
                    "sent": "So instead of doing sampling directly on the operation itself, we are trying to sampling from the empirical distribution of the of the observation and what we do is we will create a new document by resampling on the original documents.",
                    "label": 1
                },
                {
                    "sent": "And since the new document the length of the new document are flexible, so we have observed the length, the length of the document that's relaxing the first bottleneck.",
                    "label": 0
                },
                {
                    "sent": "And also simultaneously we will assign the topic to the word we sampled.",
                    "label": 0
                },
                {
                    "sent": "And so now we will also have the group information, and therefore we have relaxed both bottlenecks or at once, and more specifically we trying trying to do is that for each, for each document we will create a collection of word for each topics and we're trying to update those collections and their parameters in parallel.",
                    "label": 1
                },
                {
                    "sent": "And the result is a sampling algorithm is actually an approximate parallel sampling algorithms and approximation are coming from two sources.",
                    "label": 0
                },
                {
                    "sent": "One is there some approximation in the resampling process and also we have applied the upper bound on the number of topics.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, let me describe the key step of our approach, which is resampling and simultaneously topic assignment.",
                    "label": 0
                },
                {
                    "sent": "Let's take one document and topic K as example.",
                    "label": 0
                },
                {
                    "sent": "As initial stage we will store the original document in the stack and then we will update those collections by Atul.",
                    "label": 0
                },
                {
                    "sent": "Proposed updates one is the add a word to this collection.",
                    "label": 1
                },
                {
                    "sent": "Others delete a word from this collection and there's two update with.",
                    "label": 0
                },
                {
                    "sent": "Equal probability.",
                    "label": 0
                },
                {
                    "sent": "When we add word to this collection, will first pop word from the stack and trying to add it to this collection.",
                    "label": 1
                },
                {
                    "sent": "The acceptance rate is calculated similar as in the reversible jump MCMC.",
                    "label": 0
                },
                {
                    "sent": "If it fails, we just push it back to the stack.",
                    "label": 1
                },
                {
                    "sent": "I wouldn't delete in a word, it's just.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similar.",
                    "label": 0
                },
                {
                    "sent": "We will randomly choose the work from the collection and just trying to delete it.",
                    "label": 1
                },
                {
                    "sent": "Other variables are quite easy to update because most of them has contributed priors.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a overview of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "For each, for each topics we will first fix those fixed other parameters and focusing on update those individual collections.",
                    "label": 0
                },
                {
                    "sent": "And this is also updating the FK for them.",
                    "label": 0
                },
                {
                    "sent": "After doing this for several rounds and for all the documents we will fix this collections and update the other parameters.",
                    "label": 0
                },
                {
                    "sent": "For example the Theta Pi prime an orphan.",
                    "label": 0
                },
                {
                    "sent": "And then start a new iteration.",
                    "label": 0
                },
                {
                    "sent": "You can see that in the outer loop, our population is actually over the number of topics, which is quite different than the other product sampling approaches there.",
                    "label": 0
                },
                {
                    "sent": "Parallelization is usually over over documents, which is actually better because usually we have more documents, but Fortunately we can also do this parallel update over documents in the inner loop.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see some experiment experimental results.",
                    "label": 1
                },
                {
                    "sent": "This is on the NIPS test set which is small scale datasets with just over 2000 documents.",
                    "label": 0
                },
                {
                    "sent": "We choose this data set so that all the baselines can converge within a reasonable amount of time.",
                    "label": 0
                },
                {
                    "sent": "And this table contains a convergence time and perplexity in the end.",
                    "label": 0
                },
                {
                    "sent": "We see that thing is actually a hog wild type of parallel sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can see that although it's a convergence time, speedup scales quite well with the increase of the number of processors.",
                    "label": 1
                },
                {
                    "sent": "But it gives more and more inaccurate inference results.",
                    "label": 0
                },
                {
                    "sent": "In contrast, our algorithm GPP actually scales well in terms of both accuracy and speed up.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also experimented on the larger data set.",
                    "label": 0
                },
                {
                    "sent": "This is a New York time data set and our algorithm gives better results within the given amount of time.",
                    "label": 0
                },
                {
                    "sent": "And also we have experimented on Bitcoin blog data set we collected and here are the learned latent topics.",
                    "label": 1
                },
                {
                    "sent": "You can see that's actually quite interpretable.",
                    "label": 0
                },
                {
                    "sent": "For example, the topic one actually corresponding to the introduction or description of the Bitcoin.",
                    "label": 0
                },
                {
                    "sent": "And the topic story is actually the reaction of Chinese government towards the rise of Bitcoin.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think in conclusion, based on this different perspective, our algorithm actually provides a better tradeoff between the scalability, speed and accuracy.",
                    "label": 1
                },
                {
                    "sent": "And as the future work, I think that it's good to design some smarter scheduling techniques to further improve the scalability.",
                    "label": 0
                },
                {
                    "sent": "And also it's quite natural.",
                    "label": 1
                },
                {
                    "sent": "Think about modified this algorithms for online learning scenario.",
                    "label": 0
                },
                {
                    "sent": "And indeed, we think that's worthwhile to investigating other similar type of model to apply this type of approaches.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you everyone and I can't.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions now.",
                    "label": 0
                }
            ]
        }
    }
}