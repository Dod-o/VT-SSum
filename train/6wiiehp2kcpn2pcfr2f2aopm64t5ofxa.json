{
    "id": "6wiiehp2kcpn2pcfr2f2aopm64t5ofxa",
    "title": "Similarity-Based Classifiers: Problems and Solutions",
    "info": {
        "author": [
            "Maya Gupta, Department of Electrical Engineering, University of Washington"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_gupta_sbcps/",
    "segmentation": [
        [
            "OK.",
            "So this is the this is work in progress.",
            "We've worked out for a couple of years, will continue working on you can see it's work in progress, 'cause we're still looking for a better picture of Alera Hemi here.",
            "And before I start, I want to thank the organizers.",
            "I think they put together really nice selection of speakers and it's been very nice for the organized things."
        ],
        [
            "OK, so we're used to solving classification problems by starting with some Euclidean features, but a different paradigm is to say what if we just start by knowing some similarities, and this may be a very practical approach.",
            "I think this is a little bit more common to what we do as humans is.",
            "If we were given a problem like this to classify who painted this and we have some training samples from various painters, probably least in our heads, we don't derive Euclidean features and then try to do this.",
            "We just have some some notion almost Oracle like of how similar.",
            "These paintings are."
        ],
        [
            "So the the basic setup for this classification problem is that instead of assuming you have Euclidean features, we're going to assume that you have some training samples, but that the space they live in is some abstract space.",
            "So it might be the space of all paintings.",
            "It might be the space of all World Wars, the space of all car bombs, whatever.",
            "It could be very abstract.",
            "And then we're going to assume that for each of these training samples, you have a corresponding class label.",
            "So in this example of paintings, this might be the painter, and we'll assume your class labels from large.",
            "Some large class and that you have N training samples.",
            "OK, so not so."
        ],
        [
            "Prizing Lee in some letter based learning, there's a similarity function, but we're going to call it an underlying similarity based function because you might not actually know it.",
            "It may be just something that's out there that you can actually access what we assume that you do know, though, are the training similarities.",
            "So you have N training samples and we seem to have a matrix that send by N where each entry is the similarity between the training sample and the gate training sample, and we also assume since this is going to be a supervised problem that you have all the training class labels.",
            "So this just says if all these pictures you have these network of similarities and you can think of this as learning with a fully connected graph.",
            "OK, well will also."
        ],
        [
            "And that you have the test similarity.",
            "So if this is your test painting or test sample, you have all these lines that show you how similar they are to all of the end training samples.",
            "This is an end by one vector and will also give you the self similarity how similar this painting to itself and not all the algorithms will use that, but we'll assume that that's given.",
            "So then to summarize the problem that will be talking about today.",
            "Is to estimate the class label for some test sample X given this N by N matrix S of training samples similarities their labels YN by one vector and end by one vector of the test.",
            "Similarities to the training samples and the self similarity."
        ],
        [
            "OK, so it turns out that this sort of problem arises a lot in practice.",
            "We got into this problem because we were interested in imitating human judgment an in trying to mimic and use human judgment, and one of the ways it's easy and interface with humans is to ask them how similar things are rather than trying to ask them for features.",
            "But but these problems arise a lot in computational biology.",
            "It's often easier if you have primitives like proteins or DNA sequences or amino acids to say how similar two of them are in some possibly non metric way rather than trying to talk about some Euclidean features.",
            "Similarly, in computer vision there's been a lot of different similarities to look at objects or images.",
            "Such as the tangent distance, the earth movers distance, etc.",
            "I'll show you some results later with the Pyramid match kernel and also an information retrieval and natural language processing.",
            "Things like the cosine similarity with TF IDF vector."
        ],
        [
            "OK, so we've sort of recognized 5 distinct approaches to dealing with similarity based learning problems, and there may be other completely distinct approaches, and please go out and figure them out and let me know.",
            "One of the 1st and simplest things you could do is.",
            "You could say, well, I have all these points and I know their pairwise similarities.",
            "Maybe I could turn that into pairwise distances and I'll embed those points in a metric space using something like multi dimensional scaling.",
            "And then once once you embed them in a metric space, you can do all the standard machine learning stuff, because then you're good.",
            "OK, I'm going to argue this is sort of not the best way to approach this problem, and I think that will become more obvious as I talked about the other ways.",
            "But so basically we won't talk anymore in this talk about this, but it's certainly one approach.",
            "Another approach that may be intuitive.",
            "Some view is to treat similarities as kernels, and then you can pop that kernel into something like a support vector machine.",
            "So we'll talk a little bit about that and some of the challenges there.",
            "Another approach has been very popular this century.",
            "If you will is to think about the similarities themselves being features and I'll talk a little bit about the theoretical work there and we'll see experiments.",
            "Our most of our work has been these last two categories looking at K nearest neighbors, which is perhaps not the sexiest of methods, but it turns out that we're going to present were sort of some novel ideas there, and they can work very effectively, and then also in generative models.",
            "For similarities and the nice thing about both of these approaches is they produce output probabilities.",
            "OK, so we're going to start on the left here."
        ],
        [
            "The similarities as kernels and approaches."
        ],
        [
            "About talk about that.",
            "So can we treat similarities as kernels?",
            "Well, kernels are just inner products in some Hilbert space."
        ],
        [
            "So just to remind you, if you don't recall inner products, here's an example in the product.",
            "Probably most familiar with your product with two vectors.",
            "So if you had a vector X in a vector Z, it would look like this length here, so it's a scalar number.",
            "An inner products have some nice properties, the conjugate symmetric, they're real, they're linear, and their positive definite.",
            "If you have an inner product implies a norm and that norm then has to satisfy the triangle inequality.",
            "OK, so an inner product has sort of all these mathematical rules or regulations concerning it so.",
            "Any substance can we treat similarities?",
            "Kernels is really the question can we try?"
        ],
        [
            "Similarities as inner product.",
            "Well certainly if you give me an inner product it will seem like a similarity to me in my abstract human notion of what similarity means.",
            "Also, yeah, that inner product, you know acts like a similarity, but you can't really turn it around to say that always my notion of similarity has these mathematical properties of an inner product doesn't really hold, so let's look at an example."
        ],
        [
            "I don't know.",
            "Probably everyone who's been on the Amazon website.",
            "I don't know if you notice they keep some statistics when you view a book of what book people actually bought.",
            "So we can look at that and use that as a similarity, and in fact, I suspect that Amazon keeping track of this statistic to to be able to do things of this sort, so will say the similarity between a book and a book be is the percent that by bouquet after they viewed book, be on Amazon.",
            "OK, so here's for 96 books.",
            "We went out and we took this data.",
            "This is the similarity matrix S and you'll see there's a very strong self similarity here.",
            "So for the most part, this book is most similar to itself.",
            "People usually buy the book they were viewing, but you'll notice there's some holes like right here where for about book 67.",
            "Here people actually didn't buy it after they viewed it.",
            "Hardly ever, they tended to buy some of these other books instead.",
            "So is this in?"
        ],
        [
            "Product like, well, well, no Anan.",
            "One aspect isn't certainly not symmetric.",
            "So for example, if you viewed the elements of statistical learning then there's a certain probability that you went and bought the pattern recognition book and that was a 3%.",
            "On the other hand, if you viewed this, there was an 8% chance you ended up buying hasty chips running Friedman's book, so not symmetric, and we would expect it to be symmetric, right?",
            "We're not going to argue that it's really symmetric in our measurement, noise was just noisy.",
            "No, this is really going to be some asymmetry between, say Harry Potter and some other children's book.",
            "Or these different books.",
            "Oak."
        ],
        [
            "And we can.",
            "We can see how much of an inner product this isn't it.",
            "This isn't by looking at the eigenvalues.",
            "So here I've ranked the eigenvalues in order and this is just the size of them and this green line shows you the zero line.",
            "So remember, if there really was underlying inner product and we had this matrix and the matrix should be positive definite, all the eigenvalues should be real and positive.",
            "But in fact we've got a few eigenvalues here that are quite negative."
        ],
        [
            "OK, so S not really an inner product.",
            "That's not really a kernel matrix, but well, let's just make it anyways and we'll see how that works.",
            "Maybe it's not so bad to just approximate it and some people have suggested that they're sort of sort of noisy inner product, but I think that's philosophically not true.",
            "Does everyone know this?",
            "Some Greek myth about procrustes.",
            "He was sort of this evil person and travelers would come and they would see his cottage and it would be a rainy night so they would stop at his cottage and ask if they could stay and progressed to be like yes yes come on in.",
            "Come on in and he had one bed, the procrustes bed and it was a certain size and if you didn't fit you would just hack you off to fit or stretch you out.",
            "So this is sort of a procrustean approach to say well our similarity really isn't an inner product but let's just go ahead and make it that way.",
            "So how do we do that?",
            "How do we?",
            "How do we get rid of?",
            "Those negative eigenvalues?",
            "Well, first we're going to take our S and will symmetrise it, and then we'll look at the eigenvalues.",
            "So one of the things that you can do is just clip 'em.",
            "So if this is your original eigenvalue spectrum, you have these nasty negative eigenvalues.",
            "You just set them all to 0, so that's clipping.",
            "Now the nice thing about clipping is that if you think about the cone of PSD matrices and your matrix is out here, it's not in the cone.",
            "Clipping is like projecting it to the nearest PSD matrix in the Frobenius norm.",
            "So that's kind of a nice mathematical excuse for clipping."
        ],
        [
            "OK, another thing that people do that I find less intuitive is to flip the eigenvalues.",
            "You take these negative eigenvalues and you just flip 'em you change their sign and there's some various interpretations that don't make so much sense to me, but I think the easiest way to understand why this might be a good idea is that it's a similar effect.",
            "If you took a new S matrix by taking S transpose S, This is sort of like the square root of the eigenvalues that you would get here.",
            "So this is another way to get rid of your negativity."
        ],
        [
            "A third way, which sounds like it should be good.",
            "It turns out not to be.",
            "Is it just shift your eigenvalues up?",
            "You could say well, let me take my smallest eigenvalue and I'll just shift up all the eigenvalues, make them bigger, and this should only affect the self similarity.",
            "So it seems like it should work well, but I'll show you later it's not such a good idea.",
            "Oh, that's true.",
            "Actually it was a lot of table and a lot of results, and I did."
        ],
        [
            "I did to summarize it for you, which is should you flip, clip or shift in general over about 10 different datasets, we've seen that the best performance generally comes with Clip, and that's sort of the safest thing to do.",
            "So if you had to decide that, that would be a good aspect.",
            "But on many datasets it it turns out doesn't really matter.",
            "On some datasets it matters a lot."
        ],
        [
            "So that's motivated people to try to say well, is there sort of a best way?",
            "Can I learn the best kernel for a particular problem?",
            "And in particular, if you know that you're using SVM, there's been some approaches to learn the best kernel in that SVM context.",
            "So this, for example, is an approach that my student James will talk about it.",
            "I see Mail 2009, which is, you got your SVM primal here and now you've added a minimisation over the PSD code to find your kernel K. And then you say, well, this kernel K that you're going to find to use for SVM regularize it towards the S that you started with in the Frobenius norm an.",
            "For example, if you take this game of this regularization factor here and you hit it really strong, make it all the way asymptomatically going to Infinity.",
            "Then this becomes equivalent to clipping the matrix, but can give you some better performance, so he'll have a poster on that at the poster session here and or you can see it at I smell."
        ],
        [
            "OK, so that's sort of the similarities as kernel story like talk just briefly about similarities as features.",
            "This."
        ],
        [
            "Approach that says, well, we'd like to have a feature space 'cause we really know how to deal well with the Euclidean features.",
            "So what we can do is take the similarities between our test point and each training sample and we'll just call that N vector eh feature vector for our test sample X.",
            "So if you have N training samples, you now get an end dimensional feature space.",
            "If you get new training sample, then your features dimensional space just got a little bigger, so some of the early work on this was grapples work in 98 Leo Noble did this in 2003.",
            "Once you have this feature space, you can do anything with it, so these are people applied SPMS LP machines, some discriminant analysis.",
            "These tended not to really work any better than one KNN and less you had really noisy data and there's a potential support vector machine that at first seem like a different approach.",
            "But then if you analyze it a little, you figure out that it's just again working in this feature space with with both an L1 and L Infinity regularizer.",
            "So this seems like it's opening up the doors to cursor dimensionality, because as N goes, your dimension is also growing so.",
            "There's some questions about whether asymptomatically this is really going to work, whether it's going to converge.",
            "There's been a few different results.",
            "Our results suggest that you can make it work, but instead of choosing all N, you should choose some slow growing subset of N. For example, here you should choose log N of these guys rather than all end so that your future space is growing slower.",
            "Talk a little bit more about that later, but um, Nina Balcan and 83 Bowen Abraham Bloom have a number of results in that, and I think a couple of them might be talking tomorrow.",
            "So if you're interested in the theory, they may be talking about that tomorrow."
        ],
        [
            "OK, so let's look at some results.",
            "In this first problem on the left Amazon 47.",
            "Is classifying authors from that similarity.",
            "I showed you oral sonar is a problem where there was a.",
            "This was done at the Applied Physics lab.",
            "They took 100 sonar signals that are hard to classify whether they were targets or clutter, and they ask humans to listen to them and turns out that humans are very good at listening to sonar signals much better than our automated algorithms.",
            "So the humans told the organizers how similar they were and they were trying to figure out if these signals the machines were failing on the automatic algorithms were failing on.",
            "Could humans using similarity distinguish them?",
            "Caltech 101.",
            "This is the Caltech 101 classic data set.",
            "It's 101.",
            "Object recognition classes with about 10,000 samples.",
            "Here we use the pyramid match kernel as our similarity.",
            "The space rack is the face recognition data set, so there's 139 classes which are different people and we also use a positive definite similarity there.",
            "Miracca music similarity problem with 10 classes and voting is the standard UCI data set.",
            "It's whether people are voting Republican or Democrat or their voting records and then trying to classify them as Republican or Democrat and here we use the voting difference.",
            "So the value difference metric similarity, which has just some very tiny bit of negative eigenvalues.",
            "OK, so above the green line here is SVM where we clipped the matrix to make it a kernel and then below this are three different approaches to using the similar themselves as features.",
            "And one thing you'll see is that there's no real clear winners here.",
            "In general, the SVM clip does fine.",
            "There's a few cases where every algorithm does well, so the best in each case is bolded.",
            "So here the SVM clip did best in three of these cases.",
            "The SVM did best here.",
            "I lost this guy.",
            "He did best here and the SPM using RBF kernel did best here, so not a clear story out of that.",
            "You'll notice though, that there appears to be a hole in the table.",
            "Well that's because there's a hole in the table, so."
        ],
        [
            "I want to know that the SVM clip your fitting.",
            "You're feeling this whole space with one support vector machine and in 2006 some folks in computer vision.",
            "They got sick of training really large support vector machines on a lot of data and so they said well, could we just apply the support vector machine locally?",
            "So you'll take your test sample.",
            "You'll find some K nearest neighbors which might be a large number of K like 1024 and just run and S training SVM on that small data set.",
            "So sort of a local SVM.",
            "So this is nice because it's always feasible, right?",
            "'cause you can control the size, but you now have to find the Kenneth neighbors and it's sort of lazy.",
            "You have to wait to get test samples to train your classifier, but by doing things locally you can do a lot better on some datasets, so you'll notice by doing this SVM locally and again now locally you're clipping.",
            "This has gotten a little worse, but not much.",
            "It's still sort of in the ballpark of these other algorithms, but this is dropped from 81% error to 17.5% error.",
            "So in zanes paper, when they first promoted this, their results seem to indicate that on their datasets they did pretty much the same as training an SVM on the whole data set, and so was much more about a computational win here.",
            "Once in awhile we see this kind of big win, and we think that it's mostly because of the approximation that if you clip the entire N by N matrix, you probably really losing a lot of information.",
            "But if you just clip some small part of it, that small part you might not have to lose any information, and it may just be easier to fit the model locally.",
            "Then trying to fit one SPM alter the whole space.",
            "OK."
        ],
        [
            "Speaking about K nearest neighbor and local models will talk a little bit more about that and focus on on weights.",
            "So just a very."
        ],
        [
            "Quickly remind you about weighted nearest neighbors.",
            "You're going to take your test sample, find its canius neighbors, or whatever locality definition you want, give you to those neighbors some wait, and then take a weighted weighted average.",
            "So this is just the indicator function.",
            "Here you're summing up the way for each class, choose the class that gets the most weight.",
            "One of the nice aspects about this as it does sort of match with psychologists, at least some psychologists think we think that we have sort of an example model in our heads that we've seen some prototypical dog.",
            "We've seen a bunch of dogs, and then we try to try to match this."
        ],
        [
            "OK, another nice aspect of weighted nearest neighbors is that if you're positive weights and if the weights and no one, then you can just sum up the weights for each class to get an estimate of the class posterior.",
            "And also to emphasize again again, I think probabilities are very important in dealing with real systems, because often you really have these asymmetric costs you'd like to be able to interpret your results, and it's particularly for system integration, usually in real systems, your classifier is not the only thing going on, it's gotta get fused or integrated with some."
        ],
        [
            "Thing else.",
            "OK, so you might notice that we're from an electrical engineering Department and so we think about specs and design goals if you will.",
            "And So what are the design goals for choosing weights, particularly similarly based learning?",
            "Well, the first one I think."
        ],
        [
            "Is obvious and dates back about 50 years, which is that you might want your weight to be an increasing function of your similarity.",
            "So if this guy is more similar to him than this other near neighbor here, then even though they're on their neighbors, I'm going to give this guy more weight.",
            "The SEK."
        ],
        [
            "Design goal is a little more novel and a little more controversial, which is to say, let's say that this is my test step on.",
            "These are its neighbors.",
            "Well, Van Gogh painted a lot of sunflowers, and so you could argue that these sunflowers are really providing very correlated information about the feature space.",
            "There are sort of redundant if you will.",
            "Similarly, if this was an email problem, there's a lot of times the emails just you have a lot of forwards, and so a lot of emails will look very similar and probably provide correlated information.",
            "So to deal with these kind of redundancies."
        ],
        [
            "What we suggest is the diversity goal that you wait on sample.",
            "I should be a decreasing function of the similarity between XI and XJ, so the test sample doesn't show up in this goal.",
            "It just says that these two guys are really similar.",
            "Then they should.",
            "Then their weight should both get lowered.",
            "So it's sort of a way to say that all of these guys have to sort of share the weight that would have gone to a neighbor in this part of the space."
        ],
        [
            "OK, so how will we ever find weights that satisfy these goals?",
            "Well, Luckily this is pretty much what linear interpolation weights do for you, so just to review linear interpolation you if you have some training samples XI, you're going to put some weights on them, such as the equal X, and those were going to be positive and some new one, so linear interpolation is really great.",
            "You'll notice that when you when you go to sign these weights because they have to solve this, you need all the training samples at once.",
            "So this really is different than saying applying a Gaussian kernel or something where my weight is just some RBF function of XNXX.",
            "I hear all the XI are involved in choosing each way, so sort of a joint.",
            "Operation, that's how you get to the design goal too.",
            "But linear tribulation has some problems if we just try to apply to classification first.",
            "There might be a non unique solution.",
            "So for example if I have a test sample here and I solve for weights on X1 X 2X4, there's a lot of waiting possibilities that would satisfy the linear interpolation equations.",
            "I could put a halfway here halfway here or fourth, wait on all of them, etc."
        ],
        [
            "Another problem is that there's often no solution, right?",
            "So this is requiring me to find some weights such that X is the convex combination of the XI.",
            "But here there's no convex combination of these training samples.",
            "That's going to give me access.",
            "So in order to to do."
        ],
        [
            "With these problems, I suggested actually back in my thesis work some slight changes to make this possible for general classification problems.",
            "So when we solve for the line weights, linear interpolation with maximum entropy, there's now two terms.",
            "This first term here in the second term.",
            "And the first term says maybe we."
        ],
        [
            "Can't solve this, so we'll just change it to say make the left side here close to the right side in a square norm, so we'll just try to do the best we can, and then the second term here so that deals with the fact that they might."
        ],
        [
            "Be a solution and the second term deals with the nonuniqueness.",
            "It says well also minimize negative entropy sources.",
            "Maximize entropy of the weights, so that will always guarantee we have a solution 'cause it's convex regularizer.",
            "And because we're maximizing entropy of the weights if we didn't have anything here, the maximum entropy solution would be the uniform weight.",
            "So this is going to push our weights to all be equal, which I think makes sense.",
            "It says you know sort of give all your samples equal equal votes if you can.",
            "OK, the nice thing too about them."
        ],
        [
            "So it should be regularizer.",
            "Is that?",
            "This means that you wait to have an exponential solution and you don't necessarily want to solve for the exponential solution, but knowing that allows you to do some nice analysis because you know what the formula weights is otherwise.",
            "Otherwise you can't do that.",
            "So for example, we were able to show that this classifier with these weights would be consistent and then it falls a lot of large numbers, so you can get noise averaging OK, so this is all Euclidean stories, so we can find ways to satisfy our design goals in Euclidean space.",
            "What about similarity space?",
            "How do we do linear interpolation and similarity space?"
        ],
        [
            "So we're going to just turn allies it.",
            "So this is actually something I got to James notice.",
            "Is that if we take the square norm he ran, we expand it and just write things in matrix notation to make it a little clearer.",
            "And we're also going to change this regularizer for maximum entropy to be a Ridge regularizer in sort of an unrelated issue.",
            "Um?"
        ],
        [
            "This regulator is going to act very similarly to the maximum entropy regularizer.",
            "It's also going to push the weights to be uniform 'cause it's recognizing the variance.",
            "You can make a more direct story about minimizing the estimation variance.",
            "This way you lose some of those nice theoretical properties, or at least you can't show them 'cause you don't know what the form of this is going to be anymore, but this is going to be computationally more efficient."
        ],
        [
            "But the nice thing that lets you journalize this is that you've now got this linear interpolation problem just in terms of inner product, so these are the inner products between all of your training samples and the inner product between your test sample in your training samples.",
            "So as John Shaler mentioned in the SBM, 12 beginning of the week, as soon as you have things in inner product, you basically have the kernelized version of your algorithm.",
            "So you could replace this with kernels, kernel functions if you had them here we don't have kernel functions, we have similarity, so we're going to play some similarities as far as I know, I haven't seen any other kernelized linear interpolation.",
            "It seems odd that it would have been done before, so someone knows a reference.",
            "I'd love to have it 'cause I don't know which one reference."
        ],
        [
            "OK, so if we do that, so we replaced The xx transpose with Rs matrix here, hoping that it acts like a kernel and our test similarities here.",
            "And this leads us to what we call the Kernel Ridge interpolation weights.",
            "So we talked about those design goals.",
            "I told you linear interpolation, satisfied them.",
            "I didn't prove it.",
            "So let's just talk."
        ],
        [
            "How that shows up here.",
            "So first of all, that goal of Affinity you'd like if you're training samples similar to your test sample that gets more weight, so that shows up in these two terms here.",
            "This term by itself says minimize the weighted similarity.",
            "So if you only had this term, then your best bet would be to put all of your weight on your one nearest neighbor.",
            "But of course, that's not what you want to do.",
            "You want to spread the weight around an.",
            "Luckily you have this regularizer.",
            "Here this W transpose W, which says spread the weight around and so that'll force this term to show the weight with the other weights and other training samples."
        ],
        [
            "And then the second term here will enforce the diversity that we were looking for.",
            "So if we expand this out, we may have lost their pointer here.",
            "Can anyone see the red pointer?",
            "OK, so if we expand this matrix out we can see that the sum of the similarity between each training sample XI and XJ times WIWJ, and so we're trying to minimize this.",
            "This means that if the similar between XI and XJ goes up, that will end up having to try to push down W&WJ."
        ],
        [
            "OK, so if we could just put the similarities in here, we don't actually need to approximate them.",
            "If we do that, if we just put the."
        ],
        [
            "I'm leaving here without approximating them, and then we have a global optimization problem to solve, and we find that that works slightly.",
            "McGinley better than if we approximate things, but the global optimization can be very hard and take a long time to run."
        ],
        [
            "So in practice, we just approximate the S you make it PSD, and once you've done that then this becomes a quadratic program, which is awfully convenient.",
            "Even more convenient, these are box constraints, and so you end up with a problem that as an optimization problem, looks just like the SVM problem and your fast SVM solutions like the Smol algorithm can be applied here to make this very fast."
        ],
        [
            "OK, but if it's still not fast enough for you, you can remove the constraints on the weights.",
            "You say I don't need my weights to be positive.",
            "I don't need my.",
            "Well, you still want you with someone.",
            "You can remove the positive iti wasting your constraints and minimize the same problem.",
            "And you're going to close form solution looks like this and you can show this is equivalent to doing a local Ridge regression fit and then finding the class label.",
            "So that's sort of an old result.",
            "It's very easy to show.",
            "I didn't go through, but hopefully people know that local regression and waiting nearest neighborhood.",
            "They have these analog."
        ],
        [
            "OK, so let's just look at some example waits to see how this shows up in practice.",
            "So here's a similarity matrix S and it says that my training samples are all maximally similar to themselves and they have zero similarity to each other and they're similar to the test point goes with four 321.",
            "So obviously wait one should be the biggest and then on down, and we see that weight one is the biggest weight 2834.",
            "Here's the regularizer, so as the regularizer gets really really strong, then we have that Ridge penalty that says all the way.",
            "It should just be equal.",
            "So as a regularizer goes off to Infinity, all the weights converge to .25.",
            "So this is for the linear interpolation and this is when we take off the constraints and we have the Ridge regression and you can see that the weights wait for for little while is negative."
        ],
        [
            "OK, now the only thing we're changing, so now all of the training samples are equally similar to the test point, so they're all sort of on a hyper sphere around the test point if you will, and similarity space whatever that means.",
            "And these are training samples similarities and let's see here we can see that training sample, two training sample two is very similar.",
            "Training sample three.",
            "So change sample two very similar change sample through that for their so W2 and W3 should be a little a little weighted down because they're sort of giving us the same information.",
            "We think 'cause they're so highly highly correlated.",
            "And in fact, we see that W2 and W3 here for the care I weights are actually equal, and certainly for the care are weights are also they turn out to be equal from this formulation, which sort of makes sense.",
            "They end up sharing is actually the weight again because this regularizer the regulators is telling them to make the way."
        ],
        [
            "Is equals possible?",
            "And just the last example here with a little more arbitrary straining similarity matrix.",
            "A little arbitrary testimony matrix shows you that some kind of weird, unintuitive things can happen here.",
            "You'll see that wait for goes up and then back down.",
            "There is kind of a discontinuity here with weight 3 is for the linear interpolation and for the Ridge regression here.",
            "Wait, three actually gets bigger than wait, wait one at a certain point in the regularizer."
        ],
        [
            "OK, so this is a table with a lot of results and see if we can make this a little easier to read."
        ],
        [
            "Let's first focus on the K nearest neighbor methods, so these are four weighted nearest neighbor methods.",
            "The top is just your standard KNN.",
            "We cross validate the neighborhood size.",
            "The next one if any Canon is.",
            "If you just do design goal one, you just say if you're more similar to the test sample then you get more weight and then the KRIT and Care are are these two diversity weighting schemes.",
            "For the most part you see there's there's watcher naturally be statistically significant differences.",
            "The bold here in the whole table, whatever is bold, is the best in the table for that.",
            "Data set and everything that's not statistically significantly worse.",
            "So for example, here if any can end, does the best on the Amazon problem at at 15, but the QR is not any worse to 16.10 statistically, so the only big difference that we see it happens with the Caltech 101 data set.",
            "But this difference was actually kind of surprising, so if you do K nearest neighbors, you get 41% error.",
            "If you do a 50K NN, so can you.",
            "Haven't looked at all your data.",
            "50 kid and you look at your day.",
            "You see how similar is and you wait that an you win about 2%.",
            "So that really hardly helped you at all.",
            "You would have thought that waiting things by this similarly would help alot hardly helps at all in this case.",
            "And now if you add the diversity term you go down by about 25% in air and this is the statistically significantly lowest error we found all the training sets.",
            "So that was surprised that the diversity was mattering so much in that case.",
            "And it's also the largest data set we have for the second largest data set we have mirax were results might also be significant.",
            "So his 3000.",
            "Apples.",
            "Not a lot of differences between the algorithm, so sometimes it may not matter at all.",
            "It seems like in Caltech 101 there must be a lot of these sort of high correlation so that it matters so."
        ],
        [
            "This is just looking again at Caltech showing that it's at least 10% better.",
            "The next thing, which here is the SVM kernel with the clip approach."
        ],
        [
            "Another interesting thing to look at is I've broken up this up into local algorithms versus global algorithms, so these are all doing something on the K nearest neighbors.",
            "And again, most of the time there's not that much difference, but the Amazon data set really stands out.",
            "It was a very sparse similarity, and here you get about 80% error, 75% error here, and about 15 to 18% error with the local methods question.",
            "Fix that or did you go through several values for K it's cross validated is that is your question.",
            "Validation within the data.",
            "Range.",
            "This is for a mean value.",
            "So these are 20 random 20 randomizations where each randomization all of the samples are divided up in 80% training and a 20% test.",
            "Then the 80% training.",
            "There's a 10 fold cross validation from which we choose the K from some subset of possible Ki.",
            "Think K goes 124, eight, sixteen 3264.",
            "I think he goes up to 128 as the highest K possible is that answering your question.",
            "Oh, this is an average over those twenty runs, so yeah, sometimes they would have differed in those various runs.",
            "There are two other techniques on these days.",
            "So most of these datasets are similarity only, so this data set we created by going to the Amazon Web page, the oral sonar data set.",
            "I don't think there's any other results for they created this data set really to try to figure out if they could derive Euclidean features that matched what humans were doing.",
            "So this is also very recent work in 2006 or so.",
            "The Caltech 101.",
            "Here we ran it just like we ran all the other datasets, but the normal way to run Caltech 101 is a little different.",
            "There's some special thing that computer scientists do about how they divide up the classes or something, and we didn't want to do something different on one data set, so I can't make it comparison there.",
            "The face recognition the previous paper had just done KNN.",
            "So that would be this this 4.23%.",
            "That's some data from Hamid Krim in North Carolina.",
            "The mirax, I don't know.",
            "And the voting I think no matter what you do, you get around 5%.",
            "For different approaches."
        ],
        [
            "OK.",
            "So that was talking about how to do some good job waiting K nearest neighbors.",
            "The last thing I want to talk about is some approaches to generative models.",
            "So one approach, well, let's let's just."
        ],
        [
            "Everyone with generative models larger classifiers, so this is the idea that you've seen some data and now you're going to model the probability of that for each class.",
            "So you have some class conditional probability of what you've seen, and then you can choose the class that made that most likely.",
            "So hopefully people have some experience with him.",
            "Analysis Quite active malice, maybe Gaussian mixture models.",
            "And of course, a big advantage to general classifiers that they produce probabilities out that hopefully are somewhat accurate.",
            "But maybe it's not so."
        ],
        [
            "Imagine if they're not accurate.",
            "OK, our goal of well, let's back up one moment.",
            "So if."
        ],
        [
            "The one in a general classifier.",
            "One thing you could do is treat the similarities as features.",
            "And once you've changed the subject features or done MD S, You're in Euclidean space 'cause you have features and you can run any generative classifier you want.",
            "But if you think about, say, LDA where you're taking each class and modeling it by Gaussian.",
            "It's not so clear what those Gaussians mean.",
            "Like what does the center of that Gaussian mean?",
            "If the features are the similarity of these different training samples and what is the covariance mean?",
            "Also, in the previous work where they did that, the results were very similar to one nearest neighbor, so we didn't go that way.",
            "We wanted to do something a little different and see."
        ],
        [
            "We could directly model the similarities.",
            "But when we dropped them all those we found."
        ],
        [
            "We're not so robust, So what we ended up doing was modeling some descriptive statistics of the similarities, so we're assuming that the that everything that we need to know about the class is captured by some statistics T of S. And we've tried very statistics.",
            "It doesn't matter a lot, but the statistics we use are these centroids, statistics.",
            "So you have your similarities and we say what's the similarity of our test sample to a mean or prototype sample from class 1A?",
            "Mean or prototype sample from Class 2, etc.",
            "So this is a little weird because it's cross classes, so you're saying what's the probability of seeing the similarity between my test sample and Class 2 centroid, given that my test samples from Class G. So it's a little different than the qda models 'cause you have these different class conditionals to think about and you could find a centroid in various ways.",
            "If you really had access to the sample domain.",
            "Like if you had the set of all paintings then you could find sort of the best centroid for a class that maybe had maximum some similarity.",
            "Here we generally assume that all we have is the data were given.",
            "So for each class we take the training samples from that class and we try to find the training sample that had the maximum some similarity to the other training samples in that class.",
            "So here are centroid is limited to be from the actual data we've seen, so that makes it a little less ideal."
        ],
        [
            "OK, so I'm not going to walk you through the math because it's a whole lot of notation.",
            "Those watching it through the concepts instead, which is where we're trying to model this vector of similarities to our central class centroids.",
            "And there's now a G of these for each class G, so that's a lot.",
            "And to try to model that joint probability is very hard.",
            "So what we've done is just assume that each of those independent, which is probably not true, but like often naive Bayes, often works really well even when it's completely not true.",
            "So part of machine learning probably is just sort of that key of finding out what can you.",
            "What can you lie about, even though you know it's not true and still have effective algorithms, so we assume that these disjoint probability is really just the product of some marginals.",
            "And then we need to estimate each of those marginals.",
            "So one of those modules will look like the probability of seeing a similarity between your test sample Anna sample centroid sample from Class H. Given that you think your test samples from Class G. So to estimate this from the data, there's a few different things we could do.",
            "We went with the maximum entropy approach.",
            "We said really, all I trust is my mean similarity.",
            "I could see all the other samples from that class, how they, how the others training samples from Class G related to class mu H and I can take the mean sample of those, and I'll say that that's good, and then I'll try to find a distribution for this probability that has that mean.",
            "So there's lots and lots of distributions that have that mean, but I'll find the maximum entropy distribution.",
            "That has that mean.",
            "So, not surprisingly, whenever you assume maximum entropy and you have the right kind of constraints, you end up with an exponential.",
            "So another way to think about this model is just that we modeled this distribution as an exponential and then fit the maximum likelihood mean.",
            "So either way, you get the same the same model.",
            "Just sort of.",
            "Depends on what you're more comfortable assuming or what perspective you want to take.",
            "So that gives you this SDA model so you know just sort of modeling for each test sample.",
            "How similar to these different class centroids?",
            "So that works pretty well, but like something like UDA that has a lot of model bias because you're assuming just one centroid represent a whole class and maybe that's very poor assumption.",
            "So this is the same problem people face with like UDA where you have one Gaussian represent a whole class and the usual solution is to go with a mixture so people often use Gaussian mixture models.",
            "Probably the most novel part of this algorithm.",
            "Was to say wow, we don't want to do a mixture, and in fact later we did a mixture and it didn't.",
            "It didn't work very well, but first we just we were just really lazy.",
            "We didn't want to do a mixture so we said how else can we reduce the model bias?",
            "Because the models too rigid so we decided to just apply the model locally.",
            "So this is a similar in many ways to applying the SVM locally but sort distinct work.",
            "So we take our SDM model here.",
            "But we only calculated for each neighborhood.",
            "So now you get your test sample X.",
            "You find some K nearest neighbors and then for those K nearest neighbors you train these exponential probabilities of what your centroids are in that little neighborhood and what the probability of the samples at each centroid.",
            "So that gets you a local SDA model.",
            "Works pretty well.",
            "There's a 2007 paper on that, and then in more recent work we realize that sometimes this model had some trouble because they were in your neighborhood.",
            "There would only be 1 sample from a class, and so you wouldn't really be able to find the centroid Infinity model because your model would be just sort of this Dirac Delta at whatever probability the one training sample from that class was to its centroid.",
            "So we need to reduce the estimation variance we were having estimation variance problems, and so obviously the solution is to do some sort of.",
            "Azatian we'd like to do a Bayesian solution, but because these are discrete domain of similarities that doesn't workout so well mathematically.",
            "But we think the regularization works pretty well, so we regularize over the different localities to regularize these different local fit distributions towards each other.",
            "OK, so that will show up in few."
        ],
        [
            "And I'm not going to go through the whole table results because we have results on different datasets and the other datasets, and so it's kind of a mess.",
            "But the summary is that it's competitive with other methods I've shown you.",
            "Generally it works just a little better than K nearest neighbor and then on some datasets it works really well and is the best classifier, so it's probably a good classifier if you were trying to build a classifier and sambal to include 'cause maybe it has some key ideas and it has a very different perspective than the other classifiers we talked about."
        ],
        [
            "OK, I'm going to move into some conclusions here.",
            "So first of all, I've talked about these sort of five different approaches to thinking about similar based learning problems.",
            "We found that performance in practice really depends a lot on the data set, and because these are similarity datasets where there's, there's no guarantee with that similarity is, I think that makes sense."
        ],
        [
            "We found that by adding the diversity waiting into awaited Kenya's neighbor, this generally worked well.",
            "It never really failed an once in awhile like on the Caltech 101 data set, you got a big gain, much bigger than we expected."
        ],
        [
            "Local SDA works pretty well, but our preliminary results on regular local SDA is that it's a.",
            "It's a much more competitive classifier and probably practically useful."
        ],
        [
            "In general, again, I would argue that probabilities are useful in and I'm always very, very sad when when algorithms don't produce probabilities because it means that it's a lot harder.",
            "Usuman in practical systems.",
            "And the other key."
        ],
        [
            "Inclusion here is that local models are useful and that there are more useful than they would be in Euclidean spaces, because here we've got these issues of having to approximate things as kernels, and so if you only locally approximating them, hopefully the approximations are not as bad, and that you're similarly spaces.",
            "This kind of weird big space that has a total space.",
            "It may be very hard to model, but that locally, just as functions tend to be locally linear and locally well behaved, even your weird similarity space maybe locally well behaved, and you know, there's also some questions about whether there's an underlying manifold here.",
            "The other nice aspect about using local models, and this is true in Euclidean as well is that they are always feasible.",
            "You do have to find your nearest neighbors, but then you can do matrix inversions or kernels, and it's always pretty small."
        ],
        [
            "OK, I just want to finish with a few of the open questions.",
            "This is a pretty recent area of studying machine learning and on the one hand there there's been a lot of really good work.",
            "On other hand, there's quite a few open questions that are still left if people are interested.",
            "One is really just this issue of making SP SD, Flipclip and shifter really unsatisfying.",
            "They don't take into account the fact that you're trying to solve a classification problem, so there's sort of agnostic to what the task at hand.",
            "There's been a little work.",
            "Anne Anne doing this better and as I said, we have a nice email paper about this and you can talk to us afterwards if you're interested, but I think there's more good ideas to be had to deal with that then.",
            "Secondly, talk about fast Canon.",
            "So as as I suggested, local approaches to these problems seem to be really important, but that means you have to find your K nearest neighbors.",
            "Well, if you're Nickelodeon space, you can put together a KD tree of all tree.",
            "You can have this sort of preprocessing structure make that really fast, but those structures require.",
            "Things like concepts of medians, concepts of the triangle inequality.",
            "So if you don't have that, how can you do fast Canon search?",
            "As far as I know, it's only been one paper in this area, but you will if it's looking at fast cannon search and what he did was make some assumptions about the similarities, not as much as triangle inequality, but but some assumptions to try to get some tract ability.",
            "It would be interesting to see if you start approximating things.",
            "Can you still find that the K nearest neighbors within some closeness or within some approximation?",
            "We see very little work on similarly based regression.",
            "I think on the one had a lot of the ideas and concepts will be the same, but it'll be interesting to see how things really work out, and particularly with these issues of trying to approximate PSD matrices etc.",
            "In classification you just make it binary decisions so you can get away with a lot and maybe you can't get away with the same things.",
            "Or maybe you start to see differences that are important when you go the regression paradigm.",
            "OK, so everything we've been doing is similar.",
            "Based learning is really like learning on a fully connected graph, and there's been a lot of work on learning on graphs in general.",
            "A lot of it's been semi supervised and not supervised, and there's sort of a little bit of disconnect between these two communities where there hasn't been a lot of flow of the ideas and then understanding between the communities, so I think there's some work there to think about whether the algorithms people have developed and the concepts were learning on graphs applied to the general summary based learning problem, and the other way around.",
            "Then this point here, as we saw the performance really depends on the data set.",
            "We have about 10, maybe 12 datasets.",
            "Now that we've collected, and I'll give you the web page for the repository.",
            "But really there needs to be people going on trying this stuff out and coming back with some.",
            "Some reports of what works and what's important.",
            "Uh, another interesting area and I know this is interesting 'cause the the DoD just gave me a large grant in this area.",
            "So so obviously someone with at least a checkbook thinks is interesting.",
            "Is fusing similarities with Euclidean features.",
            "So assuming that you have both some similarity information about your training samples but also some Euclidean features to describe them, So what are the optimal things that you can be doing?",
            "And then Lastly, there's a lot of open theoretical questions in this area.",
            "We've done very little work, just a smidgen.",
            "Looking at some of these features that's in our jam wallpaper, but I'm putting that paper here partly because we have a review of some of the other related literature, alot of which is by Belkin and Bowman's ribbon.",
            "As I said, they'll be talking tomorrow, so they may be talking a little bit about that, but they have to make a lot of assumptions.",
            "We've had to make a lot assumptions as well.",
            "There's a lot of questions about Asymptomatically.",
            "Can you learn?",
            "What do you need?"
        ],
        [
            "OK, so if you're interested in learning further about this, or if you're interested in getting some of the code to run these algorithms, some of the data and some of the papers, please see a repository setup similarity learning, and this is sort of a nice paper to start with.",
            "It encapsulates most of what I've talked about today and reviews a lot of the literature in the area.",
            "And then if you have more questions, three of us are here today and this week.",
            "So the people in blue so you can come and stop us if you if you don't feel like asking a question now.",
            "OK, thank you.",
            "Question.",
            "Thanks very much again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is work in progress.",
                    "label": 0
                },
                {
                    "sent": "We've worked out for a couple of years, will continue working on you can see it's work in progress, 'cause we're still looking for a better picture of Alera Hemi here.",
                    "label": 0
                },
                {
                    "sent": "And before I start, I want to thank the organizers.",
                    "label": 0
                },
                {
                    "sent": "I think they put together really nice selection of speakers and it's been very nice for the organized things.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're used to solving classification problems by starting with some Euclidean features, but a different paradigm is to say what if we just start by knowing some similarities, and this may be a very practical approach.",
                    "label": 0
                },
                {
                    "sent": "I think this is a little bit more common to what we do as humans is.",
                    "label": 0
                },
                {
                    "sent": "If we were given a problem like this to classify who painted this and we have some training samples from various painters, probably least in our heads, we don't derive Euclidean features and then try to do this.",
                    "label": 0
                },
                {
                    "sent": "We just have some some notion almost Oracle like of how similar.",
                    "label": 0
                },
                {
                    "sent": "These paintings are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the basic setup for this classification problem is that instead of assuming you have Euclidean features, we're going to assume that you have some training samples, but that the space they live in is some abstract space.",
                    "label": 1
                },
                {
                    "sent": "So it might be the space of all paintings.",
                    "label": 0
                },
                {
                    "sent": "It might be the space of all World Wars, the space of all car bombs, whatever.",
                    "label": 0
                },
                {
                    "sent": "It could be very abstract.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to assume that for each of these training samples, you have a corresponding class label.",
                    "label": 0
                },
                {
                    "sent": "So in this example of paintings, this might be the painter, and we'll assume your class labels from large.",
                    "label": 0
                },
                {
                    "sent": "Some large class and that you have N training samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so not so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prizing Lee in some letter based learning, there's a similarity function, but we're going to call it an underlying similarity based function because you might not actually know it.",
                    "label": 1
                },
                {
                    "sent": "It may be just something that's out there that you can actually access what we assume that you do know, though, are the training similarities.",
                    "label": 0
                },
                {
                    "sent": "So you have N training samples and we seem to have a matrix that send by N where each entry is the similarity between the training sample and the gate training sample, and we also assume since this is going to be a supervised problem that you have all the training class labels.",
                    "label": 0
                },
                {
                    "sent": "So this just says if all these pictures you have these network of similarities and you can think of this as learning with a fully connected graph.",
                    "label": 0
                },
                {
                    "sent": "OK, well will also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that you have the test similarity.",
                    "label": 0
                },
                {
                    "sent": "So if this is your test painting or test sample, you have all these lines that show you how similar they are to all of the end training samples.",
                    "label": 0
                },
                {
                    "sent": "This is an end by one vector and will also give you the self similarity how similar this painting to itself and not all the algorithms will use that, but we'll assume that that's given.",
                    "label": 0
                },
                {
                    "sent": "So then to summarize the problem that will be talking about today.",
                    "label": 0
                },
                {
                    "sent": "Is to estimate the class label for some test sample X given this N by N matrix S of training samples similarities their labels YN by one vector and end by one vector of the test.",
                    "label": 1
                },
                {
                    "sent": "Similarities to the training samples and the self similarity.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it turns out that this sort of problem arises a lot in practice.",
                    "label": 0
                },
                {
                    "sent": "We got into this problem because we were interested in imitating human judgment an in trying to mimic and use human judgment, and one of the ways it's easy and interface with humans is to ask them how similar things are rather than trying to ask them for features.",
                    "label": 0
                },
                {
                    "sent": "But but these problems arise a lot in computational biology.",
                    "label": 0
                },
                {
                    "sent": "It's often easier if you have primitives like proteins or DNA sequences or amino acids to say how similar two of them are in some possibly non metric way rather than trying to talk about some Euclidean features.",
                    "label": 0
                },
                {
                    "sent": "Similarly, in computer vision there's been a lot of different similarities to look at objects or images.",
                    "label": 0
                },
                {
                    "sent": "Such as the tangent distance, the earth movers distance, etc.",
                    "label": 1
                },
                {
                    "sent": "I'll show you some results later with the Pyramid match kernel and also an information retrieval and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Things like the cosine similarity with TF IDF vector.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've sort of recognized 5 distinct approaches to dealing with similarity based learning problems, and there may be other completely distinct approaches, and please go out and figure them out and let me know.",
                    "label": 0
                },
                {
                    "sent": "One of the 1st and simplest things you could do is.",
                    "label": 0
                },
                {
                    "sent": "You could say, well, I have all these points and I know their pairwise similarities.",
                    "label": 0
                },
                {
                    "sent": "Maybe I could turn that into pairwise distances and I'll embed those points in a metric space using something like multi dimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "And then once once you embed them in a metric space, you can do all the standard machine learning stuff, because then you're good.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to argue this is sort of not the best way to approach this problem, and I think that will become more obvious as I talked about the other ways.",
                    "label": 0
                },
                {
                    "sent": "But so basically we won't talk anymore in this talk about this, but it's certainly one approach.",
                    "label": 0
                },
                {
                    "sent": "Another approach that may be intuitive.",
                    "label": 0
                },
                {
                    "sent": "Some view is to treat similarities as kernels, and then you can pop that kernel into something like a support vector machine.",
                    "label": 1
                },
                {
                    "sent": "So we'll talk a little bit about that and some of the challenges there.",
                    "label": 0
                },
                {
                    "sent": "Another approach has been very popular this century.",
                    "label": 0
                },
                {
                    "sent": "If you will is to think about the similarities themselves being features and I'll talk a little bit about the theoretical work there and we'll see experiments.",
                    "label": 0
                },
                {
                    "sent": "Our most of our work has been these last two categories looking at K nearest neighbors, which is perhaps not the sexiest of methods, but it turns out that we're going to present were sort of some novel ideas there, and they can work very effectively, and then also in generative models.",
                    "label": 0
                },
                {
                    "sent": "For similarities and the nice thing about both of these approaches is they produce output probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to start on the left here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The similarities as kernels and approaches.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About talk about that.",
                    "label": 0
                },
                {
                    "sent": "So can we treat similarities as kernels?",
                    "label": 1
                },
                {
                    "sent": "Well, kernels are just inner products in some Hilbert space.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to remind you, if you don't recall inner products, here's an example in the product.",
                    "label": 0
                },
                {
                    "sent": "Probably most familiar with your product with two vectors.",
                    "label": 0
                },
                {
                    "sent": "So if you had a vector X in a vector Z, it would look like this length here, so it's a scalar number.",
                    "label": 0
                },
                {
                    "sent": "An inner products have some nice properties, the conjugate symmetric, they're real, they're linear, and their positive definite.",
                    "label": 1
                },
                {
                    "sent": "If you have an inner product implies a norm and that norm then has to satisfy the triangle inequality.",
                    "label": 1
                },
                {
                    "sent": "OK, so an inner product has sort of all these mathematical rules or regulations concerning it so.",
                    "label": 1
                },
                {
                    "sent": "Any substance can we treat similarities?",
                    "label": 0
                },
                {
                    "sent": "Kernels is really the question can we try?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarities as inner product.",
                    "label": 0
                },
                {
                    "sent": "Well certainly if you give me an inner product it will seem like a similarity to me in my abstract human notion of what similarity means.",
                    "label": 0
                },
                {
                    "sent": "Also, yeah, that inner product, you know acts like a similarity, but you can't really turn it around to say that always my notion of similarity has these mathematical properties of an inner product doesn't really hold, so let's look at an example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Probably everyone who's been on the Amazon website.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you notice they keep some statistics when you view a book of what book people actually bought.",
                    "label": 0
                },
                {
                    "sent": "So we can look at that and use that as a similarity, and in fact, I suspect that Amazon keeping track of this statistic to to be able to do things of this sort, so will say the similarity between a book and a book be is the percent that by bouquet after they viewed book, be on Amazon.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's for 96 books.",
                    "label": 0
                },
                {
                    "sent": "We went out and we took this data.",
                    "label": 0
                },
                {
                    "sent": "This is the similarity matrix S and you'll see there's a very strong self similarity here.",
                    "label": 0
                },
                {
                    "sent": "So for the most part, this book is most similar to itself.",
                    "label": 0
                },
                {
                    "sent": "People usually buy the book they were viewing, but you'll notice there's some holes like right here where for about book 67.",
                    "label": 0
                },
                {
                    "sent": "Here people actually didn't buy it after they viewed it.",
                    "label": 0
                },
                {
                    "sent": "Hardly ever, they tended to buy some of these other books instead.",
                    "label": 0
                },
                {
                    "sent": "So is this in?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Product like, well, well, no Anan.",
                    "label": 0
                },
                {
                    "sent": "One aspect isn't certainly not symmetric.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you viewed the elements of statistical learning then there's a certain probability that you went and bought the pattern recognition book and that was a 3%.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you viewed this, there was an 8% chance you ended up buying hasty chips running Friedman's book, so not symmetric, and we would expect it to be symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "We're not going to argue that it's really symmetric in our measurement, noise was just noisy.",
                    "label": 0
                },
                {
                    "sent": "No, this is really going to be some asymmetry between, say Harry Potter and some other children's book.",
                    "label": 0
                },
                {
                    "sent": "Or these different books.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "We can see how much of an inner product this isn't it.",
                    "label": 0
                },
                {
                    "sent": "This isn't by looking at the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So here I've ranked the eigenvalues in order and this is just the size of them and this green line shows you the zero line.",
                    "label": 0
                },
                {
                    "sent": "So remember, if there really was underlying inner product and we had this matrix and the matrix should be positive definite, all the eigenvalues should be real and positive.",
                    "label": 0
                },
                {
                    "sent": "But in fact we've got a few eigenvalues here that are quite negative.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so S not really an inner product.",
                    "label": 0
                },
                {
                    "sent": "That's not really a kernel matrix, but well, let's just make it anyways and we'll see how that works.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's not so bad to just approximate it and some people have suggested that they're sort of sort of noisy inner product, but I think that's philosophically not true.",
                    "label": 0
                },
                {
                    "sent": "Does everyone know this?",
                    "label": 0
                },
                {
                    "sent": "Some Greek myth about procrustes.",
                    "label": 0
                },
                {
                    "sent": "He was sort of this evil person and travelers would come and they would see his cottage and it would be a rainy night so they would stop at his cottage and ask if they could stay and progressed to be like yes yes come on in.",
                    "label": 0
                },
                {
                    "sent": "Come on in and he had one bed, the procrustes bed and it was a certain size and if you didn't fit you would just hack you off to fit or stretch you out.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a procrustean approach to say well our similarity really isn't an inner product but let's just go ahead and make it that way.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "How do we get rid of?",
                    "label": 0
                },
                {
                    "sent": "Those negative eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "Well, first we're going to take our S and will symmetrise it, and then we'll look at the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that you can do is just clip 'em.",
                    "label": 0
                },
                {
                    "sent": "So if this is your original eigenvalue spectrum, you have these nasty negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "You just set them all to 0, so that's clipping.",
                    "label": 0
                },
                {
                    "sent": "Now the nice thing about clipping is that if you think about the cone of PSD matrices and your matrix is out here, it's not in the cone.",
                    "label": 1
                },
                {
                    "sent": "Clipping is like projecting it to the nearest PSD matrix in the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a nice mathematical excuse for clipping.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another thing that people do that I find less intuitive is to flip the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "You take these negative eigenvalues and you just flip 'em you change their sign and there's some various interpretations that don't make so much sense to me, but I think the easiest way to understand why this might be a good idea is that it's a similar effect.",
                    "label": 0
                },
                {
                    "sent": "If you took a new S matrix by taking S transpose S, This is sort of like the square root of the eigenvalues that you would get here.",
                    "label": 0
                },
                {
                    "sent": "So this is another way to get rid of your negativity.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A third way, which sounds like it should be good.",
                    "label": 0
                },
                {
                    "sent": "It turns out not to be.",
                    "label": 0
                },
                {
                    "sent": "Is it just shift your eigenvalues up?",
                    "label": 0
                },
                {
                    "sent": "You could say well, let me take my smallest eigenvalue and I'll just shift up all the eigenvalues, make them bigger, and this should only affect the self similarity.",
                    "label": 0
                },
                {
                    "sent": "So it seems like it should work well, but I'll show you later it's not such a good idea.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's true.",
                    "label": 0
                },
                {
                    "sent": "Actually it was a lot of table and a lot of results, and I did.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I did to summarize it for you, which is should you flip, clip or shift in general over about 10 different datasets, we've seen that the best performance generally comes with Clip, and that's sort of the safest thing to do.",
                    "label": 1
                },
                {
                    "sent": "So if you had to decide that, that would be a good aspect.",
                    "label": 0
                },
                {
                    "sent": "But on many datasets it it turns out doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "On some datasets it matters a lot.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's motivated people to try to say well, is there sort of a best way?",
                    "label": 0
                },
                {
                    "sent": "Can I learn the best kernel for a particular problem?",
                    "label": 1
                },
                {
                    "sent": "And in particular, if you know that you're using SVM, there's been some approaches to learn the best kernel in that SVM context.",
                    "label": 0
                },
                {
                    "sent": "So this, for example, is an approach that my student James will talk about it.",
                    "label": 0
                },
                {
                    "sent": "I see Mail 2009, which is, you got your SVM primal here and now you've added a minimisation over the PSD code to find your kernel K. And then you say, well, this kernel K that you're going to find to use for SVM regularize it towards the S that you started with in the Frobenius norm an.",
                    "label": 0
                },
                {
                    "sent": "For example, if you take this game of this regularization factor here and you hit it really strong, make it all the way asymptomatically going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then this becomes equivalent to clipping the matrix, but can give you some better performance, so he'll have a poster on that at the poster session here and or you can see it at I smell.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's sort of the similarities as kernel story like talk just briefly about similarities as features.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach that says, well, we'd like to have a feature space 'cause we really know how to deal well with the Euclidean features.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is take the similarities between our test point and each training sample and we'll just call that N vector eh feature vector for our test sample X.",
                    "label": 1
                },
                {
                    "sent": "So if you have N training samples, you now get an end dimensional feature space.",
                    "label": 1
                },
                {
                    "sent": "If you get new training sample, then your features dimensional space just got a little bigger, so some of the early work on this was grapples work in 98 Leo Noble did this in 2003.",
                    "label": 0
                },
                {
                    "sent": "Once you have this feature space, you can do anything with it, so these are people applied SPMS LP machines, some discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "These tended not to really work any better than one KNN and less you had really noisy data and there's a potential support vector machine that at first seem like a different approach.",
                    "label": 0
                },
                {
                    "sent": "But then if you analyze it a little, you figure out that it's just again working in this feature space with with both an L1 and L Infinity regularizer.",
                    "label": 0
                },
                {
                    "sent": "So this seems like it's opening up the doors to cursor dimensionality, because as N goes, your dimension is also growing so.",
                    "label": 0
                },
                {
                    "sent": "There's some questions about whether asymptomatically this is really going to work, whether it's going to converge.",
                    "label": 0
                },
                {
                    "sent": "There's been a few different results.",
                    "label": 0
                },
                {
                    "sent": "Our results suggest that you can make it work, but instead of choosing all N, you should choose some slow growing subset of N. For example, here you should choose log N of these guys rather than all end so that your future space is growing slower.",
                    "label": 1
                },
                {
                    "sent": "Talk a little bit more about that later, but um, Nina Balcan and 83 Bowen Abraham Bloom have a number of results in that, and I think a couple of them might be talking tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in the theory, they may be talking about that tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at some results.",
                    "label": 0
                },
                {
                    "sent": "In this first problem on the left Amazon 47.",
                    "label": 0
                },
                {
                    "sent": "Is classifying authors from that similarity.",
                    "label": 0
                },
                {
                    "sent": "I showed you oral sonar is a problem where there was a.",
                    "label": 0
                },
                {
                    "sent": "This was done at the Applied Physics lab.",
                    "label": 0
                },
                {
                    "sent": "They took 100 sonar signals that are hard to classify whether they were targets or clutter, and they ask humans to listen to them and turns out that humans are very good at listening to sonar signals much better than our automated algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the humans told the organizers how similar they were and they were trying to figure out if these signals the machines were failing on the automatic algorithms were failing on.",
                    "label": 0
                },
                {
                    "sent": "Could humans using similarity distinguish them?",
                    "label": 0
                },
                {
                    "sent": "Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "This is the Caltech 101 classic data set.",
                    "label": 0
                },
                {
                    "sent": "It's 101.",
                    "label": 0
                },
                {
                    "sent": "Object recognition classes with about 10,000 samples.",
                    "label": 0
                },
                {
                    "sent": "Here we use the pyramid match kernel as our similarity.",
                    "label": 0
                },
                {
                    "sent": "The space rack is the face recognition data set, so there's 139 classes which are different people and we also use a positive definite similarity there.",
                    "label": 0
                },
                {
                    "sent": "Miracca music similarity problem with 10 classes and voting is the standard UCI data set.",
                    "label": 0
                },
                {
                    "sent": "It's whether people are voting Republican or Democrat or their voting records and then trying to classify them as Republican or Democrat and here we use the voting difference.",
                    "label": 0
                },
                {
                    "sent": "So the value difference metric similarity, which has just some very tiny bit of negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK, so above the green line here is SVM where we clipped the matrix to make it a kernel and then below this are three different approaches to using the similar themselves as features.",
                    "label": 0
                },
                {
                    "sent": "And one thing you'll see is that there's no real clear winners here.",
                    "label": 0
                },
                {
                    "sent": "In general, the SVM clip does fine.",
                    "label": 0
                },
                {
                    "sent": "There's a few cases where every algorithm does well, so the best in each case is bolded.",
                    "label": 0
                },
                {
                    "sent": "So here the SVM clip did best in three of these cases.",
                    "label": 0
                },
                {
                    "sent": "The SVM did best here.",
                    "label": 0
                },
                {
                    "sent": "I lost this guy.",
                    "label": 0
                },
                {
                    "sent": "He did best here and the SPM using RBF kernel did best here, so not a clear story out of that.",
                    "label": 0
                },
                {
                    "sent": "You'll notice though, that there appears to be a hole in the table.",
                    "label": 0
                },
                {
                    "sent": "Well that's because there's a hole in the table, so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to know that the SVM clip your fitting.",
                    "label": 0
                },
                {
                    "sent": "You're feeling this whole space with one support vector machine and in 2006 some folks in computer vision.",
                    "label": 0
                },
                {
                    "sent": "They got sick of training really large support vector machines on a lot of data and so they said well, could we just apply the support vector machine locally?",
                    "label": 0
                },
                {
                    "sent": "So you'll take your test sample.",
                    "label": 0
                },
                {
                    "sent": "You'll find some K nearest neighbors which might be a large number of K like 1024 and just run and S training SVM on that small data set.",
                    "label": 0
                },
                {
                    "sent": "So sort of a local SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is nice because it's always feasible, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you can control the size, but you now have to find the Kenneth neighbors and it's sort of lazy.",
                    "label": 0
                },
                {
                    "sent": "You have to wait to get test samples to train your classifier, but by doing things locally you can do a lot better on some datasets, so you'll notice by doing this SVM locally and again now locally you're clipping.",
                    "label": 0
                },
                {
                    "sent": "This has gotten a little worse, but not much.",
                    "label": 0
                },
                {
                    "sent": "It's still sort of in the ballpark of these other algorithms, but this is dropped from 81% error to 17.5% error.",
                    "label": 0
                },
                {
                    "sent": "So in zanes paper, when they first promoted this, their results seem to indicate that on their datasets they did pretty much the same as training an SVM on the whole data set, and so was much more about a computational win here.",
                    "label": 0
                },
                {
                    "sent": "Once in awhile we see this kind of big win, and we think that it's mostly because of the approximation that if you clip the entire N by N matrix, you probably really losing a lot of information.",
                    "label": 0
                },
                {
                    "sent": "But if you just clip some small part of it, that small part you might not have to lose any information, and it may just be easier to fit the model locally.",
                    "label": 0
                },
                {
                    "sent": "Then trying to fit one SPM alter the whole space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Speaking about K nearest neighbor and local models will talk a little bit more about that and focus on on weights.",
                    "label": 0
                },
                {
                    "sent": "So just a very.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quickly remind you about weighted nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "You're going to take your test sample, find its canius neighbors, or whatever locality definition you want, give you to those neighbors some wait, and then take a weighted weighted average.",
                    "label": 1
                },
                {
                    "sent": "So this is just the indicator function.",
                    "label": 0
                },
                {
                    "sent": "Here you're summing up the way for each class, choose the class that gets the most weight.",
                    "label": 0
                },
                {
                    "sent": "One of the nice aspects about this as it does sort of match with psychologists, at least some psychologists think we think that we have sort of an example model in our heads that we've seen some prototypical dog.",
                    "label": 0
                },
                {
                    "sent": "We've seen a bunch of dogs, and then we try to try to match this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another nice aspect of weighted nearest neighbors is that if you're positive weights and if the weights and no one, then you can just sum up the weights for each class to get an estimate of the class posterior.",
                    "label": 0
                },
                {
                    "sent": "And also to emphasize again again, I think probabilities are very important in dealing with real systems, because often you really have these asymmetric costs you'd like to be able to interpret your results, and it's particularly for system integration, usually in real systems, your classifier is not the only thing going on, it's gotta get fused or integrated with some.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing else.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might notice that we're from an electrical engineering Department and so we think about specs and design goals if you will.",
                    "label": 0
                },
                {
                    "sent": "And So what are the design goals for choosing weights, particularly similarly based learning?",
                    "label": 1
                },
                {
                    "sent": "Well, the first one I think.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is obvious and dates back about 50 years, which is that you might want your weight to be an increasing function of your similarity.",
                    "label": 1
                },
                {
                    "sent": "So if this guy is more similar to him than this other near neighbor here, then even though they're on their neighbors, I'm going to give this guy more weight.",
                    "label": 0
                },
                {
                    "sent": "The SEK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Design goal is a little more novel and a little more controversial, which is to say, let's say that this is my test step on.",
                    "label": 0
                },
                {
                    "sent": "These are its neighbors.",
                    "label": 0
                },
                {
                    "sent": "Well, Van Gogh painted a lot of sunflowers, and so you could argue that these sunflowers are really providing very correlated information about the feature space.",
                    "label": 0
                },
                {
                    "sent": "There are sort of redundant if you will.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if this was an email problem, there's a lot of times the emails just you have a lot of forwards, and so a lot of emails will look very similar and probably provide correlated information.",
                    "label": 0
                },
                {
                    "sent": "So to deal with these kind of redundancies.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we suggest is the diversity goal that you wait on sample.",
                    "label": 0
                },
                {
                    "sent": "I should be a decreasing function of the similarity between XI and XJ, so the test sample doesn't show up in this goal.",
                    "label": 1
                },
                {
                    "sent": "It just says that these two guys are really similar.",
                    "label": 0
                },
                {
                    "sent": "Then they should.",
                    "label": 0
                },
                {
                    "sent": "Then their weight should both get lowered.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a way to say that all of these guys have to sort of share the weight that would have gone to a neighbor in this part of the space.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how will we ever find weights that satisfy these goals?",
                    "label": 1
                },
                {
                    "sent": "Well, Luckily this is pretty much what linear interpolation weights do for you, so just to review linear interpolation you if you have some training samples XI, you're going to put some weights on them, such as the equal X, and those were going to be positive and some new one, so linear interpolation is really great.",
                    "label": 1
                },
                {
                    "sent": "You'll notice that when you when you go to sign these weights because they have to solve this, you need all the training samples at once.",
                    "label": 0
                },
                {
                    "sent": "So this really is different than saying applying a Gaussian kernel or something where my weight is just some RBF function of XNXX.",
                    "label": 0
                },
                {
                    "sent": "I hear all the XI are involved in choosing each way, so sort of a joint.",
                    "label": 0
                },
                {
                    "sent": "Operation, that's how you get to the design goal too.",
                    "label": 0
                },
                {
                    "sent": "But linear tribulation has some problems if we just try to apply to classification first.",
                    "label": 0
                },
                {
                    "sent": "There might be a non unique solution.",
                    "label": 0
                },
                {
                    "sent": "So for example if I have a test sample here and I solve for weights on X1 X 2X4, there's a lot of waiting possibilities that would satisfy the linear interpolation equations.",
                    "label": 0
                },
                {
                    "sent": "I could put a halfway here halfway here or fourth, wait on all of them, etc.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another problem is that there's often no solution, right?",
                    "label": 0
                },
                {
                    "sent": "So this is requiring me to find some weights such that X is the convex combination of the XI.",
                    "label": 0
                },
                {
                    "sent": "But here there's no convex combination of these training samples.",
                    "label": 0
                },
                {
                    "sent": "That's going to give me access.",
                    "label": 0
                },
                {
                    "sent": "So in order to to do.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With these problems, I suggested actually back in my thesis work some slight changes to make this possible for general classification problems.",
                    "label": 0
                },
                {
                    "sent": "So when we solve for the line weights, linear interpolation with maximum entropy, there's now two terms.",
                    "label": 1
                },
                {
                    "sent": "This first term here in the second term.",
                    "label": 0
                },
                {
                    "sent": "And the first term says maybe we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can't solve this, so we'll just change it to say make the left side here close to the right side in a square norm, so we'll just try to do the best we can, and then the second term here so that deals with the fact that they might.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be a solution and the second term deals with the nonuniqueness.",
                    "label": 0
                },
                {
                    "sent": "It says well also minimize negative entropy sources.",
                    "label": 0
                },
                {
                    "sent": "Maximize entropy of the weights, so that will always guarantee we have a solution 'cause it's convex regularizer.",
                    "label": 0
                },
                {
                    "sent": "And because we're maximizing entropy of the weights if we didn't have anything here, the maximum entropy solution would be the uniform weight.",
                    "label": 1
                },
                {
                    "sent": "So this is going to push our weights to all be equal, which I think makes sense.",
                    "label": 1
                },
                {
                    "sent": "It says you know sort of give all your samples equal equal votes if you can.",
                    "label": 0
                },
                {
                    "sent": "OK, the nice thing too about them.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it should be regularizer.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "This means that you wait to have an exponential solution and you don't necessarily want to solve for the exponential solution, but knowing that allows you to do some nice analysis because you know what the formula weights is otherwise.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can't do that.",
                    "label": 0
                },
                {
                    "sent": "So for example, we were able to show that this classifier with these weights would be consistent and then it falls a lot of large numbers, so you can get noise averaging OK, so this is all Euclidean stories, so we can find ways to satisfy our design goals in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "What about similarity space?",
                    "label": 0
                },
                {
                    "sent": "How do we do linear interpolation and similarity space?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to just turn allies it.",
                    "label": 0
                },
                {
                    "sent": "So this is actually something I got to James notice.",
                    "label": 0
                },
                {
                    "sent": "Is that if we take the square norm he ran, we expand it and just write things in matrix notation to make it a little clearer.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to change this regularizer for maximum entropy to be a Ridge regularizer in sort of an unrelated issue.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This regulator is going to act very similarly to the maximum entropy regularizer.",
                    "label": 0
                },
                {
                    "sent": "It's also going to push the weights to be uniform 'cause it's recognizing the variance.",
                    "label": 0
                },
                {
                    "sent": "You can make a more direct story about minimizing the estimation variance.",
                    "label": 0
                },
                {
                    "sent": "This way you lose some of those nice theoretical properties, or at least you can't show them 'cause you don't know what the form of this is going to be anymore, but this is going to be computationally more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the nice thing that lets you journalize this is that you've now got this linear interpolation problem just in terms of inner product, so these are the inner products between all of your training samples and the inner product between your test sample in your training samples.",
                    "label": 0
                },
                {
                    "sent": "So as John Shaler mentioned in the SBM, 12 beginning of the week, as soon as you have things in inner product, you basically have the kernelized version of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you could replace this with kernels, kernel functions if you had them here we don't have kernel functions, we have similarity, so we're going to play some similarities as far as I know, I haven't seen any other kernelized linear interpolation.",
                    "label": 0
                },
                {
                    "sent": "It seems odd that it would have been done before, so someone knows a reference.",
                    "label": 0
                },
                {
                    "sent": "I'd love to have it 'cause I don't know which one reference.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we do that, so we replaced The xx transpose with Rs matrix here, hoping that it acts like a kernel and our test similarities here.",
                    "label": 0
                },
                {
                    "sent": "And this leads us to what we call the Kernel Ridge interpolation weights.",
                    "label": 1
                },
                {
                    "sent": "So we talked about those design goals.",
                    "label": 0
                },
                {
                    "sent": "I told you linear interpolation, satisfied them.",
                    "label": 0
                },
                {
                    "sent": "I didn't prove it.",
                    "label": 0
                },
                {
                    "sent": "So let's just talk.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How that shows up here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, that goal of Affinity you'd like if you're training samples similar to your test sample that gets more weight, so that shows up in these two terms here.",
                    "label": 0
                },
                {
                    "sent": "This term by itself says minimize the weighted similarity.",
                    "label": 0
                },
                {
                    "sent": "So if you only had this term, then your best bet would be to put all of your weight on your one nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "But of course, that's not what you want to do.",
                    "label": 0
                },
                {
                    "sent": "You want to spread the weight around an.",
                    "label": 0
                },
                {
                    "sent": "Luckily you have this regularizer.",
                    "label": 0
                },
                {
                    "sent": "Here this W transpose W, which says spread the weight around and so that'll force this term to show the weight with the other weights and other training samples.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the second term here will enforce the diversity that we were looking for.",
                    "label": 0
                },
                {
                    "sent": "So if we expand this out, we may have lost their pointer here.",
                    "label": 0
                },
                {
                    "sent": "Can anyone see the red pointer?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we expand this matrix out we can see that the sum of the similarity between each training sample XI and XJ times WIWJ, and so we're trying to minimize this.",
                    "label": 0
                },
                {
                    "sent": "This means that if the similar between XI and XJ goes up, that will end up having to try to push down W&WJ.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we could just put the similarities in here, we don't actually need to approximate them.",
                    "label": 0
                },
                {
                    "sent": "If we do that, if we just put the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm leaving here without approximating them, and then we have a global optimization problem to solve, and we find that that works slightly.",
                    "label": 0
                },
                {
                    "sent": "McGinley better than if we approximate things, but the global optimization can be very hard and take a long time to run.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in practice, we just approximate the S you make it PSD, and once you've done that then this becomes a quadratic program, which is awfully convenient.",
                    "label": 0
                },
                {
                    "sent": "Even more convenient, these are box constraints, and so you end up with a problem that as an optimization problem, looks just like the SVM problem and your fast SVM solutions like the Smol algorithm can be applied here to make this very fast.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but if it's still not fast enough for you, you can remove the constraints on the weights.",
                    "label": 1
                },
                {
                    "sent": "You say I don't need my weights to be positive.",
                    "label": 0
                },
                {
                    "sent": "I don't need my.",
                    "label": 0
                },
                {
                    "sent": "Well, you still want you with someone.",
                    "label": 0
                },
                {
                    "sent": "You can remove the positive iti wasting your constraints and minimize the same problem.",
                    "label": 1
                },
                {
                    "sent": "And you're going to close form solution looks like this and you can show this is equivalent to doing a local Ridge regression fit and then finding the class label.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of an old result.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to show.",
                    "label": 0
                },
                {
                    "sent": "I didn't go through, but hopefully people know that local regression and waiting nearest neighborhood.",
                    "label": 0
                },
                {
                    "sent": "They have these analog.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's just look at some example waits to see how this shows up in practice.",
                    "label": 0
                },
                {
                    "sent": "So here's a similarity matrix S and it says that my training samples are all maximally similar to themselves and they have zero similarity to each other and they're similar to the test point goes with four 321.",
                    "label": 0
                },
                {
                    "sent": "So obviously wait one should be the biggest and then on down, and we see that weight one is the biggest weight 2834.",
                    "label": 0
                },
                {
                    "sent": "Here's the regularizer, so as the regularizer gets really really strong, then we have that Ridge penalty that says all the way.",
                    "label": 0
                },
                {
                    "sent": "It should just be equal.",
                    "label": 0
                },
                {
                    "sent": "So as a regularizer goes off to Infinity, all the weights converge to .25.",
                    "label": 0
                },
                {
                    "sent": "So this is for the linear interpolation and this is when we take off the constraints and we have the Ridge regression and you can see that the weights wait for for little while is negative.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now the only thing we're changing, so now all of the training samples are equally similar to the test point, so they're all sort of on a hyper sphere around the test point if you will, and similarity space whatever that means.",
                    "label": 0
                },
                {
                    "sent": "And these are training samples similarities and let's see here we can see that training sample, two training sample two is very similar.",
                    "label": 0
                },
                {
                    "sent": "Training sample three.",
                    "label": 0
                },
                {
                    "sent": "So change sample two very similar change sample through that for their so W2 and W3 should be a little a little weighted down because they're sort of giving us the same information.",
                    "label": 0
                },
                {
                    "sent": "We think 'cause they're so highly highly correlated.",
                    "label": 0
                },
                {
                    "sent": "And in fact, we see that W2 and W3 here for the care I weights are actually equal, and certainly for the care are weights are also they turn out to be equal from this formulation, which sort of makes sense.",
                    "label": 0
                },
                {
                    "sent": "They end up sharing is actually the weight again because this regularizer the regulators is telling them to make the way.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is equals possible?",
                    "label": 0
                },
                {
                    "sent": "And just the last example here with a little more arbitrary straining similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "A little arbitrary testimony matrix shows you that some kind of weird, unintuitive things can happen here.",
                    "label": 0
                },
                {
                    "sent": "You'll see that wait for goes up and then back down.",
                    "label": 0
                },
                {
                    "sent": "There is kind of a discontinuity here with weight 3 is for the linear interpolation and for the Ridge regression here.",
                    "label": 0
                },
                {
                    "sent": "Wait, three actually gets bigger than wait, wait one at a certain point in the regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a table with a lot of results and see if we can make this a little easier to read.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's first focus on the K nearest neighbor methods, so these are four weighted nearest neighbor methods.",
                    "label": 0
                },
                {
                    "sent": "The top is just your standard KNN.",
                    "label": 0
                },
                {
                    "sent": "We cross validate the neighborhood size.",
                    "label": 0
                },
                {
                    "sent": "The next one if any Canon is.",
                    "label": 0
                },
                {
                    "sent": "If you just do design goal one, you just say if you're more similar to the test sample then you get more weight and then the KRIT and Care are are these two diversity weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "For the most part you see there's there's watcher naturally be statistically significant differences.",
                    "label": 0
                },
                {
                    "sent": "The bold here in the whole table, whatever is bold, is the best in the table for that.",
                    "label": 0
                },
                {
                    "sent": "Data set and everything that's not statistically significantly worse.",
                    "label": 0
                },
                {
                    "sent": "So for example, here if any can end, does the best on the Amazon problem at at 15, but the QR is not any worse to 16.10 statistically, so the only big difference that we see it happens with the Caltech 101 data set.",
                    "label": 0
                },
                {
                    "sent": "But this difference was actually kind of surprising, so if you do K nearest neighbors, you get 41% error.",
                    "label": 0
                },
                {
                    "sent": "If you do a 50K NN, so can you.",
                    "label": 0
                },
                {
                    "sent": "Haven't looked at all your data.",
                    "label": 0
                },
                {
                    "sent": "50 kid and you look at your day.",
                    "label": 0
                },
                {
                    "sent": "You see how similar is and you wait that an you win about 2%.",
                    "label": 0
                },
                {
                    "sent": "So that really hardly helped you at all.",
                    "label": 0
                },
                {
                    "sent": "You would have thought that waiting things by this similarly would help alot hardly helps at all in this case.",
                    "label": 0
                },
                {
                    "sent": "And now if you add the diversity term you go down by about 25% in air and this is the statistically significantly lowest error we found all the training sets.",
                    "label": 0
                },
                {
                    "sent": "So that was surprised that the diversity was mattering so much in that case.",
                    "label": 0
                },
                {
                    "sent": "And it's also the largest data set we have for the second largest data set we have mirax were results might also be significant.",
                    "label": 0
                },
                {
                    "sent": "So his 3000.",
                    "label": 0
                },
                {
                    "sent": "Apples.",
                    "label": 0
                },
                {
                    "sent": "Not a lot of differences between the algorithm, so sometimes it may not matter at all.",
                    "label": 0
                },
                {
                    "sent": "It seems like in Caltech 101 there must be a lot of these sort of high correlation so that it matters so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just looking again at Caltech showing that it's at least 10% better.",
                    "label": 0
                },
                {
                    "sent": "The next thing, which here is the SVM kernel with the clip approach.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another interesting thing to look at is I've broken up this up into local algorithms versus global algorithms, so these are all doing something on the K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And again, most of the time there's not that much difference, but the Amazon data set really stands out.",
                    "label": 0
                },
                {
                    "sent": "It was a very sparse similarity, and here you get about 80% error, 75% error here, and about 15 to 18% error with the local methods question.",
                    "label": 0
                },
                {
                    "sent": "Fix that or did you go through several values for K it's cross validated is that is your question.",
                    "label": 0
                },
                {
                    "sent": "Validation within the data.",
                    "label": 0
                },
                {
                    "sent": "Range.",
                    "label": 0
                },
                {
                    "sent": "This is for a mean value.",
                    "label": 0
                },
                {
                    "sent": "So these are 20 random 20 randomizations where each randomization all of the samples are divided up in 80% training and a 20% test.",
                    "label": 0
                },
                {
                    "sent": "Then the 80% training.",
                    "label": 0
                },
                {
                    "sent": "There's a 10 fold cross validation from which we choose the K from some subset of possible Ki.",
                    "label": 0
                },
                {
                    "sent": "Think K goes 124, eight, sixteen 3264.",
                    "label": 0
                },
                {
                    "sent": "I think he goes up to 128 as the highest K possible is that answering your question.",
                    "label": 0
                },
                {
                    "sent": "Oh, this is an average over those twenty runs, so yeah, sometimes they would have differed in those various runs.",
                    "label": 0
                },
                {
                    "sent": "There are two other techniques on these days.",
                    "label": 0
                },
                {
                    "sent": "So most of these datasets are similarity only, so this data set we created by going to the Amazon Web page, the oral sonar data set.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's any other results for they created this data set really to try to figure out if they could derive Euclidean features that matched what humans were doing.",
                    "label": 0
                },
                {
                    "sent": "So this is also very recent work in 2006 or so.",
                    "label": 0
                },
                {
                    "sent": "The Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "Here we ran it just like we ran all the other datasets, but the normal way to run Caltech 101 is a little different.",
                    "label": 0
                },
                {
                    "sent": "There's some special thing that computer scientists do about how they divide up the classes or something, and we didn't want to do something different on one data set, so I can't make it comparison there.",
                    "label": 0
                },
                {
                    "sent": "The face recognition the previous paper had just done KNN.",
                    "label": 0
                },
                {
                    "sent": "So that would be this this 4.23%.",
                    "label": 0
                },
                {
                    "sent": "That's some data from Hamid Krim in North Carolina.",
                    "label": 0
                },
                {
                    "sent": "The mirax, I don't know.",
                    "label": 0
                },
                {
                    "sent": "And the voting I think no matter what you do, you get around 5%.",
                    "label": 0
                },
                {
                    "sent": "For different approaches.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that was talking about how to do some good job waiting K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "The last thing I want to talk about is some approaches to generative models.",
                    "label": 1
                },
                {
                    "sent": "So one approach, well, let's let's just.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everyone with generative models larger classifiers, so this is the idea that you've seen some data and now you're going to model the probability of that for each class.",
                    "label": 1
                },
                {
                    "sent": "So you have some class conditional probability of what you've seen, and then you can choose the class that made that most likely.",
                    "label": 0
                },
                {
                    "sent": "So hopefully people have some experience with him.",
                    "label": 1
                },
                {
                    "sent": "Analysis Quite active malice, maybe Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "And of course, a big advantage to general classifiers that they produce probabilities out that hopefully are somewhat accurate.",
                    "label": 0
                },
                {
                    "sent": "But maybe it's not so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine if they're not accurate.",
                    "label": 0
                },
                {
                    "sent": "OK, our goal of well, let's back up one moment.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one in a general classifier.",
                    "label": 0
                },
                {
                    "sent": "One thing you could do is treat the similarities as features.",
                    "label": 1
                },
                {
                    "sent": "And once you've changed the subject features or done MD S, You're in Euclidean space 'cause you have features and you can run any generative classifier you want.",
                    "label": 0
                },
                {
                    "sent": "But if you think about, say, LDA where you're taking each class and modeling it by Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear what those Gaussians mean.",
                    "label": 0
                },
                {
                    "sent": "Like what does the center of that Gaussian mean?",
                    "label": 0
                },
                {
                    "sent": "If the features are the similarity of these different training samples and what is the covariance mean?",
                    "label": 0
                },
                {
                    "sent": "Also, in the previous work where they did that, the results were very similar to one nearest neighbor, so we didn't go that way.",
                    "label": 0
                },
                {
                    "sent": "We wanted to do something a little different and see.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could directly model the similarities.",
                    "label": 0
                },
                {
                    "sent": "But when we dropped them all those we found.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're not so robust, So what we ended up doing was modeling some descriptive statistics of the similarities, so we're assuming that the that everything that we need to know about the class is captured by some statistics T of S. And we've tried very statistics.",
                    "label": 1
                },
                {
                    "sent": "It doesn't matter a lot, but the statistics we use are these centroids, statistics.",
                    "label": 0
                },
                {
                    "sent": "So you have your similarities and we say what's the similarity of our test sample to a mean or prototype sample from class 1A?",
                    "label": 0
                },
                {
                    "sent": "Mean or prototype sample from Class 2, etc.",
                    "label": 0
                },
                {
                    "sent": "So this is a little weird because it's cross classes, so you're saying what's the probability of seeing the similarity between my test sample and Class 2 centroid, given that my test samples from Class G. So it's a little different than the qda models 'cause you have these different class conditionals to think about and you could find a centroid in various ways.",
                    "label": 1
                },
                {
                    "sent": "If you really had access to the sample domain.",
                    "label": 0
                },
                {
                    "sent": "Like if you had the set of all paintings then you could find sort of the best centroid for a class that maybe had maximum some similarity.",
                    "label": 0
                },
                {
                    "sent": "Here we generally assume that all we have is the data were given.",
                    "label": 0
                },
                {
                    "sent": "So for each class we take the training samples from that class and we try to find the training sample that had the maximum some similarity to the other training samples in that class.",
                    "label": 0
                },
                {
                    "sent": "So here are centroid is limited to be from the actual data we've seen, so that makes it a little less ideal.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm not going to walk you through the math because it's a whole lot of notation.",
                    "label": 0
                },
                {
                    "sent": "Those watching it through the concepts instead, which is where we're trying to model this vector of similarities to our central class centroids.",
                    "label": 0
                },
                {
                    "sent": "And there's now a G of these for each class G, so that's a lot.",
                    "label": 0
                },
                {
                    "sent": "And to try to model that joint probability is very hard.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is just assume that each of those independent, which is probably not true, but like often naive Bayes, often works really well even when it's completely not true.",
                    "label": 0
                },
                {
                    "sent": "So part of machine learning probably is just sort of that key of finding out what can you.",
                    "label": 0
                },
                {
                    "sent": "What can you lie about, even though you know it's not true and still have effective algorithms, so we assume that these disjoint probability is really just the product of some marginals.",
                    "label": 0
                },
                {
                    "sent": "And then we need to estimate each of those marginals.",
                    "label": 0
                },
                {
                    "sent": "So one of those modules will look like the probability of seeing a similarity between your test sample Anna sample centroid sample from Class H. Given that you think your test samples from Class G. So to estimate this from the data, there's a few different things we could do.",
                    "label": 0
                },
                {
                    "sent": "We went with the maximum entropy approach.",
                    "label": 0
                },
                {
                    "sent": "We said really, all I trust is my mean similarity.",
                    "label": 0
                },
                {
                    "sent": "I could see all the other samples from that class, how they, how the others training samples from Class G related to class mu H and I can take the mean sample of those, and I'll say that that's good, and then I'll try to find a distribution for this probability that has that mean.",
                    "label": 0
                },
                {
                    "sent": "So there's lots and lots of distributions that have that mean, but I'll find the maximum entropy distribution.",
                    "label": 0
                },
                {
                    "sent": "That has that mean.",
                    "label": 0
                },
                {
                    "sent": "So, not surprisingly, whenever you assume maximum entropy and you have the right kind of constraints, you end up with an exponential.",
                    "label": 0
                },
                {
                    "sent": "So another way to think about this model is just that we modeled this distribution as an exponential and then fit the maximum likelihood mean.",
                    "label": 0
                },
                {
                    "sent": "So either way, you get the same the same model.",
                    "label": 0
                },
                {
                    "sent": "Just sort of.",
                    "label": 0
                },
                {
                    "sent": "Depends on what you're more comfortable assuming or what perspective you want to take.",
                    "label": 0
                },
                {
                    "sent": "So that gives you this SDA model so you know just sort of modeling for each test sample.",
                    "label": 0
                },
                {
                    "sent": "How similar to these different class centroids?",
                    "label": 0
                },
                {
                    "sent": "So that works pretty well, but like something like UDA that has a lot of model bias because you're assuming just one centroid represent a whole class and maybe that's very poor assumption.",
                    "label": 0
                },
                {
                    "sent": "So this is the same problem people face with like UDA where you have one Gaussian represent a whole class and the usual solution is to go with a mixture so people often use Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "Probably the most novel part of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Was to say wow, we don't want to do a mixture, and in fact later we did a mixture and it didn't.",
                    "label": 0
                },
                {
                    "sent": "It didn't work very well, but first we just we were just really lazy.",
                    "label": 0
                },
                {
                    "sent": "We didn't want to do a mixture so we said how else can we reduce the model bias?",
                    "label": 0
                },
                {
                    "sent": "Because the models too rigid so we decided to just apply the model locally.",
                    "label": 0
                },
                {
                    "sent": "So this is a similar in many ways to applying the SVM locally but sort distinct work.",
                    "label": 0
                },
                {
                    "sent": "So we take our SDM model here.",
                    "label": 0
                },
                {
                    "sent": "But we only calculated for each neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So now you get your test sample X.",
                    "label": 0
                },
                {
                    "sent": "You find some K nearest neighbors and then for those K nearest neighbors you train these exponential probabilities of what your centroids are in that little neighborhood and what the probability of the samples at each centroid.",
                    "label": 0
                },
                {
                    "sent": "So that gets you a local SDA model.",
                    "label": 0
                },
                {
                    "sent": "Works pretty well.",
                    "label": 0
                },
                {
                    "sent": "There's a 2007 paper on that, and then in more recent work we realize that sometimes this model had some trouble because they were in your neighborhood.",
                    "label": 0
                },
                {
                    "sent": "There would only be 1 sample from a class, and so you wouldn't really be able to find the centroid Infinity model because your model would be just sort of this Dirac Delta at whatever probability the one training sample from that class was to its centroid.",
                    "label": 0
                },
                {
                    "sent": "So we need to reduce the estimation variance we were having estimation variance problems, and so obviously the solution is to do some sort of.",
                    "label": 0
                },
                {
                    "sent": "Azatian we'd like to do a Bayesian solution, but because these are discrete domain of similarities that doesn't workout so well mathematically.",
                    "label": 0
                },
                {
                    "sent": "But we think the regularization works pretty well, so we regularize over the different localities to regularize these different local fit distributions towards each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so that will show up in few.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm not going to go through the whole table results because we have results on different datasets and the other datasets, and so it's kind of a mess.",
                    "label": 0
                },
                {
                    "sent": "But the summary is that it's competitive with other methods I've shown you.",
                    "label": 0
                },
                {
                    "sent": "Generally it works just a little better than K nearest neighbor and then on some datasets it works really well and is the best classifier, so it's probably a good classifier if you were trying to build a classifier and sambal to include 'cause maybe it has some key ideas and it has a very different perspective than the other classifiers we talked about.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to move into some conclusions here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I've talked about these sort of five different approaches to thinking about similar based learning problems.",
                    "label": 0
                },
                {
                    "sent": "We found that performance in practice really depends a lot on the data set, and because these are similarity datasets where there's, there's no guarantee with that similarity is, I think that makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We found that by adding the diversity waiting into awaited Kenya's neighbor, this generally worked well.",
                    "label": 0
                },
                {
                    "sent": "It never really failed an once in awhile like on the Caltech 101 data set, you got a big gain, much bigger than we expected.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local SDA works pretty well, but our preliminary results on regular local SDA is that it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a much more competitive classifier and probably practically useful.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, again, I would argue that probabilities are useful in and I'm always very, very sad when when algorithms don't produce probabilities because it means that it's a lot harder.",
                    "label": 0
                },
                {
                    "sent": "Usuman in practical systems.",
                    "label": 0
                },
                {
                    "sent": "And the other key.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inclusion here is that local models are useful and that there are more useful than they would be in Euclidean spaces, because here we've got these issues of having to approximate things as kernels, and so if you only locally approximating them, hopefully the approximations are not as bad, and that you're similarly spaces.",
                    "label": 0
                },
                {
                    "sent": "This kind of weird big space that has a total space.",
                    "label": 0
                },
                {
                    "sent": "It may be very hard to model, but that locally, just as functions tend to be locally linear and locally well behaved, even your weird similarity space maybe locally well behaved, and you know, there's also some questions about whether there's an underlying manifold here.",
                    "label": 1
                },
                {
                    "sent": "The other nice aspect about using local models, and this is true in Euclidean as well is that they are always feasible.",
                    "label": 0
                },
                {
                    "sent": "You do have to find your nearest neighbors, but then you can do matrix inversions or kernels, and it's always pretty small.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I just want to finish with a few of the open questions.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty recent area of studying machine learning and on the one hand there there's been a lot of really good work.",
                    "label": 0
                },
                {
                    "sent": "On other hand, there's quite a few open questions that are still left if people are interested.",
                    "label": 0
                },
                {
                    "sent": "One is really just this issue of making SP SD, Flipclip and shifter really unsatisfying.",
                    "label": 0
                },
                {
                    "sent": "They don't take into account the fact that you're trying to solve a classification problem, so there's sort of agnostic to what the task at hand.",
                    "label": 0
                },
                {
                    "sent": "There's been a little work.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne doing this better and as I said, we have a nice email paper about this and you can talk to us afterwards if you're interested, but I think there's more good ideas to be had to deal with that then.",
                    "label": 0
                },
                {
                    "sent": "Secondly, talk about fast Canon.",
                    "label": 0
                },
                {
                    "sent": "So as as I suggested, local approaches to these problems seem to be really important, but that means you have to find your K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're Nickelodeon space, you can put together a KD tree of all tree.",
                    "label": 0
                },
                {
                    "sent": "You can have this sort of preprocessing structure make that really fast, but those structures require.",
                    "label": 0
                },
                {
                    "sent": "Things like concepts of medians, concepts of the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "So if you don't have that, how can you do fast Canon search?",
                    "label": 0
                },
                {
                    "sent": "As far as I know, it's only been one paper in this area, but you will if it's looking at fast cannon search and what he did was make some assumptions about the similarities, not as much as triangle inequality, but but some assumptions to try to get some tract ability.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to see if you start approximating things.",
                    "label": 0
                },
                {
                    "sent": "Can you still find that the K nearest neighbors within some closeness or within some approximation?",
                    "label": 0
                },
                {
                    "sent": "We see very little work on similarly based regression.",
                    "label": 0
                },
                {
                    "sent": "I think on the one had a lot of the ideas and concepts will be the same, but it'll be interesting to see how things really work out, and particularly with these issues of trying to approximate PSD matrices etc.",
                    "label": 0
                },
                {
                    "sent": "In classification you just make it binary decisions so you can get away with a lot and maybe you can't get away with the same things.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you start to see differences that are important when you go the regression paradigm.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything we've been doing is similar.",
                    "label": 0
                },
                {
                    "sent": "Based learning is really like learning on a fully connected graph, and there's been a lot of work on learning on graphs in general.",
                    "label": 0
                },
                {
                    "sent": "A lot of it's been semi supervised and not supervised, and there's sort of a little bit of disconnect between these two communities where there hasn't been a lot of flow of the ideas and then understanding between the communities, so I think there's some work there to think about whether the algorithms people have developed and the concepts were learning on graphs applied to the general summary based learning problem, and the other way around.",
                    "label": 0
                },
                {
                    "sent": "Then this point here, as we saw the performance really depends on the data set.",
                    "label": 0
                },
                {
                    "sent": "We have about 10, maybe 12 datasets.",
                    "label": 0
                },
                {
                    "sent": "Now that we've collected, and I'll give you the web page for the repository.",
                    "label": 0
                },
                {
                    "sent": "But really there needs to be people going on trying this stuff out and coming back with some.",
                    "label": 0
                },
                {
                    "sent": "Some reports of what works and what's important.",
                    "label": 0
                },
                {
                    "sent": "Uh, another interesting area and I know this is interesting 'cause the the DoD just gave me a large grant in this area.",
                    "label": 0
                },
                {
                    "sent": "So so obviously someone with at least a checkbook thinks is interesting.",
                    "label": 0
                },
                {
                    "sent": "Is fusing similarities with Euclidean features.",
                    "label": 1
                },
                {
                    "sent": "So assuming that you have both some similarity information about your training samples but also some Euclidean features to describe them, So what are the optimal things that you can be doing?",
                    "label": 0
                },
                {
                    "sent": "And then Lastly, there's a lot of open theoretical questions in this area.",
                    "label": 1
                },
                {
                    "sent": "We've done very little work, just a smidgen.",
                    "label": 0
                },
                {
                    "sent": "Looking at some of these features that's in our jam wallpaper, but I'm putting that paper here partly because we have a review of some of the other related literature, alot of which is by Belkin and Bowman's ribbon.",
                    "label": 0
                },
                {
                    "sent": "As I said, they'll be talking tomorrow, so they may be talking a little bit about that, but they have to make a lot of assumptions.",
                    "label": 0
                },
                {
                    "sent": "We've had to make a lot assumptions as well.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of questions about Asymptomatically.",
                    "label": 0
                },
                {
                    "sent": "Can you learn?",
                    "label": 0
                },
                {
                    "sent": "What do you need?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you're interested in learning further about this, or if you're interested in getting some of the code to run these algorithms, some of the data and some of the papers, please see a repository setup similarity learning, and this is sort of a nice paper to start with.",
                    "label": 0
                },
                {
                    "sent": "It encapsulates most of what I've talked about today and reviews a lot of the literature in the area.",
                    "label": 0
                },
                {
                    "sent": "And then if you have more questions, three of us are here today and this week.",
                    "label": 0
                },
                {
                    "sent": "So the people in blue so you can come and stop us if you if you don't feel like asking a question now.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much again.",
                    "label": 0
                }
            ]
        }
    }
}