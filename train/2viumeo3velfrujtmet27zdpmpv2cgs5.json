{
    "id": "2viumeo3velfrujtmet27zdpmpv2cgs5",
    "title": "A Convenient Framework for Efficient Parallel Multipass Algorithms",
    "info": {
        "author": [
            "Markus Weimer, Microsoft"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_weimer_cfe/",
    "segmentation": [
        [
            "My talk is about a convenient framework for efficient, parallel, multipass algorithms, and after the talk by sloth, you might wonder why we have map reduce.",
            "It works just fine.",
            "Actually.",
            "This is empirical work that shows that map reduce in the instance of Hadoop doesn't work just fine for machine learning."
        ],
        [
            "OK, so the point of you taking is that machine learning is essentially data compression.",
            "You take a lot of training data to make a little data that is called the model.",
            "We typically iterate a few times over the training data to come up with it, and we share very little state between the different iterations, typically in the order of the model size.",
            "This can be a gradient.",
            "This can be the actual model, whatever.",
            "So many algorithms can be expressed as data parallel loops with some synchronization step in between.",
            "And if you."
        ],
        [
            "Look at that and have MapReduce in mind.",
            "This might be something you come up with so you parallelize your data into different partitions.",
            "You run your one iteration over it, you send the output to the reducer.",
            "The reducer averages you stored on disk.",
            "Can you restart?",
            "The storing on disk is actually the bad part here.",
            "So first of all, you have some job set up to do every time you start a job on a shared cluster.",
            "You have to find machines to run your stuff on, and each iteration now is a job.",
            "So in each iteration you have to run the scheduler.",
            "In each iteration you have to load the data and in each iteration between iterations you communicate through the disk.",
            "All of that adds to a massive overhead."
        ],
        [
            "So what we propose instead is to change this one arrow into a bidirectional error.",
            "So after each iteration, the now called workers.",
            "I know the name is not quite ingenious.",
            "Gets back a sense, it's model or its data to some aggregator, which then does the aggregation with all the data available from the other workers and sends back a new model or a new state for the next iteration to be done on the worker.",
            "That way we only schedule machines once per training job.",
            "We avoid the scheduling overhead.",
            "Ideally, if the if we have enough workers for data set, we can keep all the data in memory, which adds to a massive speedup and we can communicate directly between the machines which which is also nice compared to communicating through disk."
        ],
        [
            "OK, so if you want to write something in this framework, if you want to write such a program, you can just use MPI and will just work.",
            "But most of the code is actually shared between different implementations of algorithms.",
            "On top of that, conceptual framework, just as in MapReduce."
        ],
        [
            "So the worker load some data, iterates over the data, communicate state and waits for input for the next path.",
            "All but the iteration over data is something that the framework creator can provide, so no need to handle all these cases.",
            "Same thing at the aggregator the aggregator."
        ],
        [
            "Receive some state from the workers aggregates it and sends it back again."
        ],
        [
            "Most of that can be shared between different programs running on the same infrastructure.",
            "This comes to no surprise because it's the same thing as in MapReduce, but."
        ],
        [
            "We keep stuff in memory.",
            "I.",
            "Another nice thing about MapReduce is failure failure, handling.",
            "How do you deal with the machine that goes down if you run stuff on 1000 machines, one machine will go down.",
            "So in the case of the worker, if you run stochastic algorithms like stochastic gradient descent in parallel, we can just ignore failure because they are stochastic.",
            "We don't need to deal with the case that some data wasn't chosen in that iteration unless the problem was because of the data.",
            "In any other case, we just restart on a different machine or have hot start machines available.",
            "The aggregator is quite crucial, so if that thing goes down we have to restart weekend."
        ],
        [
            "Not just ignore it.",
            "OK, so we implemented that on top of that we implemented parallel stochastic gradient descent on top of it, and I have about 20 seconds left, so the work has stochastic gradient.",
            "CD, Passover, the data and the aggregate are just averages."
        ],
        [
            "Models.",
            "Question is, does it work?",
            "Yes, it per iteration is just as good as sequential stochastic gradient descent."
        ],
        [
            "Details at the paper.",
            "What I want to get at is it fast?",
            "So we had exactly the same code written on top of Hadoop MapReduce and on top of this framework which again runs on top of the Hadoop infrastructure and MapReduce is about two times faster on 8 machines then on a single machine which is not all that great in our framework.",
            "If you just do 10 passes we already 20 times faster.",
            "That's because we keep stuff in memory so we can expect a super linear speedup.",
            "And in the limit, if we remove the data, initial data loading cost, we are 30 times faster.",
            "OK, thanks.",
            "I'm time's up."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk is about a convenient framework for efficient, parallel, multipass algorithms, and after the talk by sloth, you might wonder why we have map reduce.",
                    "label": 1
                },
                {
                    "sent": "It works just fine.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "This is empirical work that shows that map reduce in the instance of Hadoop doesn't work just fine for machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the point of you taking is that machine learning is essentially data compression.",
                    "label": 0
                },
                {
                    "sent": "You take a lot of training data to make a little data that is called the model.",
                    "label": 0
                },
                {
                    "sent": "We typically iterate a few times over the training data to come up with it, and we share very little state between the different iterations, typically in the order of the model size.",
                    "label": 1
                },
                {
                    "sent": "This can be a gradient.",
                    "label": 0
                },
                {
                    "sent": "This can be the actual model, whatever.",
                    "label": 0
                },
                {
                    "sent": "So many algorithms can be expressed as data parallel loops with some synchronization step in between.",
                    "label": 1
                },
                {
                    "sent": "And if you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at that and have MapReduce in mind.",
                    "label": 0
                },
                {
                    "sent": "This might be something you come up with so you parallelize your data into different partitions.",
                    "label": 0
                },
                {
                    "sent": "You run your one iteration over it, you send the output to the reducer.",
                    "label": 0
                },
                {
                    "sent": "The reducer averages you stored on disk.",
                    "label": 0
                },
                {
                    "sent": "Can you restart?",
                    "label": 0
                },
                {
                    "sent": "The storing on disk is actually the bad part here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, you have some job set up to do every time you start a job on a shared cluster.",
                    "label": 0
                },
                {
                    "sent": "You have to find machines to run your stuff on, and each iteration now is a job.",
                    "label": 0
                },
                {
                    "sent": "So in each iteration you have to run the scheduler.",
                    "label": 0
                },
                {
                    "sent": "In each iteration you have to load the data and in each iteration between iterations you communicate through the disk.",
                    "label": 0
                },
                {
                    "sent": "All of that adds to a massive overhead.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we propose instead is to change this one arrow into a bidirectional error.",
                    "label": 0
                },
                {
                    "sent": "So after each iteration, the now called workers.",
                    "label": 0
                },
                {
                    "sent": "I know the name is not quite ingenious.",
                    "label": 0
                },
                {
                    "sent": "Gets back a sense, it's model or its data to some aggregator, which then does the aggregation with all the data available from the other workers and sends back a new model or a new state for the next iteration to be done on the worker.",
                    "label": 0
                },
                {
                    "sent": "That way we only schedule machines once per training job.",
                    "label": 1
                },
                {
                    "sent": "We avoid the scheduling overhead.",
                    "label": 0
                },
                {
                    "sent": "Ideally, if the if we have enough workers for data set, we can keep all the data in memory, which adds to a massive speedup and we can communicate directly between the machines which which is also nice compared to communicating through disk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you want to write something in this framework, if you want to write such a program, you can just use MPI and will just work.",
                    "label": 0
                },
                {
                    "sent": "But most of the code is actually shared between different implementations of algorithms.",
                    "label": 0
                },
                {
                    "sent": "On top of that, conceptual framework, just as in MapReduce.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the worker load some data, iterates over the data, communicate state and waits for input for the next path.",
                    "label": 1
                },
                {
                    "sent": "All but the iteration over data is something that the framework creator can provide, so no need to handle all these cases.",
                    "label": 0
                },
                {
                    "sent": "Same thing at the aggregator the aggregator.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Receive some state from the workers aggregates it and sends it back again.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of that can be shared between different programs running on the same infrastructure.",
                    "label": 0
                },
                {
                    "sent": "This comes to no surprise because it's the same thing as in MapReduce, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We keep stuff in memory.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Another nice thing about MapReduce is failure failure, handling.",
                    "label": 1
                },
                {
                    "sent": "How do you deal with the machine that goes down if you run stuff on 1000 machines, one machine will go down.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the worker, if you run stochastic algorithms like stochastic gradient descent in parallel, we can just ignore failure because they are stochastic.",
                    "label": 0
                },
                {
                    "sent": "We don't need to deal with the case that some data wasn't chosen in that iteration unless the problem was because of the data.",
                    "label": 0
                },
                {
                    "sent": "In any other case, we just restart on a different machine or have hot start machines available.",
                    "label": 1
                },
                {
                    "sent": "The aggregator is quite crucial, so if that thing goes down we have to restart weekend.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not just ignore it.",
                    "label": 0
                },
                {
                    "sent": "OK, so we implemented that on top of that we implemented parallel stochastic gradient descent on top of it, and I have about 20 seconds left, so the work has stochastic gradient.",
                    "label": 1
                },
                {
                    "sent": "CD, Passover, the data and the aggregate are just averages.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "Question is, does it work?",
                    "label": 1
                },
                {
                    "sent": "Yes, it per iteration is just as good as sequential stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Details at the paper.",
                    "label": 0
                },
                {
                    "sent": "What I want to get at is it fast?",
                    "label": 1
                },
                {
                    "sent": "So we had exactly the same code written on top of Hadoop MapReduce and on top of this framework which again runs on top of the Hadoop infrastructure and MapReduce is about two times faster on 8 machines then on a single machine which is not all that great in our framework.",
                    "label": 1
                },
                {
                    "sent": "If you just do 10 passes we already 20 times faster.",
                    "label": 0
                },
                {
                    "sent": "That's because we keep stuff in memory so we can expect a super linear speedup.",
                    "label": 0
                },
                {
                    "sent": "And in the limit, if we remove the data, initial data loading cost, we are 30 times faster.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "I'm time's up.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}