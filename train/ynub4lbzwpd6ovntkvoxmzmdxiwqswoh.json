{
    "id": "ynub4lbzwpd6ovntkvoxmzmdxiwqswoh",
    "title": "Introduction to Graphical Models for Data Mining",
    "info": {
        "author": [
            "Arindam Banerjee, Department of Computer Science and Engineering, University of Minnesota"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/kdd2010_banerjee_igmdm/",
    "segmentation": [
        [
            "Closer to reality, right?",
            "Compared to the naive idea assumptions, it makes them closer to reality and you can capture domain knowledge in specifying."
        ],
        [
            "Dependencies, however the downside of graphical models is that learning or inference is much more difficult, so you cannot simply do maximum likelihood inference like I have a few samples IID drawn from a Gaussian.",
            "I want to find the mean.",
            "Well, I average them out, right?",
            "It?",
            "I'm done, it's not going to be that easy for graphical models because you're assuming things are they depend on each other and we will see how we do about doing estimation and inference."
        ],
        [
            "Um?",
            "So there are two flavors of graphical models that are in common use.",
            "Wanna directed models and the other is undirected model so so typically you represent this set of variables of interest as a graph where every node is available random variable which may be observed which may be hidden.",
            "I'm going to give you plenty of examples and then the edge depends shows the statisical dependency right?",
            "So if it's cloudy it'll rain.",
            "So if you have available for cloud variable for real will be an edge connecting them right?",
            "So that that kind so the directed models are what we also popularly called Bayesian networks.",
            "They capture directed or causal dependencies between variables, so there are five variables over here X 14X5.",
            "What this graph captures is that X one X2 causes X3 which then goes ahead and causes X4 and causes X5 right?",
            "So so that's what the graph capture, capture.",
            "So the direction of Arrow is sort of implied causality, so that's Bayesian networks and most of the tutorial will focus on directed graphical models or Bayesian networks.",
            "The undirected graphical models just show statistical dependency without implying any direct, gradually, right?",
            "So so examples are Mac of our conditional random fields.",
            "For example, if you are doing image analysis, if you're trying to figure out the value of this pixel.",
            "So if you're taking a satellite image and you're saying you know this is a blue, that's a.",
            "See.",
            "This is a C doesn't see, so the pixel in the center will.",
            "There's a high likelihood.",
            "That'll also be a.",
            "See because you know this is so it has dependencies on its neighbors, but it's not cautious.",
            "Right, so it's also not have extensively used in image analysis and conditional random fields over the past seven.",
            "Eight years have been extensively used in various kinds of.",
            "Structured prediction models.",
            "We are not going to spend anytime on undirected models.",
            "However, the inference methods that I'm going to be discussing today.",
            "This should be a shared between the two classes of models.",
            "Many of these inference methods are shared between the two classes of models.",
            "However, the motivation from which underrated models come from are slightly different.",
            "There are models which have both directed and undirected components.",
            "We're not going to get into those, those are those are a bit advance."
        ],
        [
            "Alright, so So what are vision networks?",
            "So you have already seen this example, so there are five variables X1 for X5 and let's say the Bayesian network you're interested in X one X2, Cos, X3 and the next three causes X4 and X = X five.",
            "So this kind of a structure let's you do is that it lets you write the joint distribution of these five variables in a nice way.",
            "What is a nice way when the joint distribution of N variables can by chain rule in probability theory can be written as quality of exile given all the previous variables.",
            "This is true for any.",
            "Distribution, what you can do in case of vision networks is that since each XI depends only on its parents and his Commission independent of the other guys given its parents, you can simplify that factorization where each Excel not does not depend on everything before it.",
            "But we the parents.",
            "So I idea about example in a bit so so this is the advantage of graphical models that you do not have to worry about the full joint distribution because it factorizes in a in a nice form.",
            "It's marrowfat."
        ],
        [
            "Others.",
            "OK, so first example, a start slide means if you have fallen asleep you want to wake up because it's something new and interesting.",
            "You want to remember.",
            "So, so this is a concrete instantiation after those five variables that I was talking about.",
            "And you know, I have to acknowledge.",
            "Navigate, this example is from the book I'm going to give the reference to the book.",
            "At the end they have generously allowed me to use these.",
            "Some of these examples in the talk, so all of these five are brilliant variables.",
            "Burglary can happen or not happen, or quicken happen or not happen depending on whether burglary and output is happening.",
            "The alarm go off or you know, and then you have to label.",
            "Jonah may depending on whether that alarm is on, you know, don't make all unique also so.",
            "This is a typical stochastic scenario, right so so?",
            "I've already spent 001.",
            "The probability of awkward is .002, and then depending on the conditions of burglary and earthquake, there's a probability that the alarm will go go F1 minus that would be the probability that Alan does not function, and Similarly so this specifies the complete joint distribution among these five variables.",
            "If you had to specify this using a table, you know there are five variables, each one of them is a brilliant, so there will be 2 to the five or 32.",
            "You know you have to specify table of 932.",
            "In this case, you were having to only specify little conditional probabilities.",
            "What is my condition product given its parents and that is all that is needed, right?",
            "So if you're if this becomes important once you're being with being millions of variables, you do not have to work with the two to the million table.",
            "You can specify this little little local tables and that'll be good enough."
        ],
        [
            "Alright, so So what?",
            "This lets you do is that these tables will let you compute the probability of any event in this in this world.",
            "So let's say if you want to compute the probability of Bea, JM.",
            "These are random variables for those individual events.",
            "The first line shows you what chain rule gives you a priority of be even be a given B and so on.",
            "So that's straightforward.",
            "From chain rule.",
            "The second line shows once you've looked at the conditional independence structure, well, B is B, provided me given the Valley does not depend on B, so.",
            "After does not decide to happen depending on whether there's a burglary going on, so there is no dependency there.",
            "A probability of a given be evil alarm depends on both burglary.",
            "An earthquake John calling you does not depend on BE an 8 only depends on a, so only that time remains and America does not depend on all four, but only once a back door remains.",
            "So in this factorization now you can see that you can now read of these values of the table that are there and compute the probability of an event right so?",
            "For a concrete example, let's say we want to find a probability of a burglary happening up quick not happening alarm happening.",
            "John not calling Ann Marie calling right so that I can write like this.",
            "Put plug in the values from those tables and I get the exact probability.",
            "You can do this for.",
            "You know 2 to the five events.",
            "Now this this does not matter for this particular Bayesian network, but if you're dealing with vision network with thousands or millions of nodes.",
            "This local factorization becomes super crucial to be able to build tractable models, right so?"
        ],
        [
            "Right, so let's look at another example.",
            "Their network, so cloud is a Boolean variable.",
            "Rain has a dependency on cloudy.",
            "You invested money in this super fancy sprinkler which sends us where it's cloudy or not and you know goes on and off depending on weather it's cloudy or not and both sprinkler or rain can cause this final event that you trust becomes wet.",
            "So this is, again, you're specifying the conditional probability table.",
            "You know cloudy chance of cloudy is half depending on cloudy.",
            "You know the chances of rain being true is higher, and the chances of the sprinkler going off is actually lower and both of them together leads to wet grass.",
            "There's a difference between this vision network."
        ],
        [
            "The previous one we had seen over here.",
            "So the difference is something that plays a very important role in inference inference."
        ],
        [
            "I love them.",
            "So in this network base available like wet grass which can happen in two different ways.",
            "Realization of that variable can happen in two different ways, either through a sprinkler or trade."
        ],
        [
            "Whereas in this case, well, you know there there are no loops, no junk cars can only happen if the alarm goes off and the number that can only happen if burglary happens or earthquake happens.",
            "So if you look at that just available in structure of this graph, it's essentially a tree."
        ],
        [
            "But as this is like a loop, then there's a loop in there.",
            "If you if you follow up with arrows and things like that.",
            "Then there are loops.",
            "There are multiple more than one ways of something happening on.",
            "In these cases, inference becomes more difficult."
        ],
        [
            "Tree structured while inference as we will see people have figured out how to solve this problem.",
            "So this is considered assault problem.",
            "But I'll show you that's all."
        ],
        [
            "It is fairly elegant action."
        ],
        [
            "Now you know you can start building complicated models for you know whatever you if everything is, you will wake up in the morning, start your car won't start so that red thing is your primary observation.",
            "Green things are things, probably you either know the value of our you can check and we can use a dipstick.",
            "You can see the light is turning on and then you know this is sort of your vision network.",
            "Don't do anything that does not work.",
            "You're probably better off calling card."
        ],
        [
            "About something so OK, So what is the main problem that we're trying to solve?",
            "So I have a Bayesian network for my domain.",
            "The main problem that one is interested in solving in graphical models is what we call inference, right?",
            "So some variables in a Bayesian network have been observed.",
            "John hasn't you're sitting in the office, you have this vision.",
            "Network Jam has not called.",
            "You may have called you.",
            "You are trying to figure out whether there's a burglary going on at your place.",
            "Right, So what you want to compute?",
            "You know you know this vision network you have observed trouve two variables.",
            "John has called you.",
            "John has not called.",
            "You may have called you.",
            "You are trying to compute the probability that a burglary is going on because you're interested in it, right?",
            "So that's that's important to you.",
            "This is the inference problem.",
            "So you construct this large vision network for your domain.",
            "You observe some variables you want to find probabilities of other variables, right?",
            "This is the inference problem, and this is where most of the effort goes in.",
            "Of course in this.",
            "Example, you can probably, you know, use basic probability to figure it out, but as the size of your graph grows, you know you need proper algorithms and guarantees on how to figure out that product."
        ],
        [
            "Alright, so.",
            "I give you a very broad overview, us in France and then we will get into some of these in the course of this tutorial.",
            "So if if you're considering a graphical model over but the graph does not have loops like the burglary network, so those are the three structured graphs we now know that efficient exact inference algorithms are possible by efficient.",
            "I'm in linear time, right?",
            "Linear in the size of the graph.",
            "This was not known for a very long time and what people were doing.",
            "People were coming up with.",
            "Individual instances are very clever looking algorithms.",
            "It started from the 1960s common filtering, 1970s Viterbi decoding forward, backward algorithm for hidden Markov models, 1980s belief propagation till towards the end 90s people realize we have the same I got.",
            "Right, so, and this realization happened towards the end of the 90s.",
            "So if you have seen hidden Markov models, there's some complicated thing that happens.",
            "And if you have seen some other models like belief propagation goes out, the same algorithm, and these people realize about 10 years back, and now there's a new clean understanding why we can do that?",
            "Why, hmm, Caesar is a special case.",
            "I will spend some time on this algorithm because this is important that cover several previous algorithms that have been discovered and discover since the 60s till the late 90s.",
            "However, if the graph has loops, there are some ideas out there, but in general this problem is very difficult, right?",
            "So there's a broad class of algorithms called junction tree algorithms which somehow convert converts the graph with loops to a bigger graph without loops.",
            "That bigger graph can be exponentially big, so even a linear time algorithm in that bigger graph is an exponential time algorithm in the original problem specification, I'm not going to discuss junction tree algorithms, But this is a very important class of algorithms and there's.",
            "Extensive literature on this.",
            "There is another class of algorithms which says that you know we are going to do whatever we were doing for trees, which is this message passing some product algorithm which I'll discuss but will will go out as if there are no loops, will keep doing the message passing anyway.",
            "And surprisingly, that thing works.",
            "But we still don't know exactly why, and I give you examples of where those are used, including your cell phones.",
            "So so it's scary.",
            "I mean, we need to understand why this thing works, so that's message passing.",
            "I'll briefly touch upon the general case, but mostly most of the work that happens with graphic loops is what we can bring."
        ],
        [
            "Are you put under approximate inference and then there are two broad classes of approximate inference algorithms, one are called variational inference, so variational inference makes a deterministic approximation of the problem.",
            "So you convert that marginalization problem.",
            "I want to find the probability of already given John didn't call Mary call into an optimization problem, you actually convert the problem into an optimization problem, and you show that you know somehow solving this optimization problem gives me a lower bound to the original problem, and things like that.",
            "We will see very nice examples of this whole method in a bunch of text analysis an I'll give you real in applications where we have used it.",
            "Other people have used it.",
            "There are many variants of variational inference, so it's kind of stands for a broad class of algorithms.",
            "Many of these ideas came from statistical physics, like mean field, better approximation, Kikuchi approximation, EP expectation propagation which is a more recent development by timing and team.",
            "Battle class is the more traditional approach, which is, you know, stochastic inference which has been around in statistics for a very, very long time.",
            "No past 5060 years.",
            "Markov chain Monte Carlo methods, and then a special case of that Gibbs sampling, and I will see how to adopt these methods for some of the large scale data mining problems and what kinds of results."
        ],
        [
            "They are giving alright, so that's the overview right?",
            "So that so that was overview.",
            "Now we're sort of entering the first part of our talk.",
            "But we will be worrying about tree structured graphs.",
            "And consider inference problem really understand the sum product algorithm as it is called NOW, which has been known by many different names for the last 3040 years.",
            "We will briefly lasted this with an example.",
            "There are also touch upon hidden Markov models, but I won't get into the details and there is an excellent tutorial by Lawrence Turbine on Hmm's which essentially cover."
        ],
        [
            "So these things.",
            "OK, so so getting back to the inference problem, I have a vision network which is tree structured, meaning that if I look at the background structure of the graph, there are no loops.",
            "And I have observed two variables.",
            "Littlejohn calls Mary calls.",
            "I want to find the probability of burglary right?",
            "This is something I'm interested in, so let's try to do this computation right.",
            "How will we do this well?",
            "I said I would visual or at least you know some way and I think what that says is the probability of B given something else.",
            "Is the distribution of be with those things divided by the probability of those things.",
            "This is basic probability.",
            "We know that something I am done if I can compute the top thing on the bottom thing alright."
        ],
        [
            "So let's compute that.",
            "So in order to compute the top thing I say I can take the joint distribution and I can sum out the variables that I don't need, which is.",
            "This is the process of marginalization.",
            "I sign out awkward ice amount around because those are things I don't need and then that gives me the marginal similarly probability of J, N. You know you get the marginal.",
            "You take the issue, you're done, so you must be seeing what are you talking about.",
            "This is easy.",
            "So good, so consider this scenario where there are not three or five variables.",
            "But our 1 million variables, each one of them that is brilliant.",
            "And what you are doing is you want to sum over all of them other than three or two.",
            "So it's, um, is over, you know, eating one or 08, picking one or zero, you're selling over a million.",
            "We invariables with both possibilities.",
            "That is due to the million so so, so the number of times in that submission grows exponentially in the number of variables.",
            "Right so.",
            "So if you actually do a complexity analysis of this algorithm, since each term can be factorized like this and can be computed in order N, and the submission will effectively be over 2 to the end, the complexity of these naive inferences order N * 2 to the end.",
            "Right, if you can do this nice team fine, you don't need any other thing I'm talking about.",
            "If you are working with a serious application which has at least you know 10s of thousands of variables that naivety is not going to work right.",
            "OK, so this people really realize long time back.",
            "In the late 60s and early 70s, the development of hidden Markov model was sort of officially the first time people realize, OK, you know if I'm trying to compute probabilities with hmm, I cannot simply do that Bayes rule thing because it's going to take forever.",
            "So this problem is faced by an HMM and I'll tell you in a minute one hmm is so OK.",
            "So the live inference idea is not going to work, so we."
        ],
        [
            "Have to be smarter so there has been several developments, but what I'm presenting is the currently agreed upon.",
            "Standard formalism using factor graphs an in the vector.",
            "Rats are nice because you can map both directed models as well as undirected models, factor roughs.",
            "So this shows how to convert a given vision network to a factor graph.",
            "What is a factor graph well?"
        ],
        [
            "In your factorization of the probabilities over here, you can see that the joint distribution factorizes in a bunch of local factors, right?"
        ],
        [
            "So you make that observation and that will be true for a major network.",
            "Then you add a factor to every variable and if a factor is a bunch of variables, participates in it.",
            "It connects to all those variables.",
            "So let's say this factor.",
            "Is more of the factor of X1 only, whereas this factor involves X one X2X3 because you know it's this factor.",
            "This factor involves X3 and X Fort Sam function.",
            "In our case these are conditional probabilities.",
            "This involves and that involves this thing, right?",
            "So so this is simply a redrawing of the vision network.",
            "I'm not doing anything fancy right?",
            "I'm simply redrawing the Bayesian network where I'm replacing this conditional probabilities with the factors with edges to all the variables that are involved.",
            "In that condition, probability OK. Now, so that's a factor graph.",
            "So now I can take this factor graph and write it a little bit more nicely as a sort of a bipartite graph where I put all the factors on one side and all the variables on another side.",
            "And I say that the joint distribution over these five variables factorizes as before, so nothing has changed.",
            "I'm just using a different notation, and I'm redrawing that factor graph like this."
        ],
        [
            "Exact same graph, so you can verify that you know you take all all those factors on one side on the variables on the other side you get this.",
            "Alright, so So what is the inference problem in this factor graph world well?",
            "If you have a, if you have a function of multiple variables which factorize are, which factorizes product of local functions.",
            "So that's far from our Bayes net.",
            "What we're trying to do is we are trying to some out everything other than XI or everything other than in Exponent X5 or something like that.",
            "So what we're trying to do is some out a whole bunch of them, only keeping one or two unchanged, right?",
            "That's the whole marginalization process."
        ],
        [
            "And that not some notation.",
            "What I mean by that not of examines that you sum over everything other than XI, so that's just a convenient notation.",
            "Now, this class of problems surprisingly show up in many, many different places, so we are focusing today on Bayesian networks in graphical models, which shows up in coding theory in communication, error correcting codes, many different places, and they are used in many different places.",
            "I'll show you examples of that, so the problem broadly is called.",
            "You want to merge."
        ],
        [
            "Please pull up the functions.",
            "How do you do it?",
            "Well, we never that naive idea won't work.",
            "I cannot simply do exactly what this equation is saying, which is you some over you know everything other than Excel because that will take exponential time.",
            "Between observation, we sort of make over here is that my submission is over this function.",
            "My summation of what?",
            "This function and this function function factorizes.",
            "So, it's, uh, some of our products of local functions.",
            "Now I'm.",
            "We go back to our beauty algebra days.",
            "And be equal that when we have a sum over product of things.",
            "I can take the common piece out and then some.",
            "Only the things that cannot be taken, so this known as distributive law, baby plus AC can be simplified as our written exactly equivalently as 8 * B + C. OK, so what's the big deal?",
            "The Big deal is if you look at this equation from a competition perspective, the left hand side involves two multiplications and one edition.",
            "The right hand side involves one multiplication and one edition.",
            "Right, you can afford this problem.",
            "You save in computation and multiplications are more more costly.",
            "So what we're going to do is we are going to sort of look at this and try try to construct efficient algorithms for doing the inference.",
            "In particular, we're going to try to push the submissions as far inside as possible.",
            "So far in OG, one of expand where we're trying to sign over everything other than X.",
            "When we push the submissions inside as far as possible so we do local Sam's and then we multiply and keep going like that.",
            "Alright, you can take any other factor like G3X3, you can push the local sums.",
            "This starts looking.",
            "You know, this starts to look more interesting because now we're saving on competition.",
            "We're not doing repeat computations because because we have pushed things inside.",
            "However, this does not solve your problem, because if you really have that million revision at work, you do not want to sit down with pen and paper to try to push submissions inside.",
            "So there has to be an algorithm way of doing this, or this, or something like that, right?",
            "So so, so that's really what the sum product algorithm does.",
            "It sounds like a whole bunch of variables while keeping a bunch of them unchanged."
        ],
        [
            "So, so the main idea is as follows.",
            "Let's say you pick a target.",
            "Now let's say G1X1 I. I want to find the margin analytics one.",
            "You you draw the graph, it's the same factor graph, I'm just you know, drying it in different ways with X one.",
            "But I don't know that that app.",
            "And the rest of the thing has a tree starting from that.",
            "If I'm interested next three, I will sort of take.",
            "X3 on top and I'll write it like that right?",
            "So so first decide which one you want to compute.",
            "Let's say we want to compute X one, then I'll drop this tree like this.",
            "Then I start passing messages from the leaves all the way up till X one and I don't want to specify what those operations are."
        ],
        [
            "Right, so let's get into the details.",
            "So the messages that I'm going to pass you know it's going to be a very simple algorithm, But what it's going?"
        ],
        [
            "Door is exactly this, like it's exactly this, but it's an algorithm key of doing this.",
            "Up"
        ],
        [
            "So when."
        ],
        [
            "So in in a tree in this kind of a tree, you can encounter either variable nodes or factor nodes.",
            "So we will have two specifications.",
            "What happens to the messages when you encounter a variable node and what happens to the messages when you encounter a fact?"
        ],
        [
            "When you count available now, you simply multiply everything that's coming in and pass it up.",
            "That's it, right?",
            "That's all you have to do.",
            "When you encounter factor mode, you simply multiply everything that's coming in from the children with the factor.",
            "And pass it up by doing a son of a map of the parent.",
            "So you send out everything other than the parent right now.",
            "So these are local functions.",
            "At this point, each factor will have only a few variables other than the parent, so this not some operations are going to be fairly straight for."
        ],
        [
            "Going back so you can see that this not some operation is only over X3.",
            "This is only over X3 and so on.",
            "So those are those are much smaller untractable notes on operations and will do that."
        ],
        [
            "Refactor node."
        ],
        [
            "Alright, so So what happens over here?",
            "Then you have your factor graph because that came from the structure of the business.",
            "Then you replace the variable nodes by the product.",
            "The factor nodes by product over everything that's coming in multiply with the factor passed on with the sum.",
            "If you actually.",
            "So you start the computation from here and we move up.",
            "If you see what you have computed over here, it's exactly this.",
            "Right, so this is a simple algebraic exercise.",
            "To verify that you are going to get exactly that expression which which has marginalized out all all the variables other than X1.",
            "Now you can readily see this is a linear time algorithm because every every word or every factor you know you do one operation per node or power factor.",
            "There are any man knows around factor, so so is it saying plus EM algorithm as opposed to end times two to buy an algorithm which we were dealing with before.",
            "Let's look at another example.",
            "So the three X3.",
            "You simply convert the factor graph into this computation tree.",
            "You know, past messages like this encoding.",
            "This is fairly straightforward because it's modular structured operation is the same, and on the top you will get exactly this.",
            "So the beauty of this algorithm is."
        ],
        [
            "Yes.",
            "But the structure of the business actually includes a very efficient algorithm.",
            "At which was long overdue from people are trying to sort of come up with efficient ways of doing this.",
            "One of the biggest success stories of this algorithm is hidden Markov models, right?",
            "So some protocol Gordon is extensively using hidden Markov models.",
            "These exact same algorithm, right?",
            "So it is known by different names.",
            "It is known as a forward backward algorithm will be decoding algorithm and things like that.",
            "But it's exactly this algorithm, so this is very reassuring.",
            "You have a linear time algorithm."
        ],
        [
            "For these things, so let's talk a little bit more about hidden Markov models.",
            "I'm not going to get into the details, I'm hoping some of you are familiar with Hmm's, but otherwise you know this is one of the most widely used probabilistic graphical models since the 70s, which have been used in many, many different applications originally came up in speech recognition but has been applied to many other way applications ever since.",
            "So so hidden Markov model.",
            "This sort of a temporal dimension to it.",
            "You have a little sequence of variables this 81.",
            "Z T -- 1 Ziti, ziti plus one.",
            "And so on.",
            "And you are observing Maxi minus one XD XD plus one.",
            "So these are bugs are variables.",
            "You are interested in, you know, various steps, so this is the corresponding factor graph and you are interested in France.",
            "Problems like what is the probability of what I'm observing, right?",
            "Or what is the probability of a certain latent variable given what I've observed, I'll give you a concrete example, right?",
            "So this is an example which helps you ground without getting into speech recognition.",
            "Let's see where this devoted researcher who sits in your office all day never goes out.",
            "You basically live there so and so.",
            "What XD is you want you want to say?",
            "Ziti is available with whether it's raining outside and XD is the variable whether your office coworker is getting an umbrella or not.",
            "Right, so you get to see whether this person is carrying an umbrella here.",
            "He's a nice guy.",
            "He checked the weather in the morning and carries a number letters, chance of rain and just looking at whether he's carrying a number.",
            "You can see that every day you are trying to infer that it's raining outside or not.",
            "So example there are more serious examples for Hmm's.",
            "So, So what this computes is the probability of this sequence, right?",
            "If it if it's very less likely, then you know these guys tricking me.",
            "He brings number every day that God be.",
            "I mean it does not run everyday, so you compute the probability said this is, you know 10 to the minus 50.",
            "So this guy is.",
            "I don't believe this guy.",
            "The second problem is you have observed this and then you're trying to figure out where this guy said that you know it had rained last Monday.",
            "What is the probability given that he got an umbrella on Sunday?",
            "Didn't bring on Monday brought on one on Tuesday and so on and so forth.",
            "So this is known as smoothing in other applications like Kalman filtering.",
            "So you made a whole bunch of observations.",
            "You're trying to figure out the latent variable.",
            "There is another class of inference problems.",
            "One is interested in.",
            "Hmm's is given that you have observed all of these XTS umbrella, not umbrella.",
            "You are trying to figure out exactly what has happened outside in the last 30 days from day one to day 30 for the train.",
            "What is the most likely sequence of real events now?",
            "So what I'm showing down below is the factor.",
            "Have corresponding to this and if you know the same reptile garden which I've just shown you, you can actually solve at least this problem pretty efficiently because you know, you take ZT, make it the route the rest of hmm becomes that tree, start passing messages, you'll get it.",
            "That is exactly what was discovered in the 1970s known as the son of a forward backward algorithm.",
            "This algorithm has also been considered and used in conditional fields, which is undirected graphical model and change structure.",
            "Condition random fields have exactly this factor graph, right?"
        ],
        [
            "So OK, so.",
            "So this is the same product algorithm, so it has two simple rules.",
            "You construct history at every variable node.",
            "You multiply everything that's coming from your descendants and the same product rule which is at factor node.",
            "You take the product of F with his descendant.",
            "No, let's say you're saying OK, this is great.",
            "I can compute the probability of X1 and X3.",
            "Plan what I want to compute the probabilities all over all of them simultaneously.",
            "One way to do that would be to do them one at a time, which will be N times you know the total number of variables.",
            "However, you can be even more efficient when you are trying to find find out all the marginals simultaneously.",
            "And that is really the full blown glory of the sampler Calgary.",
            "I see what I mean by that.",
            "So what you are trying to compute.",
            "Is the this probability for Owl City?",
            "Write simultaneously.",
            "Want to find the probability for algete simultaneously?",
            "So how do you do that?",
            "Well, instead of going GI GI XI one at a time."
        ],
        [
            "I want to find all of them.",
            "Well, we are going to do simultaneous message passing, right?",
            "So we are going to take that that algorithm that we were using by constructing the tree.",
            "The one-way single route anymore messages will go out the place and but they'll be allergic to it.",
            "So what we will do is as follows.",
            "That inner variable mode once I have got messages from everything other than one side, we will multiply those things and pass it on.",
            "So that's that's available to local function an on a local function.",
            "If I have got all the variables, all the messages coming in to that factor, I'll multiply it with the factor and pass it on.",
            "Right, this is what I want to do.",
            "This is what I was doing before.",
            "I just need to make sure that I have got all the messages I need in order to pass it on.",
            "So what I'm going to do?"
        ],
        [
            "'cause I'm going to walk through an example.",
            "So this is just."
        ],
        [
            "Zoomed in version of that walk through an example of our factor graph that the one we have been working with starting with the Boundary Network, but I would have to simultaneously find the marginals at Earl these five variables, but I don't want to do this one at a time five times, because that just starts becoming slower.",
            "So if I do that enough for each variable, it's a linear time algorithm.",
            "Have enviable, so this becomes a quadratic algorithm.",
            "That's bad.",
            "I'm going to maintain the linear complexity and find all of them right, so that's that's possible, and this be really becomes so if you have your dynamic programming part of the brain waking up.",
            "This is really a very nice form of dynamic programming.",
            "So, So what do I do over here?",
            "Is passing messages from the extreme leaves of this graph so so from FA2X1 from FB2X2, from X-412-D, and from X52 Fe?",
            "So that's the first set of messages.",
            "Now X one has them as a factor mode X1 is a factor node which has the message it needs, so it can multiply everything that's coming in.",
            "Well, there's only one message coming in on.",
            "It starts passing so that."
        ],
        [
            "Stage two, so expand passes it forward, extra process it forward.",
            "The passes it forward.",
            "If he passes it forward an and it proceeds like this.",
            "So now, at this point FC The factor has two messages coming in from X1 and X2, so it cannot multiply those things with FCA."
        ],
        [
            "Pass it along to X3 so that becomes a third piece.",
            "That's the third step of the algorithm.",
            "You pass that message on, and similarly for X3 has two messages which came in in Step 2 that it passes a message on, and so on.",
            "So you see."
        ],
        [
            "You keep going like this.",
            "Still, you have basically passed messages."
        ],
        [
            "On every edge."
        ],
        [
            "Both directions.",
            "That's linear time, right so that."
        ],
        [
            "Two times the number of edges, sorry."
        ],
        [
            "And the last step in order to compute the marginals at every point, you simply multiply all the incoming messages at any point in the algorithm.",
            "It got an incoming message you just multiplied along, so at every incoming edge it'll receive only one message and you simply multiply it along.",
            "You can show.",
            "But these will be the exact correct marginals, and this is this is sort of a linear time message passing algorithm which correctly computes a marginal for all variables involved.",
            "So this practical is sales.",
            "The inference problem for tree structured graphs, right?",
            "We will move on to more fancy models for, you know, text analysis and collaborative filtering where this trick unfortunately is not going to work because it's those are more complex."
        ],
        [
            "Mods, so just to revisit HMMS quickly.",
            "So if I if I want to sort of compute the probability of the observations, I don't know anything about the hidden variables.",
            "I'm just interested in probability after an umbrella carrying behavior for the last 30 days.",
            "What this really involves is that you have the joint distribution over all the axes and I'll disease.",
            "All these ex is a mild disease.",
            "You use em out all disease.",
            "Right, you marginalized out all disease and what you're left with is exactly this probability.",
            "The way this is done.",
            "If you look up Wikipedia.",
            "Our Readiness survey, or any other things this algorithm is known as the sum product algorithm, and this is also the further backward I got because you know it will have to maintain some parameters.",
            "It'll have to move forward.",
            "Go back.",
            "This is exactly a special case after algorithm.",
            "I'm talking about the sample held all right so, but of course you know this is an example.",
            "This is a brilliant discovery.",
            "It happened in the 70s.",
            "They realize that you know for this hmm, I can come up with a very efficient way of computing the probabilities of observations or computing this smooth probability.",
            "You know, given all observations, what is the probability what happened on day T, right?",
            "You can compute that and that's you called smoothing, but that's more widely used in common filtering so.",
            "Carbon filtering is essentially also a variant of the same idea and the basic unknown filtering, and this is known as smoothing in Kalman filter.",
            "So these ideas have been discovered several times in the past and they they have grown into their own sort of sub sub areas with many many extensions and so on.",
            "So what I'm saying is that the basic algorithm which was discovered in the in the 60s or 70s or belief propagation for business in the 80s there all instances of this one algorithm which we call the sampler.",
            "So, and there's there's a beautiful paper which I'm going to talk about at the end in the reference is the name of the paper is factor graphs?",
            "Amazon work?",
            "I'll go down, which basically explains this whole history and algorithm.",
            "That way I did it.",
            "So I'll have a few references on this and you can look it up in the slides later on."
        ],
        [
            "So this this helps."
        ],
        [
            "Observe like you know, 3040 years of literature in one algorithm.",
            "So you know these albums look complicated, but you know once you understand this genetic structure, you know all these things fall in."
        ],
        [
            "Now I'm.",
            "Some people got a little sort of more ambitious.",
            "They say they'll be structured that some of our products of local functions question."
        ],
        [
            "Greater risk if you have two parents that are not just binary.",
            "Ship"
        ],
        [
            "Sure the.",
            "Messages will be vectors.",
            "So so if if you look at like this factor and X1 picks 10 values are next to takes 10 values, then these messages will be one value for each instantiation of X1.",
            "So in the past that cover 10 and 10.",
            "But if you want to pass a vector of 100, then capture some nonlinear interaction.",
            "So OK, so if X1 and X2 interact.",
            "Right, if there's an interaction between X1 and X2, I will have a factor over here which connects X1 and X2.",
            "So FC is a is a factor that connects X one X2 and X3.",
            "Let's say in addition to this going with your example, I have a factor which just connects X1 and X2 somehow.",
            "Now, as soon as you introduce that factor, that's possible.",
            "That's a valid factor graph.",
            "You have introduced a loop in the factor graph.",
            "Right, so that goes beyond the realm of what this algorithm can crack, But then I will be talking about how you handle those situations.",
            "Then you get into what is called loopy belief propagation.",
            "You keep doing this anyway, so there will be messages that will go from X1 to that factor annex due to that factor, and that factor 2X1, and that's factored weeks too.",
            "So that's called loopy belief propagation.",
            "It's just an extension of this algorithm.",
            "So, so we stop the message passing in tree structured graph.",
            "There's a clear notion of Waterleaf is right because you know that will be that they don't have any followers of things like that, so we will stop once on every node I have.",
            "On every edge I have seen a message going in either direction in this direction and that direction, and that will typically be not twice the number of edges and is linear in the size of the graph.",
            "So so far for trees, that's why life is very simple.",
            "Once you have a loop, then those things become a problem.",
            "When do I stop right or where do I start and things like that and loopy belief propagation addresses those, but I think that should be another tutorial itself.",
            "Other questions, yeah?",
            "OK, let me actually."
        ],
        [
            "Alright.",
            "App.",
            "So say that again the example I'll work with.",
            "So why is there no factor between X1X3 and X4, let's say.",
            "So it looks.",
            "Driver have.",
            "That's why.",
            "Huh?",
            "So next we go back across next years for his life.",
            "I see what you're saying, OK, I understand your question so.",
            "So his question is that looks like X3 affect X1F X X3, so there should be a factor between X1 and X3 and X2F X X3 one.",
            "There should be a factor we do next to next three but I have put a factor connecting all three and similarly for the bottom of.",
            "Well, what I'm really.",
            "I mean putting the factor based on is that I have this term which involves these three variables, right?",
            "So my factors will come from the domain.",
            "Right so."
        ],
        [
            "In this particular case, if you look at it.",
            "If broadly, and so, if you look at this conditional probability table, if the effect of burglary an alarm so alum depends both on both of them.",
            "Right so so if I have a variable like alarm which depends on both burglary an earthquake.",
            "My conditional probability.",
            "My local function will have state variables which are beyond E and the probability for me if I have burglary affecting alarm only.",
            "An earthquake affecting alarm.",
            "Only then I'll have single factors.",
            "In that case I'll rise in independence between those two.",
            "In this case they are not independent, so if they are independent I I'm going to be more efficient because these factors are going to be smaller, involve less number of variables, so this is an example of a scenario where well and depends on both of them, right?",
            "It's not and well as over here well generally depends on alarm.",
            "If I know the value of alarm I don't need to know anything about anything.",
            "I know what John is going to do and similarly.",
            "Another value of alarm I I don't need to do.",
            "Any of the other variables because I sort of know Mary is in conditionally independent of other variables given allow.",
            "However, in case of alarm, if I only know that a burglary is happening, but I don't know whether an earthquake is happening, I won't be able to know which probability is going to be followed by the alarm, right?",
            "So I need all the arguments in that factor to know exactly what probabilities, but you know, he depending on the domain will be several other things.",
            "But you may be able to come up with the only thing that this algorithm sort of can handle is tree structured graphs, right?",
            "And once you get into the loop examples, you can extend this same algorithm to loopy belief propagation, but that's for some other day.",
            "OK, so.",
            "And actually move forward.",
            "Come.",
            "So."
        ],
        [
            "Alright, so so we were talking about.",
            "You know I'm not getting into the details of any emails because there's plenty of excellent tutorials on Hmm's and my whole point over here is to show that if you want to know why should I even care when a teams is a good example why you should care and you may be dealing with another model which does not look exactly like Hmm's, but the underlying graph structure is a tree and then you don't have to brainstorm because all three problems have been solved, right?",
            "So this we consider solve problem as far as we're concerned now.",
            "I'm.",
            "So there's a huge generalization of this whole framework to summonings, and so the whole observation involved in this, and again, this is not going to be kind of an exercise in abstract algebra, but just to remind you, you know these are these abstract structures which invite two operations, one called addition that are called multiplication, and they may be in a Boolean field then yellow field.",
            "They may be somewhere else they satisfy some basic properties like associativity, community.",
            "They have additive and multiplicative identity's and they satisfy the distributive law right?",
            "This is an abstraction."
        ],
        [
            "I just talked about.",
            "And then you can come up with any number of your favorite signings.",
            "So for example, this is a popular one where the elements are between zero and Infinity.",
            "The sum operation is Max.",
            "The product operation is problem.",
            "The corresponding algorithm is known as the Max product algorithm.",
            "So Viterbi decoding which is used in hidden Markov models to find the most likely sequence of states, is simply an alternative to the Max product.",
            "I got exact same complexity, exact same results, right?",
            "So?",
            "And if you recall, what Viterbi decoding does in case of Hmm's Max product algorithm does the exact same thing right?",
            "So So what happens in the in the Max product semiring is that?",
            "Over here in set up the if this edition is a Max.",
            "Write an ABC are positive.",
            "Then you have to read it as Max of ABN.",
            "Max FAC is a times Max of BARC.",
            "Right, so you find a maximum of BNC and then multiply that VD so you can replace this submission with a Max of the two arguments.",
            "Under distributive law still holds.",
            "So instead of summing over products, if you want to find a Max of our products, the same thing will work.",
            "If you want to compute the Mail over products of the main over some of the Max over some, you have to simply specify what domain, right?",
            "So in this in a match product case it has to be positive, so if everything is positive, the Max of baby and, AC is a times Max of the copy.",
            "So this is the exact same formulation goes through and the people in different domains in error correcting codes.",
            "Everybody know topical words in many other scenarios.",
            "They are interested in other things, which is not exactly submission over products, but then a minimum over Sam's or maximum or products, or some million operations.",
            "And the same idea holds right?",
            "So?",
            "So this is very nice.",
            "So so this whole thing came together in the late 90s.",
            "And then people look back and realize where we have rediscovered this algorithm.",
            "I don't know.",
            "Probably 15 times in the past 30 years, so belief propagation is net mapping friends in hidden Markov models, which is the Max work algorithm or alternative to Viterbi decoding common filtering.",
            "Your accounting codes are plenty of LDC codes, Turbo codes and things like that.",
            "This is a long list, right?",
            "I want to stay sort of close to what we're discussing, but if you're interested in these other things are the more abstract views."
        ],
        [
            "There there is literature out there to follow this now.",
            "I'll spend a few minutes on what happens for general graphs where there are loops.",
            "Well, I'm in pre structured graphs.",
            "We know that we are guaranteed to get the correct solution in whatever Semmering you're working with.",
            "In general graphs.",
            "This is a supremely active research topic.",
            "For the past 10 years and good progress is being made.",
            "But you know this is very much technical.",
            "I don't have a story like the previous part.",
            "I can tell that.",
            "Oh look, we have cracked this problem.",
            "We cannot track that problem right?",
            "So we have not managed to so message passing.",
            "If you generalize, I say I keep doing the message passing through.",
            "Don't care if there's a loop it in principle here convert or it may converge local minima of some objective function, and people are trying to come up with methods which converts to the global maximum of something like that.",
            "I will give some references to this literature at the end, but there are new approaches that have come up in the past few years.",
            "We are trying to get convergent and correct message passing.",
            "Even in loopy graphs, it's a very, very active topic.",
            "Now the surprising bit about this which which I think I mean, I wouldn't have believed that I'm not actually known or met.",
            "The people who are working on it is that loopy belief propagation, this belief propagation algorithm is actually used, so Xbox Live.",
            "I don't know if any of you know what that thing is, that's.",
            "Microsoft Xbox Gold Online so you know, then playing engineer, you go online and play with other players so they have a system called true scale.",
            "Which matches you with other players and that algorithm of sort of player ranking is based on belief propagation.",
            "The hedging idea is they started up from there.",
            "How you know chess Masters are matched up or things like that, but true skill is actually live and running in Xbox Live.",
            "So if you if you go online you try to play your matched up with some player from some other part of the world.",
            "A belief propagation happened right so so essentially what I'm doing.",
            "It's a very very large scale.",
            "Basing it on which message passing happens and these guys of course have fairly technical papers explaining what the factor graph is and how that happens.",
            "The motivation behind this is that if I go online, I want to play some game.",
            "If I match with like a pro by using one minute, I'll hit.",
            "I'll never log in right, but as if I'm so match with somebody who's complete newbie and I will need for one minute, I will not like that either, so I will have to be matched with somebody with similar skill sets.",
            "Under my our past games and so on and so forth.",
            "So this is a little bit more complicated than HMM is actually much more complicated than 18 months, and they do that big application for loopy belief propagation.",
            "Actually essentially what are used in coding schemes in France.",
            "The broadly these these firms class of Turbo codes, which are traditionally studied in the communication literature as convolution codes and so on.",
            "But what the algorithm is, and at this point they also know this is not the belief propagation and it works somehow.",
            "And these are used for of course in that region for defense satellite communication, especially in situations where error correction needs to be fairly robust.",
            "So so if you're getting signals from Mars, you want to do like I can codes you want to use.",
            "This, why Max is just.",
            "Metrolight Wi-Fi network.",
            "It's a protocol, and they use it over there, so these algorithms are used in many different places for very very large networks, right?",
            "So I give you some examples.",
            "Alright, so that sort of brings us to."
        ],
        [
            "When did the first part I will briefly start?",
            "You know, get started on the on the second part and then in another 10 minutes or so we will take the coffee break.",
            "This is how you know I'm coming through this decade.",
            "2000 Two 1001 people started developing even more fancy graphical models and old friends becomes even more complicated, so we'll have to go to Apple inference methods so so one of the biggest developments in this area, and this will be a large chunk of of our discussion today, or what are known as mixed membership models, right?",
            "And if you are familiar with what mixture models are which have been studied in statistics for a very long time, I will start with differentiating between these two and then focus primarily on the.",
            "One of the moon are sort of representative algorithms are mammals, college English allocation used for topic modeling and text analysis.",
            "We will discuss two different ways 2.",
            "Three different ways of doing inference in this model because that exact inference message passing business does not work in this case.",
            "It's intractable, and then I'll consider applications at generalizations including."
        ],
        [
            "And what we recently did.",
            "Alright, so since I'm going to go beyond that, you know five node network I'm going to go and do sort of thousands of hundreds of thousands of nodes network.",
            "I'll introduce some notation.",
            "So this is what we call a plate diagram.",
            "So if you have a Bayesian network where a causes B1B2B3 and you know given a these are independent, we sort of draw this up later and be an put a 3 means this is replicated three times.",
            "This is just a convenient notation and at this point actually some some colleagues are trying to build.",
            "Uh, compilers for for graphical models based on these kinds of specifications, they're trying to formalize it, but for our purposes this is just a schematic which will help us remember that, oh, this is repeated 10 million times or something like that."
        ],
        [
            "Alright, so here's the will start with the most basic model which we will use as a straw man and see how bad that model is, which is just modeling.",
            "Let's say you have feature vectors 3 dimensional feature vectors, Anuar model in each feature as independent.",
            "Let's using abortion, but that is a statisical model.",
            "You have 3 features, you have aggression for each feature.",
            "That's a model, and the way this is not illustrate how to read this thing.",
            "And in this particular case D is.",
            "Is that dimensionality an N is the number of data points.",
            "So what this means is that I have the parameters for the individual domain specific dimension specific distribution.",
            "So let's say this is the mean of the Gaussian 1 version 2, version 3.",
            "And then I generate 3 dimensional access D dimensional access.",
            "And then I generate N off them.",
            "So I generate ended up lines, each one of them is D dimensional.",
            "That's that's how to read that create diagram.",
            "This is not a fun map, right?",
            "This is the you can't do much with it.",
            "This is as bad as you are trying to do.",
            "To document modeling you have for every word in the English dictionary you have abortion.",
            "What is sort of the frequency of usage and you simply generate a document counts or word usage counts by independently sampling from the dictionary.",
            "That's no good, right?",
            "So that's not an interesting model."
        ],
        [
            "So what we do is we will start with nine days models or mixture mixture models, which is sort of a slight generalization of an Ivy structure.",
            "Which looks like this, right so?"
        ],
        [
            "So, so they essential structure of the model is that I don't have so previously."
        ],
        [
            "We look at this thing that in each dimension I in each dimension I had.",
            "One Russian right?"
        ],
        [
            "What now?"
        ],
        [
            "TV series is that well.",
            "I have politics.",
            "I have documents from politics and documents from sports.",
            "And the word distribution for documents from politics.",
            "So this is sort of the, let's say your dictionary is 3 dimensional and this is a.",
            "So they look like this.",
            "So if I'm using politics then they look like this.",
            "If I'm using a sport they look like this.",
            "So so I have two different distributions I meant.",
            "Any Member, English Dictionary, one which explains politics documents other which explains Sports document.",
            "That's an improvement, right?",
            "That's an improvement.",
            "And the way this generative model works is that for every document right for every document.",
            "What I do is I have a prior so OK, so for over the entire corpus I have a power.",
            "We sort of looks like this.",
            "How many of them are politics and how many of them are sports documents?",
            "Let's say no.",
            "This is some sports website.",
            "Some rest of their documents are sports documents and a few of them are politics documents.",
            "To generate a single document, I first sample.",
            "One of the two things right in this case I sample red, which is, let's say sports.",
            "Then I go to the distribution specification for sports.",
            "And I generate my feature vector according to that.",
            "Right?",
            "Now what I don't know.",
            "So this red thing is busy, which I don't know what I know.",
            "The only thing I know is this right by which is my final document or my final feature vector."
        ],
        [
            "Is another example.",
            "I sample from this district discrete distribution again, I get blue, so it will be a politics document, so I look at these distributions and I generate a feature vector.",
            "Right, so this is how each data point is asumed to be generated.",
            "I mean nobody generated it like this, but we are assuming that this is how they came into being.",
            "So we have observed sort of this X, Theta and power the parimeter.",
            "So so this is by you know these things are Theta Z.",
            "We have never observed right?",
            "This is sort of the mind based model."
        ],
        [
            "Ah.",
            "Now we."
        ],
        [
            "Not the girl.",
            "One more step further and this mixture model I.",
            "This has been around for a very, very long time."
        ],
        [
            "The more recent development is this last piece, right?",
            "Which is the mixed membership model.",
            "So what a mixed membership model adds over and above you know this thing is that what this would allow you to do?",
            "It assumes that our document can either be on sports bar on politics, or an entertainment, or one of these things.",
            "It is not going to allow our document to be about politics in sports.",
            "Because you have to commit, it's a mixture model.",
            "You have to commit it to one of the components, right?",
            "So what mixed membership models actually do is that they say I am going to allow a data point or a document to have mixed memberships in multiple of these themes or topics."
        ],
        [
            "Right so and we will see a very concrete example of that in a few slides, probably after the coffee break.",
            "So So what this model essentially starts off with.",
            "Is a prior distribution overall mixed memberships or discrete distributions?",
            "Now if you think of a discrete distribution, these things live on a simplex, so if you think of a discrete distribution about three things they have to satisfy, P1 plus B 2 + 3 equal to 1, and each one of them is greater equal to 0, right?",
            "So they live in a simplex, so one of the popular choices for this priorities additional distribution which defines a distribution on top of a discrete distributions.",
            "But there are many other choices that people have exposed, so the way one I assume so.",
            "So this this all fire determines that power, which may be additional distribution.",
            "These are the parameters for individual themes like politics, sports and so on and so forth.",
            "The way we assume a data point is generated is as follows from this dish redistribution I for sample mixed membership right.",
            "It says 20% sports.",
            "You know 80% politics or something like that.",
            "Then to generate every feature, now we cannot go to simply the blue one or the red one.",
            "In general, the features right?",
            "So what we do now is that for every feature we sample from this district distribution.",
            "So we sample from it, we get a green.",
            "We look at the green distribution over the blue we get from the blue distribution.",
            "Over here we generate the first feature.",
            "We sample from it again.",
            "We get all red from this thing.",
            "We generate sample a feature we sample from it.",
            "Again, we get another head and from from this red distribution we get another feature.",
            "So.",
            "This is this is how mixed membership model is different, right?",
            "So let's look at it."
        ],
        [
            "For example, so I sample a mixed membership and this will be specific for every data point.",
            "We will have mixed membership.",
            "We sampled the color for the first feature.",
            "We sample from that version color for the second feature sample from the corresponding version for the third feature General General, and then we have the features right."
        ],
        [
            "So.",
            "To sort of get a big picture overview of the difference between mixture models and mixed membership models is that the observations are same rights observations.",
            "In this case you know it's just three dimensional vectors over here 3 dimensional vectors.",
            "These are different samples.",
            "This assumes that it was generated by the red component and this was generated by the blue component, whereas this assumes that while the first one was more from the road a little from the from the blue and so now you can actually see.",
            "But the left hand side example is sort of a special case where if this was a hard word and this was all a blue, then that would sort of fall back to the mixture mixture model.",
            "However, these models are more flexible because they are they are letting the individual data points have mixed memberships to multiple of these components.",
            "Alright, so now one of the biggest success stories of mixed membership models.",
            "I mean this sort of started bringing towards them 90s again.",
            "Thomas Hoffman's work on probabilistic latent semantic indexing, which sort of presented this idea and then."
        ],
        [
            "I love the most probably representative papers and unpopular papers.",
            "For mixed membership models with applications to text is late enriched allocation.",
            "So I'll give you references to all of these papers at the end.",
            "So late initial allocation was originally presented as a Mario for doing topic modeling, but from what we have seen in the past, so so so there are K topics and this pie is that mixed membership on those topics or so for documentary the.",
            "Let's say there are 10 topics and this party is specific to a document.",
            "Which is how much of sports?",
            "How much of politics?",
            "How much of entertainment?",
            "How much of other things?",
            "This is an that's sample from additional distribution.",
            "Then, for each word you sample a specific topic and use the corresponding discrete distribution over the English dictionary to come up with the word.",
            "Now I'm going to go through this with an animation and with more examples on the line, but this is a concrete instantiation of the mixed membership abstract.",
            "Idea that I presented in a couple of slides back.",
            "So Pi is basically the distribution of our topics for each document.",
            "Z is the topic assignment for an individual word.",
            "And then beta is the distribution of rewards for each topic.",
            "So for politics will have a distribution over the English Dictionary.",
            "Sports will have our distribution over the English Dictionary.",
            "If there are K topics LBK distributions, so beat out the parameters of the basic model.",
            "Alpha is another parameter of the basic model, so the learning and inference will involve we have to estimate Alpha and beta and these are the things we want to infer or these things are these things we want to infer this document.",
            "How much of sports?",
            "How much of politics?",
            "So I think you know this, probably a coffee break going on will.",
            "Take a short 1520 minute break and then reconvene at 3:30 I guess."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Closer to reality, right?",
                    "label": 0
                },
                {
                    "sent": "Compared to the naive idea assumptions, it makes them closer to reality and you can capture domain knowledge in specifying.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dependencies, however the downside of graphical models is that learning or inference is much more difficult, so you cannot simply do maximum likelihood inference like I have a few samples IID drawn from a Gaussian.",
                    "label": 1
                },
                {
                    "sent": "I want to find the mean.",
                    "label": 0
                },
                {
                    "sent": "Well, I average them out, right?",
                    "label": 0
                },
                {
                    "sent": "It?",
                    "label": 0
                },
                {
                    "sent": "I'm done, it's not going to be that easy for graphical models because you're assuming things are they depend on each other and we will see how we do about doing estimation and inference.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there are two flavors of graphical models that are in common use.",
                    "label": 1
                },
                {
                    "sent": "Wanna directed models and the other is undirected model so so typically you represent this set of variables of interest as a graph where every node is available random variable which may be observed which may be hidden.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you plenty of examples and then the edge depends shows the statisical dependency right?",
                    "label": 0
                },
                {
                    "sent": "So if it's cloudy it'll rain.",
                    "label": 0
                },
                {
                    "sent": "So if you have available for cloud variable for real will be an edge connecting them right?",
                    "label": 0
                },
                {
                    "sent": "So that that kind so the directed models are what we also popularly called Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "They capture directed or causal dependencies between variables, so there are five variables over here X 14X5.",
                    "label": 0
                },
                {
                    "sent": "What this graph captures is that X one X2 causes X3 which then goes ahead and causes X4 and causes X5 right?",
                    "label": 0
                },
                {
                    "sent": "So so that's what the graph capture, capture.",
                    "label": 0
                },
                {
                    "sent": "So the direction of Arrow is sort of implied causality, so that's Bayesian networks and most of the tutorial will focus on directed graphical models or Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "The undirected graphical models just show statistical dependency without implying any direct, gradually, right?",
                    "label": 0
                },
                {
                    "sent": "So so examples are Mac of our conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are doing image analysis, if you're trying to figure out the value of this pixel.",
                    "label": 1
                },
                {
                    "sent": "So if you're taking a satellite image and you're saying you know this is a blue, that's a.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "This is a C doesn't see, so the pixel in the center will.",
                    "label": 0
                },
                {
                    "sent": "There's a high likelihood.",
                    "label": 0
                },
                {
                    "sent": "That'll also be a.",
                    "label": 0
                },
                {
                    "sent": "See because you know this is so it has dependencies on its neighbors, but it's not cautious.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's also not have extensively used in image analysis and conditional random fields over the past seven.",
                    "label": 0
                },
                {
                    "sent": "Eight years have been extensively used in various kinds of.",
                    "label": 0
                },
                {
                    "sent": "Structured prediction models.",
                    "label": 0
                },
                {
                    "sent": "We are not going to spend anytime on undirected models.",
                    "label": 0
                },
                {
                    "sent": "However, the inference methods that I'm going to be discussing today.",
                    "label": 0
                },
                {
                    "sent": "This should be a shared between the two classes of models.",
                    "label": 0
                },
                {
                    "sent": "Many of these inference methods are shared between the two classes of models.",
                    "label": 0
                },
                {
                    "sent": "However, the motivation from which underrated models come from are slightly different.",
                    "label": 1
                },
                {
                    "sent": "There are models which have both directed and undirected components.",
                    "label": 0
                },
                {
                    "sent": "We're not going to get into those, those are those are a bit advance.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so So what are vision networks?",
                    "label": 0
                },
                {
                    "sent": "So you have already seen this example, so there are five variables X1 for X5 and let's say the Bayesian network you're interested in X one X2, Cos, X3 and the next three causes X4 and X = X five.",
                    "label": 0
                },
                {
                    "sent": "So this kind of a structure let's you do is that it lets you write the joint distribution of these five variables in a nice way.",
                    "label": 0
                },
                {
                    "sent": "What is a nice way when the joint distribution of N variables can by chain rule in probability theory can be written as quality of exile given all the previous variables.",
                    "label": 0
                },
                {
                    "sent": "This is true for any.",
                    "label": 0
                },
                {
                    "sent": "Distribution, what you can do in case of vision networks is that since each XI depends only on its parents and his Commission independent of the other guys given its parents, you can simplify that factorization where each Excel not does not depend on everything before it.",
                    "label": 0
                },
                {
                    "sent": "But we the parents.",
                    "label": 0
                },
                {
                    "sent": "So I idea about example in a bit so so this is the advantage of graphical models that you do not have to worry about the full joint distribution because it factorizes in a in a nice form.",
                    "label": 1
                },
                {
                    "sent": "It's marrowfat.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Others.",
                    "label": 0
                },
                {
                    "sent": "OK, so first example, a start slide means if you have fallen asleep you want to wake up because it's something new and interesting.",
                    "label": 0
                },
                {
                    "sent": "You want to remember.",
                    "label": 0
                },
                {
                    "sent": "So, so this is a concrete instantiation after those five variables that I was talking about.",
                    "label": 0
                },
                {
                    "sent": "And you know, I have to acknowledge.",
                    "label": 0
                },
                {
                    "sent": "Navigate, this example is from the book I'm going to give the reference to the book.",
                    "label": 1
                },
                {
                    "sent": "At the end they have generously allowed me to use these.",
                    "label": 0
                },
                {
                    "sent": "Some of these examples in the talk, so all of these five are brilliant variables.",
                    "label": 0
                },
                {
                    "sent": "Burglary can happen or not happen, or quicken happen or not happen depending on whether burglary and output is happening.",
                    "label": 0
                },
                {
                    "sent": "The alarm go off or you know, and then you have to label.",
                    "label": 0
                },
                {
                    "sent": "Jonah may depending on whether that alarm is on, you know, don't make all unique also so.",
                    "label": 0
                },
                {
                    "sent": "This is a typical stochastic scenario, right so so?",
                    "label": 0
                },
                {
                    "sent": "I've already spent 001.",
                    "label": 0
                },
                {
                    "sent": "The probability of awkward is .002, and then depending on the conditions of burglary and earthquake, there's a probability that the alarm will go go F1 minus that would be the probability that Alan does not function, and Similarly so this specifies the complete joint distribution among these five variables.",
                    "label": 0
                },
                {
                    "sent": "If you had to specify this using a table, you know there are five variables, each one of them is a brilliant, so there will be 2 to the five or 32.",
                    "label": 0
                },
                {
                    "sent": "You know you have to specify table of 932.",
                    "label": 0
                },
                {
                    "sent": "In this case, you were having to only specify little conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "What is my condition product given its parents and that is all that is needed, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're if this becomes important once you're being with being millions of variables, you do not have to work with the two to the million table.",
                    "label": 0
                },
                {
                    "sent": "You can specify this little little local tables and that'll be good enough.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so So what?",
                    "label": 0
                },
                {
                    "sent": "This lets you do is that these tables will let you compute the probability of any event in this in this world.",
                    "label": 1
                },
                {
                    "sent": "So let's say if you want to compute the probability of Bea, JM.",
                    "label": 0
                },
                {
                    "sent": "These are random variables for those individual events.",
                    "label": 0
                },
                {
                    "sent": "The first line shows you what chain rule gives you a priority of be even be a given B and so on.",
                    "label": 0
                },
                {
                    "sent": "So that's straightforward.",
                    "label": 0
                },
                {
                    "sent": "From chain rule.",
                    "label": 0
                },
                {
                    "sent": "The second line shows once you've looked at the conditional independence structure, well, B is B, provided me given the Valley does not depend on B, so.",
                    "label": 0
                },
                {
                    "sent": "After does not decide to happen depending on whether there's a burglary going on, so there is no dependency there.",
                    "label": 0
                },
                {
                    "sent": "A probability of a given be evil alarm depends on both burglary.",
                    "label": 0
                },
                {
                    "sent": "An earthquake John calling you does not depend on BE an 8 only depends on a, so only that time remains and America does not depend on all four, but only once a back door remains.",
                    "label": 0
                },
                {
                    "sent": "So in this factorization now you can see that you can now read of these values of the table that are there and compute the probability of an event right so?",
                    "label": 0
                },
                {
                    "sent": "For a concrete example, let's say we want to find a probability of a burglary happening up quick not happening alarm happening.",
                    "label": 0
                },
                {
                    "sent": "John not calling Ann Marie calling right so that I can write like this.",
                    "label": 0
                },
                {
                    "sent": "Put plug in the values from those tables and I get the exact probability.",
                    "label": 0
                },
                {
                    "sent": "You can do this for.",
                    "label": 0
                },
                {
                    "sent": "You know 2 to the five events.",
                    "label": 0
                },
                {
                    "sent": "Now this this does not matter for this particular Bayesian network, but if you're dealing with vision network with thousands or millions of nodes.",
                    "label": 0
                },
                {
                    "sent": "This local factorization becomes super crucial to be able to build tractable models, right so?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let's look at another example.",
                    "label": 0
                },
                {
                    "sent": "Their network, so cloud is a Boolean variable.",
                    "label": 0
                },
                {
                    "sent": "Rain has a dependency on cloudy.",
                    "label": 0
                },
                {
                    "sent": "You invested money in this super fancy sprinkler which sends us where it's cloudy or not and you know goes on and off depending on weather it's cloudy or not and both sprinkler or rain can cause this final event that you trust becomes wet.",
                    "label": 0
                },
                {
                    "sent": "So this is, again, you're specifying the conditional probability table.",
                    "label": 0
                },
                {
                    "sent": "You know cloudy chance of cloudy is half depending on cloudy.",
                    "label": 0
                },
                {
                    "sent": "You know the chances of rain being true is higher, and the chances of the sprinkler going off is actually lower and both of them together leads to wet grass.",
                    "label": 0
                },
                {
                    "sent": "There's a difference between this vision network.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous one we had seen over here.",
                    "label": 0
                },
                {
                    "sent": "So the difference is something that plays a very important role in inference inference.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I love them.",
                    "label": 0
                },
                {
                    "sent": "So in this network base available like wet grass which can happen in two different ways.",
                    "label": 0
                },
                {
                    "sent": "Realization of that variable can happen in two different ways, either through a sprinkler or trade.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas in this case, well, you know there there are no loops, no junk cars can only happen if the alarm goes off and the number that can only happen if burglary happens or earthquake happens.",
                    "label": 0
                },
                {
                    "sent": "So if you look at that just available in structure of this graph, it's essentially a tree.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But as this is like a loop, then there's a loop in there.",
                    "label": 0
                },
                {
                    "sent": "If you if you follow up with arrows and things like that.",
                    "label": 0
                },
                {
                    "sent": "Then there are loops.",
                    "label": 0
                },
                {
                    "sent": "There are multiple more than one ways of something happening on.",
                    "label": 0
                },
                {
                    "sent": "In these cases, inference becomes more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree structured while inference as we will see people have figured out how to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So this is considered assault problem.",
                    "label": 0
                },
                {
                    "sent": "But I'll show you that's all.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is fairly elegant action.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you know you can start building complicated models for you know whatever you if everything is, you will wake up in the morning, start your car won't start so that red thing is your primary observation.",
                    "label": 1
                },
                {
                    "sent": "Green things are things, probably you either know the value of our you can check and we can use a dipstick.",
                    "label": 0
                },
                {
                    "sent": "You can see the light is turning on and then you know this is sort of your vision network.",
                    "label": 0
                },
                {
                    "sent": "Don't do anything that does not work.",
                    "label": 0
                },
                {
                    "sent": "You're probably better off calling card.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About something so OK, So what is the main problem that we're trying to solve?",
                    "label": 0
                },
                {
                    "sent": "So I have a Bayesian network for my domain.",
                    "label": 0
                },
                {
                    "sent": "The main problem that one is interested in solving in graphical models is what we call inference, right?",
                    "label": 0
                },
                {
                    "sent": "So some variables in a Bayesian network have been observed.",
                    "label": 0
                },
                {
                    "sent": "John hasn't you're sitting in the office, you have this vision.",
                    "label": 0
                },
                {
                    "sent": "Network Jam has not called.",
                    "label": 1
                },
                {
                    "sent": "You may have called you.",
                    "label": 0
                },
                {
                    "sent": "You are trying to figure out whether there's a burglary going on at your place.",
                    "label": 1
                },
                {
                    "sent": "Right, So what you want to compute?",
                    "label": 0
                },
                {
                    "sent": "You know you know this vision network you have observed trouve two variables.",
                    "label": 0
                },
                {
                    "sent": "John has called you.",
                    "label": 0
                },
                {
                    "sent": "John has not called.",
                    "label": 0
                },
                {
                    "sent": "You may have called you.",
                    "label": 0
                },
                {
                    "sent": "You are trying to compute the probability that a burglary is going on because you're interested in it, right?",
                    "label": 0
                },
                {
                    "sent": "So that's that's important to you.",
                    "label": 1
                },
                {
                    "sent": "This is the inference problem.",
                    "label": 0
                },
                {
                    "sent": "So you construct this large vision network for your domain.",
                    "label": 0
                },
                {
                    "sent": "You observe some variables you want to find probabilities of other variables, right?",
                    "label": 1
                },
                {
                    "sent": "This is the inference problem, and this is where most of the effort goes in.",
                    "label": 0
                },
                {
                    "sent": "Of course in this.",
                    "label": 0
                },
                {
                    "sent": "Example, you can probably, you know, use basic probability to figure it out, but as the size of your graph grows, you know you need proper algorithms and guarantees on how to figure out that product.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I give you a very broad overview, us in France and then we will get into some of these in the course of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So if if you're considering a graphical model over but the graph does not have loops like the burglary network, so those are the three structured graphs we now know that efficient exact inference algorithms are possible by efficient.",
                    "label": 1
                },
                {
                    "sent": "I'm in linear time, right?",
                    "label": 0
                },
                {
                    "sent": "Linear in the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "This was not known for a very long time and what people were doing.",
                    "label": 0
                },
                {
                    "sent": "People were coming up with.",
                    "label": 0
                },
                {
                    "sent": "Individual instances are very clever looking algorithms.",
                    "label": 0
                },
                {
                    "sent": "It started from the 1960s common filtering, 1970s Viterbi decoding forward, backward algorithm for hidden Markov models, 1980s belief propagation till towards the end 90s people realize we have the same I got.",
                    "label": 0
                },
                {
                    "sent": "Right, so, and this realization happened towards the end of the 90s.",
                    "label": 1
                },
                {
                    "sent": "So if you have seen hidden Markov models, there's some complicated thing that happens.",
                    "label": 0
                },
                {
                    "sent": "And if you have seen some other models like belief propagation goes out, the same algorithm, and these people realize about 10 years back, and now there's a new clean understanding why we can do that?",
                    "label": 0
                },
                {
                    "sent": "Why, hmm, Caesar is a special case.",
                    "label": 0
                },
                {
                    "sent": "I will spend some time on this algorithm because this is important that cover several previous algorithms that have been discovered and discover since the 60s till the late 90s.",
                    "label": 0
                },
                {
                    "sent": "However, if the graph has loops, there are some ideas out there, but in general this problem is very difficult, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a broad class of algorithms called junction tree algorithms which somehow convert converts the graph with loops to a bigger graph without loops.",
                    "label": 1
                },
                {
                    "sent": "That bigger graph can be exponentially big, so even a linear time algorithm in that bigger graph is an exponential time algorithm in the original problem specification, I'm not going to discuss junction tree algorithms, But this is a very important class of algorithms and there's.",
                    "label": 0
                },
                {
                    "sent": "Extensive literature on this.",
                    "label": 0
                },
                {
                    "sent": "There is another class of algorithms which says that you know we are going to do whatever we were doing for trees, which is this message passing some product algorithm which I'll discuss but will will go out as if there are no loops, will keep doing the message passing anyway.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly, that thing works.",
                    "label": 0
                },
                {
                    "sent": "But we still don't know exactly why, and I give you examples of where those are used, including your cell phones.",
                    "label": 0
                },
                {
                    "sent": "So so it's scary.",
                    "label": 0
                },
                {
                    "sent": "I mean, we need to understand why this thing works, so that's message passing.",
                    "label": 0
                },
                {
                    "sent": "I'll briefly touch upon the general case, but mostly most of the work that happens with graphic loops is what we can bring.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are you put under approximate inference and then there are two broad classes of approximate inference algorithms, one are called variational inference, so variational inference makes a deterministic approximation of the problem.",
                    "label": 0
                },
                {
                    "sent": "So you convert that marginalization problem.",
                    "label": 0
                },
                {
                    "sent": "I want to find the probability of already given John didn't call Mary call into an optimization problem, you actually convert the problem into an optimization problem, and you show that you know somehow solving this optimization problem gives me a lower bound to the original problem, and things like that.",
                    "label": 0
                },
                {
                    "sent": "We will see very nice examples of this whole method in a bunch of text analysis an I'll give you real in applications where we have used it.",
                    "label": 0
                },
                {
                    "sent": "Other people have used it.",
                    "label": 0
                },
                {
                    "sent": "There are many variants of variational inference, so it's kind of stands for a broad class of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Many of these ideas came from statistical physics, like mean field, better approximation, Kikuchi approximation, EP expectation propagation which is a more recent development by timing and team.",
                    "label": 0
                },
                {
                    "sent": "Battle class is the more traditional approach, which is, you know, stochastic inference which has been around in statistics for a very, very long time.",
                    "label": 0
                },
                {
                    "sent": "No past 5060 years.",
                    "label": 0
                },
                {
                    "sent": "Markov chain Monte Carlo methods, and then a special case of that Gibbs sampling, and I will see how to adopt these methods for some of the large scale data mining problems and what kinds of results.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are giving alright, so that's the overview right?",
                    "label": 0
                },
                {
                    "sent": "So that so that was overview.",
                    "label": 0
                },
                {
                    "sent": "Now we're sort of entering the first part of our talk.",
                    "label": 0
                },
                {
                    "sent": "But we will be worrying about tree structured graphs.",
                    "label": 0
                },
                {
                    "sent": "And consider inference problem really understand the sum product algorithm as it is called NOW, which has been known by many different names for the last 3040 years.",
                    "label": 0
                },
                {
                    "sent": "We will briefly lasted this with an example.",
                    "label": 0
                },
                {
                    "sent": "There are also touch upon hidden Markov models, but I won't get into the details and there is an excellent tutorial by Lawrence Turbine on Hmm's which essentially cover.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these things.",
                    "label": 0
                },
                {
                    "sent": "OK, so so getting back to the inference problem, I have a vision network which is tree structured, meaning that if I look at the background structure of the graph, there are no loops.",
                    "label": 1
                },
                {
                    "sent": "And I have observed two variables.",
                    "label": 0
                },
                {
                    "sent": "Littlejohn calls Mary calls.",
                    "label": 0
                },
                {
                    "sent": "I want to find the probability of burglary right?",
                    "label": 0
                },
                {
                    "sent": "This is something I'm interested in, so let's try to do this computation right.",
                    "label": 0
                },
                {
                    "sent": "How will we do this well?",
                    "label": 0
                },
                {
                    "sent": "I said I would visual or at least you know some way and I think what that says is the probability of B given something else.",
                    "label": 0
                },
                {
                    "sent": "Is the distribution of be with those things divided by the probability of those things.",
                    "label": 0
                },
                {
                    "sent": "This is basic probability.",
                    "label": 0
                },
                {
                    "sent": "We know that something I am done if I can compute the top thing on the bottom thing alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's compute that.",
                    "label": 0
                },
                {
                    "sent": "So in order to compute the top thing I say I can take the joint distribution and I can sum out the variables that I don't need, which is.",
                    "label": 0
                },
                {
                    "sent": "This is the process of marginalization.",
                    "label": 0
                },
                {
                    "sent": "I sign out awkward ice amount around because those are things I don't need and then that gives me the marginal similarly probability of J, N. You know you get the marginal.",
                    "label": 0
                },
                {
                    "sent": "You take the issue, you're done, so you must be seeing what are you talking about.",
                    "label": 0
                },
                {
                    "sent": "This is easy.",
                    "label": 0
                },
                {
                    "sent": "So good, so consider this scenario where there are not three or five variables.",
                    "label": 0
                },
                {
                    "sent": "But our 1 million variables, each one of them that is brilliant.",
                    "label": 0
                },
                {
                    "sent": "And what you are doing is you want to sum over all of them other than three or two.",
                    "label": 0
                },
                {
                    "sent": "So it's, um, is over, you know, eating one or 08, picking one or zero, you're selling over a million.",
                    "label": 0
                },
                {
                    "sent": "We invariables with both possibilities.",
                    "label": 0
                },
                {
                    "sent": "That is due to the million so so, so the number of times in that submission grows exponentially in the number of variables.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So if you actually do a complexity analysis of this algorithm, since each term can be factorized like this and can be computed in order N, and the submission will effectively be over 2 to the end, the complexity of these naive inferences order N * 2 to the end.",
                    "label": 0
                },
                {
                    "sent": "Right, if you can do this nice team fine, you don't need any other thing I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "If you are working with a serious application which has at least you know 10s of thousands of variables that naivety is not going to work right.",
                    "label": 0
                },
                {
                    "sent": "OK, so this people really realize long time back.",
                    "label": 0
                },
                {
                    "sent": "In the late 60s and early 70s, the development of hidden Markov model was sort of officially the first time people realize, OK, you know if I'm trying to compute probabilities with hmm, I cannot simply do that Bayes rule thing because it's going to take forever.",
                    "label": 0
                },
                {
                    "sent": "So this problem is faced by an HMM and I'll tell you in a minute one hmm is so OK.",
                    "label": 0
                },
                {
                    "sent": "So the live inference idea is not going to work, so we.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have to be smarter so there has been several developments, but what I'm presenting is the currently agreed upon.",
                    "label": 0
                },
                {
                    "sent": "Standard formalism using factor graphs an in the vector.",
                    "label": 1
                },
                {
                    "sent": "Rats are nice because you can map both directed models as well as undirected models, factor roughs.",
                    "label": 0
                },
                {
                    "sent": "So this shows how to convert a given vision network to a factor graph.",
                    "label": 0
                },
                {
                    "sent": "What is a factor graph well?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In your factorization of the probabilities over here, you can see that the joint distribution factorizes in a bunch of local factors, right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you make that observation and that will be true for a major network.",
                    "label": 0
                },
                {
                    "sent": "Then you add a factor to every variable and if a factor is a bunch of variables, participates in it.",
                    "label": 0
                },
                {
                    "sent": "It connects to all those variables.",
                    "label": 0
                },
                {
                    "sent": "So let's say this factor.",
                    "label": 0
                },
                {
                    "sent": "Is more of the factor of X1 only, whereas this factor involves X one X2X3 because you know it's this factor.",
                    "label": 0
                },
                {
                    "sent": "This factor involves X3 and X Fort Sam function.",
                    "label": 0
                },
                {
                    "sent": "In our case these are conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "This involves and that involves this thing, right?",
                    "label": 0
                },
                {
                    "sent": "So so this is simply a redrawing of the vision network.",
                    "label": 0
                },
                {
                    "sent": "I'm not doing anything fancy right?",
                    "label": 0
                },
                {
                    "sent": "I'm simply redrawing the Bayesian network where I'm replacing this conditional probabilities with the factors with edges to all the variables that are involved.",
                    "label": 0
                },
                {
                    "sent": "In that condition, probability OK. Now, so that's a factor graph.",
                    "label": 0
                },
                {
                    "sent": "So now I can take this factor graph and write it a little bit more nicely as a sort of a bipartite graph where I put all the factors on one side and all the variables on another side.",
                    "label": 0
                },
                {
                    "sent": "And I say that the joint distribution over these five variables factorizes as before, so nothing has changed.",
                    "label": 0
                },
                {
                    "sent": "I'm just using a different notation, and I'm redrawing that factor graph like this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exact same graph, so you can verify that you know you take all all those factors on one side on the variables on the other side you get this.",
                    "label": 0
                },
                {
                    "sent": "Alright, so So what is the inference problem in this factor graph world well?",
                    "label": 0
                },
                {
                    "sent": "If you have a, if you have a function of multiple variables which factorize are, which factorizes product of local functions.",
                    "label": 1
                },
                {
                    "sent": "So that's far from our Bayes net.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is we are trying to some out everything other than XI or everything other than in Exponent X5 or something like that.",
                    "label": 0
                },
                {
                    "sent": "So what we're trying to do is some out a whole bunch of them, only keeping one or two unchanged, right?",
                    "label": 0
                },
                {
                    "sent": "That's the whole marginalization process.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that not some notation.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that not of examines that you sum over everything other than XI, so that's just a convenient notation.",
                    "label": 0
                },
                {
                    "sent": "Now, this class of problems surprisingly show up in many, many different places, so we are focusing today on Bayesian networks in graphical models, which shows up in coding theory in communication, error correcting codes, many different places, and they are used in many different places.",
                    "label": 0
                },
                {
                    "sent": "I'll show you examples of that, so the problem broadly is called.",
                    "label": 0
                },
                {
                    "sent": "You want to merge.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Please pull up the functions.",
                    "label": 0
                },
                {
                    "sent": "How do you do it?",
                    "label": 0
                },
                {
                    "sent": "Well, we never that naive idea won't work.",
                    "label": 0
                },
                {
                    "sent": "I cannot simply do exactly what this equation is saying, which is you some over you know everything other than Excel because that will take exponential time.",
                    "label": 0
                },
                {
                    "sent": "Between observation, we sort of make over here is that my submission is over this function.",
                    "label": 0
                },
                {
                    "sent": "My summation of what?",
                    "label": 0
                },
                {
                    "sent": "This function and this function function factorizes.",
                    "label": 0
                },
                {
                    "sent": "So, it's, uh, some of our products of local functions.",
                    "label": 0
                },
                {
                    "sent": "Now I'm.",
                    "label": 0
                },
                {
                    "sent": "We go back to our beauty algebra days.",
                    "label": 0
                },
                {
                    "sent": "And be equal that when we have a sum over product of things.",
                    "label": 1
                },
                {
                    "sent": "I can take the common piece out and then some.",
                    "label": 0
                },
                {
                    "sent": "Only the things that cannot be taken, so this known as distributive law, baby plus AC can be simplified as our written exactly equivalently as 8 * B + C. OK, so what's the big deal?",
                    "label": 0
                },
                {
                    "sent": "The Big deal is if you look at this equation from a competition perspective, the left hand side involves two multiplications and one edition.",
                    "label": 0
                },
                {
                    "sent": "The right hand side involves one multiplication and one edition.",
                    "label": 0
                },
                {
                    "sent": "Right, you can afford this problem.",
                    "label": 0
                },
                {
                    "sent": "You save in computation and multiplications are more more costly.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we are going to sort of look at this and try try to construct efficient algorithms for doing the inference.",
                    "label": 0
                },
                {
                    "sent": "In particular, we're going to try to push the submissions as far inside as possible.",
                    "label": 0
                },
                {
                    "sent": "So far in OG, one of expand where we're trying to sign over everything other than X.",
                    "label": 0
                },
                {
                    "sent": "When we push the submissions inside as far as possible so we do local Sam's and then we multiply and keep going like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, you can take any other factor like G3X3, you can push the local sums.",
                    "label": 0
                },
                {
                    "sent": "This starts looking.",
                    "label": 0
                },
                {
                    "sent": "You know, this starts to look more interesting because now we're saving on competition.",
                    "label": 0
                },
                {
                    "sent": "We're not doing repeat computations because because we have pushed things inside.",
                    "label": 1
                },
                {
                    "sent": "However, this does not solve your problem, because if you really have that million revision at work, you do not want to sit down with pen and paper to try to push submissions inside.",
                    "label": 0
                },
                {
                    "sent": "So there has to be an algorithm way of doing this, or this, or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So so, so that's really what the sum product algorithm does.",
                    "label": 0
                },
                {
                    "sent": "It sounds like a whole bunch of variables while keeping a bunch of them unchanged.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the main idea is as follows.",
                    "label": 1
                },
                {
                    "sent": "Let's say you pick a target.",
                    "label": 0
                },
                {
                    "sent": "Now let's say G1X1 I. I want to find the margin analytics one.",
                    "label": 0
                },
                {
                    "sent": "You you draw the graph, it's the same factor graph, I'm just you know, drying it in different ways with X one.",
                    "label": 0
                },
                {
                    "sent": "But I don't know that that app.",
                    "label": 0
                },
                {
                    "sent": "And the rest of the thing has a tree starting from that.",
                    "label": 0
                },
                {
                    "sent": "If I'm interested next three, I will sort of take.",
                    "label": 0
                },
                {
                    "sent": "X3 on top and I'll write it like that right?",
                    "label": 0
                },
                {
                    "sent": "So so first decide which one you want to compute.",
                    "label": 0
                },
                {
                    "sent": "Let's say we want to compute X one, then I'll drop this tree like this.",
                    "label": 1
                },
                {
                    "sent": "Then I start passing messages from the leaves all the way up till X one and I don't want to specify what those operations are.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let's get into the details.",
                    "label": 0
                },
                {
                    "sent": "So the messages that I'm going to pass you know it's going to be a very simple algorithm, But what it's going?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Door is exactly this, like it's exactly this, but it's an algorithm key of doing this.",
                    "label": 0
                },
                {
                    "sent": "Up",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in in a tree in this kind of a tree, you can encounter either variable nodes or factor nodes.",
                    "label": 0
                },
                {
                    "sent": "So we will have two specifications.",
                    "label": 0
                },
                {
                    "sent": "What happens to the messages when you encounter a variable node and what happens to the messages when you encounter a fact?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you count available now, you simply multiply everything that's coming in and pass it up.",
                    "label": 0
                },
                {
                    "sent": "That's it, right?",
                    "label": 0
                },
                {
                    "sent": "That's all you have to do.",
                    "label": 0
                },
                {
                    "sent": "When you encounter factor mode, you simply multiply everything that's coming in from the children with the factor.",
                    "label": 0
                },
                {
                    "sent": "And pass it up by doing a son of a map of the parent.",
                    "label": 0
                },
                {
                    "sent": "So you send out everything other than the parent right now.",
                    "label": 0
                },
                {
                    "sent": "So these are local functions.",
                    "label": 0
                },
                {
                    "sent": "At this point, each factor will have only a few variables other than the parent, so this not some operations are going to be fairly straight for.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going back so you can see that this not some operation is only over X3.",
                    "label": 0
                },
                {
                    "sent": "This is only over X3 and so on.",
                    "label": 0
                },
                {
                    "sent": "So those are those are much smaller untractable notes on operations and will do that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Refactor node.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so So what happens over here?",
                    "label": 0
                },
                {
                    "sent": "Then you have your factor graph because that came from the structure of the business.",
                    "label": 0
                },
                {
                    "sent": "Then you replace the variable nodes by the product.",
                    "label": 0
                },
                {
                    "sent": "The factor nodes by product over everything that's coming in multiply with the factor passed on with the sum.",
                    "label": 0
                },
                {
                    "sent": "If you actually.",
                    "label": 0
                },
                {
                    "sent": "So you start the computation from here and we move up.",
                    "label": 0
                },
                {
                    "sent": "If you see what you have computed over here, it's exactly this.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a simple algebraic exercise.",
                    "label": 0
                },
                {
                    "sent": "To verify that you are going to get exactly that expression which which has marginalized out all all the variables other than X1.",
                    "label": 0
                },
                {
                    "sent": "Now you can readily see this is a linear time algorithm because every every word or every factor you know you do one operation per node or power factor.",
                    "label": 0
                },
                {
                    "sent": "There are any man knows around factor, so so is it saying plus EM algorithm as opposed to end times two to buy an algorithm which we were dealing with before.",
                    "label": 0
                },
                {
                    "sent": "Let's look at another example.",
                    "label": 0
                },
                {
                    "sent": "So the three X3.",
                    "label": 0
                },
                {
                    "sent": "You simply convert the factor graph into this computation tree.",
                    "label": 0
                },
                {
                    "sent": "You know, past messages like this encoding.",
                    "label": 0
                },
                {
                    "sent": "This is fairly straightforward because it's modular structured operation is the same, and on the top you will get exactly this.",
                    "label": 0
                },
                {
                    "sent": "So the beauty of this algorithm is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "But the structure of the business actually includes a very efficient algorithm.",
                    "label": 1
                },
                {
                    "sent": "At which was long overdue from people are trying to sort of come up with efficient ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "One of the biggest success stories of this algorithm is hidden Markov models, right?",
                    "label": 0
                },
                {
                    "sent": "So some protocol Gordon is extensively using hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "These exact same algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So it is known by different names.",
                    "label": 0
                },
                {
                    "sent": "It is known as a forward backward algorithm will be decoding algorithm and things like that.",
                    "label": 0
                },
                {
                    "sent": "But it's exactly this algorithm, so this is very reassuring.",
                    "label": 0
                },
                {
                    "sent": "You have a linear time algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For these things, so let's talk a little bit more about hidden Markov models.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to get into the details, I'm hoping some of you are familiar with Hmm's, but otherwise you know this is one of the most widely used probabilistic graphical models since the 70s, which have been used in many, many different applications originally came up in speech recognition but has been applied to many other way applications ever since.",
                    "label": 0
                },
                {
                    "sent": "So so hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "This sort of a temporal dimension to it.",
                    "label": 0
                },
                {
                    "sent": "You have a little sequence of variables this 81.",
                    "label": 0
                },
                {
                    "sent": "Z T -- 1 Ziti, ziti plus one.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And you are observing Maxi minus one XD XD plus one.",
                    "label": 0
                },
                {
                    "sent": "So these are bugs are variables.",
                    "label": 0
                },
                {
                    "sent": "You are interested in, you know, various steps, so this is the corresponding factor graph and you are interested in France.",
                    "label": 0
                },
                {
                    "sent": "Problems like what is the probability of what I'm observing, right?",
                    "label": 0
                },
                {
                    "sent": "Or what is the probability of a certain latent variable given what I've observed, I'll give you a concrete example, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an example which helps you ground without getting into speech recognition.",
                    "label": 0
                },
                {
                    "sent": "Let's see where this devoted researcher who sits in your office all day never goes out.",
                    "label": 0
                },
                {
                    "sent": "You basically live there so and so.",
                    "label": 0
                },
                {
                    "sent": "What XD is you want you want to say?",
                    "label": 0
                },
                {
                    "sent": "Ziti is available with whether it's raining outside and XD is the variable whether your office coworker is getting an umbrella or not.",
                    "label": 0
                },
                {
                    "sent": "Right, so you get to see whether this person is carrying an umbrella here.",
                    "label": 0
                },
                {
                    "sent": "He's a nice guy.",
                    "label": 0
                },
                {
                    "sent": "He checked the weather in the morning and carries a number letters, chance of rain and just looking at whether he's carrying a number.",
                    "label": 0
                },
                {
                    "sent": "You can see that every day you are trying to infer that it's raining outside or not.",
                    "label": 0
                },
                {
                    "sent": "So example there are more serious examples for Hmm's.",
                    "label": 0
                },
                {
                    "sent": "So, So what this computes is the probability of this sequence, right?",
                    "label": 0
                },
                {
                    "sent": "If it if it's very less likely, then you know these guys tricking me.",
                    "label": 0
                },
                {
                    "sent": "He brings number every day that God be.",
                    "label": 0
                },
                {
                    "sent": "I mean it does not run everyday, so you compute the probability said this is, you know 10 to the minus 50.",
                    "label": 0
                },
                {
                    "sent": "So this guy is.",
                    "label": 0
                },
                {
                    "sent": "I don't believe this guy.",
                    "label": 0
                },
                {
                    "sent": "The second problem is you have observed this and then you're trying to figure out where this guy said that you know it had rained last Monday.",
                    "label": 0
                },
                {
                    "sent": "What is the probability given that he got an umbrella on Sunday?",
                    "label": 0
                },
                {
                    "sent": "Didn't bring on Monday brought on one on Tuesday and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is known as smoothing in other applications like Kalman filtering.",
                    "label": 0
                },
                {
                    "sent": "So you made a whole bunch of observations.",
                    "label": 0
                },
                {
                    "sent": "You're trying to figure out the latent variable.",
                    "label": 0
                },
                {
                    "sent": "There is another class of inference problems.",
                    "label": 0
                },
                {
                    "sent": "One is interested in.",
                    "label": 0
                },
                {
                    "sent": "Hmm's is given that you have observed all of these XTS umbrella, not umbrella.",
                    "label": 0
                },
                {
                    "sent": "You are trying to figure out exactly what has happened outside in the last 30 days from day one to day 30 for the train.",
                    "label": 0
                },
                {
                    "sent": "What is the most likely sequence of real events now?",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing down below is the factor.",
                    "label": 0
                },
                {
                    "sent": "Have corresponding to this and if you know the same reptile garden which I've just shown you, you can actually solve at least this problem pretty efficiently because you know, you take ZT, make it the route the rest of hmm becomes that tree, start passing messages, you'll get it.",
                    "label": 0
                },
                {
                    "sent": "That is exactly what was discovered in the 1970s known as the son of a forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "This algorithm has also been considered and used in conditional fields, which is undirected graphical model and change structure.",
                    "label": 0
                },
                {
                    "sent": "Condition random fields have exactly this factor graph, right?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is the same product algorithm, so it has two simple rules.",
                    "label": 0
                },
                {
                    "sent": "You construct history at every variable node.",
                    "label": 0
                },
                {
                    "sent": "You multiply everything that's coming from your descendants and the same product rule which is at factor node.",
                    "label": 0
                },
                {
                    "sent": "You take the product of F with his descendant.",
                    "label": 1
                },
                {
                    "sent": "No, let's say you're saying OK, this is great.",
                    "label": 0
                },
                {
                    "sent": "I can compute the probability of X1 and X3.",
                    "label": 0
                },
                {
                    "sent": "Plan what I want to compute the probabilities all over all of them simultaneously.",
                    "label": 0
                },
                {
                    "sent": "One way to do that would be to do them one at a time, which will be N times you know the total number of variables.",
                    "label": 0
                },
                {
                    "sent": "However, you can be even more efficient when you are trying to find find out all the marginals simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And that is really the full blown glory of the sampler Calgary.",
                    "label": 0
                },
                {
                    "sent": "I see what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "So what you are trying to compute.",
                    "label": 0
                },
                {
                    "sent": "Is the this probability for Owl City?",
                    "label": 0
                },
                {
                    "sent": "Write simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Want to find the probability for algete simultaneously?",
                    "label": 0
                },
                {
                    "sent": "So how do you do that?",
                    "label": 1
                },
                {
                    "sent": "Well, instead of going GI GI XI one at a time.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to find all of them.",
                    "label": 0
                },
                {
                    "sent": "Well, we are going to do simultaneous message passing, right?",
                    "label": 0
                },
                {
                    "sent": "So we are going to take that that algorithm that we were using by constructing the tree.",
                    "label": 0
                },
                {
                    "sent": "The one-way single route anymore messages will go out the place and but they'll be allergic to it.",
                    "label": 0
                },
                {
                    "sent": "So what we will do is as follows.",
                    "label": 0
                },
                {
                    "sent": "That inner variable mode once I have got messages from everything other than one side, we will multiply those things and pass it on.",
                    "label": 0
                },
                {
                    "sent": "So that's that's available to local function an on a local function.",
                    "label": 0
                },
                {
                    "sent": "If I have got all the variables, all the messages coming in to that factor, I'll multiply it with the factor and pass it on.",
                    "label": 0
                },
                {
                    "sent": "Right, this is what I want to do.",
                    "label": 0
                },
                {
                    "sent": "This is what I was doing before.",
                    "label": 0
                },
                {
                    "sent": "I just need to make sure that I have got all the messages I need in order to pass it on.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause I'm going to walk through an example.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zoomed in version of that walk through an example of our factor graph that the one we have been working with starting with the Boundary Network, but I would have to simultaneously find the marginals at Earl these five variables, but I don't want to do this one at a time five times, because that just starts becoming slower.",
                    "label": 0
                },
                {
                    "sent": "So if I do that enough for each variable, it's a linear time algorithm.",
                    "label": 0
                },
                {
                    "sent": "Have enviable, so this becomes a quadratic algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's bad.",
                    "label": 0
                },
                {
                    "sent": "I'm going to maintain the linear complexity and find all of them right, so that's that's possible, and this be really becomes so if you have your dynamic programming part of the brain waking up.",
                    "label": 0
                },
                {
                    "sent": "This is really a very nice form of dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So, So what do I do over here?",
                    "label": 0
                },
                {
                    "sent": "Is passing messages from the extreme leaves of this graph so so from FA2X1 from FB2X2, from X-412-D, and from X52 Fe?",
                    "label": 0
                },
                {
                    "sent": "So that's the first set of messages.",
                    "label": 0
                },
                {
                    "sent": "Now X one has them as a factor mode X1 is a factor node which has the message it needs, so it can multiply everything that's coming in.",
                    "label": 0
                },
                {
                    "sent": "Well, there's only one message coming in on.",
                    "label": 0
                },
                {
                    "sent": "It starts passing so that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stage two, so expand passes it forward, extra process it forward.",
                    "label": 0
                },
                {
                    "sent": "The passes it forward.",
                    "label": 0
                },
                {
                    "sent": "If he passes it forward an and it proceeds like this.",
                    "label": 0
                },
                {
                    "sent": "So now, at this point FC The factor has two messages coming in from X1 and X2, so it cannot multiply those things with FCA.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pass it along to X3 so that becomes a third piece.",
                    "label": 0
                },
                {
                    "sent": "That's the third step of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "You pass that message on, and similarly for X3 has two messages which came in in Step 2 that it passes a message on, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you see.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You keep going like this.",
                    "label": 0
                },
                {
                    "sent": "Still, you have basically passed messages.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On every edge.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both directions.",
                    "label": 0
                },
                {
                    "sent": "That's linear time, right so that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two times the number of edges, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last step in order to compute the marginals at every point, you simply multiply all the incoming messages at any point in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It got an incoming message you just multiplied along, so at every incoming edge it'll receive only one message and you simply multiply it along.",
                    "label": 0
                },
                {
                    "sent": "You can show.",
                    "label": 0
                },
                {
                    "sent": "But these will be the exact correct marginals, and this is this is sort of a linear time message passing algorithm which correctly computes a marginal for all variables involved.",
                    "label": 0
                },
                {
                    "sent": "So this practical is sales.",
                    "label": 0
                },
                {
                    "sent": "The inference problem for tree structured graphs, right?",
                    "label": 0
                },
                {
                    "sent": "We will move on to more fancy models for, you know, text analysis and collaborative filtering where this trick unfortunately is not going to work because it's those are more complex.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mods, so just to revisit HMMS quickly.",
                    "label": 0
                },
                {
                    "sent": "So if I if I want to sort of compute the probability of the observations, I don't know anything about the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "I'm just interested in probability after an umbrella carrying behavior for the last 30 days.",
                    "label": 0
                },
                {
                    "sent": "What this really involves is that you have the joint distribution over all the axes and I'll disease.",
                    "label": 0
                },
                {
                    "sent": "All these ex is a mild disease.",
                    "label": 0
                },
                {
                    "sent": "You use em out all disease.",
                    "label": 0
                },
                {
                    "sent": "Right, you marginalized out all disease and what you're left with is exactly this probability.",
                    "label": 0
                },
                {
                    "sent": "The way this is done.",
                    "label": 0
                },
                {
                    "sent": "If you look up Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Our Readiness survey, or any other things this algorithm is known as the sum product algorithm, and this is also the further backward I got because you know it will have to maintain some parameters.",
                    "label": 0
                },
                {
                    "sent": "It'll have to move forward.",
                    "label": 0
                },
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "This is exactly a special case after algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about the sample held all right so, but of course you know this is an example.",
                    "label": 0
                },
                {
                    "sent": "This is a brilliant discovery.",
                    "label": 0
                },
                {
                    "sent": "It happened in the 70s.",
                    "label": 0
                },
                {
                    "sent": "They realize that you know for this hmm, I can come up with a very efficient way of computing the probabilities of observations or computing this smooth probability.",
                    "label": 0
                },
                {
                    "sent": "You know, given all observations, what is the probability what happened on day T, right?",
                    "label": 0
                },
                {
                    "sent": "You can compute that and that's you called smoothing, but that's more widely used in common filtering so.",
                    "label": 0
                },
                {
                    "sent": "Carbon filtering is essentially also a variant of the same idea and the basic unknown filtering, and this is known as smoothing in Kalman filter.",
                    "label": 1
                },
                {
                    "sent": "So these ideas have been discovered several times in the past and they they have grown into their own sort of sub sub areas with many many extensions and so on.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is that the basic algorithm which was discovered in the in the 60s or 70s or belief propagation for business in the 80s there all instances of this one algorithm which we call the sampler.",
                    "label": 0
                },
                {
                    "sent": "So, and there's there's a beautiful paper which I'm going to talk about at the end in the reference is the name of the paper is factor graphs?",
                    "label": 0
                },
                {
                    "sent": "Amazon work?",
                    "label": 0
                },
                {
                    "sent": "I'll go down, which basically explains this whole history and algorithm.",
                    "label": 0
                },
                {
                    "sent": "That way I did it.",
                    "label": 0
                },
                {
                    "sent": "So I'll have a few references on this and you can look it up in the slides later on.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this helps.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observe like you know, 3040 years of literature in one algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you know these albums look complicated, but you know once you understand this genetic structure, you know all these things fall in.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm.",
                    "label": 0
                },
                {
                    "sent": "Some people got a little sort of more ambitious.",
                    "label": 0
                },
                {
                    "sent": "They say they'll be structured that some of our products of local functions question.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greater risk if you have two parents that are not just binary.",
                    "label": 0
                },
                {
                    "sent": "Ship",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure the.",
                    "label": 0
                },
                {
                    "sent": "Messages will be vectors.",
                    "label": 0
                },
                {
                    "sent": "So so if if you look at like this factor and X1 picks 10 values are next to takes 10 values, then these messages will be one value for each instantiation of X1.",
                    "label": 0
                },
                {
                    "sent": "So in the past that cover 10 and 10.",
                    "label": 0
                },
                {
                    "sent": "But if you want to pass a vector of 100, then capture some nonlinear interaction.",
                    "label": 0
                },
                {
                    "sent": "So OK, so if X1 and X2 interact.",
                    "label": 0
                },
                {
                    "sent": "Right, if there's an interaction between X1 and X2, I will have a factor over here which connects X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "So FC is a is a factor that connects X one X2 and X3.",
                    "label": 0
                },
                {
                    "sent": "Let's say in addition to this going with your example, I have a factor which just connects X1 and X2 somehow.",
                    "label": 0
                },
                {
                    "sent": "Now, as soon as you introduce that factor, that's possible.",
                    "label": 0
                },
                {
                    "sent": "That's a valid factor graph.",
                    "label": 0
                },
                {
                    "sent": "You have introduced a loop in the factor graph.",
                    "label": 0
                },
                {
                    "sent": "Right, so that goes beyond the realm of what this algorithm can crack, But then I will be talking about how you handle those situations.",
                    "label": 0
                },
                {
                    "sent": "Then you get into what is called loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "You keep doing this anyway, so there will be messages that will go from X1 to that factor annex due to that factor, and that factor 2X1, and that's factored weeks too.",
                    "label": 0
                },
                {
                    "sent": "So that's called loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "It's just an extension of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So, so we stop the message passing in tree structured graph.",
                    "label": 0
                },
                {
                    "sent": "There's a clear notion of Waterleaf is right because you know that will be that they don't have any followers of things like that, so we will stop once on every node I have.",
                    "label": 0
                },
                {
                    "sent": "On every edge I have seen a message going in either direction in this direction and that direction, and that will typically be not twice the number of edges and is linear in the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "So so far for trees, that's why life is very simple.",
                    "label": 0
                },
                {
                    "sent": "Once you have a loop, then those things become a problem.",
                    "label": 0
                },
                {
                    "sent": "When do I stop right or where do I start and things like that and loopy belief propagation addresses those, but I think that should be another tutorial itself.",
                    "label": 0
                },
                {
                    "sent": "Other questions, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, let me actually.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "So say that again the example I'll work with.",
                    "label": 0
                },
                {
                    "sent": "So why is there no factor between X1X3 and X4, let's say.",
                    "label": 0
                },
                {
                    "sent": "So it looks.",
                    "label": 0
                },
                {
                    "sent": "Driver have.",
                    "label": 0
                },
                {
                    "sent": "That's why.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "So next we go back across next years for his life.",
                    "label": 0
                },
                {
                    "sent": "I see what you're saying, OK, I understand your question so.",
                    "label": 0
                },
                {
                    "sent": "So his question is that looks like X3 affect X1F X X3, so there should be a factor between X1 and X3 and X2F X X3 one.",
                    "label": 0
                },
                {
                    "sent": "There should be a factor we do next to next three but I have put a factor connecting all three and similarly for the bottom of.",
                    "label": 0
                },
                {
                    "sent": "Well, what I'm really.",
                    "label": 0
                },
                {
                    "sent": "I mean putting the factor based on is that I have this term which involves these three variables, right?",
                    "label": 0
                },
                {
                    "sent": "So my factors will come from the domain.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this particular case, if you look at it.",
                    "label": 0
                },
                {
                    "sent": "If broadly, and so, if you look at this conditional probability table, if the effect of burglary an alarm so alum depends both on both of them.",
                    "label": 0
                },
                {
                    "sent": "Right so so if I have a variable like alarm which depends on both burglary an earthquake.",
                    "label": 0
                },
                {
                    "sent": "My conditional probability.",
                    "label": 0
                },
                {
                    "sent": "My local function will have state variables which are beyond E and the probability for me if I have burglary affecting alarm only.",
                    "label": 0
                },
                {
                    "sent": "An earthquake affecting alarm.",
                    "label": 0
                },
                {
                    "sent": "Only then I'll have single factors.",
                    "label": 0
                },
                {
                    "sent": "In that case I'll rise in independence between those two.",
                    "label": 0
                },
                {
                    "sent": "In this case they are not independent, so if they are independent I I'm going to be more efficient because these factors are going to be smaller, involve less number of variables, so this is an example of a scenario where well and depends on both of them, right?",
                    "label": 0
                },
                {
                    "sent": "It's not and well as over here well generally depends on alarm.",
                    "label": 0
                },
                {
                    "sent": "If I know the value of alarm I don't need to know anything about anything.",
                    "label": 0
                },
                {
                    "sent": "I know what John is going to do and similarly.",
                    "label": 0
                },
                {
                    "sent": "Another value of alarm I I don't need to do.",
                    "label": 0
                },
                {
                    "sent": "Any of the other variables because I sort of know Mary is in conditionally independent of other variables given allow.",
                    "label": 0
                },
                {
                    "sent": "However, in case of alarm, if I only know that a burglary is happening, but I don't know whether an earthquake is happening, I won't be able to know which probability is going to be followed by the alarm, right?",
                    "label": 0
                },
                {
                    "sent": "So I need all the arguments in that factor to know exactly what probabilities, but you know, he depending on the domain will be several other things.",
                    "label": 0
                },
                {
                    "sent": "But you may be able to come up with the only thing that this algorithm sort of can handle is tree structured graphs, right?",
                    "label": 0
                },
                {
                    "sent": "And once you get into the loop examples, you can extend this same algorithm to loopy belief propagation, but that's for some other day.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And actually move forward.",
                    "label": 0
                },
                {
                    "sent": "Come.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so so we were talking about.",
                    "label": 0
                },
                {
                    "sent": "You know I'm not getting into the details of any emails because there's plenty of excellent tutorials on Hmm's and my whole point over here is to show that if you want to know why should I even care when a teams is a good example why you should care and you may be dealing with another model which does not look exactly like Hmm's, but the underlying graph structure is a tree and then you don't have to brainstorm because all three problems have been solved, right?",
                    "label": 0
                },
                {
                    "sent": "So this we consider solve problem as far as we're concerned now.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So there's a huge generalization of this whole framework to summonings, and so the whole observation involved in this, and again, this is not going to be kind of an exercise in abstract algebra, but just to remind you, you know these are these abstract structures which invite two operations, one called addition that are called multiplication, and they may be in a Boolean field then yellow field.",
                    "label": 0
                },
                {
                    "sent": "They may be somewhere else they satisfy some basic properties like associativity, community.",
                    "label": 0
                },
                {
                    "sent": "They have additive and multiplicative identity's and they satisfy the distributive law right?",
                    "label": 0
                },
                {
                    "sent": "This is an abstraction.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I just talked about.",
                    "label": 0
                },
                {
                    "sent": "And then you can come up with any number of your favorite signings.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is a popular one where the elements are between zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "The sum operation is Max.",
                    "label": 0
                },
                {
                    "sent": "The product operation is problem.",
                    "label": 0
                },
                {
                    "sent": "The corresponding algorithm is known as the Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "So Viterbi decoding which is used in hidden Markov models to find the most likely sequence of states, is simply an alternative to the Max product.",
                    "label": 0
                },
                {
                    "sent": "I got exact same complexity, exact same results, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "And if you recall, what Viterbi decoding does in case of Hmm's Max product algorithm does the exact same thing right?",
                    "label": 0
                },
                {
                    "sent": "So So what happens in the in the Max product semiring is that?",
                    "label": 0
                },
                {
                    "sent": "Over here in set up the if this edition is a Max.",
                    "label": 0
                },
                {
                    "sent": "Write an ABC are positive.",
                    "label": 0
                },
                {
                    "sent": "Then you have to read it as Max of ABN.",
                    "label": 0
                },
                {
                    "sent": "Max FAC is a times Max of BARC.",
                    "label": 0
                },
                {
                    "sent": "Right, so you find a maximum of BNC and then multiply that VD so you can replace this submission with a Max of the two arguments.",
                    "label": 0
                },
                {
                    "sent": "Under distributive law still holds.",
                    "label": 0
                },
                {
                    "sent": "So instead of summing over products, if you want to find a Max of our products, the same thing will work.",
                    "label": 0
                },
                {
                    "sent": "If you want to compute the Mail over products of the main over some of the Max over some, you have to simply specify what domain, right?",
                    "label": 0
                },
                {
                    "sent": "So in this in a match product case it has to be positive, so if everything is positive, the Max of baby and, AC is a times Max of the copy.",
                    "label": 0
                },
                {
                    "sent": "So this is the exact same formulation goes through and the people in different domains in error correcting codes.",
                    "label": 0
                },
                {
                    "sent": "Everybody know topical words in many other scenarios.",
                    "label": 0
                },
                {
                    "sent": "They are interested in other things, which is not exactly submission over products, but then a minimum over Sam's or maximum or products, or some million operations.",
                    "label": 0
                },
                {
                    "sent": "And the same idea holds right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So this is very nice.",
                    "label": 0
                },
                {
                    "sent": "So so this whole thing came together in the late 90s.",
                    "label": 0
                },
                {
                    "sent": "And then people look back and realize where we have rediscovered this algorithm.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Probably 15 times in the past 30 years, so belief propagation is net mapping friends in hidden Markov models, which is the Max work algorithm or alternative to Viterbi decoding common filtering.",
                    "label": 1
                },
                {
                    "sent": "Your accounting codes are plenty of LDC codes, Turbo codes and things like that.",
                    "label": 0
                },
                {
                    "sent": "This is a long list, right?",
                    "label": 0
                },
                {
                    "sent": "I want to stay sort of close to what we're discussing, but if you're interested in these other things are the more abstract views.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There there is literature out there to follow this now.",
                    "label": 0
                },
                {
                    "sent": "I'll spend a few minutes on what happens for general graphs where there are loops.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm in pre structured graphs.",
                    "label": 1
                },
                {
                    "sent": "We know that we are guaranteed to get the correct solution in whatever Semmering you're working with.",
                    "label": 0
                },
                {
                    "sent": "In general graphs.",
                    "label": 0
                },
                {
                    "sent": "This is a supremely active research topic.",
                    "label": 1
                },
                {
                    "sent": "For the past 10 years and good progress is being made.",
                    "label": 1
                },
                {
                    "sent": "But you know this is very much technical.",
                    "label": 0
                },
                {
                    "sent": "I don't have a story like the previous part.",
                    "label": 0
                },
                {
                    "sent": "I can tell that.",
                    "label": 1
                },
                {
                    "sent": "Oh look, we have cracked this problem.",
                    "label": 0
                },
                {
                    "sent": "We cannot track that problem right?",
                    "label": 0
                },
                {
                    "sent": "So we have not managed to so message passing.",
                    "label": 0
                },
                {
                    "sent": "If you generalize, I say I keep doing the message passing through.",
                    "label": 0
                },
                {
                    "sent": "Don't care if there's a loop it in principle here convert or it may converge local minima of some objective function, and people are trying to come up with methods which converts to the global maximum of something like that.",
                    "label": 0
                },
                {
                    "sent": "I will give some references to this literature at the end, but there are new approaches that have come up in the past few years.",
                    "label": 0
                },
                {
                    "sent": "We are trying to get convergent and correct message passing.",
                    "label": 1
                },
                {
                    "sent": "Even in loopy graphs, it's a very, very active topic.",
                    "label": 0
                },
                {
                    "sent": "Now the surprising bit about this which which I think I mean, I wouldn't have believed that I'm not actually known or met.",
                    "label": 0
                },
                {
                    "sent": "The people who are working on it is that loopy belief propagation, this belief propagation algorithm is actually used, so Xbox Live.",
                    "label": 0
                },
                {
                    "sent": "I don't know if any of you know what that thing is, that's.",
                    "label": 0
                },
                {
                    "sent": "Microsoft Xbox Gold Online so you know, then playing engineer, you go online and play with other players so they have a system called true scale.",
                    "label": 0
                },
                {
                    "sent": "Which matches you with other players and that algorithm of sort of player ranking is based on belief propagation.",
                    "label": 0
                },
                {
                    "sent": "The hedging idea is they started up from there.",
                    "label": 0
                },
                {
                    "sent": "How you know chess Masters are matched up or things like that, but true skill is actually live and running in Xbox Live.",
                    "label": 0
                },
                {
                    "sent": "So if you if you go online you try to play your matched up with some player from some other part of the world.",
                    "label": 0
                },
                {
                    "sent": "A belief propagation happened right so so essentially what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "It's a very very large scale.",
                    "label": 0
                },
                {
                    "sent": "Basing it on which message passing happens and these guys of course have fairly technical papers explaining what the factor graph is and how that happens.",
                    "label": 0
                },
                {
                    "sent": "The motivation behind this is that if I go online, I want to play some game.",
                    "label": 0
                },
                {
                    "sent": "If I match with like a pro by using one minute, I'll hit.",
                    "label": 0
                },
                {
                    "sent": "I'll never log in right, but as if I'm so match with somebody who's complete newbie and I will need for one minute, I will not like that either, so I will have to be matched with somebody with similar skill sets.",
                    "label": 0
                },
                {
                    "sent": "Under my our past games and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit more complicated than HMM is actually much more complicated than 18 months, and they do that big application for loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Actually essentially what are used in coding schemes in France.",
                    "label": 0
                },
                {
                    "sent": "The broadly these these firms class of Turbo codes, which are traditionally studied in the communication literature as convolution codes and so on.",
                    "label": 0
                },
                {
                    "sent": "But what the algorithm is, and at this point they also know this is not the belief propagation and it works somehow.",
                    "label": 0
                },
                {
                    "sent": "And these are used for of course in that region for defense satellite communication, especially in situations where error correction needs to be fairly robust.",
                    "label": 0
                },
                {
                    "sent": "So so if you're getting signals from Mars, you want to do like I can codes you want to use.",
                    "label": 0
                },
                {
                    "sent": "This, why Max is just.",
                    "label": 0
                },
                {
                    "sent": "Metrolight Wi-Fi network.",
                    "label": 0
                },
                {
                    "sent": "It's a protocol, and they use it over there, so these algorithms are used in many different places for very very large networks, right?",
                    "label": 0
                },
                {
                    "sent": "So I give you some examples.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that sort of brings us to.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When did the first part I will briefly start?",
                    "label": 0
                },
                {
                    "sent": "You know, get started on the on the second part and then in another 10 minutes or so we will take the coffee break.",
                    "label": 0
                },
                {
                    "sent": "This is how you know I'm coming through this decade.",
                    "label": 0
                },
                {
                    "sent": "2000 Two 1001 people started developing even more fancy graphical models and old friends becomes even more complicated, so we'll have to go to Apple inference methods so so one of the biggest developments in this area, and this will be a large chunk of of our discussion today, or what are known as mixed membership models, right?",
                    "label": 1
                },
                {
                    "sent": "And if you are familiar with what mixture models are which have been studied in statistics for a very long time, I will start with differentiating between these two and then focus primarily on the.",
                    "label": 0
                },
                {
                    "sent": "One of the moon are sort of representative algorithms are mammals, college English allocation used for topic modeling and text analysis.",
                    "label": 0
                },
                {
                    "sent": "We will discuss two different ways 2.",
                    "label": 0
                },
                {
                    "sent": "Three different ways of doing inference in this model because that exact inference message passing business does not work in this case.",
                    "label": 0
                },
                {
                    "sent": "It's intractable, and then I'll consider applications at generalizations including.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we recently did.",
                    "label": 0
                },
                {
                    "sent": "Alright, so since I'm going to go beyond that, you know five node network I'm going to go and do sort of thousands of hundreds of thousands of nodes network.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce some notation.",
                    "label": 0
                },
                {
                    "sent": "So this is what we call a plate diagram.",
                    "label": 0
                },
                {
                    "sent": "So if you have a Bayesian network where a causes B1B2B3 and you know given a these are independent, we sort of draw this up later and be an put a 3 means this is replicated three times.",
                    "label": 0
                },
                {
                    "sent": "This is just a convenient notation and at this point actually some some colleagues are trying to build.",
                    "label": 0
                },
                {
                    "sent": "Uh, compilers for for graphical models based on these kinds of specifications, they're trying to formalize it, but for our purposes this is just a schematic which will help us remember that, oh, this is repeated 10 million times or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here's the will start with the most basic model which we will use as a straw man and see how bad that model is, which is just modeling.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have feature vectors 3 dimensional feature vectors, Anuar model in each feature as independent.",
                    "label": 0
                },
                {
                    "sent": "Let's using abortion, but that is a statisical model.",
                    "label": 0
                },
                {
                    "sent": "You have 3 features, you have aggression for each feature.",
                    "label": 0
                },
                {
                    "sent": "That's a model, and the way this is not illustrate how to read this thing.",
                    "label": 0
                },
                {
                    "sent": "And in this particular case D is.",
                    "label": 0
                },
                {
                    "sent": "Is that dimensionality an N is the number of data points.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that I have the parameters for the individual domain specific dimension specific distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's say this is the mean of the Gaussian 1 version 2, version 3.",
                    "label": 0
                },
                {
                    "sent": "And then I generate 3 dimensional access D dimensional access.",
                    "label": 0
                },
                {
                    "sent": "And then I generate N off them.",
                    "label": 0
                },
                {
                    "sent": "So I generate ended up lines, each one of them is D dimensional.",
                    "label": 0
                },
                {
                    "sent": "That's that's how to read that create diagram.",
                    "label": 0
                },
                {
                    "sent": "This is not a fun map, right?",
                    "label": 0
                },
                {
                    "sent": "This is the you can't do much with it.",
                    "label": 0
                },
                {
                    "sent": "This is as bad as you are trying to do.",
                    "label": 0
                },
                {
                    "sent": "To document modeling you have for every word in the English dictionary you have abortion.",
                    "label": 0
                },
                {
                    "sent": "What is sort of the frequency of usage and you simply generate a document counts or word usage counts by independently sampling from the dictionary.",
                    "label": 0
                },
                {
                    "sent": "That's no good, right?",
                    "label": 0
                },
                {
                    "sent": "So that's not an interesting model.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we will start with nine days models or mixture mixture models, which is sort of a slight generalization of an Ivy structure.",
                    "label": 0
                },
                {
                    "sent": "Which looks like this, right so?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so they essential structure of the model is that I don't have so previously.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at this thing that in each dimension I in each dimension I had.",
                    "label": 0
                },
                {
                    "sent": "One Russian right?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What now?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "TV series is that well.",
                    "label": 0
                },
                {
                    "sent": "I have politics.",
                    "label": 0
                },
                {
                    "sent": "I have documents from politics and documents from sports.",
                    "label": 0
                },
                {
                    "sent": "And the word distribution for documents from politics.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the, let's say your dictionary is 3 dimensional and this is a.",
                    "label": 0
                },
                {
                    "sent": "So they look like this.",
                    "label": 0
                },
                {
                    "sent": "So if I'm using politics then they look like this.",
                    "label": 0
                },
                {
                    "sent": "If I'm using a sport they look like this.",
                    "label": 0
                },
                {
                    "sent": "So so I have two different distributions I meant.",
                    "label": 0
                },
                {
                    "sent": "Any Member, English Dictionary, one which explains politics documents other which explains Sports document.",
                    "label": 0
                },
                {
                    "sent": "That's an improvement, right?",
                    "label": 0
                },
                {
                    "sent": "That's an improvement.",
                    "label": 0
                },
                {
                    "sent": "And the way this generative model works is that for every document right for every document.",
                    "label": 0
                },
                {
                    "sent": "What I do is I have a prior so OK, so for over the entire corpus I have a power.",
                    "label": 0
                },
                {
                    "sent": "We sort of looks like this.",
                    "label": 0
                },
                {
                    "sent": "How many of them are politics and how many of them are sports documents?",
                    "label": 0
                },
                {
                    "sent": "Let's say no.",
                    "label": 0
                },
                {
                    "sent": "This is some sports website.",
                    "label": 0
                },
                {
                    "sent": "Some rest of their documents are sports documents and a few of them are politics documents.",
                    "label": 0
                },
                {
                    "sent": "To generate a single document, I first sample.",
                    "label": 0
                },
                {
                    "sent": "One of the two things right in this case I sample red, which is, let's say sports.",
                    "label": 0
                },
                {
                    "sent": "Then I go to the distribution specification for sports.",
                    "label": 0
                },
                {
                    "sent": "And I generate my feature vector according to that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now what I don't know.",
                    "label": 0
                },
                {
                    "sent": "So this red thing is busy, which I don't know what I know.",
                    "label": 0
                },
                {
                    "sent": "The only thing I know is this right by which is my final document or my final feature vector.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is another example.",
                    "label": 0
                },
                {
                    "sent": "I sample from this district discrete distribution again, I get blue, so it will be a politics document, so I look at these distributions and I generate a feature vector.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is how each data point is asumed to be generated.",
                    "label": 0
                },
                {
                    "sent": "I mean nobody generated it like this, but we are assuming that this is how they came into being.",
                    "label": 0
                },
                {
                    "sent": "So we have observed sort of this X, Theta and power the parimeter.",
                    "label": 0
                },
                {
                    "sent": "So so this is by you know these things are Theta Z.",
                    "label": 0
                },
                {
                    "sent": "We have never observed right?",
                    "label": 0
                },
                {
                    "sent": "This is sort of the mind based model.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not the girl.",
                    "label": 0
                },
                {
                    "sent": "One more step further and this mixture model I.",
                    "label": 0
                },
                {
                    "sent": "This has been around for a very, very long time.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The more recent development is this last piece, right?",
                    "label": 0
                },
                {
                    "sent": "Which is the mixed membership model.",
                    "label": 1
                },
                {
                    "sent": "So what a mixed membership model adds over and above you know this thing is that what this would allow you to do?",
                    "label": 0
                },
                {
                    "sent": "It assumes that our document can either be on sports bar on politics, or an entertainment, or one of these things.",
                    "label": 0
                },
                {
                    "sent": "It is not going to allow our document to be about politics in sports.",
                    "label": 0
                },
                {
                    "sent": "Because you have to commit, it's a mixture model.",
                    "label": 0
                },
                {
                    "sent": "You have to commit it to one of the components, right?",
                    "label": 0
                },
                {
                    "sent": "So what mixed membership models actually do is that they say I am going to allow a data point or a document to have mixed memberships in multiple of these themes or topics.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so and we will see a very concrete example of that in a few slides, probably after the coffee break.",
                    "label": 0
                },
                {
                    "sent": "So So what this model essentially starts off with.",
                    "label": 0
                },
                {
                    "sent": "Is a prior distribution overall mixed memberships or discrete distributions?",
                    "label": 0
                },
                {
                    "sent": "Now if you think of a discrete distribution, these things live on a simplex, so if you think of a discrete distribution about three things they have to satisfy, P1 plus B 2 + 3 equal to 1, and each one of them is greater equal to 0, right?",
                    "label": 0
                },
                {
                    "sent": "So they live in a simplex, so one of the popular choices for this priorities additional distribution which defines a distribution on top of a discrete distributions.",
                    "label": 0
                },
                {
                    "sent": "But there are many other choices that people have exposed, so the way one I assume so.",
                    "label": 0
                },
                {
                    "sent": "So this this all fire determines that power, which may be additional distribution.",
                    "label": 0
                },
                {
                    "sent": "These are the parameters for individual themes like politics, sports and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "The way we assume a data point is generated is as follows from this dish redistribution I for sample mixed membership right.",
                    "label": 0
                },
                {
                    "sent": "It says 20% sports.",
                    "label": 0
                },
                {
                    "sent": "You know 80% politics or something like that.",
                    "label": 0
                },
                {
                    "sent": "Then to generate every feature, now we cannot go to simply the blue one or the red one.",
                    "label": 0
                },
                {
                    "sent": "In general, the features right?",
                    "label": 0
                },
                {
                    "sent": "So what we do now is that for every feature we sample from this district distribution.",
                    "label": 0
                },
                {
                    "sent": "So we sample from it, we get a green.",
                    "label": 0
                },
                {
                    "sent": "We look at the green distribution over the blue we get from the blue distribution.",
                    "label": 0
                },
                {
                    "sent": "Over here we generate the first feature.",
                    "label": 0
                },
                {
                    "sent": "We sample from it again.",
                    "label": 0
                },
                {
                    "sent": "We get all red from this thing.",
                    "label": 0
                },
                {
                    "sent": "We generate sample a feature we sample from it.",
                    "label": 0
                },
                {
                    "sent": "Again, we get another head and from from this red distribution we get another feature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this is how mixed membership model is different, right?",
                    "label": 0
                },
                {
                    "sent": "So let's look at it.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, so I sample a mixed membership and this will be specific for every data point.",
                    "label": 0
                },
                {
                    "sent": "We will have mixed membership.",
                    "label": 0
                },
                {
                    "sent": "We sampled the color for the first feature.",
                    "label": 0
                },
                {
                    "sent": "We sample from that version color for the second feature sample from the corresponding version for the third feature General General, and then we have the features right.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To sort of get a big picture overview of the difference between mixture models and mixed membership models is that the observations are same rights observations.",
                    "label": 1
                },
                {
                    "sent": "In this case you know it's just three dimensional vectors over here 3 dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "These are different samples.",
                    "label": 0
                },
                {
                    "sent": "This assumes that it was generated by the red component and this was generated by the blue component, whereas this assumes that while the first one was more from the road a little from the from the blue and so now you can actually see.",
                    "label": 0
                },
                {
                    "sent": "But the left hand side example is sort of a special case where if this was a hard word and this was all a blue, then that would sort of fall back to the mixture mixture model.",
                    "label": 0
                },
                {
                    "sent": "However, these models are more flexible because they are they are letting the individual data points have mixed memberships to multiple of these components.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now one of the biggest success stories of mixed membership models.",
                    "label": 1
                },
                {
                    "sent": "I mean this sort of started bringing towards them 90s again.",
                    "label": 0
                },
                {
                    "sent": "Thomas Hoffman's work on probabilistic latent semantic indexing, which sort of presented this idea and then.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I love the most probably representative papers and unpopular papers.",
                    "label": 0
                },
                {
                    "sent": "For mixed membership models with applications to text is late enriched allocation.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you references to all of these papers at the end.",
                    "label": 0
                },
                {
                    "sent": "So late initial allocation was originally presented as a Mario for doing topic modeling, but from what we have seen in the past, so so so there are K topics and this pie is that mixed membership on those topics or so for documentary the.",
                    "label": 0
                },
                {
                    "sent": "Let's say there are 10 topics and this party is specific to a document.",
                    "label": 0
                },
                {
                    "sent": "Which is how much of sports?",
                    "label": 0
                },
                {
                    "sent": "How much of politics?",
                    "label": 0
                },
                {
                    "sent": "How much of entertainment?",
                    "label": 0
                },
                {
                    "sent": "How much of other things?",
                    "label": 0
                },
                {
                    "sent": "This is an that's sample from additional distribution.",
                    "label": 0
                },
                {
                    "sent": "Then, for each word you sample a specific topic and use the corresponding discrete distribution over the English dictionary to come up with the word.",
                    "label": 1
                },
                {
                    "sent": "Now I'm going to go through this with an animation and with more examples on the line, but this is a concrete instantiation of the mixed membership abstract.",
                    "label": 0
                },
                {
                    "sent": "Idea that I presented in a couple of slides back.",
                    "label": 0
                },
                {
                    "sent": "So Pi is basically the distribution of our topics for each document.",
                    "label": 1
                },
                {
                    "sent": "Z is the topic assignment for an individual word.",
                    "label": 0
                },
                {
                    "sent": "And then beta is the distribution of rewards for each topic.",
                    "label": 0
                },
                {
                    "sent": "So for politics will have a distribution over the English Dictionary.",
                    "label": 0
                },
                {
                    "sent": "Sports will have our distribution over the English Dictionary.",
                    "label": 0
                },
                {
                    "sent": "If there are K topics LBK distributions, so beat out the parameters of the basic model.",
                    "label": 0
                },
                {
                    "sent": "Alpha is another parameter of the basic model, so the learning and inference will involve we have to estimate Alpha and beta and these are the things we want to infer or these things are these things we want to infer this document.",
                    "label": 0
                },
                {
                    "sent": "How much of sports?",
                    "label": 0
                },
                {
                    "sent": "How much of politics?",
                    "label": 0
                },
                {
                    "sent": "So I think you know this, probably a coffee break going on will.",
                    "label": 0
                },
                {
                    "sent": "Take a short 1520 minute break and then reconvene at 3:30 I guess.",
                    "label": 0
                }
            ]
        }
    }
}