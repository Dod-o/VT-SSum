{
    "id": "p6xphlzu2wexvop3g6mmrxb2cazqlbj5",
    "title": "The Offset Tree for Learning with Partial Labels",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_langford_otlpl/",
    "segmentation": [
        [
            "OK, so this is joint work with Elina."
        ],
        [
            "So this is in this workshop is on a very timely subject because.",
            "Is the problems that you guys are addressing are actually a great importance to many people?",
            "So at Yahoo for example?",
            "People put up advertisements and then if people click on the advertisements then.",
            "Then there who makes money right?",
            "And so it's important to put up the good advertisements."
        ],
        [
            "And this is in fact an exploration problem, right?",
            "Because what's happening is a user comes to Yahoo with some hidden interests.",
            "They make a query and then you choose some add to display and then the user clicks on the ad or or does not right?",
            "And I guess the critical thing is that you only learn about the ad that you choose to put up.",
            "So."
        ],
        [
            "We can formalize this.",
            "As you know, the world chooses some features and some hidden interests.",
            "It reveals the features.",
            "And now you choose some action amongst your set of possible actions, and then the world reveals the reward of that particular action.",
            "So compared to the standard bandit setting, I guess it's a little bit different because maybe you don't.",
            "Maybe in the standard and setting you don't have X, and it's pretty important to have the features, because if you try to just have like a single total ordering over all the ads, that would not work very well at all.",
            "Also, if you tried to have you tried to just run a bandit algorithm for every possible query, there would be a lot of queries that only happened once and then you manage out of them would never converge to anything useful.",
            "So it's also important to think about this.",
            "The relationship of this problem in comparison to sort of more standard supervised learning.",
            "Where I guess if you got the full vector of rewards then we would we would know how to apply certain supervised learning algorithms.",
            "So this is like supervised learning except we only get one reward or it's like the mana setting except that we have features.",
            "OK, so.",
            "Um?",
            "If you, I guess the state of analysis of this problem is we have some initial algorithms."
        ],
        [
            "But there's there's an issue with computational complexity which I talked about earlier, so.",
            "I guess that issue with competition complexity also comes up in just normal supervised learning, right?",
            "Because our knowledge of PAC Learning is limited, the set of things that we know how to do pack learn in computational science fact learn as in polynomial time is actually relatively small.",
            "It nevertheless in practice it seems like.",
            "We have for tourists coincidences of real world problems and in real world learning algorithms that tend to succeed in practice.",
            "So what we would like to do is try to take advantage of these fortuitous coincidences in our for this problem, right?",
            "So question is how do we reuse existing supervised learning algorithms to solve this problem?",
            "Friend."
        ],
        [
            "Um?",
            "So the basic idea is that we're going to use a reduction, so we're going to.",
            "We have a data source which is descr."
        ],
        [
            "Saved in this fashion."
        ],
        [
            "We're going to reduce it to the kind of data that a supervised learning algorithm takes input.",
            "We're going to see we're going to be supervised learning algorithm that supervised learning to have some particular generalization performance.",
            "That generalization performance can translate into some sort of bound on on the performance in choosing ads."
        ],
        [
            "OK, so.",
            "I'm actually going to tell you about several different approaches for doing this, because I think it's informative to kind of work through things one step at a time so.",
            "This is 3 different approaches and this is the one which is the new one that I'm really talking about, but but these two I think are sort of very important background approaches and they also help you understand the different ways you could do things.",
            "OK, so the simplest approach if you go and you talk to some sort of applied person, what they might try to do is."
        ],
        [
            "Um?",
            "Is just use argmax regression right?",
            "So.",
            "You have some particular action A and you have features and.",
            "The goal is to just learn a regressor from the features to the value of the expected reward and then.",
            "And then you make a choice according to just an argmax.",
            "Very natural approach, right?",
            "Because you know that the minimizer of squared loss is expected value, so you can easily.",
            "You can easily apply regression to this and you know that if you solved the square loss problem perfectly then this would in fact be a good policy.",
            "Perfect policy in fact.",
            "OK, so.",
            "This is straightforward I guess."
        ],
        [
            "OK, so now here's the question.",
            "The reviewers for this workshop had a couple of doubts about paper, but nevertheless they were nice enough to let us actually present it.",
            "So is this for online F or offline?",
            "This is algorithm apply online or offline.",
            "Certainly applies offline, but."
        ],
        [
            "See the answer is yes, it applies to both.",
            "Because you could be.",
            "If you can do online learning of these apps and then you just have an online learned H which is a direct.",
            "Right?"
        ],
        [
            "OK, so so now how do we try to analyze this sort of thing so?",
            "I should mention that in general we have an exploration problem, and in general we're going to want to use some sort of non uniform distribution in order to optimize their exploration but but that's a complication which is easily handled according to probabilities as Gabor talked about this morning.",
            "So I'm just going to assume that the randomization is uniform and that's kind of simplifies the formulas for this stuff.",
            "So.",
            "How do we analyze this so we have some distribution D which gives us features we're going to think about a uniform distribution over actions, and then given a choice of action, we're going to draw from the conditional distribution on the rewards given the features.",
            "OK, so this is going to induce a particular distribution.",
            "And we're going to look at optimizing.",
            "Maybe the squared loss on this distribution.",
            "So there's some some optimal choice of regressor, let's call it F star.",
            "And then our aggressor is going to maybe mess up to some extent.",
            "And we're going to get the square difference in expectation with selected D prime.",
            "So this is this is a measure of how close are regressors to converging to the optimal predictor.",
            "And then so this is kind of the regret.",
            "On the induced problem is how well we do compared to how well we could have done on the problem that we created.",
            "But the problem is that we actually care about what we actually care about is the regret in our optimization of the policy right?",
            "So we wanted to choose reward, which has a large value, so we're interested in the difference of these two quantities.",
            "OK, so.",
            "The claim is for every D, for every.",
            "I'll call it a contextual bandit problem for every regressor we might learn.",
            "Um?",
            "You have the regret of the policy that's upper bounded by sqrt 2 K times the average squared loss regret.",
            "So.",
            "So some of you are thinking square root.",
            "Yeah, but actually we should be thinking skirt Boo.",
            "And the reason why is because I'm thinking of the rewards is bounded to the interval 01.",
            "Which means that if this corner here is larger than one, then you've already lost the theorem vacuous.",
            "Um?",
            "So when you're less than one, taking square root makes the thing larger.",
            "So in short hand, I guess you can think of this.",
            "The policy regret is bounded by root 2K times the binary regret binary regressor.",
            "Regret.",
            "OK, so."
        ],
        [
            "Is this an online or an offline analysis?",
            "Does this apply to online or offline learning algorithms?",
            "Yes.",
            "So the thing to notice is that this holds for all D and that means it holds for the D, which is a Delta function on One X, so it holds pointwise.",
            "Right, and that means it holds, you know, for each individual event which occurs in an online system.",
            "Because you can just take the expectation and get a new deal that way, right so?"
        ],
        [
            "So the answer is indeed yes.",
            "However, there is a caveat here, which is this is not addressing the explore exploit tradeoff.",
            "And I don't know how to address the explore exploit tradeoff with this kind of analysis.",
            "What we're trying to address this kind of analysis is, given that you have exploration data, how do you best use that exploration data to derive a policy?",
            "Several natural choices for how to do this and introductions give us some guide in which of these natural choices is the best one."
        ],
        [
            "OK, so let me let me tell you how you analyze this.",
            "It turns out to be very simple, so think about the world where one choice has conditioned on K. So we fix X.",
            "We fix in advance.",
            "I'll think about the world in which one choice has a large reward and the rest have small rewards.",
            "And now a regressor is, we think of as an adversary.",
            "It's going to try to mess up as little as possible so that it causes us to have a big regret, right?",
            "So in particular, it could say the reward here is large, even though it's actually small and they were.",
            "Here is small, even though it's actually large.",
            "And when it does that, it's going to have a regret in curd.",
            "Which is is like the this difference divided by two because we have a factor of two and then we're going to incur that twice out of K times.",
            "So we have.",
            "An average squared error.",
            "Regret of 1 / 2 K times times this squared difference.",
            "And then you can use Jensen's inequality and you can just solve for the policy regret.",
            "So this is just a very simple analysis.",
            "OK, so now."
        ],
        [
            "What one of you said that this is a shallow analysis, and I think I agree.",
            "This is a shell analysis exactly like Fisher.",
            "Consistency is a shell analysis exactly exactly like savages work on probability.",
            "Elicitation is shallow analysis.",
            "Right, so I will be happy if they sell.",
            "Analysis is taught just like that in classes.",
            "And I think it is.",
            "It is fundamental and just exactly the same sense.",
            "Alright, so so that's that's the most straightforward approach which you will encounter if people if you work with any kind of applied person.",
            "Now the question is, can we do something better?",
            "Because if we go when we look at."
        ],
        [
            "At the analysis we see we have this two in this K in the square root and it's not really clear which was essential and what's not.",
            "Maybe we can do much better, or maybe we can't."
        ],
        [
            "So they."
        ],
        [
            "It was an approach outlined in Bianca draws nice thesis where.",
            "You reduce to importance weighted multiclass classification.",
            "Right, So what does it mean to have an important way to so?",
            "I guess the idea was important is that you have an importance weight of 10 on particular example.",
            "Then that example counts 10 when you're counting your expected loss.",
            "If you have an importance weight of 1 example counts once.",
            "So just kind of a modifier on the way that you measure things.",
            "So the idea is that you have if you have a set of features, an action Anna Reward.",
            "Then you're going to create an important way multiclass example where you have a set of features, an action and an importance weight.",
            "So.",
            "This is a triple.",
            "Let the triple, but they're not the same triple, because this is an important weight.",
            "And now we want to.",
            "We want to think about everything in sort of the same language.",
            "So we want to we want to reduce this to the same primitive.",
            "So it turns out that."
        ],
        [
            "So I talked about a squared loss regressor here.",
            "But it turns out that you can reduce squared loss regression to just 01 classification and the multiplier is 1.",
            "So if we can reduce."
        ],
        [
            "This problem is just binary classification and will have a well have two formulas which gives us two different notions of how a regret bounds of regret and we would maybe prefer the tighter one.",
            "Right?",
            "So we can do that.",
            "We know how to get rid of the importance weights if you can.",
            "Just rejection samples for the portents weights.",
            "And we know how to go from multiclass to binary, and we actually know lots of ways to go from class to binary.",
            "One of the tightest ones as far as regret analysis, is this error correcting tournament.",
            "So so we will think about using that one.",
            "OK, so then the test time just becomes very straightforward.",
            "You just apply your multiclass predictor."
        ],
        [
            "OK, so.",
            "So now there's an induced binary distribution.",
            "OK, so there's something that is very powerful but reductions, but also maybe a little bit confusing if you're used to doing to seeing a very complete analysis and the reductions are composable.",
            "Right so.",
            "If I have a reduction from.",
            "Uh."
        ],
        [
            "From this contextual bandit problem to importance weighted multiclass, I can compose that with a reduction from importance weighted multi class to multiclass and then compose that with reduction from multiclass to binary.",
            "Write an because analysis was set up to be composable.",
            "I can actually compose the theorems which makes things."
        ],
        [
            "Doable, but it makes it slightly hard to understand exactly what this D prime is, so the claim is that if I go through these different reductions, I'm going to end up a single exam."
        ],
        [
            "People single XAR is going to produce some number of binary examples.",
            "And I'm going to draw uniformly from that set.",
            "So that's going to be."
        ],
        [
            "My induced binary distribution.",
            "OK. And now the claim is that for every.",
            "Contextual bandit problem for every binary classifier that I might learn 4K times the binary classifier regret.",
            "This is the difference between the 01 loss that you achieve and the minimum possible 01 loss.",
            "Upper bounds the policy regret.",
            "The induced policy regret?",
            "And the proof is straightforward, so it might have."
        ],
        [
            "A little bit mysterious where this came from.",
            "You don't actually need it because you're killing everything like a, but it's just convenient."
        ],
        [
            "Think of it is there because.",
            "When you're when you're randomizing over the actions is going to be at one in K chance that you pay a regret for the particular choice that you make, so that cancels out with a K and that's why it sort of So what you're left with is just the policy regret for each individual round, which is going to hold an expectation.",
            "I feel like I'm going too fast, so.",
            "Maybe I'm not going too fast.",
            "OK, so.",
            "Yeah, this is this.",
            "In expectation is the policy regret.",
            "And then we can just compose with the existing reductions.",
            "So the getting rid of importance weights means we multiply by K and reducing to multiclass to binary and we multiplied by 4."
        ],
        [
            "OK, so now I'll tell you about the offset tree."
        ],
        [
            "So just in terms of this, before we go there, we got rid of the square root square root was not essential.",
            "That's nice.",
            "This week two became a four, which is OK, but still not.",
            "There's still K there.",
            "That's kind of bad, and the question is, can we get rid of those?"
        ],
        [
            "So I'm going to tell you an even better reduction.",
            "So just to tell you this reduction for just the when you have two actions.",
            "And let's think of the two actions is being labeled one label minus one.",
            "And now we're going to create binary importance weighted samples.",
            "So we have a particular XA&R and this is the features.",
            "This will be the label and this will be an important weight.",
            "OK, so the importance weight is equal to how far we are from 1/2.",
            "And there's something really funny happening with the label.",
            "So labels either one or 2 -- 1.",
            "And.",
            "The action is either one or minus one, so if the reward is large, if it's one, then one month to half going to be positive number.",
            "So then the sign of a will just be a.",
            "Right?",
            "And if the reward is small, if it's 0.",
            "Then, uh.",
            "Then this will be a negative number and the sign of this quantity will be minus a.",
            "So what's going on is if we observe a large reward for some action, we pretend the other action was the right action.",
            "An if we observe if you observe a small reorder together actions, the right action.",
            "If we observe a large reward then we go with the action that we actually chose.",
            "OK, so it seems a little bit funny, but it turns out to help."
        ],
        [
            "Because we can, we can prove something.",
            "We can prove that the regret of the binary classifier that we learn from that importance weighted data set bounds the regret of our policy.",
            "There's no way we don't expect to have.",
            "There's no for there.",
            "There's no two that is just a constant of 1, which is great, so this is this is a.",
            "Spirit and courage.",
            "OK, so we have this little trick and now we would like to."
        ],
        [
            "Actually compose it, which we can do with a tree.",
            "So each node in this tree is going to be one of these binary classifiers, and.",
            "The basic idea.",
            "So I've almost told you everything about how to train right?",
            "Because what we're going to do is we're going to start a leaf, so maybe action three is taken, and then we're going to derive an example here, just like I talked about before.",
            "And then if this predictor chooses this action, we're going to derive an example here, just like I told you before.",
            "And this prediction chooses this action.",
            "We're going to drive an example here, as I told you before.",
            "But if this predictor does not choose this action, then we're going to stop.",
            "We're not going to create an example here or here.",
            "OK, so that turns out to be a senchal.",
            "If you don't do that, then you will end up predicting the wrong thing.",
            "Ascentia Lee.",
            "You want to predict the action with the largest reward rather than the subtree with the largest aggregate reward.",
            "And using this little filtering trick lets you form the right prediction problem to do that."
        ],
        [
            "OK, so.",
            "What can you prove about this?",
            "So we have this induced?",
            "Binary problem.",
            "The exact mechanism by which we create binary samples is different, but we have some.",
            "Any individual sample produces a set of binary samples.",
            "We draw uniform from that.",
            "We have the binary classifier that we learn.",
            "The claim is that for all KAD, for all binary classifiers.",
            "K -- 1 times the binary regret bounds policy regret.",
            "OK, so then we have a K. And in terms of that, you can't get rid of that.",
            "So you can prove a lower bound which says that no reduction has a better analysis.",
            "Yes, and.",
            "Alright, so one of the reviewers thought this was kind of trivial because this could be zero and and then.",
            "And then it's kind of vacuous, but actually you can prove lower bound with this having any value between one and or 0 and 1 / K -- 1.",
            "OK, so so."
        ],
        [
            "Yeah, this game is 1 so now now we're.",
            "With your additions we have.",
            "A set of algorithms with a set of analysis before us, and the question is, how do we expect them to behave right when the rubber meets the road and we actually go out and try to do something?",
            "What works better?",
            "Any guesses?",
            "Alright, so I'm speaking in front of you, so it's this one of course, but."
        ],
        [
            "OK, so we can do offline experiments just treating it as an offline type production and we can compare the offset tree to the argmax approach on a bunch of datasets from UCI and you can see that the loss for the argmax approach is typically higher than the loss for the offset tree.",
            "So what we're doing is we have a multiclass datasets or pretending that these are sort of exploration datasets, we just go in with probe at at one of the random choices to see if the loss of zero or one there.",
            "Right?",
            "OK, So what else can you do so you can compare the offset tree to the importance weighted approach and here the story is a bit less clear, right?",
            "So maybe we win a little bit more than we lose, but it is a little bit less clear.",
            "OK, so now."
        ],
        [
            "Shy.",
            "Who worked on the bandit run paper?",
            "Actually tried it out on the receive one data set and he was nice enough to give us a data set so we so we have to think back to the bandit run is really sort of the realizable case and the offset tree is kind of.",
            "It's made to work in the fully agnostic case where there's any distribution over X in our vectors.",
            "If you just use the vanilla offset tree.",
            "Depending on exactly how you define the probabilities you into performing a little bit worse, so let's see this is.",
            "Around .2 Ish which is I think about up here right here, but it's slightly worse.",
            "But if you go in you it's easy to go into the analysis of the offset tree and just re optimize it to work in the realizable case.",
            "And on this particular data set, that's the right thing to do because you can get almost perfect prediction.",
            "And then you can you get this curve which is.",
            "A bit better than this curve.",
            "Right in terms of we get better performance, so this is what is our online regret as we run through a bunch of examples.",
            "Um?",
            "Exploring as we go.",
            "And we were just using uniform random exploration that was decaying according to a paper that young and I worked on a little while ago.",
            "OK, so.",
            "So we did slightly better in this last curve.",
            "Here the really good one is what would happen if this is actually a supervised problem.",
            "If we actually knew what the right choice was for every example we saw the label of.",
            "OK, so."
        ],
        [
            "So that's it.",
            "So there's a paper on my web page if you just click on interactive learning.",
            "And there's some further discussion about this and other topics at lunch.net.",
            "Thank you.",
            "Yeah.",
            "Lost production.",
            "Was that supposed my arm?",
            "So let's take the two arm case again, and one action has reward of one another.",
            "Action was awarded 2/3, then it seems like all the tables would be 1."
        ],
        [
            "My binary classifier doesn't.",
            "It can always predict 1, right?",
            "I. Yeah, so 111 action has reward one.",
            "Another action is reward 2/3.",
            "So I was going to agree with the action that you took.",
            "Right, so the sign sign of this thing is I was doing equal a.",
            "But the importance weight is going to be different.",
            "And that will mean that you end up preferring the action, which gives you a reward of 1.",
            "So this is an important example to think about because.",
            "Think about what would happen if a half was zero.",
            "If you have zero then you have a much noisier binary problem to learn on, and then you regret would be worse.",
            "So that that is exactly the intuition for why you want to use that offset.",
            "OK, yeah.",
            "So there is the way we fix the tree was just according to the binary representation for those experiments.",
            "I don't think we've experimented with variation on this problem.",
            "We have experiment variation in other tree based problems.",
            "There is some variation in performance.",
            "I think it depends on the problem, but for a lot of problems it's actually remarkably little.",
            "So the random tree and the best tree in it working pretty similar.",
            "It surprised me substantially.",
            "What's that?",
            "Yeah, you need enough data.",
            "Inflexible enough predictors seems like having a good enough predictor at the at the nodes is actually very powerful in overcoming a bad choice of the tree."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with Elina.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is in this workshop is on a very timely subject because.",
                    "label": 0
                },
                {
                    "sent": "Is the problems that you guys are addressing are actually a great importance to many people?",
                    "label": 0
                },
                {
                    "sent": "So at Yahoo for example?",
                    "label": 0
                },
                {
                    "sent": "People put up advertisements and then if people click on the advertisements then.",
                    "label": 0
                },
                {
                    "sent": "Then there who makes money right?",
                    "label": 0
                },
                {
                    "sent": "And so it's important to put up the good advertisements.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is in fact an exploration problem, right?",
                    "label": 0
                },
                {
                    "sent": "Because what's happening is a user comes to Yahoo with some hidden interests.",
                    "label": 1
                },
                {
                    "sent": "They make a query and then you choose some add to display and then the user clicks on the ad or or does not right?",
                    "label": 0
                },
                {
                    "sent": "And I guess the critical thing is that you only learn about the ad that you choose to put up.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can formalize this.",
                    "label": 0
                },
                {
                    "sent": "As you know, the world chooses some features and some hidden interests.",
                    "label": 0
                },
                {
                    "sent": "It reveals the features.",
                    "label": 0
                },
                {
                    "sent": "And now you choose some action amongst your set of possible actions, and then the world reveals the reward of that particular action.",
                    "label": 0
                },
                {
                    "sent": "So compared to the standard bandit setting, I guess it's a little bit different because maybe you don't.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the standard and setting you don't have X, and it's pretty important to have the features, because if you try to just have like a single total ordering over all the ads, that would not work very well at all.",
                    "label": 0
                },
                {
                    "sent": "Also, if you tried to have you tried to just run a bandit algorithm for every possible query, there would be a lot of queries that only happened once and then you manage out of them would never converge to anything useful.",
                    "label": 0
                },
                {
                    "sent": "So it's also important to think about this.",
                    "label": 0
                },
                {
                    "sent": "The relationship of this problem in comparison to sort of more standard supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Where I guess if you got the full vector of rewards then we would we would know how to apply certain supervised learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So this is like supervised learning except we only get one reward or it's like the mana setting except that we have features.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If you, I guess the state of analysis of this problem is we have some initial algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's there's an issue with computational complexity which I talked about earlier, so.",
                    "label": 0
                },
                {
                    "sent": "I guess that issue with competition complexity also comes up in just normal supervised learning, right?",
                    "label": 0
                },
                {
                    "sent": "Because our knowledge of PAC Learning is limited, the set of things that we know how to do pack learn in computational science fact learn as in polynomial time is actually relatively small.",
                    "label": 0
                },
                {
                    "sent": "It nevertheless in practice it seems like.",
                    "label": 0
                },
                {
                    "sent": "We have for tourists coincidences of real world problems and in real world learning algorithms that tend to succeed in practice.",
                    "label": 0
                },
                {
                    "sent": "So what we would like to do is try to take advantage of these fortuitous coincidences in our for this problem, right?",
                    "label": 0
                },
                {
                    "sent": "So question is how do we reuse existing supervised learning algorithms to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "Friend.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that we're going to use a reduction, so we're going to.",
                    "label": 0
                },
                {
                    "sent": "We have a data source which is descr.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Saved in this fashion.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to reduce it to the kind of data that a supervised learning algorithm takes input.",
                    "label": 0
                },
                {
                    "sent": "We're going to see we're going to be supervised learning algorithm that supervised learning to have some particular generalization performance.",
                    "label": 0
                },
                {
                    "sent": "That generalization performance can translate into some sort of bound on on the performance in choosing ads.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to tell you about several different approaches for doing this, because I think it's informative to kind of work through things one step at a time so.",
                    "label": 0
                },
                {
                    "sent": "This is 3 different approaches and this is the one which is the new one that I'm really talking about, but but these two I think are sort of very important background approaches and they also help you understand the different ways you could do things.",
                    "label": 0
                },
                {
                    "sent": "OK, so the simplest approach if you go and you talk to some sort of applied person, what they might try to do is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is just use argmax regression right?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You have some particular action A and you have features and.",
                    "label": 0
                },
                {
                    "sent": "The goal is to just learn a regressor from the features to the value of the expected reward and then.",
                    "label": 0
                },
                {
                    "sent": "And then you make a choice according to just an argmax.",
                    "label": 0
                },
                {
                    "sent": "Very natural approach, right?",
                    "label": 0
                },
                {
                    "sent": "Because you know that the minimizer of squared loss is expected value, so you can easily.",
                    "label": 1
                },
                {
                    "sent": "You can easily apply regression to this and you know that if you solved the square loss problem perfectly then this would in fact be a good policy.",
                    "label": 0
                },
                {
                    "sent": "Perfect policy in fact.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is straightforward I guess.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now here's the question.",
                    "label": 0
                },
                {
                    "sent": "The reviewers for this workshop had a couple of doubts about paper, but nevertheless they were nice enough to let us actually present it.",
                    "label": 0
                },
                {
                    "sent": "So is this for online F or offline?",
                    "label": 0
                },
                {
                    "sent": "This is algorithm apply online or offline.",
                    "label": 0
                },
                {
                    "sent": "Certainly applies offline, but.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See the answer is yes, it applies to both.",
                    "label": 0
                },
                {
                    "sent": "Because you could be.",
                    "label": 0
                },
                {
                    "sent": "If you can do online learning of these apps and then you just have an online learned H which is a direct.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so now how do we try to analyze this sort of thing so?",
                    "label": 0
                },
                {
                    "sent": "I should mention that in general we have an exploration problem, and in general we're going to want to use some sort of non uniform distribution in order to optimize their exploration but but that's a complication which is easily handled according to probabilities as Gabor talked about this morning.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to assume that the randomization is uniform and that's kind of simplifies the formulas for this stuff.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do we analyze this so we have some distribution D which gives us features we're going to think about a uniform distribution over actions, and then given a choice of action, we're going to draw from the conditional distribution on the rewards given the features.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is going to induce a particular distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're going to look at optimizing.",
                    "label": 0
                },
                {
                    "sent": "Maybe the squared loss on this distribution.",
                    "label": 0
                },
                {
                    "sent": "So there's some some optimal choice of regressor, let's call it F star.",
                    "label": 0
                },
                {
                    "sent": "And then our aggressor is going to maybe mess up to some extent.",
                    "label": 0
                },
                {
                    "sent": "And we're going to get the square difference in expectation with selected D prime.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a measure of how close are regressors to converging to the optimal predictor.",
                    "label": 0
                },
                {
                    "sent": "And then so this is kind of the regret.",
                    "label": 0
                },
                {
                    "sent": "On the induced problem is how well we do compared to how well we could have done on the problem that we created.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that we actually care about what we actually care about is the regret in our optimization of the policy right?",
                    "label": 0
                },
                {
                    "sent": "So we wanted to choose reward, which has a large value, so we're interested in the difference of these two quantities.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The claim is for every D, for every.",
                    "label": 0
                },
                {
                    "sent": "I'll call it a contextual bandit problem for every regressor we might learn.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You have the regret of the policy that's upper bounded by sqrt 2 K times the average squared loss regret.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So some of you are thinking square root.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but actually we should be thinking skirt Boo.",
                    "label": 0
                },
                {
                    "sent": "And the reason why is because I'm thinking of the rewards is bounded to the interval 01.",
                    "label": 0
                },
                {
                    "sent": "Which means that if this corner here is larger than one, then you've already lost the theorem vacuous.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So when you're less than one, taking square root makes the thing larger.",
                    "label": 0
                },
                {
                    "sent": "So in short hand, I guess you can think of this.",
                    "label": 0
                },
                {
                    "sent": "The policy regret is bounded by root 2K times the binary regret binary regressor.",
                    "label": 0
                },
                {
                    "sent": "Regret.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this an online or an offline analysis?",
                    "label": 1
                },
                {
                    "sent": "Does this apply to online or offline learning algorithms?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So the thing to notice is that this holds for all D and that means it holds for the D, which is a Delta function on One X, so it holds pointwise.",
                    "label": 0
                },
                {
                    "sent": "Right, and that means it holds, you know, for each individual event which occurs in an online system.",
                    "label": 0
                },
                {
                    "sent": "Because you can just take the expectation and get a new deal that way, right so?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the answer is indeed yes.",
                    "label": 0
                },
                {
                    "sent": "However, there is a caveat here, which is this is not addressing the explore exploit tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And I don't know how to address the explore exploit tradeoff with this kind of analysis.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to address this kind of analysis is, given that you have exploration data, how do you best use that exploration data to derive a policy?",
                    "label": 0
                },
                {
                    "sent": "Several natural choices for how to do this and introductions give us some guide in which of these natural choices is the best one.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me let me tell you how you analyze this.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be very simple, so think about the world where one choice has conditioned on K. So we fix X.",
                    "label": 0
                },
                {
                    "sent": "We fix in advance.",
                    "label": 0
                },
                {
                    "sent": "I'll think about the world in which one choice has a large reward and the rest have small rewards.",
                    "label": 0
                },
                {
                    "sent": "And now a regressor is, we think of as an adversary.",
                    "label": 0
                },
                {
                    "sent": "It's going to try to mess up as little as possible so that it causes us to have a big regret, right?",
                    "label": 0
                },
                {
                    "sent": "So in particular, it could say the reward here is large, even though it's actually small and they were.",
                    "label": 0
                },
                {
                    "sent": "Here is small, even though it's actually large.",
                    "label": 0
                },
                {
                    "sent": "And when it does that, it's going to have a regret in curd.",
                    "label": 0
                },
                {
                    "sent": "Which is is like the this difference divided by two because we have a factor of two and then we're going to incur that twice out of K times.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "An average squared error.",
                    "label": 0
                },
                {
                    "sent": "Regret of 1 / 2 K times times this squared difference.",
                    "label": 0
                },
                {
                    "sent": "And then you can use Jensen's inequality and you can just solve for the policy regret.",
                    "label": 0
                },
                {
                    "sent": "So this is just a very simple analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What one of you said that this is a shallow analysis, and I think I agree.",
                    "label": 0
                },
                {
                    "sent": "This is a shell analysis exactly like Fisher.",
                    "label": 0
                },
                {
                    "sent": "Consistency is a shell analysis exactly exactly like savages work on probability.",
                    "label": 0
                },
                {
                    "sent": "Elicitation is shallow analysis.",
                    "label": 0
                },
                {
                    "sent": "Right, so I will be happy if they sell.",
                    "label": 0
                },
                {
                    "sent": "Analysis is taught just like that in classes.",
                    "label": 0
                },
                {
                    "sent": "And I think it is.",
                    "label": 0
                },
                {
                    "sent": "It is fundamental and just exactly the same sense.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so that's that's the most straightforward approach which you will encounter if people if you work with any kind of applied person.",
                    "label": 0
                },
                {
                    "sent": "Now the question is, can we do something better?",
                    "label": 0
                },
                {
                    "sent": "Because if we go when we look at.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the analysis we see we have this two in this K in the square root and it's not really clear which was essential and what's not.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can do much better, or maybe we can't.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was an approach outlined in Bianca draws nice thesis where.",
                    "label": 0
                },
                {
                    "sent": "You reduce to importance weighted multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "Right, So what does it mean to have an important way to so?",
                    "label": 0
                },
                {
                    "sent": "I guess the idea was important is that you have an importance weight of 10 on particular example.",
                    "label": 0
                },
                {
                    "sent": "Then that example counts 10 when you're counting your expected loss.",
                    "label": 0
                },
                {
                    "sent": "If you have an importance weight of 1 example counts once.",
                    "label": 0
                },
                {
                    "sent": "So just kind of a modifier on the way that you measure things.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you have if you have a set of features, an action Anna Reward.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to create an important way multiclass example where you have a set of features, an action and an importance weight.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a triple.",
                    "label": 0
                },
                {
                    "sent": "Let the triple, but they're not the same triple, because this is an important weight.",
                    "label": 0
                },
                {
                    "sent": "And now we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to think about everything in sort of the same language.",
                    "label": 0
                },
                {
                    "sent": "So we want to we want to reduce this to the same primitive.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I talked about a squared loss regressor here.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you can reduce squared loss regression to just 01 classification and the multiplier is 1.",
                    "label": 0
                },
                {
                    "sent": "So if we can reduce.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem is just binary classification and will have a well have two formulas which gives us two different notions of how a regret bounds of regret and we would maybe prefer the tighter one.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we can do that.",
                    "label": 0
                },
                {
                    "sent": "We know how to get rid of the importance weights if you can.",
                    "label": 0
                },
                {
                    "sent": "Just rejection samples for the portents weights.",
                    "label": 0
                },
                {
                    "sent": "And we know how to go from multiclass to binary, and we actually know lots of ways to go from class to binary.",
                    "label": 0
                },
                {
                    "sent": "One of the tightest ones as far as regret analysis, is this error correcting tournament.",
                    "label": 0
                },
                {
                    "sent": "So so we will think about using that one.",
                    "label": 0
                },
                {
                    "sent": "OK, so then the test time just becomes very straightforward.",
                    "label": 0
                },
                {
                    "sent": "You just apply your multiclass predictor.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So now there's an induced binary distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's something that is very powerful but reductions, but also maybe a little bit confusing if you're used to doing to seeing a very complete analysis and the reductions are composable.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "If I have a reduction from.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From this contextual bandit problem to importance weighted multiclass, I can compose that with a reduction from importance weighted multi class to multiclass and then compose that with reduction from multiclass to binary.",
                    "label": 0
                },
                {
                    "sent": "Write an because analysis was set up to be composable.",
                    "label": 0
                },
                {
                    "sent": "I can actually compose the theorems which makes things.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doable, but it makes it slightly hard to understand exactly what this D prime is, so the claim is that if I go through these different reductions, I'm going to end up a single exam.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People single XAR is going to produce some number of binary examples.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to draw uniformly from that set.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My induced binary distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. And now the claim is that for every.",
                    "label": 0
                },
                {
                    "sent": "Contextual bandit problem for every binary classifier that I might learn 4K times the binary classifier regret.",
                    "label": 0
                },
                {
                    "sent": "This is the difference between the 01 loss that you achieve and the minimum possible 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Upper bounds the policy regret.",
                    "label": 0
                },
                {
                    "sent": "The induced policy regret?",
                    "label": 0
                },
                {
                    "sent": "And the proof is straightforward, so it might have.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit mysterious where this came from.",
                    "label": 0
                },
                {
                    "sent": "You don't actually need it because you're killing everything like a, but it's just convenient.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of it is there because.",
                    "label": 0
                },
                {
                    "sent": "When you're when you're randomizing over the actions is going to be at one in K chance that you pay a regret for the particular choice that you make, so that cancels out with a K and that's why it sort of So what you're left with is just the policy regret for each individual round, which is going to hold an expectation.",
                    "label": 0
                },
                {
                    "sent": "I feel like I'm going too fast, so.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm not going too fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is this.",
                    "label": 0
                },
                {
                    "sent": "In expectation is the policy regret.",
                    "label": 0
                },
                {
                    "sent": "And then we can just compose with the existing reductions.",
                    "label": 0
                },
                {
                    "sent": "So the getting rid of importance weights means we multiply by K and reducing to multiclass to binary and we multiplied by 4.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'll tell you about the offset tree.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just in terms of this, before we go there, we got rid of the square root square root was not essential.",
                    "label": 0
                },
                {
                    "sent": "That's nice.",
                    "label": 0
                },
                {
                    "sent": "This week two became a four, which is OK, but still not.",
                    "label": 0
                },
                {
                    "sent": "There's still K there.",
                    "label": 0
                },
                {
                    "sent": "That's kind of bad, and the question is, can we get rid of those?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to tell you an even better reduction.",
                    "label": 0
                },
                {
                    "sent": "So just to tell you this reduction for just the when you have two actions.",
                    "label": 0
                },
                {
                    "sent": "And let's think of the two actions is being labeled one label minus one.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to create binary importance weighted samples.",
                    "label": 0
                },
                {
                    "sent": "So we have a particular XA&R and this is the features.",
                    "label": 0
                },
                {
                    "sent": "This will be the label and this will be an important weight.",
                    "label": 0
                },
                {
                    "sent": "OK, so the importance weight is equal to how far we are from 1/2.",
                    "label": 0
                },
                {
                    "sent": "And there's something really funny happening with the label.",
                    "label": 0
                },
                {
                    "sent": "So labels either one or 2 -- 1.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The action is either one or minus one, so if the reward is large, if it's one, then one month to half going to be positive number.",
                    "label": 0
                },
                {
                    "sent": "So then the sign of a will just be a.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And if the reward is small, if it's 0.",
                    "label": 0
                },
                {
                    "sent": "Then, uh.",
                    "label": 0
                },
                {
                    "sent": "Then this will be a negative number and the sign of this quantity will be minus a.",
                    "label": 0
                },
                {
                    "sent": "So what's going on is if we observe a large reward for some action, we pretend the other action was the right action.",
                    "label": 0
                },
                {
                    "sent": "An if we observe if you observe a small reorder together actions, the right action.",
                    "label": 0
                },
                {
                    "sent": "If we observe a large reward then we go with the action that we actually chose.",
                    "label": 0
                },
                {
                    "sent": "OK, so it seems a little bit funny, but it turns out to help.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we can, we can prove something.",
                    "label": 0
                },
                {
                    "sent": "We can prove that the regret of the binary classifier that we learn from that importance weighted data set bounds the regret of our policy.",
                    "label": 0
                },
                {
                    "sent": "There's no way we don't expect to have.",
                    "label": 0
                },
                {
                    "sent": "There's no for there.",
                    "label": 0
                },
                {
                    "sent": "There's no two that is just a constant of 1, which is great, so this is this is a.",
                    "label": 0
                },
                {
                    "sent": "Spirit and courage.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this little trick and now we would like to.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually compose it, which we can do with a tree.",
                    "label": 0
                },
                {
                    "sent": "So each node in this tree is going to be one of these binary classifiers, and.",
                    "label": 0
                },
                {
                    "sent": "The basic idea.",
                    "label": 0
                },
                {
                    "sent": "So I've almost told you everything about how to train right?",
                    "label": 0
                },
                {
                    "sent": "Because what we're going to do is we're going to start a leaf, so maybe action three is taken, and then we're going to derive an example here, just like I talked about before.",
                    "label": 0
                },
                {
                    "sent": "And then if this predictor chooses this action, we're going to derive an example here, just like I told you before.",
                    "label": 0
                },
                {
                    "sent": "And this prediction chooses this action.",
                    "label": 0
                },
                {
                    "sent": "We're going to drive an example here, as I told you before.",
                    "label": 0
                },
                {
                    "sent": "But if this predictor does not choose this action, then we're going to stop.",
                    "label": 0
                },
                {
                    "sent": "We're not going to create an example here or here.",
                    "label": 1
                },
                {
                    "sent": "OK, so that turns out to be a senchal.",
                    "label": 0
                },
                {
                    "sent": "If you don't do that, then you will end up predicting the wrong thing.",
                    "label": 0
                },
                {
                    "sent": "Ascentia Lee.",
                    "label": 1
                },
                {
                    "sent": "You want to predict the action with the largest reward rather than the subtree with the largest aggregate reward.",
                    "label": 0
                },
                {
                    "sent": "And using this little filtering trick lets you form the right prediction problem to do that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What can you prove about this?",
                    "label": 0
                },
                {
                    "sent": "So we have this induced?",
                    "label": 0
                },
                {
                    "sent": "Binary problem.",
                    "label": 0
                },
                {
                    "sent": "The exact mechanism by which we create binary samples is different, but we have some.",
                    "label": 0
                },
                {
                    "sent": "Any individual sample produces a set of binary samples.",
                    "label": 0
                },
                {
                    "sent": "We draw uniform from that.",
                    "label": 0
                },
                {
                    "sent": "We have the binary classifier that we learn.",
                    "label": 0
                },
                {
                    "sent": "The claim is that for all KAD, for all binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "K -- 1 times the binary regret bounds policy regret.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we have a K. And in terms of that, you can't get rid of that.",
                    "label": 0
                },
                {
                    "sent": "So you can prove a lower bound which says that no reduction has a better analysis.",
                    "label": 0
                },
                {
                    "sent": "Yes, and.",
                    "label": 0
                },
                {
                    "sent": "Alright, so one of the reviewers thought this was kind of trivial because this could be zero and and then.",
                    "label": 0
                },
                {
                    "sent": "And then it's kind of vacuous, but actually you can prove lower bound with this having any value between one and or 0 and 1 / K -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this game is 1 so now now we're.",
                    "label": 0
                },
                {
                    "sent": "With your additions we have.",
                    "label": 0
                },
                {
                    "sent": "A set of algorithms with a set of analysis before us, and the question is, how do we expect them to behave right when the rubber meets the road and we actually go out and try to do something?",
                    "label": 0
                },
                {
                    "sent": "What works better?",
                    "label": 0
                },
                {
                    "sent": "Any guesses?",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm speaking in front of you, so it's this one of course, but.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can do offline experiments just treating it as an offline type production and we can compare the offset tree to the argmax approach on a bunch of datasets from UCI and you can see that the loss for the argmax approach is typically higher than the loss for the offset tree.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we have a multiclass datasets or pretending that these are sort of exploration datasets, we just go in with probe at at one of the random choices to see if the loss of zero or one there.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, So what else can you do so you can compare the offset tree to the importance weighted approach and here the story is a bit less clear, right?",
                    "label": 0
                },
                {
                    "sent": "So maybe we win a little bit more than we lose, but it is a little bit less clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shy.",
                    "label": 0
                },
                {
                    "sent": "Who worked on the bandit run paper?",
                    "label": 0
                },
                {
                    "sent": "Actually tried it out on the receive one data set and he was nice enough to give us a data set so we so we have to think back to the bandit run is really sort of the realizable case and the offset tree is kind of.",
                    "label": 0
                },
                {
                    "sent": "It's made to work in the fully agnostic case where there's any distribution over X in our vectors.",
                    "label": 0
                },
                {
                    "sent": "If you just use the vanilla offset tree.",
                    "label": 0
                },
                {
                    "sent": "Depending on exactly how you define the probabilities you into performing a little bit worse, so let's see this is.",
                    "label": 0
                },
                {
                    "sent": "Around .2 Ish which is I think about up here right here, but it's slightly worse.",
                    "label": 0
                },
                {
                    "sent": "But if you go in you it's easy to go into the analysis of the offset tree and just re optimize it to work in the realizable case.",
                    "label": 0
                },
                {
                    "sent": "And on this particular data set, that's the right thing to do because you can get almost perfect prediction.",
                    "label": 0
                },
                {
                    "sent": "And then you can you get this curve which is.",
                    "label": 0
                },
                {
                    "sent": "A bit better than this curve.",
                    "label": 0
                },
                {
                    "sent": "Right in terms of we get better performance, so this is what is our online regret as we run through a bunch of examples.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Exploring as we go.",
                    "label": 0
                },
                {
                    "sent": "And we were just using uniform random exploration that was decaying according to a paper that young and I worked on a little while ago.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So we did slightly better in this last curve.",
                    "label": 0
                },
                {
                    "sent": "Here the really good one is what would happen if this is actually a supervised problem.",
                    "label": 0
                },
                {
                    "sent": "If we actually knew what the right choice was for every example we saw the label of.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "So there's a paper on my web page if you just click on interactive learning.",
                    "label": 0
                },
                {
                    "sent": "And there's some further discussion about this and other topics at lunch.net.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Lost production.",
                    "label": 0
                },
                {
                    "sent": "Was that supposed my arm?",
                    "label": 0
                },
                {
                    "sent": "So let's take the two arm case again, and one action has reward of one another.",
                    "label": 0
                },
                {
                    "sent": "Action was awarded 2/3, then it seems like all the tables would be 1.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My binary classifier doesn't.",
                    "label": 0
                },
                {
                    "sent": "It can always predict 1, right?",
                    "label": 0
                },
                {
                    "sent": "I. Yeah, so 111 action has reward one.",
                    "label": 0
                },
                {
                    "sent": "Another action is reward 2/3.",
                    "label": 0
                },
                {
                    "sent": "So I was going to agree with the action that you took.",
                    "label": 0
                },
                {
                    "sent": "Right, so the sign sign of this thing is I was doing equal a.",
                    "label": 0
                },
                {
                    "sent": "But the importance weight is going to be different.",
                    "label": 0
                },
                {
                    "sent": "And that will mean that you end up preferring the action, which gives you a reward of 1.",
                    "label": 0
                },
                {
                    "sent": "So this is an important example to think about because.",
                    "label": 0
                },
                {
                    "sent": "Think about what would happen if a half was zero.",
                    "label": 0
                },
                {
                    "sent": "If you have zero then you have a much noisier binary problem to learn on, and then you regret would be worse.",
                    "label": 0
                },
                {
                    "sent": "So that that is exactly the intuition for why you want to use that offset.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "So there is the way we fix the tree was just according to the binary representation for those experiments.",
                    "label": 0
                },
                {
                    "sent": "I don't think we've experimented with variation on this problem.",
                    "label": 0
                },
                {
                    "sent": "We have experiment variation in other tree based problems.",
                    "label": 0
                },
                {
                    "sent": "There is some variation in performance.",
                    "label": 0
                },
                {
                    "sent": "I think it depends on the problem, but for a lot of problems it's actually remarkably little.",
                    "label": 0
                },
                {
                    "sent": "So the random tree and the best tree in it working pretty similar.",
                    "label": 0
                },
                {
                    "sent": "It surprised me substantially.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you need enough data.",
                    "label": 0
                },
                {
                    "sent": "Inflexible enough predictors seems like having a good enough predictor at the at the nodes is actually very powerful in overcoming a bad choice of the tree.",
                    "label": 0
                }
            ]
        }
    }
}