{
    "id": "kj75hydywngjpeey6ojbxk5bedhgx5jl",
    "title": "Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees",
    "info": {
        "author": [
            "Pravesh Kothari, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "May 15, 2014",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_kothari_trees/",
    "segmentation": [
        [
            "Alright, so I'm going to talk about representation, approximation and learning of submodular functions using low rank decision trees.",
            "This is a joint work as it outside with metallic Feldman and Jan Vondrak of IBM Research.",
            "So let me start by defining words."
        ],
        [
            "Modular functions are, so submodular functions are from abroad class of family functions called as set functions which are defined on subsets of integers from one to N and take real values.",
            "So a set function F is submodular if it satisfies this property of diminishing marginal returns.",
            "Which means that if you have two sets as NTC, that S is included in T, then the marginal gain in adding a new element to S in the value of the function is a higher or equal to the marginal gain on adding the same new element to the value of the function to the two largest city.",
            "So that's the property of diminishing marginal returns that defines submodular functions."
        ],
        [
            "We will equivalently look at these functions to be defined on the Boolean cube by associating every binary string to a set to a subset of integers.",
            "Wentworth, in the natural way of looking at it as an indicator string of a set.",
            "Alright, so."
        ],
        [
            "So modular functions have a rich history of work on it in like variety of different contexts.",
            "For example, I guess the most important applications of submodular functions happen to be incremental optimization, where they are viewed as discrete analogue of convex functions.",
            "This connection actually can be made precise because there exists a convex continuous extension of submodular functions called us lowers extension, so modular functions appear in special cases as generalization of special instances of problems that you see in common.",
            "Station, for example graph cut functions rank functions of matroids set covering functions all happen to be submodular.",
            "They also happen to have applications in other problems of interest here, like plant location and sensor placement.",
            "The application in sensor placement stems from the idea that the information given to you by a set of sensors happens to be a submodular function, and you can use your methods of submodular optimization to get an optimal sensor placement algorithm.",
            "There are other important use happens to be in algorithmic game theory.",
            "Do precisely the property I mentioned in the last slide.",
            "That of diminishing marginal returns.",
            "This is a property which you interpret to be had by utility functions of agents.",
            "When you're modeling a game theoretic system, your prices, so naturally you'd expect applications in algorithm game theory and economix because of this property and it has been studied extensively in this context.",
            "It's."
        ],
        [
            "Our application or our main concern would be with learning submodular functions and in this context the problem was first introduced by Balkan and hardware.",
            "In 2011 they were motivated by learning and predicting submodular functions when they model pricing and utility functions of agents in game theoretic systems, they wanted to demands of agents and they were also motivated by some applications to advertisements.",
            "So with all these applications that define this new model of learning called SP Mac.",
            "Which wanted to, which was intended to capture a learning theoretic view when you are interested in commercial optimization and this model is short for probably mostly approximately correct.",
            "So let me let me just give you a technical note here that in all the works on learning, and in fact for the rest of the talk, let's just think about only non negative submodular functions.",
            "Alright, so let me let me now tell you what this P Mac model is and what Barker and how we proved in their first paper so."
        ],
        [
            "So feedback model the learner gets to see random examples from your target submodular function.",
            "The unknown submodular function and the job of the learner is to use this random examples to construct a hypothesis edge which multiplicatively approximates your unknown submodular function at all but an epsilon probability mass of points.",
            "So that's your pee Mak model, and in this model."
        ],
        [
            "And how we studied the problem for off learning submodular functions on arbitrary distributions, and they showed that the problem in fact is very hard in this setting.",
            "They gave an algorithm which produces a order routine multiplicative approximation with probability 1 minus epsilon in polynomial time for all submodular functions.",
            "And they also showed nearly matching lower bound for any polynomial time algorithm that wants to learn submodular functions for the easier case of product distributions, they showed that.",
            "One can get a log whenever epsilon multiplicative approximation in time with probability 1 minus epsilon for the class of 1 Lipschitz submodular functions with minimum value at least one.",
            "And so this is the state of art in Barker and Harvey."
        ],
        [
            "Gupta at all.",
            "Look at the problem motivated by applications to privacy and they define this model of learning.",
            "Consider this model of learning with respect to additive error.",
            "So in this model the learner gets to make queries to the function so it can ask for the value of the unknown function at any point of its choice and the job of the learner is to produce a hypothesis which additively approximates.",
            "If so, we would consider this L1 error.",
            "The average additive error for this model, and this will.",
            "This will also be interested.",
            "This will also be the case of interest for our purpose, so this will be the notion of error that we will deal with."
        ],
        [
            "In the previous model, what do you observe this stuff?"
        ],
        [
            "Worries in the in the back and Harvey model in the pmac model, you are supposed to learn the function.",
            "Just some random examples, random queries, so they're not really query so you get random points and then the value of the unknown function.",
            "Real value of the function at that point.",
            "Alright, so that's the old model."
        ],
        [
            "And in this model they showed that one can learn submodular functions on all product distributions with into the one or epsilon squared time where epsilon is the error."
        ],
        [
            "Jackie at all, observed that one can in fact just use random examples to construct a low degree polynomial approximator.",
            "A degree, one epsilon squared polynomial approximation for submodular functions, and they use this idea to give a algorithm to learn them in the same time.",
            "The good thing about this algorithm is that they also worked in the agnostic setting owing to the approximation by polynomials, which could be learned agnostically by just polynomial regression.",
            "So what's agnostic model?",
            "Let's briefly."
        ],
        [
            "Alright, so in this model we actually get an arbitrary function which is not guaranteed to be submodular.",
            "Have any structure at all, and what we are required to do is produce a hypothesis which does competetively best with respect to the best fitting submodular function.",
            "So it's supposed to give us Optus Epsilon error, update the error of the best fitting submodular function, right?"
        ],
        [
            "So, uh, in a special case of submodular function, we take values in the discrete range from integers zero to K, rush, nikova and Yaroslav, save in 2013 showed that there exists an algorithm that uses value query access to the function and produces a hypothesis in polynomial time."
        ],
        [
            "For all constant error and polynomials are constant size ranges, they considered this stronger notion of error culture disagreement error, which means, since the range is just great, we just want to bound the number of points where the function does not equal to the unknown functions value."
        ],
        [
            "So with this context, now we can look at our algorithmic results which would have which would see in a moment.",
            "So for PAC learning, we show that we can get a polynomial time algorithm in North and two to one square to the one to four for learning discuss.",
            "This improves upon the best known time bound for the PAC learning from the previous works.",
            "We also show that we can improve the time bound of agnostic learning submodular functions if we allow queries and this algorithm runs in time polynomial in North and to the website on square.",
            "So we can show that in the special case when the submodular function takes values in a discrete range, we can improve upon Rush, Nico and yellow socks at work and get an algorithm that runs in Poly and two to the K and one or epsilon, right?"
        ],
        [
            "We also complemented our results were nearly matching lower bounds and essentially the message of this lower bounds, which I'll go into it in a bit, is that our algorithms impact noise setting on nearly optimal."
        ],
        [
            "Alright, so how do we get about this results I guess?",
            "Of more importance to us is this representation approximation results for submodular functions as a consequence of this, which get our learning and learning results.",
            "And So what are these results?",
            "Well, we show that submodular functions are approximated by shallow real value decision trees.",
            "I'll define these objects in a moment.",
            "If you can't realize what exactly they are.",
            "But as a corollary, we can show that submodular functions are efficiently approximated by hunters.",
            "So what are hunters?",
            "Well, these are functions that depend only on a few variables.",
            "So how many variables will be required to approximate the model function well?"
        ],
        [
            "The number of variables just depends on your accuracy.",
            "So whatever dimension you take this a modular function to be in.",
            "You can just approximate approximate it to any constant error with the function of constantly many variables.",
            "So we got a proof of this result using about approximation by decision trees.",
            "We also showed that this implies a simple proof for submodular functions being approximated by law degree polynomials just re proving the result of Karachi at all that we considered earlier."
        ],
        [
            "Right, so since structural results are for prime interest here, let me let me tell you what I'm going to talk about is actual results in the remaining part of the talk.",
            "So this is the overview of what we will discuss.",
            "We will first tell you about how we can represent some model functions by low rank decision trees with Lipschitz submodular functions at the leaves.",
            "Then we will use this representation to construct an approximation of submodular functions using real value decision trees, which means the leaves will have real values at the end, and then we will use this.",
            "We will use a general result which will prove.",
            "That low rank binary decision trees can be approximated by low depth decision trees and this will give us our final approximation of submodular functions as low depth decision trees.",
            "So without further ado, let's let's get into this."
        ],
        [
            "So at first topic of interest is representing the modular functions using low rank decision trees of Lipschitz."
        ],
        [
            "Functions so one of these objects, recalling briefly, so we have a decision tree.",
            "This is a normal decision tree that you all know about, except that the leaves we have Alpha Lipschitz armor in a function for some parameter Alpha.",
            "So what?",
            "I will I will come to it.",
            "I'll come to in a moment.",
            "OK, So what are our objects where we're discussing this decision?",
            "Trees which have Alpha Lipschitz functions at the leaves.",
            "So what our father should function?",
            "It just shows that the marginal difference.",
            "On one distance, one neighbors of the value of the function is at most Alpha.",
            "OK, so let's recall briefly terminology for trees that we have.",
            "We have Lee."
        ],
        [
            "Then the longest path in the tree would be called as a depth of the tree."
        ],
        [
            "And we will discuss as she asked about low rank decision Tree which now."
        ],
        [
            "I'll define so this is a.",
            "This is a well studied notion.",
            "The rank of a decision tree can be defined recursively as follows.",
            "OK, so let's consider a decision tree tea with left and right subtrees as Steven Ti O."
        ],
        [
            "So ranking is defined as zero if your tree itself is just a single node.",
            "Believe otherwise it has two children.",
            "If the rank of the two children is different, you just set it as the maximum of the rank of the children.",
            "Otherwise you just add 1 to the rank of the children.",
            "So that's your rank, and in 2 two way to understand rank is to think of it as the depth of the largest complete binary tree that you can embed in your given tree.",
            "Alright, so that's rank of a tree.",
            "Now that we understand what the objects were dealing with, we can."
        ],
        [
            "State of results.",
            "So we show that some model functions can be exactly computed by decision trees of rank 2 by Alpha with Alpha Lipschitz Armada functions at each leaf.",
            "We will see why lift functions are important in a moment, OK?"
        ],
        [
            "So this result of ours is based on the decomposition of submodular functions into Lipschitz similar functions, which was used by Gupta at all in their paper also."
        ],
        [
            "So let me show you quickly how we can actually construct this decision tree approximation for just monotone submodular case.",
            "This is the easier case to understand.",
            "We can extend our proof in order in case in a moment.",
            "So suppose your function happens to be Alpha lip shades in the beginning itself.",
            "So at that time you do nothing and just set it as a leaf.",
            "You're done where other."
        ],
        [
            "Guys, we can argue by submodularity that there must exist a variable, say X3 such that when you add this element 3 to the empty set, the value of the function rises by at least Alpha.",
            "So."
        ],
        [
            "So in this case you make extrion node and you recurse on."
        ],
        [
            "Children.",
            "Alright, so that's your procedure.",
            "This can go on for as long as it wants.",
            "What we want to argue is that."
        ],
        [
            "The end object that we construct is a low rank decision tree.",
            "So why is the rank of the decision tree produce low?"
        ],
        [
            "Well, the reason is very simple.",
            "By monotonicity we can argue that the number of left turns in the tree is at most one by Alpha.",
            "Notice that in each left turn the value of the function must increase by at least Alpha, and we already considering that this range of the function is bounded between zero and one, so that gives us a simple proof that for monotone submodular functions one can approximate them.",
            "One can actually come exactly, compute them by low rank decision trees of Alpha lifted submodular function."
        ],
        [
            "Alright, So what do we do for non native?",
            "But we use this observation which is also used by Gupta at all that if S is a modular then so is F of S compliment.",
            "I would not go into details here but this is this is standard.",
            "This has been used also before.",
            "Alright so now that we believe that some other functions can be computed by all physicians order function decision trees."
        ],
        [
            "The next we go to the next part of the talk which is approximating them by real value decision trees."
        ],
        [
            "Alright, so this is the object we have for some computing a similar function for the previous part.",
            "What we're going to do is replace."
        ],
        [
            "This Alpha lecture some order functions."
        ],
        [
            "By real values.",
            "OK, So what are we going to do?",
            "Well, the key idea here, which was also used in previous work, is that you can replace each Alpha submodular function by its expectation and not incur a lot of error.",
            "So there are actually very good concentration inequalities for Lipschitz submodular functions starting with the work of Boucheron, Massart, and legacy, for example, this is the inequality we would need that F does not deviate from its expectation the basically the average error.",
            "Of F from its expectation is at most square root Alpha F is Alpha Lipschitz.",
            "Alright, so the user has added to approximate each leave by a constant it's expectation and we get a real valued decision tree."
        ],
        [
            "So this gives us a Riddle that submodular functions which we are considering to be normalized with zero and one.",
            "Since we are dealing with editing better are approximated within error epsilon by real value.",
            "Decision trees of rank at most 4 over epsilon squared."
        ],
        [
            "So now we go to our third part of a structural results which is approximating low rank binary decision trees by load up decision trees.",
            "So this part itself is a standard result.",
            "We will just apply this to our submodular function approximation to get our final corollary."
        ],
        [
            "So what do we show here?",
            "Will consider a binary decision tree T an.",
            "We will truncate it to depth D which is order R Plus log over epsilon, where R is the rank of the tree."
        ],
        [
            "And we can show that the disagreement between this T and the truncated tree is at most happens with probability at most epsilon.",
            "This is a general truncation procedure for any low ranking Gentry and."
        ],
        [
            "Generalizes the truncation based on size, which was used by cash limits and Mansour in the 1993 paper on Decision Tree learning."
        ],
        [
            "So we can apply this result as a over submodular function approximation to obtain that for every submodular function F there is a real value decision tree of depth at most one over epsilon squared, that L1 approximate search within epsilon.",
            "So that gives us our final approximation results, modular functions."
        ],
        [
            "This, as I said before, also is a quick proof of degree bound of Cherokee roll."
        ],
        [
            "Your ex is beautiful distributed on the happy to actually it works for any product distribution also.",
            "But you could think of it as uniform distribution in a special case.",
            "Yes alright?"
        ],
        [
            "So now we will quickly discuss applications for learning."
        ],
        [
            "So, so what's application is back learning.",
            "We show that there exists an algorithm which, given just random example, access to the function which returns the hypothesis that Albert approximated within epsilon and depends on just two to one square variables.",
            "This algorithm runs in time till 9 squared to the ones to for an users just log out many examples for any constant error so."
        ],
        [
            "The way we obtained this result is basically using our result from the previous part.",
            "We show that.",
            "Standard Way implies that there is a low degree polynomial approximator that depends on just a few variables.",
            "Now in general it is not known how to find the influential variables, even if you know that there are few.",
            "But for submodular functions we can utilize their properties and show that just looking at degree 1 degree 2 for your coefficient of these functions can give us what the influential variables are.",
            "Using this procedure we can just do L1 regression in the end to obtain our approximator.",
            "So that's over.",
            "That's how we get our pack learning algorithm."
        ],
        [
            "We can also get agnostic learning algorithm which works with queries in time polynomial in Enter Square and uses the Polygon examples for any constant error.",
            "So here we actually combine our results in the previous part.",
            "With the we can combine our reserve apart with the result of the political climate zone learning decision.",
            "Trees agnostic learning decision trees and but we would lose attribute efficiency.",
            "That is, we would use more number of examples and required to reduce the number of examples.",
            "What we can do is use the attribute efficient version of question.",
            "Answer An agnostic posting of Feldman and Kelly Kennedy."
        ],
        [
            "So we talk about nearly matching lower bounds, so whatever lower bounds so we show that back learning even a monotone submodular function requires exponentially more epsilon value queries, and the proof happens.",
            "The proof goes by embedding any arbitrary Boolean function in a monotone submodular function of a slightly higher dimension.",
            "So basically you can reduce learning of any Boolean function to submodular function is slightly high dimension, and that's how we get our lower bound here."
        ],
        [
            "This shows that our back Gnostic algorithm, which queries are optimal up to the exponent in epsilon one epsilon."
        ],
        [
            "In the computational side, we show that agnostically learning even monotone submodular functions.",
            "If you can beat enter the small of one or epsilon to three algorithm, then you get a faster algorithm for the problem of learning parities with noise."
        ],
        [
            "Problem is a notoriously hard problem, and the best known algorithm that we know even now is just ended the point 8K.",
            "So OK, that we believe it's hard."
        ],
        [
            "And this proof works by showing that there exist a monotone submodular function which is highly correlated with the parity."
        ],
        [
            "So this shows that the agnostic algorithm attraction at all is optimal for the exponent on epsilon."
        ],
        [
            "Alright, so that's the summary so far.",
            "We can approximate submodular function by load of decision trees and as a consequence we can do almost optimal packing agnostic learning algorithms for the class."
        ],
        [
            "In some follow up work, let me just briefly tell you want to follow work has happened after this.",
            "So Feldman Vondrak actually have improved the hotel approximation that we talked about.",
            "In this talk we can show that we can actually approximates a model function by functions of justice polynomial in one over epsilon variables and they also gave a faster algorithm for learning this class and the P Mac.",
            "The challenging pimc model of Balkan Harvey in order to follow up work within joint work with Vitaly Feldman, we show that one can actually give a fully polynomial time algorithm that is.",
            "N1 over epsilon 4 Pack and Mac learning.",
            "A special case of submodular functions.",
            "The coverage functions and.",
            "These are also well studied as valuation functions in game theory, right?",
            "So there's a follow up work."
        ],
        [
            "The open question that remains is what can you do about more general distributions.",
            "We know that on arbitrary distributions the lower bound of bulk and Harvey also holds even in the winter setting in.",
            "But for some transformation, on the other hand, we know that product distributions are certainly easy.",
            "So what can we do in between?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm going to talk about representation, approximation and learning of submodular functions using low rank decision trees.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work as it outside with metallic Feldman and Jan Vondrak of IBM Research.",
                    "label": 0
                },
                {
                    "sent": "So let me start by defining words.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modular functions are, so submodular functions are from abroad class of family functions called as set functions which are defined on subsets of integers from one to N and take real values.",
                    "label": 0
                },
                {
                    "sent": "So a set function F is submodular if it satisfies this property of diminishing marginal returns.",
                    "label": 1
                },
                {
                    "sent": "Which means that if you have two sets as NTC, that S is included in T, then the marginal gain in adding a new element to S in the value of the function is a higher or equal to the marginal gain on adding the same new element to the value of the function to the two largest city.",
                    "label": 0
                },
                {
                    "sent": "So that's the property of diminishing marginal returns that defines submodular functions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will equivalently look at these functions to be defined on the Boolean cube by associating every binary string to a set to a subset of integers.",
                    "label": 0
                },
                {
                    "sent": "Wentworth, in the natural way of looking at it as an indicator string of a set.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So modular functions have a rich history of work on it in like variety of different contexts.",
                    "label": 0
                },
                {
                    "sent": "For example, I guess the most important applications of submodular functions happen to be incremental optimization, where they are viewed as discrete analogue of convex functions.",
                    "label": 1
                },
                {
                    "sent": "This connection actually can be made precise because there exists a convex continuous extension of submodular functions called us lowers extension, so modular functions appear in special cases as generalization of special instances of problems that you see in common.",
                    "label": 1
                },
                {
                    "sent": "Station, for example graph cut functions rank functions of matroids set covering functions all happen to be submodular.",
                    "label": 1
                },
                {
                    "sent": "They also happen to have applications in other problems of interest here, like plant location and sensor placement.",
                    "label": 0
                },
                {
                    "sent": "The application in sensor placement stems from the idea that the information given to you by a set of sensors happens to be a submodular function, and you can use your methods of submodular optimization to get an optimal sensor placement algorithm.",
                    "label": 0
                },
                {
                    "sent": "There are other important use happens to be in algorithmic game theory.",
                    "label": 0
                },
                {
                    "sent": "Do precisely the property I mentioned in the last slide.",
                    "label": 0
                },
                {
                    "sent": "That of diminishing marginal returns.",
                    "label": 0
                },
                {
                    "sent": "This is a property which you interpret to be had by utility functions of agents.",
                    "label": 0
                },
                {
                    "sent": "When you're modeling a game theoretic system, your prices, so naturally you'd expect applications in algorithm game theory and economix because of this property and it has been studied extensively in this context.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our application or our main concern would be with learning submodular functions and in this context the problem was first introduced by Balkan and hardware.",
                    "label": 1
                },
                {
                    "sent": "In 2011 they were motivated by learning and predicting submodular functions when they model pricing and utility functions of agents in game theoretic systems, they wanted to demands of agents and they were also motivated by some applications to advertisements.",
                    "label": 1
                },
                {
                    "sent": "So with all these applications that define this new model of learning called SP Mac.",
                    "label": 0
                },
                {
                    "sent": "Which wanted to, which was intended to capture a learning theoretic view when you are interested in commercial optimization and this model is short for probably mostly approximately correct.",
                    "label": 1
                },
                {
                    "sent": "So let me let me just give you a technical note here that in all the works on learning, and in fact for the rest of the talk, let's just think about only non negative submodular functions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me now tell you what this P Mac model is and what Barker and how we proved in their first paper so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So feedback model the learner gets to see random examples from your target submodular function.",
                    "label": 1
                },
                {
                    "sent": "The unknown submodular function and the job of the learner is to use this random examples to construct a hypothesis edge which multiplicatively approximates your unknown submodular function at all but an epsilon probability mass of points.",
                    "label": 0
                },
                {
                    "sent": "So that's your pee Mak model, and in this model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how we studied the problem for off learning submodular functions on arbitrary distributions, and they showed that the problem in fact is very hard in this setting.",
                    "label": 1
                },
                {
                    "sent": "They gave an algorithm which produces a order routine multiplicative approximation with probability 1 minus epsilon in polynomial time for all submodular functions.",
                    "label": 1
                },
                {
                    "sent": "And they also showed nearly matching lower bound for any polynomial time algorithm that wants to learn submodular functions for the easier case of product distributions, they showed that.",
                    "label": 0
                },
                {
                    "sent": "One can get a log whenever epsilon multiplicative approximation in time with probability 1 minus epsilon for the class of 1 Lipschitz submodular functions with minimum value at least one.",
                    "label": 0
                },
                {
                    "sent": "And so this is the state of art in Barker and Harvey.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gupta at all.",
                    "label": 0
                },
                {
                    "sent": "Look at the problem motivated by applications to privacy and they define this model of learning.",
                    "label": 0
                },
                {
                    "sent": "Consider this model of learning with respect to additive error.",
                    "label": 0
                },
                {
                    "sent": "So in this model the learner gets to make queries to the function so it can ask for the value of the unknown function at any point of its choice and the job of the learner is to produce a hypothesis which additively approximates.",
                    "label": 0
                },
                {
                    "sent": "If so, we would consider this L1 error.",
                    "label": 0
                },
                {
                    "sent": "The average additive error for this model, and this will.",
                    "label": 0
                },
                {
                    "sent": "This will also be interested.",
                    "label": 0
                },
                {
                    "sent": "This will also be the case of interest for our purpose, so this will be the notion of error that we will deal with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous model, what do you observe this stuff?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worries in the in the back and Harvey model in the pmac model, you are supposed to learn the function.",
                    "label": 0
                },
                {
                    "sent": "Just some random examples, random queries, so they're not really query so you get random points and then the value of the unknown function.",
                    "label": 0
                },
                {
                    "sent": "Real value of the function at that point.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the old model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this model they showed that one can learn submodular functions on all product distributions with into the one or epsilon squared time where epsilon is the error.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jackie at all, observed that one can in fact just use random examples to construct a low degree polynomial approximator.",
                    "label": 0
                },
                {
                    "sent": "A degree, one epsilon squared polynomial approximation for submodular functions, and they use this idea to give a algorithm to learn them in the same time.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this algorithm is that they also worked in the agnostic setting owing to the approximation by polynomials, which could be learned agnostically by just polynomial regression.",
                    "label": 1
                },
                {
                    "sent": "So what's agnostic model?",
                    "label": 0
                },
                {
                    "sent": "Let's briefly.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so in this model we actually get an arbitrary function which is not guaranteed to be submodular.",
                    "label": 0
                },
                {
                    "sent": "Have any structure at all, and what we are required to do is produce a hypothesis which does competetively best with respect to the best fitting submodular function.",
                    "label": 0
                },
                {
                    "sent": "So it's supposed to give us Optus Epsilon error, update the error of the best fitting submodular function, right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, uh, in a special case of submodular function, we take values in the discrete range from integers zero to K, rush, nikova and Yaroslav, save in 2013 showed that there exists an algorithm that uses value query access to the function and produces a hypothesis in polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all constant error and polynomials are constant size ranges, they considered this stronger notion of error culture disagreement error, which means, since the range is just great, we just want to bound the number of points where the function does not equal to the unknown functions value.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this context, now we can look at our algorithmic results which would have which would see in a moment.",
                    "label": 1
                },
                {
                    "sent": "So for PAC learning, we show that we can get a polynomial time algorithm in North and two to one square to the one to four for learning discuss.",
                    "label": 0
                },
                {
                    "sent": "This improves upon the best known time bound for the PAC learning from the previous works.",
                    "label": 0
                },
                {
                    "sent": "We also show that we can improve the time bound of agnostic learning submodular functions if we allow queries and this algorithm runs in time polynomial in North and to the website on square.",
                    "label": 1
                },
                {
                    "sent": "So we can show that in the special case when the submodular function takes values in a discrete range, we can improve upon Rush, Nico and yellow socks at work and get an algorithm that runs in Poly and two to the K and one or epsilon, right?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also complemented our results were nearly matching lower bounds and essentially the message of this lower bounds, which I'll go into it in a bit, is that our algorithms impact noise setting on nearly optimal.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so how do we get about this results I guess?",
                    "label": 0
                },
                {
                    "sent": "Of more importance to us is this representation approximation results for submodular functions as a consequence of this, which get our learning and learning results.",
                    "label": 0
                },
                {
                    "sent": "And So what are these results?",
                    "label": 0
                },
                {
                    "sent": "Well, we show that submodular functions are approximated by shallow real value decision trees.",
                    "label": 1
                },
                {
                    "sent": "I'll define these objects in a moment.",
                    "label": 0
                },
                {
                    "sent": "If you can't realize what exactly they are.",
                    "label": 0
                },
                {
                    "sent": "But as a corollary, we can show that submodular functions are efficiently approximated by hunters.",
                    "label": 0
                },
                {
                    "sent": "So what are hunters?",
                    "label": 1
                },
                {
                    "sent": "Well, these are functions that depend only on a few variables.",
                    "label": 0
                },
                {
                    "sent": "So how many variables will be required to approximate the model function well?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The number of variables just depends on your accuracy.",
                    "label": 1
                },
                {
                    "sent": "So whatever dimension you take this a modular function to be in.",
                    "label": 0
                },
                {
                    "sent": "You can just approximate approximate it to any constant error with the function of constantly many variables.",
                    "label": 0
                },
                {
                    "sent": "So we got a proof of this result using about approximation by decision trees.",
                    "label": 0
                },
                {
                    "sent": "We also showed that this implies a simple proof for submodular functions being approximated by law degree polynomials just re proving the result of Karachi at all that we considered earlier.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so since structural results are for prime interest here, let me let me tell you what I'm going to talk about is actual results in the remaining part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So this is the overview of what we will discuss.",
                    "label": 0
                },
                {
                    "sent": "We will first tell you about how we can represent some model functions by low rank decision trees with Lipschitz submodular functions at the leaves.",
                    "label": 1
                },
                {
                    "sent": "Then we will use this representation to construct an approximation of submodular functions using real value decision trees, which means the leaves will have real values at the end, and then we will use this.",
                    "label": 0
                },
                {
                    "sent": "We will use a general result which will prove.",
                    "label": 0
                },
                {
                    "sent": "That low rank binary decision trees can be approximated by low depth decision trees and this will give us our final approximation of submodular functions as low depth decision trees.",
                    "label": 1
                },
                {
                    "sent": "So without further ado, let's let's get into this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at first topic of interest is representing the modular functions using low rank decision trees of Lipschitz.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions so one of these objects, recalling briefly, so we have a decision tree.",
                    "label": 0
                },
                {
                    "sent": "This is a normal decision tree that you all know about, except that the leaves we have Alpha Lipschitz armor in a function for some parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "I will I will come to it.",
                    "label": 0
                },
                {
                    "sent": "I'll come to in a moment.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are our objects where we're discussing this decision?",
                    "label": 0
                },
                {
                    "sent": "Trees which have Alpha Lipschitz functions at the leaves.",
                    "label": 0
                },
                {
                    "sent": "So what our father should function?",
                    "label": 0
                },
                {
                    "sent": "It just shows that the marginal difference.",
                    "label": 0
                },
                {
                    "sent": "On one distance, one neighbors of the value of the function is at most Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's recall briefly terminology for trees that we have.",
                    "label": 0
                },
                {
                    "sent": "We have Lee.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the longest path in the tree would be called as a depth of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will discuss as she asked about low rank decision Tree which now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll define so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a well studied notion.",
                    "label": 0
                },
                {
                    "sent": "The rank of a decision tree can be defined recursively as follows.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's consider a decision tree tea with left and right subtrees as Steven Ti O.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So ranking is defined as zero if your tree itself is just a single node.",
                    "label": 0
                },
                {
                    "sent": "Believe otherwise it has two children.",
                    "label": 0
                },
                {
                    "sent": "If the rank of the two children is different, you just set it as the maximum of the rank of the children.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you just add 1 to the rank of the children.",
                    "label": 0
                },
                {
                    "sent": "So that's your rank, and in 2 two way to understand rank is to think of it as the depth of the largest complete binary tree that you can embed in your given tree.",
                    "label": 1
                },
                {
                    "sent": "Alright, so that's rank of a tree.",
                    "label": 0
                },
                {
                    "sent": "Now that we understand what the objects were dealing with, we can.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State of results.",
                    "label": 0
                },
                {
                    "sent": "So we show that some model functions can be exactly computed by decision trees of rank 2 by Alpha with Alpha Lipschitz Armada functions at each leaf.",
                    "label": 1
                },
                {
                    "sent": "We will see why lift functions are important in a moment, OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this result of ours is based on the decomposition of submodular functions into Lipschitz similar functions, which was used by Gupta at all in their paper also.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you quickly how we can actually construct this decision tree approximation for just monotone submodular case.",
                    "label": 0
                },
                {
                    "sent": "This is the easier case to understand.",
                    "label": 0
                },
                {
                    "sent": "We can extend our proof in order in case in a moment.",
                    "label": 0
                },
                {
                    "sent": "So suppose your function happens to be Alpha lip shades in the beginning itself.",
                    "label": 0
                },
                {
                    "sent": "So at that time you do nothing and just set it as a leaf.",
                    "label": 0
                },
                {
                    "sent": "You're done where other.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, we can argue by submodularity that there must exist a variable, say X3 such that when you add this element 3 to the empty set, the value of the function rises by at least Alpha.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case you make extrion node and you recurse on.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Children.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's your procedure.",
                    "label": 0
                },
                {
                    "sent": "This can go on for as long as it wants.",
                    "label": 0
                },
                {
                    "sent": "What we want to argue is that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The end object that we construct is a low rank decision tree.",
                    "label": 0
                },
                {
                    "sent": "So why is the rank of the decision tree produce low?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the reason is very simple.",
                    "label": 0
                },
                {
                    "sent": "By monotonicity we can argue that the number of left turns in the tree is at most one by Alpha.",
                    "label": 1
                },
                {
                    "sent": "Notice that in each left turn the value of the function must increase by at least Alpha, and we already considering that this range of the function is bounded between zero and one, so that gives us a simple proof that for monotone submodular functions one can approximate them.",
                    "label": 0
                },
                {
                    "sent": "One can actually come exactly, compute them by low rank decision trees of Alpha lifted submodular function.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, So what do we do for non native?",
                    "label": 0
                },
                {
                    "sent": "But we use this observation which is also used by Gupta at all that if S is a modular then so is F of S compliment.",
                    "label": 0
                },
                {
                    "sent": "I would not go into details here but this is this is standard.",
                    "label": 0
                },
                {
                    "sent": "This has been used also before.",
                    "label": 0
                },
                {
                    "sent": "Alright so now that we believe that some other functions can be computed by all physicians order function decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next we go to the next part of the talk which is approximating them by real value decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is the object we have for some computing a similar function for the previous part.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is replace.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This Alpha lecture some order functions.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By real values.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, the key idea here, which was also used in previous work, is that you can replace each Alpha submodular function by its expectation and not incur a lot of error.",
                    "label": 0
                },
                {
                    "sent": "So there are actually very good concentration inequalities for Lipschitz submodular functions starting with the work of Boucheron, Massart, and legacy, for example, this is the inequality we would need that F does not deviate from its expectation the basically the average error.",
                    "label": 1
                },
                {
                    "sent": "Of F from its expectation is at most square root Alpha F is Alpha Lipschitz.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the user has added to approximate each leave by a constant it's expectation and we get a real valued decision tree.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this gives us a Riddle that submodular functions which we are considering to be normalized with zero and one.",
                    "label": 0
                },
                {
                    "sent": "Since we are dealing with editing better are approximated within error epsilon by real value.",
                    "label": 1
                },
                {
                    "sent": "Decision trees of rank at most 4 over epsilon squared.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we go to our third part of a structural results which is approximating low rank binary decision trees by load up decision trees.",
                    "label": 1
                },
                {
                    "sent": "So this part itself is a standard result.",
                    "label": 0
                },
                {
                    "sent": "We will just apply this to our submodular function approximation to get our final corollary.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we show here?",
                    "label": 0
                },
                {
                    "sent": "Will consider a binary decision tree T an.",
                    "label": 1
                },
                {
                    "sent": "We will truncate it to depth D which is order R Plus log over epsilon, where R is the rank of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can show that the disagreement between this T and the truncated tree is at most happens with probability at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "This is a general truncation procedure for any low ranking Gentry and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalizes the truncation based on size, which was used by cash limits and Mansour in the 1993 paper on Decision Tree learning.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can apply this result as a over submodular function approximation to obtain that for every submodular function F there is a real value decision tree of depth at most one over epsilon squared, that L1 approximate search within epsilon.",
                    "label": 0
                },
                {
                    "sent": "So that gives us our final approximation results, modular functions.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, as I said before, also is a quick proof of degree bound of Cherokee roll.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your ex is beautiful distributed on the happy to actually it works for any product distribution also.",
                    "label": 0
                },
                {
                    "sent": "But you could think of it as uniform distribution in a special case.",
                    "label": 0
                },
                {
                    "sent": "Yes alright?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we will quickly discuss applications for learning.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so what's application is back learning.",
                    "label": 0
                },
                {
                    "sent": "We show that there exists an algorithm which, given just random example, access to the function which returns the hypothesis that Albert approximated within epsilon and depends on just two to one square variables.",
                    "label": 1
                },
                {
                    "sent": "This algorithm runs in time till 9 squared to the ones to for an users just log out many examples for any constant error so.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way we obtained this result is basically using our result from the previous part.",
                    "label": 0
                },
                {
                    "sent": "We show that.",
                    "label": 0
                },
                {
                    "sent": "Standard Way implies that there is a low degree polynomial approximator that depends on just a few variables.",
                    "label": 0
                },
                {
                    "sent": "Now in general it is not known how to find the influential variables, even if you know that there are few.",
                    "label": 1
                },
                {
                    "sent": "But for submodular functions we can utilize their properties and show that just looking at degree 1 degree 2 for your coefficient of these functions can give us what the influential variables are.",
                    "label": 0
                },
                {
                    "sent": "Using this procedure we can just do L1 regression in the end to obtain our approximator.",
                    "label": 0
                },
                {
                    "sent": "So that's over.",
                    "label": 0
                },
                {
                    "sent": "That's how we get our pack learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also get agnostic learning algorithm which works with queries in time polynomial in Enter Square and uses the Polygon examples for any constant error.",
                    "label": 1
                },
                {
                    "sent": "So here we actually combine our results in the previous part.",
                    "label": 0
                },
                {
                    "sent": "With the we can combine our reserve apart with the result of the political climate zone learning decision.",
                    "label": 1
                },
                {
                    "sent": "Trees agnostic learning decision trees and but we would lose attribute efficiency.",
                    "label": 1
                },
                {
                    "sent": "That is, we would use more number of examples and required to reduce the number of examples.",
                    "label": 0
                },
                {
                    "sent": "What we can do is use the attribute efficient version of question.",
                    "label": 0
                },
                {
                    "sent": "Answer An agnostic posting of Feldman and Kelly Kennedy.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we talk about nearly matching lower bounds, so whatever lower bounds so we show that back learning even a monotone submodular function requires exponentially more epsilon value queries, and the proof happens.",
                    "label": 1
                },
                {
                    "sent": "The proof goes by embedding any arbitrary Boolean function in a monotone submodular function of a slightly higher dimension.",
                    "label": 0
                },
                {
                    "sent": "So basically you can reduce learning of any Boolean function to submodular function is slightly high dimension, and that's how we get our lower bound here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shows that our back Gnostic algorithm, which queries are optimal up to the exponent in epsilon one epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the computational side, we show that agnostically learning even monotone submodular functions.",
                    "label": 0
                },
                {
                    "sent": "If you can beat enter the small of one or epsilon to three algorithm, then you get a faster algorithm for the problem of learning parities with noise.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is a notoriously hard problem, and the best known algorithm that we know even now is just ended the point 8K.",
                    "label": 0
                },
                {
                    "sent": "So OK, that we believe it's hard.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this proof works by showing that there exist a monotone submodular function which is highly correlated with the parity.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this shows that the agnostic algorithm attraction at all is optimal for the exponent on epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the summary so far.",
                    "label": 0
                },
                {
                    "sent": "We can approximate submodular function by load of decision trees and as a consequence we can do almost optimal packing agnostic learning algorithms for the class.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some follow up work, let me just briefly tell you want to follow work has happened after this.",
                    "label": 0
                },
                {
                    "sent": "So Feldman Vondrak actually have improved the hotel approximation that we talked about.",
                    "label": 0
                },
                {
                    "sent": "In this talk we can show that we can actually approximates a model function by functions of justice polynomial in one over epsilon variables and they also gave a faster algorithm for learning this class and the P Mac.",
                    "label": 0
                },
                {
                    "sent": "The challenging pimc model of Balkan Harvey in order to follow up work within joint work with Vitaly Feldman, we show that one can actually give a fully polynomial time algorithm that is.",
                    "label": 1
                },
                {
                    "sent": "N1 over epsilon 4 Pack and Mac learning.",
                    "label": 1
                },
                {
                    "sent": "A special case of submodular functions.",
                    "label": 0
                },
                {
                    "sent": "The coverage functions and.",
                    "label": 0
                },
                {
                    "sent": "These are also well studied as valuation functions in game theory, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a follow up work.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The open question that remains is what can you do about more general distributions.",
                    "label": 1
                },
                {
                    "sent": "We know that on arbitrary distributions the lower bound of bulk and Harvey also holds even in the winter setting in.",
                    "label": 0
                },
                {
                    "sent": "But for some transformation, on the other hand, we know that product distributions are certainly easy.",
                    "label": 0
                },
                {
                    "sent": "So what can we do in between?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}