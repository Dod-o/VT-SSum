{
    "id": "yqtnekeiopkro7z7dhhxopp5ed4hwauw",
    "title": "Turning Down the Noise in the Blogosphere",
    "info": {
        "author": [
            "Gaurav Veda, Carnegie Mellon University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_veda_tdnb/",
    "segmentation": [
        [
            "OK, so I'm going to be there and today I'm going to talk about turning down the noise in the blogosphere.",
            "This is joint work with Collarini, Dafna, Shahaf and Carlos question."
        ],
        [
            "So today we face the problem of information overload in the blogosphere.",
            "There are millions of posts that get published every day and clearly most of these stories are talking about extremely important issues.",
            "For example, on September 9th, 2008, the most important issue being talked about was lipstick on a pig.",
            "And while the Blogworld was busy discussing this extremely important issue, there were some other minor things going on on this day that got little coverage.",
            "For example, the collapse of the US financial system.",
            "So our goal is to."
        ],
        [
            "Turn down the noise by selecting a small representative set of blogposts that cover the most important stories out there.",
            "For example, this is what a typical day looks like in the blogosphere.",
            "Here the size of a word is proportional to its frequency.",
            "So as you can see, some of the important features on this day were New York, Israel, Gaza, and Barack Obama."
        ],
        [
            "So what we would ideally want is to get at least one story for each of these issues, one about the Israel Gaza conflict, the Hudson River plane landing in New York and about Barack Obama's inauguration.",
            "So.",
            "Once you get these important when we get these prevalent stories, you might say that you know this is good.",
            "These are prevalent, but I don't care about these stories in general.",
            "What is important for you is different from what is important for me."
        ],
        [
            "So what we want to do is we want to tailor post selection to user base.",
            "For example, this workload represents the post selected for a particular day without personalization.",
            "As you can see it covers the important stories in the game.",
            "However, someone like Sedan might say, you know I'm interested in sports, so I want to see articles about soccer or basketball etc.",
            "So what we want to do is once we get this feedback from the van, we want to show him stories that he's interested in.",
            "So things about Manchester City football team etc."
        ],
        [
            "So to achieve these goals, we first formalize the notion of covering the blogosphere, and we then present a near optimal post selection algorithm.",
            "To personalize both selection to user interests, we learn a coverage function that is personalized and our algorithm achieves no regret.",
            "We further evaluate on real block data by conducting user studies and comparing against the most popular blog aggregation sites."
        ],
        [
            "So now I focus on governing the blogosphere and selecting posts.",
            "So the blogosphere is an extremely unstructured source of information.",
            "In order to formalize this problem, we have to impart some structure to it.",
            "And we do this by first extracting some features from each post.",
            "And then formulating a coverage objective function.",
            "And we have this function.",
            "We optimize it and select the set of words to be presented to the user and now talk about each of these."
        ],
        [
            "Tips?",
            "So we model a document as a collection of features.",
            "The features could be low level, such as noun phrases and named entities like Barack Obama, China, etc.",
            "Or they could be high level suggest topics from a topic model.",
            "Where a topic is just a probability distribution over words."
        ],
        [
            "Good, so now we have a set of features and a bunch of posts, and each post covers some features in order to formalize coverage, we define a cover function cover DF, which represents the amount by which a document D covers a post F. For example, the amount by which the purple Post covers the feature.",
            "Barack Obama.",
            "And since we are interested in selecting a set, of course Analagous Lee.",
            "We define the cover function cover AF for a set of both a as the amount by which the set covers the feature.",
            "Obama.",
            "So given this notion of coverage, a simple, perhaps naive way of."
        ],
        [
            "Selecting posts is by formulating post selection as a Max cover problem.",
            "So here we are interested in selecting a set of capos that cover the most features.",
            "However, the simple approach has two main problems.",
            "The first problem is that it treats all the features occurring in a document as being equal, for example.",
            "This post, which is about Barack Obama speech on economy but just happens to mention the venue Fairfax.",
            "Would cover the features, Obama, the economy and Fairfax equally well.",
            "The second problem with this approach is that it treats the features all the features to have an equal importance across the entire corpus as well.",
            "So here Obama in Fairfax would have the same importance across corpus and selecting a single post that contains them would cover the feature completely.",
            "So now I talk about each of these."
        ],
        [
            "The visual problems in more detail.",
            "So consider this post which is about the two dates of a particular rock band.",
            "Although it contains the word Washington DC, this article is not really about Washington, So what we would ideally want is that the coverage of Washington by this article should be really small.",
            "And we solve this problem by defining a probabilistic notion of coverage.",
            "So color DF is now the probability of the feature F given the post D. And more concretely, if we use topics as features, then cover DF is the probability that the post D is about topic F."
        ],
        [
            "Moving on to the second problem, some features are more important than others.",
            "For example, the feature Barack Obama is at least right now more important than my advisor Carlos Guestrin, and what we want is we want a higher utility associated with covering the more important features.",
            "So we solve this problem in two ways.",
            "The first is by associating a weight WF with each feature F, which could be, for example, just the frequency of the feature in our corpus.",
            "And the 2nd way in which we address this problem is by covering an important feature using multiple posts.",
            "I'm not going to talk about the second approach."
        ],
        [
            "And although we want to cover an important feature with multiple both at the same time, we do not want all our posts to be about the same feature.",
            "And we solve this problem by defining the cover function for a set A.",
            "As the probability that at least one post in the city.",
            "Covers the feature.",
            "As an example, consider these two posts.",
            "Let's say the 1st post.",
            "Covers the future Barack Obama with the probability of .5 and the 2nd post.",
            "Covers Obama with the probability of .4.",
            "Then the probability that the set of these two posts covers the feature.",
            "Obama is equal to the probability at least one post covers him and this is equal to .7.",
            "Assuming independence of posts.",
            "So notice that this probability is greater than the probability that the single post covers Obama and hence there is some gain on covering the feature.",
            "Obama with multiple posts and at the same time this is less than the sum of the individual coverage probabilities by the post.",
            "So it means we have this property of diminishing returns and this is what ensures that all the posts in R selected said are not about the same feature."
        ],
        [
            "So now that we have addressed these problems with Max cover.",
            "We are now ready to formulate the objective function so our objective function for a feature consists of the weight of the feature multiplied by the coverage of the feature by the set, of course, and objective function as a sum over all the features.",
            "Now, since the cover function has this property of diminishing returns.",
            "Our objective function itself exhibits this property, and such a function is called a submodular function.",
            "Exact maximization of a submodular function is hard, however, using a simple greedy algorithm we can achieve a 63% approximation does.",
            "We can obtain a near optimal solution to optimize our objective function."
        ],
        [
            "Just to recap, we started out by extracting some features from a blog post and formulating an objective function.",
            "Since the objective function was submodular, we were able to optimize it by using a simple greedy approach and selecting a set of course."
        ],
        [
            "So let's look at how we do so.",
            "We evaluated our approach on real block data collected over a two week.",
            "In January.",
            "After preprocessing we still ended up with about 200,000 posts per day.",
            "We took two variants of our algorithm.",
            "The first one, which we call TD and plus LDA, use high level features, in particular topics, from the latent Dirichlet allocation topic model and the second one TDM.",
            "Does any use low level features?",
            "Noun phrases are named entities.",
            "And we evaluated by doing a user study involving 27 subjects.",
            "Our metrics were topicality and redundancy."
        ],
        [
            "So our first metric is topicality, that is, how relevant is a particular story to the current events of the day and to measure this, we conducted a user study in which we.",
            "Downloaded the top stories across several categories like Sports, Entertainment, politics.",
            "I gave you this access to this reference set once users had access to this reference set, we showed them a set of posts selected using either our algorithm or one of our comparison algorithms, and we ask them, is this post topical?",
            "That is, is it related to any of the major stories in the day?"
        ],
        [
            "On this metric they can lock, will perform poorly.",
            "Our approach is shown here in Orange and our approach which uses high level features, does as well as Google and Yahoo Buzz."
        ],
        [
            "So.",
            "We want a diverse set of post to present to the users and we measure this by doing another user study.",
            "In this user study we showed users a set of posts in order and we ask them to mark a post as redundant if it was similar in content to any of the previous posts in the set."
        ],
        [
            "On this metric, both our approaches perform really well and we do as well as Yahoo.",
            "Google on the other hand, performs poorly, and on average there are about two posts which are redundant out of 10 posts."
        ],
        [
            "So to summarize, the results for coverage, Google has good topicality, but high redundancy.",
            "Yahoo performs well on both our metrics, but it uses rich features such as the clickthrough rate search trends, user voting, etc.",
            "We, on the other hand, using only text based features do as well as Yahoo, and better than all the others."
        ],
        [
            "So this is great based on what we've seen so far.",
            "We can select a set of posts which are about the important stories of the day.",
            "But as I said, you might not be interested in these important."
        ],
        [
            "Stories, for example, one of my Co authors colored might be interested in politics and reading about Barack Obama, whereas I might be interested in pop culture and the affairs of singer Britney Spears.",
            "So our goal now is to learn a personalized coverage function."
        ],
        [
            "Using limited user feedback.",
            "And in our approach.",
            "Once we have a set of posts, we will ask the user to give us feedback on some of these posts.",
            "That is to indicate whether they would like to read a post or not and they only have to do it for some posts and then once we get this feedback, we incorporate into our coverage function to obtain a personalized coverage function which we then optimize to generate a personalized set of posts data to the user's interests."
        ],
        [
            "We model user preferences by using a personalization weight vector Pi, where Pi F represents the user's preference for feature F. For example, a user who might be interested in politics would have a personalization weight vector in which the features Obama and the Senate have a high weight, whereas the other features like Britney Spears and sedan.",
            "Hello wait, a sports fan on the other hand, might have a high weight for sedan and federal and a low weight for the other features.",
            "So our goal is to learn this personalization weight vector."
        ],
        [
            "And we do this in the following way.",
            "When we don't receive any feedback, we assume the user likes all these features equally well.",
            "We select a set of posts and present them to the user.",
            "Now the user gives us some feedback and we then use a multiplicative weights update rule to update our personalization weight vector.",
            "So, for example, if the user said that they are interested in reading a post about Obama and bin Laden, then the weights on those features goes up.",
            "And since we normalize our weight vector, the weights on features other features like China and Britney Spears.",
            "Goes down a little and we then generate the next set.",
            "Of course by using this new personalization weight vector.",
            "And we continue in this fashion."
        ],
        [
            "OK so I just showed you one way of learning these personalization weight vectors an depending on the feedback given by us given by the user to us and the weight vector that we used to generate the set of posts we receive some reward at each step.",
            "The question is, what do we compare against?",
            "How do we know that we are doing well?",
            "One thing that you can do is assume that you already know the entire set of posts and the feedback that the user gives on them, and then you can learn.",
            "You can compute the best personalization weight vector given all this information.",
            "So we will use the same personalization weight vector for each step, and again depending on the feedback and the weight vector will receive some reward.",
            "Now we prove that for our algorithm, the average difference between the optimal fixed weight vector and the average reward obtained by using our learning strategy goes to zero, and this is a desirable property for any online learning algorithm, and it is referred to as no regret in the online learning literature.",
            "So using a simple multiplicative weights update rule we achieve no regret."
        ],
        [
            "So, just to recap our approach, we started out by extracting some features and formulating an objective function.",
            "We then optimize this to select a set of posts on which the user gave us some feedback.",
            "We incorporated this feedback using online learning to update the the coverage function and learn and get a personalized coverage function and we then optimize this to select a personalized set of posts and this loop continues."
        ],
        [
            "So now let's come to the evaluation.",
            "We started out by trying to simulate a sports fan.",
            "So every day for about 15 days we said that sedan likes all the posts from Fanhouse, a particular sports blog, and we looked at the personalization ratio, that is the ratio of the personalized objective to the unpersonalized objective.",
            "If this value is greater than one for a particular post, then it means that we have been able to bias our objective function to favor such a post.",
            "And on this metric we see that as we increase the number of days of personalization.",
            "Our objective functions preference for a post from Fanhouse keeps on increasing and not just that.",
            "Our preference for selecting a post from Deadspin, another sports blog also keeps on increasing an if we look at Huffington Post, which is the popular politics blog, then our preference for selecting a post from that blog keeps on going down.",
            "So that means we have been able to gravitate our objective function to prefer sports.",
            "So this is great.",
            "We were able to bias or objective function in the way we wanted.",
            "But what we're really interested in is selecting posts."
        ],
        [
            "Now I come to the evaluation.",
            "For that we need some personalization for India.",
            "So we said we like any post that is about India in the selected set and we dislike everything else and notice that this corpus was from January and there weren't a lot of posts about India during this time.",
            "However, still after five epochs, the top story that we get is about the Mumbai terrorist attack.",
            "Anile and if we keep on continuing to personalize in this way, then after 15 epochs that four out of the top 10 stories are about India and now we start getting even very low key stories.",
            "So we were able to personalize post selection.",
            "To give us pose that we like."
        ],
        [
            "As a final Test, we did a user study.",
            "In this we generated a set of posts and ask users to give us feedback.",
            "We then generated the next set by incorporating this feedback using our algorithm and so on.",
            "We did this for a number of days and we compared using an unpersonalized post selection strategy in which we still ask the users to give us feedback.",
            "That is to tell us whether they like the post or not, but the post selected for the next day.",
            "We're not based on the feedback.",
            "They were just based on the simple coverage function defined earlier."
        ],
        [
            "And it turns out that on an average, users prefer personalized posts more than unpersonalized pose does.",
            "We conclude that we were able to successfully Taylor Post selection to users interests."
        ],
        [
            "And to summarize, we first formalize this notion of covering the blogosphere, and I presented a near optimal algorithm for selecting posts.",
            "To tailor post selection to users interest, I showed how we can learn a personalized coverage function and our algorithm in particular, achieves no regret.",
            "We evaluated on real block data.",
            "And our approach for coverage using only post content does as well as or better than.",
            "All the other comparison techniques that even use richer features.",
            "And Furthermore, using simulations an user study, we established that we were able to successfully Taylor Post selection to user preferences.",
            "We are in the process of launching a website that uses our algorithm to select blog posts in real time and right now we have a demo available at www.turndownthenoise.com.",
            "I encourage you to visit our site and we should have a full functional website by the end of summer.",
            "Thank you."
        ],
        [
            "Memory.",
            "Hi I'm just wondering if you have any results of hard work and then yourselves to use something like Claritin filtering which doesn't use any content at all.",
            "Just the thumbs up, thumbs down feedback so some of these approaches, for example the Yahoo buzz.",
            "It is based on user voting and hence it is based on collaborative filtering because if a lot of users voted to re up then it goes up and that's what shows.",
            "So we are comparing against those techniques.",
            "We weren't able to use collaborating filtering for our approach because we just didn't have.",
            "Enough information and enough user base.",
            "I was noticing that in the last slide you showed that the gap between the personalized data in the random forever, the standard with the gap is closing as they could be just noise, but but do you think that there's some of the fact that you're you're overtraining orbiting Earth?",
            "Two wells had as you use up then you're not necessarily able to guess what the person if you're filtering out things that might be interesting.",
            "But just didn't hurt the presents.",
            "So OK."
        ],
        [
            "First, let me say that.",
            "I mean so based on this, on an average you can see that we prefer personalized course and your question is like as time goes on, it seems the gap is reducing so.",
            "Part of the reason could also be that because of the way we did our user study in which basically this is a psychological thing that people expect things to get better, and so that is also part of the reason why you see the Unpersonalized line going up, whereas the personalizes still right up there.",
            "And I guess we would once we have a fully functional website then we will be able to do more extensive experimentation and see like what's the effect over longer time periods.",
            "But also it depends on the learning rate.",
            "So right now we just use some learning rate.",
            "But if we do indeed see that this effect happens that we are able to personalize, like we're personalizing very aggressively, then we can potentially like tune the learning rate to take care of that problem.",
            "You had very impressively low redundancy rates.",
            "Aspects of your technique where forces in Geneva for loaded and see.",
            "So if you remember I spoke about this diminishing returns property right?",
            "So that property.",
            "So we define our coverage function in a probabilistic fashion and it has diminishing returns property.",
            "So what that means is that if you've already covered a feature, covering it again gives you lower utility.",
            "So that's why, like initially, you would want to cover.",
            "The different different features, which are all very important, and then once you've done that, only then would you try to cover the features again.",
            "So this is what helps us in achieving diversity and low redundancy.",
            "Copying the center features in the beginning that allow you to focus your attention on those features.",
            "For example, then you might have difficulty multiple ways of characterizing or potentially the same story.",
            "Different uses of language, for example, right?",
            "So if we have like this like so, we do not have coreference resolution and things like that right now.",
            "So it is true that we will have a problem if there are like the same feature which are like two different features which are talking about this.",
            "Exactly the same thing and have a high weight.",
            "Then we would indeed select two stories, one story for each one of them.",
            "But that depends on the features.",
            "An algorithm or approach works for any set of features.",
            "So if you choose features using Coreference resolution, that problem will not occur, for example.",
            "My special guest.",
            "Could you tell us a little more about the users ID selected for evaluating the results in the sense that other users, experts of the topics that the blogs are covering?",
            "Or were they like, you know, oh, I want to listen about.",
            "I want to read about this topic today, but another topic tomorrow.",
            "And if there is a dependence of the results of those so users were not experts on any subject.",
            "So for example, for the coverage experiment we went to.",
            "Various websites in different categories.",
            "So for politics we went to CNN, Washington Post, Reuters and so on.",
            "And we downloaded the top stories and we give users access to this set.",
            "So and then we ask him to compare whether one of the stories that we show is related to any of these stories.",
            "So that removed that that that was able to like remove any bias that the users had because they already had access to this reference collection and we just had to compare with that.",
            "And for the personalization user study, we just ask them to just be natural and just say whether they like something or dislike something so.",
            "Start answer your question.",
            "A more controlled experiment, probably for testing the personalization, would be taken expert right?",
            "And he keeps every day, he tells you.",
            "Yes, I like politics.",
            "I like politics like politics, right?",
            "And your personalization is indeed able to capture that.",
            "Then you should the code that you showed.",
            "Then for each topic.",
            "And if you use an expert as the user then you're.",
            "Definitely I love those that you just right.",
            "So what you're saying is then this takes care of that.",
            "So here we tried to simulate a sports fan and we said we like all articles about from this particular sports blogs and we were able to show that the preference for an article from this blog or some other sports block keeps on increasing, whereas the preference for some other blogs like politics blog keeps on decreasing.",
            "So this sort of takes care of the expert situation that you were talking about, I guess.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to be there and today I'm going to talk about turning down the noise in the blogosphere.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Collarini, Dafna, Shahaf and Carlos question.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So today we face the problem of information overload in the blogosphere.",
                    "label": 0
                },
                {
                    "sent": "There are millions of posts that get published every day and clearly most of these stories are talking about extremely important issues.",
                    "label": 1
                },
                {
                    "sent": "For example, on September 9th, 2008, the most important issue being talked about was lipstick on a pig.",
                    "label": 0
                },
                {
                    "sent": "And while the Blogworld was busy discussing this extremely important issue, there were some other minor things going on on this day that got little coverage.",
                    "label": 0
                },
                {
                    "sent": "For example, the collapse of the US financial system.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turn down the noise by selecting a small representative set of blogposts that cover the most important stories out there.",
                    "label": 1
                },
                {
                    "sent": "For example, this is what a typical day looks like in the blogosphere.",
                    "label": 0
                },
                {
                    "sent": "Here the size of a word is proportional to its frequency.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, some of the important features on this day were New York, Israel, Gaza, and Barack Obama.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we would ideally want is to get at least one story for each of these issues, one about the Israel Gaza conflict, the Hudson River plane landing in New York and about Barack Obama's inauguration.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Once you get these important when we get these prevalent stories, you might say that you know this is good.",
                    "label": 0
                },
                {
                    "sent": "These are prevalent, but I don't care about these stories in general.",
                    "label": 0
                },
                {
                    "sent": "What is important for you is different from what is important for me.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do is we want to tailor post selection to user base.",
                    "label": 1
                },
                {
                    "sent": "For example, this workload represents the post selected for a particular day without personalization.",
                    "label": 0
                },
                {
                    "sent": "As you can see it covers the important stories in the game.",
                    "label": 0
                },
                {
                    "sent": "However, someone like Sedan might say, you know I'm interested in sports, so I want to see articles about soccer or basketball etc.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is once we get this feedback from the van, we want to show him stories that he's interested in.",
                    "label": 0
                },
                {
                    "sent": "So things about Manchester City football team etc.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to achieve these goals, we first formalize the notion of covering the blogosphere, and we then present a near optimal post selection algorithm.",
                    "label": 1
                },
                {
                    "sent": "To personalize both selection to user interests, we learn a coverage function that is personalized and our algorithm achieves no regret.",
                    "label": 1
                },
                {
                    "sent": "We further evaluate on real block data by conducting user studies and comparing against the most popular blog aggregation sites.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I focus on governing the blogosphere and selecting posts.",
                    "label": 0
                },
                {
                    "sent": "So the blogosphere is an extremely unstructured source of information.",
                    "label": 0
                },
                {
                    "sent": "In order to formalize this problem, we have to impart some structure to it.",
                    "label": 0
                },
                {
                    "sent": "And we do this by first extracting some features from each post.",
                    "label": 0
                },
                {
                    "sent": "And then formulating a coverage objective function.",
                    "label": 0
                },
                {
                    "sent": "And we have this function.",
                    "label": 0
                },
                {
                    "sent": "We optimize it and select the set of words to be presented to the user and now talk about each of these.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tips?",
                    "label": 0
                },
                {
                    "sent": "So we model a document as a collection of features.",
                    "label": 0
                },
                {
                    "sent": "The features could be low level, such as noun phrases and named entities like Barack Obama, China, etc.",
                    "label": 1
                },
                {
                    "sent": "Or they could be high level suggest topics from a topic model.",
                    "label": 1
                },
                {
                    "sent": "Where a topic is just a probability distribution over words.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good, so now we have a set of features and a bunch of posts, and each post covers some features in order to formalize coverage, we define a cover function cover DF, which represents the amount by which a document D covers a post F. For example, the amount by which the purple Post covers the feature.",
                    "label": 1
                },
                {
                    "sent": "Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "And since we are interested in selecting a set, of course Analagous Lee.",
                    "label": 0
                },
                {
                    "sent": "We define the cover function cover AF for a set of both a as the amount by which the set covers the feature.",
                    "label": 0
                },
                {
                    "sent": "Obama.",
                    "label": 0
                },
                {
                    "sent": "So given this notion of coverage, a simple, perhaps naive way of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Selecting posts is by formulating post selection as a Max cover problem.",
                    "label": 0
                },
                {
                    "sent": "So here we are interested in selecting a set of capos that cover the most features.",
                    "label": 1
                },
                {
                    "sent": "However, the simple approach has two main problems.",
                    "label": 0
                },
                {
                    "sent": "The first problem is that it treats all the features occurring in a document as being equal, for example.",
                    "label": 0
                },
                {
                    "sent": "This post, which is about Barack Obama speech on economy but just happens to mention the venue Fairfax.",
                    "label": 0
                },
                {
                    "sent": "Would cover the features, Obama, the economy and Fairfax equally well.",
                    "label": 0
                },
                {
                    "sent": "The second problem with this approach is that it treats the features all the features to have an equal importance across the entire corpus as well.",
                    "label": 0
                },
                {
                    "sent": "So here Obama in Fairfax would have the same importance across corpus and selecting a single post that contains them would cover the feature completely.",
                    "label": 0
                },
                {
                    "sent": "So now I talk about each of these.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The visual problems in more detail.",
                    "label": 0
                },
                {
                    "sent": "So consider this post which is about the two dates of a particular rock band.",
                    "label": 0
                },
                {
                    "sent": "Although it contains the word Washington DC, this article is not really about Washington, So what we would ideally want is that the coverage of Washington by this article should be really small.",
                    "label": 0
                },
                {
                    "sent": "And we solve this problem by defining a probabilistic notion of coverage.",
                    "label": 0
                },
                {
                    "sent": "So color DF is now the probability of the feature F given the post D. And more concretely, if we use topics as features, then cover DF is the probability that the post D is about topic F.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving on to the second problem, some features are more important than others.",
                    "label": 0
                },
                {
                    "sent": "For example, the feature Barack Obama is at least right now more important than my advisor Carlos Guestrin, and what we want is we want a higher utility associated with covering the more important features.",
                    "label": 0
                },
                {
                    "sent": "So we solve this problem in two ways.",
                    "label": 0
                },
                {
                    "sent": "The first is by associating a weight WF with each feature F, which could be, for example, just the frequency of the feature in our corpus.",
                    "label": 1
                },
                {
                    "sent": "And the 2nd way in which we address this problem is by covering an important feature using multiple posts.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about the second approach.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And although we want to cover an important feature with multiple both at the same time, we do not want all our posts to be about the same feature.",
                    "label": 0
                },
                {
                    "sent": "And we solve this problem by defining the cover function for a set A.",
                    "label": 1
                },
                {
                    "sent": "As the probability that at least one post in the city.",
                    "label": 1
                },
                {
                    "sent": "Covers the feature.",
                    "label": 0
                },
                {
                    "sent": "As an example, consider these two posts.",
                    "label": 0
                },
                {
                    "sent": "Let's say the 1st post.",
                    "label": 0
                },
                {
                    "sent": "Covers the future Barack Obama with the probability of .5 and the 2nd post.",
                    "label": 0
                },
                {
                    "sent": "Covers Obama with the probability of .4.",
                    "label": 0
                },
                {
                    "sent": "Then the probability that the set of these two posts covers the feature.",
                    "label": 0
                },
                {
                    "sent": "Obama is equal to the probability at least one post covers him and this is equal to .7.",
                    "label": 0
                },
                {
                    "sent": "Assuming independence of posts.",
                    "label": 0
                },
                {
                    "sent": "So notice that this probability is greater than the probability that the single post covers Obama and hence there is some gain on covering the feature.",
                    "label": 0
                },
                {
                    "sent": "Obama with multiple posts and at the same time this is less than the sum of the individual coverage probabilities by the post.",
                    "label": 0
                },
                {
                    "sent": "So it means we have this property of diminishing returns and this is what ensures that all the posts in R selected said are not about the same feature.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now that we have addressed these problems with Max cover.",
                    "label": 0
                },
                {
                    "sent": "We are now ready to formulate the objective function so our objective function for a feature consists of the weight of the feature multiplied by the coverage of the feature by the set, of course, and objective function as a sum over all the features.",
                    "label": 0
                },
                {
                    "sent": "Now, since the cover function has this property of diminishing returns.",
                    "label": 0
                },
                {
                    "sent": "Our objective function itself exhibits this property, and such a function is called a submodular function.",
                    "label": 0
                },
                {
                    "sent": "Exact maximization of a submodular function is hard, however, using a simple greedy algorithm we can achieve a 63% approximation does.",
                    "label": 1
                },
                {
                    "sent": "We can obtain a near optimal solution to optimize our objective function.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to recap, we started out by extracting some features from a blog post and formulating an objective function.",
                    "label": 0
                },
                {
                    "sent": "Since the objective function was submodular, we were able to optimize it by using a simple greedy approach and selecting a set of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at how we do so.",
                    "label": 0
                },
                {
                    "sent": "We evaluated our approach on real block data collected over a two week.",
                    "label": 0
                },
                {
                    "sent": "In January.",
                    "label": 0
                },
                {
                    "sent": "After preprocessing we still ended up with about 200,000 posts per day.",
                    "label": 1
                },
                {
                    "sent": "We took two variants of our algorithm.",
                    "label": 1
                },
                {
                    "sent": "The first one, which we call TD and plus LDA, use high level features, in particular topics, from the latent Dirichlet allocation topic model and the second one TDM.",
                    "label": 0
                },
                {
                    "sent": "Does any use low level features?",
                    "label": 0
                },
                {
                    "sent": "Noun phrases are named entities.",
                    "label": 1
                },
                {
                    "sent": "And we evaluated by doing a user study involving 27 subjects.",
                    "label": 0
                },
                {
                    "sent": "Our metrics were topicality and redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our first metric is topicality, that is, how relevant is a particular story to the current events of the day and to measure this, we conducted a user study in which we.",
                    "label": 0
                },
                {
                    "sent": "Downloaded the top stories across several categories like Sports, Entertainment, politics.",
                    "label": 0
                },
                {
                    "sent": "I gave you this access to this reference set once users had access to this reference set, we showed them a set of posts selected using either our algorithm or one of our comparison algorithms, and we ask them, is this post topical?",
                    "label": 0
                },
                {
                    "sent": "That is, is it related to any of the major stories in the day?",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this metric they can lock, will perform poorly.",
                    "label": 0
                },
                {
                    "sent": "Our approach is shown here in Orange and our approach which uses high level features, does as well as Google and Yahoo Buzz.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want a diverse set of post to present to the users and we measure this by doing another user study.",
                    "label": 0
                },
                {
                    "sent": "In this user study we showed users a set of posts in order and we ask them to mark a post as redundant if it was similar in content to any of the previous posts in the set.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this metric, both our approaches perform really well and we do as well as Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Google on the other hand, performs poorly, and on average there are about two posts which are redundant out of 10 posts.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, the results for coverage, Google has good topicality, but high redundancy.",
                    "label": 0
                },
                {
                    "sent": "Yahoo performs well on both our metrics, but it uses rich features such as the clickthrough rate search trends, user voting, etc.",
                    "label": 1
                },
                {
                    "sent": "We, on the other hand, using only text based features do as well as Yahoo, and better than all the others.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is great based on what we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "We can select a set of posts which are about the important stories of the day.",
                    "label": 0
                },
                {
                    "sent": "But as I said, you might not be interested in these important.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stories, for example, one of my Co authors colored might be interested in politics and reading about Barack Obama, whereas I might be interested in pop culture and the affairs of singer Britney Spears.",
                    "label": 0
                },
                {
                    "sent": "So our goal now is to learn a personalized coverage function.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using limited user feedback.",
                    "label": 0
                },
                {
                    "sent": "And in our approach.",
                    "label": 0
                },
                {
                    "sent": "Once we have a set of posts, we will ask the user to give us feedback on some of these posts.",
                    "label": 0
                },
                {
                    "sent": "That is to indicate whether they would like to read a post or not and they only have to do it for some posts and then once we get this feedback, we incorporate into our coverage function to obtain a personalized coverage function which we then optimize to generate a personalized set of posts data to the user's interests.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We model user preferences by using a personalization weight vector Pi, where Pi F represents the user's preference for feature F. For example, a user who might be interested in politics would have a personalization weight vector in which the features Obama and the Senate have a high weight, whereas the other features like Britney Spears and sedan.",
                    "label": 1
                },
                {
                    "sent": "Hello wait, a sports fan on the other hand, might have a high weight for sedan and federal and a low weight for the other features.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to learn this personalization weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do this in the following way.",
                    "label": 0
                },
                {
                    "sent": "When we don't receive any feedback, we assume the user likes all these features equally well.",
                    "label": 0
                },
                {
                    "sent": "We select a set of posts and present them to the user.",
                    "label": 0
                },
                {
                    "sent": "Now the user gives us some feedback and we then use a multiplicative weights update rule to update our personalization weight vector.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if the user said that they are interested in reading a post about Obama and bin Laden, then the weights on those features goes up.",
                    "label": 0
                },
                {
                    "sent": "And since we normalize our weight vector, the weights on features other features like China and Britney Spears.",
                    "label": 0
                },
                {
                    "sent": "Goes down a little and we then generate the next set.",
                    "label": 0
                },
                {
                    "sent": "Of course by using this new personalization weight vector.",
                    "label": 0
                },
                {
                    "sent": "And we continue in this fashion.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I just showed you one way of learning these personalization weight vectors an depending on the feedback given by us given by the user to us and the weight vector that we used to generate the set of posts we receive some reward at each step.",
                    "label": 0
                },
                {
                    "sent": "The question is, what do we compare against?",
                    "label": 0
                },
                {
                    "sent": "How do we know that we are doing well?",
                    "label": 0
                },
                {
                    "sent": "One thing that you can do is assume that you already know the entire set of posts and the feedback that the user gives on them, and then you can learn.",
                    "label": 1
                },
                {
                    "sent": "You can compute the best personalization weight vector given all this information.",
                    "label": 0
                },
                {
                    "sent": "So we will use the same personalization weight vector for each step, and again depending on the feedback and the weight vector will receive some reward.",
                    "label": 0
                },
                {
                    "sent": "Now we prove that for our algorithm, the average difference between the optimal fixed weight vector and the average reward obtained by using our learning strategy goes to zero, and this is a desirable property for any online learning algorithm, and it is referred to as no regret in the online learning literature.",
                    "label": 1
                },
                {
                    "sent": "So using a simple multiplicative weights update rule we achieve no regret.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to recap our approach, we started out by extracting some features and formulating an objective function.",
                    "label": 0
                },
                {
                    "sent": "We then optimize this to select a set of posts on which the user gave us some feedback.",
                    "label": 0
                },
                {
                    "sent": "We incorporated this feedback using online learning to update the the coverage function and learn and get a personalized coverage function and we then optimize this to select a personalized set of posts and this loop continues.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's come to the evaluation.",
                    "label": 0
                },
                {
                    "sent": "We started out by trying to simulate a sports fan.",
                    "label": 1
                },
                {
                    "sent": "So every day for about 15 days we said that sedan likes all the posts from Fanhouse, a particular sports blog, and we looked at the personalization ratio, that is the ratio of the personalized objective to the unpersonalized objective.",
                    "label": 1
                },
                {
                    "sent": "If this value is greater than one for a particular post, then it means that we have been able to bias our objective function to favor such a post.",
                    "label": 0
                },
                {
                    "sent": "And on this metric we see that as we increase the number of days of personalization.",
                    "label": 0
                },
                {
                    "sent": "Our objective functions preference for a post from Fanhouse keeps on increasing and not just that.",
                    "label": 0
                },
                {
                    "sent": "Our preference for selecting a post from Deadspin, another sports blog also keeps on increasing an if we look at Huffington Post, which is the popular politics blog, then our preference for selecting a post from that blog keeps on going down.",
                    "label": 0
                },
                {
                    "sent": "So that means we have been able to gravitate our objective function to prefer sports.",
                    "label": 0
                },
                {
                    "sent": "So this is great.",
                    "label": 0
                },
                {
                    "sent": "We were able to bias or objective function in the way we wanted.",
                    "label": 0
                },
                {
                    "sent": "But what we're really interested in is selecting posts.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I come to the evaluation.",
                    "label": 0
                },
                {
                    "sent": "For that we need some personalization for India.",
                    "label": 0
                },
                {
                    "sent": "So we said we like any post that is about India in the selected set and we dislike everything else and notice that this corpus was from January and there weren't a lot of posts about India during this time.",
                    "label": 1
                },
                {
                    "sent": "However, still after five epochs, the top story that we get is about the Mumbai terrorist attack.",
                    "label": 1
                },
                {
                    "sent": "Anile and if we keep on continuing to personalize in this way, then after 15 epochs that four out of the top 10 stories are about India and now we start getting even very low key stories.",
                    "label": 0
                },
                {
                    "sent": "So we were able to personalize post selection.",
                    "label": 0
                },
                {
                    "sent": "To give us pose that we like.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a final Test, we did a user study.",
                    "label": 1
                },
                {
                    "sent": "In this we generated a set of posts and ask users to give us feedback.",
                    "label": 0
                },
                {
                    "sent": "We then generated the next set by incorporating this feedback using our algorithm and so on.",
                    "label": 0
                },
                {
                    "sent": "We did this for a number of days and we compared using an unpersonalized post selection strategy in which we still ask the users to give us feedback.",
                    "label": 0
                },
                {
                    "sent": "That is to tell us whether they like the post or not, but the post selected for the next day.",
                    "label": 0
                },
                {
                    "sent": "We're not based on the feedback.",
                    "label": 0
                },
                {
                    "sent": "They were just based on the simple coverage function defined earlier.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that on an average, users prefer personalized posts more than unpersonalized pose does.",
                    "label": 0
                },
                {
                    "sent": "We conclude that we were able to successfully Taylor Post selection to users interests.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to summarize, we first formalize this notion of covering the blogosphere, and I presented a near optimal algorithm for selecting posts.",
                    "label": 0
                },
                {
                    "sent": "To tailor post selection to users interest, I showed how we can learn a personalized coverage function and our algorithm in particular, achieves no regret.",
                    "label": 1
                },
                {
                    "sent": "We evaluated on real block data.",
                    "label": 1
                },
                {
                    "sent": "And our approach for coverage using only post content does as well as or better than.",
                    "label": 1
                },
                {
                    "sent": "All the other comparison techniques that even use richer features.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, using simulations an user study, we established that we were able to successfully Taylor Post selection to user preferences.",
                    "label": 0
                },
                {
                    "sent": "We are in the process of launching a website that uses our algorithm to select blog posts in real time and right now we have a demo available at www.turndownthenoise.com.",
                    "label": 0
                },
                {
                    "sent": "I encourage you to visit our site and we should have a full functional website by the end of summer.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory.",
                    "label": 0
                },
                {
                    "sent": "Hi I'm just wondering if you have any results of hard work and then yourselves to use something like Claritin filtering which doesn't use any content at all.",
                    "label": 0
                },
                {
                    "sent": "Just the thumbs up, thumbs down feedback so some of these approaches, for example the Yahoo buzz.",
                    "label": 0
                },
                {
                    "sent": "It is based on user voting and hence it is based on collaborative filtering because if a lot of users voted to re up then it goes up and that's what shows.",
                    "label": 0
                },
                {
                    "sent": "So we are comparing against those techniques.",
                    "label": 0
                },
                {
                    "sent": "We weren't able to use collaborating filtering for our approach because we just didn't have.",
                    "label": 0
                },
                {
                    "sent": "Enough information and enough user base.",
                    "label": 0
                },
                {
                    "sent": "I was noticing that in the last slide you showed that the gap between the personalized data in the random forever, the standard with the gap is closing as they could be just noise, but but do you think that there's some of the fact that you're you're overtraining orbiting Earth?",
                    "label": 0
                },
                {
                    "sent": "Two wells had as you use up then you're not necessarily able to guess what the person if you're filtering out things that might be interesting.",
                    "label": 0
                },
                {
                    "sent": "But just didn't hurt the presents.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, let me say that.",
                    "label": 0
                },
                {
                    "sent": "I mean so based on this, on an average you can see that we prefer personalized course and your question is like as time goes on, it seems the gap is reducing so.",
                    "label": 0
                },
                {
                    "sent": "Part of the reason could also be that because of the way we did our user study in which basically this is a psychological thing that people expect things to get better, and so that is also part of the reason why you see the Unpersonalized line going up, whereas the personalizes still right up there.",
                    "label": 0
                },
                {
                    "sent": "And I guess we would once we have a fully functional website then we will be able to do more extensive experimentation and see like what's the effect over longer time periods.",
                    "label": 0
                },
                {
                    "sent": "But also it depends on the learning rate.",
                    "label": 0
                },
                {
                    "sent": "So right now we just use some learning rate.",
                    "label": 0
                },
                {
                    "sent": "But if we do indeed see that this effect happens that we are able to personalize, like we're personalizing very aggressively, then we can potentially like tune the learning rate to take care of that problem.",
                    "label": 0
                },
                {
                    "sent": "You had very impressively low redundancy rates.",
                    "label": 0
                },
                {
                    "sent": "Aspects of your technique where forces in Geneva for loaded and see.",
                    "label": 0
                },
                {
                    "sent": "So if you remember I spoke about this diminishing returns property right?",
                    "label": 0
                },
                {
                    "sent": "So that property.",
                    "label": 0
                },
                {
                    "sent": "So we define our coverage function in a probabilistic fashion and it has diminishing returns property.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that if you've already covered a feature, covering it again gives you lower utility.",
                    "label": 0
                },
                {
                    "sent": "So that's why, like initially, you would want to cover.",
                    "label": 0
                },
                {
                    "sent": "The different different features, which are all very important, and then once you've done that, only then would you try to cover the features again.",
                    "label": 0
                },
                {
                    "sent": "So this is what helps us in achieving diversity and low redundancy.",
                    "label": 0
                },
                {
                    "sent": "Copying the center features in the beginning that allow you to focus your attention on those features.",
                    "label": 0
                },
                {
                    "sent": "For example, then you might have difficulty multiple ways of characterizing or potentially the same story.",
                    "label": 0
                },
                {
                    "sent": "Different uses of language, for example, right?",
                    "label": 0
                },
                {
                    "sent": "So if we have like this like so, we do not have coreference resolution and things like that right now.",
                    "label": 0
                },
                {
                    "sent": "So it is true that we will have a problem if there are like the same feature which are like two different features which are talking about this.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same thing and have a high weight.",
                    "label": 0
                },
                {
                    "sent": "Then we would indeed select two stories, one story for each one of them.",
                    "label": 0
                },
                {
                    "sent": "But that depends on the features.",
                    "label": 0
                },
                {
                    "sent": "An algorithm or approach works for any set of features.",
                    "label": 0
                },
                {
                    "sent": "So if you choose features using Coreference resolution, that problem will not occur, for example.",
                    "label": 0
                },
                {
                    "sent": "My special guest.",
                    "label": 0
                },
                {
                    "sent": "Could you tell us a little more about the users ID selected for evaluating the results in the sense that other users, experts of the topics that the blogs are covering?",
                    "label": 0
                },
                {
                    "sent": "Or were they like, you know, oh, I want to listen about.",
                    "label": 0
                },
                {
                    "sent": "I want to read about this topic today, but another topic tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And if there is a dependence of the results of those so users were not experts on any subject.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the coverage experiment we went to.",
                    "label": 0
                },
                {
                    "sent": "Various websites in different categories.",
                    "label": 0
                },
                {
                    "sent": "So for politics we went to CNN, Washington Post, Reuters and so on.",
                    "label": 0
                },
                {
                    "sent": "And we downloaded the top stories and we give users access to this set.",
                    "label": 0
                },
                {
                    "sent": "So and then we ask him to compare whether one of the stories that we show is related to any of these stories.",
                    "label": 0
                },
                {
                    "sent": "So that removed that that that was able to like remove any bias that the users had because they already had access to this reference collection and we just had to compare with that.",
                    "label": 0
                },
                {
                    "sent": "And for the personalization user study, we just ask them to just be natural and just say whether they like something or dislike something so.",
                    "label": 0
                },
                {
                    "sent": "Start answer your question.",
                    "label": 0
                },
                {
                    "sent": "A more controlled experiment, probably for testing the personalization, would be taken expert right?",
                    "label": 0
                },
                {
                    "sent": "And he keeps every day, he tells you.",
                    "label": 0
                },
                {
                    "sent": "Yes, I like politics.",
                    "label": 0
                },
                {
                    "sent": "I like politics like politics, right?",
                    "label": 0
                },
                {
                    "sent": "And your personalization is indeed able to capture that.",
                    "label": 0
                },
                {
                    "sent": "Then you should the code that you showed.",
                    "label": 0
                },
                {
                    "sent": "Then for each topic.",
                    "label": 0
                },
                {
                    "sent": "And if you use an expert as the user then you're.",
                    "label": 0
                },
                {
                    "sent": "Definitely I love those that you just right.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying is then this takes care of that.",
                    "label": 0
                },
                {
                    "sent": "So here we tried to simulate a sports fan and we said we like all articles about from this particular sports blogs and we were able to show that the preference for an article from this blog or some other sports block keeps on increasing, whereas the preference for some other blogs like politics blog keeps on decreasing.",
                    "label": 0
                },
                {
                    "sent": "So this sort of takes care of the expert situation that you were talking about, I guess.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}