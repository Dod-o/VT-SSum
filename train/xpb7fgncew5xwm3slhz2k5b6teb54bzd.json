{
    "id": "xpb7fgncew5xwm3slhz2k5b6teb54bzd",
    "title": "Dropout: A simple and effective way to improve neural networks",
    "info": {
        "author": [
            "Geoffrey E. Hinton, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks"
        ]
    },
    "url": "http://videolectures.net/nips2012_hinton_networks/",
    "segmentation": [
        [
            "Thank you Alex.",
            "Um?"
        ],
        [
            "I first want to summarize why you didn't need to come to NIPS for the last 15 years.",
            "It turned out that the reason back Prob didn't win everything back in the 1980s is 'cause computers weren't fast enough and datasets weren't big enough.",
            "What's happened since then?",
            "Is computers got faster?",
            "Datasets got bigger, the labeled ones, especially in speech.",
            "And we found a better way to initialize the weights, but that's the sort of rather minor thing.",
            "And so the question now is, is there anything these big deep Nets can't?"
        ],
        [
            "Too, 'cause they're winning acoustic models for speech.",
            "They're winning for object recognition.",
            "They're winning for predicting the activity of molecules.",
            "One thing it seems hard to do with a big neural net is to average many models and that's 'cause each net takes a long time to train.",
            "Even if you're Google and a test time in particular, we don't want to have to run lots of large Nets."
        ],
        [
            "Now we want to average models because everyone who wins the competition does it by averaging models.",
            "From which you can infer to win.",
            "A competition is average models, probably.",
            "The models that seem to work best at present by averaging decision trees, so random forests do very well, lots of things, and that's because decision trees are very fast to train and also very fast to test time.",
            "So you can afford to average lots of decision trees.",
            "Let's called random forests and you make the different decision trees do different things by giving them different training data.",
            "I guess you all know that now there's two ways to average models.",
            "So if you have."
        ],
        [
            "Player Model B and they predict class probabilities.",
            "I could take a mixture of them and I could say the class probabilities are got by just averaging the individual.",
            "That's an equally weighted mixture, or I could take a product of what the models say.",
            "So I just multiply the probabilities together, take the square root.",
            "If I'm taking geometric mean and then I get some numbers that don't add up to one.",
            "So I have to divide by the sum of these numbers.",
            "So this is a geometric mean and this is an arithmetic mean.",
            "Then you can do either both of them work pretty well.",
            "I'm going to do geometric means 'cause that drops out of the method.",
            "I'm going to talk about."
        ],
        [
            "So here's how you can use lots of different neural Nets and average what they say and not pay a price at Test time.",
            "Um?",
            "Let's start with the net with one hidden layer.",
            "What we do when we're training it is when we present a training case, we randomly omit each hidden unit with the probability of nought.",
            ".5.",
            "We got nought .5 by solving the equation X = 1 -- X.",
            "And.",
            "If you do that, each time you give an input.",
            "Then what happens is you're using a different architecture.",
            "If I've got a chicken units, is 2 to the H different architectures, so that's a lot of models.",
            "That's many more models than anybody else doing model averaging users each model.",
            "Moreover, only ever sees one training case 'cause once H is large, you're never going to repeat the same pattern here.",
            "And if you go through the data many times, it's important each time you see the same training case again to use a different pattern of dropout.",
            "Now, the reason you can fit only seeing one training case per model is that we tie the weights together so a given hidden units when it's used always has the same weights to the input in the same way to the output.",
            "So that's a very good regularizer, 'cause what you're saying is instead of putting the weights towards 0 like L1 or L2, you're pulling the weights towards what other models want them to be.",
            "And that's presumably something better than 0."
        ],
        [
            "So we can think of this as a way of doing model averaging of two to the H models.",
            "Um?",
            "With all the sharing we're managed to fit these models even though most models are actually never used at all in training.",
            "And what we'd like to do now is a test time.",
            "The obvious thing to do is do this random drop out and do it many, many different times in average, and that may be what the brain does, but it takes time to do that.",
            "So a nice pro."
        ],
        [
            "This is that we're going to do the following at Test time.",
            "We're going to take the hidden units that we've learned, and at training time, we drop them out with probability of half a test time.",
            "We just have their outgoing weights.",
            "So the expected value coming out of one of those units stays the same at training and test time.",
            "And if we do that and we have a softmax at the output, we are exactly taking the geometric mean of all two to the edge models.",
            "It just drops out of the math.",
            "So that means that making the model is just a factor of two bigger test time, we can get the exact geometric mean of all these models, and that's that's basically what we couldn't do with neural Nets before.",
            "We couldn't train lots of different models and then have something efficient to test time.",
            "If we have more."
        ],
        [
            "In less we just drop out of nought .5 in every layer that works better on the whole.",
            "And with more hidden layers, if you have the outgoing weight of each hidden unit at Test time, you're no longer taking the exact geometric mean of what you do got, but it's sort of like mean field.",
            "You're getting approximately that, and it works just fine.",
            "Um?",
            "This shows my deep respect for mathematics."
        ],
        [
            "If you have an input layer.",
            "You can do the same in the input layer.",
            "Typically it's best not to drop out half of them, you drop out less than half of them, and that's already been discovered that was done in Yoshua Bengio's lab by your shoe, and Hugo and Pascal Vincent.",
            "He's got a French name Pascal van song.",
            "And I think some other people.",
            "They discover for autoencoders it's a very good idea to drop out some of the inputs.",
            "We're just doing the same thing, but for the hidden layers as well.",
            "Now all of you are familiar with dropout is very well known that it works in one special."
        ],
        [
            "Case.",
            "Suppose I'm doing logistic regression and I have a large number of inputs.",
            "Suppose I have, say, a billion.",
            "I worked at Google over the summer, supposed to have a billion inputs, and I only have, say, a billion training examples.",
            "Um, what we could do is we could drop out all but one input.",
            "And then fit a logistic regression model.",
            "And then at Test time.",
            "This isn't quite the same as drop.",
            "It is very similar attest time.",
            "What I could do is just divide the outputs of all these units by N ends a number of units.",
            "And if I forget to do the division by N, that's called Naive Bayes.",
            "If I do the division by N, I'll get the same maximum here for two possible answers.",
            "So naive Bayes is just a version of drop out or it's very similar to dropout, and it's well known that it works if you got if you're liable to overfit, it works better than logistic regression, so averaging all these models together, which is what naive Bayes is doing, is a good idea."
        ],
        [
            "So the question is, how well does this dropout work if you try it on big neural Nets.",
            "Obviously, if your neural net is not overfitting, you don't need a regularizer like this.",
            "Um?",
            "But if you're an excellent, not overfitting, that means you're not using a big enough neural net.",
            "Um, there's people who say, you know, we got lots of data, so now we don't need to worry about overfitting.",
            "They just don't understand how the brain works.",
            "In the brain you have many many more parameters and you have training cases.",
            "You have about 10 to the nine training cases.",
            "Fortunately for me it's about 2 * 10 to the nine and you have about 10 to 14 parameters.",
            "And that's 'cause synapses are a whole lot cheaper than experiences.",
            "And so you want to have lots of synapses per experience.",
            "And that means you're going to get an overfitting problem.",
            "So.",
            "Let's look at one experiment done by Nitish."
        ],
        [
            "Esteva he did lots of experiments on drop out, but this is one of them.",
            "He took Tim it.",
            "He took a chaldee implementation done by with DAMP over software.",
            "And he trained a standard acoustic model sort of deepnet acoustic model, and it gets 22.7% error on this particular version of timid.",
            "When you fine tune it in the normal way, which is just by using a small learning rate and backdrop.",
            "If you fine tune it using dropout, then he gets down to with again a small learning rate, but use dropout.",
            "He gets down to 19.7, so that's a 10% improvement.",
            "Or something like that.",
            "No, it's more than 10%.",
            "OK, typically get like 10% improvements out of this.",
            "At the time that was a record for sort of speaker independent image.",
            "People have since got that down to 18.7% and actually someone in my lab got it down to 17.7%."
        ],
        [
            "If you look at what happens during the fine tuning, this is the standard thing you would do with neural Nets.",
            "Before you would do some pre training.",
            "Then you start fine tuning and I'm showing you here the frame classification rate, not the phone error rate, which is why it's different numbers and you see the classic early stopping curve where it comes down and then it overfits.",
            "This is just various different size models and.",
            "If you don't use this regularizer, you overfit and so you should do early stopping and get this.",
            "A minimum here, but the point is if you use dropout it just keeps going down and actually for the same length of time it gets to about the same place, so it's not actually slower than early stopping.",
            "It, but it just keeps coming down and it does go up eventually slightly.",
            "In the end it's a bit like boosting, which for a long time people thought it didn't get worse.",
            "It does get worse in the end, but much but very gently compared with this.",
            "OK, so that was one very nice example.",
            "There's another example you've heard already, but I can't resist mentioning it.",
            "When you got a good result, you just have to keep saying it.",
            "With this"
        ],
        [
            "Competition on recognizing objects in images where the classification task is to say what you get 5 bets about what might be in the image.",
            "You heard this in a spotlight yesterday.",
            "There's 1.2 million training images, so this is the first series of tasks in which we had big enough datasets.",
            "When your next already showing.",
            "And it's the first time we have fast enough computers to train them.",
            "Um?",
            "Many of the best computervision groups tried this and the results look."
        ],
        [
            "Like this, the error rates on predicting the label correctly in your top five bets.",
            "They all cluster around 2627%.",
            "Um?",
            "There's some other people who are pretty good, but not quite as good as these best systems.",
            "And then if you look at what you get with the neural net with dropout.",
            "You get this is an analog representation.",
            "You do a whole lot better and actually if you give it some more training data, you get down to 15%.",
            "So this method is very happy with more training data.",
            "These methods are lots of hand engineering.",
            "If you look at the descriptions of these methods, the pages and pages if you look at the description of this method, we used to deep neural network drop it.",
            "That's a bit unfair.",
            "'cause actually the description of this method is we use every trick that Jan invented over the last 20 years plus drop out.",
            "And every trick that you invented over the last 20 years.",
            "Plus GPU's will get you back here and then drop out gets you the last few percent.",
            "So drop is probably worth about 3 or 4% on this.",
            "But that's still a lot."
        ],
        [
            "OK.",
            "Here's a better way to think about dropout.",
            "Well, I'm going much too fast so I can put all the stuff at the end.",
            "Here.",
            "I could even tell my joke about Jerry Sandusky.",
            "Here's a better way to think about dropout that.",
            "If a hidden unit knows which other hidden units going to present, it can code up to it.",
            "And if you think back propagation is doing, is putting its trying to make each hidden unit fix up the mess that's leftover from the other hidden units.",
            "That's more obvious if you're doing regression where basically you take what all the other hidden units will make the answer be.",
            "You look at the difference from the target, and that's how you change the weights of this hidden unit.",
            "So everybody is trying to fix up the mess leftover by everybody else.",
            "And that's a guaranteed way to over overfit.",
            "What's happening when you do dropout is you're saying I don't know who I'm going to have to collaborate with.",
            "It's going to be some random subset of the other.",
            "These other guys, so I don't know what mess they're going to leave over now.",
            "If in general they leave a mess in One Direction, then I'll fix that up, but I better do something that's individually useful.",
            "I'm particularly classification.",
            "That means the individual hidden units will tend to do things.",
            "They tend to be more like rugged individualists than like namby, pamby, Socialists.",
            "Actually socialist, so that's an unfortunate analogy, but there you go.",
            "And then when you've got all these rugged individualists and you average them, you'll get a good job and you won't overfit.",
            "If we compare this with the.",
            "Bayesian approach."
        ],
        [
            "Then embays what you're doing is you're going to have lots of separate models.",
            "They're going to be samples from the posterior 'cause you're going to Monte Carlo.",
            "In the limit, the Bayesian things, the right thing to do, and it will win and we have found an example where Bayes beats dropout.",
            "Some fortunate, but there you go.",
            "But the problem with Bayes is the test time you have to run all these damn models.",
            "Um, we drop out what you're going to do is you're going to learn.",
            "Exponentially, many models, that is, you're not you're going to have many more models in your mix at Test time, and you've got a very efficient way of using them test time so that you don't suffer from this problem of having to run lots of models at Test time.",
            "Um?",
            "You"
        ],
        [
            "Can read about.",
            "You can read our rejected science paper on my web page.",
            "It's a nice easy read 'cause it was written for science.",
            "Anne, it's before 11:20."
        ],
        [
            "So here's an alternative to drop out.",
            "I'm suppose that.",
            "In drop out, what we're going to do is we can typically logistic unit, but also we use other kinds of units.",
            "But suppose it was a logistic unit.",
            "You take a logistic hidden unit, it computes the probability P. And then you might send that to the next layer.",
            "It computes a number P. The output of the logistic, and you might send this number to the next layer.",
            "Or you might not.",
            "That's dropout.",
            "So you send this P with probability of nought .5.",
            "Now you'll get exactly the same expected value if you send a nought .5 with probability P. So let's suppose we call a spike nought .5 and we send that with probability P. That's very similar to dropout.",
            "And so that sort of raises an issue.",
            "Maybe the brain is doing something very like dropout by using stochastic spikes."
        ],
        [
            "So if you look at the variance, if we just send one bit.",
            "Then what we discover is.",
            "So what we're going to do is not do dropout.",
            "Just take a normal feedforward neural net.",
            "But instead of sending the real valued output of the logistic to the next layer, we're going to send either a one or a zero with that probability.",
            "So we drop out, you get a variance of P ^2 / 4 when spikes or half an with the stochastic bits you get a variance of Peter 1 minus here before, which are Luckily the same when P = 1/2.",
            "When P is low, stochastic bits actually have more variance and the limit when P is though is Apostle in Europe.",
            "So these small probability events so actually pass on your arms with low firing rates actually have even more variance than drop out.",
            "And so.",
            "You can actually take a feedforward neural net and when you run it forwards instead of running the normal way, you just round these peas and stochastically and what happens then?"
        ],
        [
            "Um?",
            "Is that?",
            "You discover that you get worse performance on the training data.",
            "I tried this years ago on the trend get worse performance and it's slower to train, but you get much better performance on the test data.",
            "So actually sending stochastic spikes is better than sending real values when you back propagate you back propagate as if you sent real values.",
            "And so there's a little amusing piece of history here.",
            "Which is that?",
            "In 2005 we discovered you can pre trained deep Nets by using restricted Boltzmann machines which have stochastic hidden units and then you get this big stack and then when you finish doing that you say OK.",
            "I was using a graphical model and I stacked all these things up using a graphical model and I got a graphical model called a deep belief net which has a restricted Boltzmann machine at the top and then top down Directive connections and it's all kosher and it's all a graphical model and I've got a fast approximate way of doing inference and then let's just throw away all that theory and let's do.",
            "Treat it as a deterministic feedforward net on all we did with that theory is just initialize the weights and it turns out that works very well and that's how these acoustic models for speech were developed.",
            "With that within that framework.",
            "But actually, if you don't throw away all that model, all the graphical model and you say I'm going to keep doing inference the same way as are doing it in these Boltzmann machines.",
            "I'm going to stochastic hidden bits and just do it like that.",
            "It actually works better so occasionally throwing away all the theories about move.",
            "Usually it helps, but occasion it's bad move.",
            "Um?",
            "And so I want to finish by talking about some explanations of why cortical neurons don't send analog values.",
            "And the issue here is that.",
            "OK. Terry and I sort of reverse views on this every few years.",
            "So.",
            "And if you were doing signal processing and you had a bunch of things to do signal processing with, you'd want to use real numbers.",
            "I mean, you'd be insane not to use real numbers for doing signal processing.",
            "You want linear filters.",
            "You want nonlinear filters.",
            "You want to send real numbers around.",
            "And it appears sometimes that the brain doesn't.",
            "The brain sends stochastic spikes.",
            "How could that possibly be a good thing to do?",
            "And the answer is, well, if you take a feed forward net like this and try it with real numbers and you try to stick spikes, it generalizes much better with stochastic spikes.",
            "So what engineers are doing is they're doing things like fitting a common filter to some data, and they know the kind of model they're trying to fit, and they gotta limit sort of limited ambitions on this kind of model.",
            "Is doing is not like that at all.",
            "The brain gets this huge buzzing blooming confusion and it has no clue what kind of model it auto fit.",
            "So it uses crowdsourcing.",
            "Just fits a gazillion models and averages and hope for the best.",
            "And that's a very good thing to do if you have no clue what you're doing.",
            "And that's what you're doing when you stochastic spikes.",
            "So actually the brain doesn't want to send these accurate analog values.",
            "So if you look at the explanations, let's just assume for a minute that the brain doesn't use accurate factoring.",
            "If you look at the explanations for why it doesn't use the time of a spike to convey an analog value."
        ],
        [
            "Then there's various explanations people put forward.",
            "One explanation is that there's no efficient way that they could send online values, but that's obvious nonsense.",
            "'cause then you could use the time of a spike?",
            "It doesn't cost anymore sugar to send a spike it a particular time, then just send it to random time.",
            "So that argument is no good.",
            "Another argument is evolution didn't figure it out.",
            "That is, evolution had hundreds of millions years to figure this out, evolution knows how to take the same stem cells and turn them into teeth and eyeballs, and it just never figured out that you could use the time of a spike.",
            "That seems completely bizarre.",
            "Sorry.",
            "I forgot where I am this year.",
            "There is an alternative theory, which is God didn't figure it out.",
            "It was all designed by God and he didn't figure it out.",
            "OK, I would hate you to think there was just one theory here.",
            "But anyway, someone didn't figure it out and I don't find that plausible, so.",
            "The only theory I know that makes any sense if we really do have stochastically time spikes is that that's a better thing to do.",
            "And I've just shown you that for generalization that's a better thing to do.",
            "OK, I'm done with my bit of the talk and."
        ],
        [
            "Now I'm going to hand over to George, I should say.",
            "George is the student to actually run the first deep net for doing I'm acoustic models and he was supposed new ported effectively to Microsoft and help them get the large recap first vocabulary system.",
            "The first system allowed for Calgary and then George decided to enter this competition for predicting the activity of molecules.",
            "And he led the team that did that, and unfortunately, a newspaper Miss reported that is me leading the team.",
            "I had almost nothing to do with it and George is now going to talk about the how he won that competition for predicting the activity molecules.",
            "So actually we could maybe have one or two questions about your part of the talk while George is setting up.",
            "OK, so are there any questions right now?",
            "Um, OK otherwise will Oh yeah so am I on yes, so I often joke with my students that it's difficult to tell the difference between a bug in your program and a good regularizer.",
            "So this is 1 interesting bug you can introduce in your program.",
            "Instead, can you really prove it's a good regularizer?",
            "Well, I think I mean the referees were very suspicious of our paper.",
            "And I think the way to prove it is just keep winning competitions.",
            "And I think this is sort of convergence theorem that if you keep winning competitions, the probability Bankston gets smaller and smaller.",
            "Better books out there.",
            "We could put it.",
            "I'm not saying this is the best bug.",
            "For example, stochastic spikes are pretty good bug too.",
            "Anymore questions.",
            "This is a very simple question, I'm just wondering what the human performance of that that image classification task is.",
            "If anyone has any idea, it's well.",
            "It's like it's complicated because some of the classes are like 15 different classes of fungus.",
            "And any normal person can't tell the difference.",
            "So really to get the human rate you'd need to have these abnormal people on the particular classes.",
            "SPECT if you did that, you can do much better than this.",
            "Anymore questions.",
            "Can you comment on the relationship with dropout to pruning?",
            "Dropout seems to be a temporal form of pruning, and I was just wondering if you had looked at using dropout on different time scales.",
            "OK, I people have looked at using different forms of dropout on different space scales, so you can drop out whole blocks.",
            "Manziel's looked at that and that works nicely.",
            "I want to look at it on different timescales, but it's not quite the same as pruning because you're keeping these sort of entropy of having all these models improving.",
            "You typically just have one model.",
            "17% number you just talk about for the speech.",
            "So the question was whether you can say something about the 17% average for speech.",
            "It's 17.7 and it was submitted to Icast, but my guess is the program chair of Icast can have a look at the paper.",
            "OK, I've got one last question I've been wondering how this actually relates to robustness in function classes, because basically what you're doing is you're limiting the influence of any particular coordinate on the final estimate.",
            "So have you have you started looking at the theoretical properties of that?",
            "No, but I'm sure you will.",
            "OK, I think we better cook.",
            "Lose the Earth, pass out to Georgia."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Alex.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I first want to summarize why you didn't need to come to NIPS for the last 15 years.",
                    "label": 0
                },
                {
                    "sent": "It turned out that the reason back Prob didn't win everything back in the 1980s is 'cause computers weren't fast enough and datasets weren't big enough.",
                    "label": 0
                },
                {
                    "sent": "What's happened since then?",
                    "label": 0
                },
                {
                    "sent": "Is computers got faster?",
                    "label": 0
                },
                {
                    "sent": "Datasets got bigger, the labeled ones, especially in speech.",
                    "label": 1
                },
                {
                    "sent": "And we found a better way to initialize the weights, but that's the sort of rather minor thing.",
                    "label": 1
                },
                {
                    "sent": "And so the question now is, is there anything these big deep Nets can't?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Too, 'cause they're winning acoustic models for speech.",
                    "label": 0
                },
                {
                    "sent": "They're winning for object recognition.",
                    "label": 0
                },
                {
                    "sent": "They're winning for predicting the activity of molecules.",
                    "label": 0
                },
                {
                    "sent": "One thing it seems hard to do with a big neural net is to average many models and that's 'cause each net takes a long time to train.",
                    "label": 1
                },
                {
                    "sent": "Even if you're Google and a test time in particular, we don't want to have to run lots of large Nets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we want to average models because everyone who wins the competition does it by averaging models.",
                    "label": 0
                },
                {
                    "sent": "From which you can infer to win.",
                    "label": 1
                },
                {
                    "sent": "A competition is average models, probably.",
                    "label": 0
                },
                {
                    "sent": "The models that seem to work best at present by averaging decision trees, so random forests do very well, lots of things, and that's because decision trees are very fast to train and also very fast to test time.",
                    "label": 1
                },
                {
                    "sent": "So you can afford to average lots of decision trees.",
                    "label": 0
                },
                {
                    "sent": "Let's called random forests and you make the different decision trees do different things by giving them different training data.",
                    "label": 1
                },
                {
                    "sent": "I guess you all know that now there's two ways to average models.",
                    "label": 0
                },
                {
                    "sent": "So if you have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Player Model B and they predict class probabilities.",
                    "label": 0
                },
                {
                    "sent": "I could take a mixture of them and I could say the class probabilities are got by just averaging the individual.",
                    "label": 0
                },
                {
                    "sent": "That's an equally weighted mixture, or I could take a product of what the models say.",
                    "label": 0
                },
                {
                    "sent": "So I just multiply the probabilities together, take the square root.",
                    "label": 0
                },
                {
                    "sent": "If I'm taking geometric mean and then I get some numbers that don't add up to one.",
                    "label": 0
                },
                {
                    "sent": "So I have to divide by the sum of these numbers.",
                    "label": 0
                },
                {
                    "sent": "So this is a geometric mean and this is an arithmetic mean.",
                    "label": 0
                },
                {
                    "sent": "Then you can do either both of them work pretty well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do geometric means 'cause that drops out of the method.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's how you can use lots of different neural Nets and average what they say and not pay a price at Test time.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's start with the net with one hidden layer.",
                    "label": 1
                },
                {
                    "sent": "What we do when we're training it is when we present a training case, we randomly omit each hidden unit with the probability of nought.",
                    "label": 1
                },
                {
                    "sent": ".5.",
                    "label": 0
                },
                {
                    "sent": "We got nought .5 by solving the equation X = 1 -- X.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you do that, each time you give an input.",
                    "label": 0
                },
                {
                    "sent": "Then what happens is you're using a different architecture.",
                    "label": 0
                },
                {
                    "sent": "If I've got a chicken units, is 2 to the H different architectures, so that's a lot of models.",
                    "label": 0
                },
                {
                    "sent": "That's many more models than anybody else doing model averaging users each model.",
                    "label": 0
                },
                {
                    "sent": "Moreover, only ever sees one training case 'cause once H is large, you're never going to repeat the same pattern here.",
                    "label": 0
                },
                {
                    "sent": "And if you go through the data many times, it's important each time you see the same training case again to use a different pattern of dropout.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason you can fit only seeing one training case per model is that we tie the weights together so a given hidden units when it's used always has the same weights to the input in the same way to the output.",
                    "label": 0
                },
                {
                    "sent": "So that's a very good regularizer, 'cause what you're saying is instead of putting the weights towards 0 like L1 or L2, you're pulling the weights towards what other models want them to be.",
                    "label": 0
                },
                {
                    "sent": "And that's presumably something better than 0.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can think of this as a way of doing model averaging of two to the H models.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "With all the sharing we're managed to fit these models even though most models are actually never used at all in training.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to do now is a test time.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing to do is do this random drop out and do it many, many different times in average, and that may be what the brain does, but it takes time to do that.",
                    "label": 0
                },
                {
                    "sent": "So a nice pro.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is that we're going to do the following at Test time.",
                    "label": 1
                },
                {
                    "sent": "We're going to take the hidden units that we've learned, and at training time, we drop them out with probability of half a test time.",
                    "label": 1
                },
                {
                    "sent": "We just have their outgoing weights.",
                    "label": 0
                },
                {
                    "sent": "So the expected value coming out of one of those units stays the same at training and test time.",
                    "label": 1
                },
                {
                    "sent": "And if we do that and we have a softmax at the output, we are exactly taking the geometric mean of all two to the edge models.",
                    "label": 0
                },
                {
                    "sent": "It just drops out of the math.",
                    "label": 0
                },
                {
                    "sent": "So that means that making the model is just a factor of two bigger test time, we can get the exact geometric mean of all these models, and that's that's basically what we couldn't do with neural Nets before.",
                    "label": 0
                },
                {
                    "sent": "We couldn't train lots of different models and then have something efficient to test time.",
                    "label": 0
                },
                {
                    "sent": "If we have more.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In less we just drop out of nought .5 in every layer that works better on the whole.",
                    "label": 0
                },
                {
                    "sent": "And with more hidden layers, if you have the outgoing weight of each hidden unit at Test time, you're no longer taking the exact geometric mean of what you do got, but it's sort of like mean field.",
                    "label": 1
                },
                {
                    "sent": "You're getting approximately that, and it works just fine.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This shows my deep respect for mathematics.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have an input layer.",
                    "label": 1
                },
                {
                    "sent": "You can do the same in the input layer.",
                    "label": 0
                },
                {
                    "sent": "Typically it's best not to drop out half of them, you drop out less than half of them, and that's already been discovered that was done in Yoshua Bengio's lab by your shoe, and Hugo and Pascal Vincent.",
                    "label": 0
                },
                {
                    "sent": "He's got a French name Pascal van song.",
                    "label": 0
                },
                {
                    "sent": "And I think some other people.",
                    "label": 0
                },
                {
                    "sent": "They discover for autoencoders it's a very good idea to drop out some of the inputs.",
                    "label": 0
                },
                {
                    "sent": "We're just doing the same thing, but for the hidden layers as well.",
                    "label": 0
                },
                {
                    "sent": "Now all of you are familiar with dropout is very well known that it works in one special.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "Suppose I'm doing logistic regression and I have a large number of inputs.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have, say, a billion.",
                    "label": 0
                },
                {
                    "sent": "I worked at Google over the summer, supposed to have a billion inputs, and I only have, say, a billion training examples.",
                    "label": 0
                },
                {
                    "sent": "Um, what we could do is we could drop out all but one input.",
                    "label": 1
                },
                {
                    "sent": "And then fit a logistic regression model.",
                    "label": 1
                },
                {
                    "sent": "And then at Test time.",
                    "label": 0
                },
                {
                    "sent": "This isn't quite the same as drop.",
                    "label": 0
                },
                {
                    "sent": "It is very similar attest time.",
                    "label": 0
                },
                {
                    "sent": "What I could do is just divide the outputs of all these units by N ends a number of units.",
                    "label": 0
                },
                {
                    "sent": "And if I forget to do the division by N, that's called Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "If I do the division by N, I'll get the same maximum here for two possible answers.",
                    "label": 0
                },
                {
                    "sent": "So naive Bayes is just a version of drop out or it's very similar to dropout, and it's well known that it works if you got if you're liable to overfit, it works better than logistic regression, so averaging all these models together, which is what naive Bayes is doing, is a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, how well does this dropout work if you try it on big neural Nets.",
                    "label": 1
                },
                {
                    "sent": "Obviously, if your neural net is not overfitting, you don't need a regularizer like this.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But if you're an excellent, not overfitting, that means you're not using a big enough neural net.",
                    "label": 0
                },
                {
                    "sent": "Um, there's people who say, you know, we got lots of data, so now we don't need to worry about overfitting.",
                    "label": 1
                },
                {
                    "sent": "They just don't understand how the brain works.",
                    "label": 0
                },
                {
                    "sent": "In the brain you have many many more parameters and you have training cases.",
                    "label": 0
                },
                {
                    "sent": "You have about 10 to the nine training cases.",
                    "label": 1
                },
                {
                    "sent": "Fortunately for me it's about 2 * 10 to the nine and you have about 10 to 14 parameters.",
                    "label": 0
                },
                {
                    "sent": "And that's 'cause synapses are a whole lot cheaper than experiences.",
                    "label": 0
                },
                {
                    "sent": "And so you want to have lots of synapses per experience.",
                    "label": 0
                },
                {
                    "sent": "And that means you're going to get an overfitting problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's look at one experiment done by Nitish.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Esteva he did lots of experiments on drop out, but this is one of them.",
                    "label": 0
                },
                {
                    "sent": "He took Tim it.",
                    "label": 0
                },
                {
                    "sent": "He took a chaldee implementation done by with DAMP over software.",
                    "label": 0
                },
                {
                    "sent": "And he trained a standard acoustic model sort of deepnet acoustic model, and it gets 22.7% error on this particular version of timid.",
                    "label": 1
                },
                {
                    "sent": "When you fine tune it in the normal way, which is just by using a small learning rate and backdrop.",
                    "label": 1
                },
                {
                    "sent": "If you fine tune it using dropout, then he gets down to with again a small learning rate, but use dropout.",
                    "label": 0
                },
                {
                    "sent": "He gets down to 19.7, so that's a 10% improvement.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "No, it's more than 10%.",
                    "label": 0
                },
                {
                    "sent": "OK, typically get like 10% improvements out of this.",
                    "label": 1
                },
                {
                    "sent": "At the time that was a record for sort of speaker independent image.",
                    "label": 0
                },
                {
                    "sent": "People have since got that down to 18.7% and actually someone in my lab got it down to 17.7%.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at what happens during the fine tuning, this is the standard thing you would do with neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Before you would do some pre training.",
                    "label": 0
                },
                {
                    "sent": "Then you start fine tuning and I'm showing you here the frame classification rate, not the phone error rate, which is why it's different numbers and you see the classic early stopping curve where it comes down and then it overfits.",
                    "label": 0
                },
                {
                    "sent": "This is just various different size models and.",
                    "label": 0
                },
                {
                    "sent": "If you don't use this regularizer, you overfit and so you should do early stopping and get this.",
                    "label": 0
                },
                {
                    "sent": "A minimum here, but the point is if you use dropout it just keeps going down and actually for the same length of time it gets to about the same place, so it's not actually slower than early stopping.",
                    "label": 0
                },
                {
                    "sent": "It, but it just keeps coming down and it does go up eventually slightly.",
                    "label": 0
                },
                {
                    "sent": "In the end it's a bit like boosting, which for a long time people thought it didn't get worse.",
                    "label": 0
                },
                {
                    "sent": "It does get worse in the end, but much but very gently compared with this.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was one very nice example.",
                    "label": 0
                },
                {
                    "sent": "There's another example you've heard already, but I can't resist mentioning it.",
                    "label": 0
                },
                {
                    "sent": "When you got a good result, you just have to keep saying it.",
                    "label": 0
                },
                {
                    "sent": "With this",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Competition on recognizing objects in images where the classification task is to say what you get 5 bets about what might be in the image.",
                    "label": 1
                },
                {
                    "sent": "You heard this in a spotlight yesterday.",
                    "label": 1
                },
                {
                    "sent": "There's 1.2 million training images, so this is the first series of tasks in which we had big enough datasets.",
                    "label": 0
                },
                {
                    "sent": "When your next already showing.",
                    "label": 0
                },
                {
                    "sent": "And it's the first time we have fast enough computers to train them.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Many of the best computervision groups tried this and the results look.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like this, the error rates on predicting the label correctly in your top five bets.",
                    "label": 1
                },
                {
                    "sent": "They all cluster around 2627%.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There's some other people who are pretty good, but not quite as good as these best systems.",
                    "label": 0
                },
                {
                    "sent": "And then if you look at what you get with the neural net with dropout.",
                    "label": 0
                },
                {
                    "sent": "You get this is an analog representation.",
                    "label": 0
                },
                {
                    "sent": "You do a whole lot better and actually if you give it some more training data, you get down to 15%.",
                    "label": 0
                },
                {
                    "sent": "So this method is very happy with more training data.",
                    "label": 0
                },
                {
                    "sent": "These methods are lots of hand engineering.",
                    "label": 0
                },
                {
                    "sent": "If you look at the descriptions of these methods, the pages and pages if you look at the description of this method, we used to deep neural network drop it.",
                    "label": 0
                },
                {
                    "sent": "That's a bit unfair.",
                    "label": 0
                },
                {
                    "sent": "'cause actually the description of this method is we use every trick that Jan invented over the last 20 years plus drop out.",
                    "label": 0
                },
                {
                    "sent": "And every trick that you invented over the last 20 years.",
                    "label": 0
                },
                {
                    "sent": "Plus GPU's will get you back here and then drop out gets you the last few percent.",
                    "label": 0
                },
                {
                    "sent": "So drop is probably worth about 3 or 4% on this.",
                    "label": 0
                },
                {
                    "sent": "But that's still a lot.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Here's a better way to think about dropout.",
                    "label": 1
                },
                {
                    "sent": "Well, I'm going much too fast so I can put all the stuff at the end.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "I could even tell my joke about Jerry Sandusky.",
                    "label": 0
                },
                {
                    "sent": "Here's a better way to think about dropout that.",
                    "label": 0
                },
                {
                    "sent": "If a hidden unit knows which other hidden units going to present, it can code up to it.",
                    "label": 1
                },
                {
                    "sent": "And if you think back propagation is doing, is putting its trying to make each hidden unit fix up the mess that's leftover from the other hidden units.",
                    "label": 0
                },
                {
                    "sent": "That's more obvious if you're doing regression where basically you take what all the other hidden units will make the answer be.",
                    "label": 0
                },
                {
                    "sent": "You look at the difference from the target, and that's how you change the weights of this hidden unit.",
                    "label": 0
                },
                {
                    "sent": "So everybody is trying to fix up the mess leftover by everybody else.",
                    "label": 0
                },
                {
                    "sent": "And that's a guaranteed way to over overfit.",
                    "label": 0
                },
                {
                    "sent": "What's happening when you do dropout is you're saying I don't know who I'm going to have to collaborate with.",
                    "label": 0
                },
                {
                    "sent": "It's going to be some random subset of the other.",
                    "label": 0
                },
                {
                    "sent": "These other guys, so I don't know what mess they're going to leave over now.",
                    "label": 0
                },
                {
                    "sent": "If in general they leave a mess in One Direction, then I'll fix that up, but I better do something that's individually useful.",
                    "label": 0
                },
                {
                    "sent": "I'm particularly classification.",
                    "label": 0
                },
                {
                    "sent": "That means the individual hidden units will tend to do things.",
                    "label": 0
                },
                {
                    "sent": "They tend to be more like rugged individualists than like namby, pamby, Socialists.",
                    "label": 0
                },
                {
                    "sent": "Actually socialist, so that's an unfortunate analogy, but there you go.",
                    "label": 1
                },
                {
                    "sent": "And then when you've got all these rugged individualists and you average them, you'll get a good job and you won't overfit.",
                    "label": 0
                },
                {
                    "sent": "If we compare this with the.",
                    "label": 0
                },
                {
                    "sent": "Bayesian approach.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then embays what you're doing is you're going to have lots of separate models.",
                    "label": 0
                },
                {
                    "sent": "They're going to be samples from the posterior 'cause you're going to Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "In the limit, the Bayesian things, the right thing to do, and it will win and we have found an example where Bayes beats dropout.",
                    "label": 0
                },
                {
                    "sent": "Some fortunate, but there you go.",
                    "label": 0
                },
                {
                    "sent": "But the problem with Bayes is the test time you have to run all these damn models.",
                    "label": 0
                },
                {
                    "sent": "Um, we drop out what you're going to do is you're going to learn.",
                    "label": 0
                },
                {
                    "sent": "Exponentially, many models, that is, you're not you're going to have many more models in your mix at Test time, and you've got a very efficient way of using them test time so that you don't suffer from this problem of having to run lots of models at Test time.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can read about.",
                    "label": 0
                },
                {
                    "sent": "You can read our rejected science paper on my web page.",
                    "label": 0
                },
                {
                    "sent": "It's a nice easy read 'cause it was written for science.",
                    "label": 0
                },
                {
                    "sent": "Anne, it's before 11:20.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an alternative to drop out.",
                    "label": 0
                },
                {
                    "sent": "I'm suppose that.",
                    "label": 0
                },
                {
                    "sent": "In drop out, what we're going to do is we can typically logistic unit, but also we use other kinds of units.",
                    "label": 0
                },
                {
                    "sent": "But suppose it was a logistic unit.",
                    "label": 0
                },
                {
                    "sent": "You take a logistic hidden unit, it computes the probability P. And then you might send that to the next layer.",
                    "label": 0
                },
                {
                    "sent": "It computes a number P. The output of the logistic, and you might send this number to the next layer.",
                    "label": 0
                },
                {
                    "sent": "Or you might not.",
                    "label": 0
                },
                {
                    "sent": "That's dropout.",
                    "label": 0
                },
                {
                    "sent": "So you send this P with probability of nought .5.",
                    "label": 0
                },
                {
                    "sent": "Now you'll get exactly the same expected value if you send a nought .5 with probability P. So let's suppose we call a spike nought .5 and we send that with probability P. That's very similar to dropout.",
                    "label": 0
                },
                {
                    "sent": "And so that sort of raises an issue.",
                    "label": 0
                },
                {
                    "sent": "Maybe the brain is doing something very like dropout by using stochastic spikes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at the variance, if we just send one bit.",
                    "label": 0
                },
                {
                    "sent": "Then what we discover is.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is not do dropout.",
                    "label": 0
                },
                {
                    "sent": "Just take a normal feedforward neural net.",
                    "label": 0
                },
                {
                    "sent": "But instead of sending the real valued output of the logistic to the next layer, we're going to send either a one or a zero with that probability.",
                    "label": 0
                },
                {
                    "sent": "So we drop out, you get a variance of P ^2 / 4 when spikes or half an with the stochastic bits you get a variance of Peter 1 minus here before, which are Luckily the same when P = 1/2.",
                    "label": 0
                },
                {
                    "sent": "When P is low, stochastic bits actually have more variance and the limit when P is though is Apostle in Europe.",
                    "label": 0
                },
                {
                    "sent": "So these small probability events so actually pass on your arms with low firing rates actually have even more variance than drop out.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "You can actually take a feedforward neural net and when you run it forwards instead of running the normal way, you just round these peas and stochastically and what happens then?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "You discover that you get worse performance on the training data.",
                    "label": 0
                },
                {
                    "sent": "I tried this years ago on the trend get worse performance and it's slower to train, but you get much better performance on the test data.",
                    "label": 0
                },
                {
                    "sent": "So actually sending stochastic spikes is better than sending real values when you back propagate you back propagate as if you sent real values.",
                    "label": 0
                },
                {
                    "sent": "And so there's a little amusing piece of history here.",
                    "label": 0
                },
                {
                    "sent": "Which is that?",
                    "label": 0
                },
                {
                    "sent": "In 2005 we discovered you can pre trained deep Nets by using restricted Boltzmann machines which have stochastic hidden units and then you get this big stack and then when you finish doing that you say OK.",
                    "label": 0
                },
                {
                    "sent": "I was using a graphical model and I stacked all these things up using a graphical model and I got a graphical model called a deep belief net which has a restricted Boltzmann machine at the top and then top down Directive connections and it's all kosher and it's all a graphical model and I've got a fast approximate way of doing inference and then let's just throw away all that theory and let's do.",
                    "label": 0
                },
                {
                    "sent": "Treat it as a deterministic feedforward net on all we did with that theory is just initialize the weights and it turns out that works very well and that's how these acoustic models for speech were developed.",
                    "label": 0
                },
                {
                    "sent": "With that within that framework.",
                    "label": 0
                },
                {
                    "sent": "But actually, if you don't throw away all that model, all the graphical model and you say I'm going to keep doing inference the same way as are doing it in these Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stochastic hidden bits and just do it like that.",
                    "label": 0
                },
                {
                    "sent": "It actually works better so occasionally throwing away all the theories about move.",
                    "label": 0
                },
                {
                    "sent": "Usually it helps, but occasion it's bad move.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so I want to finish by talking about some explanations of why cortical neurons don't send analog values.",
                    "label": 1
                },
                {
                    "sent": "And the issue here is that.",
                    "label": 0
                },
                {
                    "sent": "OK. Terry and I sort of reverse views on this every few years.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And if you were doing signal processing and you had a bunch of things to do signal processing with, you'd want to use real numbers.",
                    "label": 0
                },
                {
                    "sent": "I mean, you'd be insane not to use real numbers for doing signal processing.",
                    "label": 0
                },
                {
                    "sent": "You want linear filters.",
                    "label": 0
                },
                {
                    "sent": "You want nonlinear filters.",
                    "label": 1
                },
                {
                    "sent": "You want to send real numbers around.",
                    "label": 0
                },
                {
                    "sent": "And it appears sometimes that the brain doesn't.",
                    "label": 1
                },
                {
                    "sent": "The brain sends stochastic spikes.",
                    "label": 0
                },
                {
                    "sent": "How could that possibly be a good thing to do?",
                    "label": 0
                },
                {
                    "sent": "And the answer is, well, if you take a feed forward net like this and try it with real numbers and you try to stick spikes, it generalizes much better with stochastic spikes.",
                    "label": 0
                },
                {
                    "sent": "So what engineers are doing is they're doing things like fitting a common filter to some data, and they know the kind of model they're trying to fit, and they gotta limit sort of limited ambitions on this kind of model.",
                    "label": 0
                },
                {
                    "sent": "Is doing is not like that at all.",
                    "label": 0
                },
                {
                    "sent": "The brain gets this huge buzzing blooming confusion and it has no clue what kind of model it auto fit.",
                    "label": 0
                },
                {
                    "sent": "So it uses crowdsourcing.",
                    "label": 0
                },
                {
                    "sent": "Just fits a gazillion models and averages and hope for the best.",
                    "label": 0
                },
                {
                    "sent": "And that's a very good thing to do if you have no clue what you're doing.",
                    "label": 0
                },
                {
                    "sent": "And that's what you're doing when you stochastic spikes.",
                    "label": 1
                },
                {
                    "sent": "So actually the brain doesn't want to send these accurate analog values.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the explanations, let's just assume for a minute that the brain doesn't use accurate factoring.",
                    "label": 0
                },
                {
                    "sent": "If you look at the explanations for why it doesn't use the time of a spike to convey an analog value.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then there's various explanations people put forward.",
                    "label": 0
                },
                {
                    "sent": "One explanation is that there's no efficient way that they could send online values, but that's obvious nonsense.",
                    "label": 0
                },
                {
                    "sent": "'cause then you could use the time of a spike?",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost anymore sugar to send a spike it a particular time, then just send it to random time.",
                    "label": 0
                },
                {
                    "sent": "So that argument is no good.",
                    "label": 0
                },
                {
                    "sent": "Another argument is evolution didn't figure it out.",
                    "label": 0
                },
                {
                    "sent": "That is, evolution had hundreds of millions years to figure this out, evolution knows how to take the same stem cells and turn them into teeth and eyeballs, and it just never figured out that you could use the time of a spike.",
                    "label": 0
                },
                {
                    "sent": "That seems completely bizarre.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "I forgot where I am this year.",
                    "label": 0
                },
                {
                    "sent": "There is an alternative theory, which is God didn't figure it out.",
                    "label": 0
                },
                {
                    "sent": "It was all designed by God and he didn't figure it out.",
                    "label": 0
                },
                {
                    "sent": "OK, I would hate you to think there was just one theory here.",
                    "label": 0
                },
                {
                    "sent": "But anyway, someone didn't figure it out and I don't find that plausible, so.",
                    "label": 0
                },
                {
                    "sent": "The only theory I know that makes any sense if we really do have stochastically time spikes is that that's a better thing to do.",
                    "label": 0
                },
                {
                    "sent": "And I've just shown you that for generalization that's a better thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm done with my bit of the talk and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to hand over to George, I should say.",
                    "label": 0
                },
                {
                    "sent": "George is the student to actually run the first deep net for doing I'm acoustic models and he was supposed new ported effectively to Microsoft and help them get the large recap first vocabulary system.",
                    "label": 0
                },
                {
                    "sent": "The first system allowed for Calgary and then George decided to enter this competition for predicting the activity of molecules.",
                    "label": 0
                },
                {
                    "sent": "And he led the team that did that, and unfortunately, a newspaper Miss reported that is me leading the team.",
                    "label": 0
                },
                {
                    "sent": "I had almost nothing to do with it and George is now going to talk about the how he won that competition for predicting the activity molecules.",
                    "label": 0
                },
                {
                    "sent": "So actually we could maybe have one or two questions about your part of the talk while George is setting up.",
                    "label": 0
                },
                {
                    "sent": "OK, so are there any questions right now?",
                    "label": 0
                },
                {
                    "sent": "Um, OK otherwise will Oh yeah so am I on yes, so I often joke with my students that it's difficult to tell the difference between a bug in your program and a good regularizer.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 interesting bug you can introduce in your program.",
                    "label": 0
                },
                {
                    "sent": "Instead, can you really prove it's a good regularizer?",
                    "label": 0
                },
                {
                    "sent": "Well, I think I mean the referees were very suspicious of our paper.",
                    "label": 0
                },
                {
                    "sent": "And I think the way to prove it is just keep winning competitions.",
                    "label": 0
                },
                {
                    "sent": "And I think this is sort of convergence theorem that if you keep winning competitions, the probability Bankston gets smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "Better books out there.",
                    "label": 0
                },
                {
                    "sent": "We could put it.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying this is the best bug.",
                    "label": 0
                },
                {
                    "sent": "For example, stochastic spikes are pretty good bug too.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple question, I'm just wondering what the human performance of that that image classification task is.",
                    "label": 0
                },
                {
                    "sent": "If anyone has any idea, it's well.",
                    "label": 0
                },
                {
                    "sent": "It's like it's complicated because some of the classes are like 15 different classes of fungus.",
                    "label": 0
                },
                {
                    "sent": "And any normal person can't tell the difference.",
                    "label": 0
                },
                {
                    "sent": "So really to get the human rate you'd need to have these abnormal people on the particular classes.",
                    "label": 0
                },
                {
                    "sent": "SPECT if you did that, you can do much better than this.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Can you comment on the relationship with dropout to pruning?",
                    "label": 0
                },
                {
                    "sent": "Dropout seems to be a temporal form of pruning, and I was just wondering if you had looked at using dropout on different time scales.",
                    "label": 0
                },
                {
                    "sent": "OK, I people have looked at using different forms of dropout on different space scales, so you can drop out whole blocks.",
                    "label": 0
                },
                {
                    "sent": "Manziel's looked at that and that works nicely.",
                    "label": 0
                },
                {
                    "sent": "I want to look at it on different timescales, but it's not quite the same as pruning because you're keeping these sort of entropy of having all these models improving.",
                    "label": 0
                },
                {
                    "sent": "You typically just have one model.",
                    "label": 0
                },
                {
                    "sent": "17% number you just talk about for the speech.",
                    "label": 0
                },
                {
                    "sent": "So the question was whether you can say something about the 17% average for speech.",
                    "label": 0
                },
                {
                    "sent": "It's 17.7 and it was submitted to Icast, but my guess is the program chair of Icast can have a look at the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, I've got one last question I've been wondering how this actually relates to robustness in function classes, because basically what you're doing is you're limiting the influence of any particular coordinate on the final estimate.",
                    "label": 0
                },
                {
                    "sent": "So have you have you started looking at the theoretical properties of that?",
                    "label": 0
                },
                {
                    "sent": "No, but I'm sure you will.",
                    "label": 0
                },
                {
                    "sent": "OK, I think we better cook.",
                    "label": 0
                },
                {
                    "sent": "Lose the Earth, pass out to Georgia.",
                    "label": 0
                }
            ]
        }
    }
}