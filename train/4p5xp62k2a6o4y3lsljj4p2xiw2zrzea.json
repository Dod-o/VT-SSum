{
    "id": "4p5xp62k2a6o4y3lsljj4p2xiw2zrzea",
    "title": "Algorithms for Predicting Structured Data",
    "info": {
        "author": [
            "Thomas Gartner, Fraunhofer IAIS",
            "Shankar Vembu, Department of Computer Science, University of Illinois at Urbana-Champaign"
        ],
        "published": "Nov. 16, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2010_gartner_vembu_apsd/",
    "segmentation": [
        [
            "OK, so.",
            "Algorithms for predicting structured data.",
            "My name is Shankar Rambo.",
            "I'm from the University of Illinois Urbana Champaign.",
            "My Co presenter is Thomas Gardner.",
            "He will talk take over during the second part."
        ],
        [
            "OK, so before I get started.",
            "Let me just say a few words about the focus of this tutorial.",
            "So it's structured prediction is.",
            "It's a pretty huge topic with lots of applications, so if you go to an NLP conference or a computer vision conference, you will see.",
            "Lots of papers on structured prediction and in most of those application papers you may have to modify existing algorithms in nontrivial ways to solve the problem at hand.",
            "The focus of this letter, what I plan to do here is talk about the basic tools of the fundamental algorithms that are needed to solve structured prediction problems.",
            "I won't go deep into applications.",
            "There are specialized tutorials for structured prediction for NLP or computer vision.",
            "At those conferences, you might want to have a look at them.",
            "I'm right here, I'm just going to focus on the algorithms.",
            "I will point to some applications whenever it's required, but I won't go deep into applications OK?",
            "Alright, and and one more thing, if you have any questions feel free to interrupt me.",
            "I think that's how it is done here.",
            "Don't don't postpone the questions to the end, I don't think that's going to work.",
            "Alright.",
            "So structured prediction is.",
            "It's the problem of predicting multiple outputs with complex internal structure and dependencies among them.",
            "So this is this is different from.",
            "Single valued prediction problems that you might be knowing, like binary classification or regression, where you are predicting only one output, a single variable.",
            "In structured prediction you have multiple outputs.",
            "Um, it's again different from multivariate regression because the.",
            "Yeah.",
            "Louder oh OK. Alright, is it OK now?",
            "Great, OK, it's also different from multivariate regression, where only the correlations in the outputs are taken into account.",
            "But the problem is in structured prediction.",
            "How much more complex structure?",
            "So the techniques that are used to solve multivariate regression problems?",
            "The techniques that you might find in statistics literature do not immediately apply to this problem, so that you may want to keep this in mind."
        ],
        [
            "OK, let's let's have a quick look at a couple of applications.",
            "Part of speech tagging is an application in natural language processing where.",
            "The input is a sentence and the output is.",
            "You may want to predict the part of speech tags for this particular, but the words in the sentence.",
            "This is structured prediction problem because you don't, you don't want to predict the labels independently.",
            "You may want to take the sequential structure into account contact information into account when you're predicting these part of speech tags.",
            "So this is an example of structured prediction."
        ],
        [
            "Here is another example.",
            "Linguistic parsing again, is is an application in NLP.",
            "The input is a sentence.",
            "Again, the output is a power screen corresponding to this sentence.",
            "Here you have a recursive structure or a tree structure, so you this is a combinatorial structure and you may want to take this structure into account when you are.",
            "Solving this problem so the input space would be the space of all sentences and the output space is the space of all trees corresponding to this sentence.",
            "So that's this linguistic parsing."
        ],
        [
            "And there are apart from these applications, there are several well defined machine learning problems that can be solved using in a structured prediction framework.",
            "Multiclass prediction is a very simple problem.",
            "Um, in terms of structure prediction so.",
            "Multi Label Classification is another example where given and given an input and a set of labels you want to predict a subset of these labels.",
            "And then there is hierarchical classification, which is common in problems like document classification, where you have a taxonomy of.",
            "Topics and you want to categorize documents based on this taxonomy so you have a tree structure.",
            "And that's hierarchical classification.",
            "Label ranking is again a well defined machine learning problem where for a given input and as a set of labels you you want to.",
            "Predict a total order of these labels so it's not just one class or a subset of 1 label or a subset of labels you want to predict a total order, OK?",
            "And therefore the speed output space is the space of all permutations of these levels, so that's labeled ranking.",
            "So these are some of the machine learning problems, and nowadays people solve these problems in our structured prediction framework."
        ],
        [
            "And as I said, there are several real world applications.",
            "Natural language processing.",
            "We have seen a couple of examples.",
            "There are also problems in computational biology, bioinformatics and lots of problems in computer vision and also robotics.",
            "So there are several real world applications for structured prediction."
        ],
        [
            "Alright, so.",
            "This is the outline.",
            "There are three parts in the tutorial.",
            "In in the 1st and there are equally distributed, so the first part will be for 90 minutes, give or take a few minutes.",
            "The second part will be for around 60 minutes and the third part is is shorter.",
            "It sits for 30 minutes, so that's three hours of material OK?",
            "In the first part I'll be talking about the first part in the first part.",
            "What I'll do is I will talk about the basic algorithms.",
            "I will so the plan is I will start with binary classification or regression and then I'll tell you how to extend these algorithms to us with the setting of structured prediction.",
            "And in the second part, it's almost we talk about etc.",
            "Approximate inference, so you will see in the later slides that influence the so called inferences is an important ingredient of structured prediction and there are implications if you use either exact or approximate inference and Thomas will be discussing these problems that might arise there.",
            "In part three, I will.",
            "I will check over Part 3 where I have a couple of interesting applications, their applications and there really motivated the design for new algorithms.",
            "Structured prediction algorithms.",
            "It's weekly supervised learning, so it's it's not supervised or unsupervised, but the supervision is in in in a weaker fashion that's weakly supervised learning, so this is the outline."
        ],
        [
            "As I already said, the focus of the tutorial will be on algorithms.",
            "And.",
            "I'm going to focus on discriminative methods in structured prediction.",
            "OK, I will.",
            "I will tell you what discriminative methods are in a couple of minutes, but I won't be talking about generator methods, so there is a distinction there."
        ],
        [
            "OK, so this is interesting, so this is a list of topics that I do not cover.",
            "OK?",
            "Um advanced optimization methods.",
            "I will talk about optimization method and it's more than enough for you to get started.",
            "But if you are looking for large scale applications you might want to consider more specialized algorithms so and I won't talk about advanced optimization methods, But if you need the references, I'll be happy to give them to you.",
            "There are some connections to reinforcement learning and I won't be talking about them.",
            "There was a tutorial at ACL this year by Holidome and he talked about structured prediction and the connections to reinforcement learning, so I recommend going through those slides if you're interested.",
            "There aren't many learning theoretic results.",
            "There are a few papers, but I'm not going to talk about learning theoretic results, but you can have a look at those papers if you're interested.",
            "Inference in graphical models.",
            "So as I already said, the so called inference is an important ingredient in structured prediction.",
            "Um?",
            "In most of the applications are in most of the problems.",
            "It turns out that you might have to do some sort of inference in in a graphical model, and there are lots of algorithms that talk about for doing inference in graphical models.",
            "I won't be going into those details because there's a lot of structure and I definitely don't have the time for that, but I can tell you that there were a couple or maybe more more than two tutorials that computer vision conferences recently.",
            "Maybe this year, I think last year and.",
            "2008 they do a very good job and if you are interested in graphical models, inference in graphical models, I recommend going through those tutorials.",
            "And finally, I won't be talking about Semi supervised learning and unsupervised learning.",
            "OK, I will mostly focus on supervised learning.",
            "Um?",
            "So here this is the list of topics that I do not cover.",
            "So I just wanted to let you know that you know the material is not completed.",
            "It's only about algorithms.",
            "But if you want to get a complete picture, you have to really go through applications and the tutorials, given it applied at computer vision, conferences or NLP conferences OK?"
        ],
        [
            "Alright, so let's get started with part one.",
            "So the part one is structural binary to structure.",
            "So as I said."
        ],
        [
            "My plan is to begin with the basic binary classification algorithms, so I will talk about Perceptron and I will tell you how to extend it to structured Perceptron.",
            "I will talk about regression, the standard regularised least squares regression, and I will tell you how to extend it to structured prediction.",
            "Similarly, I will talk about SVM's and then extend it for the structured prediction case.",
            "I will talk about logistic regression and tell you how to extend it to structured prediction, which is conditional random fields.",
            "So Interestingly, also there is.",
            "There is nothing new you know.",
            "Structured prediction algorithms did not come out of the blue, so the existing algorithms can be naturally extended for the structured prediction case.",
            "Of course you will face some technical challenges depending on the complexity of your structure, but that's where you know you have to spend a lot of effort in solving the problem.",
            "At a conceptual level, all these extensions follow naturally.",
            "OK, so that's the road."
        ],
        [
            "Map.",
            "Preliminary so before I even get started with structured prediction, let me just give you some background information so that we are on the same page.",
            "As far as notation is concerned.",
            "So I have a few slides on some basics."
        ],
        [
            "Jeff, let's start with supervised learning.",
            "So this is everybody knows this, but let me just.",
            "Talk about this.",
            "There is in supervised learning.",
            "We have an input space and an output space.",
            "The input space is your features.",
            "The output space for the for classification, it's just a set of two elements plus one and minus one for regression problems.",
            "Is that set of real valued numbers?",
            "And the assumption is that there is some underlying unknown underlying distribution on the inputs and outputs, and from this distribution you are given a bunch of training data drawn IID independently and identically distributed.",
            "OK, now the goal of supervised learning or the goal of machine learning is.",
            "It's basically a function approximation problem where you are trying to learn where you're trying to learn a function that Maps the inputs to the outputs.",
            "OK, and the function has to be learned in such a way that it performs well on unseen examples.",
            "That's really important.",
            "And the performance is measured with respect to some loss function.",
            "OK, so this is this is supervised learning and then notation."
        ],
        [
            "OK, I I told you that I will talk about discriminative methods so there is a difference between a fundamental difference between generative discriminative learning an in generative learning, what you try to do is model the joint, estimate the parameters of of the probability of the joint on X&Y.",
            "OK, and then you perform predictions using the bicycle.",
            "So now your base is an example.",
            "Hidden Markov model is an example in in discriminative learning you.",
            "So what we really care is the discriminant function, right?",
            "So you don't want to spend a lot of effort in learning the joint distribution on inputs and outputs.",
            "So what you want to do is model the conditional distribution of Y given X. OK, that's what you care about.",
            "So logistic regression regression example.",
            "Or you could also learn a function that Maps inputs to the outputs, so that's the discriminant function.",
            "That's all you care about.",
            "OK, so this is discriminative learning, so I will mostly talk about discriminative learning."
        ],
        [
            "OK, most of the methods and discriminative learning can be seen through the.",
            "Framework of regularised risk minimization.",
            "You might be knowing this.",
            "So in regularizers risk minimization you have a loss function and then you try to minimize this loss function in addition to the loss function you have a term that you know it controls the complexity of your model.",
            "OK, so that's Omega is a regularizer, the L is the last function an your function is coming from some Hilbert space.",
            "This is this is, you know, in an abstract sense.",
            "And Lambda is a regularization parameter that rates of the loss function and the complexity parameter of your model or."
        ],
        [
            "Ocean.",
            "So here is a very simple example that you know.",
            "Linear classifiers, so in linear classifiers the function is just a dot product of.",
            "Weight vector, which is the parameters that you want to estimate and some features on X. OK, and then you want to minimize some loss function.",
            "Given the labels and the inputs.",
            "And the regularizer could take the form of for example L2 norm of the weight vector.",
            "You could also have other forms of regularizers here, but the regularization itself is not important for this tutorial.",
            "OK, so let's let's just stick to L2 regularizers.",
            "L2 norm of the weight vectors?",
            "And the file is just a feature map which Maps the input space with some features."
        ],
        [
            "OK, so in structured prediction, so this is important.",
            "Let's see what the difference is between structured prediction and the standard supervised learning in structured prediction.",
            "Again, you have an input space in output space, but the output space is complex.",
            "OK.",
            "So it could.",
            "It could be trees or sequences, or some combinatorial structure.",
            "Some special structure, so the output space is complex.",
            "You have M examples.",
            "Of inputs and outputs drawn IID from some distribution now.",
            "The goal is to learn a joint scoring function on the inputs and outputs.",
            "So this is the crucial difference.",
            "In supervised learning.",
            "If you recall, you were learning a function that Maps the inputs to the outputs.",
            "Over here you learn a joint scoring function that Maps input output pairs to some real valued number.",
            "An intuitive what this means is that the joint scoring function is some kind of an affinity measure, and which tells you how good the structures are for a particular input.",
            "OK if.",
            "For example, if the parse tree corresponds well to the given sentence, then it will have a very high score.",
            "If it if it has some missing edges or if it is incorrect, then it will have a very nice low score.",
            "So that's the semantics of the scoring function.",
            "And of course, ultimately you don't want to.",
            "You know, just learn a scoring function, right?",
            "You want to do predictions?",
            "You want to predict wise an that is done according to the following rule.",
            "So given a scoring function, you simply.",
            "Output the structure with the higher score, so that's that's an optimization problem.",
            "OK, so argmax of the function over your entire entire output space for linear models.",
            "It's just the weight vector.",
            "The DOT product of the weight vector and the features.",
            "One more crucial differences which I forgot to tell you is the features are defined on the inputs and outputs OK.",
            "In supervised learning in the standard setting you have the features defined on the inputs, but over here the fees are.",
            "The map of the fears map input output pairs to some features.",
            "OK, so this is this is an important slide, is the notation clear?",
            "Any questions?",
            "There are other ways to do it, and I have some slides on that so, but this is this is kind of the state of the art, yeah?",
            "For example, I'll talk about that there's an algorithm which doesn't really follow this pattern."
        ],
        [
            "Alright, so join feature Maps.",
            "As I said in structured prediction you have to.",
            "Extract features from the inputs and outputs so fine Maps input output pairs to some feature space.",
            "And then.",
            "You also need to have a joint input output kernel, so the kernel is not just on the inputs like in SPMS.",
            "Now the kernel is on input output pairs.",
            "OK, which is just the dot product of the features in some Hilbert space.",
            "OK, is this clear OK?"
        ],
        [
            "OK, here is a very simple example.",
            "Multi Label classification I I believe you are aware of multilevel classification.",
            "So let's say the features are coming from the N dimensional Euclidean space and the labels are zero and vectors of dimension D. OK, Ann and very simple way to design features is just take the chronicler of the tensor product between the axes and wise so you just replicate the features and then you can set them to zero or one depending on the label.",
            "OK, and then the tensor product kernel actually corresponds to factorization of the joint input output kernel into the kernel on the inputs and the kernel on the outputs.",
            "So technically it simplifies the optimization problem, but we won't go into those details now.",
            "OK, so here is.",
            "This is a very simple example of how you might define features on the inputs and outputs.",
            "OK."
        ],
        [
            "Alright, so so that's I'm done with the preliminary, so that's the notation you should be aware of.",
            "Now, let's talk about let's first talk about loss functions.",
            "I will start with the loss functions that you use for binary classification or regression, and then I'll tell you how you could, you know, extend them for structured prediction, let's."
        ],
        [
            "To begin with.",
            "OK, so that's the component that I'm going to talk about.",
            "The last fun."
        ],
        [
            "Chen let's begin with the 01 loss.",
            "For binary classification, the commonly used loss function is 01 loss.",
            "So if the two levels are the same, the loss is 0.",
            "If not, it's one.",
            "OK, this is this is clear for this structured case.",
            "This is how it is defined.",
            "So you might notice that it's defined with respect to an input output pair and also the scoring function.",
            "OK, the L Max Delta of F, X&Y.",
            "Which is nothing but.",
            "The DD is a discrete loss function on the output space, so for example if you are if you have trees, you might want to have a loss function on trees.",
            "OK, and it's typically it's a discrete loss function and the way the loss itself is defined as.",
            "Is given there, so you you take the argmax over the entire output space.",
            "Given the scoring function and then you see how far it is from the true label.",
            "That defines your loss function.",
            "OK, so you see how you remove from binary classification to structure prediction, so this is typically the last function that you want to minimize.",
            "Unfortunately, both the 01 loss and also the structure loss in the discrete case there are non convex and non differentiable, so you cannot really optimize them.",
            "And therefore you have to use some target losses and these are glasses are convex upper bounds OK.",
            "Hinge loss is an example.",
            "Lee Squares Squared Loss is an example.",
            "Logistic loss is an example.",
            "OK, is this clear?"
        ],
        [
            "So squared loss squared loss is used in regression problems.",
            "It's just the square difference of the labels.",
            "The extension to structure prediction is nontrivial.",
            "Ann Thomas will talk about an algorithm in Part 2 that actually minimizes an extension of squared loss for the structured prediction case, so I will.",
            "I will leave it here, so more on this will come later.",
            "So that's the squared."
        ],
        [
            "Pause.",
            "And this is a.",
            "This is the popular hinge loss.",
            "For binary case, it is defined as the Max of 0 and 1 -- y * Z An for the structured case it's defined over there, so so the idea is that Delta of XY.",
            "So you have to make sure that the correct label is.",
            "The structure with a higher score is higher than all the other incorrect pairs, and they should differ by the Delta, which is which has the meaning of a margin.",
            "This will become clear when we talk about SVM's.",
            "OK, and one more important thing is this hinge loss and also the squared loss.",
            "There are convex upper bounds on the binary discrete loss functions that we saw earlier."
        ],
        [
            "So logistic loss is defined as given there.",
            "This is used in logistic regression.",
            "It's just the negative log likelihood of your exponential family that he used in logistic regression.",
            "And for a structured prediction, it has a natural extension, the main difference being that you have to compute the sum over the entire output space and you have to make sure that it is actually greater than.",
            "It is less than the true input output pair OK, and that will give you how far.",
            "That gives you a measure of how far the predicted label is from the true label."
        ],
        [
            "Similarly, exponential loss, which is which is minimized by variant of Adaboost.",
            "Again, if you see that the main difference is you have to compute the sum over the entire output space and then compare it with the true input output pair and that gives you a measure of similarity or dissimilarity."
        ],
        [
            "Alright, you must.",
            "You must be familiar with this picture, So what I just want to show you here is.",
            "You have this 01 loss and all the other loss functions.",
            "I don't think it's very clear here.",
            "All the other loss functions are convex upper bounds.",
            "OK, so you typically end up minimizing those convex upper bounds.",
            "You can see the squared loss, the hinge loss, the logistic loss and also the exponential loss there."
        ],
        [
            "Alright, now we have seen the loss functions.",
            "Let's see how who could use how we could minimize this loss function, and these gives rise to different learning algorithms or parameter estimation."
        ],
        [
            "OK, let's let's begin with perceptron and then move on to structured Perceptron."
        ],
        [
            "Alright, so positron?",
            "It's an online learning algorithm.",
            "It learns a linear classifier and the predictions are performed according to the the sign of the dot product of your weight vector and the input and the algorithm itself proceeds in rounds.",
            "So you first initialize it to some zero vector.",
            "And in each round you receive an input, you predict according to the sign of the dot product between the weight vector and the input.",
            "You receive the true label, which is plus one or minus one.",
            "And if the labels differ, you perform some update.",
            "OK and they updated it given there, and it's very natural because let's say you are making a mistake and the true label is plus one, but you predicted minus one.",
            "In this case you want to increase the weight vector and that's what the update rule does.",
            "Similarly, for the other case, if if you the true label is minus one and you're predicting plus one.",
            "You have made a mistake and then the update rule follows naturally.",
            "OK, so that's how this is done."
        ],
        [
            "This is a well known result.",
            "There is actually a mistake bound which says that if the data is linearly separable, so the positives and the negatives are well separated by some margin gamma.",
            "If this is the case, then the number of mistakes made by the algorithm is actually bounded by our by, whole square OK, and this is again intuitive because if the data is well separated, which means a comma has a very high value, then the number of mistakes you make is very low.",
            "OK, and this R is just a.",
            "It's just the direct, it's just the already the norm of the weight vector so.",
            "The most important thing is the comma.",
            "How well your data is separated.",
            "And of course."
        ],
        [
            "Um?",
            "Data is not always well separated.",
            "There might be noise in the data, in which case you know you might have to do something else.",
            "Um?",
            "In these cases, the bound differs by a small quantity which is given by DD is actually a measure of how inseparable your data is.",
            "OK, and it appears in the numerator, so if the data is not separable you will be making a lot of mistakes.",
            "OK, so that's the only difference between this bound and the bound.",
            "In the previous case.",
            "OK, so these are relevant results."
        ],
        [
            "And now we will see how you could.",
            "You could extend Perceptron for the structured prediction case.",
            "So let me begin with what is called constraint classification.",
            "So now we are entering into the structured prediction setting.",
            "Constraint classification is one of the early algorithms that extended perceptron for structured prediction problems.",
            "It was specifically designed for label ranking problems.",
            "But let's let's consider multiclass prediction, which is actually an example of label ranking.",
            "In multiclass prediction, the output space is just D labels.",
            "And here what you want to do is learn the weight vectors right?",
            "And predict according to the following rule.",
            "So what you do is take the dot product of the weight vectors and the input for all the labels.",
            "And then just reading the label with the higher score.",
            "OK, so this is the prediction rule."
        ],
        [
            "And here is the update rule that this is an extension of Perceptron and here is here is the algorithm.",
            "Again, initialize all the weight vectors to 0.",
            "And it precedes and algorithm operates in rounds.",
            "In every round you receive an input and some label.",
            "And now in Step 2 you you do the following construction.",
            "Imagine our bipartite graph, a directed bipartite graph, where on one hand on one side you have the true label on the other side you have the you know the incorrect labels and you have a directed edge from the correct labels to the incorrect levels.",
            "Now in step three what you do is consider every edge in the bipartite graph.",
            "And then check whether you know for every pick you check whether the dot product with the input is for P is greater than zero or not.",
            "If it is less than zero, you have made a mistake.",
            "Right, so P is the true level.",
            "Q is the incorrect level.",
            "And if WP Times X3 is less than W Q * 60, you have made a mistake.",
            "OK, and in that case you have to perform.",
            "You have to do something and that's the update rule and the update to update rule is again natural.",
            "So since you have made a mistake for the weight vector corresponding to P, you want to promote this.",
            "You want to increase the values and you do that simply by adding the features to the X to the weight.",
            "And for the other case, for Q you simply demote the weight vector, so you subtract X from WQ and you do this for all the edges in the bipartite graph.",
            "OK, so this is more like ranking right?",
            "And this is this actually false multiclass problem.",
            "It's different from one versus all.",
            "Because you are considering pairs of labels here.",
            "It's also different from pairwise classification, because in pairwise classification you learn all of these square models of these square weight vectors.",
            "Here you're learning only divide vectors.",
            "OK, so that's a very crucial difference.",
            "And you should see how this is a natural extension of perceptron.",
            "For structured prediction, OK, it's not really structured prediction, it's multiclass prediction an.",
            "Is this is this clear?",
            "This is important.",
            "OK, so now we have moved from binary classification to multiclass classification.",
            "And why are multiclass classification?",
            "We will go to structured prediction, so that's the structure perceptron."
        ],
        [
            "OK, so now we will use the standard notation.",
            "So there is a linear scoring.",
            "You want to learn a linear scoring function.",
            "You have a features you have features on inputs and outputs, and the weight vector.",
            "This is again an online learning algorithm so.",
            "It operates in rounds.",
            "You initialize the weight vector to be 0.",
            "In every iteration, you first receive the input.",
            "And Step 2 is important.",
            "You predict according to the rules that I showed before.",
            "OK, so you have the learning you have the scoring function.",
            "You have the weight vector so you have the scoring function and then you just output the vector which has got the higher score.",
            "That's the prediction.",
            "And now you receive the true label.",
            "If it is incorrect.",
            "OK, you don't have to check that condition because it's implicit in Step 4.",
            "General is the following, so you just add the true the features corresponding to the true input output and the features corresponding to the incorrect input output pair.",
            "So you you have to see that in Step 4 this is more like the promotion, demotion, update that I described earlier.",
            "So you are making sure that the features corresponding to the true input output are getting added to the way traitor and the features corresponding to the incorrect label are getting subtracted from the vector.",
            "So the idea on you have the promotion step and also the demotion step in one shot.",
            "OK, so that's the extension of positron and also the IT follows wire constraint classification.",
            "Is this clear?",
            "The crucial difference, of course, is how you predict.",
            "Any questions so far?",
            "Alright, so OK. Now you might say this is this is this is pretty simple right?",
            "So we have a perceptron.",
            "It's easy to implement and now we have structured perceptron.",
            "The update rule is looks very similar and we can easily implement it.",
            "So we're done.",
            "We're done with structured prediction, right?",
            "But that's not the case because in Step 2 this there is this argmax problem.",
            "I will call it argmax problem from now onwards, and that's that's really important.",
            "That's that's where you have to spend a lot of effort.",
            "It depends very much on the application.",
            "So depends on the complexity or the technical challenge.",
            "In structured prediction arises from the fact that you have to solve this inference at some point of time in your training procedure.",
            "In your learning algorithm and this this will you will see that this appears everywhere.",
            "Depending on the application, you might have to design specialized algorithms to solve the argmax problem.",
            "Once you have this argmax problem, fixed everything else follows naturally.",
            "OK.",
            "So that's where the difficulty lies.",
            "The difficulty lies in Step 2, but for this tutorial I will treat it as a black box."
        ],
        [
            "OK, similar to Perceptron we have mistake bounds.",
            "For structured perceptron too, it looks very similar.",
            "In fact it's the same thing.",
            "For the separable cage, you see that the number of mistakes made on sequence of examples is at most R-squared by, Square.",
            "Where gamma is, you know that the margin as defined."
        ],
        [
            "Here.",
            "And again for the interpret, because you have something very similar to perceptron, you have the.",
            "The D it gives you a measure of how inseparable data is, and that it appears in the numerator.",
            "So if the data is highly inseparable, then you're you know you run into problems and then you have the margin appearing in the denominator.",
            "OK, so.",
            "These are the mistake bounds for."
        ],
        [
            "Doctor perceptron?",
            "OK, so now we are done with Perceptron.",
            "OK, let me just.",
            "Recapitulate we we started with perceptron and motor structure prediction.",
            "Why are multi class classification?",
            "OK now let's let's see how this can be done for regression problems.",
            "So I will start with regression least squares regression that everybody is familiar with and then I will show you how to extend it for structured prediction which which is called kernel dependency estimation.",
            "Katie is Colonel dependence."
        ],
        [
            "Estimation.",
            "OK regularise miss progression.",
            "So you are trying to minimize the squared loss and then you have a L2 norm on the weight vector.",
            "The solution can be written down in closed form.",
            "It follows from basic math.",
            "Um?",
            "And in over their access, the data matrix or the input matrix?",
            "And why is the vector of all the labels and I is the identity matrix just to make sure that you understand the notation.",
            "So just the the difficulties just inverting the matrix.",
            "I mean, if you want to close from solution, otherwise you have to use gradient descent.",
            "OK, so this is this is clear.",
            "And then of course there is also a kernelized version for this.",
            "And now instead of the weight vector you have, you're trying to learn a function coming from the Hilbert space.",
            "And there is you can there is represented Theorem which tells you that the function is just dot product of seas.",
            "Seas are the kernel expansion coefficients and dot product between the seizan the kernels, the kernel values, an opty one can be written in the form of using kernels, and again you have a closed form solution with just amounts to inverting the kernel matrix multiplied with the weight vector.",
            "OK, so this is this again standard.",
            "This is regression actually.",
            "OK, now now let's see.",
            "How we could extend this?",
            "Um, let me just go back.",
            "So one natural way to extend rigorously squares for multivariate prediction or structure prediction is so we have the X is and why is there right and wise?",
            "In that case, it's just a single value.",
            "You could simply consider vectors there and then you could write down a closed form solution.",
            "So instead of why there, which appears in the solution, you will just have a matrix of response variables.",
            "But the problem is this is equivalent to solving independent regressors.",
            "You're not taking the correlations of the structure into account.",
            "OK, so that's not really the solution.",
            "When you move to vectors."
        ],
        [
            "So you may have to do something else."
        ],
        [
            "And which is what is done in kernel dependency estimation.",
            "In KDE, the first step is to define a feature map for the output space.",
            "OK, so this is again important, so you have to define PSI that Maps the outputs to some features.",
            "So, so let's say we have some Hilbert space on the rise with the corresponding kernel.",
            "On the output.",
            "And this is just the dot product of the features on the wise.",
            "And note that now it is possible to consider a large class of nonlinear loss functions in the output space because of the kernel.",
            "OK, it's it's a kernel method.",
            "Now here is the main idea.",
            "What Katie does is the following.",
            "It uses kernel PCA, kernel, principal component analysis to decorrelate the output.",
            "And then trains independent are large independent regressors.",
            "OK, so that's how you take the correlations of the output space into account you first decorrelate the output using some technique and once you have the code later, the output you could learn independent regressors and we know how to do that using least squares regression.",
            "OK."
        ],
        [
            "OK so here is how the algorithm works.",
            "So you first have to decompose the output into, let's say K orthogonal directions using kernel principal component analysis.",
            "And then what you do is you learn independent regressors for each of these directions using regularize least squares.",
            "But now this is not the end of the problem and the solution because the predictions that are coming from Step 2 is in a transformed space, right?",
            "You have to go back to the original space, and that is done.",
            "Using that is done by solving the preimage problem, which is actually given there.",
            "So you just try to minimize.",
            "Two vectors and these vectors are one vector, is just your predictions from your least squares and the other vector is the vector of projections.",
            "OK, so the V is are the projections the dot product of V ANSSI are the projections.",
            "And you just compare it.",
            "You just minimize the L2 norm of these vectors and then output the minimum value of it and that will give you the output in the in the original space that you want.",
            "And of course, this is again, I'm treating this as a black box because this is, this is not.",
            "It may not be easy to solve this problem the preimage problem.",
            "It depends very much on the application.",
            "OK, so this is, you know it's like similar to the inference problem.",
            "Is this clear?",
            "Well, if you decorrelate the output, you can treat the response variables as independent and then learn independent regressors.",
            "If they are dependent, you can do regression.",
            "Oh, you mean to say that even if the outputs are correlated, you want to just learn regressors on them?",
            "That won't work, right?",
            "That's not going to work.",
            "In fact, that is the problem.",
            "Anne.",
            "And this is not just.",
            "Yes, there are people you stick people.",
            "Yes, before they do regression.",
            "I think they used techniques like Canonical correlation analysis and they don't correlate the outputs in some sense.",
            "They have to be aware of regulating the outputs, otherwise it won't give you good results.",
            "Anymore questions.",
            "OK."
        ],
        [
            "Oh OK, so here is a small technical node, so this is a kernel method, right so?",
            "You don't really, so I said that we have to have a feature, a feature representation for the outputs, but you don't need an explicit representation.",
            "So the projections can be computed directly using the kernel values, which is actually given there.",
            "So all you need to do is take the kernel on the output space, do some centering operations, perform eigenvalue decomposition on it, and then once once you get this alpha's, which are the eigenvectors, you could compute the projections directly using the equation that is given there.",
            "So you just need to define a kernel on the outputs.",
            "You don't really need an explicit feature representation.",
            "OK, so that's just a technical."
        ],
        [
            "Note OK, so we are done with regression.",
            "So we I told you how you could extend regression to the structured prediction case which was the kernel dependency estimation.",
            "Now let's see how you could extend support vector machines to the structured setting."
        ],
        [
            "OK, so you must be familiar with this, so support vector machine, it's it's.",
            "It's large margin classification.",
            "Again, you have a.",
            "Let's consider a linear classifiers.",
            "For now you have a weight vector.",
            "You have some features, it minimizes the hinge loss and the optimization is given.",
            "There you minimize the hinge loss and then also an additional term which is the L2 norm.",
            "Typically you also introduce Slack variables to take noise into account, and then this is the optimization problem.",
            "OK, so this is."
        ],
        [
            "Is the notation.",
            "You can write down the dual.",
            "There are some advantages of writing down the deal.",
            "You could solve nonlinear problems.",
            "It has the optimization problem is clean because you have box constraints there.",
            "It's easy to solve.",
            "And then again, you have a representative theorem which tells you that the function is actually the dot product of the kernel expansion coefficients and the kernel values.",
            "OK, so this is support vector machine into slow."
        ],
        [
            "Right now, let's see how we could extend SVM's to a structured SVM or Max margin Markov network.",
            "So structured SVM minimizes the structured hinge loss.",
            "And there are two formulations.",
            "One is called the slack rescaling and the other one is called the margin rescaling."
        ],
        [
            "Anne.",
            "Let's start with Slack rescaling, so here what you tried to do is you minimize the L2 norm of the weight vector.",
            "And.",
            "Subject to the following constraints, so these constraints are important and here, here, here in lies the main difference between structured streams and SVM.",
            "So for every input output pair, correct input output pair you have to make sure that it gets a score higher than all the other in all the other input output pairs, right?",
            "So that's what the constraints tell you.",
            "And the constraints are typically exponentially number.",
            "OK, and the reason why it is called Slack reskilling, it's because.",
            "You re scale the slack variable for every instance with the loss function.",
            "So recall OK, structured SVM.",
            "This is minimizing the structured hinge loss.",
            "But then in addition to that you have the discrete loss function right?",
            "And that has to appear in some way, and that's it appears in the denominator of this variable.",
            "So this means that every you know you're forcing the true input output pairs to get a score with a margin.",
            "Given by 1 minus you know the slack variable.",
            "Is this clear?",
            "So you see the difference between SVM and structured SVM.",
            "OK."
        ],
        [
            "And then there is another formulation which is called margin rescaling.",
            "So here what you do is instead of having a margin of one, you replace the margin with the loss function itself.",
            "So this means, let's say a particular why is is is very far away from the true Y and and therefore you know the scoring function for the true input output should have a very.",
            "You know it should exceed the incorrect one by a margin which is given by the loss between the two Y and the incorrect, why?",
            "OK, so that's how the margin is defined for this setting.",
            "So the only difference between margin rescaling and slacker scaling is the way you re scale this.",
            "The way you introduce a lot the last function to influence the setting.",
            "Yeah, any questions.",
            "OK, I thought I saw a hand go up OK.",
            "So there are there there are OK Now you might ask what is the OK?",
            "Why do we have to?",
            "Formulations, margin rescaling and Slack rescaling so there are some advantages and disadvantages to both these settings.",
            "Margin rescaling is easier to solve technically, because as you will see in the later slides, it's it's.",
            "It amounts to solving the inference problem that you use for prediction.",
            "Anne."
        ],
        [
            "Slacker is killing is it's actually difficult to optimize, but on the the advantage of slack rescaling is it's invariant to its.",
            "The IT is.",
            "Invariant too.",
            "The scale of the loss function itself.",
            "So if you scale your loss function.",
            "This will not get effect."
        ],
        [
            "But the margin rescaling will get affected.",
            "And the other problem with margin rescaling is you spend a lot of effort in incorrect input outputs.",
            "Because, you know, it's actually the way the loss function is influencing the margin.",
            "So if you are the incorrect input, output is far away, then you are spending a lot of effort in making sure that you know the true input output is getting a very high score.",
            "So you are spending a lot of effort, so you don't want to do that.",
            "So there are advantages and disadvantages of both these formulations.",
            "So depending on your problem, you might want to consider using one of these."
        ],
        [
            "Alright, so so so here is the main problem.",
            "We have seen that there are exponential number of constraints, right so?",
            "If you look at those constraints.",
            "The constraints are for all the instances and for all the elements in the output space.",
            "OK, so that's typically exponentially number.",
            "So how do we solve the problem?",
            "So the idea here is there is a technique called cutting plane method which uses the following idea.",
            "It suffices to design A subroutine which is called as augmented in French.",
            "To compute.",
            "The quantities that are shown there.",
            "So the first one is for.",
            "The slack reskilling formulation and the second one is for the margin rescaling formulation and we will see in the later slides how we could use this subroutine to solve the optimization problem.",
            "It eventually boils down to iteratively computing these most violated constraints and adding them to the optimization problem, and that's that's enough, and you could solve the problem to design precision as well."
        ],
        [
            "See now.",
            "So here is the cutting plane method.",
            "So I want you to focus on Step 5, which is the inference problem.",
            "Step 5 and step 6 so.",
            "As I told you, it suffices to solve the inference problem.",
            "There, the loss augmented inference.",
            "Which is which is typically tells you it gives you the most violated constraint, right?",
            "And then depending on the condition which is given in step eight, you keep adding these most violated constraints an.",
            "You can show that polynomial number of constraints suffices to actually solve the problem to decide precision.",
            "OK, so you don't really need exponential number of constraints.",
            "If you have a subroutine which computes the most violated constraint, you could use that to bring down the complexity of the learning optimization problem, and this is the cutting plane method.",
            "Is this clear?"
        ],
        [
            "Alright, so as I said, we have some theoretical guarantees.",
            "The first one is polynomial time termination, so the algorithm that I showed you in the previous slide, it terminates in a polynomial number of iterations because you are only adding polynomial number of constraints and every step in the algorithm runs in polynomial time, so you only have polynomial number of iterations.",
            "And the algorithm is correct in the following sense.",
            "It's also optimization problem accurately to a desired precision epsilon, which means that all the exponential number of constraints are satisfied to the desired position.",
            "So you don't really solve the problem to the optimal.",
            "You don't get the optimal solution, but you can, you know, bring it down as close as possible depending on the precision parameter, which you will give.",
            "And then there is again in standard empirical respond which the some of the slack variables have a bunch.",
            "The empirical risk.",
            "And.",
            "One thing that you should keep in mind is these theoretical guarantees.",
            "Are for the case where you have an exact where you can do exact inference.",
            "So if you can compute if you can do step 6 exactly, so that's exact inference, and then you know all the theoretical guarantees follow.",
            "If not, then you run into problems.",
            "OK, and that's the focus of Part 2 of the tutorial."
        ],
        [
            "Alright, and I think I'll skip this right?",
            "So there is also a kernelized version for structured SVM's.",
            "You can.",
            "There is also represented theorem.",
            "And one thing you might want to notice is that the kernel is now on the pairs of inputs and outputs.",
            "And the alphas are your dual variables anyway."
        ],
        [
            "I'll skip this slide.",
            "OK, at this point I want to.",
            "Revisit structured perceptron.",
            "Because if you if you recall.",
            "Structured both run in Step 2.",
            "You are solving the inference problem right?",
            "But you might notice that the loss function Delta doesn't appear anywhere.",
            "OK.",
            "So in in SPMS and structured as firms, you saw how the loss function influences the learning procedure.",
            "It actually influences the margin.",
            "But in structured perceptron.",
            "It appears nowhere right in Step 2.",
            "You're just minimizing, you're just solving the inference of the plane inference problem.",
            "So this this might create some problems for some applications, so you really want to take the last function into account.",
            "So how do you do that?",
            "By a simple."
        ],
        [
            "And which is replace step two with the loss augmented inference.",
            "OK.",
            "In step one, you receive the input output pair and predict and the prediction is done.",
            "Installed inference problem by also taking the loss function.",
            "So now you have you have an online learning algorithm which also takes the loss function into account.",
            "So you can see this as the online version of structured SVM.",
            "OK, but of course you it, it's up to you to design A subroutine that solves a loss.",
            "Augmented inference that might be difficult in some cases.",
            "But this is the crucial difference, and again the update rule is same and then you have a learning rate later.",
            "And this is you have an online version of structured SVM.",
            "This is also called the subgradient method in the literature.",
            "So that's lost.",
            "Structured perceptron with also admitted inference.",
            "Any questions?",
            "Yeah.",
            "The loss function in the kernel.",
            "Yeah, that's right, that's true actually.",
            "And if you if you are only using in fact if you are using the kernel on the outputs as a loss function, that's what Katie does.",
            "The kernel dependency estimation.",
            "But over here you also have the loss specifically being used to influence the margin.",
            "So it's it's actually did know know you could have the same thing.",
            "But typically you you have a different kernel and also.",
            "A loss function.",
            "Yeah, but that's that.",
            "A good question.",
            "More questions.",
            "Alright."
        ],
        [
            "So.",
            "So where are we?",
            "So we were we talked about.",
            "Support vector machines.",
            "We moved to structured SVM's.",
            "And then I also told you how you could, you know, modify structured Perceptron 2.",
            "Introduce the loss function into the.",
            "Update rule.",
            "Now there there are other formulations to solve the optimization problem in structured SVM and here is 1 formulation which is called the min Max formulation.",
            "So here is how it works.",
            "So recall the brute force enumeration this is.",
            "This is what I showed you in the earlier slide, right?",
            "You have the L2 norm.",
            "The objective in the objective and then the constraints.",
            "You have exponential number of constraints.",
            "Which.",
            "Which forces all the true input output pairs to get a score higher than the incorrect ones.",
            "OK, so now how do you solve this?",
            "We have seen one way to solve this, which is the cutting plane method.",
            "And here is another way to solve this."
        ],
        [
            "Problem.",
            "The Min Max formulation.",
            "What it does is.",
            "You replace the exponential number of constraints into one constraint, one constraint for example.",
            "And how do you do that?",
            "You simply take the Max.",
            "OK, so this is equivalent to.",
            "You know, enumerating all the exponential number of constraints?",
            "OK, you see the difference between this set of constraints and the constraints in the previous slide.",
            "OK, and now once you have written it down as a Max over that expression, you have M number of constraints.",
            "But again, you have to solve this inference problem.",
            "So what you can do is for some problems.",
            "You could replace this Max problem as an LP inference.",
            "Linear programming inference.",
            "OK, so for a certain combinatorial structures you can plug in LP inference there and you write down the dual using the LP duality.",
            "You replace the Max with men.",
            "And now you have a minimization throughout.",
            "Independent for certain structures, you could it so happens that you could rewrite the entire optimization problem as a concise QP.",
            "And by concise I mean you have polynomial number of.",
            "Constraints and variables.",
            "OK, so that's the min Max formulation.",
            "OK, did I'm not explaining the steps in detail, but this is the idea.",
            "At a high level.",
            "So there are some problems like matching that you could you could actually solve this so you don't really need to solve inference problems in every iteration like structured SVM does.",
            "You could write it down as a concise QP and solve the whole thing in one shot.",
            "OK, so that's the min Max formulation."
        ],
        [
            "OK, so we are done with large margin classifiers for structured."
        ],
        [
            "OK so the only thing that is left is logistic regression and how we could extend it to conditional random fields.",
            "Any questions?",
            "Yeah.",
            "Oh, OK. What what did I have there?",
            "This is this is for.",
            "Is this for the margin rescaling?",
            "It's actually OK in the paper it's defined for margin rescaling.",
            "Well if you have slack reskilling then you may help to do something too.",
            "Solve the math problem or write it down.",
            "I'm not sure if you could plug in an LP influence.",
            "It depends on your structure.",
            "Yep.",
            "But conceptually, you could have also this like variables.",
            "This, like rescaling formulation there.",
            "Any other questions?",
            "OK, so let's let's get into logistic regression and CRF's.",
            "How much time do I have 25 minutes?",
            "OK, let's see if I can finish this."
        ],
        [
            "Logistik regression, as you all know, is a probabilistic binary classifier.",
            "So.",
            "The likelihood function is is given there.",
            "It's basically an exponential family model, so it's here what you're trying to do is.",
            "You are trying to predict the probabilities right?",
            "It's not just plus one or minus one.",
            "You want to predict the probability that the label belongs to plus one or minus one.",
            "And I have written down the likelihood function in a slightly different way than what you might find in the literature, but this is for convenience and you will see how you could extend this to a structured prediction.",
            "So in the numerator.",
            "You have the exponent over the dot product of the scoring function multiplied by the label and the denominator is just a normalizer.",
            "It takes care of it.",
            "Make sure that your probability is a proper probability distribution, right?",
            "So you have a plus Y and also minus Y there.",
            "For the for the binary class.",
            "OK, and how do you estimate the parameters?",
            "This is done by minimizing the negative log likelihood.",
            "The fear of Y given access.",
            "The log is the likelihood function you want to estimate the parameters that maximize this likelihood function.",
            "For technical reasons, you typically end up taking the negative log likelihood and minimizing OK.",
            "This."
        ],
        [
            "Try just regression before I extend this for the structured prediction, guess I have to talk about exponential family distributions just a few slides on exponential family you might.",
            "You might already know this.",
            "So exponential family is a family of probability distributions.",
            "It's the distribution itself is given there.",
            "The fees are, you know it.",
            "They're called the sufficient statistics.",
            "And the Z of W is what is called the partition function or the numerator that you know.",
            "Make sure that you have a proper probability distribution, and this is typically.",
            "OK there I have an integral, so for the discrete case you have a summation.",
            "OK so I just use that interchangeably.",
            "So this is exponential family model.",
            "Um, and note that I am.",
            "This is only for the inputs, right?",
            "So we're we're still be in the standard setting.",
            "We haven't gone through."
        ],
        [
            "Structured city there are some nice properties of exponential family and that's why people used it and.",
            "The one thing the first thing is so GFW is the logarithm is is the log of the partition function.",
            "So it's called a log partition function.",
            "The nice thing is G of W is accumulated generator, which means you take the derivative of this function with respect your weights or your parameters.",
            "You get the expectation over the features.",
            "And you take the second derivative of this function with respect to the weights.",
            "You will see that you get the covariance of the features.",
            "And why is it important?",
            "Because the second derivative is a covariance function, this means.",
            "The log partition function itself is convex.",
            "Right, and then when in so this is important?",
            "Because when you are minimizing the negative log likelihood, you typically end up with a convex optimization problem.",
            "And this is how it follows.",
            "OK.",
            "The GFW is a cumulant generator, the secondary is a covariance function, and therefore you know GF W is a convex function.",
            "And because she is convex, you know the original optimization problem itself becomes convex because all the terms are convex there.",
            "And that's exactly the reason why you know.",
            "People use exponential family models."
        ],
        [
            "So this slide is again important in structured prediction.",
            "What you have is conditional exponential family.",
            "OK, so here the family of distributions define the probability of Y given X.",
            "And the way it is written down is shown there.",
            "Now you have the features on the inputs and outputs.",
            "OK, previously you only had five of X.",
            "Now we have Phi of X&Y and then again you have the log partition function.",
            "So this is conditional exponential family.",
            "You see, the difference between this and the one I showed before, right?",
            "And this is how we move to structure prediction."
        ],
        [
            "OK, so I told you that you minimize to estimate the parameters you end up minimizing the negative log likelihood.",
            "But it might be helpful to introduce a prior on the weight vectors and then take the do what is known as map estimation.",
            "OK, typically people use a Gaussian prior on W. And when you take the logarithm, you see that it's basically the L2 norm of the weight vector.",
            "OK, and this is again regularised risk minimization.",
            "So you have an L2 norm and then you also have the negative log likelihood, which is the last function it actually follows from having Gaussian prior on the weight vectors.",
            "So now we know that all the terms in the optimization problem is convex, so you could solve it using off the shelf solvers."
        ],
        [
            "So conditional random fields is an exam is just an example of exponential conditional expansion.",
            "It's an example of a model that uses conditional exponential family.",
            "Um?",
            "Let's talk about linear chain conditional random fields.",
            "So the voice of the labels.",
            "So for example, device could be our part of speech tags and the X is are your words.",
            "Um?",
            "The probability there conditional distribution itself could be written down.",
            "It actually decomposes over the clicks and the clicks are the pairs of wise index and also pairwise labels.",
            "And then you can decompose the entire quantity into the submission over your the length of the sequence of the dot product between the clicks the features corresponding to the clicks and a weight vector corresponding to that particular edge.",
            "And the nice thing about linear chain having this particular chain so conditional random fields are not only for linear chains, you could have arbitrary graph structures.",
            "But then you you will have problems in computing the partition function.",
            "It so happens that for this particular chain like structure you could do inference using dynamic programming techniques like the Viterbi algorithm.",
            "And then of course you you can estimate the parameters using either maximum likelihood are map estimation.",
            "OK, so this is CRF's.",
            "Any questions here?",
            "Alright."
        ],
        [
            "OK, now there there is a fundamental difference between hidden Markov models and so your apps, right?",
            "So Hmm's are generated models and see our discriminative models.",
            "So in HMMS, what you try to do is maximize the joint likelihood on the inputs and outputs, which is not exactly what you want to do for conditional random fields.",
            "You only focus on the conditional.",
            "And the graphical models are given over there.",
            "OK, so this is your.",
            "Any questions here?",
            "Alright, so we are done with logistic regression an also conditional random fields.",
            "And this, I believe, is the last topic.",
            "Learning reductions.",
            "OK, so until now, what what I've shown you is how to extend binary classifiers or regressors.",
            "To the structured prediction case right now, here is here is something really interesting here.",
            "What learning reduction is what it tries to do is.",
            "It will, you're trying to reduce structured prediction to a binary classification problem, so it's in.",
            "It's the other way around.",
            "OK."
        ],
        [
            "So what is a reduction?",
            "So there was a tutorial at ICML last year by John Langford.",
            "You might want to have a look at that.",
            "So what is the reduction reduction is transforming complex learning problems into simpler problems.",
            "OK, so you have a structured prediction problem.",
            "Why do you want to design specialized algorithms to solve this problem?",
            "Or why do you want to extend?",
            "You know, existing algorithms to structured prediction cares, just reduce it.",
            "So that's the basic idea.",
            "So what is desired, of course, is.",
            "If you have good performance on the core problem, so if your binary classifier is really really good.",
            "It should somehow, you know, translate it to the complex problem.",
            "So good performance on the core problem should imply good performance on the complex problem, so that's where the theoretical guarantees come into picture.",
            "So there are lots of examples in the tutorial.",
            "There you can reduce multiclass prediction to binary classification.",
            "You can reduce ranking to classification and cost sensitive classification to manage classification.",
            "And this is what we're interested in this tutorial.",
            "You can reduce structured prediction to binary classification.",
            "That's what is called Scion.",
            "Search based algorithm for structure prediction.",
            "OK, let's have a quick look at what this is."
        ],
        [
            "So here is the idea.",
            "So what you want to do is reduce structured prediction to binary.",
            "So we are.",
            "We are going to assume that the outputs are decomposable, so outputs can be written down as a sequence of wise of length T for example.",
            "What team does is it learns a policy, so the policies you can treat this as a classifier.",
            "OK, it's a classifier that Maps couple tools of XY.",
            "To the future prediction.",
            "OK, and that she could be anything there.",
            "So it takes X and a sequence of wise and then it tells you what the next why is.",
            "It turns out.",
            "You could reduce the structured prediction to cost sensitive classification.",
            "It's actually cost sensitive multiclass classification.",
            "So that's what she does.",
            "And then there are very good performance guarantees.",
            "So we.",
            "There are good guarantees on custom classification and then this paper shows that these guarantees can translate your good performance on the structured prediction problem itself.",
            "OK, so you take a structured prediction problem, reduce it to consider classification, take the causative classification problem, reduce it to binary classification so you have a chain of reductions there.",
            "OK, so all you need is just a binary classifier and then you can solve structured prediction problems.",
            "And you also should know that there is no argmax problem here.",
            "There is no inference problem."
        ],
        [
            "OK, I won't go into the details of the algorithm, but I just want to quickly talk about the reduction itself, the reduction to consider classification.",
            "So here's how it works.",
            "So what what you want to do is you have a distribution on the structured examples, right?",
            "You have a distribution on ex wise for the structured prediction problem.",
            "From this distribution of examples you want to come up with a distribution over constant examples.",
            "OK, constant examples are, you know you have an input and a cost vector, which tells you what the cost is for predicting a particular label.",
            "OK, so let's say we have.",
            "Uh.",
            "KK level so each each WHI takes care values OK so the key is different from T. You should keep that in mind so T is the length of your.",
            "The sequence of the retract all the label vector in your structure prediction problem and each Y itself takes K values.",
            "OK, now here is how you come up with the distribution of over constant examples.",
            "For every tick.",
            "Every structure, particular example XY pairs.",
            "Now what you do is just sample some tea from the set of 130.",
            "And now run your policy or run your classifier.",
            "40 -- 1 steps to give you the predictions.",
            "OK, so these are given a policy and some tree.",
            "You you you.",
            "Predict from Y1 to Y 2 -- 1.",
            "And the input to the cost sensitive classification is generated using the steps three and three.",
            "So the input is just X and the wise.",
            "OK, so this is the input.",
            "And the cost.",
            "Now recall that for Cousins to classification you need to have a cost vector of length K right?",
            "And the costs are just the expectation or.",
            "The loss of the true label, and.",
            "And the future predictions.",
            "And then you notice that there is K in between.",
            "OK, so this this actually tells you what will happen if you predict K at the at a particular state.",
            "And in reinforcement learning terms.",
            "And that's exactly the cost.",
            "And in constant reclassification.",
            "So you might see that you know it actually generates a lot of examples.",
            "So this is actually done for.",
            "Notice that this is for every structured prediction example and for every key you generate a lot of examples.",
            "But then eventually you will only be solving a constant city classification problem, which we know can be reduced to bandit classification problem.",
            "So in in simple terms what you do is you take structured prediction examples, generate lots and lots of examples.",
            "Of constant classification and then reduce it to binary classification.",
            "You can still, you know you.",
            "You can solve binary classification for large datasets and therefore there are theoretical guarantees for this particular procedure.",
            "OK, so there are a lot of details that I'm skipping.",
            "You have to design an appropriate policy and you have to take care of the loss function there.",
            "But this is just to give you a high level idea of Sunworks.",
            "So this is this is really interesting because you know it's it's.",
            "Until now, I actually how you could, you know, move from binary classification to structured prediction.",
            "But this is this is completely different.",
            "You are actually moving from structure prediction to binary, which could be useful.",
            "It's actually solves a lot of NLP problems."
        ],
        [
            "Do you have any questions now?",
            "OK, we're almost done.",
            "So this is, I think this is this is this concludes part one, so we have seen different loss functions.",
            "Loss function for binary classification.",
            "Loss functions were structured prediction and we have talked about discriminative methods for structured prediction.",
            "I told you how to extend Perceptron to structured perceptron least squares regression to kernel dependency estimation, support vector machines, Destructors, VM's.",
            "Logistic regression to see our apps.",
            "And we have also seen the reverse direction which is reducing structured prediction to Carson City classification.",
            "OK, so that concludes part one.",
            "After the break, Thomas will take over for the Part 2.",
            "So and in Part 2, Thomas will mostly talk about what happens if you cannot solve the inference problem exactly.",
            "What are the implications of using approximate inference and what kind of problems do you face there?",
            "What is the complexity of learning an?",
            "Is it?",
            "Is it even possible to design learning algorithms that do not use any inference at all?",
            "So that's what we will talk about in the new training algorithms so it will talk about algorithms where you solve it slightly different, completely different problem which is different from, you know, the standard argmax imprints.",
            "So that's Part 2.",
            "OK, so.",
            "Any questions?",
            "OK, forgot to mention this.",
            "The slides for Part 2 are not yet online.",
            "So it will be made online after their tutorial.",
            "So if you check the website I have slides for part one and Part 2, but the slides Thomas is going to use a different from what is there on the website so.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Algorithms for predicting structured data.",
                    "label": 1
                },
                {
                    "sent": "My name is Shankar Rambo.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University of Illinois Urbana Champaign.",
                    "label": 0
                },
                {
                    "sent": "My Co presenter is Thomas Gardner.",
                    "label": 0
                },
                {
                    "sent": "He will talk take over during the second part.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before I get started.",
                    "label": 0
                },
                {
                    "sent": "Let me just say a few words about the focus of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So it's structured prediction is.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty huge topic with lots of applications, so if you go to an NLP conference or a computer vision conference, you will see.",
                    "label": 0
                },
                {
                    "sent": "Lots of papers on structured prediction and in most of those application papers you may have to modify existing algorithms in nontrivial ways to solve the problem at hand.",
                    "label": 0
                },
                {
                    "sent": "The focus of this letter, what I plan to do here is talk about the basic tools of the fundamental algorithms that are needed to solve structured prediction problems.",
                    "label": 0
                },
                {
                    "sent": "I won't go deep into applications.",
                    "label": 0
                },
                {
                    "sent": "There are specialized tutorials for structured prediction for NLP or computer vision.",
                    "label": 0
                },
                {
                    "sent": "At those conferences, you might want to have a look at them.",
                    "label": 0
                },
                {
                    "sent": "I'm right here, I'm just going to focus on the algorithms.",
                    "label": 0
                },
                {
                    "sent": "I will point to some applications whenever it's required, but I won't go deep into applications OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, and and one more thing, if you have any questions feel free to interrupt me.",
                    "label": 0
                },
                {
                    "sent": "I think that's how it is done here.",
                    "label": 0
                },
                {
                    "sent": "Don't don't postpone the questions to the end, I don't think that's going to work.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So structured prediction is.",
                    "label": 0
                },
                {
                    "sent": "It's the problem of predicting multiple outputs with complex internal structure and dependencies among them.",
                    "label": 0
                },
                {
                    "sent": "So this is this is different from.",
                    "label": 0
                },
                {
                    "sent": "Single valued prediction problems that you might be knowing, like binary classification or regression, where you are predicting only one output, a single variable.",
                    "label": 0
                },
                {
                    "sent": "In structured prediction you have multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "Um, it's again different from multivariate regression because the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Louder oh OK. Alright, is it OK now?",
                    "label": 0
                },
                {
                    "sent": "Great, OK, it's also different from multivariate regression, where only the correlations in the outputs are taken into account.",
                    "label": 0
                },
                {
                    "sent": "But the problem is in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "How much more complex structure?",
                    "label": 0
                },
                {
                    "sent": "So the techniques that are used to solve multivariate regression problems?",
                    "label": 0
                },
                {
                    "sent": "The techniques that you might find in statistics literature do not immediately apply to this problem, so that you may want to keep this in mind.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's let's have a quick look at a couple of applications.",
                    "label": 0
                },
                {
                    "sent": "Part of speech tagging is an application in natural language processing where.",
                    "label": 0
                },
                {
                    "sent": "The input is a sentence and the output is.",
                    "label": 0
                },
                {
                    "sent": "You may want to predict the part of speech tags for this particular, but the words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "This is structured prediction problem because you don't, you don't want to predict the labels independently.",
                    "label": 0
                },
                {
                    "sent": "You may want to take the sequential structure into account contact information into account when you're predicting these part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is another example.",
                    "label": 0
                },
                {
                    "sent": "Linguistic parsing again, is is an application in NLP.",
                    "label": 0
                },
                {
                    "sent": "The input is a sentence.",
                    "label": 0
                },
                {
                    "sent": "Again, the output is a power screen corresponding to this sentence.",
                    "label": 0
                },
                {
                    "sent": "Here you have a recursive structure or a tree structure, so you this is a combinatorial structure and you may want to take this structure into account when you are.",
                    "label": 0
                },
                {
                    "sent": "Solving this problem so the input space would be the space of all sentences and the output space is the space of all trees corresponding to this sentence.",
                    "label": 0
                },
                {
                    "sent": "So that's this linguistic parsing.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are apart from these applications, there are several well defined machine learning problems that can be solved using in a structured prediction framework.",
                    "label": 0
                },
                {
                    "sent": "Multiclass prediction is a very simple problem.",
                    "label": 1
                },
                {
                    "sent": "Um, in terms of structure prediction so.",
                    "label": 0
                },
                {
                    "sent": "Multi Label Classification is another example where given and given an input and a set of labels you want to predict a subset of these labels.",
                    "label": 0
                },
                {
                    "sent": "And then there is hierarchical classification, which is common in problems like document classification, where you have a taxonomy of.",
                    "label": 0
                },
                {
                    "sent": "Topics and you want to categorize documents based on this taxonomy so you have a tree structure.",
                    "label": 0
                },
                {
                    "sent": "And that's hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "Label ranking is again a well defined machine learning problem where for a given input and as a set of labels you you want to.",
                    "label": 0
                },
                {
                    "sent": "Predict a total order of these labels so it's not just one class or a subset of 1 label or a subset of labels you want to predict a total order, OK?",
                    "label": 0
                },
                {
                    "sent": "And therefore the speed output space is the space of all permutations of these levels, so that's labeled ranking.",
                    "label": 0
                },
                {
                    "sent": "So these are some of the machine learning problems, and nowadays people solve these problems in our structured prediction framework.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I said, there are several real world applications.",
                    "label": 1
                },
                {
                    "sent": "Natural language processing.",
                    "label": 0
                },
                {
                    "sent": "We have seen a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "There are also problems in computational biology, bioinformatics and lots of problems in computer vision and also robotics.",
                    "label": 1
                },
                {
                    "sent": "So there are several real world applications for structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "This is the outline.",
                    "label": 0
                },
                {
                    "sent": "There are three parts in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "In in the 1st and there are equally distributed, so the first part will be for 90 minutes, give or take a few minutes.",
                    "label": 0
                },
                {
                    "sent": "The second part will be for around 60 minutes and the third part is is shorter.",
                    "label": 0
                },
                {
                    "sent": "It sits for 30 minutes, so that's three hours of material OK?",
                    "label": 0
                },
                {
                    "sent": "In the first part I'll be talking about the first part in the first part.",
                    "label": 0
                },
                {
                    "sent": "What I'll do is I will talk about the basic algorithms.",
                    "label": 0
                },
                {
                    "sent": "I will so the plan is I will start with binary classification or regression and then I'll tell you how to extend these algorithms to us with the setting of structured prediction.",
                    "label": 0
                },
                {
                    "sent": "And in the second part, it's almost we talk about etc.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference, so you will see in the later slides that influence the so called inferences is an important ingredient of structured prediction and there are implications if you use either exact or approximate inference and Thomas will be discussing these problems that might arise there.",
                    "label": 1
                },
                {
                    "sent": "In part three, I will.",
                    "label": 0
                },
                {
                    "sent": "I will check over Part 3 where I have a couple of interesting applications, their applications and there really motivated the design for new algorithms.",
                    "label": 0
                },
                {
                    "sent": "Structured prediction algorithms.",
                    "label": 0
                },
                {
                    "sent": "It's weekly supervised learning, so it's it's not supervised or unsupervised, but the supervision is in in in a weaker fashion that's weakly supervised learning, so this is the outline.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I already said, the focus of the tutorial will be on algorithms.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus on discriminative methods in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, I will.",
                    "label": 0
                },
                {
                    "sent": "I will tell you what discriminative methods are in a couple of minutes, but I won't be talking about generator methods, so there is a distinction there.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is interesting, so this is a list of topics that I do not cover.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Um advanced optimization methods.",
                    "label": 0
                },
                {
                    "sent": "I will talk about optimization method and it's more than enough for you to get started.",
                    "label": 0
                },
                {
                    "sent": "But if you are looking for large scale applications you might want to consider more specialized algorithms so and I won't talk about advanced optimization methods, But if you need the references, I'll be happy to give them to you.",
                    "label": 0
                },
                {
                    "sent": "There are some connections to reinforcement learning and I won't be talking about them.",
                    "label": 1
                },
                {
                    "sent": "There was a tutorial at ACL this year by Holidome and he talked about structured prediction and the connections to reinforcement learning, so I recommend going through those slides if you're interested.",
                    "label": 0
                },
                {
                    "sent": "There aren't many learning theoretic results.",
                    "label": 0
                },
                {
                    "sent": "There are a few papers, but I'm not going to talk about learning theoretic results, but you can have a look at those papers if you're interested.",
                    "label": 0
                },
                {
                    "sent": "Inference in graphical models.",
                    "label": 0
                },
                {
                    "sent": "So as I already said, the so called inference is an important ingredient in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In most of the applications are in most of the problems.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you might have to do some sort of inference in in a graphical model, and there are lots of algorithms that talk about for doing inference in graphical models.",
                    "label": 0
                },
                {
                    "sent": "I won't be going into those details because there's a lot of structure and I definitely don't have the time for that, but I can tell you that there were a couple or maybe more more than two tutorials that computer vision conferences recently.",
                    "label": 0
                },
                {
                    "sent": "Maybe this year, I think last year and.",
                    "label": 1
                },
                {
                    "sent": "2008 they do a very good job and if you are interested in graphical models, inference in graphical models, I recommend going through those tutorials.",
                    "label": 0
                },
                {
                    "sent": "And finally, I won't be talking about Semi supervised learning and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, I will mostly focus on supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here this is the list of topics that I do not cover.",
                    "label": 0
                },
                {
                    "sent": "So I just wanted to let you know that you know the material is not completed.",
                    "label": 0
                },
                {
                    "sent": "It's only about algorithms.",
                    "label": 0
                },
                {
                    "sent": "But if you want to get a complete picture, you have to really go through applications and the tutorials, given it applied at computer vision, conferences or NLP conferences OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's get started with part one.",
                    "label": 0
                },
                {
                    "sent": "So the part one is structural binary to structure.",
                    "label": 1
                },
                {
                    "sent": "So as I said.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My plan is to begin with the basic binary classification algorithms, so I will talk about Perceptron and I will tell you how to extend it to structured Perceptron.",
                    "label": 0
                },
                {
                    "sent": "I will talk about regression, the standard regularised least squares regression, and I will tell you how to extend it to structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Similarly, I will talk about SVM's and then extend it for the structured prediction case.",
                    "label": 0
                },
                {
                    "sent": "I will talk about logistic regression and tell you how to extend it to structured prediction, which is conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "So Interestingly, also there is.",
                    "label": 0
                },
                {
                    "sent": "There is nothing new you know.",
                    "label": 0
                },
                {
                    "sent": "Structured prediction algorithms did not come out of the blue, so the existing algorithms can be naturally extended for the structured prediction case.",
                    "label": 0
                },
                {
                    "sent": "Of course you will face some technical challenges depending on the complexity of your structure, but that's where you know you have to spend a lot of effort in solving the problem.",
                    "label": 0
                },
                {
                    "sent": "At a conceptual level, all these extensions follow naturally.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the road.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Map.",
                    "label": 0
                },
                {
                    "sent": "Preliminary so before I even get started with structured prediction, let me just give you some background information so that we are on the same page.",
                    "label": 0
                },
                {
                    "sent": "As far as notation is concerned.",
                    "label": 0
                },
                {
                    "sent": "So I have a few slides on some basics.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeff, let's start with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So this is everybody knows this, but let me just.",
                    "label": 0
                },
                {
                    "sent": "Talk about this.",
                    "label": 0
                },
                {
                    "sent": "There is in supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We have an input space and an output space.",
                    "label": 0
                },
                {
                    "sent": "The input space is your features.",
                    "label": 0
                },
                {
                    "sent": "The output space for the for classification, it's just a set of two elements plus one and minus one for regression problems.",
                    "label": 0
                },
                {
                    "sent": "Is that set of real valued numbers?",
                    "label": 0
                },
                {
                    "sent": "And the assumption is that there is some underlying unknown underlying distribution on the inputs and outputs, and from this distribution you are given a bunch of training data drawn IID independently and identically distributed.",
                    "label": 0
                },
                {
                    "sent": "OK, now the goal of supervised learning or the goal of machine learning is.",
                    "label": 0
                },
                {
                    "sent": "It's basically a function approximation problem where you are trying to learn where you're trying to learn a function that Maps the inputs to the outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, and the function has to be learned in such a way that it performs well on unseen examples.",
                    "label": 1
                },
                {
                    "sent": "That's really important.",
                    "label": 0
                },
                {
                    "sent": "And the performance is measured with respect to some loss function.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is this is supervised learning and then notation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I I told you that I will talk about discriminative methods so there is a difference between a fundamental difference between generative discriminative learning an in generative learning, what you try to do is model the joint, estimate the parameters of of the probability of the joint on X&Y.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you perform predictions using the bicycle.",
                    "label": 0
                },
                {
                    "sent": "So now your base is an example.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov model is an example in in discriminative learning you.",
                    "label": 0
                },
                {
                    "sent": "So what we really care is the discriminant function, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't want to spend a lot of effort in learning the joint distribution on inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is model the conditional distribution of Y given X. OK, that's what you care about.",
                    "label": 0
                },
                {
                    "sent": "So logistic regression regression example.",
                    "label": 1
                },
                {
                    "sent": "Or you could also learn a function that Maps inputs to the outputs, so that's the discriminant function.",
                    "label": 0
                },
                {
                    "sent": "That's all you care about.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is discriminative learning, so I will mostly talk about discriminative learning.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, most of the methods and discriminative learning can be seen through the.",
                    "label": 0
                },
                {
                    "sent": "Framework of regularised risk minimization.",
                    "label": 1
                },
                {
                    "sent": "You might be knowing this.",
                    "label": 0
                },
                {
                    "sent": "So in regularizers risk minimization you have a loss function and then you try to minimize this loss function in addition to the loss function you have a term that you know it controls the complexity of your model.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's Omega is a regularizer, the L is the last function an your function is coming from some Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "This is this is, you know, in an abstract sense.",
                    "label": 0
                },
                {
                    "sent": "And Lambda is a regularization parameter that rates of the loss function and the complexity parameter of your model or.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "So here is a very simple example that you know.",
                    "label": 0
                },
                {
                    "sent": "Linear classifiers, so in linear classifiers the function is just a dot product of.",
                    "label": 1
                },
                {
                    "sent": "Weight vector, which is the parameters that you want to estimate and some features on X. OK, and then you want to minimize some loss function.",
                    "label": 0
                },
                {
                    "sent": "Given the labels and the inputs.",
                    "label": 0
                },
                {
                    "sent": "And the regularizer could take the form of for example L2 norm of the weight vector.",
                    "label": 0
                },
                {
                    "sent": "You could also have other forms of regularizers here, but the regularization itself is not important for this tutorial.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's just stick to L2 regularizers.",
                    "label": 0
                },
                {
                    "sent": "L2 norm of the weight vectors?",
                    "label": 1
                },
                {
                    "sent": "And the file is just a feature map which Maps the input space with some features.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in structured prediction, so this is important.",
                    "label": 1
                },
                {
                    "sent": "Let's see what the difference is between structured prediction and the standard supervised learning in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Again, you have an input space in output space, but the output space is complex.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it could.",
                    "label": 0
                },
                {
                    "sent": "It could be trees or sequences, or some combinatorial structure.",
                    "label": 0
                },
                {
                    "sent": "Some special structure, so the output space is complex.",
                    "label": 0
                },
                {
                    "sent": "You have M examples.",
                    "label": 0
                },
                {
                    "sent": "Of inputs and outputs drawn IID from some distribution now.",
                    "label": 0
                },
                {
                    "sent": "The goal is to learn a joint scoring function on the inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "So this is the crucial difference.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If you recall, you were learning a function that Maps the inputs to the outputs.",
                    "label": 0
                },
                {
                    "sent": "Over here you learn a joint scoring function that Maps input output pairs to some real valued number.",
                    "label": 0
                },
                {
                    "sent": "An intuitive what this means is that the joint scoring function is some kind of an affinity measure, and which tells you how good the structures are for a particular input.",
                    "label": 0
                },
                {
                    "sent": "OK if.",
                    "label": 0
                },
                {
                    "sent": "For example, if the parse tree corresponds well to the given sentence, then it will have a very high score.",
                    "label": 0
                },
                {
                    "sent": "If it if it has some missing edges or if it is incorrect, then it will have a very nice low score.",
                    "label": 0
                },
                {
                    "sent": "So that's the semantics of the scoring function.",
                    "label": 0
                },
                {
                    "sent": "And of course, ultimately you don't want to.",
                    "label": 0
                },
                {
                    "sent": "You know, just learn a scoring function, right?",
                    "label": 0
                },
                {
                    "sent": "You want to do predictions?",
                    "label": 0
                },
                {
                    "sent": "You want to predict wise an that is done according to the following rule.",
                    "label": 0
                },
                {
                    "sent": "So given a scoring function, you simply.",
                    "label": 0
                },
                {
                    "sent": "Output the structure with the higher score, so that's that's an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so argmax of the function over your entire entire output space for linear models.",
                    "label": 0
                },
                {
                    "sent": "It's just the weight vector.",
                    "label": 0
                },
                {
                    "sent": "The DOT product of the weight vector and the features.",
                    "label": 0
                },
                {
                    "sent": "One more crucial differences which I forgot to tell you is the features are defined on the inputs and outputs OK.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning in the standard setting you have the features defined on the inputs, but over here the fees are.",
                    "label": 0
                },
                {
                    "sent": "The map of the fears map input output pairs to some features.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is an important slide, is the notation clear?",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "There are other ways to do it, and I have some slides on that so, but this is this is kind of the state of the art, yeah?",
                    "label": 0
                },
                {
                    "sent": "For example, I'll talk about that there's an algorithm which doesn't really follow this pattern.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so join feature Maps.",
                    "label": 0
                },
                {
                    "sent": "As I said in structured prediction you have to.",
                    "label": 0
                },
                {
                    "sent": "Extract features from the inputs and outputs so fine Maps input output pairs to some feature space.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "You also need to have a joint input output kernel, so the kernel is not just on the inputs like in SPMS.",
                    "label": 0
                },
                {
                    "sent": "Now the kernel is on input output pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, which is just the dot product of the features in some Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, is this clear OK?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is a very simple example.",
                    "label": 0
                },
                {
                    "sent": "Multi Label classification I I believe you are aware of multilevel classification.",
                    "label": 0
                },
                {
                    "sent": "So let's say the features are coming from the N dimensional Euclidean space and the labels are zero and vectors of dimension D. OK, Ann and very simple way to design features is just take the chronicler of the tensor product between the axes and wise so you just replicate the features and then you can set them to zero or one depending on the label.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the tensor product kernel actually corresponds to factorization of the joint input output kernel into the kernel on the inputs and the kernel on the outputs.",
                    "label": 1
                },
                {
                    "sent": "So technically it simplifies the optimization problem, but we won't go into those details now.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple example of how you might define features on the inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so so that's I'm done with the preliminary, so that's the notation you should be aware of.",
                    "label": 0
                },
                {
                    "sent": "Now, let's talk about let's first talk about loss functions.",
                    "label": 1
                },
                {
                    "sent": "I will start with the loss functions that you use for binary classification or regression, and then I'll tell you how you could, you know, extend them for structured prediction, let's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To begin with.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the component that I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "The last fun.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chen let's begin with the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "For binary classification, the commonly used loss function is 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So if the two levels are the same, the loss is 0.",
                    "label": 0
                },
                {
                    "sent": "If not, it's one.",
                    "label": 0
                },
                {
                    "sent": "OK, this is this is clear for this structured case.",
                    "label": 0
                },
                {
                    "sent": "This is how it is defined.",
                    "label": 0
                },
                {
                    "sent": "So you might notice that it's defined with respect to an input output pair and also the scoring function.",
                    "label": 0
                },
                {
                    "sent": "OK, the L Max Delta of F, X&Y.",
                    "label": 0
                },
                {
                    "sent": "Which is nothing but.",
                    "label": 0
                },
                {
                    "sent": "The DD is a discrete loss function on the output space, so for example if you are if you have trees, you might want to have a loss function on trees.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's typically it's a discrete loss function and the way the loss itself is defined as.",
                    "label": 0
                },
                {
                    "sent": "Is given there, so you you take the argmax over the entire output space.",
                    "label": 0
                },
                {
                    "sent": "Given the scoring function and then you see how far it is from the true label.",
                    "label": 0
                },
                {
                    "sent": "That defines your loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see how you remove from binary classification to structure prediction, so this is typically the last function that you want to minimize.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, both the 01 loss and also the structure loss in the discrete case there are non convex and non differentiable, so you cannot really optimize them.",
                    "label": 0
                },
                {
                    "sent": "And therefore you have to use some target losses and these are glasses are convex upper bounds OK.",
                    "label": 1
                },
                {
                    "sent": "Hinge loss is an example.",
                    "label": 0
                },
                {
                    "sent": "Lee Squares Squared Loss is an example.",
                    "label": 0
                },
                {
                    "sent": "Logistic loss is an example.",
                    "label": 0
                },
                {
                    "sent": "OK, is this clear?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So squared loss squared loss is used in regression problems.",
                    "label": 1
                },
                {
                    "sent": "It's just the square difference of the labels.",
                    "label": 1
                },
                {
                    "sent": "The extension to structure prediction is nontrivial.",
                    "label": 1
                },
                {
                    "sent": "Ann Thomas will talk about an algorithm in Part 2 that actually minimizes an extension of squared loss for the structured prediction case, so I will.",
                    "label": 0
                },
                {
                    "sent": "I will leave it here, so more on this will come later.",
                    "label": 0
                },
                {
                    "sent": "So that's the squared.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                },
                {
                    "sent": "This is the popular hinge loss.",
                    "label": 0
                },
                {
                    "sent": "For binary case, it is defined as the Max of 0 and 1 -- y * Z An for the structured case it's defined over there, so so the idea is that Delta of XY.",
                    "label": 0
                },
                {
                    "sent": "So you have to make sure that the correct label is.",
                    "label": 0
                },
                {
                    "sent": "The structure with a higher score is higher than all the other incorrect pairs, and they should differ by the Delta, which is which has the meaning of a margin.",
                    "label": 0
                },
                {
                    "sent": "This will become clear when we talk about SVM's.",
                    "label": 0
                },
                {
                    "sent": "OK, and one more important thing is this hinge loss and also the squared loss.",
                    "label": 0
                },
                {
                    "sent": "There are convex upper bounds on the binary discrete loss functions that we saw earlier.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So logistic loss is defined as given there.",
                    "label": 0
                },
                {
                    "sent": "This is used in logistic regression.",
                    "label": 0
                },
                {
                    "sent": "It's just the negative log likelihood of your exponential family that he used in logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And for a structured prediction, it has a natural extension, the main difference being that you have to compute the sum over the entire output space and you have to make sure that it is actually greater than.",
                    "label": 0
                },
                {
                    "sent": "It is less than the true input output pair OK, and that will give you how far.",
                    "label": 0
                },
                {
                    "sent": "That gives you a measure of how far the predicted label is from the true label.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, exponential loss, which is which is minimized by variant of Adaboost.",
                    "label": 0
                },
                {
                    "sent": "Again, if you see that the main difference is you have to compute the sum over the entire output space and then compare it with the true input output pair and that gives you a measure of similarity or dissimilarity.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, you must.",
                    "label": 0
                },
                {
                    "sent": "You must be familiar with this picture, So what I just want to show you here is.",
                    "label": 0
                },
                {
                    "sent": "You have this 01 loss and all the other loss functions.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's very clear here.",
                    "label": 0
                },
                {
                    "sent": "All the other loss functions are convex upper bounds.",
                    "label": 1
                },
                {
                    "sent": "OK, so you typically end up minimizing those convex upper bounds.",
                    "label": 0
                },
                {
                    "sent": "You can see the squared loss, the hinge loss, the logistic loss and also the exponential loss there.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, now we have seen the loss functions.",
                    "label": 0
                },
                {
                    "sent": "Let's see how who could use how we could minimize this loss function, and these gives rise to different learning algorithms or parameter estimation.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's let's begin with perceptron and then move on to structured Perceptron.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so positron?",
                    "label": 0
                },
                {
                    "sent": "It's an online learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "It learns a linear classifier and the predictions are performed according to the the sign of the dot product of your weight vector and the input and the algorithm itself proceeds in rounds.",
                    "label": 0
                },
                {
                    "sent": "So you first initialize it to some zero vector.",
                    "label": 0
                },
                {
                    "sent": "And in each round you receive an input, you predict according to the sign of the dot product between the weight vector and the input.",
                    "label": 1
                },
                {
                    "sent": "You receive the true label, which is plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "And if the labels differ, you perform some update.",
                    "label": 0
                },
                {
                    "sent": "OK and they updated it given there, and it's very natural because let's say you are making a mistake and the true label is plus one, but you predicted minus one.",
                    "label": 0
                },
                {
                    "sent": "In this case you want to increase the weight vector and that's what the update rule does.",
                    "label": 0
                },
                {
                    "sent": "Similarly, for the other case, if if you the true label is minus one and you're predicting plus one.",
                    "label": 0
                },
                {
                    "sent": "You have made a mistake and then the update rule follows naturally.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how this is done.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a well known result.",
                    "label": 0
                },
                {
                    "sent": "There is actually a mistake bound which says that if the data is linearly separable, so the positives and the negatives are well separated by some margin gamma.",
                    "label": 0
                },
                {
                    "sent": "If this is the case, then the number of mistakes made by the algorithm is actually bounded by our by, whole square OK, and this is again intuitive because if the data is well separated, which means a comma has a very high value, then the number of mistakes you make is very low.",
                    "label": 1
                },
                {
                    "sent": "OK, and this R is just a.",
                    "label": 0
                },
                {
                    "sent": "It's just the direct, it's just the already the norm of the weight vector so.",
                    "label": 0
                },
                {
                    "sent": "The most important thing is the comma.",
                    "label": 0
                },
                {
                    "sent": "How well your data is separated.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Data is not always well separated.",
                    "label": 0
                },
                {
                    "sent": "There might be noise in the data, in which case you know you might have to do something else.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In these cases, the bound differs by a small quantity which is given by DD is actually a measure of how inseparable your data is.",
                    "label": 0
                },
                {
                    "sent": "OK, and it appears in the numerator, so if the data is not separable you will be making a lot of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the only difference between this bound and the bound.",
                    "label": 0
                },
                {
                    "sent": "In the previous case.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are relevant results.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we will see how you could.",
                    "label": 0
                },
                {
                    "sent": "You could extend Perceptron for the structured prediction case.",
                    "label": 1
                },
                {
                    "sent": "So let me begin with what is called constraint classification.",
                    "label": 0
                },
                {
                    "sent": "So now we are entering into the structured prediction setting.",
                    "label": 0
                },
                {
                    "sent": "Constraint classification is one of the early algorithms that extended perceptron for structured prediction problems.",
                    "label": 0
                },
                {
                    "sent": "It was specifically designed for label ranking problems.",
                    "label": 0
                },
                {
                    "sent": "But let's let's consider multiclass prediction, which is actually an example of label ranking.",
                    "label": 1
                },
                {
                    "sent": "In multiclass prediction, the output space is just D labels.",
                    "label": 0
                },
                {
                    "sent": "And here what you want to do is learn the weight vectors right?",
                    "label": 0
                },
                {
                    "sent": "And predict according to the following rule.",
                    "label": 0
                },
                {
                    "sent": "So what you do is take the dot product of the weight vectors and the input for all the labels.",
                    "label": 0
                },
                {
                    "sent": "And then just reading the label with the higher score.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the prediction rule.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the update rule that this is an extension of Perceptron and here is here is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Again, initialize all the weight vectors to 0.",
                    "label": 0
                },
                {
                    "sent": "And it precedes and algorithm operates in rounds.",
                    "label": 0
                },
                {
                    "sent": "In every round you receive an input and some label.",
                    "label": 0
                },
                {
                    "sent": "And now in Step 2 you you do the following construction.",
                    "label": 0
                },
                {
                    "sent": "Imagine our bipartite graph, a directed bipartite graph, where on one hand on one side you have the true label on the other side you have the you know the incorrect labels and you have a directed edge from the correct labels to the incorrect levels.",
                    "label": 0
                },
                {
                    "sent": "Now in step three what you do is consider every edge in the bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "And then check whether you know for every pick you check whether the dot product with the input is for P is greater than zero or not.",
                    "label": 0
                },
                {
                    "sent": "If it is less than zero, you have made a mistake.",
                    "label": 0
                },
                {
                    "sent": "Right, so P is the true level.",
                    "label": 0
                },
                {
                    "sent": "Q is the incorrect level.",
                    "label": 0
                },
                {
                    "sent": "And if WP Times X3 is less than W Q * 60, you have made a mistake.",
                    "label": 0
                },
                {
                    "sent": "OK, and in that case you have to perform.",
                    "label": 0
                },
                {
                    "sent": "You have to do something and that's the update rule and the update to update rule is again natural.",
                    "label": 0
                },
                {
                    "sent": "So since you have made a mistake for the weight vector corresponding to P, you want to promote this.",
                    "label": 0
                },
                {
                    "sent": "You want to increase the values and you do that simply by adding the features to the X to the weight.",
                    "label": 0
                },
                {
                    "sent": "And for the other case, for Q you simply demote the weight vector, so you subtract X from WQ and you do this for all the edges in the bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is more like ranking right?",
                    "label": 0
                },
                {
                    "sent": "And this is this actually false multiclass problem.",
                    "label": 0
                },
                {
                    "sent": "It's different from one versus all.",
                    "label": 0
                },
                {
                    "sent": "Because you are considering pairs of labels here.",
                    "label": 0
                },
                {
                    "sent": "It's also different from pairwise classification, because in pairwise classification you learn all of these square models of these square weight vectors.",
                    "label": 0
                },
                {
                    "sent": "Here you're learning only divide vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a very crucial difference.",
                    "label": 0
                },
                {
                    "sent": "And you should see how this is a natural extension of perceptron.",
                    "label": 0
                },
                {
                    "sent": "For structured prediction, OK, it's not really structured prediction, it's multiclass prediction an.",
                    "label": 0
                },
                {
                    "sent": "Is this is this clear?",
                    "label": 0
                },
                {
                    "sent": "This is important.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have moved from binary classification to multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "And why are multiclass classification?",
                    "label": 0
                },
                {
                    "sent": "We will go to structured prediction, so that's the structure perceptron.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we will use the standard notation.",
                    "label": 0
                },
                {
                    "sent": "So there is a linear scoring.",
                    "label": 0
                },
                {
                    "sent": "You want to learn a linear scoring function.",
                    "label": 1
                },
                {
                    "sent": "You have a features you have features on inputs and outputs, and the weight vector.",
                    "label": 0
                },
                {
                    "sent": "This is again an online learning algorithm so.",
                    "label": 0
                },
                {
                    "sent": "It operates in rounds.",
                    "label": 0
                },
                {
                    "sent": "You initialize the weight vector to be 0.",
                    "label": 0
                },
                {
                    "sent": "In every iteration, you first receive the input.",
                    "label": 0
                },
                {
                    "sent": "And Step 2 is important.",
                    "label": 0
                },
                {
                    "sent": "You predict according to the rules that I showed before.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have the learning you have the scoring function.",
                    "label": 0
                },
                {
                    "sent": "You have the weight vector so you have the scoring function and then you just output the vector which has got the higher score.",
                    "label": 0
                },
                {
                    "sent": "That's the prediction.",
                    "label": 1
                },
                {
                    "sent": "And now you receive the true label.",
                    "label": 0
                },
                {
                    "sent": "If it is incorrect.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't have to check that condition because it's implicit in Step 4.",
                    "label": 0
                },
                {
                    "sent": "General is the following, so you just add the true the features corresponding to the true input output and the features corresponding to the incorrect input output pair.",
                    "label": 0
                },
                {
                    "sent": "So you you have to see that in Step 4 this is more like the promotion, demotion, update that I described earlier.",
                    "label": 0
                },
                {
                    "sent": "So you are making sure that the features corresponding to the true input output are getting added to the way traitor and the features corresponding to the incorrect label are getting subtracted from the vector.",
                    "label": 0
                },
                {
                    "sent": "So the idea on you have the promotion step and also the demotion step in one shot.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the extension of positron and also the IT follows wire constraint classification.",
                    "label": 0
                },
                {
                    "sent": "Is this clear?",
                    "label": 0
                },
                {
                    "sent": "The crucial difference, of course, is how you predict.",
                    "label": 0
                },
                {
                    "sent": "Any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Alright, so OK. Now you might say this is this is this is pretty simple right?",
                    "label": 1
                },
                {
                    "sent": "So we have a perceptron.",
                    "label": 0
                },
                {
                    "sent": "It's easy to implement and now we have structured perceptron.",
                    "label": 0
                },
                {
                    "sent": "The update rule is looks very similar and we can easily implement it.",
                    "label": 0
                },
                {
                    "sent": "So we're done.",
                    "label": 0
                },
                {
                    "sent": "We're done with structured prediction, right?",
                    "label": 0
                },
                {
                    "sent": "But that's not the case because in Step 2 this there is this argmax problem.",
                    "label": 0
                },
                {
                    "sent": "I will call it argmax problem from now onwards, and that's that's really important.",
                    "label": 0
                },
                {
                    "sent": "That's that's where you have to spend a lot of effort.",
                    "label": 0
                },
                {
                    "sent": "It depends very much on the application.",
                    "label": 0
                },
                {
                    "sent": "So depends on the complexity or the technical challenge.",
                    "label": 0
                },
                {
                    "sent": "In structured prediction arises from the fact that you have to solve this inference at some point of time in your training procedure.",
                    "label": 0
                },
                {
                    "sent": "In your learning algorithm and this this will you will see that this appears everywhere.",
                    "label": 0
                },
                {
                    "sent": "Depending on the application, you might have to design specialized algorithms to solve the argmax problem.",
                    "label": 0
                },
                {
                    "sent": "Once you have this argmax problem, fixed everything else follows naturally.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's where the difficulty lies.",
                    "label": 0
                },
                {
                    "sent": "The difficulty lies in Step 2, but for this tutorial I will treat it as a black box.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, similar to Perceptron we have mistake bounds.",
                    "label": 0
                },
                {
                    "sent": "For structured perceptron too, it looks very similar.",
                    "label": 0
                },
                {
                    "sent": "In fact it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "For the separable cage, you see that the number of mistakes made on sequence of examples is at most R-squared by, Square.",
                    "label": 1
                },
                {
                    "sent": "Where gamma is, you know that the margin as defined.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And again for the interpret, because you have something very similar to perceptron, you have the.",
                    "label": 0
                },
                {
                    "sent": "The D it gives you a measure of how inseparable data is, and that it appears in the numerator.",
                    "label": 0
                },
                {
                    "sent": "So if the data is highly inseparable, then you're you know you run into problems and then you have the margin appearing in the denominator.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "These are the mistake bounds for.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctor perceptron?",
                    "label": 0
                },
                {
                    "sent": "OK, so now we are done with Perceptron.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just.",
                    "label": 0
                },
                {
                    "sent": "Recapitulate we we started with perceptron and motor structure prediction.",
                    "label": 0
                },
                {
                    "sent": "Why are multi class classification?",
                    "label": 0
                },
                {
                    "sent": "OK now let's let's see how this can be done for regression problems.",
                    "label": 0
                },
                {
                    "sent": "So I will start with regression least squares regression that everybody is familiar with and then I will show you how to extend it for structured prediction which which is called kernel dependency estimation.",
                    "label": 0
                },
                {
                    "sent": "Katie is Colonel dependence.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimation.",
                    "label": 0
                },
                {
                    "sent": "OK regularise miss progression.",
                    "label": 0
                },
                {
                    "sent": "So you are trying to minimize the squared loss and then you have a L2 norm on the weight vector.",
                    "label": 0
                },
                {
                    "sent": "The solution can be written down in closed form.",
                    "label": 0
                },
                {
                    "sent": "It follows from basic math.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And in over their access, the data matrix or the input matrix?",
                    "label": 0
                },
                {
                    "sent": "And why is the vector of all the labels and I is the identity matrix just to make sure that you understand the notation.",
                    "label": 0
                },
                {
                    "sent": "So just the the difficulties just inverting the matrix.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you want to close from solution, otherwise you have to use gradient descent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is clear.",
                    "label": 0
                },
                {
                    "sent": "And then of course there is also a kernelized version for this.",
                    "label": 0
                },
                {
                    "sent": "And now instead of the weight vector you have, you're trying to learn a function coming from the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "And there is you can there is represented Theorem which tells you that the function is just dot product of seas.",
                    "label": 0
                },
                {
                    "sent": "Seas are the kernel expansion coefficients and dot product between the seizan the kernels, the kernel values, an opty one can be written in the form of using kernels, and again you have a closed form solution with just amounts to inverting the kernel matrix multiplied with the weight vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this again standard.",
                    "label": 0
                },
                {
                    "sent": "This is regression actually.",
                    "label": 0
                },
                {
                    "sent": "OK, now now let's see.",
                    "label": 0
                },
                {
                    "sent": "How we could extend this?",
                    "label": 0
                },
                {
                    "sent": "Um, let me just go back.",
                    "label": 0
                },
                {
                    "sent": "So one natural way to extend rigorously squares for multivariate prediction or structure prediction is so we have the X is and why is there right and wise?",
                    "label": 0
                },
                {
                    "sent": "In that case, it's just a single value.",
                    "label": 0
                },
                {
                    "sent": "You could simply consider vectors there and then you could write down a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "So instead of why there, which appears in the solution, you will just have a matrix of response variables.",
                    "label": 0
                },
                {
                    "sent": "But the problem is this is equivalent to solving independent regressors.",
                    "label": 0
                },
                {
                    "sent": "You're not taking the correlations of the structure into account.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's not really the solution.",
                    "label": 0
                },
                {
                    "sent": "When you move to vectors.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you may have to do something else.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And which is what is done in kernel dependency estimation.",
                    "label": 0
                },
                {
                    "sent": "In KDE, the first step is to define a feature map for the output space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is again important, so you have to define PSI that Maps the outputs to some features.",
                    "label": 0
                },
                {
                    "sent": "So, so let's say we have some Hilbert space on the rise with the corresponding kernel.",
                    "label": 0
                },
                {
                    "sent": "On the output.",
                    "label": 0
                },
                {
                    "sent": "And this is just the dot product of the features on the wise.",
                    "label": 0
                },
                {
                    "sent": "And note that now it is possible to consider a large class of nonlinear loss functions in the output space because of the kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, it's it's a kernel method.",
                    "label": 0
                },
                {
                    "sent": "Now here is the main idea.",
                    "label": 0
                },
                {
                    "sent": "What Katie does is the following.",
                    "label": 1
                },
                {
                    "sent": "It uses kernel PCA, kernel, principal component analysis to decorrelate the output.",
                    "label": 0
                },
                {
                    "sent": "And then trains independent are large independent regressors.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how you take the correlations of the output space into account you first decorrelate the output using some technique and once you have the code later, the output you could learn independent regressors and we know how to do that using least squares regression.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here is how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So you first have to decompose the output into, let's say K orthogonal directions using kernel principal component analysis.",
                    "label": 1
                },
                {
                    "sent": "And then what you do is you learn independent regressors for each of these directions using regularize least squares.",
                    "label": 0
                },
                {
                    "sent": "But now this is not the end of the problem and the solution because the predictions that are coming from Step 2 is in a transformed space, right?",
                    "label": 0
                },
                {
                    "sent": "You have to go back to the original space, and that is done.",
                    "label": 0
                },
                {
                    "sent": "Using that is done by solving the preimage problem, which is actually given there.",
                    "label": 1
                },
                {
                    "sent": "So you just try to minimize.",
                    "label": 0
                },
                {
                    "sent": "Two vectors and these vectors are one vector, is just your predictions from your least squares and the other vector is the vector of projections.",
                    "label": 0
                },
                {
                    "sent": "OK, so the V is are the projections the dot product of V ANSSI are the projections.",
                    "label": 0
                },
                {
                    "sent": "And you just compare it.",
                    "label": 0
                },
                {
                    "sent": "You just minimize the L2 norm of these vectors and then output the minimum value of it and that will give you the output in the in the original space that you want.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is again, I'm treating this as a black box because this is, this is not.",
                    "label": 0
                },
                {
                    "sent": "It may not be easy to solve this problem the preimage problem.",
                    "label": 0
                },
                {
                    "sent": "It depends very much on the application.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is, you know it's like similar to the inference problem.",
                    "label": 0
                },
                {
                    "sent": "Is this clear?",
                    "label": 0
                },
                {
                    "sent": "Well, if you decorrelate the output, you can treat the response variables as independent and then learn independent regressors.",
                    "label": 0
                },
                {
                    "sent": "If they are dependent, you can do regression.",
                    "label": 0
                },
                {
                    "sent": "Oh, you mean to say that even if the outputs are correlated, you want to just learn regressors on them?",
                    "label": 0
                },
                {
                    "sent": "That won't work, right?",
                    "label": 0
                },
                {
                    "sent": "That's not going to work.",
                    "label": 0
                },
                {
                    "sent": "In fact, that is the problem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And this is not just.",
                    "label": 0
                },
                {
                    "sent": "Yes, there are people you stick people.",
                    "label": 0
                },
                {
                    "sent": "Yes, before they do regression.",
                    "label": 0
                },
                {
                    "sent": "I think they used techniques like Canonical correlation analysis and they don't correlate the outputs in some sense.",
                    "label": 0
                },
                {
                    "sent": "They have to be aware of regulating the outputs, otherwise it won't give you good results.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh OK, so here is a small technical node, so this is a kernel method, right so?",
                    "label": 0
                },
                {
                    "sent": "You don't really, so I said that we have to have a feature, a feature representation for the outputs, but you don't need an explicit representation.",
                    "label": 0
                },
                {
                    "sent": "So the projections can be computed directly using the kernel values, which is actually given there.",
                    "label": 0
                },
                {
                    "sent": "So all you need to do is take the kernel on the output space, do some centering operations, perform eigenvalue decomposition on it, and then once once you get this alpha's, which are the eigenvectors, you could compute the projections directly using the equation that is given there.",
                    "label": 0
                },
                {
                    "sent": "So you just need to define a kernel on the outputs.",
                    "label": 0
                },
                {
                    "sent": "You don't really need an explicit feature representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just a technical.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note OK, so we are done with regression.",
                    "label": 0
                },
                {
                    "sent": "So we I told you how you could extend regression to the structured prediction case which was the kernel dependency estimation.",
                    "label": 0
                },
                {
                    "sent": "Now let's see how you could extend support vector machines to the structured setting.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you must be familiar with this, so support vector machine, it's it's.",
                    "label": 1
                },
                {
                    "sent": "It's large margin classification.",
                    "label": 0
                },
                {
                    "sent": "Again, you have a.",
                    "label": 0
                },
                {
                    "sent": "Let's consider a linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "For now you have a weight vector.",
                    "label": 0
                },
                {
                    "sent": "You have some features, it minimizes the hinge loss and the optimization is given.",
                    "label": 1
                },
                {
                    "sent": "There you minimize the hinge loss and then also an additional term which is the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Typically you also introduce Slack variables to take noise into account, and then this is the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the notation.",
                    "label": 0
                },
                {
                    "sent": "You can write down the dual.",
                    "label": 0
                },
                {
                    "sent": "There are some advantages of writing down the deal.",
                    "label": 0
                },
                {
                    "sent": "You could solve nonlinear problems.",
                    "label": 0
                },
                {
                    "sent": "It has the optimization problem is clean because you have box constraints there.",
                    "label": 0
                },
                {
                    "sent": "It's easy to solve.",
                    "label": 0
                },
                {
                    "sent": "And then again, you have a representative theorem which tells you that the function is actually the dot product of the kernel expansion coefficients and the kernel values.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is support vector machine into slow.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now, let's see how we could extend SVM's to a structured SVM or Max margin Markov network.",
                    "label": 0
                },
                {
                    "sent": "So structured SVM minimizes the structured hinge loss.",
                    "label": 1
                },
                {
                    "sent": "And there are two formulations.",
                    "label": 0
                },
                {
                    "sent": "One is called the slack rescaling and the other one is called the margin rescaling.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Let's start with Slack rescaling, so here what you tried to do is you minimize the L2 norm of the weight vector.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Subject to the following constraints, so these constraints are important and here, here, here in lies the main difference between structured streams and SVM.",
                    "label": 0
                },
                {
                    "sent": "So for every input output pair, correct input output pair you have to make sure that it gets a score higher than all the other in all the other input output pairs, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what the constraints tell you.",
                    "label": 0
                },
                {
                    "sent": "And the constraints are typically exponentially number.",
                    "label": 0
                },
                {
                    "sent": "OK, and the reason why it is called Slack reskilling, it's because.",
                    "label": 0
                },
                {
                    "sent": "You re scale the slack variable for every instance with the loss function.",
                    "label": 0
                },
                {
                    "sent": "So recall OK, structured SVM.",
                    "label": 1
                },
                {
                    "sent": "This is minimizing the structured hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But then in addition to that you have the discrete loss function right?",
                    "label": 0
                },
                {
                    "sent": "And that has to appear in some way, and that's it appears in the denominator of this variable.",
                    "label": 0
                },
                {
                    "sent": "So this means that every you know you're forcing the true input output pairs to get a score with a margin.",
                    "label": 0
                },
                {
                    "sent": "Given by 1 minus you know the slack variable.",
                    "label": 0
                },
                {
                    "sent": "Is this clear?",
                    "label": 0
                },
                {
                    "sent": "So you see the difference between SVM and structured SVM.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there is another formulation which is called margin rescaling.",
                    "label": 1
                },
                {
                    "sent": "So here what you do is instead of having a margin of one, you replace the margin with the loss function itself.",
                    "label": 0
                },
                {
                    "sent": "So this means, let's say a particular why is is is very far away from the true Y and and therefore you know the scoring function for the true input output should have a very.",
                    "label": 0
                },
                {
                    "sent": "You know it should exceed the incorrect one by a margin which is given by the loss between the two Y and the incorrect, why?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how the margin is defined for this setting.",
                    "label": 0
                },
                {
                    "sent": "So the only difference between margin rescaling and slacker scaling is the way you re scale this.",
                    "label": 0
                },
                {
                    "sent": "The way you introduce a lot the last function to influence the setting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, any questions.",
                    "label": 0
                },
                {
                    "sent": "OK, I thought I saw a hand go up OK.",
                    "label": 0
                },
                {
                    "sent": "So there are there there are OK Now you might ask what is the OK?",
                    "label": 0
                },
                {
                    "sent": "Why do we have to?",
                    "label": 0
                },
                {
                    "sent": "Formulations, margin rescaling and Slack rescaling so there are some advantages and disadvantages to both these settings.",
                    "label": 0
                },
                {
                    "sent": "Margin rescaling is easier to solve technically, because as you will see in the later slides, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It amounts to solving the inference problem that you use for prediction.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slacker is killing is it's actually difficult to optimize, but on the the advantage of slack rescaling is it's invariant to its.",
                    "label": 0
                },
                {
                    "sent": "The IT is.",
                    "label": 0
                },
                {
                    "sent": "Invariant too.",
                    "label": 0
                },
                {
                    "sent": "The scale of the loss function itself.",
                    "label": 0
                },
                {
                    "sent": "So if you scale your loss function.",
                    "label": 0
                },
                {
                    "sent": "This will not get effect.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the margin rescaling will get affected.",
                    "label": 1
                },
                {
                    "sent": "And the other problem with margin rescaling is you spend a lot of effort in incorrect input outputs.",
                    "label": 0
                },
                {
                    "sent": "Because, you know, it's actually the way the loss function is influencing the margin.",
                    "label": 0
                },
                {
                    "sent": "So if you are the incorrect input, output is far away, then you are spending a lot of effort in making sure that you know the true input output is getting a very high score.",
                    "label": 0
                },
                {
                    "sent": "So you are spending a lot of effort, so you don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "So there are advantages and disadvantages of both these formulations.",
                    "label": 0
                },
                {
                    "sent": "So depending on your problem, you might want to consider using one of these.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so so so here is the main problem.",
                    "label": 0
                },
                {
                    "sent": "We have seen that there are exponential number of constraints, right so?",
                    "label": 1
                },
                {
                    "sent": "If you look at those constraints.",
                    "label": 0
                },
                {
                    "sent": "The constraints are for all the instances and for all the elements in the output space.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's typically exponentially number.",
                    "label": 0
                },
                {
                    "sent": "So how do we solve the problem?",
                    "label": 0
                },
                {
                    "sent": "So the idea here is there is a technique called cutting plane method which uses the following idea.",
                    "label": 0
                },
                {
                    "sent": "It suffices to design A subroutine which is called as augmented in French.",
                    "label": 1
                },
                {
                    "sent": "To compute.",
                    "label": 0
                },
                {
                    "sent": "The quantities that are shown there.",
                    "label": 0
                },
                {
                    "sent": "So the first one is for.",
                    "label": 0
                },
                {
                    "sent": "The slack reskilling formulation and the second one is for the margin rescaling formulation and we will see in the later slides how we could use this subroutine to solve the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It eventually boils down to iteratively computing these most violated constraints and adding them to the optimization problem, and that's that's enough, and you could solve the problem to design precision as well.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See now.",
                    "label": 0
                },
                {
                    "sent": "So here is the cutting plane method.",
                    "label": 0
                },
                {
                    "sent": "So I want you to focus on Step 5, which is the inference problem.",
                    "label": 0
                },
                {
                    "sent": "Step 5 and step 6 so.",
                    "label": 0
                },
                {
                    "sent": "As I told you, it suffices to solve the inference problem.",
                    "label": 0
                },
                {
                    "sent": "There, the loss augmented inference.",
                    "label": 0
                },
                {
                    "sent": "Which is which is typically tells you it gives you the most violated constraint, right?",
                    "label": 0
                },
                {
                    "sent": "And then depending on the condition which is given in step eight, you keep adding these most violated constraints an.",
                    "label": 0
                },
                {
                    "sent": "You can show that polynomial number of constraints suffices to actually solve the problem to decide precision.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't really need exponential number of constraints.",
                    "label": 0
                },
                {
                    "sent": "If you have a subroutine which computes the most violated constraint, you could use that to bring down the complexity of the learning optimization problem, and this is the cutting plane method.",
                    "label": 0
                },
                {
                    "sent": "Is this clear?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so as I said, we have some theoretical guarantees.",
                    "label": 1
                },
                {
                    "sent": "The first one is polynomial time termination, so the algorithm that I showed you in the previous slide, it terminates in a polynomial number of iterations because you are only adding polynomial number of constraints and every step in the algorithm runs in polynomial time, so you only have polynomial number of iterations.",
                    "label": 1
                },
                {
                    "sent": "And the algorithm is correct in the following sense.",
                    "label": 1
                },
                {
                    "sent": "It's also optimization problem accurately to a desired precision epsilon, which means that all the exponential number of constraints are satisfied to the desired position.",
                    "label": 0
                },
                {
                    "sent": "So you don't really solve the problem to the optimal.",
                    "label": 0
                },
                {
                    "sent": "You don't get the optimal solution, but you can, you know, bring it down as close as possible depending on the precision parameter, which you will give.",
                    "label": 0
                },
                {
                    "sent": "And then there is again in standard empirical respond which the some of the slack variables have a bunch.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One thing that you should keep in mind is these theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "Are for the case where you have an exact where you can do exact inference.",
                    "label": 0
                },
                {
                    "sent": "So if you can compute if you can do step 6 exactly, so that's exact inference, and then you know all the theoretical guarantees follow.",
                    "label": 0
                },
                {
                    "sent": "If not, then you run into problems.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's the focus of Part 2 of the tutorial.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and I think I'll skip this right?",
                    "label": 0
                },
                {
                    "sent": "So there is also a kernelized version for structured SVM's.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "There is also represented theorem.",
                    "label": 0
                },
                {
                    "sent": "And one thing you might want to notice is that the kernel is now on the pairs of inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "And the alphas are your dual variables anyway.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll skip this slide.",
                    "label": 0
                },
                {
                    "sent": "OK, at this point I want to.",
                    "label": 0
                },
                {
                    "sent": "Revisit structured perceptron.",
                    "label": 0
                },
                {
                    "sent": "Because if you if you recall.",
                    "label": 0
                },
                {
                    "sent": "Structured both run in Step 2.",
                    "label": 0
                },
                {
                    "sent": "You are solving the inference problem right?",
                    "label": 0
                },
                {
                    "sent": "But you might notice that the loss function Delta doesn't appear anywhere.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in in SPMS and structured as firms, you saw how the loss function influences the learning procedure.",
                    "label": 0
                },
                {
                    "sent": "It actually influences the margin.",
                    "label": 0
                },
                {
                    "sent": "But in structured perceptron.",
                    "label": 0
                },
                {
                    "sent": "It appears nowhere right in Step 2.",
                    "label": 0
                },
                {
                    "sent": "You're just minimizing, you're just solving the inference of the plane inference problem.",
                    "label": 0
                },
                {
                    "sent": "So this this might create some problems for some applications, so you really want to take the last function into account.",
                    "label": 0
                },
                {
                    "sent": "So how do you do that?",
                    "label": 0
                },
                {
                    "sent": "By a simple.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And which is replace step two with the loss augmented inference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In step one, you receive the input output pair and predict and the prediction is done.",
                    "label": 0
                },
                {
                    "sent": "Installed inference problem by also taking the loss function.",
                    "label": 0
                },
                {
                    "sent": "So now you have you have an online learning algorithm which also takes the loss function into account.",
                    "label": 0
                },
                {
                    "sent": "So you can see this as the online version of structured SVM.",
                    "label": 0
                },
                {
                    "sent": "OK, but of course you it, it's up to you to design A subroutine that solves a loss.",
                    "label": 0
                },
                {
                    "sent": "Augmented inference that might be difficult in some cases.",
                    "label": 0
                },
                {
                    "sent": "But this is the crucial difference, and again the update rule is same and then you have a learning rate later.",
                    "label": 0
                },
                {
                    "sent": "And this is you have an online version of structured SVM.",
                    "label": 0
                },
                {
                    "sent": "This is also called the subgradient method in the literature.",
                    "label": 0
                },
                {
                    "sent": "So that's lost.",
                    "label": 0
                },
                {
                    "sent": "Structured perceptron with also admitted inference.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The loss function in the kernel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, that's true actually.",
                    "label": 0
                },
                {
                    "sent": "And if you if you are only using in fact if you are using the kernel on the outputs as a loss function, that's what Katie does.",
                    "label": 0
                },
                {
                    "sent": "The kernel dependency estimation.",
                    "label": 0
                },
                {
                    "sent": "But over here you also have the loss specifically being used to influence the margin.",
                    "label": 0
                },
                {
                    "sent": "So it's it's actually did know know you could have the same thing.",
                    "label": 0
                },
                {
                    "sent": "But typically you you have a different kernel and also.",
                    "label": 0
                },
                {
                    "sent": "A loss function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that's that.",
                    "label": 0
                },
                {
                    "sent": "A good question.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So where are we?",
                    "label": 0
                },
                {
                    "sent": "So we were we talked about.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines.",
                    "label": 0
                },
                {
                    "sent": "We moved to structured SVM's.",
                    "label": 0
                },
                {
                    "sent": "And then I also told you how you could, you know, modify structured Perceptron 2.",
                    "label": 0
                },
                {
                    "sent": "Introduce the loss function into the.",
                    "label": 0
                },
                {
                    "sent": "Update rule.",
                    "label": 0
                },
                {
                    "sent": "Now there there are other formulations to solve the optimization problem in structured SVM and here is 1 formulation which is called the min Max formulation.",
                    "label": 0
                },
                {
                    "sent": "So here is how it works.",
                    "label": 0
                },
                {
                    "sent": "So recall the brute force enumeration this is.",
                    "label": 1
                },
                {
                    "sent": "This is what I showed you in the earlier slide, right?",
                    "label": 0
                },
                {
                    "sent": "You have the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "The objective in the objective and then the constraints.",
                    "label": 0
                },
                {
                    "sent": "You have exponential number of constraints.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Which forces all the true input output pairs to get a score higher than the incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so now how do you solve this?",
                    "label": 0
                },
                {
                    "sent": "We have seen one way to solve this, which is the cutting plane method.",
                    "label": 0
                },
                {
                    "sent": "And here is another way to solve this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "The Min Max formulation.",
                    "label": 0
                },
                {
                    "sent": "What it does is.",
                    "label": 0
                },
                {
                    "sent": "You replace the exponential number of constraints into one constraint, one constraint for example.",
                    "label": 0
                },
                {
                    "sent": "And how do you do that?",
                    "label": 0
                },
                {
                    "sent": "You simply take the Max.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "You know, enumerating all the exponential number of constraints?",
                    "label": 0
                },
                {
                    "sent": "OK, you see the difference between this set of constraints and the constraints in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "OK, and now once you have written it down as a Max over that expression, you have M number of constraints.",
                    "label": 0
                },
                {
                    "sent": "But again, you have to solve this inference problem.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is for some problems.",
                    "label": 0
                },
                {
                    "sent": "You could replace this Max problem as an LP inference.",
                    "label": 0
                },
                {
                    "sent": "Linear programming inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so for a certain combinatorial structures you can plug in LP inference there and you write down the dual using the LP duality.",
                    "label": 0
                },
                {
                    "sent": "You replace the Max with men.",
                    "label": 1
                },
                {
                    "sent": "And now you have a minimization throughout.",
                    "label": 0
                },
                {
                    "sent": "Independent for certain structures, you could it so happens that you could rewrite the entire optimization problem as a concise QP.",
                    "label": 1
                },
                {
                    "sent": "And by concise I mean you have polynomial number of.",
                    "label": 0
                },
                {
                    "sent": "Constraints and variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the min Max formulation.",
                    "label": 0
                },
                {
                    "sent": "OK, did I'm not explaining the steps in detail, but this is the idea.",
                    "label": 0
                },
                {
                    "sent": "At a high level.",
                    "label": 0
                },
                {
                    "sent": "So there are some problems like matching that you could you could actually solve this so you don't really need to solve inference problems in every iteration like structured SVM does.",
                    "label": 0
                },
                {
                    "sent": "You could write it down as a concise QP and solve the whole thing in one shot.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the min Max formulation.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we are done with large margin classifiers for structured.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so the only thing that is left is logistic regression and how we could extend it to conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK. What what did I have there?",
                    "label": 0
                },
                {
                    "sent": "This is this is for.",
                    "label": 0
                },
                {
                    "sent": "Is this for the margin rescaling?",
                    "label": 0
                },
                {
                    "sent": "It's actually OK in the paper it's defined for margin rescaling.",
                    "label": 0
                },
                {
                    "sent": "Well if you have slack reskilling then you may help to do something too.",
                    "label": 0
                },
                {
                    "sent": "Solve the math problem or write it down.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you could plug in an LP influence.",
                    "label": 0
                },
                {
                    "sent": "It depends on your structure.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "But conceptually, you could have also this like variables.",
                    "label": 0
                },
                {
                    "sent": "This, like rescaling formulation there.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's get into logistic regression and CRF's.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have 25 minutes?",
                    "label": 0
                },
                {
                    "sent": "OK, let's see if I can finish this.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Logistik regression, as you all know, is a probabilistic binary classifier.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The likelihood function is is given there.",
                    "label": 0
                },
                {
                    "sent": "It's basically an exponential family model, so it's here what you're trying to do is.",
                    "label": 0
                },
                {
                    "sent": "You are trying to predict the probabilities right?",
                    "label": 0
                },
                {
                    "sent": "It's not just plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "You want to predict the probability that the label belongs to plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "And I have written down the likelihood function in a slightly different way than what you might find in the literature, but this is for convenience and you will see how you could extend this to a structured prediction.",
                    "label": 0
                },
                {
                    "sent": "So in the numerator.",
                    "label": 0
                },
                {
                    "sent": "You have the exponent over the dot product of the scoring function multiplied by the label and the denominator is just a normalizer.",
                    "label": 0
                },
                {
                    "sent": "It takes care of it.",
                    "label": 0
                },
                {
                    "sent": "Make sure that your probability is a proper probability distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a plus Y and also minus Y there.",
                    "label": 0
                },
                {
                    "sent": "For the for the binary class.",
                    "label": 0
                },
                {
                    "sent": "OK, and how do you estimate the parameters?",
                    "label": 0
                },
                {
                    "sent": "This is done by minimizing the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The fear of Y given access.",
                    "label": 0
                },
                {
                    "sent": "The log is the likelihood function you want to estimate the parameters that maximize this likelihood function.",
                    "label": 0
                },
                {
                    "sent": "For technical reasons, you typically end up taking the negative log likelihood and minimizing OK.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try just regression before I extend this for the structured prediction, guess I have to talk about exponential family distributions just a few slides on exponential family you might.",
                    "label": 0
                },
                {
                    "sent": "You might already know this.",
                    "label": 0
                },
                {
                    "sent": "So exponential family is a family of probability distributions.",
                    "label": 1
                },
                {
                    "sent": "It's the distribution itself is given there.",
                    "label": 0
                },
                {
                    "sent": "The fees are, you know it.",
                    "label": 1
                },
                {
                    "sent": "They're called the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "And the Z of W is what is called the partition function or the numerator that you know.",
                    "label": 0
                },
                {
                    "sent": "Make sure that you have a proper probability distribution, and this is typically.",
                    "label": 0
                },
                {
                    "sent": "OK there I have an integral, so for the discrete case you have a summation.",
                    "label": 0
                },
                {
                    "sent": "OK so I just use that interchangeably.",
                    "label": 0
                },
                {
                    "sent": "So this is exponential family model.",
                    "label": 0
                },
                {
                    "sent": "Um, and note that I am.",
                    "label": 0
                },
                {
                    "sent": "This is only for the inputs, right?",
                    "label": 0
                },
                {
                    "sent": "So we're we're still be in the standard setting.",
                    "label": 0
                },
                {
                    "sent": "We haven't gone through.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structured city there are some nice properties of exponential family and that's why people used it and.",
                    "label": 0
                },
                {
                    "sent": "The one thing the first thing is so GFW is the logarithm is is the log of the partition function.",
                    "label": 0
                },
                {
                    "sent": "So it's called a log partition function.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is G of W is accumulated generator, which means you take the derivative of this function with respect your weights or your parameters.",
                    "label": 0
                },
                {
                    "sent": "You get the expectation over the features.",
                    "label": 0
                },
                {
                    "sent": "And you take the second derivative of this function with respect to the weights.",
                    "label": 0
                },
                {
                    "sent": "You will see that you get the covariance of the features.",
                    "label": 0
                },
                {
                    "sent": "And why is it important?",
                    "label": 0
                },
                {
                    "sent": "Because the second derivative is a covariance function, this means.",
                    "label": 0
                },
                {
                    "sent": "The log partition function itself is convex.",
                    "label": 1
                },
                {
                    "sent": "Right, and then when in so this is important?",
                    "label": 0
                },
                {
                    "sent": "Because when you are minimizing the negative log likelihood, you typically end up with a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And this is how it follows.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The GFW is a cumulant generator, the secondary is a covariance function, and therefore you know GF W is a convex function.",
                    "label": 1
                },
                {
                    "sent": "And because she is convex, you know the original optimization problem itself becomes convex because all the terms are convex there.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the reason why you know.",
                    "label": 0
                },
                {
                    "sent": "People use exponential family models.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this slide is again important in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "What you have is conditional exponential family.",
                    "label": 0
                },
                {
                    "sent": "OK, so here the family of distributions define the probability of Y given X.",
                    "label": 0
                },
                {
                    "sent": "And the way it is written down is shown there.",
                    "label": 0
                },
                {
                    "sent": "Now you have the features on the inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, previously you only had five of X.",
                    "label": 1
                },
                {
                    "sent": "Now we have Phi of X&Y and then again you have the log partition function.",
                    "label": 0
                },
                {
                    "sent": "So this is conditional exponential family.",
                    "label": 1
                },
                {
                    "sent": "You see, the difference between this and the one I showed before, right?",
                    "label": 0
                },
                {
                    "sent": "And this is how we move to structure prediction.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I told you that you minimize to estimate the parameters you end up minimizing the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "But it might be helpful to introduce a prior on the weight vectors and then take the do what is known as map estimation.",
                    "label": 0
                },
                {
                    "sent": "OK, typically people use a Gaussian prior on W. And when you take the logarithm, you see that it's basically the L2 norm of the weight vector.",
                    "label": 1
                },
                {
                    "sent": "OK, and this is again regularised risk minimization.",
                    "label": 0
                },
                {
                    "sent": "So you have an L2 norm and then you also have the negative log likelihood, which is the last function it actually follows from having Gaussian prior on the weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So now we know that all the terms in the optimization problem is convex, so you could solve it using off the shelf solvers.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conditional random fields is an exam is just an example of exponential conditional expansion.",
                    "label": 0
                },
                {
                    "sent": "It's an example of a model that uses conditional exponential family.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's talk about linear chain conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "So the voice of the labels.",
                    "label": 0
                },
                {
                    "sent": "So for example, device could be our part of speech tags and the X is are your words.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The probability there conditional distribution itself could be written down.",
                    "label": 0
                },
                {
                    "sent": "It actually decomposes over the clicks and the clicks are the pairs of wise index and also pairwise labels.",
                    "label": 0
                },
                {
                    "sent": "And then you can decompose the entire quantity into the submission over your the length of the sequence of the dot product between the clicks the features corresponding to the clicks and a weight vector corresponding to that particular edge.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about linear chain having this particular chain so conditional random fields are not only for linear chains, you could have arbitrary graph structures.",
                    "label": 0
                },
                {
                    "sent": "But then you you will have problems in computing the partition function.",
                    "label": 0
                },
                {
                    "sent": "It so happens that for this particular chain like structure you could do inference using dynamic programming techniques like the Viterbi algorithm.",
                    "label": 1
                },
                {
                    "sent": "And then of course you you can estimate the parameters using either maximum likelihood are map estimation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is CRF's.",
                    "label": 0
                },
                {
                    "sent": "Any questions here?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now there there is a fundamental difference between hidden Markov models and so your apps, right?",
                    "label": 0
                },
                {
                    "sent": "So Hmm's are generated models and see our discriminative models.",
                    "label": 0
                },
                {
                    "sent": "So in HMMS, what you try to do is maximize the joint likelihood on the inputs and outputs, which is not exactly what you want to do for conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "You only focus on the conditional.",
                    "label": 0
                },
                {
                    "sent": "And the graphical models are given over there.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is your.",
                    "label": 0
                },
                {
                    "sent": "Any questions here?",
                    "label": 0
                },
                {
                    "sent": "Alright, so we are done with logistic regression an also conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "And this, I believe, is the last topic.",
                    "label": 0
                },
                {
                    "sent": "Learning reductions.",
                    "label": 0
                },
                {
                    "sent": "OK, so until now, what what I've shown you is how to extend binary classifiers or regressors.",
                    "label": 0
                },
                {
                    "sent": "To the structured prediction case right now, here is here is something really interesting here.",
                    "label": 0
                },
                {
                    "sent": "What learning reduction is what it tries to do is.",
                    "label": 0
                },
                {
                    "sent": "It will, you're trying to reduce structured prediction to a binary classification problem, so it's in.",
                    "label": 0
                },
                {
                    "sent": "It's the other way around.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a reduction?",
                    "label": 0
                },
                {
                    "sent": "So there was a tutorial at ICML last year by John Langford.",
                    "label": 0
                },
                {
                    "sent": "You might want to have a look at that.",
                    "label": 0
                },
                {
                    "sent": "So what is the reduction reduction is transforming complex learning problems into simpler problems.",
                    "label": 1
                },
                {
                    "sent": "OK, so you have a structured prediction problem.",
                    "label": 0
                },
                {
                    "sent": "Why do you want to design specialized algorithms to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "Or why do you want to extend?",
                    "label": 0
                },
                {
                    "sent": "You know, existing algorithms to structured prediction cares, just reduce it.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So what is desired, of course, is.",
                    "label": 0
                },
                {
                    "sent": "If you have good performance on the core problem, so if your binary classifier is really really good.",
                    "label": 0
                },
                {
                    "sent": "It should somehow, you know, translate it to the complex problem.",
                    "label": 0
                },
                {
                    "sent": "So good performance on the core problem should imply good performance on the complex problem, so that's where the theoretical guarantees come into picture.",
                    "label": 1
                },
                {
                    "sent": "So there are lots of examples in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "There you can reduce multiclass prediction to binary classification.",
                    "label": 0
                },
                {
                    "sent": "You can reduce ranking to classification and cost sensitive classification to manage classification.",
                    "label": 1
                },
                {
                    "sent": "And this is what we're interested in this tutorial.",
                    "label": 0
                },
                {
                    "sent": "You can reduce structured prediction to binary classification.",
                    "label": 0
                },
                {
                    "sent": "That's what is called Scion.",
                    "label": 0
                },
                {
                    "sent": "Search based algorithm for structure prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, let's have a quick look at what this is.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the idea.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is reduce structured prediction to binary.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "We are going to assume that the outputs are decomposable, so outputs can be written down as a sequence of wise of length T for example.",
                    "label": 0
                },
                {
                    "sent": "What team does is it learns a policy, so the policies you can treat this as a classifier.",
                    "label": 1
                },
                {
                    "sent": "OK, it's a classifier that Maps couple tools of XY.",
                    "label": 0
                },
                {
                    "sent": "To the future prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, and that she could be anything there.",
                    "label": 0
                },
                {
                    "sent": "So it takes X and a sequence of wise and then it tells you what the next why is.",
                    "label": 0
                },
                {
                    "sent": "It turns out.",
                    "label": 0
                },
                {
                    "sent": "You could reduce the structured prediction to cost sensitive classification.",
                    "label": 1
                },
                {
                    "sent": "It's actually cost sensitive multiclass classification.",
                    "label": 1
                },
                {
                    "sent": "So that's what she does.",
                    "label": 0
                },
                {
                    "sent": "And then there are very good performance guarantees.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "There are good guarantees on custom classification and then this paper shows that these guarantees can translate your good performance on the structured prediction problem itself.",
                    "label": 1
                },
                {
                    "sent": "OK, so you take a structured prediction problem, reduce it to consider classification, take the causative classification problem, reduce it to binary classification so you have a chain of reductions there.",
                    "label": 1
                },
                {
                    "sent": "OK, so all you need is just a binary classifier and then you can solve structured prediction problems.",
                    "label": 0
                },
                {
                    "sent": "And you also should know that there is no argmax problem here.",
                    "label": 0
                },
                {
                    "sent": "There is no inference problem.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I won't go into the details of the algorithm, but I just want to quickly talk about the reduction itself, the reduction to consider classification.",
                    "label": 0
                },
                {
                    "sent": "So here's how it works.",
                    "label": 0
                },
                {
                    "sent": "So what what you want to do is you have a distribution on the structured examples, right?",
                    "label": 0
                },
                {
                    "sent": "You have a distribution on ex wise for the structured prediction problem.",
                    "label": 0
                },
                {
                    "sent": "From this distribution of examples you want to come up with a distribution over constant examples.",
                    "label": 1
                },
                {
                    "sent": "OK, constant examples are, you know you have an input and a cost vector, which tells you what the cost is for predicting a particular label.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say we have.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "KK level so each each WHI takes care values OK so the key is different from T. You should keep that in mind so T is the length of your.",
                    "label": 0
                },
                {
                    "sent": "The sequence of the retract all the label vector in your structure prediction problem and each Y itself takes K values.",
                    "label": 0
                },
                {
                    "sent": "OK, now here is how you come up with the distribution of over constant examples.",
                    "label": 0
                },
                {
                    "sent": "For every tick.",
                    "label": 0
                },
                {
                    "sent": "Every structure, particular example XY pairs.",
                    "label": 0
                },
                {
                    "sent": "Now what you do is just sample some tea from the set of 130.",
                    "label": 0
                },
                {
                    "sent": "And now run your policy or run your classifier.",
                    "label": 0
                },
                {
                    "sent": "40 -- 1 steps to give you the predictions.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are given a policy and some tree.",
                    "label": 0
                },
                {
                    "sent": "You you you.",
                    "label": 0
                },
                {
                    "sent": "Predict from Y1 to Y 2 -- 1.",
                    "label": 0
                },
                {
                    "sent": "And the input to the cost sensitive classification is generated using the steps three and three.",
                    "label": 0
                },
                {
                    "sent": "So the input is just X and the wise.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the input.",
                    "label": 0
                },
                {
                    "sent": "And the cost.",
                    "label": 0
                },
                {
                    "sent": "Now recall that for Cousins to classification you need to have a cost vector of length K right?",
                    "label": 0
                },
                {
                    "sent": "And the costs are just the expectation or.",
                    "label": 0
                },
                {
                    "sent": "The loss of the true label, and.",
                    "label": 0
                },
                {
                    "sent": "And the future predictions.",
                    "label": 0
                },
                {
                    "sent": "And then you notice that there is K in between.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this actually tells you what will happen if you predict K at the at a particular state.",
                    "label": 0
                },
                {
                    "sent": "And in reinforcement learning terms.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the cost.",
                    "label": 0
                },
                {
                    "sent": "And in constant reclassification.",
                    "label": 0
                },
                {
                    "sent": "So you might see that you know it actually generates a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "So this is actually done for.",
                    "label": 1
                },
                {
                    "sent": "Notice that this is for every structured prediction example and for every key you generate a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "But then eventually you will only be solving a constant city classification problem, which we know can be reduced to bandit classification problem.",
                    "label": 0
                },
                {
                    "sent": "So in in simple terms what you do is you take structured prediction examples, generate lots and lots of examples.",
                    "label": 0
                },
                {
                    "sent": "Of constant classification and then reduce it to binary classification.",
                    "label": 0
                },
                {
                    "sent": "You can still, you know you.",
                    "label": 0
                },
                {
                    "sent": "You can solve binary classification for large datasets and therefore there are theoretical guarantees for this particular procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are a lot of details that I'm skipping.",
                    "label": 0
                },
                {
                    "sent": "You have to design an appropriate policy and you have to take care of the loss function there.",
                    "label": 0
                },
                {
                    "sent": "But this is just to give you a high level idea of Sunworks.",
                    "label": 0
                },
                {
                    "sent": "So this is this is really interesting because you know it's it's.",
                    "label": 0
                },
                {
                    "sent": "Until now, I actually how you could, you know, move from binary classification to structured prediction.",
                    "label": 0
                },
                {
                    "sent": "But this is this is completely different.",
                    "label": 0
                },
                {
                    "sent": "You are actually moving from structure prediction to binary, which could be useful.",
                    "label": 0
                },
                {
                    "sent": "It's actually solves a lot of NLP problems.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you have any questions now?",
                    "label": 0
                },
                {
                    "sent": "OK, we're almost done.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think this is this is this concludes part one, so we have seen different loss functions.",
                    "label": 0
                },
                {
                    "sent": "Loss function for binary classification.",
                    "label": 0
                },
                {
                    "sent": "Loss functions were structured prediction and we have talked about discriminative methods for structured prediction.",
                    "label": 1
                },
                {
                    "sent": "I told you how to extend Perceptron to structured perceptron least squares regression to kernel dependency estimation, support vector machines, Destructors, VM's.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression to see our apps.",
                    "label": 0
                },
                {
                    "sent": "And we have also seen the reverse direction which is reducing structured prediction to Carson City classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so that concludes part one.",
                    "label": 0
                },
                {
                    "sent": "After the break, Thomas will take over for the Part 2.",
                    "label": 0
                },
                {
                    "sent": "So and in Part 2, Thomas will mostly talk about what happens if you cannot solve the inference problem exactly.",
                    "label": 0
                },
                {
                    "sent": "What are the implications of using approximate inference and what kind of problems do you face there?",
                    "label": 0
                },
                {
                    "sent": "What is the complexity of learning an?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Is it even possible to design learning algorithms that do not use any inference at all?",
                    "label": 0
                },
                {
                    "sent": "So that's what we will talk about in the new training algorithms so it will talk about algorithms where you solve it slightly different, completely different problem which is different from, you know, the standard argmax imprints.",
                    "label": 0
                },
                {
                    "sent": "So that's Part 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, forgot to mention this.",
                    "label": 0
                },
                {
                    "sent": "The slides for Part 2 are not yet online.",
                    "label": 0
                },
                {
                    "sent": "So it will be made online after their tutorial.",
                    "label": 0
                },
                {
                    "sent": "So if you check the website I have slides for part one and Part 2, but the slides Thomas is going to use a different from what is there on the website so.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}