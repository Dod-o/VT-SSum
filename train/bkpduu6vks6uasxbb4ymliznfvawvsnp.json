{
    "id": "bkpduu6vks6uasxbb4ymliznfvawvsnp",
    "title": "Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models",
    "info": {
        "author": [
            "Edward Challis, Dept. of Computer Science, University College London"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/aistats2011_challis_gaussian/",
    "segmentation": [
        [
            "OK, so I'm presenting work I've done with my supervisor, David Barber at University College London, so on concave Gaussian variational approximations for basean generalized linear models.",
            "OK, so.",
            "OK, oh just to give you a brief overview of the scope and context of the work we do.",
            "Bayesian linear models are really widely used right, and typically these problems can be quite low."
        ],
        [
            "Large, so both the data dimension and the number of data points can be in the hundreds of."
        ],
        [
            "1000 what's more, the posteriors are usually non Gaussian, right and intractable, and so we need approximate inference methods."
        ],
        [
            "But here we focus just on methods that give us a lower bound on the log marginal likelihood."
        ],
        [
            "And traditionally two methods are used to obtain those local."
        ],
        [
            "To obtain those variational bounds, either local variational bounding methods or variation."
        ],
        [
            "Location bounding methods and our contribution is within the VG bound."
        ],
        [
            "Relational Gaussian bounding setting, right?",
            "So we provide a relationship."
        ],
        [
            "Variational Gaussian bounds and local variational bounds.",
            "We showed."
        ],
        [
            "For large class models, the VG bound is tighter than the local bound.",
            "That the VG objective is concave and that the VG bound can be implemented and optimized quite cheaply, OK?",
            "So, um."
        ],
        [
            "So."
        ],
        [
            "So yeah, so in Beijing date with."
        ],
        [
            "We need to get the posterior right.",
            "We get that using Bayes rule and we have to compute this integral here, right?",
            "So we need to calculate the marginal likelihood.",
            "But in generalized linear modeling specifically right, often our posteriors look something like this, right?",
            "So we have some Gaussian function on WR parameters and then a product over end of site functions right five?",
            "So it's linear because the argument of fire here is linear in W and generalized because.",
            "Flight can be some nonlinear function, so either the Gaussian term here like correspond to the prior or.",
            "Then the product on N. Here is the likelihood, but it can also be the other way around, right?",
            "So the Gaussian might be our likelihood on some additive Gaussian noise model or something like that, right?",
            "So computing the marginal likelihood here is intractable usually."
        ],
        [
            "OK, so basically just a question, is a classic example right?",
            "So we so we have binary class labels are site function Phi.",
            "Here is that estik sigmoid.",
            "Then we have a graphical model that looks something like this and Gaussian prior on our weights W right?",
            "So then each each probability of each class label is multis 5 SN on W, transpose X and we get posterior.",
            "That looks like this so.",
            "Pictorially, we just have some two class data, but it's a Gaussian prior on W."
        ],
        [
            "We go likelihood that looks like this and then the product of our prior and likelihood here has this form right and we need to integrate that non."
        ],
        [
            "Russian non standard form to get the posterior OK so."
        ],
        [
            "To briefly review the intuition behind the local variational bounding method in the variational Gaussian planning techniques, they say if we have an integral said that is a product of N slight functions, F, right?",
            "And we don't know how to compute this integral.",
            "So and we want to."
        ],
        [
            "Bound it, one method we could do that is by bounding the integrand itself, right?",
            "So if we can replace each function F here with the function G such that G lower bounds F and that we know how to integrate the."
        ],
        [
            "Doctor those cheese, then we can just make that substitution compute the integral, and then optimize respect to the end variational parameters side right to get the best lower bound on that said."
        ],
        [
            "Another method is just use the KL divergent right so the kullback Leibler divergent.",
            "So if we define a distribution P as the integrand divided by Zedd right and then we calculate the KL between some Gaussian distribution Q and this defined P, then we can just rearrange terms and get an expression for a lower bound on the log marginal likelihood, right?",
            "And so this technique.",
            "This latter method really tries to.",
            "Approximate the integral as a whole OK.",
            "So the obvious question is, what's the relationship between both these methods?",
            "So specifically."
        ],
        [
            "In this generalized linear model set up that we had earlier, where are marginal likelihood looks like this, right?",
            "The idea is to replace each site function Phi with a function as a squared exponential form, right?",
            "So on the left.",
            "Here I plot.",
            "The red curve is like the when the slide function is logistic sigmoid and the blue dotted curve is squared exponential lower bound right and then on the right is another example.",
            "So here the five function is the Laplace function and we have the squared exponential lower bound in blue, right?",
            "So since the product of squared exponential is also squared exponential, we can just collect all the terms and express this.",
            "This product of five functions as this lower bound right?",
            "So lower bound it with this.",
            "With this term here we just substitute in and we can compute the integral and we have to optimize it.",
            "Respect those in variational parameters."
        ],
        [
            "So the variational Gaussian bound in this generalized linear model set up that we have looks like this.",
            "So we have the first time.",
            "Here is the entropy of our Gaussian distribution Q and the angle brackets here correspond to taking the expectation with respect to that Gaussian Q. OK so.",
            "What we're trying to do here is very the parameters of some Gaussian distribution such that it globally tries to capture the mass of our posterior right in terms of minimizing the KL divergent between it.",
            "So that's what this picture we have some some posterior in red and then in blue is the is the tightest Gaussian approximation to it in terms of minimizing the KL right?",
            "And that's what gives us the best bound on the marginal likelihood.",
            "OK, so while both of these methods been known about some time.",
            "Local variational methods received a lot more attention by the community.",
            "OK, so recently.",
            "Matthias Seeger, showing that the local variational bound is convex with respect to the variational parameters, sigh.",
            "He's also developed fast, scalable algorithm algorithms that have allowed the local bounding procedure to be applied to much larger problems than ever before.",
            "One consequence of these implementations, however, is that computing the bound in these large, large problems is not itself tractable, so we can optimize the parameters, but we don't get a value for the lower bound."
        ],
        [
            "And as you can see, there's not much is known about the Fiji procedure.",
            "So what we show in the paper is what we provide in the paper is we show that."
        ],
        [
            "In fact, the variational Gaussian bound is concave with respect to the mean covariance of the Gaussian Q.",
            "That optimization can be made scalable by using constraint parameterisations of covariance that computing the bound itself is scalable.",
            "So we can compute lower bounds for the marginal likelihood in large systems, and that for unconstrained parameterisations of covariance.",
            "A large class of models.",
            "The VG bound is tighter than the local.",
            "OK, so I'm going to talk about that."
        ],
        [
            "That last point first, so.",
            "If we want to investigate the relationship between the variational Gaussian bound and the local bound, one of the simplest ways to do that is just that.",
            "Why don't we just substitute in our local bound on our potential function Phi, right?",
            "So if we substitute that into RVG bound, we get this expression that we could be Tilda.",
            "So what I tried to show in this what this picture is trying to show right is that on the X axis, here we're varying MNS and on the why we've got these these two bounds.",
            "So the red red red curve here is B~ and the blue blue curve is the variational Gaussian bound, so will be showing the paper is that if you optimize be tilled or respect to Ms at that optimum, it's in fact equivalent to the local variational bound.",
            "So what that means is since B~ is less than the VG bound, we can just substitute those optimal moments.",
            "In those optimal moments into the VG bound to attain a tighter approximation, right?",
            "But that's not necessarily the optimum of the VG bound, so we can achieve even tighter approximation by optimizing the variational Gaussian bound itself.",
            "OK, so the experiments in the paper showed that actually this bound value could be quite significant at the moments don't count.",
            "The moments don't coincide and that the difference in bound value isn't uniform across different models, right?",
            "So so either procedure suggests different models optimal in terms of in terms of abound in the module, like here."
        ],
        [
            "So the neck."
        ],
        [
            "Thing we address is the scalability of the variational Gaussian bounding procedure, and obviously a desirable property for any objectivism you want to optimize its concavity.",
            "So here's the VG bound the first time.",
            "Here is just the entropy of Q, and that's essentially just the log determinant of covariance.",
            "So here we parameterized covariance matrix S in terms of its Cholesky decomposition.",
            "The next time here is just essentially a negative quadratic in M&C, so that's concave.",
            "The only term.",
            "There's any doubt about here is the log file time right so?",
            "Way we look at this is by.",
            "Expressing this term in another form so since.",
            "So since the argument of fire here is linear in W right, but you can just project the statistics of our Gaussian Q onto that vector H and then we can transform variables so on the left.",
            "Here we have an expectation with respect to a multivariate Gaussian and then on the right.",
            "Here we have the expectation respect for univariate Stan."
        ],
        [
            "Normal Gaussian right?",
            "So we just project those down and then take that expectation.",
            "Interestingly.",
            "The integrand of this expectation here is not concave with respect to the variational parameters, so here we very very efficient parameters and see how this term changes right?",
            "So that's actually not a non concave function, But the expectation as a whole is, and so the balance concave.",
            "So we have this."
        ],
        [
            "Result.",
            "So obviously storing and optimizing with respect to the dsquared parameters.",
            "You need to specify a full Cholesky matrix is going to be infeasible and not scalable.",
            "So for a full Cholesky matrix, a single bound ingredi."
        ],
        [
            "Computation scales order ND squared right?",
            "So one of the simplest ways we can reduce that complexity is just by imposing some sort of sparsity on our Cholesky matrix, right?",
            "So the easiest way to do that is to use some sort of banded structure.",
            "And for Bandwidth K, the complexity reduces to NDK, right?",
            "So about grading Commission scales order NDK.",
            "But the disadvantage of this method, of course is that the covariance matrix resulting from this factorization is also going to be sparse.",
            "So we're explicitly ignoring all covariances index out of this.",
            "This bandwidth, right?",
            "So to overcome that?"
        ],
        [
            "We looked at another sparsity restriction which has this kind of sparsity structure.",
            "And then so the covariance resulting from this spot Cholesky matrix with this sparsity structure is non sparse, right?",
            "So we are making no explicit assumptions about factorization or independence upfront, so that's quite nice and bounded gradient computation, scale order, MDK.",
            "We look at a couple of other parameterizations in the paper, which I'm not going to look at here.",
            "Look at now, but the point to really the things I want to say is that there exists scalable parameterisations of covariance that don't make explicit assumptions about factorization, which respect the concavity of the bound.",
            "That's the point here."
        ],
        [
            "So.",
            "Yeah, just."
        ],
        [
            "To briefly review some of the experiments so the first like one of the examples is a beige and logistic regression task with 36,000 training points and 20,000 dimensions.",
            "So the data here is quite sparse .25% sparsity.",
            "Um?",
            "So we implemented it for the variational Gaussian bound using a diagonal covariance Chevron parameterization with 100 columns.",
            "And we compare those results to the local local using a local solver.",
            "Almost local, fast, scalable solver right?",
            "And the only thing really, I want you to take home from this is that it's very easy to apply the VG method to problems of this size that we can achieve convergence within a matter of like just a few minutes in comparison with fast local methods and that we can compute the bound in these systems.",
            "So another classic example of Beijing."
        ],
        [
            "Your modeling is inverse modeling, so here we have.",
            "We observed some wide dimensional.",
            "We have some real vector Y and we wish to infer its higher dimensional latent cause W right so?",
            "So we assume this generative model Y = M * W plus some additive Gaussian noise where M is fat right terms of that matrix and typically impose sparsity on our prior right?",
            "So here we use the Laplace prior with hyperparameter Tau.",
            "So just to just to highlight the kind of differences in bound value, we just apply this to a toy inverse modeling problem, right?",
            "So we just simulate an inverse."
        ],
        [
            "Bing program.",
            "Where the problem is small enough so that we can implement local local methods and evaluate evaluate the local variational bound.",
            "So here on the X axis we very towel and then we plot the bound value obtained using different using the variational Gaussian bound, the local bound and different parameterisations of covariance.",
            "So this green curve here is the VG bound calculated using a full Cholesky matrix.",
            "The blue curve is the VG power calculated using a Chevron parameterisation, then black.",
            "Here is the local bound and then read.",
            "Here is the local bound optimized using the fast solver right with it, which uses a rank 50 covariance approximation.",
            "So the thing to note is that each each bound suggests different optimal hyperparameters, right?",
            "So the difference in bound value can be quite big, but also you know the consequences.",
            "Which boundary use has consequences for things like model selection and hyperparameter learning?",
            "OK, so just to summarize."
        ],
        [
            "We provide we showed in a large class of models, VG bounds.",
            "The tighter local bounds with unconstrained covariance."
        ],
        [
            "With regards to the variational cache and scalability."
        ],
        [
            "Is concave constraint parameterisations scalable and the bound itself is tractable in large systems?",
            "And also Lastly, a practical thing is that implementation is really quite simple, right?",
            "So we just need to calculate the gradients and we can use off the shelf optimizers."
        ],
        [
            "Thanks this code available."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm presenting work I've done with my supervisor, David Barber at University College London, so on concave Gaussian variational approximations for basean generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, oh just to give you a brief overview of the scope and context of the work we do.",
                    "label": 0
                },
                {
                    "sent": "Bayesian linear models are really widely used right, and typically these problems can be quite low.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large, so both the data dimension and the number of data points can be in the hundreds of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1000 what's more, the posteriors are usually non Gaussian, right and intractable, and so we need approximate inference methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here we focus just on methods that give us a lower bound on the log marginal likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And traditionally two methods are used to obtain those local.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To obtain those variational bounds, either local variational bounding methods or variation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location bounding methods and our contribution is within the VG bound.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relational Gaussian bounding setting, right?",
                    "label": 0
                },
                {
                    "sent": "So we provide a relationship.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variational Gaussian bounds and local variational bounds.",
                    "label": 0
                },
                {
                    "sent": "We showed.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For large class models, the VG bound is tighter than the local bound.",
                    "label": 1
                },
                {
                    "sent": "That the VG objective is concave and that the VG bound can be implemented and optimized quite cheaply, OK?",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, so in Beijing date with.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to get the posterior right.",
                    "label": 0
                },
                {
                    "sent": "We get that using Bayes rule and we have to compute this integral here, right?",
                    "label": 0
                },
                {
                    "sent": "So we need to calculate the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "But in generalized linear modeling specifically right, often our posteriors look something like this, right?",
                    "label": 0
                },
                {
                    "sent": "So we have some Gaussian function on WR parameters and then a product over end of site functions right five?",
                    "label": 0
                },
                {
                    "sent": "So it's linear because the argument of fire here is linear in W and generalized because.",
                    "label": 0
                },
                {
                    "sent": "Flight can be some nonlinear function, so either the Gaussian term here like correspond to the prior or.",
                    "label": 0
                },
                {
                    "sent": "Then the product on N. Here is the likelihood, but it can also be the other way around, right?",
                    "label": 0
                },
                {
                    "sent": "So the Gaussian might be our likelihood on some additive Gaussian noise model or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So computing the marginal likelihood here is intractable usually.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically just a question, is a classic example right?",
                    "label": 0
                },
                {
                    "sent": "So we so we have binary class labels are site function Phi.",
                    "label": 0
                },
                {
                    "sent": "Here is that estik sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Then we have a graphical model that looks something like this and Gaussian prior on our weights W right?",
                    "label": 0
                },
                {
                    "sent": "So then each each probability of each class label is multis 5 SN on W, transpose X and we get posterior.",
                    "label": 0
                },
                {
                    "sent": "That looks like this so.",
                    "label": 0
                },
                {
                    "sent": "Pictorially, we just have some two class data, but it's a Gaussian prior on W.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go likelihood that looks like this and then the product of our prior and likelihood here has this form right and we need to integrate that non.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Russian non standard form to get the posterior OK so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To briefly review the intuition behind the local variational bounding method in the variational Gaussian planning techniques, they say if we have an integral said that is a product of N slight functions, F, right?",
                    "label": 0
                },
                {
                    "sent": "And we don't know how to compute this integral.",
                    "label": 0
                },
                {
                    "sent": "So and we want to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bound it, one method we could do that is by bounding the integrand itself, right?",
                    "label": 0
                },
                {
                    "sent": "So if we can replace each function F here with the function G such that G lower bounds F and that we know how to integrate the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctor those cheese, then we can just make that substitution compute the integral, and then optimize respect to the end variational parameters side right to get the best lower bound on that said.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another method is just use the KL divergent right so the kullback Leibler divergent.",
                    "label": 0
                },
                {
                    "sent": "So if we define a distribution P as the integrand divided by Zedd right and then we calculate the KL between some Gaussian distribution Q and this defined P, then we can just rearrange terms and get an expression for a lower bound on the log marginal likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "And so this technique.",
                    "label": 0
                },
                {
                    "sent": "This latter method really tries to.",
                    "label": 0
                },
                {
                    "sent": "Approximate the integral as a whole OK.",
                    "label": 0
                },
                {
                    "sent": "So the obvious question is, what's the relationship between both these methods?",
                    "label": 1
                },
                {
                    "sent": "So specifically.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this generalized linear model set up that we had earlier, where are marginal likelihood looks like this, right?",
                    "label": 0
                },
                {
                    "sent": "The idea is to replace each site function Phi with a function as a squared exponential form, right?",
                    "label": 0
                },
                {
                    "sent": "So on the left.",
                    "label": 0
                },
                {
                    "sent": "Here I plot.",
                    "label": 0
                },
                {
                    "sent": "The red curve is like the when the slide function is logistic sigmoid and the blue dotted curve is squared exponential lower bound right and then on the right is another example.",
                    "label": 0
                },
                {
                    "sent": "So here the five function is the Laplace function and we have the squared exponential lower bound in blue, right?",
                    "label": 0
                },
                {
                    "sent": "So since the product of squared exponential is also squared exponential, we can just collect all the terms and express this.",
                    "label": 0
                },
                {
                    "sent": "This product of five functions as this lower bound right?",
                    "label": 0
                },
                {
                    "sent": "So lower bound it with this.",
                    "label": 0
                },
                {
                    "sent": "With this term here we just substitute in and we can compute the integral and we have to optimize it.",
                    "label": 0
                },
                {
                    "sent": "Respect those in variational parameters.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the variational Gaussian bound in this generalized linear model set up that we have looks like this.",
                    "label": 1
                },
                {
                    "sent": "So we have the first time.",
                    "label": 0
                },
                {
                    "sent": "Here is the entropy of our Gaussian distribution Q and the angle brackets here correspond to taking the expectation with respect to that Gaussian Q. OK so.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do here is very the parameters of some Gaussian distribution such that it globally tries to capture the mass of our posterior right in terms of minimizing the KL divergent between it.",
                    "label": 0
                },
                {
                    "sent": "So that's what this picture we have some some posterior in red and then in blue is the is the tightest Gaussian approximation to it in terms of minimizing the KL right?",
                    "label": 0
                },
                {
                    "sent": "And that's what gives us the best bound on the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so while both of these methods been known about some time.",
                    "label": 0
                },
                {
                    "sent": "Local variational methods received a lot more attention by the community.",
                    "label": 0
                },
                {
                    "sent": "OK, so recently.",
                    "label": 0
                },
                {
                    "sent": "Matthias Seeger, showing that the local variational bound is convex with respect to the variational parameters, sigh.",
                    "label": 1
                },
                {
                    "sent": "He's also developed fast, scalable algorithm algorithms that have allowed the local bounding procedure to be applied to much larger problems than ever before.",
                    "label": 0
                },
                {
                    "sent": "One consequence of these implementations, however, is that computing the bound in these large, large problems is not itself tractable, so we can optimize the parameters, but we don't get a value for the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you can see, there's not much is known about the Fiji procedure.",
                    "label": 0
                },
                {
                    "sent": "So what we show in the paper is what we provide in the paper is we show that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, the variational Gaussian bound is concave with respect to the mean covariance of the Gaussian Q.",
                    "label": 0
                },
                {
                    "sent": "That optimization can be made scalable by using constraint parameterisations of covariance that computing the bound itself is scalable.",
                    "label": 0
                },
                {
                    "sent": "So we can compute lower bounds for the marginal likelihood in large systems, and that for unconstrained parameterisations of covariance.",
                    "label": 0
                },
                {
                    "sent": "A large class of models.",
                    "label": 0
                },
                {
                    "sent": "The VG bound is tighter than the local.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm going to talk about that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That last point first, so.",
                    "label": 0
                },
                {
                    "sent": "If we want to investigate the relationship between the variational Gaussian bound and the local bound, one of the simplest ways to do that is just that.",
                    "label": 0
                },
                {
                    "sent": "Why don't we just substitute in our local bound on our potential function Phi, right?",
                    "label": 0
                },
                {
                    "sent": "So if we substitute that into RVG bound, we get this expression that we could be Tilda.",
                    "label": 0
                },
                {
                    "sent": "So what I tried to show in this what this picture is trying to show right is that on the X axis, here we're varying MNS and on the why we've got these these two bounds.",
                    "label": 0
                },
                {
                    "sent": "So the red red red curve here is B~ and the blue blue curve is the variational Gaussian bound, so will be showing the paper is that if you optimize be tilled or respect to Ms at that optimum, it's in fact equivalent to the local variational bound.",
                    "label": 0
                },
                {
                    "sent": "So what that means is since B~ is less than the VG bound, we can just substitute those optimal moments.",
                    "label": 0
                },
                {
                    "sent": "In those optimal moments into the VG bound to attain a tighter approximation, right?",
                    "label": 0
                },
                {
                    "sent": "But that's not necessarily the optimum of the VG bound, so we can achieve even tighter approximation by optimizing the variational Gaussian bound itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so the experiments in the paper showed that actually this bound value could be quite significant at the moments don't count.",
                    "label": 0
                },
                {
                    "sent": "The moments don't coincide and that the difference in bound value isn't uniform across different models, right?",
                    "label": 0
                },
                {
                    "sent": "So so either procedure suggests different models optimal in terms of in terms of abound in the module, like here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the neck.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing we address is the scalability of the variational Gaussian bounding procedure, and obviously a desirable property for any objectivism you want to optimize its concavity.",
                    "label": 0
                },
                {
                    "sent": "So here's the VG bound the first time.",
                    "label": 0
                },
                {
                    "sent": "Here is just the entropy of Q, and that's essentially just the log determinant of covariance.",
                    "label": 0
                },
                {
                    "sent": "So here we parameterized covariance matrix S in terms of its Cholesky decomposition.",
                    "label": 0
                },
                {
                    "sent": "The next time here is just essentially a negative quadratic in M&C, so that's concave.",
                    "label": 0
                },
                {
                    "sent": "The only term.",
                    "label": 0
                },
                {
                    "sent": "There's any doubt about here is the log file time right so?",
                    "label": 0
                },
                {
                    "sent": "Way we look at this is by.",
                    "label": 0
                },
                {
                    "sent": "Expressing this term in another form so since.",
                    "label": 0
                },
                {
                    "sent": "So since the argument of fire here is linear in W right, but you can just project the statistics of our Gaussian Q onto that vector H and then we can transform variables so on the left.",
                    "label": 0
                },
                {
                    "sent": "Here we have an expectation with respect to a multivariate Gaussian and then on the right.",
                    "label": 0
                },
                {
                    "sent": "Here we have the expectation respect for univariate Stan.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normal Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "So we just project those down and then take that expectation.",
                    "label": 0
                },
                {
                    "sent": "Interestingly.",
                    "label": 0
                },
                {
                    "sent": "The integrand of this expectation here is not concave with respect to the variational parameters, so here we very very efficient parameters and see how this term changes right?",
                    "label": 0
                },
                {
                    "sent": "So that's actually not a non concave function, But the expectation as a whole is, and so the balance concave.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Result.",
                    "label": 0
                },
                {
                    "sent": "So obviously storing and optimizing with respect to the dsquared parameters.",
                    "label": 0
                },
                {
                    "sent": "You need to specify a full Cholesky matrix is going to be infeasible and not scalable.",
                    "label": 0
                },
                {
                    "sent": "So for a full Cholesky matrix, a single bound ingredi.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Computation scales order ND squared right?",
                    "label": 0
                },
                {
                    "sent": "So one of the simplest ways we can reduce that complexity is just by imposing some sort of sparsity on our Cholesky matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So the easiest way to do that is to use some sort of banded structure.",
                    "label": 0
                },
                {
                    "sent": "And for Bandwidth K, the complexity reduces to NDK, right?",
                    "label": 0
                },
                {
                    "sent": "So about grading Commission scales order NDK.",
                    "label": 0
                },
                {
                    "sent": "But the disadvantage of this method, of course is that the covariance matrix resulting from this factorization is also going to be sparse.",
                    "label": 0
                },
                {
                    "sent": "So we're explicitly ignoring all covariances index out of this.",
                    "label": 0
                },
                {
                    "sent": "This bandwidth, right?",
                    "label": 0
                },
                {
                    "sent": "So to overcome that?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at another sparsity restriction which has this kind of sparsity structure.",
                    "label": 0
                },
                {
                    "sent": "And then so the covariance resulting from this spot Cholesky matrix with this sparsity structure is non sparse, right?",
                    "label": 0
                },
                {
                    "sent": "So we are making no explicit assumptions about factorization or independence upfront, so that's quite nice and bounded gradient computation, scale order, MDK.",
                    "label": 0
                },
                {
                    "sent": "We look at a couple of other parameterizations in the paper, which I'm not going to look at here.",
                    "label": 0
                },
                {
                    "sent": "Look at now, but the point to really the things I want to say is that there exists scalable parameterisations of covariance that don't make explicit assumptions about factorization, which respect the concavity of the bound.",
                    "label": 0
                },
                {
                    "sent": "That's the point here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To briefly review some of the experiments so the first like one of the examples is a beige and logistic regression task with 36,000 training points and 20,000 dimensions.",
                    "label": 1
                },
                {
                    "sent": "So the data here is quite sparse .25% sparsity.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we implemented it for the variational Gaussian bound using a diagonal covariance Chevron parameterization with 100 columns.",
                    "label": 0
                },
                {
                    "sent": "And we compare those results to the local local using a local solver.",
                    "label": 0
                },
                {
                    "sent": "Almost local, fast, scalable solver right?",
                    "label": 0
                },
                {
                    "sent": "And the only thing really, I want you to take home from this is that it's very easy to apply the VG method to problems of this size that we can achieve convergence within a matter of like just a few minutes in comparison with fast local methods and that we can compute the bound in these systems.",
                    "label": 0
                },
                {
                    "sent": "So another classic example of Beijing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your modeling is inverse modeling, so here we have.",
                    "label": 0
                },
                {
                    "sent": "We observed some wide dimensional.",
                    "label": 0
                },
                {
                    "sent": "We have some real vector Y and we wish to infer its higher dimensional latent cause W right so?",
                    "label": 0
                },
                {
                    "sent": "So we assume this generative model Y = M * W plus some additive Gaussian noise where M is fat right terms of that matrix and typically impose sparsity on our prior right?",
                    "label": 0
                },
                {
                    "sent": "So here we use the Laplace prior with hyperparameter Tau.",
                    "label": 0
                },
                {
                    "sent": "So just to just to highlight the kind of differences in bound value, we just apply this to a toy inverse modeling problem, right?",
                    "label": 0
                },
                {
                    "sent": "So we just simulate an inverse.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing program.",
                    "label": 0
                },
                {
                    "sent": "Where the problem is small enough so that we can implement local local methods and evaluate evaluate the local variational bound.",
                    "label": 0
                },
                {
                    "sent": "So here on the X axis we very towel and then we plot the bound value obtained using different using the variational Gaussian bound, the local bound and different parameterisations of covariance.",
                    "label": 0
                },
                {
                    "sent": "So this green curve here is the VG bound calculated using a full Cholesky matrix.",
                    "label": 0
                },
                {
                    "sent": "The blue curve is the VG power calculated using a Chevron parameterisation, then black.",
                    "label": 0
                },
                {
                    "sent": "Here is the local bound and then read.",
                    "label": 0
                },
                {
                    "sent": "Here is the local bound optimized using the fast solver right with it, which uses a rank 50 covariance approximation.",
                    "label": 0
                },
                {
                    "sent": "So the thing to note is that each each bound suggests different optimal hyperparameters, right?",
                    "label": 0
                },
                {
                    "sent": "So the difference in bound value can be quite big, but also you know the consequences.",
                    "label": 0
                },
                {
                    "sent": "Which boundary use has consequences for things like model selection and hyperparameter learning?",
                    "label": 0
                },
                {
                    "sent": "OK, so just to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We provide we showed in a large class of models, VG bounds.",
                    "label": 0
                },
                {
                    "sent": "The tighter local bounds with unconstrained covariance.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With regards to the variational cache and scalability.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is concave constraint parameterisations scalable and the bound itself is tractable in large systems?",
                    "label": 1
                },
                {
                    "sent": "And also Lastly, a practical thing is that implementation is really quite simple, right?",
                    "label": 0
                },
                {
                    "sent": "So we just need to calculate the gradients and we can use off the shelf optimizers.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks this code available.",
                    "label": 0
                }
            ]
        }
    }
}