{
    "id": "dztn3wfw3yf4vachor6hkv6xe2qil3b2",
    "title": "Towards Computer Understanding of Human Interactions",
    "info": {
        "author": [
            "Iain Mccowan, Dalle Molle Institute for Perceptual Artificial Intelligence"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2004",
        "category": [
            "Top->Computer Science->Human Computer Interaction"
        ]
    },
    "url": "http://videolectures.net/mlmi04ch_mccowan_tcuhi/",
    "segmentation": [
        [
            "The talk I have the paper we submitted to this workshop was entitled towards computer understanding of human interactions, and it was basically a vague overview of a lot of research areas that we're doing at Eddie apps such as microphone arrays, speaker verification, and other other things.",
            "But today to make the talk a bit more focused and in depth, I'm going to focus on one of the areas that we think would probably more interesting to the general audience say, which is recognizing sequences of meeting actions.",
            "And this is work that's been done with a number of colleagues at Eddie App.",
            "As you can see that."
        ],
        [
            "So our basic view of meetings and how we're approaching them is to view meetings as sequences of actions, and this is really a fairly intuitive view of meetings.",
            "If you think about it because we're all familiar with seeing summaries of meetings, agendas or minutes of meetings which are essentially sequences of key things that happen throughout the meeting, now keep phases of the meetings, summarize discussion points, things like this so our research is aiming.",
            "Around this view to automatically structure meetings as sequences of meeting actions where we see meeting actions as being properties of the meeting that really actions from the meeting that really belong to the group as a whole rather than one particular individual."
        ],
        [
            "So just to give you a quick view of what we're trying to do is as our goal, we'd like to eventually, you know, have automatically an automatic system that can split up the meeting here, something like this.",
            "For example, we could have in one view of the meeting we could see that it's broken up by a presentation for a certain.",
            "Followed by a discussion.",
            "Maybe if we look at the topic that the group was discussing, maybe at the start Fred was talking about his beach holiday before he really got into the nitty gritty of the budget and.",
            "At another level, related to Jonathan's point, before that, most of what's in meetings is is fairly boring, and we really want to skip the important parts.",
            "It would be nice if we could, we can structure the meeting as a sequence is of high interest points versus low interest points by judging the interest level of the participants in the meeting, and eventually maybe a high you know a long distance view will be to try and recognize that the task that the groups doing, whether they're sharing information when they're making decisions."
        ],
        [
            "Storming things like this is what we're trying to work towards, and this framework really poses 3 research problems that each.",
            "I mean there's a lot of research to be done in each of these questions here.",
            "Probably my talk today will focus on the third one, so the three ones are what meeting actions.",
            "Can we define?",
            "What evidence can we observe in the meeting to allow us to recognize these actions, and how can we model the underlying process that's generating the observations?"
        ],
        [
            "So quickly as I just presented before, I guess the meeting actions that we want to define depends on the view that we're looking at.",
            "The view that we require for the meeting.",
            "As examples I presented before, and so we can see that there's probably we can view probably multiple parallel views of the meeting depending on what thing we're interested in looking at.",
            "So we we pose this problem then, as as recognizing parallel sets of meeting actions where each set of meeting action.",
            "Should address a particular research question or describe one particular view of a meeting and within that set the actions in that set should be mutually exclusive and exhaustive, just so it's nice."
        ],
        [
            "For computers.",
            "So in defining these as many different ways that we could define mean action sets, the ones that I'm going to talk about in this talk today are probably more technology driven for our initial things.",
            "We just basically asked the question what sort of actions do we think we could do with state of the art technology?",
            "Other ways you could do it would be application driven, the sort of things that Pierre was presenting yesterday.",
            "For example in the browser technologies, what queries the users want to do, what things are they interested in finding in the meetings?",
            "We could use that to motivate.",
            "Action sets that we could define that could be theoretical motivations.",
            "For example, there's group research in social psychology and we could grab coding systems from that and try to recognize them, or else we could do automatically basically get the observations that we can and see what we can cluster automatically there to see what the key key phases are."
        ],
        [
            "To observe these similar to Jonathan's talk this morning, we've set up a instrumented meeting room at at the app which has a number of cameras and microphone array in the center of the table.",
            "In our effort to solve the farfield problem, and.",
            "This is actually a fairly old version of the meeting room, but it's the one that relates to the experiments they will present later in the talk.",
            "For a more up-to-date version of the meeting room with a few more devices and fancy things, Darren Moore and some others have posted this afternoon if you're interested.",
            "But basically we have three widish angle cameras, microphone array, lapel microphones for each person, and we've got a whiteboard and projector screen in the meeting room, and all of our meetings at the moment."
        ],
        [
            "Four person meetings just to keep it simple for us.",
            "So from this from these cameras, microphones and other devices that we've now got in the meeting room, we can extract a number of multimodal observations.",
            "I've got multi in quotations there because obviously it's only bimodal and we're always we have a tendency to use multi to mean to basically just, but I think with the new device that we've got in the meeting room now and if we if we consider for example speech words or emotions or.",
            "Your notes that we're taking all the presentation slides modalities, then I think we're moving towards truly multimodal observations in future work.",
            "But what we've done today, what we've done.",
            "To date.",
            "We've used features from the audio modality.",
            "For example, the activity audio activity in certain regions of the room, the pitch, and the energy and the speaking rate taken from lapel microphones and in the visual modality.",
            "We've got things like the this person's head centroid.",
            "Their hand positions and maybe the angles of their arms and things like this.",
            "So we extract all these automatically from the different cameras and microphones that we've got in the room, and also from the from the front on wide angle camera.",
            "We extract features that tell us when someone's presenting when someone is physically in front of the white board or in front of the presentation area."
        ],
        [
            "So skip quite quickly through all that and the major focus of the talk that I want to present is how we're addressing the problem of modeling these meeting actions, and I'd say that there's we see that there's really 2 main important characteristics of the meeting actions.",
            "The first one is the multimodal nature, which I've already described.",
            "We all know that people act through not only just speech, but also gestures, expressions.",
            "They gaze they could be using devices that could be writing notes.",
            "There could be other audio nonspeech cues such as laughter or backchannels, so we can see that an individual could be acting through visual modality, audio mentality, or other things.",
            "And in modeling these the the modalities of 1 certain person and not necessarily that could be important.",
            "The correlation between the modalities.",
            "But we could also think that there could be the observations in these modalities could be occurring at different rates and maybe asynchronously with one another.",
            "So there.",
            "Could be important things that we need to consider in modeling them.",
            "And Secondly, there's the group nature of the meeting.",
            "There's not just one person in the meeting who's acting in there multiple modalities, but you could see a meeting as a as a multiple streams were each stream as an individual person acting in a multimodal manner.",
            "So in general, meeting actions are a property of the group of as a whole rather than one particular person.",
            "And again, these people could be acting asynchronously.",
            "For example, one person could start a presentation while some people are still.",
            "Participating in the previous discussion, etc."
        ],
        [
            "So the way we're we've approached so far to to modeling these is based on HMMS, which I assume are fairly well known by the audience here.",
            "But in particular, we've looked at HMM variants that are capable of modeling multiple interacting streams, because that's how we see the problem for the meetings and some variants that we've looked at include.",
            "I put early integration hmm, which is basically just a single hmm with features or concatenate into one stream, but that's certainly 11 way of modeling multiple streams, in which we're assuming that the streams evolved frame synchronously and the full correlation between the features important.",
            "A second way is to decouple the streams and have a multi stream hmm in which we train each stream independently and then we can recombine the likelihoods.",
            "Certain link anchor points like at the end of a end of an action or even.",
            "At the end of each frame.",
            "And this this decouples the features between the stream, so it allows independence between the feature level, which can also be an advantage in that if there's noise in a particular stream, then it's not going to affect the other ones, and it can allow some interstream asynchrony depending on where you put the anchor points.",
            "A couple of other ones."
        ],
        [
            "Looked at asynchronous hmm which is a model that was proposed by Sammy Benjo a couple of years ago now in which multiple streams are modeled using a single state sequence but given state in that sequence may emit on one or more of the streams that given time depending on a synchronization variable.",
            "And this has a really good advantage of modeling the full feature level correlation as well as allowing asynchrony in different data rates between the streams.",
            "And final one we looked at as a couple of hmm which is similar to the multistream hmm, but where there are coupling links also between the different streams and this is in effect allows some different streams to have some form of causal effect on on other ones and in some way this models correlation at the state level rather feature level."
        ],
        [
            "So we looked at these different HMM models and the idea was to investigate different ways of combining audio and visual modalities as two streams or else look at the problem as a four stream problem in which the people were streams and look at different ways of combining them with these models.",
            "Initially we just proposed an intuitive or we thought it was intuitive set of eight meeting actions of things that we we expected would happen in meetings, including monologues by any of the participants of presentation, whiteboard discussion, or or group notetaking, where everyone obediently takes notes at the same time for us to research this, we started by just scripting a corpus.",
            "So we basically made a 5 minute meetings where we told people you know from here to here this person should talk and then from here to here you should take notes.",
            "From here to here, someone should present so it was very, very artificial in that sense.",
            "But within that, within those constraints, people acted naturally, and so at the signal level at least and at the personal behavior level that the data is is realistic and natural, and it's allowed us to to make initial start in some of these research and to evaluate, we look at the action error rate, which is basically the same as the word error rate used in speech recognition."
        ],
        [
            "So, just presenting summarizing the results.",
            "In summary, the best system that we had was the early integration.",
            "Hmm for this one, in which we are modeling the full correlation of all the features and individuals.",
            "And this system gave us 9% action error rate and to show you what that means.",
            "I've got the confusion matrix of the actions.",
            "There we can see that a lot of the actions were discussion but also it's quite successful in discriminating the monologues and the note taking and presentation, whiteboard etc.",
            "I guess more interesting Lee, if I could summarize."
        ],
        [
            "Findings that we found in really analyzing how the different models worked in modeling different streams and the difference relative differences between the models.",
            "Just force summarize findings were one there was a benefit to a multimodal approach.",
            "The audio, visual models or consistently outperformed either just audio or just visual ones found that was very important to model the correlation between individual participants.",
            "When we D coupled the individual features into a multi stream.",
            "Hmm, we found that performed alot alot worse than the models that captured some of the correlation between people.",
            "We found that the group action level at least.",
            "So we just.",
            "Where we model each individual's audio features altogether and each individual's visual features.",
            "Altogether, we found that there was no significant improvement to be gained by by allowing some asynchrony between the audio and visual modalities.",
            "And we also found.",
            "However, we did find that there was important asynchrony between in between individual people acting within the within the group meant actions.",
            "So from this just quickly, we've gone really."
        ],
        [
            "This and looked at a too late approach, so the system are presented so far is basically this.",
            "We go directly from the features to the group level actions, but we could.",
            "In a layered approach, we could first try to recognize a certain set of individual actions for each person, and then recognize the group actions from the."
        ],
        [
            "Ace and I won't go into detail on this layout hmm approach because I know the keynote speaker later, Nuria Oliver will be talking about layout HMMS, but basically this model has a number of key advantages, specially in our context where we've got limited training data and we've got problems basically of the complexity of the problem related to the amount of data, but we've got to train our models and it by decoupling it into two layers.",
            "We get the advantage that each hmm has a small observation space to model.",
            "Because the individual HMMS to recognize individual actions are person independent, we can train them effectively with four times the amount of data because we can train them with data from each participant and there's some other advice."
        ],
        [
            "It's just there.",
            "Just to show you what we're talking about there, so we can.",
            "We can, for example, train individual HMMS to segment a person stream in terms of speaking, writing and when they are idle, and then try to recognize at the group level when there's for example monologue and notetaking discussion, etc."
        ],
        [
            "So for these experiments, just quickly we extended the because we're had better discrimination with this model, we extended the action set to also include things where there was monologue and notetaking presentation and notetaking.",
            "And comparing with the single layer HMM system we had was the same, gave 24% action error rate and the two layer system improved this to 15% action error rate which is a significant improvement.",
            "And here the best results were obtained when the individual layer was modeled with the audiovisual asynchronous hmm.",
            "So this showed us that for a single person is important to model to allow to allow some asynchrony between their audio and visual modalities.",
            "In this case, while previous experiments showed that at the group level, it wasn't interesting at the individual level at least it's interesting to be able to model this."
        ],
        [
            "So I'm going research.",
            "We have presented some of the things we've done there, but we're also progressing on other things.",
            "For example, one of the main things that stopped us progressing further, I guess is is the data that we've had access to to this point.",
            "So currently we're putting a big effort into recording a much richer data corpus to our more research into into more actions.",
            "We've also looked at.",
            "As a simple set of actions on the group level of interest were basically we've got high level interest or neutral as two classes, and on that we've achieved some promising initial results of around 73% frame accuracy.",
            "Recognizing the higher neutral interest level points from the same feature set voice.",
            "So done unsupervised clustering of the meeting actions, which has been very promising an we're also currently trying to investigate tractable approximations for asynchronous Hmm's in many streams, which is a big problem."
        ],
        [
            "In summary, I think the problem we're looking at and the problem we presented today, certainly it's a it's a problem for ongoing research.",
            "It's not something we've solved, and because of you know it's a big job.",
            "We haven't progressed necessarily that far, but we think it's a very interesting research task.",
            "Within the meetings domain, there are lot of questions to be answered and challenges for the research community, such as how to deal with many different modalities that we're going to be having in the meeting room.",
            "How do we know how to progress from these low level features to also incorporate higher level?",
            "Features such as words or emotions, how to efficiently model large number of interacting streams, etc etc.",
            "So thank you.",
            "So questions.",
            "Coffee.",
            "So I was very interested by one of your first slides, who showed various passive various factors, actually that may help you choose a set of meeting actions.",
            "So it was Thierry potential applications feasibility.",
            "So my question is, are you planning a still richer set of meeting actions going beyond the 4th game that you've shown?",
            "And if yes, which is the main factor that will?",
            "This friend is finding a new set, so the answer is yes, particularly within the Amy project we're currently working on defining different phenomena that people are interested in researching in different ways that we can view on meeting, meeting model and defining.",
            "Yeah, for example, the dialogue acts that were interested in looking at the group level or maybe group tasks and things like this, and to define these different phenomenon where trying to annotate on this new corpus where we're resorting to theory researching.",
            "Literature in the field, but also, you know, a certain amount of intuition and practical aspects related to the technologies that we've gotten in practicalities of actually doing the annotation and things like this.",
            "So I think we're resorting to all the different ways of defining them, and we are doing it.",
            "OK collaboration.",
            "Picture.",
            "This English did the New Jersey we're extracting for the audio.",
            "You say more knowledge is because so the features we were currently used from the lapel microphone for each participant, we look at the pitch, the energy and the speaking rate, but the main feature that we found interesting is taken from the microphone array and it's based on the forgiven or for the participants region around their seat or around the presentation or whiteboard area.",
            "We extract basically a measure of the speech activity in that region using the microphone array.",
            "And that's proven to be a very useful feature for for the actions we were looking at places.",
            "Thank you, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The talk I have the paper we submitted to this workshop was entitled towards computer understanding of human interactions, and it was basically a vague overview of a lot of research areas that we're doing at Eddie apps such as microphone arrays, speaker verification, and other other things.",
                    "label": 1
                },
                {
                    "sent": "But today to make the talk a bit more focused and in depth, I'm going to focus on one of the areas that we think would probably more interesting to the general audience say, which is recognizing sequences of meeting actions.",
                    "label": 0
                },
                {
                    "sent": "And this is work that's been done with a number of colleagues at Eddie App.",
                    "label": 0
                },
                {
                    "sent": "As you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our basic view of meetings and how we're approaching them is to view meetings as sequences of actions, and this is really a fairly intuitive view of meetings.",
                    "label": 1
                },
                {
                    "sent": "If you think about it because we're all familiar with seeing summaries of meetings, agendas or minutes of meetings which are essentially sequences of key things that happen throughout the meeting, now keep phases of the meetings, summarize discussion points, things like this so our research is aiming.",
                    "label": 0
                },
                {
                    "sent": "Around this view to automatically structure meetings as sequences of meeting actions where we see meeting actions as being properties of the meeting that really actions from the meeting that really belong to the group as a whole rather than one particular individual.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you a quick view of what we're trying to do is as our goal, we'd like to eventually, you know, have automatically an automatic system that can split up the meeting here, something like this.",
                    "label": 0
                },
                {
                    "sent": "For example, we could have in one view of the meeting we could see that it's broken up by a presentation for a certain.",
                    "label": 0
                },
                {
                    "sent": "Followed by a discussion.",
                    "label": 0
                },
                {
                    "sent": "Maybe if we look at the topic that the group was discussing, maybe at the start Fred was talking about his beach holiday before he really got into the nitty gritty of the budget and.",
                    "label": 0
                },
                {
                    "sent": "At another level, related to Jonathan's point, before that, most of what's in meetings is is fairly boring, and we really want to skip the important parts.",
                    "label": 0
                },
                {
                    "sent": "It would be nice if we could, we can structure the meeting as a sequence is of high interest points versus low interest points by judging the interest level of the participants in the meeting, and eventually maybe a high you know a long distance view will be to try and recognize that the task that the groups doing, whether they're sharing information when they're making decisions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Storming things like this is what we're trying to work towards, and this framework really poses 3 research problems that each.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a lot of research to be done in each of these questions here.",
                    "label": 0
                },
                {
                    "sent": "Probably my talk today will focus on the third one, so the three ones are what meeting actions.",
                    "label": 0
                },
                {
                    "sent": "Can we define?",
                    "label": 0
                },
                {
                    "sent": "What evidence can we observe in the meeting to allow us to recognize these actions, and how can we model the underlying process that's generating the observations?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quickly as I just presented before, I guess the meeting actions that we want to define depends on the view that we're looking at.",
                    "label": 1
                },
                {
                    "sent": "The view that we require for the meeting.",
                    "label": 0
                },
                {
                    "sent": "As examples I presented before, and so we can see that there's probably we can view probably multiple parallel views of the meeting depending on what thing we're interested in looking at.",
                    "label": 1
                },
                {
                    "sent": "So we we pose this problem then, as as recognizing parallel sets of meeting actions where each set of meeting action.",
                    "label": 1
                },
                {
                    "sent": "Should address a particular research question or describe one particular view of a meeting and within that set the actions in that set should be mutually exclusive and exhaustive, just so it's nice.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For computers.",
                    "label": 0
                },
                {
                    "sent": "So in defining these as many different ways that we could define mean action sets, the ones that I'm going to talk about in this talk today are probably more technology driven for our initial things.",
                    "label": 1
                },
                {
                    "sent": "We just basically asked the question what sort of actions do we think we could do with state of the art technology?",
                    "label": 1
                },
                {
                    "sent": "Other ways you could do it would be application driven, the sort of things that Pierre was presenting yesterday.",
                    "label": 0
                },
                {
                    "sent": "For example in the browser technologies, what queries the users want to do, what things are they interested in finding in the meetings?",
                    "label": 0
                },
                {
                    "sent": "We could use that to motivate.",
                    "label": 0
                },
                {
                    "sent": "Action sets that we could define that could be theoretical motivations.",
                    "label": 1
                },
                {
                    "sent": "For example, there's group research in social psychology and we could grab coding systems from that and try to recognize them, or else we could do automatically basically get the observations that we can and see what we can cluster automatically there to see what the key key phases are.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To observe these similar to Jonathan's talk this morning, we've set up a instrumented meeting room at at the app which has a number of cameras and microphone array in the center of the table.",
                    "label": 0
                },
                {
                    "sent": "In our effort to solve the farfield problem, and.",
                    "label": 0
                },
                {
                    "sent": "This is actually a fairly old version of the meeting room, but it's the one that relates to the experiments they will present later in the talk.",
                    "label": 1
                },
                {
                    "sent": "For a more up-to-date version of the meeting room with a few more devices and fancy things, Darren Moore and some others have posted this afternoon if you're interested.",
                    "label": 0
                },
                {
                    "sent": "But basically we have three widish angle cameras, microphone array, lapel microphones for each person, and we've got a whiteboard and projector screen in the meeting room, and all of our meetings at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Four person meetings just to keep it simple for us.",
                    "label": 0
                },
                {
                    "sent": "So from this from these cameras, microphones and other devices that we've now got in the meeting room, we can extract a number of multimodal observations.",
                    "label": 0
                },
                {
                    "sent": "I've got multi in quotations there because obviously it's only bimodal and we're always we have a tendency to use multi to mean to basically just, but I think with the new device that we've got in the meeting room now and if we if we consider for example speech words or emotions or.",
                    "label": 0
                },
                {
                    "sent": "Your notes that we're taking all the presentation slides modalities, then I think we're moving towards truly multimodal observations in future work.",
                    "label": 0
                },
                {
                    "sent": "But what we've done today, what we've done.",
                    "label": 0
                },
                {
                    "sent": "To date.",
                    "label": 0
                },
                {
                    "sent": "We've used features from the audio modality.",
                    "label": 0
                },
                {
                    "sent": "For example, the activity audio activity in certain regions of the room, the pitch, and the energy and the speaking rate taken from lapel microphones and in the visual modality.",
                    "label": 0
                },
                {
                    "sent": "We've got things like the this person's head centroid.",
                    "label": 0
                },
                {
                    "sent": "Their hand positions and maybe the angles of their arms and things like this.",
                    "label": 0
                },
                {
                    "sent": "So we extract all these automatically from the different cameras and microphones that we've got in the room, and also from the from the front on wide angle camera.",
                    "label": 0
                },
                {
                    "sent": "We extract features that tell us when someone's presenting when someone is physically in front of the white board or in front of the presentation area.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So skip quite quickly through all that and the major focus of the talk that I want to present is how we're addressing the problem of modeling these meeting actions, and I'd say that there's we see that there's really 2 main important characteristics of the meeting actions.",
                    "label": 1
                },
                {
                    "sent": "The first one is the multimodal nature, which I've already described.",
                    "label": 1
                },
                {
                    "sent": "We all know that people act through not only just speech, but also gestures, expressions.",
                    "label": 0
                },
                {
                    "sent": "They gaze they could be using devices that could be writing notes.",
                    "label": 0
                },
                {
                    "sent": "There could be other audio nonspeech cues such as laughter or backchannels, so we can see that an individual could be acting through visual modality, audio mentality, or other things.",
                    "label": 1
                },
                {
                    "sent": "And in modeling these the the modalities of 1 certain person and not necessarily that could be important.",
                    "label": 1
                },
                {
                    "sent": "The correlation between the modalities.",
                    "label": 0
                },
                {
                    "sent": "But we could also think that there could be the observations in these modalities could be occurring at different rates and maybe asynchronously with one another.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 1
                },
                {
                    "sent": "Could be important things that we need to consider in modeling them.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, there's the group nature of the meeting.",
                    "label": 0
                },
                {
                    "sent": "There's not just one person in the meeting who's acting in there multiple modalities, but you could see a meeting as a as a multiple streams were each stream as an individual person acting in a multimodal manner.",
                    "label": 1
                },
                {
                    "sent": "So in general, meeting actions are a property of the group of as a whole rather than one particular person.",
                    "label": 0
                },
                {
                    "sent": "And again, these people could be acting asynchronously.",
                    "label": 0
                },
                {
                    "sent": "For example, one person could start a presentation while some people are still.",
                    "label": 0
                },
                {
                    "sent": "Participating in the previous discussion, etc.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we're we've approached so far to to modeling these is based on HMMS, which I assume are fairly well known by the audience here.",
                    "label": 0
                },
                {
                    "sent": "But in particular, we've looked at HMM variants that are capable of modeling multiple interacting streams, because that's how we see the problem for the meetings and some variants that we've looked at include.",
                    "label": 1
                },
                {
                    "sent": "I put early integration hmm, which is basically just a single hmm with features or concatenate into one stream, but that's certainly 11 way of modeling multiple streams, in which we're assuming that the streams evolved frame synchronously and the full correlation between the features important.",
                    "label": 0
                },
                {
                    "sent": "A second way is to decouple the streams and have a multi stream hmm in which we train each stream independently and then we can recombine the likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Certain link anchor points like at the end of a end of an action or even.",
                    "label": 0
                },
                {
                    "sent": "At the end of each frame.",
                    "label": 0
                },
                {
                    "sent": "And this this decouples the features between the stream, so it allows independence between the feature level, which can also be an advantage in that if there's noise in a particular stream, then it's not going to affect the other ones, and it can allow some interstream asynchrony depending on where you put the anchor points.",
                    "label": 0
                },
                {
                    "sent": "A couple of other ones.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looked at asynchronous hmm which is a model that was proposed by Sammy Benjo a couple of years ago now in which multiple streams are modeled using a single state sequence but given state in that sequence may emit on one or more of the streams that given time depending on a synchronization variable.",
                    "label": 1
                },
                {
                    "sent": "And this has a really good advantage of modeling the full feature level correlation as well as allowing asynchrony in different data rates between the streams.",
                    "label": 0
                },
                {
                    "sent": "And final one we looked at as a couple of hmm which is similar to the multistream hmm, but where there are coupling links also between the different streams and this is in effect allows some different streams to have some form of causal effect on on other ones and in some way this models correlation at the state level rather feature level.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we looked at these different HMM models and the idea was to investigate different ways of combining audio and visual modalities as two streams or else look at the problem as a four stream problem in which the people were streams and look at different ways of combining them with these models.",
                    "label": 0
                },
                {
                    "sent": "Initially we just proposed an intuitive or we thought it was intuitive set of eight meeting actions of things that we we expected would happen in meetings, including monologues by any of the participants of presentation, whiteboard discussion, or or group notetaking, where everyone obediently takes notes at the same time for us to research this, we started by just scripting a corpus.",
                    "label": 1
                },
                {
                    "sent": "So we basically made a 5 minute meetings where we told people you know from here to here this person should talk and then from here to here you should take notes.",
                    "label": 0
                },
                {
                    "sent": "From here to here, someone should present so it was very, very artificial in that sense.",
                    "label": 1
                },
                {
                    "sent": "But within that, within those constraints, people acted naturally, and so at the signal level at least and at the personal behavior level that the data is is realistic and natural, and it's allowed us to to make initial start in some of these research and to evaluate, we look at the action error rate, which is basically the same as the word error rate used in speech recognition.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just presenting summarizing the results.",
                    "label": 0
                },
                {
                    "sent": "In summary, the best system that we had was the early integration.",
                    "label": 0
                },
                {
                    "sent": "Hmm for this one, in which we are modeling the full correlation of all the features and individuals.",
                    "label": 0
                },
                {
                    "sent": "And this system gave us 9% action error rate and to show you what that means.",
                    "label": 0
                },
                {
                    "sent": "I've got the confusion matrix of the actions.",
                    "label": 0
                },
                {
                    "sent": "There we can see that a lot of the actions were discussion but also it's quite successful in discriminating the monologues and the note taking and presentation, whiteboard etc.",
                    "label": 0
                },
                {
                    "sent": "I guess more interesting Lee, if I could summarize.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Findings that we found in really analyzing how the different models worked in modeling different streams and the difference relative differences between the models.",
                    "label": 0
                },
                {
                    "sent": "Just force summarize findings were one there was a benefit to a multimodal approach.",
                    "label": 1
                },
                {
                    "sent": "The audio, visual models or consistently outperformed either just audio or just visual ones found that was very important to model the correlation between individual participants.",
                    "label": 1
                },
                {
                    "sent": "When we D coupled the individual features into a multi stream.",
                    "label": 1
                },
                {
                    "sent": "Hmm, we found that performed alot alot worse than the models that captured some of the correlation between people.",
                    "label": 0
                },
                {
                    "sent": "We found that the group action level at least.",
                    "label": 0
                },
                {
                    "sent": "So we just.",
                    "label": 0
                },
                {
                    "sent": "Where we model each individual's audio features altogether and each individual's visual features.",
                    "label": 0
                },
                {
                    "sent": "Altogether, we found that there was no significant improvement to be gained by by allowing some asynchrony between the audio and visual modalities.",
                    "label": 1
                },
                {
                    "sent": "And we also found.",
                    "label": 0
                },
                {
                    "sent": "However, we did find that there was important asynchrony between in between individual people acting within the within the group meant actions.",
                    "label": 0
                },
                {
                    "sent": "So from this just quickly, we've gone really.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This and looked at a too late approach, so the system are presented so far is basically this.",
                    "label": 0
                },
                {
                    "sent": "We go directly from the features to the group level actions, but we could.",
                    "label": 0
                },
                {
                    "sent": "In a layered approach, we could first try to recognize a certain set of individual actions for each person, and then recognize the group actions from the.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ace and I won't go into detail on this layout hmm approach because I know the keynote speaker later, Nuria Oliver will be talking about layout HMMS, but basically this model has a number of key advantages, specially in our context where we've got limited training data and we've got problems basically of the complexity of the problem related to the amount of data, but we've got to train our models and it by decoupling it into two layers.",
                    "label": 0
                },
                {
                    "sent": "We get the advantage that each hmm has a small observation space to model.",
                    "label": 1
                },
                {
                    "sent": "Because the individual HMMS to recognize individual actions are person independent, we can train them effectively with four times the amount of data because we can train them with data from each participant and there's some other advice.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just there.",
                    "label": 0
                },
                {
                    "sent": "Just to show you what we're talking about there, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can, for example, train individual HMMS to segment a person stream in terms of speaking, writing and when they are idle, and then try to recognize at the group level when there's for example monologue and notetaking discussion, etc.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for these experiments, just quickly we extended the because we're had better discrimination with this model, we extended the action set to also include things where there was monologue and notetaking presentation and notetaking.",
                    "label": 1
                },
                {
                    "sent": "And comparing with the single layer HMM system we had was the same, gave 24% action error rate and the two layer system improved this to 15% action error rate which is a significant improvement.",
                    "label": 0
                },
                {
                    "sent": "And here the best results were obtained when the individual layer was modeled with the audiovisual asynchronous hmm.",
                    "label": 1
                },
                {
                    "sent": "So this showed us that for a single person is important to model to allow to allow some asynchrony between their audio and visual modalities.",
                    "label": 0
                },
                {
                    "sent": "In this case, while previous experiments showed that at the group level, it wasn't interesting at the individual level at least it's interesting to be able to model this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going research.",
                    "label": 0
                },
                {
                    "sent": "We have presented some of the things we've done there, but we're also progressing on other things.",
                    "label": 0
                },
                {
                    "sent": "For example, one of the main things that stopped us progressing further, I guess is is the data that we've had access to to this point.",
                    "label": 0
                },
                {
                    "sent": "So currently we're putting a big effort into recording a much richer data corpus to our more research into into more actions.",
                    "label": 0
                },
                {
                    "sent": "We've also looked at.",
                    "label": 0
                },
                {
                    "sent": "As a simple set of actions on the group level of interest were basically we've got high level interest or neutral as two classes, and on that we've achieved some promising initial results of around 73% frame accuracy.",
                    "label": 1
                },
                {
                    "sent": "Recognizing the higher neutral interest level points from the same feature set voice.",
                    "label": 0
                },
                {
                    "sent": "So done unsupervised clustering of the meeting actions, which has been very promising an we're also currently trying to investigate tractable approximations for asynchronous Hmm's in many streams, which is a big problem.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In summary, I think the problem we're looking at and the problem we presented today, certainly it's a it's a problem for ongoing research.",
                    "label": 0
                },
                {
                    "sent": "It's not something we've solved, and because of you know it's a big job.",
                    "label": 0
                },
                {
                    "sent": "We haven't progressed necessarily that far, but we think it's a very interesting research task.",
                    "label": 1
                },
                {
                    "sent": "Within the meetings domain, there are lot of questions to be answered and challenges for the research community, such as how to deal with many different modalities that we're going to be having in the meeting room.",
                    "label": 1
                },
                {
                    "sent": "How do we know how to progress from these low level features to also incorporate higher level?",
                    "label": 0
                },
                {
                    "sent": "Features such as words or emotions, how to efficiently model large number of interacting streams, etc etc.",
                    "label": 1
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "So questions.",
                    "label": 0
                },
                {
                    "sent": "Coffee.",
                    "label": 0
                },
                {
                    "sent": "So I was very interested by one of your first slides, who showed various passive various factors, actually that may help you choose a set of meeting actions.",
                    "label": 0
                },
                {
                    "sent": "So it was Thierry potential applications feasibility.",
                    "label": 0
                },
                {
                    "sent": "So my question is, are you planning a still richer set of meeting actions going beyond the 4th game that you've shown?",
                    "label": 0
                },
                {
                    "sent": "And if yes, which is the main factor that will?",
                    "label": 0
                },
                {
                    "sent": "This friend is finding a new set, so the answer is yes, particularly within the Amy project we're currently working on defining different phenomena that people are interested in researching in different ways that we can view on meeting, meeting model and defining.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for example, the dialogue acts that were interested in looking at the group level or maybe group tasks and things like this, and to define these different phenomenon where trying to annotate on this new corpus where we're resorting to theory researching.",
                    "label": 0
                },
                {
                    "sent": "Literature in the field, but also, you know, a certain amount of intuition and practical aspects related to the technologies that we've gotten in practicalities of actually doing the annotation and things like this.",
                    "label": 0
                },
                {
                    "sent": "So I think we're resorting to all the different ways of defining them, and we are doing it.",
                    "label": 0
                },
                {
                    "sent": "OK collaboration.",
                    "label": 0
                },
                {
                    "sent": "Picture.",
                    "label": 0
                },
                {
                    "sent": "This English did the New Jersey we're extracting for the audio.",
                    "label": 0
                },
                {
                    "sent": "You say more knowledge is because so the features we were currently used from the lapel microphone for each participant, we look at the pitch, the energy and the speaking rate, but the main feature that we found interesting is taken from the microphone array and it's based on the forgiven or for the participants region around their seat or around the presentation or whiteboard area.",
                    "label": 0
                },
                {
                    "sent": "We extract basically a measure of the speech activity in that region using the microphone array.",
                    "label": 0
                },
                {
                    "sent": "And that's proven to be a very useful feature for for the actions we were looking at places.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}