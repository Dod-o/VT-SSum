{
    "id": "xfj5g4a3dy4rp6bmk22tr67iicz6f6lf",
    "title": "Efficient Mixture Modeling with RKHS Embeddings: A PAC-Bayesian Analysis",
    "info": {
        "author": [
            "Matthew Higgs, Department of Computer Science, University College London"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_higgs_emm/",
    "segmentation": [
        [
            "This talk was originally for yesterday, and it probably would have been more at home.",
            "There is not quite as elegant as the last two talks.",
            "It's a bit more basic and the material is really still in its infancy, so apologies if there's any errors.",
            "Roughly what it is is to do with the subtitle, so the title is.",
            "With regards to algorithm that I'll run through, that shows quite good.",
            "Promising experiments results, and then I'll try and take you through a construction of a PAC Bayes analysis to explain its performance.",
            "K."
        ],
        [
            "Save the talk.",
            "It's quite short.",
            "Ann and I won't be offended if you fall asleep 'cause it's quite late.",
            "It's broken down into three components.",
            "Just the first gives a bit of motivation.",
            "2nd is just first attempt at a PAC.",
            "Bayes bound that with a bit of tweaking you can fit the algorithm into.",
            "And then I'll just close with some discussion about choosing the KL term and some klukan."
        ],
        [
            "Reading comments.",
            "K so first of all."
        ],
        [
            "We're dealing with density estimation, and you could formulate that as the minimization of some distance measure over space distributions and the distance measure will be looking at is this maximum mean discrepancy?",
            "OK, so just been a notation first, P&Q are probability measures on our D. I'm going to stick to our DP&Q and not your prior and posterior.",
            "From most of the talks yesterday, I thought I'd confuse you a little bit with the notation.",
            "F is just some measurable set.",
            "Real valued functions, so set of measurable real valued functions on Rd and I'm going to use the notation F subscript Q just to donate the denote the expectation of F with respect to Q.",
            "So."
        ],
        [
            "Distance measure will be looking at is this maximum mean discrepancy now?",
            "I've cited offer grettings paper 'cause they coined the name maximum in scrap and see, but this is obviously quite standard tool in probability theory, so you choose your function class and then the measure of distance between your distributions is the Supreme of the absolute value of the expectations absolute difference of the expectations.",
            "And the properties of this distance measure follow directly from your choice of function class.",
            "So some examples."
        ],
        [
            "And actually, for each of these examples this the maximum discrepancy is a metric, so you can differentiate between unique distributions.",
            "One of the examples I haven't given, which is the one we're going to focus on, is when F is the unit ball in a reproducing Kernel Hill."
        ],
        [
            "Space.",
            "OK, so we define our kernel.",
            "It's positive, symmetric, positive definite with finite trace and we construct our reproducing Hilbert spec, kernel Hilbert space with the norm that follows from the inner product.",
            "And then I'm going to use the notation here.",
            "This K subscript Q.",
            "To donate, denote the expectation of our kernel function with respect to the probability measure Q in one of the arguments, and the importance of this object is that.",
            "The result belongs to our RKHS, and for appropriate kernels.",
            "This mapping from distributions to our KHS is injective OK, so to differentiate between two distributions we can look at their the distance between their elements in the reproducing kernel Hilbert space, and this is will explain."
        ],
        [
            "In the proposition from Song and others in 2008.",
            "So again, I'm sticking to Ord here.",
            "But it holds for more general spaces.",
            "So for any P&Q for F, the unit ball in some arc ahs your maximum mean discrepancy between Q&P is equal to the norm distance in the reproducing kernel Hilbert space of those mapped elements.",
            "OK, so we've removed a supremum.",
            "And then of course you can take advantage of the reproducing property of the kernel.",
            "To expand this out.",
            "And then you've got something you've removed the soup."
        ],
        [
            "Premium, you've got something where this.",
            "This is this expectation with respect both arguments.",
            "So on Q&P and you end up with something that you can compute up to the accuracy of your approximation of an integral, but for certain dense put certain distributions Q and certain kernels K. This actually can have a closed form expression as well, and So what this does or did is to motivate."
        ],
        [
            "The reproducing kernel moment matching algorithm of the paper that I just cited from song.",
            "Where you have."
        ],
        [
            "A sample of IID data.",
            "And we're going to restrict our parameters to the open multinomial manifold.",
            "OK, so we're going to look at our mixture.",
            "And that's what we'll be learning.",
            "OK, we'll have some posterior.",
            "OK, not a posterior some approximation Q theater, which is a mixture of candidate measures, probability measures.",
            "And because we've said that it's a mixture an because."
        ],
        [
            "Is of this form?",
            "You can rewrite minimize."
        ],
        [
            "Ocean.",
            "Of this squared norm.",
            "As a quadratic program.",
            "OK, where are regularization parameters being thrown in to deal with overfitting for the fact that we're minimizing with respect to the empirical distribution?"
        ],
        [
            "So the question is, can this even be fit into a pet base style analysis and what I'm going to do next is to run through.",
            "So yeah, what pipe Agensys counties do we have for this minimum?"
        ],
        [
            "Is I'm going to run through a PAC Bayes bound quite a simple one.",
            "That's quite simple to derive and then show how.",
            "With a bit of tweaking, fit the algorithm into it."
        ],
        [
            "OK, so you've got some IID sample.",
            "You've got a measurable set of, so really this should be a parameter space for our.",
            "You statistic kernels or C not related to the kernels before.",
            "To define our KHS these are to define our used cystic that bounded between A&B and I'm going to look at a normalized you statistics I'm calling at U bar S where I've just normalized using ambah with mean U bar and this is so bitter cheating But so that I can scale it down to 01 and fit it into the standard loss functions the KL loss.",
            "And the fici, which is the Bernoulli local playstyle loss."
        ],
        [
            "So I've called them corroborates because they follow from a more general theorem.",
            "So for any prior on our parameter set of used at kernels with probability greater than one minus Delta over the draw of the sample for any row on use that kernels.",
            "We have these two equations.",
            "They look very similar to the standard PAC Bayes bounds.",
            "Apart from the fact we have second order you statistics in here and we have this N / 2 rounded down to close integer which as you might guess suggests that in the proof simply going to decompose the usage statistic into ID blocks, which is exactly what I do."
        ],
        [
            "Hey guys.",
            "Of order two, yes, did I?",
            "I didn't say that today.",
            "Well HXX pro, so yeah I've ordered 2."
        ],
        [
            "Um?",
            "So yeah, just define the permutation."
        ],
        [
            "On one up 10 to find out IID blocks using standard method and then.",
            "I want to look at the KL loss and the Bernoulli log replaced our loss, so I'm just going to do it for general convex function on 01 square."
        ],
        [
            "Ed.",
            "Then you get this general theorem so.",
            "For any Pi probability on my cell to the joy of F and hero get this.",
            "And where this is the Laplace transform?",
            "And then by using Jensen's inequality.",
            "So you split that up into the IID blocks and then obviously take these some an the 1 / N factorial outside using Jensen's.",
            "And then you can just bound these using standard methods.",
            "But noting that these are 01 variables, I'm going to use."
        ],
        [
            "Lemma for Maura which you have Jenny actually pointed out to me, but it's quite a general result.",
            "The for any convex function.",
            "If you're banning the expectation over I 01 IID variables then it's always up bounded by the expectation of the convex function using the Bernoulli random variables such that that means are the same.",
            "OK, and that."
        ],
        [
            "Just means that you can use just upper bound this by the.",
            "Bernoulli random variables with the same mean and just use the standard analysis that you've seen or yesterday and today.",
            "Noting that this is still one.",
            "And now we have this N / 2 rounded down, where this is of order with M as said before.",
            "So we've got."
        ],
        [
            "These for this use statistic.",
            "Question is."
        ],
        [
            "Do we actually have use?"
        ],
        [
            "Autistic, and the answer is no, but we almost do.",
            "'cause we we introduced this bias term.",
            "Which is quite nice quantity in itself.",
            "So here the key features are approximation.",
            "You're mapping that to the reproducing kernel Hilbert space.",
            "You're then mapping each of your.",
            "Examples to reproduce and kind of Hilbert space as well.",
            "And it's like a variance term in the reproducing kernel Hilbert space."
        ],
        [
            "And then just to ease my notation, going to use KL subscript half.",
            "Obviously this is still convex in both arguments and O for Q = P, and I'm just so we remember that.",
            "Approximation is some mixture of candidate densities.",
            "And you get for any reproducing kernel Hilbert space."
        ],
        [
            "With F, the unit ball in H any prior pie so member in the zornow mixture parameters with probability greater than one minus Delta overdraw.",
            "The sample for any posterior rohat with mean theater hat.",
            "This holds where.",
            "We've got the bias term here, but this is over N -- 1, so for reasonable when it shrinks down, the CK is just to re scale this so we can fit it into the KL an.",
            "And we end up.",
            "So we pay.",
            "This is the penalty we pay for deviating to EU statistic bound and we get twice the KL here, which I'll see.",
            "I'll just do a brief of the proof.",
            "So we define our use."
        ],
        [
            "Autistic indexed by theater theater prime."
        ],
        [
            "And we then plug into the used at bound, but I just gave but we take rho squared, an Pi squared probability product.",
            "Probability measures on the multinomial manifold squared.",
            "So so probably put.",
            "For any pie, and then for any row after heroes squared off to here.",
            "So you plug them in.",
            "And then obviously the KL between Rose Garden Pi squared is just twice the KL between Rome pie and see I've just condense that down 'cause we're not worried about that at the moment."
        ],
        [
            "And then you can.",
            "You're paying the price for the bias term.",
            "You just commute because we're taking with respect to this product, measure can commute it inside in a product.",
            "And likely taking respect expectation with respect to the sample.",
            "Because we've got this XI not equal to XJ to get that."
        ],
        [
            "Say now just a little bit on choosing the K out 'cause."
        ],
        [
            "We we have this bound, but we are dealing with distributions on the multinomial manifold.",
            "So an obvious choice.",
            "Would be to take the richley."
        ],
        [
            "With.",
            "Rohas mean theater hat.",
            "And an uninformative prior.",
            "So if you're thinking of your, you've got your mixture, your candidate densities for mixture, just if they all say the same, you're uninformative.",
            "It's just like a pozan standard parzen window estimator.",
            "Then the KL has quite a complicated form where you have to deal with gamma and digamma functions, so another."
        ],
        [
            "Possibility.",
            "Is just to project your multinomial manifold onto the parallel hyperplane that passes through the origin.",
            "Using our mapping towel.",
            "With GC2 is the geometric mean."
        ],
        [
            "So it's two RL, but it's to the hyperplane, the parallel hyperplane that passes through the origin.",
            "And then you can define a. Gaussian place a Gaussian on.",
            "The projection.",
            "OK, so this.",
            "Grad subscript Theta Tau is the Jacobian of the town matrix to ensure that this is a density.",
            "This is.",
            "The book Measure NRL, but restricted to.",
            "The most you know manifold."
        ],
        [
            "Then you can change the coordinates to Tao and integrate.",
            "When evaluating the KL to get.",
            "Uh.",
            "Something that is analogous to the case, say for the SVM.",
            "When you're just pacing isotropic Gaussians.",
            "OK, so now I've got this regularization term that is essentially a squared norm of your map points.",
            "K."
        ],
        [
            "That actually was shorter than I thought.",
            "OK, so some."
        ],
        [
            "Maybe just a simple path based bound on maximum mean script and see between the input distribution and a mixture based approximation given a class of prime posterior with simple KL divergent.",
            "And then future work will look at finding project."
        ],
        [
            "Actions from the multinomial manifold 2 RL and distributions Rohan Pai.",
            "That'll give you good Cal for optimization if you wanted to look at minimization of the bound, 'cause I'm not sure."
        ],
        [
            "So as of yet with this mapping, how?",
            "How well we have to deal with that.",
            "And all."
        ],
        [
            "So.",
            "The obvious thing this was a first attempt is just to examine that directly and not taking the deviation to AU statistic bound and you can see else here.",
            "If you write out, expand out these squared norms.",
            "The squared KQ, Theta hat terms cancel, so it's linear in KQ, Theta hat and then you need to look at that so.",
            "If you did deviate to EU statistic method then you could not decompose into ID blocks.",
            "You could apply.",
            "Maybe some Martin Gale argument, or you could forget you statistics and go directly for this and that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk was originally for yesterday, and it probably would have been more at home.",
                    "label": 0
                },
                {
                    "sent": "There is not quite as elegant as the last two talks.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more basic and the material is really still in its infancy, so apologies if there's any errors.",
                    "label": 0
                },
                {
                    "sent": "Roughly what it is is to do with the subtitle, so the title is.",
                    "label": 0
                },
                {
                    "sent": "With regards to algorithm that I'll run through, that shows quite good.",
                    "label": 0
                },
                {
                    "sent": "Promising experiments results, and then I'll try and take you through a construction of a PAC Bayes analysis to explain its performance.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Save the talk.",
                    "label": 0
                },
                {
                    "sent": "It's quite short.",
                    "label": 0
                },
                {
                    "sent": "Ann and I won't be offended if you fall asleep 'cause it's quite late.",
                    "label": 0
                },
                {
                    "sent": "It's broken down into three components.",
                    "label": 0
                },
                {
                    "sent": "Just the first gives a bit of motivation.",
                    "label": 0
                },
                {
                    "sent": "2nd is just first attempt at a PAC.",
                    "label": 0
                },
                {
                    "sent": "Bayes bound that with a bit of tweaking you can fit the algorithm into.",
                    "label": 0
                },
                {
                    "sent": "And then I'll just close with some discussion about choosing the KL term and some klukan.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reading comments.",
                    "label": 0
                },
                {
                    "sent": "K so first of all.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're dealing with density estimation, and you could formulate that as the minimization of some distance measure over space distributions and the distance measure will be looking at is this maximum mean discrepancy?",
                    "label": 1
                },
                {
                    "sent": "OK, so just been a notation first, P&Q are probability measures on our D. I'm going to stick to our DP&Q and not your prior and posterior.",
                    "label": 1
                },
                {
                    "sent": "From most of the talks yesterday, I thought I'd confuse you a little bit with the notation.",
                    "label": 0
                },
                {
                    "sent": "F is just some measurable set.",
                    "label": 0
                },
                {
                    "sent": "Real valued functions, so set of measurable real valued functions on Rd and I'm going to use the notation F subscript Q just to donate the denote the expectation of F with respect to Q.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distance measure will be looking at is this maximum mean discrepancy now?",
                    "label": 1
                },
                {
                    "sent": "I've cited offer grettings paper 'cause they coined the name maximum in scrap and see, but this is obviously quite standard tool in probability theory, so you choose your function class and then the measure of distance between your distributions is the Supreme of the absolute value of the expectations absolute difference of the expectations.",
                    "label": 0
                },
                {
                    "sent": "And the properties of this distance measure follow directly from your choice of function class.",
                    "label": 0
                },
                {
                    "sent": "So some examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, for each of these examples this the maximum discrepancy is a metric, so you can differentiate between unique distributions.",
                    "label": 0
                },
                {
                    "sent": "One of the examples I haven't given, which is the one we're going to focus on, is when F is the unit ball in a reproducing Kernel Hill.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "OK, so we define our kernel.",
                    "label": 0
                },
                {
                    "sent": "It's positive, symmetric, positive definite with finite trace and we construct our reproducing Hilbert spec, kernel Hilbert space with the norm that follows from the inner product.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to use the notation here.",
                    "label": 0
                },
                {
                    "sent": "This K subscript Q.",
                    "label": 0
                },
                {
                    "sent": "To donate, denote the expectation of our kernel function with respect to the probability measure Q in one of the arguments, and the importance of this object is that.",
                    "label": 0
                },
                {
                    "sent": "The result belongs to our RKHS, and for appropriate kernels.",
                    "label": 0
                },
                {
                    "sent": "This mapping from distributions to our KHS is injective OK, so to differentiate between two distributions we can look at their the distance between their elements in the reproducing kernel Hilbert space, and this is will explain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the proposition from Song and others in 2008.",
                    "label": 0
                },
                {
                    "sent": "So again, I'm sticking to Ord here.",
                    "label": 0
                },
                {
                    "sent": "But it holds for more general spaces.",
                    "label": 0
                },
                {
                    "sent": "So for any P&Q for F, the unit ball in some arc ahs your maximum mean discrepancy between Q&P is equal to the norm distance in the reproducing kernel Hilbert space of those mapped elements.",
                    "label": 1
                },
                {
                    "sent": "OK, so we've removed a supremum.",
                    "label": 0
                },
                {
                    "sent": "And then of course you can take advantage of the reproducing property of the kernel.",
                    "label": 0
                },
                {
                    "sent": "To expand this out.",
                    "label": 0
                },
                {
                    "sent": "And then you've got something you've removed the soup.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Premium, you've got something where this.",
                    "label": 0
                },
                {
                    "sent": "This is this expectation with respect both arguments.",
                    "label": 0
                },
                {
                    "sent": "So on Q&P and you end up with something that you can compute up to the accuracy of your approximation of an integral, but for certain dense put certain distributions Q and certain kernels K. This actually can have a closed form expression as well, and So what this does or did is to motivate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reproducing kernel moment matching algorithm of the paper that I just cited from song.",
                    "label": 0
                },
                {
                    "sent": "Where you have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sample of IID data.",
                    "label": 0
                },
                {
                    "sent": "And we're going to restrict our parameters to the open multinomial manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to look at our mixture.",
                    "label": 0
                },
                {
                    "sent": "And that's what we'll be learning.",
                    "label": 0
                },
                {
                    "sent": "OK, we'll have some posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, not a posterior some approximation Q theater, which is a mixture of candidate measures, probability measures.",
                    "label": 0
                },
                {
                    "sent": "And because we've said that it's a mixture an because.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is of this form?",
                    "label": 0
                },
                {
                    "sent": "You can rewrite minimize.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "Of this squared norm.",
                    "label": 0
                },
                {
                    "sent": "As a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "OK, where are regularization parameters being thrown in to deal with overfitting for the fact that we're minimizing with respect to the empirical distribution?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, can this even be fit into a pet base style analysis and what I'm going to do next is to run through.",
                    "label": 0
                },
                {
                    "sent": "So yeah, what pipe Agensys counties do we have for this minimum?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is I'm going to run through a PAC Bayes bound quite a simple one.",
                    "label": 0
                },
                {
                    "sent": "That's quite simple to derive and then show how.",
                    "label": 0
                },
                {
                    "sent": "With a bit of tweaking, fit the algorithm into it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you've got some IID sample.",
                    "label": 0
                },
                {
                    "sent": "You've got a measurable set of, so really this should be a parameter space for our.",
                    "label": 0
                },
                {
                    "sent": "You statistic kernels or C not related to the kernels before.",
                    "label": 0
                },
                {
                    "sent": "To define our KHS these are to define our used cystic that bounded between A&B and I'm going to look at a normalized you statistics I'm calling at U bar S where I've just normalized using ambah with mean U bar and this is so bitter cheating But so that I can scale it down to 01 and fit it into the standard loss functions the KL loss.",
                    "label": 0
                },
                {
                    "sent": "And the fici, which is the Bernoulli local playstyle loss.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've called them corroborates because they follow from a more general theorem.",
                    "label": 0
                },
                {
                    "sent": "So for any prior on our parameter set of used at kernels with probability greater than one minus Delta over the draw of the sample for any row on use that kernels.",
                    "label": 1
                },
                {
                    "sent": "We have these two equations.",
                    "label": 0
                },
                {
                    "sent": "They look very similar to the standard PAC Bayes bounds.",
                    "label": 0
                },
                {
                    "sent": "Apart from the fact we have second order you statistics in here and we have this N / 2 rounded down to close integer which as you might guess suggests that in the proof simply going to decompose the usage statistic into ID blocks, which is exactly what I do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey guys.",
                    "label": 0
                },
                {
                    "sent": "Of order two, yes, did I?",
                    "label": 0
                },
                {
                    "sent": "I didn't say that today.",
                    "label": 0
                },
                {
                    "sent": "Well HXX pro, so yeah I've ordered 2.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So yeah, just define the permutation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On one up 10 to find out IID blocks using standard method and then.",
                    "label": 0
                },
                {
                    "sent": "I want to look at the KL loss and the Bernoulli log replaced our loss, so I'm just going to do it for general convex function on 01 square.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ed.",
                    "label": 0
                },
                {
                    "sent": "Then you get this general theorem so.",
                    "label": 0
                },
                {
                    "sent": "For any Pi probability on my cell to the joy of F and hero get this.",
                    "label": 0
                },
                {
                    "sent": "And where this is the Laplace transform?",
                    "label": 0
                },
                {
                    "sent": "And then by using Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "So you split that up into the IID blocks and then obviously take these some an the 1 / N factorial outside using Jensen's.",
                    "label": 0
                },
                {
                    "sent": "And then you can just bound these using standard methods.",
                    "label": 0
                },
                {
                    "sent": "But noting that these are 01 variables, I'm going to use.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lemma for Maura which you have Jenny actually pointed out to me, but it's quite a general result.",
                    "label": 0
                },
                {
                    "sent": "The for any convex function.",
                    "label": 0
                },
                {
                    "sent": "If you're banning the expectation over I 01 IID variables then it's always up bounded by the expectation of the convex function using the Bernoulli random variables such that that means are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, and that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just means that you can use just upper bound this by the.",
                    "label": 0
                },
                {
                    "sent": "Bernoulli random variables with the same mean and just use the standard analysis that you've seen or yesterday and today.",
                    "label": 0
                },
                {
                    "sent": "Noting that this is still one.",
                    "label": 0
                },
                {
                    "sent": "And now we have this N / 2 rounded down, where this is of order with M as said before.",
                    "label": 0
                },
                {
                    "sent": "So we've got.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These for this use statistic.",
                    "label": 0
                },
                {
                    "sent": "Question is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we actually have use?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Autistic, and the answer is no, but we almost do.",
                    "label": 1
                },
                {
                    "sent": "'cause we we introduced this bias term.",
                    "label": 1
                },
                {
                    "sent": "Which is quite nice quantity in itself.",
                    "label": 0
                },
                {
                    "sent": "So here the key features are approximation.",
                    "label": 0
                },
                {
                    "sent": "You're mapping that to the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "You're then mapping each of your.",
                    "label": 0
                },
                {
                    "sent": "Examples to reproduce and kind of Hilbert space as well.",
                    "label": 0
                },
                {
                    "sent": "And it's like a variance term in the reproducing kernel Hilbert space.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then just to ease my notation, going to use KL subscript half.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is still convex in both arguments and O for Q = P, and I'm just so we remember that.",
                    "label": 0
                },
                {
                    "sent": "Approximation is some mixture of candidate densities.",
                    "label": 0
                },
                {
                    "sent": "And you get for any reproducing kernel Hilbert space.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With F, the unit ball in H any prior pie so member in the zornow mixture parameters with probability greater than one minus Delta overdraw.",
                    "label": 0
                },
                {
                    "sent": "The sample for any posterior rohat with mean theater hat.",
                    "label": 0
                },
                {
                    "sent": "This holds where.",
                    "label": 0
                },
                {
                    "sent": "We've got the bias term here, but this is over N -- 1, so for reasonable when it shrinks down, the CK is just to re scale this so we can fit it into the KL an.",
                    "label": 0
                },
                {
                    "sent": "And we end up.",
                    "label": 0
                },
                {
                    "sent": "So we pay.",
                    "label": 0
                },
                {
                    "sent": "This is the penalty we pay for deviating to EU statistic bound and we get twice the KL here, which I'll see.",
                    "label": 0
                },
                {
                    "sent": "I'll just do a brief of the proof.",
                    "label": 0
                },
                {
                    "sent": "So we define our use.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Autistic indexed by theater theater prime.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we then plug into the used at bound, but I just gave but we take rho squared, an Pi squared probability product.",
                    "label": 0
                },
                {
                    "sent": "Probability measures on the multinomial manifold squared.",
                    "label": 0
                },
                {
                    "sent": "So so probably put.",
                    "label": 0
                },
                {
                    "sent": "For any pie, and then for any row after heroes squared off to here.",
                    "label": 0
                },
                {
                    "sent": "So you plug them in.",
                    "label": 0
                },
                {
                    "sent": "And then obviously the KL between Rose Garden Pi squared is just twice the KL between Rome pie and see I've just condense that down 'cause we're not worried about that at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can.",
                    "label": 0
                },
                {
                    "sent": "You're paying the price for the bias term.",
                    "label": 0
                },
                {
                    "sent": "You just commute because we're taking with respect to this product, measure can commute it inside in a product.",
                    "label": 0
                },
                {
                    "sent": "And likely taking respect expectation with respect to the sample.",
                    "label": 0
                },
                {
                    "sent": "Because we've got this XI not equal to XJ to get that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say now just a little bit on choosing the K out 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we have this bound, but we are dealing with distributions on the multinomial manifold.",
                    "label": 0
                },
                {
                    "sent": "So an obvious choice.",
                    "label": 0
                },
                {
                    "sent": "Would be to take the richley.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "Rohas mean theater hat.",
                    "label": 0
                },
                {
                    "sent": "And an uninformative prior.",
                    "label": 0
                },
                {
                    "sent": "So if you're thinking of your, you've got your mixture, your candidate densities for mixture, just if they all say the same, you're uninformative.",
                    "label": 0
                },
                {
                    "sent": "It's just like a pozan standard parzen window estimator.",
                    "label": 0
                },
                {
                    "sent": "Then the KL has quite a complicated form where you have to deal with gamma and digamma functions, so another.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possibility.",
                    "label": 0
                },
                {
                    "sent": "Is just to project your multinomial manifold onto the parallel hyperplane that passes through the origin.",
                    "label": 0
                },
                {
                    "sent": "Using our mapping towel.",
                    "label": 0
                },
                {
                    "sent": "With GC2 is the geometric mean.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's two RL, but it's to the hyperplane, the parallel hyperplane that passes through the origin.",
                    "label": 0
                },
                {
                    "sent": "And then you can define a. Gaussian place a Gaussian on.",
                    "label": 0
                },
                {
                    "sent": "The projection.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Grad subscript Theta Tau is the Jacobian of the town matrix to ensure that this is a density.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The book Measure NRL, but restricted to.",
                    "label": 0
                },
                {
                    "sent": "The most you know manifold.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can change the coordinates to Tao and integrate.",
                    "label": 0
                },
                {
                    "sent": "When evaluating the KL to get.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Something that is analogous to the case, say for the SVM.",
                    "label": 0
                },
                {
                    "sent": "When you're just pacing isotropic Gaussians.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I've got this regularization term that is essentially a squared norm of your map points.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That actually was shorter than I thought.",
                    "label": 0
                },
                {
                    "sent": "OK, so some.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe just a simple path based bound on maximum mean script and see between the input distribution and a mixture based approximation given a class of prime posterior with simple KL divergent.",
                    "label": 0
                },
                {
                    "sent": "And then future work will look at finding project.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions from the multinomial manifold 2 RL and distributions Rohan Pai.",
                    "label": 0
                },
                {
                    "sent": "That'll give you good Cal for optimization if you wanted to look at minimization of the bound, 'cause I'm not sure.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as of yet with this mapping, how?",
                    "label": 0
                },
                {
                    "sent": "How well we have to deal with that.",
                    "label": 0
                },
                {
                    "sent": "And all.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing this was a first attempt is just to examine that directly and not taking the deviation to AU statistic bound and you can see else here.",
                    "label": 0
                },
                {
                    "sent": "If you write out, expand out these squared norms.",
                    "label": 0
                },
                {
                    "sent": "The squared KQ, Theta hat terms cancel, so it's linear in KQ, Theta hat and then you need to look at that so.",
                    "label": 0
                },
                {
                    "sent": "If you did deviate to EU statistic method then you could not decompose into ID blocks.",
                    "label": 0
                },
                {
                    "sent": "You could apply.",
                    "label": 0
                },
                {
                    "sent": "Maybe some Martin Gale argument, or you could forget you statistics and go directly for this and that's it.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}