{
    "id": "ngzgubsqoououqwulfrnd3prr3krmxf3",
    "title": "Manifold-adaptive dimension estimation",
    "info": {
        "author": [
            "Amir-massoud Farahmand, University of Alberta"
        ],
        "published": "June 24, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Manifold Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_farahmand_made/",
    "segmentation": [
        [
            "Is that sin?"
        ],
        [
            "Many real world applications we need to deal with high dimensional data data such that data that comes from a vision sense source or the case that we use.",
            "We want to fill several sources of sensors.",
            "Which distro applications are quite common robotics application or the case that we want to expand features in order to make the problem somehow easier?",
            "For example, making it linear clusters of data, linear separable for the classification problem?",
            "Which kernel methods can be considered as this one?"
        ],
        [
            "But the problem that everyone everyone knows is the problem of curse of dimensionality when the dimension of the data increases, the complexity of solving the problem increases exponentially.",
            "For instance, in this figure I show the minimax lower bound for the regression problem.",
            "Here we have the number of samples and this is mean squared error and you see that for vendor dimension is for example one the performance is very well but.",
            "When the dimension is 100, which 100 is not really that big.",
            "For vision problem you see that almost it does not learn anything.",
            "So."
        ],
        [
            "It makes the problem difficult for us than we may conclude something like this that we should do some kind of dimensional reduction before going on before processing on the data, or we should not add features unnecessarily.",
            "We should avoid.",
            "We should be very careful.",
            "Careful to add features to our datasets and increase the dimension of input space in order to make the work of some other part of the data processing easier.",
            "Or we should not accept the project that involves high dimensional data.",
            "App.",
            "What's one may ask if it's possible to do anything else?",
            "If we are really cursed by the curse of dimensionality or we can do something about it and my point is that well, sometimes we can do something."
        ],
        [
            "The reason is that in many cases we have some regularity's in the data.",
            "We have some structure in the data which makes the problem much easier.",
            "The lower bounds that I showed you was the kind of minimax lower bound.",
            "It means that the environment choose some datasets, some choose some public distribution.",
            "That is very bad for your algorithm, and but in many cases nature is not working against you and it has so.",
            "We can expect that our data has different kind of regularity's like a smoothness.",
            "We know that if we have a smoothness in the target function, then the affective dimension of the problem is divided by the degree of smoothness or if the target function is sparse in the sense that if we expand the target function in the set of basis just a few number of those bases are available.",
            "We can also exploit these properties.",
            "Or if you know that the clusters of data are well separated and there is low noise at the boundaries of decision surface.",
            "And the other case, which is the focus of my work, is the lower if the data has this property that it comes from a lower dimensional submanifold.",
            "This also may help and I'm going to talk about it more.",
            "There are several methods, methods of commonly known as manifold learning methods that try to use these kind of property of data.",
            "Also there are some.",
            "Well approaches different learning problems like semi supervised learning reinforcement learning that are trying to exploit.",
            "Use this manifold learning ideas to design more efficient problems, more efficient solutions to their problems.",
            "Before going on, I just want to mention what I mean by that.",
            "Dimension of the manifold and what I mean by the dimension of embedding space, because I use it several times, but probably all of you know, here we have a 2 dimensional Mobius strip.",
            "Which is so the manifold dimension here is 2 which is embedded in three dimensional space."
        ],
        [
            "Well, that's it.",
            "So what is my goal?",
            "My goal?",
            "Very high level goal is to see if to design and analyze manifold adaptive machine learning methods.",
            "What I mean by manifold adaptive this term, that means that's the methods that the convergence rate or the performance depends on the dimension of the manifold and not the dimension of the embedding space.",
            "So the goal is ultimate goal is showing that well, these are specific methods.",
            "Is manifold adaptive performance just depends on the.",
            "Dimension of the underlying manifold and not the input space."
        ],
        [
            "There are several questions here which.",
            "Suppose that if we can always exploit these property of data, or is it possible to modify current method machine learning methods to exploit those kind of circularities very well?",
            "But here at least I show that for one machine learning problem which is estimating the dimension of the manifold, we can have manifold adaptive results, which means that the convergence rate.",
            "It depends on the dimension of the manifold and not the dimension of the data."
        ],
        [
            "OK, so one may ask why we need these kind of manufacture dimension estimation.",
            "I think the answer is somehow obvious.",
            "We need in that dimension in many learning methods, many current manifold learning methods needed need that dimension as a parimeter an also I expect that in need of those methods that will come in the near future.",
            "Those manifold learning methods, those need to know that dimension of the manifold to tune.",
            "Are meters those parameters like the read of Colonel in the kernel smoothing methods or the parimeter?",
            "Adoration of Empirical loss to the regularization in penalized least Square?",
            "You know, to know that dimension probably.",
            "And so the point is that, well, this dimension is not."
        ],
        [
            "Nona Priory there are several methods for finding the dimension of the manifold, but the problem with those methods is that.",
            "There is almost no rigorous analyzes of that methods or somehow heuristic methods that intuitively works, but there is no analyzers that show that those method works, and more importantly, most of the even the few of them that has any result as symptomatic result.",
            "That syntactic result is not useful for manifold adaptive methods, because asymptotic results imply that you can just convert your solution in the Infinity.",
            "But if you want to have.",
            "Methods that performs depending on the dimension of the manifold, you need finite sample results for every part of your."
        ],
        [
            "Glitter.",
            "So what's our contribution?",
            "This paper.",
            "We introduce a new algorithm which is a simple and efficient.",
            "This is based on K nearest neighborhood idea.",
            "Also, the method is manifold adaptive and we provide proof the convo."
        ],
        [
            "Straight of it, yeah, so the general idea is very simple and it is based on finding the volume of some balls in spaces and then sync with what the scale that for volume increases when you increase is increased, the radius of that ball.",
            "So for simple case suppose that you have a circle on a plane and it has a radius R. What's the?",
            "Great, what's the volume or area of that circle?",
            "The area scales up with R-squared, so the same is true for other Acadian spaces.",
            "If you have a 3 dimensional space.",
            "You will have our cubed ratio or something like this.",
            "Here we generalize this notion to probabilities, so we have.",
            "This is XI.",
            "Some samples from the manifold and we calculate the probability that that X belongs to ball centered at X with radius R, and we define data in this way.",
            "This is 8 as some function which depends on X and depends on R and times are to the power of D. This is very intuitive if you have a Canadian space with unify."
        ],
        [
            "Sample link for example in this case.",
            "So the number of points in this ball which I denoted by red points.",
            "Depends on some constant, so it is constant here multiplied by R-squared.",
            "But because of the curvature of the manifold the Zeta is not constant anymore.",
            "It changes when you change move."
        ],
        [
            "On the manifold.",
            "But the point is that, well, if the key point is actually, that's if you have.",
            "Just move very little on the manifold, so radius of that ball is very small.",
            "The Zeta thing is almost constant, so we use this property and device method for."
        ],
        [
            "Finding the dimension of the manifold.",
            "The method works like this.",
            "Well, you see if we take the logarithm of both sides D, the dimension of the manifold appears at that in a linear fashion, so.",
            "If you have some some different radius and we calculate this probability for those points, then if we consider this as accounts as a constant term, then we can fit a linear fit align to that to these data points and fine."
        ],
        [
            "Out the true value of the our method is actually so simple we just take two points and to our points are selected.",
            "In this way we pitch K and then find the radius corresponding to Kates nearest neighborhood of this central points.",
            "So we find this ball.",
            "And we find the radius of this ball, and then we pick K halfed closest data points, and we find the radius so.",
            "We can get these things with approximation that I mentioned and we get to this estimation of the manifold."
        ],
        [
            "So this estimation has some error.",
            "This actually we have two sources of error.",
            "One is approximation error that comes from considering that this data is fixed is constant and the other is that comes from the estimation error which observed.",
            "Which comes from estimating these probabilities by empirical distributions.",
            "So I estimate these things by K. / N which you see here case the number of points in these ball ANAN is the total number of sample point."
        ],
        [
            "OK, so the result is as follows.",
            "We have.",
            "This estimation for every random points as before, and the result says that under some regularity assumptions, an ETA, which means that the manifold should not change that properly, provided that we have sufficient number of data points with high probability error between the estimation and the true dimension of the manifold is bounded by some constant multiplied by one term which correspond to approximation error one.",
            "The other, which is usual estimation error and the interesting point here, is that the dimension that appears in this upper back."
        ],
        [
            "Mount is the dimension at which estimation appears most, and Arthur and shows that's the property.",
            "That's this voting case does not give the correct estimation is exponential, exponential in and the dimension of the manifold appears as C to the power of D, which is small.",
            "D is the dimension of the manifold, so it's a manifold adaptive result and therefore average in case the same is true.",
            "This is exponential with North.",
            "But we have some DBT, which is the dimension of the embedding space appears in the result, but it comes in a mild fashion.",
            "It is a polynomial sense.",
            "It's not in the C to the power of."
        ],
        [
            "Booty so.",
            "And we have some experiment."
        ],
        [
            "And for one experiment as shown here is that I picked some points, some random points from sphere, one sphere is 4 dimensional sphere embedded in five dimensional space, the other is 8 dimensional sphere embedded in nine dimensional space.",
            "And I changed the number of samples and then I found the mean absolute dimension estimation error, which means that the error between the result of the.",
            "Averaging case averaging aggregation but without rounding it to an integer number and then find the difference between.",
            "That's the estimated dimension with the true dimension and then I have rage all of those.",
            "All of these errors and you can see that.",
            "As we expected, this is for four dimensional.",
            "This is 48 dimensional and this low 4 four dimensional is higher because, well, this is.",
            "This should be faster because they difficulty of problem is less than."
        ],
        [
            "Problem for 8 dimensional case.",
            "The other is that I picked some 2 dimensional movies for manifold and then embedded in a 3 dimensional space with some mapping 6 dimensional and 12 dimensional.",
            "And again I compare the same.",
            "This is the number of samples.",
            "This is the error and you see that the performance all of all of them or all."
        ],
        [
            "Most the same.",
            "So we have tested out other cases, different spheres, single story which is 1 dimensional manifold.",
            "Mobiles and Swiss roll.",
            "We change the number of samples and hear out of the parentheses we have the result percentage of.",
            "Correct, answering current dimension estimation of averaging method and other independent basis for voting methods.",
            "And you see that as we expect, when the number of dimension grows, the error decreases."
        ],
        [
            "So as a conclusion, I can say that, well, we devise a new algorithm.",
            "It's a simple and efficient algorithm.",
            "The results were competitive to other pre other methods that we had before.",
            "Also important points was that the result is manifold adaptive.",
            "We have a convergence.",
            "We prove the convergence rate and their rate depends on the dimension of the manifold.",
            "So the question is that if we can apply, we can have other machine learning methods that are manifold adaptive or not.",
            "Well, well, one question is that if correct machine learning methods are already manifold adaptive but just no one analyze them, the other of the other question is that if they are not manifold adapted by the current form, can we change them in a way that they become manifold adaptive and exploit the properties of manifold explicitly?",
            "So our answer is that for K nearest neighborhood regression, it is actually manifold adaptive, so.",
            "We do not need to change it.",
            "We are working on, but the problem of Kenyan Robin Hood is that it does not capture the smoothness of the data.",
            "So we are working on more sophisticated method of penalized least Square and try to see if it's manifold adaptive or not.",
            "And my final comment is that well if the notion of dimensional reduction an important thing or not.",
            "Or should we change think about think again about the notion of dimension reduction if we can find manifold adaptive methods that.",
            "X can exploit the dimension of the Magnificat, exploits the property of.",
            "That manifold comes from data comes from manifold.",
            "We may not need to use dimensional reduction unless for finding out which dimensions are irrelevant.",
            "OK, thank you.",
            "Thank you very much.",
            "So it seems like so there's this core assumption in your method that the data is uniformly distributed.",
            "No, no, no.",
            "The assumption was that well, with actually the effect of.",
            "Point X.",
            "Like exponentially more sparsely as I move away from exegesis distance, right?",
            "That's going to distort your all volume.",
            "Yeah yeah, that's right.",
            "If you have some conditions of Etta which depends on the well, the way that data is sampled.",
            "Slowly, slowly yes.",
            "We have some condition like Lipschitz Ness of data which depends on the geometry and depends on the distribution we are sampling.",
            "So yeah, if you do not have any sample link in any point on the manifold, you cannot do anything.",
            "I'll be standing in the case where you don't make assumptions.",
            "Maybe you make assumptions with high probability.",
            "Yeah, we are actually working on this thing and we are trying to relax these assumptions and it seems that it's true that if we have this assumption of Lipschitz or something like that with high probability, but not for sure, we can also get these results.",
            "But this is the work in progress.",
            "OK.",
            "I don't think other workers.",
            "Brand.",
            "Used Yep, this idea of K using K nearest neighborhood thing and then finding the volume seems not to be in you and many other methods are somehow close to this thing.",
            "Yeah, I agree, but the main important thing is that we showed us it's a convergence result, which is manifold adapt.",
            "Actually I have a question.",
            "You're bound seem to be large deviation type fonts and usually they are not very, very tight.",
            "So why I'm estimating the dimensions so precise?",
            "Well here despond.",
            "So what was your question this?",
            "This is a proper boundary.",
            "In the in the experiment cases you you, you got the right dimension.",
            "Yeah, now these these types of pounds usually I lose in statistical learning that's right.",
            "Well, actually, but the important point about the bound is that, well, the constants may be loose, but the fact that the difficulty of problem depends on the dimension of the manifold is the most important thing that this bond shows.",
            "So we have we have actually calculated some of these constants, but because it depends on the manifold and we don't, we don't know what are those constants on the manifold, so I cannot say.",
            "How tight or how loose this bound is, but the important thing is that the general behavior when you increase the number of samples should be somehow interpreted by these bounds.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that sin?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many real world applications we need to deal with high dimensional data data such that data that comes from a vision sense source or the case that we use.",
                    "label": 0
                },
                {
                    "sent": "We want to fill several sources of sensors.",
                    "label": 0
                },
                {
                    "sent": "Which distro applications are quite common robotics application or the case that we want to expand features in order to make the problem somehow easier?",
                    "label": 0
                },
                {
                    "sent": "For example, making it linear clusters of data, linear separable for the classification problem?",
                    "label": 0
                },
                {
                    "sent": "Which kernel methods can be considered as this one?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the problem that everyone everyone knows is the problem of curse of dimensionality when the dimension of the data increases, the complexity of solving the problem increases exponentially.",
                    "label": 1
                },
                {
                    "sent": "For instance, in this figure I show the minimax lower bound for the regression problem.",
                    "label": 0
                },
                {
                    "sent": "Here we have the number of samples and this is mean squared error and you see that for vendor dimension is for example one the performance is very well but.",
                    "label": 1
                },
                {
                    "sent": "When the dimension is 100, which 100 is not really that big.",
                    "label": 0
                },
                {
                    "sent": "For vision problem you see that almost it does not learn anything.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It makes the problem difficult for us than we may conclude something like this that we should do some kind of dimensional reduction before going on before processing on the data, or we should not add features unnecessarily.",
                    "label": 0
                },
                {
                    "sent": "We should avoid.",
                    "label": 0
                },
                {
                    "sent": "We should be very careful.",
                    "label": 0
                },
                {
                    "sent": "Careful to add features to our datasets and increase the dimension of input space in order to make the work of some other part of the data processing easier.",
                    "label": 1
                },
                {
                    "sent": "Or we should not accept the project that involves high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "What's one may ask if it's possible to do anything else?",
                    "label": 0
                },
                {
                    "sent": "If we are really cursed by the curse of dimensionality or we can do something about it and my point is that well, sometimes we can do something.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason is that in many cases we have some regularity's in the data.",
                    "label": 0
                },
                {
                    "sent": "We have some structure in the data which makes the problem much easier.",
                    "label": 0
                },
                {
                    "sent": "The lower bounds that I showed you was the kind of minimax lower bound.",
                    "label": 0
                },
                {
                    "sent": "It means that the environment choose some datasets, some choose some public distribution.",
                    "label": 0
                },
                {
                    "sent": "That is very bad for your algorithm, and but in many cases nature is not working against you and it has so.",
                    "label": 0
                },
                {
                    "sent": "We can expect that our data has different kind of regularity's like a smoothness.",
                    "label": 0
                },
                {
                    "sent": "We know that if we have a smoothness in the target function, then the affective dimension of the problem is divided by the degree of smoothness or if the target function is sparse in the sense that if we expand the target function in the set of basis just a few number of those bases are available.",
                    "label": 0
                },
                {
                    "sent": "We can also exploit these properties.",
                    "label": 0
                },
                {
                    "sent": "Or if you know that the clusters of data are well separated and there is low noise at the boundaries of decision surface.",
                    "label": 1
                },
                {
                    "sent": "And the other case, which is the focus of my work, is the lower if the data has this property that it comes from a lower dimensional submanifold.",
                    "label": 0
                },
                {
                    "sent": "This also may help and I'm going to talk about it more.",
                    "label": 0
                },
                {
                    "sent": "There are several methods, methods of commonly known as manifold learning methods that try to use these kind of property of data.",
                    "label": 0
                },
                {
                    "sent": "Also there are some.",
                    "label": 1
                },
                {
                    "sent": "Well approaches different learning problems like semi supervised learning reinforcement learning that are trying to exploit.",
                    "label": 0
                },
                {
                    "sent": "Use this manifold learning ideas to design more efficient problems, more efficient solutions to their problems.",
                    "label": 0
                },
                {
                    "sent": "Before going on, I just want to mention what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "Dimension of the manifold and what I mean by the dimension of embedding space, because I use it several times, but probably all of you know, here we have a 2 dimensional Mobius strip.",
                    "label": 0
                },
                {
                    "sent": "Which is so the manifold dimension here is 2 which is embedded in three dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, that's it.",
                    "label": 0
                },
                {
                    "sent": "So what is my goal?",
                    "label": 0
                },
                {
                    "sent": "My goal?",
                    "label": 0
                },
                {
                    "sent": "Very high level goal is to see if to design and analyze manifold adaptive machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "What I mean by manifold adaptive this term, that means that's the methods that the convergence rate or the performance depends on the dimension of the manifold and not the dimension of the embedding space.",
                    "label": 0
                },
                {
                    "sent": "So the goal is ultimate goal is showing that well, these are specific methods.",
                    "label": 0
                },
                {
                    "sent": "Is manifold adaptive performance just depends on the.",
                    "label": 0
                },
                {
                    "sent": "Dimension of the underlying manifold and not the input space.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are several questions here which.",
                    "label": 1
                },
                {
                    "sent": "Suppose that if we can always exploit these property of data, or is it possible to modify current method machine learning methods to exploit those kind of circularities very well?",
                    "label": 0
                },
                {
                    "sent": "But here at least I show that for one machine learning problem which is estimating the dimension of the manifold, we can have manifold adaptive results, which means that the convergence rate.",
                    "label": 0
                },
                {
                    "sent": "It depends on the dimension of the manifold and not the dimension of the data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one may ask why we need these kind of manufacture dimension estimation.",
                    "label": 0
                },
                {
                    "sent": "I think the answer is somehow obvious.",
                    "label": 0
                },
                {
                    "sent": "We need in that dimension in many learning methods, many current manifold learning methods needed need that dimension as a parimeter an also I expect that in need of those methods that will come in the near future.",
                    "label": 0
                },
                {
                    "sent": "Those manifold learning methods, those need to know that dimension of the manifold to tune.",
                    "label": 1
                },
                {
                    "sent": "Are meters those parameters like the read of Colonel in the kernel smoothing methods or the parimeter?",
                    "label": 0
                },
                {
                    "sent": "Adoration of Empirical loss to the regularization in penalized least Square?",
                    "label": 0
                },
                {
                    "sent": "You know, to know that dimension probably.",
                    "label": 0
                },
                {
                    "sent": "And so the point is that, well, this dimension is not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nona Priory there are several methods for finding the dimension of the manifold, but the problem with those methods is that.",
                    "label": 0
                },
                {
                    "sent": "There is almost no rigorous analyzes of that methods or somehow heuristic methods that intuitively works, but there is no analyzers that show that those method works, and more importantly, most of the even the few of them that has any result as symptomatic result.",
                    "label": 0
                },
                {
                    "sent": "That syntactic result is not useful for manifold adaptive methods, because asymptotic results imply that you can just convert your solution in the Infinity.",
                    "label": 0
                },
                {
                    "sent": "But if you want to have.",
                    "label": 0
                },
                {
                    "sent": "Methods that performs depending on the dimension of the manifold, you need finite sample results for every part of your.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Glitter.",
                    "label": 0
                },
                {
                    "sent": "So what's our contribution?",
                    "label": 0
                },
                {
                    "sent": "This paper.",
                    "label": 0
                },
                {
                    "sent": "We introduce a new algorithm which is a simple and efficient.",
                    "label": 1
                },
                {
                    "sent": "This is based on K nearest neighborhood idea.",
                    "label": 0
                },
                {
                    "sent": "Also, the method is manifold adaptive and we provide proof the convo.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight of it, yeah, so the general idea is very simple and it is based on finding the volume of some balls in spaces and then sync with what the scale that for volume increases when you increase is increased, the radius of that ball.",
                    "label": 0
                },
                {
                    "sent": "So for simple case suppose that you have a circle on a plane and it has a radius R. What's the?",
                    "label": 0
                },
                {
                    "sent": "Great, what's the volume or area of that circle?",
                    "label": 0
                },
                {
                    "sent": "The area scales up with R-squared, so the same is true for other Acadian spaces.",
                    "label": 0
                },
                {
                    "sent": "If you have a 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You will have our cubed ratio or something like this.",
                    "label": 0
                },
                {
                    "sent": "Here we generalize this notion to probabilities, so we have.",
                    "label": 0
                },
                {
                    "sent": "This is XI.",
                    "label": 0
                },
                {
                    "sent": "Some samples from the manifold and we calculate the probability that that X belongs to ball centered at X with radius R, and we define data in this way.",
                    "label": 0
                },
                {
                    "sent": "This is 8 as some function which depends on X and depends on R and times are to the power of D. This is very intuitive if you have a Canadian space with unify.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample link for example in this case.",
                    "label": 0
                },
                {
                    "sent": "So the number of points in this ball which I denoted by red points.",
                    "label": 0
                },
                {
                    "sent": "Depends on some constant, so it is constant here multiplied by R-squared.",
                    "label": 0
                },
                {
                    "sent": "But because of the curvature of the manifold the Zeta is not constant anymore.",
                    "label": 0
                },
                {
                    "sent": "It changes when you change move.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the manifold.",
                    "label": 0
                },
                {
                    "sent": "But the point is that, well, if the key point is actually, that's if you have.",
                    "label": 0
                },
                {
                    "sent": "Just move very little on the manifold, so radius of that ball is very small.",
                    "label": 0
                },
                {
                    "sent": "The Zeta thing is almost constant, so we use this property and device method for.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finding the dimension of the manifold.",
                    "label": 0
                },
                {
                    "sent": "The method works like this.",
                    "label": 0
                },
                {
                    "sent": "Well, you see if we take the logarithm of both sides D, the dimension of the manifold appears at that in a linear fashion, so.",
                    "label": 0
                },
                {
                    "sent": "If you have some some different radius and we calculate this probability for those points, then if we consider this as accounts as a constant term, then we can fit a linear fit align to that to these data points and fine.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out the true value of the our method is actually so simple we just take two points and to our points are selected.",
                    "label": 0
                },
                {
                    "sent": "In this way we pitch K and then find the radius corresponding to Kates nearest neighborhood of this central points.",
                    "label": 0
                },
                {
                    "sent": "So we find this ball.",
                    "label": 0
                },
                {
                    "sent": "And we find the radius of this ball, and then we pick K halfed closest data points, and we find the radius so.",
                    "label": 0
                },
                {
                    "sent": "We can get these things with approximation that I mentioned and we get to this estimation of the manifold.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this estimation has some error.",
                    "label": 0
                },
                {
                    "sent": "This actually we have two sources of error.",
                    "label": 0
                },
                {
                    "sent": "One is approximation error that comes from considering that this data is fixed is constant and the other is that comes from the estimation error which observed.",
                    "label": 0
                },
                {
                    "sent": "Which comes from estimating these probabilities by empirical distributions.",
                    "label": 0
                },
                {
                    "sent": "So I estimate these things by K. / N which you see here case the number of points in these ball ANAN is the total number of sample point.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the result is as follows.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "This estimation for every random points as before, and the result says that under some regularity assumptions, an ETA, which means that the manifold should not change that properly, provided that we have sufficient number of data points with high probability error between the estimation and the true dimension of the manifold is bounded by some constant multiplied by one term which correspond to approximation error one.",
                    "label": 1
                },
                {
                    "sent": "The other, which is usual estimation error and the interesting point here, is that the dimension that appears in this upper back.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mount is the dimension at which estimation appears most, and Arthur and shows that's the property.",
                    "label": 0
                },
                {
                    "sent": "That's this voting case does not give the correct estimation is exponential, exponential in and the dimension of the manifold appears as C to the power of D, which is small.",
                    "label": 0
                },
                {
                    "sent": "D is the dimension of the manifold, so it's a manifold adaptive result and therefore average in case the same is true.",
                    "label": 0
                },
                {
                    "sent": "This is exponential with North.",
                    "label": 0
                },
                {
                    "sent": "But we have some DBT, which is the dimension of the embedding space appears in the result, but it comes in a mild fashion.",
                    "label": 1
                },
                {
                    "sent": "It is a polynomial sense.",
                    "label": 0
                },
                {
                    "sent": "It's not in the C to the power of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Booty so.",
                    "label": 0
                },
                {
                    "sent": "And we have some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for one experiment as shown here is that I picked some points, some random points from sphere, one sphere is 4 dimensional sphere embedded in five dimensional space, the other is 8 dimensional sphere embedded in nine dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And I changed the number of samples and then I found the mean absolute dimension estimation error, which means that the error between the result of the.",
                    "label": 1
                },
                {
                    "sent": "Averaging case averaging aggregation but without rounding it to an integer number and then find the difference between.",
                    "label": 0
                },
                {
                    "sent": "That's the estimated dimension with the true dimension and then I have rage all of those.",
                    "label": 0
                },
                {
                    "sent": "All of these errors and you can see that.",
                    "label": 0
                },
                {
                    "sent": "As we expected, this is for four dimensional.",
                    "label": 0
                },
                {
                    "sent": "This is 48 dimensional and this low 4 four dimensional is higher because, well, this is.",
                    "label": 0
                },
                {
                    "sent": "This should be faster because they difficulty of problem is less than.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem for 8 dimensional case.",
                    "label": 0
                },
                {
                    "sent": "The other is that I picked some 2 dimensional movies for manifold and then embedded in a 3 dimensional space with some mapping 6 dimensional and 12 dimensional.",
                    "label": 0
                },
                {
                    "sent": "And again I compare the same.",
                    "label": 0
                },
                {
                    "sent": "This is the number of samples.",
                    "label": 1
                },
                {
                    "sent": "This is the error and you see that the performance all of all of them or all.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most the same.",
                    "label": 0
                },
                {
                    "sent": "So we have tested out other cases, different spheres, single story which is 1 dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Mobiles and Swiss roll.",
                    "label": 0
                },
                {
                    "sent": "We change the number of samples and hear out of the parentheses we have the result percentage of.",
                    "label": 0
                },
                {
                    "sent": "Correct, answering current dimension estimation of averaging method and other independent basis for voting methods.",
                    "label": 0
                },
                {
                    "sent": "And you see that as we expect, when the number of dimension grows, the error decreases.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a conclusion, I can say that, well, we devise a new algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's a simple and efficient algorithm.",
                    "label": 0
                },
                {
                    "sent": "The results were competitive to other pre other methods that we had before.",
                    "label": 0
                },
                {
                    "sent": "Also important points was that the result is manifold adaptive.",
                    "label": 0
                },
                {
                    "sent": "We have a convergence.",
                    "label": 1
                },
                {
                    "sent": "We prove the convergence rate and their rate depends on the dimension of the manifold.",
                    "label": 0
                },
                {
                    "sent": "So the question is that if we can apply, we can have other machine learning methods that are manifold adaptive or not.",
                    "label": 0
                },
                {
                    "sent": "Well, well, one question is that if correct machine learning methods are already manifold adaptive but just no one analyze them, the other of the other question is that if they are not manifold adapted by the current form, can we change them in a way that they become manifold adaptive and exploit the properties of manifold explicitly?",
                    "label": 0
                },
                {
                    "sent": "So our answer is that for K nearest neighborhood regression, it is actually manifold adaptive, so.",
                    "label": 0
                },
                {
                    "sent": "We do not need to change it.",
                    "label": 0
                },
                {
                    "sent": "We are working on, but the problem of Kenyan Robin Hood is that it does not capture the smoothness of the data.",
                    "label": 0
                },
                {
                    "sent": "So we are working on more sophisticated method of penalized least Square and try to see if it's manifold adaptive or not.",
                    "label": 0
                },
                {
                    "sent": "And my final comment is that well if the notion of dimensional reduction an important thing or not.",
                    "label": 0
                },
                {
                    "sent": "Or should we change think about think again about the notion of dimension reduction if we can find manifold adaptive methods that.",
                    "label": 0
                },
                {
                    "sent": "X can exploit the dimension of the Magnificat, exploits the property of.",
                    "label": 0
                },
                {
                    "sent": "That manifold comes from data comes from manifold.",
                    "label": 0
                },
                {
                    "sent": "We may not need to use dimensional reduction unless for finding out which dimensions are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So it seems like so there's this core assumption in your method that the data is uniformly distributed.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "The assumption was that well, with actually the effect of.",
                    "label": 0
                },
                {
                    "sent": "Point X.",
                    "label": 0
                },
                {
                    "sent": "Like exponentially more sparsely as I move away from exegesis distance, right?",
                    "label": 0
                },
                {
                    "sent": "That's going to distort your all volume.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "If you have some conditions of Etta which depends on the well, the way that data is sampled.",
                    "label": 0
                },
                {
                    "sent": "Slowly, slowly yes.",
                    "label": 0
                },
                {
                    "sent": "We have some condition like Lipschitz Ness of data which depends on the geometry and depends on the distribution we are sampling.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you do not have any sample link in any point on the manifold, you cannot do anything.",
                    "label": 0
                },
                {
                    "sent": "I'll be standing in the case where you don't make assumptions.",
                    "label": 1
                },
                {
                    "sent": "Maybe you make assumptions with high probability.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we are actually working on this thing and we are trying to relax these assumptions and it seems that it's true that if we have this assumption of Lipschitz or something like that with high probability, but not for sure, we can also get these results.",
                    "label": 0
                },
                {
                    "sent": "But this is the work in progress.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I don't think other workers.",
                    "label": 0
                },
                {
                    "sent": "Brand.",
                    "label": 0
                },
                {
                    "sent": "Used Yep, this idea of K using K nearest neighborhood thing and then finding the volume seems not to be in you and many other methods are somehow close to this thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree, but the main important thing is that we showed us it's a convergence result, which is manifold adapt.",
                    "label": 0
                },
                {
                    "sent": "Actually I have a question.",
                    "label": 0
                },
                {
                    "sent": "You're bound seem to be large deviation type fonts and usually they are not very, very tight.",
                    "label": 0
                },
                {
                    "sent": "So why I'm estimating the dimensions so precise?",
                    "label": 0
                },
                {
                    "sent": "Well here despond.",
                    "label": 0
                },
                {
                    "sent": "So what was your question this?",
                    "label": 0
                },
                {
                    "sent": "This is a proper boundary.",
                    "label": 0
                },
                {
                    "sent": "In the in the experiment cases you you, you got the right dimension.",
                    "label": 0
                },
                {
                    "sent": "Yeah, now these these types of pounds usually I lose in statistical learning that's right.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, but the important point about the bound is that, well, the constants may be loose, but the fact that the difficulty of problem depends on the dimension of the manifold is the most important thing that this bond shows.",
                    "label": 0
                },
                {
                    "sent": "So we have we have actually calculated some of these constants, but because it depends on the manifold and we don't, we don't know what are those constants on the manifold, so I cannot say.",
                    "label": 0
                },
                {
                    "sent": "How tight or how loose this bound is, but the important thing is that the general behavior when you increase the number of samples should be somehow interpreted by these bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}