{
    "id": "7gwkjqmrgl7gqis6nxyikchbd6c4ujtz",
    "title": "Kernel Methods",
    "info": {
        "author": [
            "Alex Smola, Amazon"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_smola_km/",
    "segmentation": [
        [
            "So thank you very much for the invitation.",
            "I'm happy to be here.",
            "I must add.",
            "Well, I didn't do anything really for this summer school.",
            "This was all the local organizing committee."
        ],
        [
            "Well, you will see that large values of Theta are necessary to drive the distribution close to one or close to 0.",
            "Sorry, the probability close to one or close to 0.",
            "So I'm not saying that this is necessarily a very good prior.",
            "But if you have no other idea, this might actually be not a bad thing.",
            "I'll be discussing conjugate priors in the moment.",
            "And we'll get back to why this is actually not such a stupid idea in the research talk."
        ],
        [
            "OK, so let's see how it works.",
            "Blue things are the ones that you will get through maximum likelihood estimation.",
            "So it's exactly the same plot as what you saw before the red bars or what you get by using maximum posterior estimation.",
            "OK. Now what you see is well.",
            "I didn't observe any force here.",
            "Nonetheless, my Bayesian procedure or penalized log likelihood procedure if you want if you want to be pedantic, does something quite useful here, so it still gives it the benefit of doubt and says well, maybe I didn't see enough data, so I should actually up like this.",
            "It down ways.",
            "These extreme probabilities as well.",
            "The other thing that you should see is as we get more and more data.",
            "This is the blue and the red curve gets very close to each other.",
            "Actually, what you see from this is I probably didn't use a strong enough prior.",
            "Because actually know that this was.",
            "Fair dice would say constructed it this way.",
            "And it I would want that this actually voice things even well, considerably evenly here.",
            "Well, OK, but.",
            "So this tells you that the Bayesian procedure.",
            "In this case, is not the magic bullet, but it can improve things quite a bit."
        ],
        [
            "Now.",
            "Well, there's another prior and I'm just going to mention it briefly on two slides.",
            "There's this thing called conjugate priors.",
            "I'm mentioning it because quite often you would read some statistics paper and they will talk about say, OK.",
            "Here's the binomial distribution.",
            "Then we use a conjugate distribution like the Dirichlet blah blah blah.",
            "Or we estimate with the normal distribution and then we use as the conjugate the wish heart and so on and so on and so on.",
            "And it's quite intimidating initially when you read this, you think like how did they pull this out of the hat?",
            "There's actually very, very easy procedure how you can get the conjugate distribution if you know the exponential family distribution that you're dealing with.",
            "That's just all have fun, funny names and they all come with funny parametrizations.",
            "So probably the first thing you might want to do if some paper talks about this, just write it back in this standardized exponential families forming and everything is actually not very scary.",
            "See for instance, might read of the Chinese restaurant process.",
            "And in this case you have certain self reinforcing properties.",
            "Now just show these.",
            "Explain the situation afterwards.",
            "But basically this is pretty close to what the Chinese restaurant process will do.",
            "Is it seasonal observations?",
            "OK.",
            "So and the idea is very simple.",
            "So what we actually have is that my posterior will look different from the likelihood.",
            "This is maybe not such a good idea.",
            "Because, say for instance, if you go out with your friends and play a lot of.",
            "Games with devices then you will just have seen a lot of data.",
            "So you might think that it's a good way of.",
            "Representing your prior knowledge in the form of a lot of extra data that you've seen.",
            "Like you've played with him before, maybe 10 times.",
            "So maybe overall you toss the dice 100 times before now you play with them again and you toss it 10 times, but actually it's as if you still remember those 100 times.",
            "Do you toss the dice before?",
            "So that's a convenient way of representing prior knowledge, just as extra data.",
            "Now.",
            "What I want is.",
            "But PSA to parameterized by X has a similar functional form as P of X parameters voice data.",
            "Well.",
            "So.",
            "Pfc to parameters by some coefficient a, so I'm just guessing the solution and I'll show you that this does the trick.",
            "I'll just write it as E to the inner product between Theta and MCO times a.",
            "Minus MCO times G of Theta.",
            "Let me just explain on the whiteboard what's going on.",
            "So remember the exponential family distribution.",
            "Phi of X. Parameterized by theater.",
            "Pause.",
            "E to the inner product between 4 pics.",
            "And Theta.",
            "Minus.",
            "G of Theta.",
            "OK.",
            "In particular, if we had a lot of data.",
            "This expression here.",
            "Would be replaced by.",
            "Some.",
            "M times mu.",
            "And this will be replaced by M. Hey.",
            "So now I can write this.",
            "Also ask.",
            "Vendor product between the vector Theta.",
            "And G of Theta.",
            "OK. And.",
            "Well.",
            "They make em times mu hat.",
            "And.",
            "M. Now what is this good for?",
            "Well, this looks like an inner product where this is now my future map.",
            "So this looks like.",
            "Hey.",
            "Five of Satan now.",
            "And this looks like some other parameter.",
            "OK.",
            "So what if I picked some distribution on the theaters?",
            "Where my 5 seater looks like this and then I have some number here.",
            "This is precisely what's happening over there.",
            "OK, in that case I can just get my Pfc to give an X. OK. Pfc to give an X just proportional to.",
            "P of X given Theta times P of data.",
            "And let me call these parameters here.",
            "Mo Times mu 0 M 0.",
            "The reason why you might want to call them this way is this is like the effective number of observations that you have in your prior.",
            "This is the effective mean.",
            "OK. OK, now all I have to do is just plug this in so we get.",
            "He to the.",
            "Now we have some.",
            "Well.",
            "M times mu hat.",
            "Plus Mo times musiro.",
            "Sing time here.",
            "In approach with data.",
            "Minus.",
            "And here we have.",
            "N + M O times G of data.",
            "OK, then we'll get some normalization.",
            "Right?",
            "Now, what does this look like?",
            "So if I divide this by M + M zero.",
            "OK, and then I optimize over Theta.",
            "Well, this is like.",
            "Maximum likelihood.",
            "Estimation problem.",
            "Just with the key difference that now all the sudden.",
            "I will get.",
            "Instead of the original number of observations.",
            "M + M O mini.",
            "And instead of the mean that I'm serving, I get.",
            "So to say some fake mean.",
            "This is a really convenient way of encoding a prior, even procedurally.",
            "So for instance.",
            "You might not necessarily be willing to specify a priori exactly.",
            "But you might have an idea of what typical observations look like.",
            "So one way of making sure that the estimator does something reasonable is you just throw those additional observations that you have dreamt up, because you believe that these are the right ones into estimation procedure.",
            "But beware, because if you have very little data and you dream up the wrong observations, your procedure will not be very good.",
            "OK, so let me show you what happens with the dice."
        ],
        [
            "OK, and you've probably done that before when sort of Laplace's rule before.",
            "OK, so this occurs for instance in natural language processing.",
            "So let's say you have a bag of words representation of your document.",
            "In some words, just don't happen to occur because the document is very short or something like that.",
            "So then what you do is you just increment all your word counts by some small number.",
            "And use the slightly incremented work counts then to actually do any further processing.",
            "And this just happens to be.",
            "The effect of a conjugate prior, so you're doing density estimation.",
            "Where you're using the conjugate prior, namely that all work answer pretty much even where you might actually it at different numbers to it.",
            "As a way of encoding your prior knowledge.",
            "K. And this actually works fairly nicely.",
            "So what we now end up doing is basically we just add, well, some.",
            "Basic work onto it.",
            "And divide by that larger number.",
            "So here's again, my maximum likelihood estimator.",
            "Again, it's just trust the example of tossing the dice.",
            "20 times and these would be my standard estimates, and then I assume that, well, I've seen six additional observations.",
            "And they're really perfect.",
            "This one 1, two or three, and so on and so on and so on.",
            "And look what happens.",
            "Now the numbers are much closer to 16.",
            "Not exactly perfect, but they're perfectly, but they're fairly reasonable.",
            "OK. That's a typo, obviously.",
            "It should be one.",
            "Just to keep you on your toes.",
            "Now, if we actually assume that we've seen 100 extra observations, you will get numbers that look really pretty close to 1 / 6.",
            "So what this tells you is that if you have a prior.",
            "That's very strong.",
            "And this solution is close to what your prior presumes is the right answer.",
            "You will get the free good outcome.",
            "But Conversely, it also tells you well if you prize very strong and it's wrong, then you will get a really lousy outcome.",
            "So that's the stupid joke of what the Bayesian is.",
            "So when you send the Bayesian too.",
            "A dark room.",
            "And you tell him to find the donkey.",
            "But there's actually a horse in the room.",
            "You'll come back and say, well, yes, he found the mule.",
            "And this is really what's happening here.",
            "OK. Any questions on conjugate priors?"
        ],
        [
            "Now let's just get back to the optimization problems for a moment.",
            "Remember, we had maximum likelihood.",
            "That was a fairly simple problem, so we had basically those log partition functions, GF data.",
            "This linear term here and this was our optimization problem.",
            "Hey.",
            "If we take a normal prior, but what we get is this additional Theta squared term.",
            "And we solve this optimization problem.",
            "So it basically gets that term plus one over Sigma squared Theta.",
            "That's really the only difference.",
            "And then if we have a conjugate prior, we get this into the problem up here, but just that now our means look slightly different.",
            "So that's basically the problem that we will be getting.",
            "OK.",
            "So.",
            "If you.",
            "Can use this really efficiently.",
            "I mean, you can pretty much derive all the rest of the equations that come now.",
            "OK, so it's really really simple optimization problems.",
            "Sometimes when I go into details they will start looking ugly because, well, you need to substitute in specific forms of tree of data that will make it look really complicated or specific file, fixed size or specific ways of actually evaluating this efficiently, But this is really the basic idea and it's not difficult."
        ],
        [
            "OK, So what we get is nice convex optimization problems.",
            "And so all we have to do is we matched empirical observations and expectations.",
            "But we can get overfitting if we just do maximum likelihood.",
            "And for this you then use maximum for serious timation.",
            "So you could integrate out, but that's usually messy and not even going to talk about that.",
            "So you might want to really look at some Markov chain Monte Carlo sampling talk and go through it in detail.",
            "But it turns out that quite often just going for the mode of distribution can be a good idea.",
            "Not always, but in many cases it's good enough.",
            "We've looked at two very popular priors, Gaussian prior and the conjugate prior.",
            "They got surprise basically just well, if my parameter is small, then fine contradict prior was like.",
            "Well, let me just assume I've dreamt up some extra data and added to it.",
            "And will it just solve that problem?",
            "And the really important thing is these are all convex problems.",
            "And there are lots of ways how you can end up with a non convex problem.",
            "We might touch upon some of them later on.",
            "But this at least is all fairly nice.",
            "Any questions now?",
            "OK, well then I'll move on.",
            "So graphical models.",
            "So what I'll do is I'll briefly refresh what Sam and Martin did last week.",
            "I'll talk about conditional independence.",
            "the Hammers official decomposition and message passing, and dynamic programming.",
            "So if this is all familiar to you, just relax and take a nap."
        ],
        [
            "OK, conditional independence.",
            "Well, so X&X prime are conditionally independent given C. If the joint probability of X&X prime given see factorizes.",
            "So nice simple example would be.",
            "If X above the P of X is the probability that I'll be carrying an umbrella.",
            "If X prime is the probability that management will carry an umbrella.",
            "Now Manfred and I usually don't talk to each other before to decide whether we will carry an umbrella, but we both might look out of the window.",
            "And we look at it and see if there's some clouds.",
            "So therefore it's probably a good idea to bring an umbrella.",
            "So she would be in the clouds.",
            "Or the absence thereof.",
            "And so therefore, well, probability that Manfred and I will bring an umbrella will be highly correlated.",
            "At least they assume that and.",
            "But well, given C names, the clouds.",
            "Those two events are independent.",
            "Now I could go and model the overall probability of each of us bring an umbrella in the same way.",
            "Now this is a really complicated distribution in that we're maybe 1890 people in the room, so it's a distribution over a two to the 80 dimensional state if they really wanted to store all those coefficients, well, there's no computer that would do it, at least not at the moment.",
            "But I could go and model each individual probability fairly efficiently, so I could, for instance, watch you for a couple of days and see where they bring an umbrella or not.",
            "That would be pretty good model for you.",
            "And then assuming that you don't talk to each other before whether you will bring an umbrella or not, this will give a pretty good estimate of the overall probability of people bringing umbrellas.",
            "OK, so that's conditional independence.",
            "And this is really what would allow you to estimate the distribution of umbrellas very easily.",
            "Even so, if you don't use any of this, factorization knowledge would be nearly impossible problem.",
            "OK, now obviously.",
            "Statisticians are clever bunch in depth generalized this.",
            "So given some graph with some vertices V and edges E, we associate a random variable X with every.",
            "Vertex on the graph.",
            "Whether you have an umbrella or not, this is a vertex in the graph.",
            "And the sun might be another vertex, or the clouds or whatever.",
            "Now subsets of those random variables are conditionally independent given XY if when removing those vertices from the graph.",
            "The graph breaks up into disjoint subsets.",
            "Well, that's really a man."
        ],
        [
            "It's full of a definition.",
            "Let's look at some pictures.",
            "So that's the umbrella example.",
            "Well, that would be another one Markov chain.",
            "So simple example would be the traffic lights stand at the traffic light and if you know the current state well the previous states in the future states decouple from each other, so you don't need to know at least knowing how traffic lights work, what all the previous states were.",
            "If you know the current one.",
            "Then you can easily predict what's going to happen in the future.",
            "History some people think looks like that.",
            "So that the future events only are determined by the present.",
            "That's a more general.",
            "Crafting model, so I'll only be talking about undirected graphical models except for a small exception.",
            "And here well, if I remove those three red nodes.",
            "These two green blocks decompose.",
            "So even though I have a whole bunch of random variables here, they might be connected in really complicated ways.",
            "I can model all that fairly efficiently by really just dealing with those in dependencies.",
            "You probably still remember from Martins talk.",
            "There's this celebrated theorem of Hammersley in Clifford.",
            "Actually, they wrote it up and their proof was wrong.",
            "And then Julian bizarre prove that I think, like 2 three years later.",
            "But since they were."
        ],
        [
            "First one to write that theorem bears their name.",
            "I guess Zach was just really modest.",
            "So for that we need to get the notion of cliques.",
            "So click is a subset of a graph which is fully connected.",
            "So yeah, and the maximal cliques are just, well, maximal.",
            "Once in the graph, and they obviously also defined the graph.",
            "I've drawn three clicks here.",
            "On that graph.",
            "Can you tell me some more clicks?",
            "There are two more clicks on that.",
            "Maximal clicks on that graph that I haven't really drawn yet.",
            "Any suggestions?",
            "Otherwise, I would randomly pick somebody.",
            "This one here.",
            "Yes, very good.",
            "That's one maximal click.",
            "Is another one missing?",
            "Somebody else?",
            "There is one reason.",
            "So the one that we're trusting in the hip and connecting with the green, yes, very good.",
            "And So what we can do is we can write this entire graph down as a set of maximal cliques.",
            "This one, that one that one, that one and this one.",
            "This is actually fairly well behaved graph in terms of graphical models inferences we will see afterwards, so we wouldn't have to do anything like moralization triangulation and all that.",
            "So this is actually really nice.",
            "So this will allow me to specify dependencies between variables and then I use just some graphical models in different algorithms for inference.",
            "So what I think will start to dawn on you now is that these exponential families and.",
            "The models that Sam was talking about actually fairly closely connected.",
            "Now you can just draw those models.",
            "You instantiate the various terms as we will see with the right set of sufficient statistics and you normalize and everything will workout quite nicely.",
            "But might not be quite that obvious to you yet is how this is all connected to, say, John Phillips talk or chicken stock.",
            "Well, the idea there will be that many cases you can look at conditional models.",
            "So for instance, let's say I have.",
            "A binomial distribution, so I have.",
            "Distribution of ones and zeros.",
            "OK, let's say it's apples and oranges.",
            "And let's say I condition this probability on a picture.",
            "What do I get?",
            "I get a classifier.",
            "So I'm now modeling the probability of Apple or orange conditioned on, well, the picture of an Apple or an orange.",
            "So immediately I get a classifier.",
            "Now let's say.",
            "I have a normal distribution.",
            "That condition on some location.",
            "Well, then, what do I get?",
            "I get the regression estimator.",
            "They are not estimating the mean and possibly the variance.",
            "Conditionally, that gives you regression.",
            "It actually will give you heteroskedastic regression estimator, which is something really nice.",
            "But it's very very simple.",
            "OK, now let's."
        ],
        [
            "Put this to use.",
            "So the Hammersley Clifford theorem.",
            "Think about another 15 minutes or two.",
            "Not how much time do you think are probably another 15.",
            "OK, good.",
            "So this theorem says.",
            "That if I have some undirected graphical model with the conditional independence properties, then the joint distribution P of X. Decomposes into sums on the maximal cliques.",
            "So This is why, for instance, in the weather example I was able to actually model the joint distribution of us all carrying umbrellas around in such a simple way 'cause I only had to model the connection between.",
            "The cloud and my carrying the umbrella or somebody else carrying the umbrella.",
            "So these functions here can depend on considerably fewer variables than the joint distribution here.",
            "Obviously we still have this big problem here, namely the normalization that I need to take care of, but other than that, this is all really nice.",
            "OK, a small catch.",
            "PFX needs to be non zero on the entire domain.",
            "So if you have some serious in the domain, you just need to exclude it from the domain in the 1st place and everything works out fine again.",
            "But it's completely clear because mean each other something can never become zero.",
            "It can be arbitrary close this year, but never 0.",
            "Now.",
            "If we then use this, how much the 5th theorem for the exponential families?",
            "So far you would think well he's been talking about conditional models.",
            "Graphical models an exponential family models.",
            "How do they relate?",
            "Well, actually, in this case these two are really nicely connected, because what you will get is that the sufficient statistics for fix will just decompose into sufficient statistics on the maximal cliques.",
            "OK.",
            "Doesn't sound terribly exciting.",
            "Now let's take the inner product between those terms.",
            "Make it something that will decompose into inner products between functions that are defined on the maximal cliques only.",
            "If you remember something, let's talk.",
            "This was a Colonel.",
            "So what happens is the kernel.",
            "On a big set of random variables.",
            "Decomposes into kernels on the maximal cliques.",
            "If you have certain conditional independence properties.",
            "So if you have a big and fancy and complicated.",
            "Dependency structure.",
            "You just look at the maximal cliques and you know what your kernel needs to decompose like.",
            "This is also something that you can actually actually then use for feature selection and click determination."
        ],
        [
            "So how do we prove this?",
            "So basically just matching up terms we say no.",
            "This expression here has to look like that.",
            "Despite taking logs and matching up terms now, this initially depends on all the random variables at once, and these are functions just in the maximal cliques.",
            "This has to hold for all distributions.",
            "That's just the constant, so that's easy.",
            "So we just decompose.",
            "Say to in terms of and also normal basis.",
            "Can we just take care of those synergy?",
            "Well, then, this expression here.",
            "Can be written as such.",
            "Cause this thing times the corresponding coefficients theater I has to match the right hand side and this only works if you have individual terms which depend on the maximal cliques.",
            "This has to hold for all size and Theta.",
            "Then you just collect all those terms depending on the maximal cliques together and you define those as just feature Maps on the maximal cliques themselves.",
            "Now we can get this in a product.",
            "That's really just straightforward linear algebra.",
            "Bit tedious, but."
        ],
        [
            "Not deep.",
            "OK, now let's take an example normal distribution.",
            "So this is actually something that.",
            "Martin already had on his slides.",
            "We didn't discuss that we would show the same example, but we'll just see it would probably slightly different numbers, but other than that it's the same.",
            "So remember that for normal distribution our file fix was just X and XX transposed OK.",
            "The only thing that I've kind of snuck past two is now all of a sudden those axes are vector valued.",
            "Everybody knows how to write the vector valued normal distribution, so we would also see that, well, you have to fix transpose inverse covariance matrix X term for normal distribution.",
            "Now we know that our far fix must decompose into the subsets involving only variables from each maximal clique.",
            "So I know that I have normal distribution with certain conditional independence properties.",
            "Sometimes this is called a Gaussian Markov random field, so if you see somebody writing this, you know this is code for saying something very simple.",
            "Namely, you have a normal distribution and you have conditional independence properties.",
            "Now for the linear term.",
            "Well that's just fine anyway.",
            "That decomposes very nicely, so the only really coupling terms are those XI extra terms.",
            "They correspond to an edge in this graph.",
            "OK, now we know that for a normal distribution we have X transposed inverse covariance matrix X.",
            "We must not have any coupling terms between vertices that don't have any edge.",
            "What this means is that the inverse covariance matrix has to be sparse.",
            "OK."
        ],
        [
            "Have a look.",
            "Stop, this is my normal distribution.",
            "I have some coefficients data here.",
            "And this must be 0 whenever there's no edge.",
            "So then I can just write theater to SD universe covariance matrix.",
            "And then, well, let's just read it off.",
            "So there's an age between one and two.",
            "OK, so we can have those terms.",
            "And actually I made a mistake here.",
            "Because this should be 2 and there should be one.",
            "So swap those two numbers here.",
            "OK. And there's an age between the two and three.",
            "And then 3, four and five are fully connected.",
            "So remember, sometimes you would actually write your graph through this adjacency matrix, and I've told you in addition to all that, that yes, this actually makes a lot of sense, because this really corresponds to Gaussian Markov random field where.",
            "Unsera means that they have some dependency.",
            "So where solved large sparse linear systems before?",
            "OK, OK, well at least there are three four people.",
            "Now the hint is if you do that, it's probably really good idea if you don't use just the standard Matlab routine for solving a linear system, but you use a specific sparse linear solver.",
            "Because this sparse linear solvers are actually much faster because he exploiting the fact that the linear system has lots of zeros.",
            "The way those systems proceed is part illuminating one variable after the other.",
            "And this, as it so happens, is exactly the same procedure as the message passing algorithm.",
            "And we'll get to that a little bit.",
            "So the point here is well.",
            "The inference procedures that we've talked about in graphical models last week we just eliminate one variable after the other.",
            "We wanted to ensure that it doesn't couple with too much else.",
            "That is the same problem that you will get if you solve a linear system, because if you remove the wrong variable, all the other variables that it depends on will couple and then you will get a really dense system.",
            "And it takes a long time to solve.",
            "And you can use the very same techniques for finding a good elimination order on the graphical model.",
            "And on the sparse linear system.",
            "OK, this is here.",
            "OK."
        ],
        [
            "Well.",
            "Let's move on.",
            "So let's see how we do this, actually.",
            "Just really, really simple Markov chain.",
            "So GF data and remember I had to have the log of the sum over my entire domain and now here in the Markov chain I have expressions which depend only on adjacent random variables.",
            "So far fixed EXT plus one in some data.",
            "So this is some imov XD, XD plus one and half the summer.",
            "All the Explorer XT of the product over all those empty of XD XD plus one.",
            "OK. And how do I solve this?",
            "Well, if you look at it well, you can expand it all out.",
            "I just pushed the summation in so this is like an antenna that I'm pulling out and I'm just pushing the summations in as far as they go.",
            "And well, obviously I cannot push it beyond that variable.",
            "So for instance, the X3 summation can only go as far as that, because that's the first time where I'm hitting an X3.",
            "Next day I can push as far right as possible up to the last term.",
            "Now this only depends on X, T -- 1 XT.",
            "That's great, so I can just sum this out.",
            "So I will get some number here, which some function that it will only depend on XT minus one.",
            "They will have the summation over XT minus one here.",
            "Of some coupling term between T -- 20 -- 1 and this number here so I can do that summation, I can do the next one and the next one and the next one.",
            "My wife is good because it means that I can do this summation over well, like exponentially many numbers.",
            "In linear time.",
            "So if you've heard of the Viterbi, the forward backward algorithm or some product algorithm.",
            "Or shortest path them control programming that are all the same.",
            "It's just that some operate with the Max some operate with.",
            "Product that's wait, I mean, algebraically speaking, it's what's called a different simmering.",
            "So there's one that's called the Max plus semiring.",
            "This one that's called the standard plus time simmering.",
            "And then there's the lock plus and plus immunity.",
            "And depending on which one you pick.",
            "The equations might look a bit different, but the procedure is exactly the same.",
            "OK, it's just that basically all you do is you redefine what those sum and product operations do.",
            "I mean, this is actually how you going implemented, so you implement some objects and that makes it like something or product.",
            "And then you just redefine those and you get the different algorithm.",
            "Makes it very flexible.",
            "It's maybe not the fastest way of doing it, but very efficient.",
            "OK.",
            "So the reason why this all worked was because we could.",
            "Push the summations in.",
            "So this is what we exploited the distributive law.",
            "There's a paper called the Generalized Distributive Law.",
            "This covers that in lots of details, lots more than I can cover here.",
            "It's a great paper."
        ],
        [
            "OK, now let's have a look at it again.",
            "So what we do is, well, we can do this summation from from the Indian.",
            "That's what I explained, OK?",
            "But it could equally well also start something from the front.",
            "It's like we do a forward and backward pass and we can store all those intermediate values.",
            "Well, why would this be useful?",
            "Let's say I want to compute the probability that this red node takes on a value of one or zero.",
            "But what I have to do is have to sum over all the random variables here in sum over all the random variables here.",
            "But this is exactly what my forward pass does.",
            "This is exactly what the backward pass stars.",
            "So I can compute this probability of the random variable here.",
            "Right integrated out all the green ones very easily by just running the forward pass up to the right variable.",
            "Running the backwards pass up to this red variable and I'm done.",
            "OK. Now let's assume somebody tells you what you want to get those probabilities for all those nodes.",
            "Nothing easier than that.",
            "We just run the forward pass through all the intermediate values, run the backwards past or all the intermediate values and then I can get the rest very easily.",
            "So this is the simplest case of message passing."
        ],
        [
            "Now imagine we've got a tree.",
            "I want to get the probability that this variable is 01.",
            "So after some over all those variables after some over all those variables and have to sum over all these.",
            "And you get all those three incoming messages, and you can compute that probability.",
            "Now how do you get the message that you can send on?",
            "Well, it's just some over this.",
            "Some of that take the product.",
            "Some of this variable here and you've got the next message going out.",
            "So this is really the very very simple communications protocol for message passing algorithm.",
            "Receive messages from everyone but the one where you want to send a message to.",
            "Remove all the dependencies on the other incoming nodes.",
            "And send it on.",
            "OK, and the cool thing is you can run this in parallel, so each of those things can be a little computer, little sensor network node.",
            "And then, well, you send a message here.",
            "You send one layer once one there you do that on all the nodes.",
            "In the end, you have all the messages that you need to get probabilities.",
            "It's.",
            "Really nice and simple idea.",
            "Any questions here?",
            "Who can tell me if I run this in parallel?",
            "It is every node does this at the same time, how long it will take.",
            "Until I get the correct message here.",
            "And I'll tell you one trick with initialization if you haven't got the right message from the other node yet, you just invent something.",
            "OK. And assume that this is the.",
            "This would be the right message, OK?",
            "Now, how long would it take to get the correct probabilities here?",
            "Anyone TS?",
            "Let's think about it for a moment.",
            "See in order for this message here to be correct.",
            "This node needs to send a message here.",
            "That now needs to do something and send the message one.",
            "So in order to get that message correct, I need 2 steps.",
            "12 with this one I also need 2 steps.",
            "1, two in this case.",
            "Here I need three steps 123.",
            "OK.",
            "So after three iterations of this parallel algorithm, Now this message here is.",
            "I can compute this probability exactly.",
            "How long would it take for this note?",
            "Any suggestions?",
            "Well, let's count.",
            "Hello.",
            "1.",
            "234 what?",
            "I've stepped.",
            "So it's really easy.",
            "You look at the graph, just count what the longest path is and that's what it takes until everything is converged.",
            "So.",
            "So really easy algorithm."
        ],
        [
            "Um?",
            "Yep.",
            "Randomly, Yep.",
            "Where is that?",
            "Well because basically.",
            "All the messages until you get the right one.",
            "I mean, they don't really matter.",
            "I mean, you might as well not send any correct message in that case.",
            "So if you were to implement that on a single node.",
            "You would not implement it in parallel.",
            "What you would do then is the following.",
            "I would first so in that case I would just first look at well where can I actually already send a message to and it would start at the leaves of this tree.",
            "OK so I can send all the messages from the leaves can do this one that one that one that one that one that one that one OK. Whatever the other notes do during that time doesn't really matter.",
            "Now all the sudden this node realizes up.",
            "I've got the message from here message from there so I can at least send a message on here.",
            "OK, so they forward this.",
            "Now.",
            "Well, this note here realizes I've received some incoming messages from here and there.",
            "I can forward it there, and likewise the right node.",
            "That's the next step that we will be doing.",
            "All the other nodes cannot really do anything because they haven't received any other message back.",
            "But now things start getting interesting.",
            "Now this note here.",
            "OK, it's received a message from here to receive the message from there so it can send a message up there.",
            "It can also send a message down here.",
            "OK.",
            "So by that time already this written out in this green node will or will know exactly what their probabilities are.",
            "Now, once this green note, he has received its incoming message.",
            "It rises OK. Got the message from here from there so I can send it on there.",
            "Likewise I put the message from here and from here I can send it up there.",
            "So this is how you would implement it sequentially.",
            "This is considerably less wasteful, but if your other nodes do something.",
            "Stupid.",
            "Well in the meantime, well OK, there's no harm.",
            "So I guess my question is why?",
            "Why not wait until you get incoming message?",
            "OK, if you have a single computer on which you implement all this, you're absolutely right.",
            "What I suggested is stupid, but if you have, say, a really big graphical model and each of those nodes is a computer.",
            "You might not even know how big the network is.",
            "And so it's a good idea just to get started running.",
            "The other thing is, and this is quite a curious phenomenon, sometimes really far away nodes may not have much of an effect on what you're going to do locally.",
            "So even though you started with the wrong message eventually, as you pass it through lots and lots and lots and lots of nodes which already do something to do the right thing, they'll make a wrong thing, right?",
            "Motors, a point.",
            "If you have a network of computers, some of them might go down sometime might might be running.",
            "And you might not really know beforehand which ones are up and which ones are down.",
            "Obviously you wouldn't want to just send a random message along if you don't have any other don't have an incoming message.",
            "So a really good idea is to take some prior.",
            "And then send the right message that looks most similar to what you would have received.",
            "Two neighbors so there's a very nice paper by colors, question and Mark Paskin from UI 2005, and they do exactly that, and they show that very, very quickly.",
            "You can get conversions in this way if you have a nicely factorizing prior.",
            "And in their case they actually have the real scenario of taking since the network nodes and they have to talk to each other and sometimes they break or sometimes the connection doesn't work and it's a very beautiful paper which does exactly that.",
            "The thing is, if you can run the algorithm independent in each node, it's just so much easier to code up.",
            "So you're winning both in terms of statistics and in terms of implementation.",
            "Which is a great situation.",
            "Any further questions?",
            "OK, so of course I've only told you the beautiful part of the story.",
            "What happens if?",
            "OK, so this is these are still the equations, but afterwards you might wonder what happens if I don't have a tree but some arbitrary graph.",
            "So let's assume I have some clique potentials in my SIX Ray now might as well initialize all those messages.",
            "My JFX jakkals one.",
            "And then I update the outgoing messages.",
            "This way, so that's just all the incoming ones, plus the connection and I sum over all the other nodes.",
            "That gives me the message going from I to J.",
            "This is really just an equations with a explaining pictures before you can easily read arrived at.",
            "And well, this message passing algorithm converges after in iterations.",
            "That's what we've proved before.",
            "Now the hack is just use that for graphs with loops and you help.",
            "So this is what Martin was talking about as loopy belief propagation.",
            "And he actually gave pretty good reasons why this sometimes might be a good idea."
        ],
        [
            "No.",
            "Let's say we have some graph like this one here.",
            "It's only a nice graph, is nicely triangulated.",
            "Then I can set something up that's called a junction tree.",
            "So what I do is I write out the maximal cliques that's a maximum click 123 and there's 1235.",
            "There's 1567 and that's 1354.",
            "OK, now just write them out and then I write which nodes connect those clicks.",
            "So the intersection between these two are the other vertices two and three.",
            "His note is the Vertex wife, and here's the vertex five and three.",
            "And this graph has what's called the running intersection property.",
            "And this means that, well, if, say, the number 3 is shared between this and that.",
            "Click, then it has to be shared all along the path, which is exactly true.",
            "Likewise, the Fife is shared between these two paths, and that's also true.",
            "So what that means is if I have some dependency that involves this node 5.",
            "And I'm dealing with it down here and I also have to deal with it up here.",
            "Then these two effects need to be connected.",
            "OK. And then all I do is I just do my message passing now on the junction tree.",
            "And the key difference is that before that I only had to deal maybe with the states of a single variable.",
            "So for binary variable they only needed to send on 2 numbers.",
            "Cheap now if.",
            "So for instance, I have two bonded variables already, will have to send on 4 numbers.",
            "Say 510 value variables.",
            "How many numbers do I have to send along?",
            "Any volunteers?",
            "Well, two to the 10 is 1024 so I have to send along a lot of numbers.",
            "So this thing grows exponentially with the size of the separating set.",
            "So that's really bad.",
            "If you do this, track."
        ],
        [
            "Station and you end up with a very thick junction tree as it's called.",
            "Then the algorithm will run very slowly and will need a lot of memory and all that, so you might not want to do this.",
            "So these are exactly the cases where what Martin suggested might be more efficient.",
            "OK, now these are the same equations as what I've shown you before for the trees.",
            "Let's just go back to slides.",
            "See, these are the messages for the tree.",
            "And these are the messages for the junction tree.",
            "So the key difference is now while those accesses are just, well, not not just a single line available anymore, but they're just whatever is in the separators it.",
            "So we just have fancier quantities.",
            "But the algorithm is exactly the same.",
            "So what we do is we just replaced our tree with the hypertree.",
            "This is like you know you have a network of roads and at some point the traffic gets too large and you will add a second or third or fourth line.",
            "And this is really cool end of it.",
            "So it's just that you have to carry several variables along.",
            "And that's what's called a hyper tree or hyper graph.",
            "Just like rather roads having highways.",
            "Then again, you can use that for graphs with loops and you hope.",
            "And there are nice strategies how you can do something in between like you.",
            "Well, make a slightly fancier message passing algorithm and this is what Martin was referring to as the work by Edie."
        ],
        [
            "K. So again his example.",
            "So in order to do something here to send this message along, I need to get the message.",
            "Which depends on Y2 and Y-3.",
            "Right, because these are the variables that really matter here.",
            "Here I need to send a message that depends on what variables Wi-Fi from Y-3.",
            "Now I have those two incoming.",
            "And this is what I have to send up.",
            "So that means I can integrate out of the variables two and three, and this is the only message that I need to send an send this on, then do something else here.",
            "They do that at every node.",
            "So it's really the same strategies with the trees, just it now.",
            "The variables are a little bit more complicated.",
            "OK."
        ],
        [
            "OK, where in the homework stretch?",
            "So I already explained this algorithm really can go bad with the treewidth.",
            "So if you have a really really fat junction tree.",
            "Then I have to send a lot of messages along.",
            "It takes a long time algorithm run very slowly and they might run out of memory.",
            "I can just use loopy belief propagation.",
            "There's a very nice paper by Alex Vela, and they have essentially a convergence monitor.",
            "Is this loopy belief propagation might or might not converge to the right thing, So what you do is you.",
            "Run like a control device, something that tells you how close you are from a fixed point so they have a nice upper bound for that.",
            "You can check.",
            "Whether you're too far away or not, can you run this along and then after the fact that you've run the algorithm, this device will tell you well whether you have reasonable hope to believe that your solution is correct or not.",
            "Anne.",
            "Full you could also average or very spanning trees.",
            "This is what Martin referred to.",
            "You can use sampling methods, so there's a very nice paper by Nana Diffracts in coworkers.",
            "He basically conditions on subsets of the tree samples from the rest and iterates efficiently.",
            "And that works fairly nicely.",
            "It's also pretty easy to implement.",
            "Or you can use semidefinite relaxations.",
            "And this is a very, very active area of research.",
            "How to work around the issue of those intractable graphical models.",
            "It's a very fertile area because I mean you can show that the overall problem, even though it looks like a nice convex thing, is actually NP hard.",
            "And so you want to find good approximations and you want to characterize in which situations you can do something sensible.",
            "And if you go to UI, for instance, you will find lots of papers on that."
        ],
        [
            "So let's just sum up.",
            "And then we definitely did deserve a good break.",
            "Um?",
            "Yes, I think I've got 15 minutes overtime.",
            "So let's just recall the overall optimization problem.",
            "Maximum likelihood was take this lock partition function.",
            "Minus.",
            "There's sufficient statistics and you just sit that derivative to 0.",
            "This is what you get.",
            "In other words, the expectation has to match the programming.",
            "The same thing holds for my graphical models.",
            "It's just that my file fixes may look a little bit more complicated, and in order to compute the expectations, have to do with more work, but it's the same equation.",
            "Then if I want to regularize, I might just either say, well, I just want my theaters to be small.",
            "I get this problem.",
            "Hello can you?",
            "This message will be your friend.",
            "Or you can say, well, I have some conjugate prior.",
            "In other words, I dream up some data.",
            "And I saw for that.",
            "And any of those two methods will give you.",
            "Slightly better behaved estimates."
        ],
        [
            "OK, so summing up.",
            "What we did is we look at the hammer hammer Clifford theorem.",
            "Reviewed in conditional independence message passing.",
            "And in the end I talked a little bit about approximate inference and loopy belief propagation.",
            "And I guess the take home message for you is, well, just try it out.",
            "See what the loopy BP does.",
            "Anything useful at all?",
            "If it does, you've solved the problem.",
            "If it doesn't, then you should start worrying about better solutions.",
            "But there's a fair chance that if you have, problem is fairly simple, you will already get a decent answer.",
            "OK.",
            "So that's what we did.",
            "Exponential family.",
            "Bit of statistical inference.",
            "Base in middle models and in the end we did some graphical models.",
            "So I think now we all deserve a break, but if you have any questions, please ask Now or during the break.",
            "Any questions now everybody really tired.",
            "OK, let's have a break then."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much for the invitation.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to be here.",
                    "label": 0
                },
                {
                    "sent": "I must add.",
                    "label": 0
                },
                {
                    "sent": "Well, I didn't do anything really for this summer school.",
                    "label": 0
                },
                {
                    "sent": "This was all the local organizing committee.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you will see that large values of Theta are necessary to drive the distribution close to one or close to 0.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the probability close to one or close to 0.",
                    "label": 0
                },
                {
                    "sent": "So I'm not saying that this is necessarily a very good prior.",
                    "label": 0
                },
                {
                    "sent": "But if you have no other idea, this might actually be not a bad thing.",
                    "label": 0
                },
                {
                    "sent": "I'll be discussing conjugate priors in the moment.",
                    "label": 0
                },
                {
                    "sent": "And we'll get back to why this is actually not such a stupid idea in the research talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how it works.",
                    "label": 0
                },
                {
                    "sent": "Blue things are the ones that you will get through maximum likelihood estimation.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly the same plot as what you saw before the red bars or what you get by using maximum posterior estimation.",
                    "label": 0
                },
                {
                    "sent": "OK. Now what you see is well.",
                    "label": 0
                },
                {
                    "sent": "I didn't observe any force here.",
                    "label": 0
                },
                {
                    "sent": "Nonetheless, my Bayesian procedure or penalized log likelihood procedure if you want if you want to be pedantic, does something quite useful here, so it still gives it the benefit of doubt and says well, maybe I didn't see enough data, so I should actually up like this.",
                    "label": 0
                },
                {
                    "sent": "It down ways.",
                    "label": 0
                },
                {
                    "sent": "These extreme probabilities as well.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you should see is as we get more and more data.",
                    "label": 0
                },
                {
                    "sent": "This is the blue and the red curve gets very close to each other.",
                    "label": 0
                },
                {
                    "sent": "Actually, what you see from this is I probably didn't use a strong enough prior.",
                    "label": 0
                },
                {
                    "sent": "Because actually know that this was.",
                    "label": 0
                },
                {
                    "sent": "Fair dice would say constructed it this way.",
                    "label": 0
                },
                {
                    "sent": "And it I would want that this actually voice things even well, considerably evenly here.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, but.",
                    "label": 0
                },
                {
                    "sent": "So this tells you that the Bayesian procedure.",
                    "label": 0
                },
                {
                    "sent": "In this case, is not the magic bullet, but it can improve things quite a bit.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Well, there's another prior and I'm just going to mention it briefly on two slides.",
                    "label": 0
                },
                {
                    "sent": "There's this thing called conjugate priors.",
                    "label": 0
                },
                {
                    "sent": "I'm mentioning it because quite often you would read some statistics paper and they will talk about say, OK.",
                    "label": 0
                },
                {
                    "sent": "Here's the binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we use a conjugate distribution like the Dirichlet blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "Or we estimate with the normal distribution and then we use as the conjugate the wish heart and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And it's quite intimidating initially when you read this, you think like how did they pull this out of the hat?",
                    "label": 0
                },
                {
                    "sent": "There's actually very, very easy procedure how you can get the conjugate distribution if you know the exponential family distribution that you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "That's just all have fun, funny names and they all come with funny parametrizations.",
                    "label": 0
                },
                {
                    "sent": "So probably the first thing you might want to do if some paper talks about this, just write it back in this standardized exponential families forming and everything is actually not very scary.",
                    "label": 0
                },
                {
                    "sent": "See for instance, might read of the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "And in this case you have certain self reinforcing properties.",
                    "label": 0
                },
                {
                    "sent": "Now just show these.",
                    "label": 0
                },
                {
                    "sent": "Explain the situation afterwards.",
                    "label": 0
                },
                {
                    "sent": "But basically this is pretty close to what the Chinese restaurant process will do.",
                    "label": 0
                },
                {
                    "sent": "Is it seasonal observations?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So and the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "So what we actually have is that my posterior will look different from the likelihood.",
                    "label": 1
                },
                {
                    "sent": "This is maybe not such a good idea.",
                    "label": 0
                },
                {
                    "sent": "Because, say for instance, if you go out with your friends and play a lot of.",
                    "label": 0
                },
                {
                    "sent": "Games with devices then you will just have seen a lot of data.",
                    "label": 0
                },
                {
                    "sent": "So you might think that it's a good way of.",
                    "label": 0
                },
                {
                    "sent": "Representing your prior knowledge in the form of a lot of extra data that you've seen.",
                    "label": 0
                },
                {
                    "sent": "Like you've played with him before, maybe 10 times.",
                    "label": 0
                },
                {
                    "sent": "So maybe overall you toss the dice 100 times before now you play with them again and you toss it 10 times, but actually it's as if you still remember those 100 times.",
                    "label": 0
                },
                {
                    "sent": "Do you toss the dice before?",
                    "label": 0
                },
                {
                    "sent": "So that's a convenient way of representing prior knowledge, just as extra data.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What I want is.",
                    "label": 0
                },
                {
                    "sent": "But PSA to parameterized by X has a similar functional form as P of X parameters voice data.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Pfc to parameters by some coefficient a, so I'm just guessing the solution and I'll show you that this does the trick.",
                    "label": 0
                },
                {
                    "sent": "I'll just write it as E to the inner product between Theta and MCO times a.",
                    "label": 0
                },
                {
                    "sent": "Minus MCO times G of Theta.",
                    "label": 0
                },
                {
                    "sent": "Let me just explain on the whiteboard what's going on.",
                    "label": 0
                },
                {
                    "sent": "So remember the exponential family distribution.",
                    "label": 0
                },
                {
                    "sent": "Phi of X. Parameterized by theater.",
                    "label": 0
                },
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "E to the inner product between 4 pics.",
                    "label": 0
                },
                {
                    "sent": "And Theta.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "G of Theta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In particular, if we had a lot of data.",
                    "label": 1
                },
                {
                    "sent": "This expression here.",
                    "label": 0
                },
                {
                    "sent": "Would be replaced by.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "M times mu.",
                    "label": 0
                },
                {
                    "sent": "And this will be replaced by M. Hey.",
                    "label": 0
                },
                {
                    "sent": "So now I can write this.",
                    "label": 0
                },
                {
                    "sent": "Also ask.",
                    "label": 0
                },
                {
                    "sent": "Vendor product between the vector Theta.",
                    "label": 0
                },
                {
                    "sent": "And G of Theta.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "They make em times mu hat.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "M. Now what is this good for?",
                    "label": 0
                },
                {
                    "sent": "Well, this looks like an inner product where this is now my future map.",
                    "label": 0
                },
                {
                    "sent": "So this looks like.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "Five of Satan now.",
                    "label": 0
                },
                {
                    "sent": "And this looks like some other parameter.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what if I picked some distribution on the theaters?",
                    "label": 0
                },
                {
                    "sent": "Where my 5 seater looks like this and then I have some number here.",
                    "label": 0
                },
                {
                    "sent": "This is precisely what's happening over there.",
                    "label": 0
                },
                {
                    "sent": "OK, in that case I can just get my Pfc to give an X. OK. Pfc to give an X just proportional to.",
                    "label": 0
                },
                {
                    "sent": "P of X given Theta times P of data.",
                    "label": 0
                },
                {
                    "sent": "And let me call these parameters here.",
                    "label": 0
                },
                {
                    "sent": "Mo Times mu 0 M 0.",
                    "label": 0
                },
                {
                    "sent": "The reason why you might want to call them this way is this is like the effective number of observations that you have in your prior.",
                    "label": 0
                },
                {
                    "sent": "This is the effective mean.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, now all I have to do is just plug this in so we get.",
                    "label": 0
                },
                {
                    "sent": "He to the.",
                    "label": 0
                },
                {
                    "sent": "Now we have some.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "M times mu hat.",
                    "label": 0
                },
                {
                    "sent": "Plus Mo times musiro.",
                    "label": 0
                },
                {
                    "sent": "Sing time here.",
                    "label": 0
                },
                {
                    "sent": "In approach with data.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "And here we have.",
                    "label": 0
                },
                {
                    "sent": "N + M O times G of data.",
                    "label": 0
                },
                {
                    "sent": "OK, then we'll get some normalization.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now, what does this look like?",
                    "label": 0
                },
                {
                    "sent": "So if I divide this by M + M zero.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I optimize over Theta.",
                    "label": 0
                },
                {
                    "sent": "Well, this is like.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Estimation problem.",
                    "label": 0
                },
                {
                    "sent": "Just with the key difference that now all the sudden.",
                    "label": 0
                },
                {
                    "sent": "I will get.",
                    "label": 0
                },
                {
                    "sent": "Instead of the original number of observations.",
                    "label": 0
                },
                {
                    "sent": "M + M O mini.",
                    "label": 0
                },
                {
                    "sent": "And instead of the mean that I'm serving, I get.",
                    "label": 1
                },
                {
                    "sent": "So to say some fake mean.",
                    "label": 0
                },
                {
                    "sent": "This is a really convenient way of encoding a prior, even procedurally.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "You might not necessarily be willing to specify a priori exactly.",
                    "label": 0
                },
                {
                    "sent": "But you might have an idea of what typical observations look like.",
                    "label": 0
                },
                {
                    "sent": "So one way of making sure that the estimator does something reasonable is you just throw those additional observations that you have dreamt up, because you believe that these are the right ones into estimation procedure.",
                    "label": 0
                },
                {
                    "sent": "But beware, because if you have very little data and you dream up the wrong observations, your procedure will not be very good.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me show you what happens with the dice.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and you've probably done that before when sort of Laplace's rule before.",
                    "label": 0
                },
                {
                    "sent": "OK, so this occurs for instance in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a bag of words representation of your document.",
                    "label": 0
                },
                {
                    "sent": "In some words, just don't happen to occur because the document is very short or something like that.",
                    "label": 0
                },
                {
                    "sent": "So then what you do is you just increment all your word counts by some small number.",
                    "label": 0
                },
                {
                    "sent": "And use the slightly incremented work counts then to actually do any further processing.",
                    "label": 0
                },
                {
                    "sent": "And this just happens to be.",
                    "label": 0
                },
                {
                    "sent": "The effect of a conjugate prior, so you're doing density estimation.",
                    "label": 1
                },
                {
                    "sent": "Where you're using the conjugate prior, namely that all work answer pretty much even where you might actually it at different numbers to it.",
                    "label": 0
                },
                {
                    "sent": "As a way of encoding your prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "K. And this actually works fairly nicely.",
                    "label": 0
                },
                {
                    "sent": "So what we now end up doing is basically we just add, well, some.",
                    "label": 0
                },
                {
                    "sent": "Basic work onto it.",
                    "label": 0
                },
                {
                    "sent": "And divide by that larger number.",
                    "label": 0
                },
                {
                    "sent": "So here's again, my maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "Again, it's just trust the example of tossing the dice.",
                    "label": 0
                },
                {
                    "sent": "20 times and these would be my standard estimates, and then I assume that, well, I've seen six additional observations.",
                    "label": 0
                },
                {
                    "sent": "And they're really perfect.",
                    "label": 0
                },
                {
                    "sent": "This one 1, two or three, and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And look what happens.",
                    "label": 0
                },
                {
                    "sent": "Now the numbers are much closer to 16.",
                    "label": 0
                },
                {
                    "sent": "Not exactly perfect, but they're perfectly, but they're fairly reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK. That's a typo, obviously.",
                    "label": 0
                },
                {
                    "sent": "It should be one.",
                    "label": 0
                },
                {
                    "sent": "Just to keep you on your toes.",
                    "label": 0
                },
                {
                    "sent": "Now, if we actually assume that we've seen 100 extra observations, you will get numbers that look really pretty close to 1 / 6.",
                    "label": 0
                },
                {
                    "sent": "So what this tells you is that if you have a prior.",
                    "label": 0
                },
                {
                    "sent": "That's very strong.",
                    "label": 0
                },
                {
                    "sent": "And this solution is close to what your prior presumes is the right answer.",
                    "label": 0
                },
                {
                    "sent": "You will get the free good outcome.",
                    "label": 0
                },
                {
                    "sent": "But Conversely, it also tells you well if you prize very strong and it's wrong, then you will get a really lousy outcome.",
                    "label": 0
                },
                {
                    "sent": "So that's the stupid joke of what the Bayesian is.",
                    "label": 0
                },
                {
                    "sent": "So when you send the Bayesian too.",
                    "label": 0
                },
                {
                    "sent": "A dark room.",
                    "label": 0
                },
                {
                    "sent": "And you tell him to find the donkey.",
                    "label": 1
                },
                {
                    "sent": "But there's actually a horse in the room.",
                    "label": 0
                },
                {
                    "sent": "You'll come back and say, well, yes, he found the mule.",
                    "label": 1
                },
                {
                    "sent": "And this is really what's happening here.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions on conjugate priors?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's just get back to the optimization problems for a moment.",
                    "label": 0
                },
                {
                    "sent": "Remember, we had maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "That was a fairly simple problem, so we had basically those log partition functions, GF data.",
                    "label": 0
                },
                {
                    "sent": "This linear term here and this was our optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 1
                },
                {
                    "sent": "If we take a normal prior, but what we get is this additional Theta squared term.",
                    "label": 0
                },
                {
                    "sent": "And we solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So it basically gets that term plus one over Sigma squared Theta.",
                    "label": 1
                },
                {
                    "sent": "That's really the only difference.",
                    "label": 0
                },
                {
                    "sent": "And then if we have a conjugate prior, we get this into the problem up here, but just that now our means look slightly different.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the problem that we will be getting.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "Can use this really efficiently.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can pretty much derive all the rest of the equations that come now.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's really really simple optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when I go into details they will start looking ugly because, well, you need to substitute in specific forms of tree of data that will make it look really complicated or specific file, fixed size or specific ways of actually evaluating this efficiently, But this is really the basic idea and it's not difficult.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we get is nice convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "And so all we have to do is we matched empirical observations and expectations.",
                    "label": 0
                },
                {
                    "sent": "But we can get overfitting if we just do maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And for this you then use maximum for serious timation.",
                    "label": 0
                },
                {
                    "sent": "So you could integrate out, but that's usually messy and not even going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "So you might want to really look at some Markov chain Monte Carlo sampling talk and go through it in detail.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that quite often just going for the mode of distribution can be a good idea.",
                    "label": 0
                },
                {
                    "sent": "Not always, but in many cases it's good enough.",
                    "label": 0
                },
                {
                    "sent": "We've looked at two very popular priors, Gaussian prior and the conjugate prior.",
                    "label": 0
                },
                {
                    "sent": "They got surprise basically just well, if my parameter is small, then fine contradict prior was like.",
                    "label": 0
                },
                {
                    "sent": "Well, let me just assume I've dreamt up some extra data and added to it.",
                    "label": 0
                },
                {
                    "sent": "And will it just solve that problem?",
                    "label": 0
                },
                {
                    "sent": "And the really important thing is these are all convex problems.",
                    "label": 0
                },
                {
                    "sent": "And there are lots of ways how you can end up with a non convex problem.",
                    "label": 0
                },
                {
                    "sent": "We might touch upon some of them later on.",
                    "label": 0
                },
                {
                    "sent": "But this at least is all fairly nice.",
                    "label": 0
                },
                {
                    "sent": "Any questions now?",
                    "label": 0
                },
                {
                    "sent": "OK, well then I'll move on.",
                    "label": 0
                },
                {
                    "sent": "So graphical models.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do is I'll briefly refresh what Sam and Martin did last week.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about conditional independence.",
                    "label": 1
                },
                {
                    "sent": "the Hammers official decomposition and message passing, and dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So if this is all familiar to you, just relax and take a nap.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Well, so X&X prime are conditionally independent given C. If the joint probability of X&X prime given see factorizes.",
                    "label": 1
                },
                {
                    "sent": "So nice simple example would be.",
                    "label": 0
                },
                {
                    "sent": "If X above the P of X is the probability that I'll be carrying an umbrella.",
                    "label": 0
                },
                {
                    "sent": "If X prime is the probability that management will carry an umbrella.",
                    "label": 0
                },
                {
                    "sent": "Now Manfred and I usually don't talk to each other before to decide whether we will carry an umbrella, but we both might look out of the window.",
                    "label": 0
                },
                {
                    "sent": "And we look at it and see if there's some clouds.",
                    "label": 0
                },
                {
                    "sent": "So therefore it's probably a good idea to bring an umbrella.",
                    "label": 0
                },
                {
                    "sent": "So she would be in the clouds.",
                    "label": 0
                },
                {
                    "sent": "Or the absence thereof.",
                    "label": 0
                },
                {
                    "sent": "And so therefore, well, probability that Manfred and I will bring an umbrella will be highly correlated.",
                    "label": 0
                },
                {
                    "sent": "At least they assume that and.",
                    "label": 0
                },
                {
                    "sent": "But well, given C names, the clouds.",
                    "label": 0
                },
                {
                    "sent": "Those two events are independent.",
                    "label": 0
                },
                {
                    "sent": "Now I could go and model the overall probability of each of us bring an umbrella in the same way.",
                    "label": 0
                },
                {
                    "sent": "Now this is a really complicated distribution in that we're maybe 1890 people in the room, so it's a distribution over a two to the 80 dimensional state if they really wanted to store all those coefficients, well, there's no computer that would do it, at least not at the moment.",
                    "label": 0
                },
                {
                    "sent": "But I could go and model each individual probability fairly efficiently, so I could, for instance, watch you for a couple of days and see where they bring an umbrella or not.",
                    "label": 0
                },
                {
                    "sent": "That would be pretty good model for you.",
                    "label": 0
                },
                {
                    "sent": "And then assuming that you don't talk to each other before whether you will bring an umbrella or not, this will give a pretty good estimate of the overall probability of people bringing umbrellas.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's conditional independence.",
                    "label": 0
                },
                {
                    "sent": "And this is really what would allow you to estimate the distribution of umbrellas very easily.",
                    "label": 0
                },
                {
                    "sent": "Even so, if you don't use any of this, factorization knowledge would be nearly impossible problem.",
                    "label": 0
                },
                {
                    "sent": "OK, now obviously.",
                    "label": 0
                },
                {
                    "sent": "Statisticians are clever bunch in depth generalized this.",
                    "label": 0
                },
                {
                    "sent": "So given some graph with some vertices V and edges E, we associate a random variable X with every.",
                    "label": 1
                },
                {
                    "sent": "Vertex on the graph.",
                    "label": 0
                },
                {
                    "sent": "Whether you have an umbrella or not, this is a vertex in the graph.",
                    "label": 0
                },
                {
                    "sent": "And the sun might be another vertex, or the clouds or whatever.",
                    "label": 1
                },
                {
                    "sent": "Now subsets of those random variables are conditionally independent given XY if when removing those vertices from the graph.",
                    "label": 0
                },
                {
                    "sent": "The graph breaks up into disjoint subsets.",
                    "label": 0
                },
                {
                    "sent": "Well, that's really a man.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's full of a definition.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some pictures.",
                    "label": 0
                },
                {
                    "sent": "So that's the umbrella example.",
                    "label": 0
                },
                {
                    "sent": "Well, that would be another one Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So simple example would be the traffic lights stand at the traffic light and if you know the current state well the previous states in the future states decouple from each other, so you don't need to know at least knowing how traffic lights work, what all the previous states were.",
                    "label": 0
                },
                {
                    "sent": "If you know the current one.",
                    "label": 0
                },
                {
                    "sent": "Then you can easily predict what's going to happen in the future.",
                    "label": 0
                },
                {
                    "sent": "History some people think looks like that.",
                    "label": 0
                },
                {
                    "sent": "So that the future events only are determined by the present.",
                    "label": 0
                },
                {
                    "sent": "That's a more general.",
                    "label": 0
                },
                {
                    "sent": "Crafting model, so I'll only be talking about undirected graphical models except for a small exception.",
                    "label": 0
                },
                {
                    "sent": "And here well, if I remove those three red nodes.",
                    "label": 0
                },
                {
                    "sent": "These two green blocks decompose.",
                    "label": 0
                },
                {
                    "sent": "So even though I have a whole bunch of random variables here, they might be connected in really complicated ways.",
                    "label": 0
                },
                {
                    "sent": "I can model all that fairly efficiently by really just dealing with those in dependencies.",
                    "label": 0
                },
                {
                    "sent": "You probably still remember from Martins talk.",
                    "label": 0
                },
                {
                    "sent": "There's this celebrated theorem of Hammersley in Clifford.",
                    "label": 0
                },
                {
                    "sent": "Actually, they wrote it up and their proof was wrong.",
                    "label": 0
                },
                {
                    "sent": "And then Julian bizarre prove that I think, like 2 three years later.",
                    "label": 0
                },
                {
                    "sent": "But since they were.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First one to write that theorem bears their name.",
                    "label": 0
                },
                {
                    "sent": "I guess Zach was just really modest.",
                    "label": 0
                },
                {
                    "sent": "So for that we need to get the notion of cliques.",
                    "label": 0
                },
                {
                    "sent": "So click is a subset of a graph which is fully connected.",
                    "label": 1
                },
                {
                    "sent": "So yeah, and the maximal cliques are just, well, maximal.",
                    "label": 1
                },
                {
                    "sent": "Once in the graph, and they obviously also defined the graph.",
                    "label": 0
                },
                {
                    "sent": "I've drawn three clicks here.",
                    "label": 0
                },
                {
                    "sent": "On that graph.",
                    "label": 0
                },
                {
                    "sent": "Can you tell me some more clicks?",
                    "label": 0
                },
                {
                    "sent": "There are two more clicks on that.",
                    "label": 0
                },
                {
                    "sent": "Maximal clicks on that graph that I haven't really drawn yet.",
                    "label": 0
                },
                {
                    "sent": "Any suggestions?",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I would randomly pick somebody.",
                    "label": 0
                },
                {
                    "sent": "This one here.",
                    "label": 0
                },
                {
                    "sent": "Yes, very good.",
                    "label": 0
                },
                {
                    "sent": "That's one maximal click.",
                    "label": 0
                },
                {
                    "sent": "Is another one missing?",
                    "label": 0
                },
                {
                    "sent": "Somebody else?",
                    "label": 0
                },
                {
                    "sent": "There is one reason.",
                    "label": 0
                },
                {
                    "sent": "So the one that we're trusting in the hip and connecting with the green, yes, very good.",
                    "label": 0
                },
                {
                    "sent": "And So what we can do is we can write this entire graph down as a set of maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "This one, that one that one, that one and this one.",
                    "label": 0
                },
                {
                    "sent": "This is actually fairly well behaved graph in terms of graphical models inferences we will see afterwards, so we wouldn't have to do anything like moralization triangulation and all that.",
                    "label": 0
                },
                {
                    "sent": "So this is actually really nice.",
                    "label": 1
                },
                {
                    "sent": "So this will allow me to specify dependencies between variables and then I use just some graphical models in different algorithms for inference.",
                    "label": 0
                },
                {
                    "sent": "So what I think will start to dawn on you now is that these exponential families and.",
                    "label": 0
                },
                {
                    "sent": "The models that Sam was talking about actually fairly closely connected.",
                    "label": 0
                },
                {
                    "sent": "Now you can just draw those models.",
                    "label": 0
                },
                {
                    "sent": "You instantiate the various terms as we will see with the right set of sufficient statistics and you normalize and everything will workout quite nicely.",
                    "label": 0
                },
                {
                    "sent": "But might not be quite that obvious to you yet is how this is all connected to, say, John Phillips talk or chicken stock.",
                    "label": 0
                },
                {
                    "sent": "Well, the idea there will be that many cases you can look at conditional models.",
                    "label": 0
                },
                {
                    "sent": "So for instance, let's say I have.",
                    "label": 0
                },
                {
                    "sent": "A binomial distribution, so I have.",
                    "label": 0
                },
                {
                    "sent": "Distribution of ones and zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say it's apples and oranges.",
                    "label": 0
                },
                {
                    "sent": "And let's say I condition this probability on a picture.",
                    "label": 0
                },
                {
                    "sent": "What do I get?",
                    "label": 0
                },
                {
                    "sent": "I get a classifier.",
                    "label": 0
                },
                {
                    "sent": "So I'm now modeling the probability of Apple or orange conditioned on, well, the picture of an Apple or an orange.",
                    "label": 0
                },
                {
                    "sent": "So immediately I get a classifier.",
                    "label": 0
                },
                {
                    "sent": "Now let's say.",
                    "label": 0
                },
                {
                    "sent": "I have a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "That condition on some location.",
                    "label": 0
                },
                {
                    "sent": "Well, then, what do I get?",
                    "label": 0
                },
                {
                    "sent": "I get the regression estimator.",
                    "label": 0
                },
                {
                    "sent": "They are not estimating the mean and possibly the variance.",
                    "label": 0
                },
                {
                    "sent": "Conditionally, that gives you regression.",
                    "label": 0
                },
                {
                    "sent": "It actually will give you heteroskedastic regression estimator, which is something really nice.",
                    "label": 0
                },
                {
                    "sent": "But it's very very simple.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put this to use.",
                    "label": 0
                },
                {
                    "sent": "So the Hammersley Clifford theorem.",
                    "label": 1
                },
                {
                    "sent": "Think about another 15 minutes or two.",
                    "label": 0
                },
                {
                    "sent": "Not how much time do you think are probably another 15.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So this theorem says.",
                    "label": 0
                },
                {
                    "sent": "That if I have some undirected graphical model with the conditional independence properties, then the joint distribution P of X. Decomposes into sums on the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "So This is why, for instance, in the weather example I was able to actually model the joint distribution of us all carrying umbrellas around in such a simple way 'cause I only had to model the connection between.",
                    "label": 0
                },
                {
                    "sent": "The cloud and my carrying the umbrella or somebody else carrying the umbrella.",
                    "label": 0
                },
                {
                    "sent": "So these functions here can depend on considerably fewer variables than the joint distribution here.",
                    "label": 0
                },
                {
                    "sent": "Obviously we still have this big problem here, namely the normalization that I need to take care of, but other than that, this is all really nice.",
                    "label": 0
                },
                {
                    "sent": "OK, a small catch.",
                    "label": 0
                },
                {
                    "sent": "PFX needs to be non zero on the entire domain.",
                    "label": 1
                },
                {
                    "sent": "So if you have some serious in the domain, you just need to exclude it from the domain in the 1st place and everything works out fine again.",
                    "label": 0
                },
                {
                    "sent": "But it's completely clear because mean each other something can never become zero.",
                    "label": 0
                },
                {
                    "sent": "It can be arbitrary close this year, but never 0.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we then use this, how much the 5th theorem for the exponential families?",
                    "label": 0
                },
                {
                    "sent": "So far you would think well he's been talking about conditional models.",
                    "label": 0
                },
                {
                    "sent": "Graphical models an exponential family models.",
                    "label": 0
                },
                {
                    "sent": "How do they relate?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, in this case these two are really nicely connected, because what you will get is that the sufficient statistics for fix will just decompose into sufficient statistics on the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Doesn't sound terribly exciting.",
                    "label": 0
                },
                {
                    "sent": "Now let's take the inner product between those terms.",
                    "label": 0
                },
                {
                    "sent": "Make it something that will decompose into inner products between functions that are defined on the maximal cliques only.",
                    "label": 0
                },
                {
                    "sent": "If you remember something, let's talk.",
                    "label": 0
                },
                {
                    "sent": "This was a Colonel.",
                    "label": 0
                },
                {
                    "sent": "So what happens is the kernel.",
                    "label": 0
                },
                {
                    "sent": "On a big set of random variables.",
                    "label": 0
                },
                {
                    "sent": "Decomposes into kernels on the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "If you have certain conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "So if you have a big and fancy and complicated.",
                    "label": 0
                },
                {
                    "sent": "Dependency structure.",
                    "label": 0
                },
                {
                    "sent": "You just look at the maximal cliques and you know what your kernel needs to decompose like.",
                    "label": 0
                },
                {
                    "sent": "This is also something that you can actually actually then use for feature selection and click determination.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we prove this?",
                    "label": 0
                },
                {
                    "sent": "So basically just matching up terms we say no.",
                    "label": 0
                },
                {
                    "sent": "This expression here has to look like that.",
                    "label": 0
                },
                {
                    "sent": "Despite taking logs and matching up terms now, this initially depends on all the random variables at once, and these are functions just in the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "This has to hold for all distributions.",
                    "label": 0
                },
                {
                    "sent": "That's just the constant, so that's easy.",
                    "label": 0
                },
                {
                    "sent": "So we just decompose.",
                    "label": 0
                },
                {
                    "sent": "Say to in terms of and also normal basis.",
                    "label": 0
                },
                {
                    "sent": "Can we just take care of those synergy?",
                    "label": 0
                },
                {
                    "sent": "Well, then, this expression here.",
                    "label": 0
                },
                {
                    "sent": "Can be written as such.",
                    "label": 0
                },
                {
                    "sent": "Cause this thing times the corresponding coefficients theater I has to match the right hand side and this only works if you have individual terms which depend on the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "This has to hold for all size and Theta.",
                    "label": 0
                },
                {
                    "sent": "Then you just collect all those terms depending on the maximal cliques together and you define those as just feature Maps on the maximal cliques themselves.",
                    "label": 0
                },
                {
                    "sent": "Now we can get this in a product.",
                    "label": 0
                },
                {
                    "sent": "That's really just straightforward linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Bit tedious, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not deep.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's take an example normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is actually something that.",
                    "label": 0
                },
                {
                    "sent": "Martin already had on his slides.",
                    "label": 0
                },
                {
                    "sent": "We didn't discuss that we would show the same example, but we'll just see it would probably slightly different numbers, but other than that it's the same.",
                    "label": 0
                },
                {
                    "sent": "So remember that for normal distribution our file fix was just X and XX transposed OK.",
                    "label": 0
                },
                {
                    "sent": "The only thing that I've kind of snuck past two is now all of a sudden those axes are vector valued.",
                    "label": 0
                },
                {
                    "sent": "Everybody knows how to write the vector valued normal distribution, so we would also see that, well, you have to fix transpose inverse covariance matrix X term for normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we know that our far fix must decompose into the subsets involving only variables from each maximal clique.",
                    "label": 1
                },
                {
                    "sent": "So I know that I have normal distribution with certain conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this is called a Gaussian Markov random field, so if you see somebody writing this, you know this is code for saying something very simple.",
                    "label": 0
                },
                {
                    "sent": "Namely, you have a normal distribution and you have conditional independence properties.",
                    "label": 1
                },
                {
                    "sent": "Now for the linear term.",
                    "label": 0
                },
                {
                    "sent": "Well that's just fine anyway.",
                    "label": 1
                },
                {
                    "sent": "That decomposes very nicely, so the only really coupling terms are those XI extra terms.",
                    "label": 1
                },
                {
                    "sent": "They correspond to an edge in this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, now we know that for a normal distribution we have X transposed inverse covariance matrix X.",
                    "label": 1
                },
                {
                    "sent": "We must not have any coupling terms between vertices that don't have any edge.",
                    "label": 0
                },
                {
                    "sent": "What this means is that the inverse covariance matrix has to be sparse.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a look.",
                    "label": 0
                },
                {
                    "sent": "Stop, this is my normal distribution.",
                    "label": 0
                },
                {
                    "sent": "I have some coefficients data here.",
                    "label": 0
                },
                {
                    "sent": "And this must be 0 whenever there's no edge.",
                    "label": 0
                },
                {
                    "sent": "So then I can just write theater to SD universe covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And then, well, let's just read it off.",
                    "label": 0
                },
                {
                    "sent": "So there's an age between one and two.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can have those terms.",
                    "label": 0
                },
                {
                    "sent": "And actually I made a mistake here.",
                    "label": 0
                },
                {
                    "sent": "Because this should be 2 and there should be one.",
                    "label": 0
                },
                {
                    "sent": "So swap those two numbers here.",
                    "label": 0
                },
                {
                    "sent": "OK. And there's an age between the two and three.",
                    "label": 0
                },
                {
                    "sent": "And then 3, four and five are fully connected.",
                    "label": 0
                },
                {
                    "sent": "So remember, sometimes you would actually write your graph through this adjacency matrix, and I've told you in addition to all that, that yes, this actually makes a lot of sense, because this really corresponds to Gaussian Markov random field where.",
                    "label": 0
                },
                {
                    "sent": "Unsera means that they have some dependency.",
                    "label": 0
                },
                {
                    "sent": "So where solved large sparse linear systems before?",
                    "label": 0
                },
                {
                    "sent": "OK, OK, well at least there are three four people.",
                    "label": 0
                },
                {
                    "sent": "Now the hint is if you do that, it's probably really good idea if you don't use just the standard Matlab routine for solving a linear system, but you use a specific sparse linear solver.",
                    "label": 0
                },
                {
                    "sent": "Because this sparse linear solvers are actually much faster because he exploiting the fact that the linear system has lots of zeros.",
                    "label": 0
                },
                {
                    "sent": "The way those systems proceed is part illuminating one variable after the other.",
                    "label": 0
                },
                {
                    "sent": "And this, as it so happens, is exactly the same procedure as the message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we'll get to that a little bit.",
                    "label": 0
                },
                {
                    "sent": "So the point here is well.",
                    "label": 0
                },
                {
                    "sent": "The inference procedures that we've talked about in graphical models last week we just eliminate one variable after the other.",
                    "label": 1
                },
                {
                    "sent": "We wanted to ensure that it doesn't couple with too much else.",
                    "label": 0
                },
                {
                    "sent": "That is the same problem that you will get if you solve a linear system, because if you remove the wrong variable, all the other variables that it depends on will couple and then you will get a really dense system.",
                    "label": 0
                },
                {
                    "sent": "And it takes a long time to solve.",
                    "label": 0
                },
                {
                    "sent": "And you can use the very same techniques for finding a good elimination order on the graphical model.",
                    "label": 0
                },
                {
                    "sent": "And on the sparse linear system.",
                    "label": 0
                },
                {
                    "sent": "OK, this is here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Let's move on.",
                    "label": 0
                },
                {
                    "sent": "So let's see how we do this, actually.",
                    "label": 0
                },
                {
                    "sent": "Just really, really simple Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So GF data and remember I had to have the log of the sum over my entire domain and now here in the Markov chain I have expressions which depend only on adjacent random variables.",
                    "label": 0
                },
                {
                    "sent": "So far fixed EXT plus one in some data.",
                    "label": 0
                },
                {
                    "sent": "So this is some imov XD, XD plus one and half the summer.",
                    "label": 0
                },
                {
                    "sent": "All the Explorer XT of the product over all those empty of XD XD plus one.",
                    "label": 0
                },
                {
                    "sent": "OK. And how do I solve this?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at it well, you can expand it all out.",
                    "label": 0
                },
                {
                    "sent": "I just pushed the summation in so this is like an antenna that I'm pulling out and I'm just pushing the summations in as far as they go.",
                    "label": 0
                },
                {
                    "sent": "And well, obviously I cannot push it beyond that variable.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the X3 summation can only go as far as that, because that's the first time where I'm hitting an X3.",
                    "label": 0
                },
                {
                    "sent": "Next day I can push as far right as possible up to the last term.",
                    "label": 0
                },
                {
                    "sent": "Now this only depends on X, T -- 1 XT.",
                    "label": 0
                },
                {
                    "sent": "That's great, so I can just sum this out.",
                    "label": 0
                },
                {
                    "sent": "So I will get some number here, which some function that it will only depend on XT minus one.",
                    "label": 0
                },
                {
                    "sent": "They will have the summation over XT minus one here.",
                    "label": 0
                },
                {
                    "sent": "Of some coupling term between T -- 20 -- 1 and this number here so I can do that summation, I can do the next one and the next one and the next one.",
                    "label": 0
                },
                {
                    "sent": "My wife is good because it means that I can do this summation over well, like exponentially many numbers.",
                    "label": 0
                },
                {
                    "sent": "In linear time.",
                    "label": 0
                },
                {
                    "sent": "So if you've heard of the Viterbi, the forward backward algorithm or some product algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or shortest path them control programming that are all the same.",
                    "label": 0
                },
                {
                    "sent": "It's just that some operate with the Max some operate with.",
                    "label": 0
                },
                {
                    "sent": "Product that's wait, I mean, algebraically speaking, it's what's called a different simmering.",
                    "label": 0
                },
                {
                    "sent": "So there's one that's called the Max plus semiring.",
                    "label": 0
                },
                {
                    "sent": "This one that's called the standard plus time simmering.",
                    "label": 0
                },
                {
                    "sent": "And then there's the lock plus and plus immunity.",
                    "label": 0
                },
                {
                    "sent": "And depending on which one you pick.",
                    "label": 0
                },
                {
                    "sent": "The equations might look a bit different, but the procedure is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just that basically all you do is you redefine what those sum and product operations do.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is actually how you going implemented, so you implement some objects and that makes it like something or product.",
                    "label": 0
                },
                {
                    "sent": "And then you just redefine those and you get the different algorithm.",
                    "label": 0
                },
                {
                    "sent": "Makes it very flexible.",
                    "label": 0
                },
                {
                    "sent": "It's maybe not the fastest way of doing it, but very efficient.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the reason why this all worked was because we could.",
                    "label": 0
                },
                {
                    "sent": "Push the summations in.",
                    "label": 0
                },
                {
                    "sent": "So this is what we exploited the distributive law.",
                    "label": 0
                },
                {
                    "sent": "There's a paper called the Generalized Distributive Law.",
                    "label": 0
                },
                {
                    "sent": "This covers that in lots of details, lots more than I can cover here.",
                    "label": 0
                },
                {
                    "sent": "It's a great paper.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now let's have a look at it again.",
                    "label": 0
                },
                {
                    "sent": "So what we do is, well, we can do this summation from from the Indian.",
                    "label": 0
                },
                {
                    "sent": "That's what I explained, OK?",
                    "label": 0
                },
                {
                    "sent": "But it could equally well also start something from the front.",
                    "label": 0
                },
                {
                    "sent": "It's like we do a forward and backward pass and we can store all those intermediate values.",
                    "label": 1
                },
                {
                    "sent": "Well, why would this be useful?",
                    "label": 0
                },
                {
                    "sent": "Let's say I want to compute the probability that this red node takes on a value of one or zero.",
                    "label": 1
                },
                {
                    "sent": "But what I have to do is have to sum over all the random variables here in sum over all the random variables here.",
                    "label": 1
                },
                {
                    "sent": "But this is exactly what my forward pass does.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what the backward pass stars.",
                    "label": 0
                },
                {
                    "sent": "So I can compute this probability of the random variable here.",
                    "label": 0
                },
                {
                    "sent": "Right integrated out all the green ones very easily by just running the forward pass up to the right variable.",
                    "label": 1
                },
                {
                    "sent": "Running the backwards pass up to this red variable and I'm done.",
                    "label": 1
                },
                {
                    "sent": "OK. Now let's assume somebody tells you what you want to get those probabilities for all those nodes.",
                    "label": 0
                },
                {
                    "sent": "Nothing easier than that.",
                    "label": 0
                },
                {
                    "sent": "We just run the forward pass through all the intermediate values, run the backwards past or all the intermediate values and then I can get the rest very easily.",
                    "label": 0
                },
                {
                    "sent": "So this is the simplest case of message passing.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now imagine we've got a tree.",
                    "label": 0
                },
                {
                    "sent": "I want to get the probability that this variable is 01.",
                    "label": 0
                },
                {
                    "sent": "So after some over all those variables after some over all those variables and have to sum over all these.",
                    "label": 0
                },
                {
                    "sent": "And you get all those three incoming messages, and you can compute that probability.",
                    "label": 0
                },
                {
                    "sent": "Now how do you get the message that you can send on?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just some over this.",
                    "label": 0
                },
                {
                    "sent": "Some of that take the product.",
                    "label": 0
                },
                {
                    "sent": "Some of this variable here and you've got the next message going out.",
                    "label": 0
                },
                {
                    "sent": "So this is really the very very simple communications protocol for message passing algorithm.",
                    "label": 1
                },
                {
                    "sent": "Receive messages from everyone but the one where you want to send a message to.",
                    "label": 0
                },
                {
                    "sent": "Remove all the dependencies on the other incoming nodes.",
                    "label": 0
                },
                {
                    "sent": "And send it on.",
                    "label": 0
                },
                {
                    "sent": "OK, and the cool thing is you can run this in parallel, so each of those things can be a little computer, little sensor network node.",
                    "label": 0
                },
                {
                    "sent": "And then, well, you send a message here.",
                    "label": 0
                },
                {
                    "sent": "You send one layer once one there you do that on all the nodes.",
                    "label": 0
                },
                {
                    "sent": "In the end, you have all the messages that you need to get probabilities.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Really nice and simple idea.",
                    "label": 0
                },
                {
                    "sent": "Any questions here?",
                    "label": 0
                },
                {
                    "sent": "Who can tell me if I run this in parallel?",
                    "label": 0
                },
                {
                    "sent": "It is every node does this at the same time, how long it will take.",
                    "label": 0
                },
                {
                    "sent": "Until I get the correct message here.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you one trick with initialization if you haven't got the right message from the other node yet, you just invent something.",
                    "label": 0
                },
                {
                    "sent": "OK. And assume that this is the.",
                    "label": 0
                },
                {
                    "sent": "This would be the right message, OK?",
                    "label": 0
                },
                {
                    "sent": "Now, how long would it take to get the correct probabilities here?",
                    "label": 0
                },
                {
                    "sent": "Anyone TS?",
                    "label": 0
                },
                {
                    "sent": "Let's think about it for a moment.",
                    "label": 0
                },
                {
                    "sent": "See in order for this message here to be correct.",
                    "label": 0
                },
                {
                    "sent": "This node needs to send a message here.",
                    "label": 0
                },
                {
                    "sent": "That now needs to do something and send the message one.",
                    "label": 0
                },
                {
                    "sent": "So in order to get that message correct, I need 2 steps.",
                    "label": 0
                },
                {
                    "sent": "12 with this one I also need 2 steps.",
                    "label": 0
                },
                {
                    "sent": "1, two in this case.",
                    "label": 0
                },
                {
                    "sent": "Here I need three steps 123.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So after three iterations of this parallel algorithm, Now this message here is.",
                    "label": 0
                },
                {
                    "sent": "I can compute this probability exactly.",
                    "label": 0
                },
                {
                    "sent": "How long would it take for this note?",
                    "label": 0
                },
                {
                    "sent": "Any suggestions?",
                    "label": 0
                },
                {
                    "sent": "Well, let's count.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "234 what?",
                    "label": 0
                },
                {
                    "sent": "I've stepped.",
                    "label": 0
                },
                {
                    "sent": "So it's really easy.",
                    "label": 0
                },
                {
                    "sent": "You look at the graph, just count what the longest path is and that's what it takes until everything is converged.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So really easy algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Randomly, Yep.",
                    "label": 0
                },
                {
                    "sent": "Where is that?",
                    "label": 0
                },
                {
                    "sent": "Well because basically.",
                    "label": 0
                },
                {
                    "sent": "All the messages until you get the right one.",
                    "label": 0
                },
                {
                    "sent": "I mean, they don't really matter.",
                    "label": 0
                },
                {
                    "sent": "I mean, you might as well not send any correct message in that case.",
                    "label": 0
                },
                {
                    "sent": "So if you were to implement that on a single node.",
                    "label": 0
                },
                {
                    "sent": "You would not implement it in parallel.",
                    "label": 0
                },
                {
                    "sent": "What you would do then is the following.",
                    "label": 0
                },
                {
                    "sent": "I would first so in that case I would just first look at well where can I actually already send a message to and it would start at the leaves of this tree.",
                    "label": 0
                },
                {
                    "sent": "OK so I can send all the messages from the leaves can do this one that one that one that one that one that one that one OK. Whatever the other notes do during that time doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "Now all the sudden this node realizes up.",
                    "label": 0
                },
                {
                    "sent": "I've got the message from here message from there so I can at least send a message on here.",
                    "label": 0
                },
                {
                    "sent": "OK, so they forward this.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Well, this note here realizes I've received some incoming messages from here and there.",
                    "label": 0
                },
                {
                    "sent": "I can forward it there, and likewise the right node.",
                    "label": 0
                },
                {
                    "sent": "That's the next step that we will be doing.",
                    "label": 0
                },
                {
                    "sent": "All the other nodes cannot really do anything because they haven't received any other message back.",
                    "label": 0
                },
                {
                    "sent": "But now things start getting interesting.",
                    "label": 0
                },
                {
                    "sent": "Now this note here.",
                    "label": 0
                },
                {
                    "sent": "OK, it's received a message from here to receive the message from there so it can send a message up there.",
                    "label": 0
                },
                {
                    "sent": "It can also send a message down here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So by that time already this written out in this green node will or will know exactly what their probabilities are.",
                    "label": 0
                },
                {
                    "sent": "Now, once this green note, he has received its incoming message.",
                    "label": 0
                },
                {
                    "sent": "It rises OK. Got the message from here from there so I can send it on there.",
                    "label": 0
                },
                {
                    "sent": "Likewise I put the message from here and from here I can send it up there.",
                    "label": 1
                },
                {
                    "sent": "So this is how you would implement it sequentially.",
                    "label": 0
                },
                {
                    "sent": "This is considerably less wasteful, but if your other nodes do something.",
                    "label": 0
                },
                {
                    "sent": "Stupid.",
                    "label": 0
                },
                {
                    "sent": "Well in the meantime, well OK, there's no harm.",
                    "label": 0
                },
                {
                    "sent": "So I guess my question is why?",
                    "label": 0
                },
                {
                    "sent": "Why not wait until you get incoming message?",
                    "label": 0
                },
                {
                    "sent": "OK, if you have a single computer on which you implement all this, you're absolutely right.",
                    "label": 0
                },
                {
                    "sent": "What I suggested is stupid, but if you have, say, a really big graphical model and each of those nodes is a computer.",
                    "label": 0
                },
                {
                    "sent": "You might not even know how big the network is.",
                    "label": 0
                },
                {
                    "sent": "And so it's a good idea just to get started running.",
                    "label": 0
                },
                {
                    "sent": "The other thing is, and this is quite a curious phenomenon, sometimes really far away nodes may not have much of an effect on what you're going to do locally.",
                    "label": 0
                },
                {
                    "sent": "So even though you started with the wrong message eventually, as you pass it through lots and lots and lots and lots of nodes which already do something to do the right thing, they'll make a wrong thing, right?",
                    "label": 0
                },
                {
                    "sent": "Motors, a point.",
                    "label": 0
                },
                {
                    "sent": "If you have a network of computers, some of them might go down sometime might might be running.",
                    "label": 0
                },
                {
                    "sent": "And you might not really know beforehand which ones are up and which ones are down.",
                    "label": 0
                },
                {
                    "sent": "Obviously you wouldn't want to just send a random message along if you don't have any other don't have an incoming message.",
                    "label": 0
                },
                {
                    "sent": "So a really good idea is to take some prior.",
                    "label": 0
                },
                {
                    "sent": "And then send the right message that looks most similar to what you would have received.",
                    "label": 0
                },
                {
                    "sent": "Two neighbors so there's a very nice paper by colors, question and Mark Paskin from UI 2005, and they do exactly that, and they show that very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "You can get conversions in this way if you have a nicely factorizing prior.",
                    "label": 0
                },
                {
                    "sent": "And in their case they actually have the real scenario of taking since the network nodes and they have to talk to each other and sometimes they break or sometimes the connection doesn't work and it's a very beautiful paper which does exactly that.",
                    "label": 0
                },
                {
                    "sent": "The thing is, if you can run the algorithm independent in each node, it's just so much easier to code up.",
                    "label": 0
                },
                {
                    "sent": "So you're winning both in terms of statistics and in terms of implementation.",
                    "label": 0
                },
                {
                    "sent": "Which is a great situation.",
                    "label": 0
                },
                {
                    "sent": "Any further questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so of course I've only told you the beautiful part of the story.",
                    "label": 0
                },
                {
                    "sent": "What happens if?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is these are still the equations, but afterwards you might wonder what happens if I don't have a tree but some arbitrary graph.",
                    "label": 0
                },
                {
                    "sent": "So let's assume I have some clique potentials in my SIX Ray now might as well initialize all those messages.",
                    "label": 0
                },
                {
                    "sent": "My JFX jakkals one.",
                    "label": 0
                },
                {
                    "sent": "And then I update the outgoing messages.",
                    "label": 0
                },
                {
                    "sent": "This way, so that's just all the incoming ones, plus the connection and I sum over all the other nodes.",
                    "label": 0
                },
                {
                    "sent": "That gives me the message going from I to J.",
                    "label": 0
                },
                {
                    "sent": "This is really just an equations with a explaining pictures before you can easily read arrived at.",
                    "label": 0
                },
                {
                    "sent": "And well, this message passing algorithm converges after in iterations.",
                    "label": 1
                },
                {
                    "sent": "That's what we've proved before.",
                    "label": 1
                },
                {
                    "sent": "Now the hack is just use that for graphs with loops and you help.",
                    "label": 0
                },
                {
                    "sent": "So this is what Martin was talking about as loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "And he actually gave pretty good reasons why this sometimes might be a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have some graph like this one here.",
                    "label": 0
                },
                {
                    "sent": "It's only a nice graph, is nicely triangulated.",
                    "label": 0
                },
                {
                    "sent": "Then I can set something up that's called a junction tree.",
                    "label": 0
                },
                {
                    "sent": "So what I do is I write out the maximal cliques that's a maximum click 123 and there's 1235.",
                    "label": 0
                },
                {
                    "sent": "There's 1567 and that's 1354.",
                    "label": 0
                },
                {
                    "sent": "OK, now just write them out and then I write which nodes connect those clicks.",
                    "label": 0
                },
                {
                    "sent": "So the intersection between these two are the other vertices two and three.",
                    "label": 0
                },
                {
                    "sent": "His note is the Vertex wife, and here's the vertex five and three.",
                    "label": 0
                },
                {
                    "sent": "And this graph has what's called the running intersection property.",
                    "label": 0
                },
                {
                    "sent": "And this means that, well, if, say, the number 3 is shared between this and that.",
                    "label": 0
                },
                {
                    "sent": "Click, then it has to be shared all along the path, which is exactly true.",
                    "label": 0
                },
                {
                    "sent": "Likewise, the Fife is shared between these two paths, and that's also true.",
                    "label": 0
                },
                {
                    "sent": "So what that means is if I have some dependency that involves this node 5.",
                    "label": 0
                },
                {
                    "sent": "And I'm dealing with it down here and I also have to deal with it up here.",
                    "label": 0
                },
                {
                    "sent": "Then these two effects need to be connected.",
                    "label": 0
                },
                {
                    "sent": "OK. And then all I do is I just do my message passing now on the junction tree.",
                    "label": 1
                },
                {
                    "sent": "And the key difference is that before that I only had to deal maybe with the states of a single variable.",
                    "label": 0
                },
                {
                    "sent": "So for binary variable they only needed to send on 2 numbers.",
                    "label": 0
                },
                {
                    "sent": "Cheap now if.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I have two bonded variables already, will have to send on 4 numbers.",
                    "label": 0
                },
                {
                    "sent": "Say 510 value variables.",
                    "label": 0
                },
                {
                    "sent": "How many numbers do I have to send along?",
                    "label": 0
                },
                {
                    "sent": "Any volunteers?",
                    "label": 0
                },
                {
                    "sent": "Well, two to the 10 is 1024 so I have to send along a lot of numbers.",
                    "label": 0
                },
                {
                    "sent": "So this thing grows exponentially with the size of the separating set.",
                    "label": 0
                },
                {
                    "sent": "So that's really bad.",
                    "label": 0
                },
                {
                    "sent": "If you do this, track.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station and you end up with a very thick junction tree as it's called.",
                    "label": 0
                },
                {
                    "sent": "Then the algorithm will run very slowly and will need a lot of memory and all that, so you might not want to do this.",
                    "label": 0
                },
                {
                    "sent": "So these are exactly the cases where what Martin suggested might be more efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, now these are the same equations as what I've shown you before for the trees.",
                    "label": 0
                },
                {
                    "sent": "Let's just go back to slides.",
                    "label": 0
                },
                {
                    "sent": "See, these are the messages for the tree.",
                    "label": 0
                },
                {
                    "sent": "And these are the messages for the junction tree.",
                    "label": 1
                },
                {
                    "sent": "So the key difference is now while those accesses are just, well, not not just a single line available anymore, but they're just whatever is in the separators it.",
                    "label": 0
                },
                {
                    "sent": "So we just have fancier quantities.",
                    "label": 0
                },
                {
                    "sent": "But the algorithm is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we just replaced our tree with the hypertree.",
                    "label": 0
                },
                {
                    "sent": "This is like you know you have a network of roads and at some point the traffic gets too large and you will add a second or third or fourth line.",
                    "label": 0
                },
                {
                    "sent": "And this is really cool end of it.",
                    "label": 0
                },
                {
                    "sent": "So it's just that you have to carry several variables along.",
                    "label": 0
                },
                {
                    "sent": "And that's what's called a hyper tree or hyper graph.",
                    "label": 0
                },
                {
                    "sent": "Just like rather roads having highways.",
                    "label": 0
                },
                {
                    "sent": "Then again, you can use that for graphs with loops and you hope.",
                    "label": 1
                },
                {
                    "sent": "And there are nice strategies how you can do something in between like you.",
                    "label": 1
                },
                {
                    "sent": "Well, make a slightly fancier message passing algorithm and this is what Martin was referring to as the work by Edie.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. So again his example.",
                    "label": 0
                },
                {
                    "sent": "So in order to do something here to send this message along, I need to get the message.",
                    "label": 0
                },
                {
                    "sent": "Which depends on Y2 and Y-3.",
                    "label": 0
                },
                {
                    "sent": "Right, because these are the variables that really matter here.",
                    "label": 0
                },
                {
                    "sent": "Here I need to send a message that depends on what variables Wi-Fi from Y-3.",
                    "label": 0
                },
                {
                    "sent": "Now I have those two incoming.",
                    "label": 0
                },
                {
                    "sent": "And this is what I have to send up.",
                    "label": 0
                },
                {
                    "sent": "So that means I can integrate out of the variables two and three, and this is the only message that I need to send an send this on, then do something else here.",
                    "label": 0
                },
                {
                    "sent": "They do that at every node.",
                    "label": 0
                },
                {
                    "sent": "So it's really the same strategies with the trees, just it now.",
                    "label": 0
                },
                {
                    "sent": "The variables are a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, where in the homework stretch?",
                    "label": 1
                },
                {
                    "sent": "So I already explained this algorithm really can go bad with the treewidth.",
                    "label": 0
                },
                {
                    "sent": "So if you have a really really fat junction tree.",
                    "label": 0
                },
                {
                    "sent": "Then I have to send a lot of messages along.",
                    "label": 0
                },
                {
                    "sent": "It takes a long time algorithm run very slowly and they might run out of memory.",
                    "label": 1
                },
                {
                    "sent": "I can just use loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "There's a very nice paper by Alex Vela, and they have essentially a convergence monitor.",
                    "label": 0
                },
                {
                    "sent": "Is this loopy belief propagation might or might not converge to the right thing, So what you do is you.",
                    "label": 0
                },
                {
                    "sent": "Run like a control device, something that tells you how close you are from a fixed point so they have a nice upper bound for that.",
                    "label": 0
                },
                {
                    "sent": "You can check.",
                    "label": 0
                },
                {
                    "sent": "Whether you're too far away or not, can you run this along and then after the fact that you've run the algorithm, this device will tell you well whether you have reasonable hope to believe that your solution is correct or not.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "Full you could also average or very spanning trees.",
                    "label": 0
                },
                {
                    "sent": "This is what Martin referred to.",
                    "label": 0
                },
                {
                    "sent": "You can use sampling methods, so there's a very nice paper by Nana Diffracts in coworkers.",
                    "label": 1
                },
                {
                    "sent": "He basically conditions on subsets of the tree samples from the rest and iterates efficiently.",
                    "label": 0
                },
                {
                    "sent": "And that works fairly nicely.",
                    "label": 0
                },
                {
                    "sent": "It's also pretty easy to implement.",
                    "label": 0
                },
                {
                    "sent": "Or you can use semidefinite relaxations.",
                    "label": 1
                },
                {
                    "sent": "And this is a very, very active area of research.",
                    "label": 0
                },
                {
                    "sent": "How to work around the issue of those intractable graphical models.",
                    "label": 0
                },
                {
                    "sent": "It's a very fertile area because I mean you can show that the overall problem, even though it looks like a nice convex thing, is actually NP hard.",
                    "label": 0
                },
                {
                    "sent": "And so you want to find good approximations and you want to characterize in which situations you can do something sensible.",
                    "label": 0
                },
                {
                    "sent": "And if you go to UI, for instance, you will find lots of papers on that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just sum up.",
                    "label": 0
                },
                {
                    "sent": "And then we definitely did deserve a good break.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, I think I've got 15 minutes overtime.",
                    "label": 0
                },
                {
                    "sent": "So let's just recall the overall optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood was take this lock partition function.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "There's sufficient statistics and you just sit that derivative to 0.",
                    "label": 0
                },
                {
                    "sent": "This is what you get.",
                    "label": 0
                },
                {
                    "sent": "In other words, the expectation has to match the programming.",
                    "label": 0
                },
                {
                    "sent": "The same thing holds for my graphical models.",
                    "label": 0
                },
                {
                    "sent": "It's just that my file fixes may look a little bit more complicated, and in order to compute the expectations, have to do with more work, but it's the same equation.",
                    "label": 0
                },
                {
                    "sent": "Then if I want to regularize, I might just either say, well, I just want my theaters to be small.",
                    "label": 0
                },
                {
                    "sent": "I get this problem.",
                    "label": 0
                },
                {
                    "sent": "Hello can you?",
                    "label": 0
                },
                {
                    "sent": "This message will be your friend.",
                    "label": 0
                },
                {
                    "sent": "Or you can say, well, I have some conjugate prior.",
                    "label": 0
                },
                {
                    "sent": "In other words, I dream up some data.",
                    "label": 0
                },
                {
                    "sent": "And I saw for that.",
                    "label": 0
                },
                {
                    "sent": "And any of those two methods will give you.",
                    "label": 0
                },
                {
                    "sent": "Slightly better behaved estimates.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so summing up.",
                    "label": 0
                },
                {
                    "sent": "What we did is we look at the hammer hammer Clifford theorem.",
                    "label": 1
                },
                {
                    "sent": "Reviewed in conditional independence message passing.",
                    "label": 1
                },
                {
                    "sent": "And in the end I talked a little bit about approximate inference and loopy belief propagation.",
                    "label": 1
                },
                {
                    "sent": "And I guess the take home message for you is, well, just try it out.",
                    "label": 0
                },
                {
                    "sent": "See what the loopy BP does.",
                    "label": 0
                },
                {
                    "sent": "Anything useful at all?",
                    "label": 0
                },
                {
                    "sent": "If it does, you've solved the problem.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, then you should start worrying about better solutions.",
                    "label": 0
                },
                {
                    "sent": "But there's a fair chance that if you have, problem is fairly simple, you will already get a decent answer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's what we did.",
                    "label": 0
                },
                {
                    "sent": "Exponential family.",
                    "label": 0
                },
                {
                    "sent": "Bit of statistical inference.",
                    "label": 0
                },
                {
                    "sent": "Base in middle models and in the end we did some graphical models.",
                    "label": 0
                },
                {
                    "sent": "So I think now we all deserve a break, but if you have any questions, please ask Now or during the break.",
                    "label": 0
                },
                {
                    "sent": "Any questions now everybody really tired.",
                    "label": 0
                },
                {
                    "sent": "OK, let's have a break then.",
                    "label": 0
                }
            ]
        }
    }
}