{
    "id": "y5vwziymbo3vpfgfsnn755qz7hdedi7f",
    "title": "Generalization Bounds for Indefinite Kernel Machines",
    "info": {
        "author": [
            "Ali Rahimi, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT",
            "Nathan Srebro, Toyota Technological Institute at Chicago"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/wehys08_rahimi_gbikm/",
    "segmentation": [
        [
            "So this is joint work with a lot of people, and that in itself is going to be an interesting part of the talk, maybe even more interesting than that."
        ],
        [
            "The results themselves, so I joined Intel three years ago and Intel mostly builds hardware and software.",
            "You don't?",
            "They don't really make money off of theorems, but I'm going to tell you how this how this work came out of some software problem.",
            "My inspiration was to build a system where you point your cell phone at whatever object is in your life and get a web page associated with it.",
            "And modify that web page.",
            "You could basically turn the whole world into a Wikipedia index so you know you would point out an object and it would say, oh, this is a printer.",
            "And here's all this information about this printer."
        ],
        [
            "Let's get on your cell phone.",
            "That was my dream.",
            "And we built this system.",
            "Send together phone number right.",
            "The way it actually works if you point it to the person right now it says person.",
            "Like the product, really like you, you can take a photo and Amazon will tell your face.",
            "Tell something similar.",
            "Yeah it works with planar objects.",
            "As long as the object that they sell is planar, it works really well.",
            "If it's got a curvature, they either don't sell it or they don't recognize it.",
            "So I'll tell you a little bit about how this system works.",
            "We basically spent a couple of years.",
            "So going through the computer vision literature, picking out components, putting them together, and generally all of these object recognition systems works something like this.",
            "An image comes in.",
            "You have a training data set of images that you required a few 100,000 images or something like that.",
            "Each of them has a label and you take your input image that you'd like to classify and compare it in some way against your training images.",
            "These are similarity measurements between the incoming image and the training images, and then you somehow based on these similarity measures produce a classification.",
            "So the way we do things is."
        ],
        [
            "What we settled on is a measure of similarity between images that involves looking at local descriptors.",
            "In these images just looking at edges and finding correspondences between edges and a training image and a test image an once we find these correspondences somehow induces a similarity between the images.",
            "There are many ways to do this, but the ones that work well on images will typically do something like this.",
            "This is where the problem starts."
        ],
        [
            "Generally, the similarity measures that work really well don't have any nice mathematical properties at all.",
            "They're certainly not positive, definitely, so if you were hoping of so.",
            "If you were hoping to use an SVM.",
            "Not going to work, they don't satisfy triangle inequality.",
            "The images you typically won't represent them as vectors in RN.",
            "They typically end up being represented in some structured form of bags of local descriptors, and oftentimes K is not even a differentiable function.",
            "Even if it were.",
            "A similarity measure on RN.",
            "So what happens the way the computer vision Community deals with with this lack of structure is basically the classifier that they use is nearest neighbors.",
            "Or K nearest neighbors and we know from other domains that K nearest neighbors is not that accurate, so we'd like to do something better."
        ],
        [
            "Let me remind you what would happen if if K were positive definite.",
            "You could use an SVM.",
            "The decision boundary you learn is a weighted sum of kernels placed on your training data.",
            "And you would train it by minimizing the empirical loss subject to some constraints on the coefficients."
        ],
        [
            "And we have a lot of results about how this about the fact that this generalizes well.",
            "If you score well on your empirical data on your on your training set, then then the probability that you'll score well on that, you'll have a true low true risk is going to be quite low, so we know how to bound this.",
            "And generally the way we bound this is that we write down the hypothesis space weighted sums of kernels on the training data.",
            "In a form that doesn't actually depends on the training data you write it down as as a linear hypothesis space where you feature eyes, your inputs using the eigen functions of the kernel and now really what you're dealing with is a hypothesis space that in no way depends on on the training data set at all.",
            "So all of the tricks that we use to bound this relying quite heavily on the fact that this class of functions does not depend on the training data.",
            "Not able to give you an example of when this breaks."
        ],
        [
            "So what can you do in case is not positive definite?",
            "So it turns out empirically that this type of thing works quite well.",
            "Weighted sums of kernels, even if the kernels aren't positive definite, work amazingly well.",
            "They'll work better than nearest neighbors.",
            "How do you train him?",
            "Well, you can again minimize an empirical risk and put some kind of a constraint on the coefficients.",
            "And we could argue about what kinds of coefficients that put on them.",
            "This is not the only way to train this.",
            "Various people have proposed, for example, dealing with the fact that K is not positive definite by somehow taking the positive that the kernel matrix an making it positive definite by either shifting its spectrum or clipping its spectrum or or other tricks like this.",
            "So this doesn't require you to do anything like that.",
            "This is just.",
            "Minimize this convex loss in terms of Alpha and constraint Alpha to be in an L1 ball.",
            "It's extremely fast to implement too.",
            "And it works well.",
            "Incidentally, in case you were curious, when K is positive definite, this does generalize this L1 ball constraint on the alphas implies the L2 constraint from the VM, so so.",
            "So when K is positive definite, there's no question that this will work well.",
            "The question is, what happens when K is not?"
        ],
        [
            "Positive definite.",
            "Here's another reason why this is a useful thing you'd want to do.",
            "You have a a training data set of 100,000 images.",
            "When a new image test image comes in, you would like to avoid having to compare the test image against all training images that you've seen.",
            "You'd like to do some kind of a prototype selection and empirically we see that your performance can go up.",
            "Actually, your accuracy will actually go up from around 85% to 95%.",
            "If you're careful about selecting these prototypes and don't use all the prototypes so it's not just a matter of putting in a one regularizer, maybe you just want to also select by looking at the data a good subset of training examples to serve as prototypes."
        ],
        [
            "Certainly if."
        ],
        [
            "This generalizes.",
            "This will also generalize.",
            "This is a subset of the other one."
        ],
        [
            "So.",
            "So we built a system and it worked pretty well, but it's upsetting to be using this algorithm that too recently to yesterday at lunch had very few theoretical guarantees, so I after a couple of years of experiments I.",
            "Came up with a way to show that these things would work well.",
            "I'll show you a slide one slide on that.",
            "I was pointed to this workshop, so I submit it here.",
            "Week later I get an email.",
            "Oh great, you're in an I had plans to go visit naughty at TTI.",
            "I talked about something else offhand during lunch.",
            "I mentioned this result and not.",
            "He said this is not very satisfying.",
            "I just dropped it.",
            "I got on the airplane.",
            "And I came up with a much simpler proof which I sent to not be in a tea set, and this is not very satisfying either.",
            "Let me get back to you in a couple of days.",
            "So not he sent me an email with this beautiful result that he will talk about in a few minutes.",
            "That's much simpler and more elegant than than my original result.",
            "And yesterday he had lunch with Cheyenne.",
            "I think that the same thing that he did to me, possibly shunted to naughty.",
            "I hope so he knows how.",
            "So.",
            "In fact, I don't even know what he's going to talk about honestly, 'cause it's so fresh.",
            "Anyway, so I'm going to just briefly mention what this proof look like."
        ],
        [
            "It involves an intermediate result result that's maybe useful, so we were trying to bound this probability that minimizing the impact or something that does well in on the empirical test doesn't do well in expectation hypothesis space depends on the data, so my proof technique was generally to say, well, let's just make this hypothesis space depend on a ghost sample.",
            "Drawn IID and then bound this.",
            "This is quite easy to bound, and the proof uses this.",
            "This observation that that in a Banach space, if you're in a bonnick space of functions, if you draw a bunch of functions in the space ID from some distribution on the space and compute their convex Hall.",
            "And then do this again with another set of samples.",
            "And compute their convex hulls.",
            "Then these two convex holes are going to be very similar to each other, so there's a concentration of measure that happens in terms of the distance of the convex hulls of functions that you draw ID in this space.",
            "So this is my original proof and not he's just going to show you something his much more elegant proof."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with a lot of people, and that in itself is going to be an interesting part of the talk, maybe even more interesting than that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results themselves, so I joined Intel three years ago and Intel mostly builds hardware and software.",
                    "label": 0
                },
                {
                    "sent": "You don't?",
                    "label": 0
                },
                {
                    "sent": "They don't really make money off of theorems, but I'm going to tell you how this how this work came out of some software problem.",
                    "label": 0
                },
                {
                    "sent": "My inspiration was to build a system where you point your cell phone at whatever object is in your life and get a web page associated with it.",
                    "label": 0
                },
                {
                    "sent": "And modify that web page.",
                    "label": 0
                },
                {
                    "sent": "You could basically turn the whole world into a Wikipedia index so you know you would point out an object and it would say, oh, this is a printer.",
                    "label": 0
                },
                {
                    "sent": "And here's all this information about this printer.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's get on your cell phone.",
                    "label": 0
                },
                {
                    "sent": "That was my dream.",
                    "label": 0
                },
                {
                    "sent": "And we built this system.",
                    "label": 0
                },
                {
                    "sent": "Send together phone number right.",
                    "label": 0
                },
                {
                    "sent": "The way it actually works if you point it to the person right now it says person.",
                    "label": 0
                },
                {
                    "sent": "Like the product, really like you, you can take a photo and Amazon will tell your face.",
                    "label": 0
                },
                {
                    "sent": "Tell something similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah it works with planar objects.",
                    "label": 0
                },
                {
                    "sent": "As long as the object that they sell is planar, it works really well.",
                    "label": 0
                },
                {
                    "sent": "If it's got a curvature, they either don't sell it or they don't recognize it.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you a little bit about how this system works.",
                    "label": 0
                },
                {
                    "sent": "We basically spent a couple of years.",
                    "label": 0
                },
                {
                    "sent": "So going through the computer vision literature, picking out components, putting them together, and generally all of these object recognition systems works something like this.",
                    "label": 0
                },
                {
                    "sent": "An image comes in.",
                    "label": 0
                },
                {
                    "sent": "You have a training data set of images that you required a few 100,000 images or something like that.",
                    "label": 0
                },
                {
                    "sent": "Each of them has a label and you take your input image that you'd like to classify and compare it in some way against your training images.",
                    "label": 1
                },
                {
                    "sent": "These are similarity measurements between the incoming image and the training images, and then you somehow based on these similarity measures produce a classification.",
                    "label": 0
                },
                {
                    "sent": "So the way we do things is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we settled on is a measure of similarity between images that involves looking at local descriptors.",
                    "label": 0
                },
                {
                    "sent": "In these images just looking at edges and finding correspondences between edges and a training image and a test image an once we find these correspondences somehow induces a similarity between the images.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to do this, but the ones that work well on images will typically do something like this.",
                    "label": 0
                },
                {
                    "sent": "This is where the problem starts.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generally, the similarity measures that work really well don't have any nice mathematical properties at all.",
                    "label": 0
                },
                {
                    "sent": "They're certainly not positive, definitely, so if you were hoping of so.",
                    "label": 0
                },
                {
                    "sent": "If you were hoping to use an SVM.",
                    "label": 0
                },
                {
                    "sent": "Not going to work, they don't satisfy triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "The images you typically won't represent them as vectors in RN.",
                    "label": 0
                },
                {
                    "sent": "They typically end up being represented in some structured form of bags of local descriptors, and oftentimes K is not even a differentiable function.",
                    "label": 0
                },
                {
                    "sent": "Even if it were.",
                    "label": 0
                },
                {
                    "sent": "A similarity measure on RN.",
                    "label": 1
                },
                {
                    "sent": "So what happens the way the computer vision Community deals with with this lack of structure is basically the classifier that they use is nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Or K nearest neighbors and we know from other domains that K nearest neighbors is not that accurate, so we'd like to do something better.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me remind you what would happen if if K were positive definite.",
                    "label": 0
                },
                {
                    "sent": "You could use an SVM.",
                    "label": 1
                },
                {
                    "sent": "The decision boundary you learn is a weighted sum of kernels placed on your training data.",
                    "label": 0
                },
                {
                    "sent": "And you would train it by minimizing the empirical loss subject to some constraints on the coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have a lot of results about how this about the fact that this generalizes well.",
                    "label": 0
                },
                {
                    "sent": "If you score well on your empirical data on your on your training set, then then the probability that you'll score well on that, you'll have a true low true risk is going to be quite low, so we know how to bound this.",
                    "label": 0
                },
                {
                    "sent": "And generally the way we bound this is that we write down the hypothesis space weighted sums of kernels on the training data.",
                    "label": 0
                },
                {
                    "sent": "In a form that doesn't actually depends on the training data you write it down as as a linear hypothesis space where you feature eyes, your inputs using the eigen functions of the kernel and now really what you're dealing with is a hypothesis space that in no way depends on on the training data set at all.",
                    "label": 0
                },
                {
                    "sent": "So all of the tricks that we use to bound this relying quite heavily on the fact that this class of functions does not depend on the training data.",
                    "label": 1
                },
                {
                    "sent": "Not able to give you an example of when this breaks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what can you do in case is not positive definite?",
                    "label": 0
                },
                {
                    "sent": "So it turns out empirically that this type of thing works quite well.",
                    "label": 0
                },
                {
                    "sent": "Weighted sums of kernels, even if the kernels aren't positive definite, work amazingly well.",
                    "label": 0
                },
                {
                    "sent": "They'll work better than nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "How do you train him?",
                    "label": 0
                },
                {
                    "sent": "Well, you can again minimize an empirical risk and put some kind of a constraint on the coefficients.",
                    "label": 0
                },
                {
                    "sent": "And we could argue about what kinds of coefficients that put on them.",
                    "label": 0
                },
                {
                    "sent": "This is not the only way to train this.",
                    "label": 0
                },
                {
                    "sent": "Various people have proposed, for example, dealing with the fact that K is not positive definite by somehow taking the positive that the kernel matrix an making it positive definite by either shifting its spectrum or clipping its spectrum or or other tricks like this.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't require you to do anything like that.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "Minimize this convex loss in terms of Alpha and constraint Alpha to be in an L1 ball.",
                    "label": 0
                },
                {
                    "sent": "It's extremely fast to implement too.",
                    "label": 0
                },
                {
                    "sent": "And it works well.",
                    "label": 0
                },
                {
                    "sent": "Incidentally, in case you were curious, when K is positive definite, this does generalize this L1 ball constraint on the alphas implies the L2 constraint from the VM, so so.",
                    "label": 0
                },
                {
                    "sent": "So when K is positive definite, there's no question that this will work well.",
                    "label": 0
                },
                {
                    "sent": "The question is, what happens when K is not?",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positive definite.",
                    "label": 0
                },
                {
                    "sent": "Here's another reason why this is a useful thing you'd want to do.",
                    "label": 0
                },
                {
                    "sent": "You have a a training data set of 100,000 images.",
                    "label": 0
                },
                {
                    "sent": "When a new image test image comes in, you would like to avoid having to compare the test image against all training images that you've seen.",
                    "label": 0
                },
                {
                    "sent": "You'd like to do some kind of a prototype selection and empirically we see that your performance can go up.",
                    "label": 0
                },
                {
                    "sent": "Actually, your accuracy will actually go up from around 85% to 95%.",
                    "label": 0
                },
                {
                    "sent": "If you're careful about selecting these prototypes and don't use all the prototypes so it's not just a matter of putting in a one regularizer, maybe you just want to also select by looking at the data a good subset of training examples to serve as prototypes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Certainly if.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This generalizes.",
                    "label": 0
                },
                {
                    "sent": "This will also generalize.",
                    "label": 0
                },
                {
                    "sent": "This is a subset of the other one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we built a system and it worked pretty well, but it's upsetting to be using this algorithm that too recently to yesterday at lunch had very few theoretical guarantees, so I after a couple of years of experiments I.",
                    "label": 0
                },
                {
                    "sent": "Came up with a way to show that these things would work well.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a slide one slide on that.",
                    "label": 0
                },
                {
                    "sent": "I was pointed to this workshop, so I submit it here.",
                    "label": 0
                },
                {
                    "sent": "Week later I get an email.",
                    "label": 0
                },
                {
                    "sent": "Oh great, you're in an I had plans to go visit naughty at TTI.",
                    "label": 0
                },
                {
                    "sent": "I talked about something else offhand during lunch.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this result and not.",
                    "label": 0
                },
                {
                    "sent": "He said this is not very satisfying.",
                    "label": 0
                },
                {
                    "sent": "I just dropped it.",
                    "label": 0
                },
                {
                    "sent": "I got on the airplane.",
                    "label": 0
                },
                {
                    "sent": "And I came up with a much simpler proof which I sent to not be in a tea set, and this is not very satisfying either.",
                    "label": 0
                },
                {
                    "sent": "Let me get back to you in a couple of days.",
                    "label": 0
                },
                {
                    "sent": "So not he sent me an email with this beautiful result that he will talk about in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "That's much simpler and more elegant than than my original result.",
                    "label": 0
                },
                {
                    "sent": "And yesterday he had lunch with Cheyenne.",
                    "label": 0
                },
                {
                    "sent": "I think that the same thing that he did to me, possibly shunted to naughty.",
                    "label": 0
                },
                {
                    "sent": "I hope so he knows how.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In fact, I don't even know what he's going to talk about honestly, 'cause it's so fresh.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so I'm going to just briefly mention what this proof look like.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It involves an intermediate result result that's maybe useful, so we were trying to bound this probability that minimizing the impact or something that does well in on the empirical test doesn't do well in expectation hypothesis space depends on the data, so my proof technique was generally to say, well, let's just make this hypothesis space depend on a ghost sample.",
                    "label": 0
                },
                {
                    "sent": "Drawn IID and then bound this.",
                    "label": 0
                },
                {
                    "sent": "This is quite easy to bound, and the proof uses this.",
                    "label": 0
                },
                {
                    "sent": "This observation that that in a Banach space, if you're in a bonnick space of functions, if you draw a bunch of functions in the space ID from some distribution on the space and compute their convex Hall.",
                    "label": 0
                },
                {
                    "sent": "And then do this again with another set of samples.",
                    "label": 0
                },
                {
                    "sent": "And compute their convex hulls.",
                    "label": 0
                },
                {
                    "sent": "Then these two convex holes are going to be very similar to each other, so there's a concentration of measure that happens in terms of the distance of the convex hulls of functions that you draw ID in this space.",
                    "label": 0
                },
                {
                    "sent": "So this is my original proof and not he's just going to show you something his much more elegant proof.",
                    "label": 0
                }
            ]
        }
    }
}