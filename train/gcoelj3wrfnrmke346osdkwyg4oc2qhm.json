{
    "id": "gcoelj3wrfnrmke346osdkwyg4oc2qhm",
    "title": "Declarative Data Transformations for Linked Data Generation: the case of DBpedia",
    "info": {
        "author": [
            "Ben De Meester, Ghent University"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_de_meester_linked_data_generation/",
    "segmentation": [
        [
            "Hi good evening everyone.",
            "I'm banned Mr from you again University I'm going to start with apologizing.",
            "I know that this is something that has been done over and over and people get sick of it.",
            "But in loving memory of all the Barack Obama examples we had over the."
        ],
        [
            "Full of years we had, I really want to use our mascot like for one last time.",
            "So we're talking about DPI.",
            "We're talking about Barack Obama, so."
        ],
        [
            "How can we create like the linked Barack, what we want in the end is the Barack resource that is linked to all his data using certain properties and having certain values.",
            "So for this we need."
        ],
        [
            "Schema and data.",
            "The schema is the DPD ontology and the data is the data that resides in the infoboxes of Wikipedia.",
            "So."
        ],
        [
            "Look at this.",
            "It's a pretty specific case.",
            "The sources wikitext is not a really common data type of which you use to generate link data.",
            "It uses specific schema transformations, the DPD ontology, so it's not some pretty specific for this use case, but most importantly it uses very specific data transformations when you enter data in Wikipedia in the infoboxes you basically get.",
            "Free text field where you can enter.",
            "Pretty much anything you want.",
            "You can have some kind of structured inputs, but you don't have to.",
            "It's a crowdsourced data set so people can enter whatever and you can have a large variety of the data that is put in simple things like the birth dates of Barack Obama, or like the King of Belgium, where I'm from can be entered completely differently and shown completely differently as well.",
            "And this is people with different cultures and through things, and it's not only for dates, it's also about measurements of Heights.",
            "Do you square Mile Square area in kilometers?",
            "People do it on the wrong time all the time and this is something that the pedia extraction framework had to focus on to get actually the good values out of the mess that is manually entered in Wikipedia.",
            "So for this specific case, they."
        ],
        [
            "Created the DPD extraction framework and part a large part of that extraction framework are the transformation functions.",
            "The parsing functions.",
            "These are things that create the structured data from the input values and they are really robust.",
            "Today.",
            "They solve a lot of edge cases, they are tested against thousands of Wikipedia pages.",
            "There they are one of the building blocks of one of the most common known.",
            "Linked datasets that we have and it's all wrapped inside of the extraction framework.",
            "We it's me."
        ],
        [
            "That it's case specific and it's very coupled within that extraction framework.",
            "It's hard coded within that means that you cannot use the extraction framework as it is now for any other use cases and.",
            "That you cannot use those parsing functions.",
            "Those like really good functions that we all like really can use.",
            "You cannot use them outside of the DB Pedia extraction framework.",
            "And that's a real pity.",
            "I mean, kind of sucks.",
            "But if you look at.",
            "Like what what has been done before then you see for schema transformations this is something that is actually already like partially solved.",
            "Because if you look at schema transformations, so mapping the raw data into linked data, changing the schema, so adding the properties they use, like mapping languages."
        ],
        [
            "Declarative schema transformations.",
            "That means that you can base yourself on declarative data form like a configuration file to generate your link data.",
            "That means that the tooling is used case independent and that everything is decoupled from the implementation.",
            "You can use the same mapping document in another implementation.",
            "So what if we use similar approach, not just for schema transformations, but also for data transformations, and we combine those two?"
        ],
        [
            "I mean if we combine both declarative schema and data transformations, then we're going to like skyrocket link.",
            "Data generation is going to be awesome.",
            "And that's exactly what I will try to."
        ],
        [
            "Explain.",
            "So first I would like to give an overview of existing approaches why this isn't possible as it is now why DP?",
            "I had to go to a hard coded approach and what the disadvantages are.",
            "All current approaches and then I will show what we have, what we provide our approach and the implementation that we have."
        ],
        [
            "So first, the existing approaches focusing on those data transformations.",
            "I'm going to go from basically 0 support of data transformations to basically any supportive data transformations, and then you have direct mappings, successive steps, embedded data transformations, and a fully hard coded solution."
        ],
        [
            "Direct mappings.",
            "You basically go from the original data to RDF with minimal change in the structure of the data.",
            "So for example CSV on the web or or adding a context to Jason making a Jason LD is going as a direct mapping, but that clearly is not sufficient for.",
            "For something like infoboxes for DB pedia.",
            "Because how do you go from something like this with any schema or data transformations to resources?",
            "So let's skip that one."
        ],
        [
            "Successive steps means that you first do the data transformations and then do the schema transformations, or vice versa.",
            "For example, what is being done in R2.",
            "RML is one of the mapping languages where you depends on the SQL views, so that basically say when you get it out of the data set, you first do some transformation and then R2, RML will use it to go to the link data.",
            "But it means it's pretty restricted.",
            "You're depending on those SQL views.",
            "In the case of R2, RML, you cannot do any data transformations that you want, and it's not really combinable.",
            "The data transformation in the schema transformations.",
            "For example, if you know that born should return a date that is actually a combination between schema transformation and data transformation.",
            "But if it are successive steps, you cannot combine them, so you also kind of restrict what you can do if you do successive steps."
        ],
        [
            "There are also tools out there, for example openrefine, that that allow embedded that have embedded data transformation, so they provide you with a fixed set of tools that you can use.",
            "Or some kind of scripting environment where you can show some minor snippets you can put in some minor snippets, but it's also pretty limited because parsing the PD and you should like check the code if you're into that kind of stuff.",
            "It's more than just parsing.",
            "It's not just splitting a string or doing some regular expressions.",
            "It's pretty hardcore stuff and normal data transformations.",
            "The predefined set of data transformations just.",
            "Don't cut it.",
            "Also, if you have a scripting environment and either you have to like port all the current code into like something like JavaScript or you have problems with embedding external libraries, reading from a configuration file.",
            "So it's still pretty restricted.",
            "And finally, it's coupled.",
            "The types of data transformations that you can do are coupled with the tooling, so you're kind of stuck with one tool, and if you want to switch then you just can't.",
            "So."
        ],
        [
            "If you have all those, then it's kind of clear that DPJ had to go to some kind of hardcoded extraction framework framework where first all the wiki pages are selected.",
            "The ones with the info boxes or extract it, then the schema transformation is done, and then the data transformations are executed.",
            "So basically, if you have a lot."
        ],
        [
            "Of Wikipedia pages liking of Belgium some disambiguation page."
        ],
        [
            "And of course, then you select the ones that."
        ],
        [
            "We have valuable information that have those infoboxes you."
        ],
        [
            "Extract the wikitext out of that, so this is like a simplified example.",
            "And with this wikitext you start with the schema transformations."
        ],
        [
            "So you end up with something like this where you basically have all the raw values and then that's where the data transformation functions like those parsing functions will work on on those raw data values."
        ],
        [
            "And they get the link to the rock that we all know and love.",
            "But it's got this."
        ],
        [
            "Messages, I mean again, the data transformations are coupled with in your hard coded framework.",
            "You cannot use a different framework, and it's very, very case specific.",
            "If you can only use that for what one use case, both the data transformations and the framework as a whole.",
            "So."
        ],
        [
            "To wrap up.",
            "The disadvantage is that we currently have is that we have restricted UN combinable coupled and case specific existing approaches, which makes Obama said we need to make him glad."
        ],
        [
            "We need to have unrestricted data transformations where we can combine the schema and the data transformations, not coupled with the implementation and have a case independent solution and that is exactly."
        ],
        [
            "What we propose.",
            "So our approach is."
        ],
        [
            "To align the collaborative schema transformations with declarative data transformations by aligning them on the one hand, you can still use them both separately, so you can still do the successive steps if you would want that, but by aligning them you're allowed.",
            "You allow yourself to combine those data and schema transformations at the same time.",
            "By having declarative data transformations you.",
            "De Couple the declaration of your parsing functions with the actual implementation.",
            "So you say I want to use this kind of function, but the implementation is outside of that framework, so you don't put a restriction on your mapping document what kind of transformations you support.",
            "You just say the reference to this function, and then you can use that outside of it's no longer coupled with the mapping document.",
            "And because those in parsing functions are outside, you can reuse them outside your generation framework.",
            "You can reuse the same parsing functions in analyzing or in another kind of generation framework, and finally by aligning them all declaratively, you end up with one configuration file.",
            "That means that you no longer depends on the use case.",
            "Your tool can be used for the PDF or any other generation, and it's the mapping document is also not implementation specific.",
            "If you would have the same mapping documents, but you would have something that would work like in JavaScript in the browser or some kind of command line or web service doesn't matter.",
            "The only point of configuration is your mapping document."
        ],
        [
            "So.",
            "How did we do this?",
            "Well, first I will go into like the more modeling theoretical parts and then like the actual tooling that we provide.",
            "So what do we need for actually being able to do that?",
            "Well, you need."
        ],
        [
            "Declarative schema transformation language.",
            "Something that allows you to map and do the schema transformations.",
            "For this we need something that is source agnostic because we had the wikitext, so you need something that can cope with other data sources and something schema agnostic, so no direct mapping like CSV on the web things, and for this we use the RDF mapping language, so this RDF mapping language is an extension of R2, RML.",
            "Which is the W3C recommendation for mapping database data into length data, but exactly extended to support heterogeneous data sources, so that's like.",
            "A really good fit and.",
            "The advantage of doing things RML will also like be explained by Heiko tomorrow, so stay tuned.",
            "Second, parts are the declarative data transformations.",
            "So you need something that allows you to declare your data transformations, your posture, functions parsing functions without depending on the implementation being implementation agnostic, and for this we used the function ontology, which does exactly that.",
            "It shows you how to declare a function, says where audience, what are the outs, what does it do?",
            "But it doesn't say this is the implementation, he doesn't.",
            "You don't have to specify how it is implemented.",
            "You don't have to embed some kind of snippet snippet, JavaScript code, anything like that.",
            "It's completely decouples, so those are the two schema transformation languages that we used and we align them using function mapping function value, which I will go a little bit more into detail right now."
        ],
        [
            "If I go in the right direction.",
            "The optimal mappings are looking kind of like this, so if we want to create link Brock, we have the person mapping that's links to the wiki text and has a certain subject and for example for the birth date you specify, what is the predicate.",
            "So DB Pedia property, birth dates and the reference, so this reference is like a direct reference to the raw value of your source, so then you have something which you see here.",
            "So this is similar to what the extraction framework does in the first step, you do like a pure schema transformation using RML.",
            "If we combine."
        ],
        [
            "That with the function ontology mapping there you basically say this is a function that executes this specific function.",
            "The DB pedia partial this audience dirty outs and the input string is exactly the same.",
            "Reference the birthdate reference and then you don't have any schema transformations.",
            "But then you actually parse the raw value, which is what you want.",
            "So."
        ],
        [
            "So if we combine them, it looks pretty obvious where like the connection should be.",
            "So instead of going directly referencing the raw birthdate value, we connect with."
        ],
        [
            "The function map my animation.",
            "We connect to their function map and we go through the function.",
            "The parsing function to go from the raw data value via the parsing back to the birthdate mapping and that way we actually have the linked Barack that we want."
        ],
        [
            "Tooling wise we extended the RML processor that exists for obvious reasons.",
            "If we use our Mel might as well use its processor two includes the Wikitext extractor which was actually a pretty natural thing to do because the whole point of the RML and VRML processor is to support Heathrow Genius data sources.",
            "So including another data source pretty straightforward.",
            "We also had to support the function mapping function of value to make the connection with the actual function processor.",
            "And the function processor is a new generic separate tool.",
            "This is the processor that.",
            "Parses the function declarations and then uses that to dynamically loads and call the function.",
            "So basically, if you have a certain UI for a certain function, you can dereference that function to get the implementation, and then you can dynamically load that function and call it whenever you need it.",
            "And finally, and this is pretty cool, we externalized the DB pedia parsing functions basically as is so as they were completely embedded within the extraction framework.",
            "We could just copy paste them so it was like Scala.",
            "So we just copy pasted this color code.",
            "So it's exactly the same thing and we externalized it as a library now.",
            "So now you can use that no longer bound to that extraction framework.",
            "You can use that wherever, whenever.",
            "And that's pretty cool actually."
        ],
        [
            "So.",
            "If we look back at how the extraction framework used to be, it was one big hard coded.",
            "Framework where the schema and data transformations were done successively, we now the only thing that is specific to Wikipedia is actually the selection and extraction of the infoboxes, which also kind of makes sense if you have to go from Wikipedia.",
            "But everything else is actually our generic tools, so the RML processor is not dependent on DB pedia.",
            "The function processor is a generic tool.",
            "The only thing that is specific for DB pedia is your configuration file.",
            "The combination of RML mappings and fno mappings.",
            "So this document so every time when the.",
            "You in this document you find a function map, so you have some kind of function that needs to be executed."
        ],
        [
            "The RML processor retrieves the value from the info box, which is what your Mail processor was made for.",
            "It."
        ],
        [
            "Sends that value to the function processor together with like which function he should execute.",
            "So then he goes fetching the function either locally or remotely, and then he execute."
        ],
        [
            "The external DB pedia functions.",
            "To get the actual new parsed value, that value gets returned to the."
        ],
        [
            "And processor and then return the function processor returns the return value so."
        ],
        [
            "To the RML processor and in the end we get.",
            "Exactly the same RDF data as the.",
            "TPX Traction framework had."
        ],
        [
            "So to wrap up, we looked into the current approach to solve for data transformations, DP add some quite high standards, quite high needs for the data transformations, and currently we only could fall back to hard code.",
            "It's solutions by aligning schema and data transformations declaratively, we have like 1 configuration document, one mapping document that allows you to do everything without depending on the implementation.",
            "And we."
        ],
        [
            "The exactly same DB pedia data.",
            "With the additional advantages that you no longer depends on the implementation.",
            "So you can use the mapping document for other implementations.",
            "You don't depend on the use case.",
            "The tooling you can reuse for other use cases, there are completely generic.",
            "The DB pedia parsing functions are external libraries or they can be used for any other use Case No longer bound to DB pedia and like a final cool thing is that the data transformations can use existing parsing functions like the ones from DB pedia or they can new use new external libraries.",
            "So if someone comes up with a date parsing function that is better than the current.",
            "DB PEDIA parsing function.",
            "It's basically changing one declaration in your mapping document and you have like instant better generation of DB pedia.",
            "Obama approves.",
            "I approve too, but.",
            "Please do not take our word for it, we are."
        ],
        [
            "Demo tomorrow at Booth 49.",
            "We have to also online fno dot GitHub dot IO dot DB pedia demo so please check it out and together we can make link data generation grade again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi good evening everyone.",
                    "label": 0
                },
                {
                    "sent": "I'm banned Mr from you again University I'm going to start with apologizing.",
                    "label": 0
                },
                {
                    "sent": "I know that this is something that has been done over and over and people get sick of it.",
                    "label": 0
                },
                {
                    "sent": "But in loving memory of all the Barack Obama examples we had over the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full of years we had, I really want to use our mascot like for one last time.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about DPI.",
                    "label": 0
                },
                {
                    "sent": "We're talking about Barack Obama, so.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we create like the linked Barack, what we want in the end is the Barack resource that is linked to all his data using certain properties and having certain values.",
                    "label": 0
                },
                {
                    "sent": "So for this we need.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Schema and data.",
                    "label": 0
                },
                {
                    "sent": "The schema is the DPD ontology and the data is the data that resides in the infoboxes of Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at this.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty specific case.",
                    "label": 1
                },
                {
                    "sent": "The sources wikitext is not a really common data type of which you use to generate link data.",
                    "label": 0
                },
                {
                    "sent": "It uses specific schema transformations, the DPD ontology, so it's not some pretty specific for this use case, but most importantly it uses very specific data transformations when you enter data in Wikipedia in the infoboxes you basically get.",
                    "label": 0
                },
                {
                    "sent": "Free text field where you can enter.",
                    "label": 0
                },
                {
                    "sent": "Pretty much anything you want.",
                    "label": 0
                },
                {
                    "sent": "You can have some kind of structured inputs, but you don't have to.",
                    "label": 0
                },
                {
                    "sent": "It's a crowdsourced data set so people can enter whatever and you can have a large variety of the data that is put in simple things like the birth dates of Barack Obama, or like the King of Belgium, where I'm from can be entered completely differently and shown completely differently as well.",
                    "label": 0
                },
                {
                    "sent": "And this is people with different cultures and through things, and it's not only for dates, it's also about measurements of Heights.",
                    "label": 0
                },
                {
                    "sent": "Do you square Mile Square area in kilometers?",
                    "label": 0
                },
                {
                    "sent": "People do it on the wrong time all the time and this is something that the pedia extraction framework had to focus on to get actually the good values out of the mess that is manually entered in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So for this specific case, they.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Created the DPD extraction framework and part a large part of that extraction framework are the transformation functions.",
                    "label": 0
                },
                {
                    "sent": "The parsing functions.",
                    "label": 0
                },
                {
                    "sent": "These are things that create the structured data from the input values and they are really robust.",
                    "label": 0
                },
                {
                    "sent": "Today.",
                    "label": 0
                },
                {
                    "sent": "They solve a lot of edge cases, they are tested against thousands of Wikipedia pages.",
                    "label": 0
                },
                {
                    "sent": "There they are one of the building blocks of one of the most common known.",
                    "label": 0
                },
                {
                    "sent": "Linked datasets that we have and it's all wrapped inside of the extraction framework.",
                    "label": 0
                },
                {
                    "sent": "We it's me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That it's case specific and it's very coupled within that extraction framework.",
                    "label": 0
                },
                {
                    "sent": "It's hard coded within that means that you cannot use the extraction framework as it is now for any other use cases and.",
                    "label": 0
                },
                {
                    "sent": "That you cannot use those parsing functions.",
                    "label": 0
                },
                {
                    "sent": "Those like really good functions that we all like really can use.",
                    "label": 0
                },
                {
                    "sent": "You cannot use them outside of the DB Pedia extraction framework.",
                    "label": 0
                },
                {
                    "sent": "And that's a real pity.",
                    "label": 0
                },
                {
                    "sent": "I mean, kind of sucks.",
                    "label": 0
                },
                {
                    "sent": "But if you look at.",
                    "label": 0
                },
                {
                    "sent": "Like what what has been done before then you see for schema transformations this is something that is actually already like partially solved.",
                    "label": 0
                },
                {
                    "sent": "Because if you look at schema transformations, so mapping the raw data into linked data, changing the schema, so adding the properties they use, like mapping languages.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Declarative schema transformations.",
                    "label": 0
                },
                {
                    "sent": "That means that you can base yourself on declarative data form like a configuration file to generate your link data.",
                    "label": 0
                },
                {
                    "sent": "That means that the tooling is used case independent and that everything is decoupled from the implementation.",
                    "label": 1
                },
                {
                    "sent": "You can use the same mapping document in another implementation.",
                    "label": 1
                },
                {
                    "sent": "So what if we use similar approach, not just for schema transformations, but also for data transformations, and we combine those two?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean if we combine both declarative schema and data transformations, then we're going to like skyrocket link.",
                    "label": 1
                },
                {
                    "sent": "Data generation is going to be awesome.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what I will try to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explain.",
                    "label": 0
                },
                {
                    "sent": "So first I would like to give an overview of existing approaches why this isn't possible as it is now why DP?",
                    "label": 0
                },
                {
                    "sent": "I had to go to a hard coded approach and what the disadvantages are.",
                    "label": 0
                },
                {
                    "sent": "All current approaches and then I will show what we have, what we provide our approach and the implementation that we have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first, the existing approaches focusing on those data transformations.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go from basically 0 support of data transformations to basically any supportive data transformations, and then you have direct mappings, successive steps, embedded data transformations, and a fully hard coded solution.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Direct mappings.",
                    "label": 0
                },
                {
                    "sent": "You basically go from the original data to RDF with minimal change in the structure of the data.",
                    "label": 0
                },
                {
                    "sent": "So for example CSV on the web or or adding a context to Jason making a Jason LD is going as a direct mapping, but that clearly is not sufficient for.",
                    "label": 0
                },
                {
                    "sent": "For something like infoboxes for DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Because how do you go from something like this with any schema or data transformations to resources?",
                    "label": 1
                },
                {
                    "sent": "So let's skip that one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Successive steps means that you first do the data transformations and then do the schema transformations, or vice versa.",
                    "label": 1
                },
                {
                    "sent": "For example, what is being done in R2.",
                    "label": 0
                },
                {
                    "sent": "RML is one of the mapping languages where you depends on the SQL views, so that basically say when you get it out of the data set, you first do some transformation and then R2, RML will use it to go to the link data.",
                    "label": 0
                },
                {
                    "sent": "But it means it's pretty restricted.",
                    "label": 0
                },
                {
                    "sent": "You're depending on those SQL views.",
                    "label": 0
                },
                {
                    "sent": "In the case of R2, RML, you cannot do any data transformations that you want, and it's not really combinable.",
                    "label": 0
                },
                {
                    "sent": "The data transformation in the schema transformations.",
                    "label": 0
                },
                {
                    "sent": "For example, if you know that born should return a date that is actually a combination between schema transformation and data transformation.",
                    "label": 0
                },
                {
                    "sent": "But if it are successive steps, you cannot combine them, so you also kind of restrict what you can do if you do successive steps.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are also tools out there, for example openrefine, that that allow embedded that have embedded data transformation, so they provide you with a fixed set of tools that you can use.",
                    "label": 0
                },
                {
                    "sent": "Or some kind of scripting environment where you can show some minor snippets you can put in some minor snippets, but it's also pretty limited because parsing the PD and you should like check the code if you're into that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "It's more than just parsing.",
                    "label": 0
                },
                {
                    "sent": "It's not just splitting a string or doing some regular expressions.",
                    "label": 0
                },
                {
                    "sent": "It's pretty hardcore stuff and normal data transformations.",
                    "label": 1
                },
                {
                    "sent": "The predefined set of data transformations just.",
                    "label": 1
                },
                {
                    "sent": "Don't cut it.",
                    "label": 0
                },
                {
                    "sent": "Also, if you have a scripting environment and either you have to like port all the current code into like something like JavaScript or you have problems with embedding external libraries, reading from a configuration file.",
                    "label": 0
                },
                {
                    "sent": "So it's still pretty restricted.",
                    "label": 0
                },
                {
                    "sent": "And finally, it's coupled.",
                    "label": 0
                },
                {
                    "sent": "The types of data transformations that you can do are coupled with the tooling, so you're kind of stuck with one tool, and if you want to switch then you just can't.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have all those, then it's kind of clear that DPJ had to go to some kind of hardcoded extraction framework framework where first all the wiki pages are selected.",
                    "label": 0
                },
                {
                    "sent": "The ones with the info boxes or extract it, then the schema transformation is done, and then the data transformations are executed.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you have a lot.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of Wikipedia pages liking of Belgium some disambiguation page.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, then you select the ones that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have valuable information that have those infoboxes you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extract the wikitext out of that, so this is like a simplified example.",
                    "label": 0
                },
                {
                    "sent": "And with this wikitext you start with the schema transformations.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you end up with something like this where you basically have all the raw values and then that's where the data transformation functions like those parsing functions will work on on those raw data values.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they get the link to the rock that we all know and love.",
                    "label": 0
                },
                {
                    "sent": "But it's got this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Messages, I mean again, the data transformations are coupled with in your hard coded framework.",
                    "label": 0
                },
                {
                    "sent": "You cannot use a different framework, and it's very, very case specific.",
                    "label": 0
                },
                {
                    "sent": "If you can only use that for what one use case, both the data transformations and the framework as a whole.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To wrap up.",
                    "label": 0
                },
                {
                    "sent": "The disadvantage is that we currently have is that we have restricted UN combinable coupled and case specific existing approaches, which makes Obama said we need to make him glad.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to have unrestricted data transformations where we can combine the schema and the data transformations, not coupled with the implementation and have a case independent solution and that is exactly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we propose.",
                    "label": 0
                },
                {
                    "sent": "So our approach is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To align the collaborative schema transformations with declarative data transformations by aligning them on the one hand, you can still use them both separately, so you can still do the successive steps if you would want that, but by aligning them you're allowed.",
                    "label": 0
                },
                {
                    "sent": "You allow yourself to combine those data and schema transformations at the same time.",
                    "label": 0
                },
                {
                    "sent": "By having declarative data transformations you.",
                    "label": 0
                },
                {
                    "sent": "De Couple the declaration of your parsing functions with the actual implementation.",
                    "label": 0
                },
                {
                    "sent": "So you say I want to use this kind of function, but the implementation is outside of that framework, so you don't put a restriction on your mapping document what kind of transformations you support.",
                    "label": 0
                },
                {
                    "sent": "You just say the reference to this function, and then you can use that outside of it's no longer coupled with the mapping document.",
                    "label": 0
                },
                {
                    "sent": "And because those in parsing functions are outside, you can reuse them outside your generation framework.",
                    "label": 0
                },
                {
                    "sent": "You can reuse the same parsing functions in analyzing or in another kind of generation framework, and finally by aligning them all declaratively, you end up with one configuration file.",
                    "label": 0
                },
                {
                    "sent": "That means that you no longer depends on the use case.",
                    "label": 0
                },
                {
                    "sent": "Your tool can be used for the PDF or any other generation, and it's the mapping document is also not implementation specific.",
                    "label": 0
                },
                {
                    "sent": "If you would have the same mapping documents, but you would have something that would work like in JavaScript in the browser or some kind of command line or web service doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "The only point of configuration is your mapping document.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How did we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, first I will go into like the more modeling theoretical parts and then like the actual tooling that we provide.",
                    "label": 0
                },
                {
                    "sent": "So what do we need for actually being able to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, you need.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Declarative schema transformation language.",
                    "label": 0
                },
                {
                    "sent": "Something that allows you to map and do the schema transformations.",
                    "label": 0
                },
                {
                    "sent": "For this we need something that is source agnostic because we had the wikitext, so you need something that can cope with other data sources and something schema agnostic, so no direct mapping like CSV on the web things, and for this we use the RDF mapping language, so this RDF mapping language is an extension of R2, RML.",
                    "label": 0
                },
                {
                    "sent": "Which is the W3C recommendation for mapping database data into length data, but exactly extended to support heterogeneous data sources, so that's like.",
                    "label": 0
                },
                {
                    "sent": "A really good fit and.",
                    "label": 0
                },
                {
                    "sent": "The advantage of doing things RML will also like be explained by Heiko tomorrow, so stay tuned.",
                    "label": 0
                },
                {
                    "sent": "Second, parts are the declarative data transformations.",
                    "label": 0
                },
                {
                    "sent": "So you need something that allows you to declare your data transformations, your posture, functions parsing functions without depending on the implementation being implementation agnostic, and for this we used the function ontology, which does exactly that.",
                    "label": 0
                },
                {
                    "sent": "It shows you how to declare a function, says where audience, what are the outs, what does it do?",
                    "label": 0
                },
                {
                    "sent": "But it doesn't say this is the implementation, he doesn't.",
                    "label": 0
                },
                {
                    "sent": "You don't have to specify how it is implemented.",
                    "label": 0
                },
                {
                    "sent": "You don't have to embed some kind of snippet snippet, JavaScript code, anything like that.",
                    "label": 0
                },
                {
                    "sent": "It's completely decouples, so those are the two schema transformation languages that we used and we align them using function mapping function value, which I will go a little bit more into detail right now.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I go in the right direction.",
                    "label": 0
                },
                {
                    "sent": "The optimal mappings are looking kind of like this, so if we want to create link Brock, we have the person mapping that's links to the wiki text and has a certain subject and for example for the birth date you specify, what is the predicate.",
                    "label": 0
                },
                {
                    "sent": "So DB Pedia property, birth dates and the reference, so this reference is like a direct reference to the raw value of your source, so then you have something which you see here.",
                    "label": 0
                },
                {
                    "sent": "So this is similar to what the extraction framework does in the first step, you do like a pure schema transformation using RML.",
                    "label": 0
                },
                {
                    "sent": "If we combine.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That with the function ontology mapping there you basically say this is a function that executes this specific function.",
                    "label": 0
                },
                {
                    "sent": "The DB pedia partial this audience dirty outs and the input string is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Reference the birthdate reference and then you don't have any schema transformations.",
                    "label": 0
                },
                {
                    "sent": "But then you actually parse the raw value, which is what you want.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we combine them, it looks pretty obvious where like the connection should be.",
                    "label": 0
                },
                {
                    "sent": "So instead of going directly referencing the raw birthdate value, we connect with.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function map my animation.",
                    "label": 0
                },
                {
                    "sent": "We connect to their function map and we go through the function.",
                    "label": 0
                },
                {
                    "sent": "The parsing function to go from the raw data value via the parsing back to the birthdate mapping and that way we actually have the linked Barack that we want.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tooling wise we extended the RML processor that exists for obvious reasons.",
                    "label": 0
                },
                {
                    "sent": "If we use our Mel might as well use its processor two includes the Wikitext extractor which was actually a pretty natural thing to do because the whole point of the RML and VRML processor is to support Heathrow Genius data sources.",
                    "label": 0
                },
                {
                    "sent": "So including another data source pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "We also had to support the function mapping function of value to make the connection with the actual function processor.",
                    "label": 0
                },
                {
                    "sent": "And the function processor is a new generic separate tool.",
                    "label": 0
                },
                {
                    "sent": "This is the processor that.",
                    "label": 0
                },
                {
                    "sent": "Parses the function declarations and then uses that to dynamically loads and call the function.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you have a certain UI for a certain function, you can dereference that function to get the implementation, and then you can dynamically load that function and call it whenever you need it.",
                    "label": 0
                },
                {
                    "sent": "And finally, and this is pretty cool, we externalized the DB pedia parsing functions basically as is so as they were completely embedded within the extraction framework.",
                    "label": 0
                },
                {
                    "sent": "We could just copy paste them so it was like Scala.",
                    "label": 0
                },
                {
                    "sent": "So we just copy pasted this color code.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly the same thing and we externalized it as a library now.",
                    "label": 0
                },
                {
                    "sent": "So now you can use that no longer bound to that extraction framework.",
                    "label": 0
                },
                {
                    "sent": "You can use that wherever, whenever.",
                    "label": 0
                },
                {
                    "sent": "And that's pretty cool actually.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we look back at how the extraction framework used to be, it was one big hard coded.",
                    "label": 0
                },
                {
                    "sent": "Framework where the schema and data transformations were done successively, we now the only thing that is specific to Wikipedia is actually the selection and extraction of the infoboxes, which also kind of makes sense if you have to go from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But everything else is actually our generic tools, so the RML processor is not dependent on DB pedia.",
                    "label": 0
                },
                {
                    "sent": "The function processor is a generic tool.",
                    "label": 0
                },
                {
                    "sent": "The only thing that is specific for DB pedia is your configuration file.",
                    "label": 0
                },
                {
                    "sent": "The combination of RML mappings and fno mappings.",
                    "label": 0
                },
                {
                    "sent": "So this document so every time when the.",
                    "label": 0
                },
                {
                    "sent": "You in this document you find a function map, so you have some kind of function that needs to be executed.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The RML processor retrieves the value from the info box, which is what your Mail processor was made for.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sends that value to the function processor together with like which function he should execute.",
                    "label": 0
                },
                {
                    "sent": "So then he goes fetching the function either locally or remotely, and then he execute.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The external DB pedia functions.",
                    "label": 0
                },
                {
                    "sent": "To get the actual new parsed value, that value gets returned to the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And processor and then return the function processor returns the return value so.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the RML processor and in the end we get.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same RDF data as the.",
                    "label": 0
                },
                {
                    "sent": "TPX Traction framework had.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to wrap up, we looked into the current approach to solve for data transformations, DP add some quite high standards, quite high needs for the data transformations, and currently we only could fall back to hard code.",
                    "label": 0
                },
                {
                    "sent": "It's solutions by aligning schema and data transformations declaratively, we have like 1 configuration document, one mapping document that allows you to do everything without depending on the implementation.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The exactly same DB pedia data.",
                    "label": 0
                },
                {
                    "sent": "With the additional advantages that you no longer depends on the implementation.",
                    "label": 0
                },
                {
                    "sent": "So you can use the mapping document for other implementations.",
                    "label": 0
                },
                {
                    "sent": "You don't depend on the use case.",
                    "label": 0
                },
                {
                    "sent": "The tooling you can reuse for other use cases, there are completely generic.",
                    "label": 0
                },
                {
                    "sent": "The DB pedia parsing functions are external libraries or they can be used for any other use Case No longer bound to DB pedia and like a final cool thing is that the data transformations can use existing parsing functions like the ones from DB pedia or they can new use new external libraries.",
                    "label": 0
                },
                {
                    "sent": "So if someone comes up with a date parsing function that is better than the current.",
                    "label": 1
                },
                {
                    "sent": "DB PEDIA parsing function.",
                    "label": 0
                },
                {
                    "sent": "It's basically changing one declaration in your mapping document and you have like instant better generation of DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Obama approves.",
                    "label": 0
                },
                {
                    "sent": "I approve too, but.",
                    "label": 0
                },
                {
                    "sent": "Please do not take our word for it, we are.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Demo tomorrow at Booth 49.",
                    "label": 0
                },
                {
                    "sent": "We have to also online fno dot GitHub dot IO dot DB pedia demo so please check it out and together we can make link data generation grade again.",
                    "label": 0
                }
            ]
        }
    }
}