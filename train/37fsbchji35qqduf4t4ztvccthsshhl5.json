{
    "id": "37fsbchji35qqduf4t4ztvccthsshhl5",
    "title": "Multi-Objective Markov Decision Processes for Decision Support",
    "info": {
        "author": [
            "Dan Lizotte, Department of Computer Science, University of Western Ontario"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_lizotte_decision_support/",
    "segmentation": [
        [
            "Hi everybody, it's great to be here at like to thank the organizers for giving me this opportunity to tell you about something."
        ],
        [
            "That is very dear to my heart.",
            "So I would like to talk today about an application area that I think is currently underserved by reinforcement learning, and I think it's important that we make an effort to push into this area because we as a community have the opportunity to have a significant impact.",
            "So my thesis kind of today is that automating and supporting sequential decision making are similar, but they're different problems and I want to talk today about some methods that I've been working on that are specifically geared toward.",
            "Supporting decisions that are made by humans and kind of ties in actually with endless talk and and Patrick's talk about augmenting the capabilities of humans.",
            "Humans in the loop is a very important aspect of what I wanted to talk to you about today, so I'm just going to talk about one piece that I believe is important for using MPs.",
            "NRL methods for decision support and that is accommodating the situation where you've got more than one reward that you actually do care about.",
            "And I'm going to talk about this in the context of what sometimes called the mom DP literature.",
            "There's a great survey that I recommend you read if you're interested.",
            "I'm going to introduce a method that we're calling non deterministic fitted queue for multiple objectives.",
            "Where we're going to produce a set valued policy instead of a policy that just gives you a single action, and I'm going to show you an example of how this might be used for decision support using some clinical trial data."
        ],
        [
            "So we probably seen this box diagram like I don't know, maybe it doesn't.",
            "I'm so far I want to show up, put it up there one more time and just for the sake of making things concrete, I think it's fair to say that for the computer, scientists in the crowd when they see this box diagram, what they're really thinking about in the agent boxes."
        ],
        [
            "Kind of something like this.",
            "Which is wonderful an the environment box would be something kind of like that.",
            "OK, so so the whole premise background is about producing an autonomous agent that is going to be able to behave in an excellent way in an environment to do awesome things.",
            "And this model has lots of useful properties.",
            "This way of thinking about problems, lots of useful properties, including being forward thinking, being adaptive to changes in state, and being able to work in a goal directed way by observing rewards and reacting to that."
        ],
        [
            "So there is an analogous setting in clinical decision support where you replace, by the way, any MD's in the audience.",
            "Is there a doctor in the house?",
            "No MD's OK so I'm OK, but they could be online."
        ],
        [
            "So it's natural to at least consider that this framework you know you could think about the doc as an agent.",
            "Anna patient as an environment.",
            "And the doc is choosing treatments or actions.",
            "You know sequentially overtime, especially if you're talking about a chronic disease, the patient's health state is evolving.",
            "The doctors responding to that, and the doc is also observing some kind of happiness that the patient has with their health status.",
            "Actually, it's probably some joint happiness of the patient and the doc together, which is sort of analogous to reward.",
            "So if I'm in a different type of meeting, I use different words sometimes.",
            "But the basic idea is it's a reasonable fit.",
            "OK, so given that we have this way of thinking about problems, how can we leverage it include?"
        ],
        [
            "Decision support.",
            "So.",
            "I really want to drive this home.",
            "The reason I think we could make an important impact is that we have expertise as a community and as individuals in models for describing problems where we want to maximize long-term success.",
            "We want to make context independent decisions.",
            "Decisions that are based on state individualization is is the keyword that you might use.",
            "If you're in a medical setting, OK and the model handles uncertain futures naturally, we obviously don't know.",
            "We often do not know exactly how a patient is going to respond to treatment.",
            "That's just how life is.",
            "But the model has that in it.",
            "And we can learn policies from data.",
            "We can learn good policies from data potentially.",
            "So.",
            "I think the motivation is relatively easy at this level.",
            "To make the claim that RLS, an MVP's in these related sequential decision making methods should be making impacts in clinical decisions, especially for chronic disease.",
            "And I talked to computer scientists about tests and depending on their enthusiasm levels, sometimes they say."
        ],
        [
            "Hey.",
            "Awesome, let's do it.",
            "For those of you who are less energy than I am, that's the emergency medical hologram from Star Trek Voyager, and they often think.",
            "Great, we will have total control.",
            "And I admire their enthusiasm I think.",
            "And actually you know, especially given Clara stock, the first talk of the day about safety, an about marrying the idea of a safe space in which you can act with learning and evolving.",
            "I think that that actually is one of the key things that is going to push us.",
            "Someone in this direction, again no MD's in the House, so I can say that but really.",
            "We can make a much better impact now."
        ],
        [
            "By instead of thinking about replacing the agent that's making the decisions, let's talk about how can we support the decisions that are being made by a human.",
            "Anne.",
            "And if anybody wants to get a better sense, maybe of why this has to be so nuanced, this communication with position, I recommend that you Google the following terms.",
            "This could be like interactive.",
            "If we want, you can open a new tab next to Facebook.",
            "Evidence.",
            "Based medicine rubbish.",
            "And look for a talk by a woman named Trish.",
            "It is fantastic and really it's.",
            "It's an anecdotal talk.",
            "She does a great job of explaining the she's a physician herself and then she had a mishap that require treatment and she got exposed to evidence based medicine and it she tells a wonderful story an it really illustrates the kind of nuance we need to have if we're interacting in that space.",
            "So recommend that video and.",
            "So the thing that I wanted to talk about today was in clinical decision settings like one of the things that's the problem is that we often don't have a unified idea of what a scalar reward should be.",
            "Their symptoms, there are side effects, there's cost.",
            "There's all kinds of other stuff that comes into play and.",
            "An acknowledging that without throwing the baby out with the bathwater that is acknowledging that there are more than one reward, that we might care about.",
            "But at the same time, keeping the forward thinking individualization like sequential decision making stuff that's in our oh another and its related methods is what I want to do.",
            "So when you're doing OK, so I'm going to talk about a batch method.",
            "So this is Batch RL and I'm going to accommodate more than one reward and I'm going to do finite horizon.",
            "In fact, I'm just going to talk about two time steps because that's enough to make it interesting.",
            "Doesn't matter.",
            "So if you were doing fitted Q, what you would do is start at the last time step and you would try and estimate the Q function at time two.",
            "That's my last time step.",
            "Once you have that Q function at the last time step, what you do is in order to get your Q1 at the previous time step.",
            "You estimate that by assuming that you're going to act optimally with respect to the Q function at the last time step, right?",
            "So you take a Max over actions at all the states that you've got observed, and you use that to produce your estimate for Q at time one.",
            "Great, so an and you could take that Max if you have a single reward and there's you know your Q function at a particular states listed numbers, you pick the big one and then you take the associated action and that tells you your policy.",
            "Your single policy that you're prescribing at that last time step.",
            "So.",
            "If you have more than one reward that you're thinking about, then the idea of taking a Max falls apart because depending on preferences over rewards, you might care more or less about.",
            "Different things that might lead you to choose different actions."
        ],
        [
            "So what we're going to do is consider a list of relevant rewards instead of taking out Max, we're going to.",
            "Eliminate some of the actions that are definitely bad, like that operato dominated."
        ],
        [
            "Keep the remainder of those, and that's going to be our policy.",
            "Our set valued policy that will learn at the last time step."
        ],
        [
            "OK, so for example, if I've got two rewards that I care about.",
            "And I'm looking at a particular state S2.",
            "OK, I look at action one for each of these two rewards, reward why and Rewards Ed an under in this setting under.",
            "Actually when I get 99 like that's my vector valued reward and the other action I get 2 two.",
            "Well then you know I like action one no matter which reward I care about or even a mixture of them or whatever.",
            "So in that case my set valued policy will just say action one."
        ],
        [
            "If it was less simple and I had.",
            "You know, action one give me lots of why, but crappy said and then actually minus one gave me the other way around.",
            "Happens all the time in medicine, by the way, symptoms and side effects different treatments.",
            "You obviously often can have your cake and eat it too.",
            "So in this case we would say look I don't know what the preferences are of my decision maker.",
            "I'm going to produce a set valued policy myself value policy for that state will include both actions."
        ],
        [
            "So.",
            "OK, so you would have seen this kind of once earlier, But anyway doesn't matter.",
            "So now that I've got that, I can say, OK, I'm going to fit cues, so at the end I'll estimate my QYQZ and then once I've got that set valued policy I'll back up.",
            "Do the first step, but the problem is that I don't know what policy I'm going to follow at that last stage.",
            "I have a set valued policy.",
            "OK, but that's a non deterministic policy.",
            "It's an unmade choice.",
            "It's not a stochastic policy, so I don't have a way of estimating Q1 because I haven't yet made my decision about what I'm going to do a queue at time .2."
        ],
        [
            "So what I'm going to do instead is.",
            "Consider a set of non set valued policy's.",
            "Did you parse that?",
            "That are all compatible with the set value policy that I produced and compare."
        ],
        [
            "Ability just means."
        ],
        [
            "Um?",
            "The the action valued policy has to have to each choice it makes has to belong to the set valued policy.",
            "From that I just learned OK, so that's what compatibility is.",
            "It's just saying I'm going to consider some policies that match with the set value policy and."
        ],
        [
            "These are just some examples."
        ],
        [
            "And So what I'm going to do is learn for a set of these compatible pictues.",
            "What's the corresponding cube function at time one?",
            "So now my keyword timeline depends on state and action and the policy that I'm going to follow at the second timestamp as it always did before there was only one.",
            "OK. Anne."
        ],
        [
            "And so you get sort of a cloud of points.",
            "At a particular state, for each action, and the reason there's a cloud is that I'm following different policies.",
            "A couple of practical."
        ],
        [
            "Issues that I will just highlight that are interesting to the computer scientists.",
            "There can be a lot of these compatible pictues which people a lot of people probably have already identified this because if I have a lot of if my set value policy for many states has more than one option, those all those options multiply.",
            "If I'm allowed to choose them independently and I get a ridiculous number of policies.",
            "However.",
            "If I bring in knowledge about how I'm representing my Q function, like if for example I'm using a linear Q function which is common in this setting.",
            "Just take my word for it.",
            "A lot of these policies alot of these size of the action set to the end.",
            "Policies are kind of senseless and should really be excluded.",
            "So I'm using the space that I have chosen for my Q functions to inform what collection of compatible policies I want to look at at 2nd."
        ],
        [
            "Date.",
            "Um?"
        ],
        [
            "Because if I don't, I can get, so this is a little toy example with a 1 dimensional state space.",
            "Oh, the Bell tolls for me.",
            "The top panel is a representation of a set valued policy where for some parts of the state space I have a Singleton as my output in the middle.",
            "My set valued policy would be.",
            "Both actions would be in it.",
            "And then the policy underneath it is a deterministic policy that's consistent, but it's sort of crazy.",
            "I mean, if you imagine that this is some measurement of a person that you make this, you know, make these really, really fine complex divisions in order to accommodate that that from a practical standpoint, if this was like I don't know some sort of a measure of depressive symptoms.",
            "It would be indefensible to use this as a policy."
        ],
        [
            "So what we do instead is consider policies that are feature consistent, that is, policies that you might have learned with a scalar reward.",
            "Some scalar doesn't like a crazy terrible scalar reward, but they have to be learnable by some scalar reward, and then that cuts down the complexity from exponential in N 2 polynomial in an.",
            "So the exponent depends on sort of the capacity of space of functions that you're using for Q.",
            "So Natarajan dimension is like it's one of the multiclass analogs of VC dimension.",
            "So we've got a much better dependence.",
            "Now.",
            "The dependences on like the size of the function space you're thinking about rather than the size of the number of points in your data set."
        ],
        [
            "0."
        ],
        [
            "This makes things practical.",
            "Believe it or not, especially because the current data are available are often not big data other smaller data because a lot of them come from randomized trials that have more like a couple of 1000 examples rather than millions.",
            "So even though that dependence is still polynomial, the exponent can be kind of medium sized because they're short horizon and we're looking for simple rules.",
            "It makes it feasible to do this.",
            "I'm not going to detail about the results here, actually, because you can come to my poster and I will not talk to you further, but I hope I have impressed upon you."
        ],
        [
            "You that.",
            "Sequential decision support.",
            "Is an area that we could be really making important contributions in.",
            "This is one step towards better methods in that space.",
            "What I would really like is a model of that includes the agent, the environment, and then also the decision maker's one collection together, and I think going forward that would be a nice way to formally formalize things so.",
            "Thank you again posters 222 there's a fair amount of detail actually extended abstract two, so if you are interested to know more, please have a look at that Ann."
        ],
        [
            "Thank you and I'd like to thank my partner in crime, Eric Labor at the back.",
            "Maybe I'll ask a question so.",
            "I'm.",
            "Another way to deal with this issue for for decision support could have been to, say provide some interface by which doctors can say.",
            "What if I cared in this proportion for these two reward functions were the policy look like and let them explore in some visualization.",
            "Weigh the consequences of changing the mixture into a scalar reward function and then let them.",
            "There are somewhat intelligent, they are.",
            "Yeah, the evidence suggests that yeah, you're absolutely right.",
            "And actually, I've worked on that.",
            "Previously, and we know how to do that in a.",
            "In the case of linear combinations of rewards, and you bring up a really excellent point in that part of this whole exercise.",
            "I mean, this was about what can you compute under certain assumptions, right?",
            "But the other piece of how exactly does one interact with the physician at the end of the day, once you've computed whatever you're going to compute is as important.",
            "An I would actually.",
            "I'm interested in moving to a more statistician would call a nonparametric setting where you almost.",
            "Illustrate the possible trajectories for a particular patient somehow.",
            "You're right, and so you would have those trajectories somehow displayed and the physician would be able to, like you say, adjust how they might want to behave in a future, and there would be a back and forth process of making that decision."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everybody, it's great to be here at like to thank the organizers for giving me this opportunity to tell you about something.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is very dear to my heart.",
                    "label": 0
                },
                {
                    "sent": "So I would like to talk today about an application area that I think is currently underserved by reinforcement learning, and I think it's important that we make an effort to push into this area because we as a community have the opportunity to have a significant impact.",
                    "label": 0
                },
                {
                    "sent": "So my thesis kind of today is that automating and supporting sequential decision making are similar, but they're different problems and I want to talk today about some methods that I've been working on that are specifically geared toward.",
                    "label": 1
                },
                {
                    "sent": "Supporting decisions that are made by humans and kind of ties in actually with endless talk and and Patrick's talk about augmenting the capabilities of humans.",
                    "label": 0
                },
                {
                    "sent": "Humans in the loop is a very important aspect of what I wanted to talk to you about today, so I'm just going to talk about one piece that I believe is important for using MPs.",
                    "label": 1
                },
                {
                    "sent": "NRL methods for decision support and that is accommodating the situation where you've got more than one reward that you actually do care about.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about this in the context of what sometimes called the mom DP literature.",
                    "label": 0
                },
                {
                    "sent": "There's a great survey that I recommend you read if you're interested.",
                    "label": 1
                },
                {
                    "sent": "I'm going to introduce a method that we're calling non deterministic fitted queue for multiple objectives.",
                    "label": 0
                },
                {
                    "sent": "Where we're going to produce a set valued policy instead of a policy that just gives you a single action, and I'm going to show you an example of how this might be used for decision support using some clinical trial data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we probably seen this box diagram like I don't know, maybe it doesn't.",
                    "label": 0
                },
                {
                    "sent": "I'm so far I want to show up, put it up there one more time and just for the sake of making things concrete, I think it's fair to say that for the computer, scientists in the crowd when they see this box diagram, what they're really thinking about in the agent boxes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of something like this.",
                    "label": 0
                },
                {
                    "sent": "Which is wonderful an the environment box would be something kind of like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the whole premise background is about producing an autonomous agent that is going to be able to behave in an excellent way in an environment to do awesome things.",
                    "label": 0
                },
                {
                    "sent": "And this model has lots of useful properties.",
                    "label": 0
                },
                {
                    "sent": "This way of thinking about problems, lots of useful properties, including being forward thinking, being adaptive to changes in state, and being able to work in a goal directed way by observing rewards and reacting to that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is an analogous setting in clinical decision support where you replace, by the way, any MD's in the audience.",
                    "label": 0
                },
                {
                    "sent": "Is there a doctor in the house?",
                    "label": 0
                },
                {
                    "sent": "No MD's OK so I'm OK, but they could be online.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's natural to at least consider that this framework you know you could think about the doc as an agent.",
                    "label": 0
                },
                {
                    "sent": "Anna patient as an environment.",
                    "label": 0
                },
                {
                    "sent": "And the doc is choosing treatments or actions.",
                    "label": 0
                },
                {
                    "sent": "You know sequentially overtime, especially if you're talking about a chronic disease, the patient's health state is evolving.",
                    "label": 0
                },
                {
                    "sent": "The doctors responding to that, and the doc is also observing some kind of happiness that the patient has with their health status.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's probably some joint happiness of the patient and the doc together, which is sort of analogous to reward.",
                    "label": 0
                },
                {
                    "sent": "So if I'm in a different type of meeting, I use different words sometimes.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is it's a reasonable fit.",
                    "label": 0
                },
                {
                    "sent": "OK, so given that we have this way of thinking about problems, how can we leverage it include?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decision support.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I really want to drive this home.",
                    "label": 0
                },
                {
                    "sent": "The reason I think we could make an important impact is that we have expertise as a community and as individuals in models for describing problems where we want to maximize long-term success.",
                    "label": 0
                },
                {
                    "sent": "We want to make context independent decisions.",
                    "label": 0
                },
                {
                    "sent": "Decisions that are based on state individualization is is the keyword that you might use.",
                    "label": 0
                },
                {
                    "sent": "If you're in a medical setting, OK and the model handles uncertain futures naturally, we obviously don't know.",
                    "label": 1
                },
                {
                    "sent": "We often do not know exactly how a patient is going to respond to treatment.",
                    "label": 0
                },
                {
                    "sent": "That's just how life is.",
                    "label": 0
                },
                {
                    "sent": "But the model has that in it.",
                    "label": 1
                },
                {
                    "sent": "And we can learn policies from data.",
                    "label": 0
                },
                {
                    "sent": "We can learn good policies from data potentially.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think the motivation is relatively easy at this level.",
                    "label": 0
                },
                {
                    "sent": "To make the claim that RLS, an MVP's in these related sequential decision making methods should be making impacts in clinical decisions, especially for chronic disease.",
                    "label": 0
                },
                {
                    "sent": "And I talked to computer scientists about tests and depending on their enthusiasm levels, sometimes they say.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "Awesome, let's do it.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are less energy than I am, that's the emergency medical hologram from Star Trek Voyager, and they often think.",
                    "label": 0
                },
                {
                    "sent": "Great, we will have total control.",
                    "label": 0
                },
                {
                    "sent": "And I admire their enthusiasm I think.",
                    "label": 0
                },
                {
                    "sent": "And actually you know, especially given Clara stock, the first talk of the day about safety, an about marrying the idea of a safe space in which you can act with learning and evolving.",
                    "label": 0
                },
                {
                    "sent": "I think that that actually is one of the key things that is going to push us.",
                    "label": 0
                },
                {
                    "sent": "Someone in this direction, again no MD's in the House, so I can say that but really.",
                    "label": 0
                },
                {
                    "sent": "We can make a much better impact now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By instead of thinking about replacing the agent that's making the decisions, let's talk about how can we support the decisions that are being made by a human.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And if anybody wants to get a better sense, maybe of why this has to be so nuanced, this communication with position, I recommend that you Google the following terms.",
                    "label": 0
                },
                {
                    "sent": "This could be like interactive.",
                    "label": 0
                },
                {
                    "sent": "If we want, you can open a new tab next to Facebook.",
                    "label": 0
                },
                {
                    "sent": "Evidence.",
                    "label": 0
                },
                {
                    "sent": "Based medicine rubbish.",
                    "label": 0
                },
                {
                    "sent": "And look for a talk by a woman named Trish.",
                    "label": 0
                },
                {
                    "sent": "It is fantastic and really it's.",
                    "label": 0
                },
                {
                    "sent": "It's an anecdotal talk.",
                    "label": 0
                },
                {
                    "sent": "She does a great job of explaining the she's a physician herself and then she had a mishap that require treatment and she got exposed to evidence based medicine and it she tells a wonderful story an it really illustrates the kind of nuance we need to have if we're interacting in that space.",
                    "label": 0
                },
                {
                    "sent": "So recommend that video and.",
                    "label": 0
                },
                {
                    "sent": "So the thing that I wanted to talk about today was in clinical decision settings like one of the things that's the problem is that we often don't have a unified idea of what a scalar reward should be.",
                    "label": 0
                },
                {
                    "sent": "Their symptoms, there are side effects, there's cost.",
                    "label": 0
                },
                {
                    "sent": "There's all kinds of other stuff that comes into play and.",
                    "label": 0
                },
                {
                    "sent": "An acknowledging that without throwing the baby out with the bathwater that is acknowledging that there are more than one reward, that we might care about.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, keeping the forward thinking individualization like sequential decision making stuff that's in our oh another and its related methods is what I want to do.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing OK, so I'm going to talk about a batch method.",
                    "label": 0
                },
                {
                    "sent": "So this is Batch RL and I'm going to accommodate more than one reward and I'm going to do finite horizon.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm just going to talk about two time steps because that's enough to make it interesting.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So if you were doing fitted Q, what you would do is start at the last time step and you would try and estimate the Q function at time two.",
                    "label": 0
                },
                {
                    "sent": "That's my last time step.",
                    "label": 0
                },
                {
                    "sent": "Once you have that Q function at the last time step, what you do is in order to get your Q1 at the previous time step.",
                    "label": 0
                },
                {
                    "sent": "You estimate that by assuming that you're going to act optimally with respect to the Q function at the last time step, right?",
                    "label": 0
                },
                {
                    "sent": "So you take a Max over actions at all the states that you've got observed, and you use that to produce your estimate for Q at time one.",
                    "label": 0
                },
                {
                    "sent": "Great, so an and you could take that Max if you have a single reward and there's you know your Q function at a particular states listed numbers, you pick the big one and then you take the associated action and that tells you your policy.",
                    "label": 0
                },
                {
                    "sent": "Your single policy that you're prescribing at that last time step.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you have more than one reward that you're thinking about, then the idea of taking a Max falls apart because depending on preferences over rewards, you might care more or less about.",
                    "label": 0
                },
                {
                    "sent": "Different things that might lead you to choose different actions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to do is consider a list of relevant rewards instead of taking out Max, we're going to.",
                    "label": 0
                },
                {
                    "sent": "Eliminate some of the actions that are definitely bad, like that operato dominated.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Keep the remainder of those, and that's going to be our policy.",
                    "label": 0
                },
                {
                    "sent": "Our set valued policy that will learn at the last time step.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for example, if I've got two rewards that I care about.",
                    "label": 0
                },
                {
                    "sent": "And I'm looking at a particular state S2.",
                    "label": 0
                },
                {
                    "sent": "OK, I look at action one for each of these two rewards, reward why and Rewards Ed an under in this setting under.",
                    "label": 0
                },
                {
                    "sent": "Actually when I get 99 like that's my vector valued reward and the other action I get 2 two.",
                    "label": 0
                },
                {
                    "sent": "Well then you know I like action one no matter which reward I care about or even a mixture of them or whatever.",
                    "label": 0
                },
                {
                    "sent": "So in that case my set valued policy will just say action one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it was less simple and I had.",
                    "label": 0
                },
                {
                    "sent": "You know, action one give me lots of why, but crappy said and then actually minus one gave me the other way around.",
                    "label": 0
                },
                {
                    "sent": "Happens all the time in medicine, by the way, symptoms and side effects different treatments.",
                    "label": 0
                },
                {
                    "sent": "You obviously often can have your cake and eat it too.",
                    "label": 0
                },
                {
                    "sent": "So in this case we would say look I don't know what the preferences are of my decision maker.",
                    "label": 0
                },
                {
                    "sent": "I'm going to produce a set valued policy myself value policy for that state will include both actions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so you would have seen this kind of once earlier, But anyway doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So now that I've got that, I can say, OK, I'm going to fit cues, so at the end I'll estimate my QYQZ and then once I've got that set valued policy I'll back up.",
                    "label": 0
                },
                {
                    "sent": "Do the first step, but the problem is that I don't know what policy I'm going to follow at that last stage.",
                    "label": 0
                },
                {
                    "sent": "I have a set valued policy.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's a non deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "It's an unmade choice.",
                    "label": 0
                },
                {
                    "sent": "It's not a stochastic policy, so I don't have a way of estimating Q1 because I haven't yet made my decision about what I'm going to do a queue at time .2.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to do instead is.",
                    "label": 0
                },
                {
                    "sent": "Consider a set of non set valued policy's.",
                    "label": 0
                },
                {
                    "sent": "Did you parse that?",
                    "label": 0
                },
                {
                    "sent": "That are all compatible with the set value policy that I produced and compare.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ability just means.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The the action valued policy has to have to each choice it makes has to belong to the set valued policy.",
                    "label": 0
                },
                {
                    "sent": "From that I just learned OK, so that's what compatibility is.",
                    "label": 0
                },
                {
                    "sent": "It's just saying I'm going to consider some policies that match with the set value policy and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are just some examples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what I'm going to do is learn for a set of these compatible pictues.",
                    "label": 0
                },
                {
                    "sent": "What's the corresponding cube function at time one?",
                    "label": 0
                },
                {
                    "sent": "So now my keyword timeline depends on state and action and the policy that I'm going to follow at the second timestamp as it always did before there was only one.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you get sort of a cloud of points.",
                    "label": 0
                },
                {
                    "sent": "At a particular state, for each action, and the reason there's a cloud is that I'm following different policies.",
                    "label": 0
                },
                {
                    "sent": "A couple of practical.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issues that I will just highlight that are interesting to the computer scientists.",
                    "label": 0
                },
                {
                    "sent": "There can be a lot of these compatible pictues which people a lot of people probably have already identified this because if I have a lot of if my set value policy for many states has more than one option, those all those options multiply.",
                    "label": 0
                },
                {
                    "sent": "If I'm allowed to choose them independently and I get a ridiculous number of policies.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "If I bring in knowledge about how I'm representing my Q function, like if for example I'm using a linear Q function which is common in this setting.",
                    "label": 0
                },
                {
                    "sent": "Just take my word for it.",
                    "label": 0
                },
                {
                    "sent": "A lot of these policies alot of these size of the action set to the end.",
                    "label": 0
                },
                {
                    "sent": "Policies are kind of senseless and should really be excluded.",
                    "label": 0
                },
                {
                    "sent": "So I'm using the space that I have chosen for my Q functions to inform what collection of compatible policies I want to look at at 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Date.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because if I don't, I can get, so this is a little toy example with a 1 dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "Oh, the Bell tolls for me.",
                    "label": 0
                },
                {
                    "sent": "The top panel is a representation of a set valued policy where for some parts of the state space I have a Singleton as my output in the middle.",
                    "label": 0
                },
                {
                    "sent": "My set valued policy would be.",
                    "label": 0
                },
                {
                    "sent": "Both actions would be in it.",
                    "label": 0
                },
                {
                    "sent": "And then the policy underneath it is a deterministic policy that's consistent, but it's sort of crazy.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you imagine that this is some measurement of a person that you make this, you know, make these really, really fine complex divisions in order to accommodate that that from a practical standpoint, if this was like I don't know some sort of a measure of depressive symptoms.",
                    "label": 0
                },
                {
                    "sent": "It would be indefensible to use this as a policy.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do instead is consider policies that are feature consistent, that is, policies that you might have learned with a scalar reward.",
                    "label": 0
                },
                {
                    "sent": "Some scalar doesn't like a crazy terrible scalar reward, but they have to be learnable by some scalar reward, and then that cuts down the complexity from exponential in N 2 polynomial in an.",
                    "label": 0
                },
                {
                    "sent": "So the exponent depends on sort of the capacity of space of functions that you're using for Q.",
                    "label": 0
                },
                {
                    "sent": "So Natarajan dimension is like it's one of the multiclass analogs of VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So we've got a much better dependence.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The dependences on like the size of the function space you're thinking about rather than the size of the number of points in your data set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "0.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This makes things practical.",
                    "label": 0
                },
                {
                    "sent": "Believe it or not, especially because the current data are available are often not big data other smaller data because a lot of them come from randomized trials that have more like a couple of 1000 examples rather than millions.",
                    "label": 0
                },
                {
                    "sent": "So even though that dependence is still polynomial, the exponent can be kind of medium sized because they're short horizon and we're looking for simple rules.",
                    "label": 0
                },
                {
                    "sent": "It makes it feasible to do this.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to detail about the results here, actually, because you can come to my poster and I will not talk to you further, but I hope I have impressed upon you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You that.",
                    "label": 0
                },
                {
                    "sent": "Sequential decision support.",
                    "label": 0
                },
                {
                    "sent": "Is an area that we could be really making important contributions in.",
                    "label": 0
                },
                {
                    "sent": "This is one step towards better methods in that space.",
                    "label": 0
                },
                {
                    "sent": "What I would really like is a model of that includes the agent, the environment, and then also the decision maker's one collection together, and I think going forward that would be a nice way to formally formalize things so.",
                    "label": 0
                },
                {
                    "sent": "Thank you again posters 222 there's a fair amount of detail actually extended abstract two, so if you are interested to know more, please have a look at that Ann.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and I'd like to thank my partner in crime, Eric Labor at the back.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll ask a question so.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Another way to deal with this issue for for decision support could have been to, say provide some interface by which doctors can say.",
                    "label": 0
                },
                {
                    "sent": "What if I cared in this proportion for these two reward functions were the policy look like and let them explore in some visualization.",
                    "label": 0
                },
                {
                    "sent": "Weigh the consequences of changing the mixture into a scalar reward function and then let them.",
                    "label": 0
                },
                {
                    "sent": "There are somewhat intelligent, they are.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the evidence suggests that yeah, you're absolutely right.",
                    "label": 0
                },
                {
                    "sent": "And actually, I've worked on that.",
                    "label": 0
                },
                {
                    "sent": "Previously, and we know how to do that in a.",
                    "label": 0
                },
                {
                    "sent": "In the case of linear combinations of rewards, and you bring up a really excellent point in that part of this whole exercise.",
                    "label": 0
                },
                {
                    "sent": "I mean, this was about what can you compute under certain assumptions, right?",
                    "label": 0
                },
                {
                    "sent": "But the other piece of how exactly does one interact with the physician at the end of the day, once you've computed whatever you're going to compute is as important.",
                    "label": 0
                },
                {
                    "sent": "An I would actually.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in moving to a more statistician would call a nonparametric setting where you almost.",
                    "label": 0
                },
                {
                    "sent": "Illustrate the possible trajectories for a particular patient somehow.",
                    "label": 0
                },
                {
                    "sent": "You're right, and so you would have those trajectories somehow displayed and the physician would be able to, like you say, adjust how they might want to behave in a future, and there would be a back and forth process of making that decision.",
                    "label": 0
                }
            ]
        }
    }
}